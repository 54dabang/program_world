第1章 分布式单词计数
1.1 Storm topology的组成部分——stream、spout和bolt
1.1.1 Storm
1.1.2 spout
1.1.3 bolt
1.2 单词计数topology的数据流
1.2.1 语句生成bolt
1.2.2 语句分割bolt
1.2.3 单词计割bolt
1.2.4 上报bolt
1.3 实现单词计数top
1.3.1 配置开发环境
1.3.2 实现Sentence
1.3.3 实现语句分割bolt
1.3.4 实现单词计割bolt
1.3.5 实现上报bolt
1.3.6 实现单词计数topo
1.4 Storm的并发机制
1.4.1 WordCountTopology的并发机制
1.4.2 给topology增加woker
1.4.3 配置executor和task
1.5 理解数据流分组
1.6 有保障机制的数据处理
1.6.1 spout的可靠性
1.6.2 bolt的可靠性
1.6.3 可靠的单词计数
总结

第2章 配置Storm集群
2.1 Storm集群的框架
2.1.1 理解nimbus守护进程
2.1.2 supervisor守护进程的工作方式
2.1.3 Apache ZooKeeper简介
……
第3章 Trident和传感器数据
第4章 实时趋势分析
第5章 实时图形分析
第6章 人工智能
第7章 整合Druid进行金融分析
第8章 自然语言处理
第9章 在Hadoop上部署Storm进行广告分析
第10章 云环境下的S

目 录
第1章 大数据技术前景及分析平台 1
1.1 大数据的概念 1
1.2 大数据的维度范式 2
1.3 大数据生态系统 3
1.4 大数据基础设施 4
1.5 大数据生态系统组件 5
1.5.1 构建业务解决方案 8
1.5.2 数据集处理 8
1.5.3 解决方案实施 8
1.5.4 呈现 9
1.6 分布式批处理 9
1.7 分布式数据库（NoSQL） 13
1.7.1 NoSQL数据库的优势 15
1.7.2 选择NoSQL数据库 16
1.8 实时处理 16
1.8.1 电信或移动通信场景 17
1.8.2 运输和物流 17
1.8.3 互联的车辆 18
1.8.4 金融部门 18
1.9 本章小结 18
第2章 熟悉Storm 19
2.1 Storm概述 19
2.2 Storm的发展 20
2.3 Storm的抽象概念 22
2.3.1 流 22
2.3.2 拓扑 22
2.3.3 Spout 23
2.3.4 Bolt 23
2.3.5 任务 24
2.3.6 工作者 25
2.4 Storm的架构及其组件 25
2.4.1 Zookeeper集群 25
2.4.2 Storm集群 25
2.5 如何以及何时使用Storm 27
2.6 Storm的内部特性 32
2.6.1 Storm的并行性 32
2.6.2 Storm的内部消息处理 34
2.7 本章小结 36
第3章 用Storm处理数据 37
3.1 Storm输入数据源 37
3.2 认识Kafka 38
3.2.1 关于Kafka的更多知识 39
3.2.2 Storm的其他输入数据源 43
3.2.3 Kafka作为输入数据源 46
3.3 数据处理的可靠性 47
3.3.1 锚定的概念和可靠性 49
3.3.2 Storm的acking框架 51
3.4 Storm的简单模式 52
3.4.1 联结 52
3.4.2 批处理 53
3.5 Storm的持久性 53
3.6 本章小结 58
第4章 Trident概述和Storm性能优化 59
4.1 使用Trident 59
4.1.1 事务 60
4.1.2 Trident 拓扑 60
4.1.3 Trident操作 61
4.2 理解LMAX 65
4.2.1 内存和缓存 66
4.2.2 环形缓冲区—粉碎器的心脏 69
4.3 Storm的节点间通信 72
4.3.1 ZeroMQ 73
4.3.2 Storm的ZeroMQ配置 74
4.3.3 Netty 74
4.4 理解Storm UI 75
4.4.1 Storm UI登录页面 75
4.4.2 拓扑首页 78
4.5 优化Storm性能 80
4.6 本章小结 83
第5章 熟悉Kinesis 84
5.1 Kinesis架构概述 84
5.1.1 Amazon Kinesis的优势和用例 84
5.1.2 高级体系结构 86
5.1.3 Kinesis的组件 87
5.2 创建Kinesis流服务 90
5.2.1 访问AWS 90
5.2.2 配置开发环境 91
5.2.3 创建Kinesis流 93
5.2.4 创建Kinesis流生产者 97
5.2.5 创建Kinesis流消费者 102
5.2.6 产生和消耗犯罪警报 102
5.3 本章小结 105


第一部分　准备工作
第1章　基础环境准备 2
1.1　软件环境准备 2
1.2　集群环境准备 4
1.2.1　Zookeeper集群部署 4
1.2.2　Hadoop部署 6
1.3　小结 15
第二部分　核心技术
第2章　Spark详解 18
2.1　Spark概述 18
2.1.1　Spark概述 18
2.1.2　Shuff?le详解 25
2.2　Spark SQL 29
2.2.1　SparkSession 29
2.2.2　DataFrame 30
2.2.3　DataSet 35
2.3　Structured Streaming 35
2.3.1　数据源 36
2.3.2　输出到外部存储 38
2.3.3　WordCount示例 40
2.4　Spark优化 42
2.4.1　数据优化 42
2.4.2　代码优化 44
2.4.3　参数优化 46
2.5　小结 48
第3章　Druid原理及部署 49
3.1　架构设计 49
3.1.1　节点类型 49
3.1.2　Segment介绍 57
3.1.3　容错处理 59
3.1.4　路由节点 60
3.2　集群部署 63
3.2.1　集群规划 63
3.2.2　配置安装 64
3.3　小结 72
第4章　Druid数据摄入 73
4.1　模式设计 73
4.1.1　设计概述 73
4.1.2　数据解析 75
4.1.3　Segment分区 79
4.1.4　模式更改 81
4.2　批量数据摄入 81
4.3　流数据摄入 87
4.3.1　Tranquility 88
4.3.2　StreamPush 91
4.3.3　从Kafka中摄取数据 92
4.4　数据更新 94
4.5　小结 95
第5章　Druid客户端 96
5.1　涉及组件 96
5.1.1　查询相关 96
5.1.2　过滤器 99
5.1.3　聚合粒度 101
5.1.4　聚合器 105
5.2　查询类型 109
5.2.1　时间序列查询 109
5.2.2　TopN查询 111
5.2.3　分组查询 113
5.2.4　元数据查询 117
5.2.5　搜索查询 121
5.3　查询API 125
5.3.1　RESTful介绍 125
5.3.2　Jersey客户端 126
5.4　小结 129
第6章　日志收集 130
6.1　Flume介绍 130
6.1.1　基本架构 131
6.2　Flume应用实践 144
6.2.1　拦截器、选择器实践 144
6.2.2　负载均衡、故障转移实践 149
6.2.3　设计与实践 150
6.3　小结 154
第7章　分布式消息队列 155
7.1　Kafka介绍 155
7.1.1　基本架构 155
7.1.2　高吞吐的实现 157
7.1.3　高可用的实现 160
7.2　安装部署 161
7.2.1　Broker配置参数 161
7.2.2　分布式部署 162
7.3　客户端API 163
7.3.1　Producer API 164
7.3.2　Consumer API 165
7.4　小结 169
第三部分　项目实践
第8章　数据平台 172
8.1　需求分析 172
8.2　功能实现 173
8.2.1　架构设计 173
8.2.2　关键功能实现 175
8.3　小结 184
第9章　监控系统 185
9.1　Inf?luxDB 185
9.1.1　Inf?luxDB简介 186
9.1.2　Inf?luxDB安装 186
9.1.3　Inf?luxDB操作 188
9.1.4　Inf?luxDB客户端 191
9.2　JMXTrans 192
9.2.1　JMXTrans介绍 192
9.2.2　JMXTrans安装 194
9.2.3　JMXTrans使用 195
9.3　Grafana 198
9.3.1　Grafana安装 198
9.3.2　Grafana使用 199
9.4　小结 208


第1章PythonSpark机器学习与Hadoop大数据1
1.1机器学习的介绍2
1.2Spark的介绍5
1.3Spark数据处理RDD、DataFrame、SparkSQL7
1.4使用Python开发Spark机器学习与大数据应用8
1.5PythonSpark机器学习9
1.6SparkMLPipeline机器学习流程介绍10
1.7Spark2.0的介绍12
1.8大数据定义13
1.9Hadoop简介14
1.10HadoopHDFS分布式文件系统14
1.11HadoopMapReduce的介绍17
1.12结论18
第2章VirtualBox虚拟机软件的安装19
2.1VirtualBox的下载和安装20
2.2设置VirtualBox存储文件夹23
2.3在VirtualBox创建虚拟机25
2.4结论29
第3章UbuntuLinux操作系统的安装30
3.1UbuntuLinux操作系统的安装31
3.2在Virtual设置Ubuntu虚拟光盘文件33
3.3开始安装Ubuntu35
3.4启动Ubuntu40
3.5安装增强功能41
3.6设置默认输入法45
3.7设置“终端”程序48
3.8设置“终端”程序为白底黑字49
3.9设置共享剪贴板50
3.10设置最佳下载服务器52
3.11结论56
第4章HadoopSingleNodeCluster的安装57
4.1安装JDK58
4.2设置SSH无密码登录61
4.3下载安装Hadoop64
4.4设置Hadoop环境变量67
4.5修改Hadoop配置设置文件69
4.6创建并格式化HDFS目录73
4.7启动Hadoop74
4.8打开HadoopResource-ManagerWeb界面76
4.9NameNodeHDFSWeb界面78
4.10结论79
第5章HadoopMultiNodeCluster的安装80
5.1把SingleNodeCluster复制到data183
5.2设置VirtualBox网卡84
5.3设置data1服务器87
5.4复制data1服务器到data2、data3、master94
5.5设置data2服务器97
5.6设置data3服务器100
5.7设置master服务器102
5.8master连接到data1、data2、data3创建HDFS目录107
5.9创建并格式化NameNodeHDFS目录110
5.10启动HadoopMultiNodeCluster112
5.11打开HadoopResourceManagerWeb界面114
5.12打开NameNodeWeb界面115
5.13停止HadoopMultiNodeCluster116
5.14结论116
第6章HadoopHDFS命令117
6.1启动HadoopMulti-NodeCluster118
6.2创建与查看HDFS目录120
6.3从本地计算机复制文件到HDFS122
6.4将HDFS上的文件复制到本地计算机127
6.5复制与删除HDFS文件129
6.6在HadoopHDFSWeb用户界面浏览HDFS131
6.7结论134
第7章HadoopMapReduce135
7.1简单介绍WordCount.java136
7.2编辑WordCount.java137
7.3编译WordCount.java141
7.4创建测试文本文件143
7.5运行WordCount.java145
7.6查看运行结果146
7.7结论147
第8章PythonSpark的介绍与安装148
8.1Scala的介绍与安装150
8.2安装Spark153
8.3启动pyspark交互式界面156
8.4设置pyspark显示信息157
8.5创建测试用的文本文件159
8.6本地运行pyspark程序161
8.7在HadoopYARN运行pyspark163
8.8构建SparkStandaloneCluster运行环境165
8.9在SparkStandalone运行pyspark171
8.10SparkWebUI界面173
8.11结论175
目录
第1章1
1.1引言1
1.2基本术2
1.3假设空间4
1.4归纳偏好6
1.5发展历程10
1.6应用现状13
1.7阅读材料16
习题19
参考文献20
休息一会儿22
第2章模型评估与选择23
2.1经验误差与过拟合23
2.2评估方法24
2.2.1留出法25
2.2.2交叉验证法26
2.2.3自助法27
2.2.4调参与最终模型28
2.3性能度量28
2.3.1错误率与精度29
2.3.2查准率、查全率与F130
2.3.3ROC与AUC33
2.3.4代价敏感错误率与代价曲线35
2.4比较检验37
2.4.1假设检验37
2.4.2交叉验证t检验40
2.4.3McNemar检验41
2.4.4Friedman检验与后续检验42
2.5偏差与方差44
2.6阅读材料46
习题48
参考文献49
休息一会儿51
第3章线性模型53
3.1基本形式53
3.2线性回归53
3.3对数几率回归57
3.4线性判别分析60
3.5多分类学习63
3.6类别不平衡问题66
3.7阅读材料67
习题69
参考文献70
休息一会儿72
第4章决策树73
4.1基本流程73
4.2划分选择75
4.2.1信息增益75
4.2.2增益率77
4.2.3基尼指数79
4.3剪枝处理79
4.3.1预剪枝80
4.3.2后剪枝82
4.4连续与缺失值83
4.4.1连续值处理83
4.4.2缺失值处理85
4.5多变量决策树88
4.6阅读材料92
习题93
参考文献94
休息一会儿95
第5章神经网络97
5.1神经元模型97
5.2感知机与多层网络98
5.3误差逆传播算法101
5.4全局最小与局部极小106
5.5其他常见神经网络108
5.5.1RBF网络108
5.5.2ART网络108
5.5.3SOM网络109
5.5.4级联相关网络110
5.5.5Elman网络111
5.5.6Boltzmann机111
5.6深度学习113
5.7阅读材料115
习题116
参考文献117
休息一会儿120
第6章支持向量机121
6.1间隔与支持向量121
6.2对偶问题123
6.3核函数126
6.4软间隔与正则化129
6.5支持向量回归133
6.6核方法137
6.7阅读材料139
习题141
参考文献142
休息一会儿145
第7章贝叶斯分类器147
7.1贝叶斯决策论147
7.2极大似然估计149
7.3朴素贝叶斯分类器150
7.4半朴素贝叶斯分类器154
7.5贝叶斯网156
7.5.1结构157
7.5.2学习159
7.5.3推断161
7.6EM算法162
7.7阅读材料164
习题166
参考文献167
休息一会儿169
第8章集成学习171
8.1个体与集成171
8.2Boosting173
8.3Bagging与随机森林178
8.3.1Bagging178
8.3.2随机森林179
8.4结合策略181
8.4.1平均法181
8.4.2投票法182
8.4.3学习法183
8.5多样性185
8.5.1误差--分歧分解185
8.5.2多样性度量186
8.5.3多样性增强188
8.6阅读材料190
习题192
参考文献193
休息一会儿196
第9章聚类197
9.1聚类任务197
9.2性能度量197
9.3距离计算199
9.4原型聚类202
9.4.1k均值算法202
9.4.2学习向量量化204
9.4.3高斯混合聚类206
9.5密度聚类211
9.6层次聚类214
9.7阅读材料217
习题220
参考文献221
休息一会儿224
第10章降维与度量学习225
10.1k近邻学习225
10.2低维嵌入226
10.3主成分分析229
10.4核化线性降维232
10.5流形学习234
10.5.1等度量映射234
10.5.2局部线性嵌入235
10.6度量学习237
10.7阅读材料240
习题242
参考文献243
休息一会儿246
第11章特征选择与稀疏学习247
11.1子集搜索与评价247
11.2过滤式选择249
11.3包裹式选择250
11.4嵌入式选择与L$_1$正则化252
11.5稀疏表示与字典学习254
11.6压缩感知257
11.7阅读材料260
习题262
参考文献263
休息一会儿266
第12章计算学习理论267
12.1基础知识267
12.2PAC学习268
12.3有限假设空间270
12.3.1可分情形270
12.3.2不可分情形272
12.4VC维273
12.5Rademacher复杂度279
12.6稳定性284
12.7阅读材料287
习题289
参考文献290
休息一会儿292
第13章半监督学习293
13.1未标记样本293
13.2生成式方法295
13.3半监督SVM298
13.4图半监督学习300
13.5基于分歧的方法304
13.6半监督聚类307
13.7阅读材料311
习题313
参考文献314
休息一会儿317
第14章概率图模型319
14.1隐马尔可夫模型319
14.2马尔可夫随机场322
14.3条件随机场325
14.4学习与推断328
14.4.1变量消去328
14.4.2信念传播330
14.5近似推断331
14.5.1MCMC采样331
14.5.2变分推断334
14.6话题模型337
14.7阅读材料339
习题341
参考文献342
休息一会儿345
第15章规则学习347
15.1基本概念347
15.2序贯覆盖349
15.3剪枝优化352
15.4一阶规则学习354
15.5归纳逻辑程序设计357
15.5.1最小一般泛化358
15.5.2逆归结359
15.6阅读材料363
习题365
参考文献366
休息一会儿369
第16章强化学习371
16.1任务与奖赏371
16.2$K$-摇臂赌博机373
16.2.1探索与利用373
16.2.2$\epsilon$-贪心374
16.2.3Softmax375
16.3有模型学习377
16.3.1策略评估377
16.3.2策略改进379
16.3.3策略迭代与值迭代381
16.4免模型学习382
16.4.1蒙特卡罗强化学习383
16.4.2时序差分学习386
16.5值函数近似388
16.6模仿学习390
16.6.1直接模仿学习391
16.6.2逆强化学习391
16.7阅读材料393
附录399
A矩阵399
B优化403
C概率分布409
后记417
索引419



第1章　Storm简介 1



1.1　什么是大数据 1



1.1.1　大数据的四大特性 2



1.1.2　大数据工具 3



1.2　Storm如何应用于大数据应用场景 5



1.3　为什么你希望使用Storm 9



1.4　小结 10



第2章　Storm核心概念 11



2.1　问题定义：GitHub提交数监控看板 11



2.1.1　数据：起点和终点 12



2.1.2　分解问题 12



2.2　Storm基础概念 13



2.2.1　拓扑 13



2.2.2　元组 15



2.2.3　流 16



2.2.4　spout 17



2.2.5　bolt 18



2.2.6　流分组 20



2.3　在Storm中实现GitHub提交数监控看板 22



2.3.1　建立一个Storm工程 22



2.3.2　实现spout 23



2.3.3　实现bolt 26



2.3.4　集成各个部分组成拓扑 29



2.4　小结 30



第3章　拓扑设计 31



3.1　拓扑设计方法 32



3.2　问题定义：一个社交热力图 32



3.3　将解决方案映射至Storm的逻辑 33



3.3.1　考虑数据流本身施加的要求 33



3.3.2　将数据点表示为元组 34



3.3.3　确定拓扑组成的步骤 35



3.4　设计的初步实现 38



3.4.1　spout：从数据源读取数据 38



3.4.2　bolt：连接至外部服务 39



3.4.3　bolt：将数据寄放在内存里 41



3.4.4　bolt：持久化存储到数据库 45



3.4.5　定义组件间的流分组策略 47



3.4.6　在本地集群模式中构建一个拓扑 48



3.5　扩展拓扑 49



3.5.1　理解Storm中的并行机制 50



3.5.2　调整拓扑配置来解决设计中遗留的瓶颈 54



3.5.3　调整拓扑以解决数据流中固有的瓶颈 60



3.6　拓扑的设计范式 63



3.6.1　分解为功能组件的设计方法 65



3.6.2　基于重分配来分解组件的设计方法 65



3.6.3　最简单的功能组件与最少的重分配次数 69



3.7　小结 70



第4章　设计健壮的拓扑 71



4.1　对可靠性的要求 71



4.2　问题定义：一个信用卡授权系统 72



4.2.1　有重试特性的概念性解决方案 72



4.2.2　定义数据点 74



4.2.3　在Storm上实现带有重试特性的方案 74



4.3　bolt基础实现 76



4.3.1　AuthorizeCreditCard的实现 76



4.3.2　ProcessedOrderNotification的实现 77



4.4　消息处理保障 78



4.4.1　元组状态：处理完成或失败 78



4.4.2　bolt中的锚定、应答和容错 80



4.4.3　spout在消息处理保障中的角色 84



4.5　回放语义 87



4.5.1　Storm中可靠性的级别 87



4.5.2　在Storm拓扑中检查仅一次处理 88



4.5.3　检查拓扑中的可靠性保障 89



4.6　小结 94



第5章　拓扑由本地到远程的实施 95



5.1　Storm集群 96



5.1.1　解析工作结点 98



5.1.2　基于信用卡授权拓扑的上下文来理解工作结点 99



5.2　Storm集群容错中的快速失败机制 100



5.3　安装Storm集群 101



5.3.1　配置Zookeeper集群 101



5.3.2　在 Storm 的主结点和工作结点上安装依赖组件 102



5.3.3　安装Storm到主结点和工作结点 102



5.3.4　通过storm.yaml配置主结点和工作结点 102



5.3.5　在监督机制下启动Nimbus和Supervisor 103



5.4　在Storm集群上运行拓扑 104



5.4.1　重新考虑如何将拓扑组件组合在一起 104



5.4.2　在本地模式下运行拓扑 105



5.4.3　在一个远程 Storm 集群上运行拓扑 105



5.4.4　在一个远程Storm集群上部署拓扑 106



5.5　Storm UI及其在集群中的角色 107



5.5.1　Storm UI：Storm集群概要 107



5.5.2　Storm UI：独立拓扑概要 111



5.5.3　Storm UI：独立spout/bolt概要 115



5.6　小结 118



第6章　对Storm进行调优 120



6.1　问题定义：Daily Deals!重生版 121



6.1.1　创建概念性解决方案 121



6.1.2　将方案转换为Storm设计 122



6.2　初始化实施 122



6.2.1　spout：读取自一个数据源 124



6.2.2　bolt：查找推荐商品 125



6.2.3　bolt：为每个商品查询详细信息 126



6.2.4　bolt：保存推荐的商品详情 127



6.3　调优：我想为它提速 128



6.3.1　Storm UI：调优的定位工具 128



6.3.2　为性能值建立一个基线集 130



6.3.3　判断瓶颈 131



6.3.4　spout：控制数据流入拓扑的速率 135



6.4　延迟率：当外部系统依然能正常工作时 137



6.4.1　在拓扑中模拟延迟 137



6.4.2　延迟的外因和内因 139



6.5　Storm的指标统计API 143



6.5.1　使用Storm的内建



CountMetric 143



6.5.2　设置一个指标接收器 144



6.5.3　创建一个自定义的SuccessRateMetric 145



6.5.4　创建一个自定义的MultiSuccessRateMetric 147



6.6　小结 149



第7章　资源冲突 150



7.1　调整一个工作结点上运行的工作进程数量 152



7.1.1　问题 152



7.1.2　解决方案 152



7.1.3　讨论 153



7.2　修改工作进程（JVM）上的内存分配 153



7.2.1　问题 153



7.2.2　解决方案 154



7.2.3　讨论 154



7.3　定位拓扑上运行的工作结点/进程 154



7.3.1　问题 154



7.3.2　解决方案 155



7.3.3　讨论 155



7.4　在一个Storm集群中的工作进程冲突 156



7.4.1　问题 157



7.4.2　解决方案 157



7.4.3　讨论 158



7.5　在一个工作进程（JVM）中的内存冲突 159


7.5.1　问题 162
