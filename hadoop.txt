第1章　数据挖掘基础2
1.1　某知名连锁餐饮企业的困惑2
1.2　从餐饮服务到数据挖掘3
1.3　数据挖掘的基本任务4
1.4　数据挖掘建模过程4
1.4.1　定义挖掘目标4
1.4.2　数据取样5
1.4.3　数据探索6
1.4.4　数据预处理12
1.4.5　挖掘建模14
1.4.6　模型评价14
1.5　餐饮服务中的大数据应用15
1.6　小结15
第2章　Hadoop基础16
2.1　概述16
2.1.1　Hadoop简介16
2.1.2　Hadoop生态系统17
2.2　安装与配置19
2.3　Hadoop原理26
2.3.1　Hadoop HDFS原理26
2.3.2　Hadoop MapReduce原理27
2.3.3　Hadoop YARN原理28
2.4　动手实践30
2.5　小结33
第3章　Hadoop生态系统：Hive34
3.1　概述34
3.1.1　Hive简介34
3.1.2　Hive安装与配置35
3.2　Hive原理38
3.2.1　Hive架构38
3.2.2　Hive的数据模型40
3.3　动手实践41
3.4　小结45
第4章　Hadoop生态系统：HBase46
4.1　概述46
4.1.1　HBase简介46
4.1.2　HBase安装与配置47
4.2　HBase原理50
4.2.1　HBase架构50
4.2.2　HBase与RDBMS51
4.2.3　HBase访问接口52
4.2.4　HBase数据模型53
4.3　动手实践54
4.4　小结61
第5章　大数据挖掘建模平台62
5.1　常用的大数据平台62
5.2　TipDM-HB大数据挖掘建模平台63
5.2.1　TipDM-HB大数据挖掘建模平台的功能63
5.2.2　TipDM-HB大数据挖掘建模平台操作流程及实例65
5.2.3　TipDM-HB大数据挖掘建模平台的特点67
5.3　小结68
第6章　挖掘建模69
6.1　分类与预测69
6.1.1　实现过程69
6.1.2　常用的分类与预测算法70
6.1.3　决策树71
6.1.4　Mahout中Random Forests算法的实现原理75
6.1.5　动手实践79
6.2　聚类分析83
6.2.1　常用聚类分析算法83
6.2.2　K-Means聚类算法84
6.2.3　Mahout中K-Means算法的实现原理88
6.2.4　动手实践90
6.3　关联规则93
6.3.1　常用的关联规则算法93
6.3.2　FP-Growth关联规则算法94
6.3.3　Mahout中Parallel Frequent Pattern Mining算法的实现原理98
6.3.4　动手实践100
6.4　协同过滤102
6.4.1　常用的协同过滤算法102
6.4.2　基于项目的协同过滤算法简介102
6.4.3　Mahout中Itembased Collaborative Filtering算法的实现原理103
6.4.4　动手实践106
6.5　小结109
实　战　篇
第7章　法律咨询数据分析与服务推荐112
7.1　背景与挖掘目标112
7.2　分析方法与过程114
7.2.1　数据抽取120
7.2.2　数据探索分析120
7.2.3　数据预处理125
7.2.4　模型构建130
7.3　上机实验139
7.4　拓展思考140
7.5　小结145
第8章　电商产品评论数据情感分析146
8.1　背景与挖掘目标146
8.2　分析方法与过程146
8.2.1　评论数据采集147
8.2.2　评论预处理150
8.2.3　文本评论分词155
8.2.4　构建模型155
8.3　上机实验167
8.4　拓展思考168
8.5　小结169
第9章　航空公司客户价值分析170
9.1　背景与挖掘目标170
9.2　分析方法与过程171
9.2.1　数据抽取174
9.2.2　数据探索分析174
9.2.3　数据预处理175
9.2.4　模型构建177
9.3　上机实验182
9.4　拓展思考183
9.5　小结183
第10章　基站定位数据商圈分析184
10.1　背景与挖掘目标184
10.2　分析方法与过程186
10.2.1　数据抽取186
10.2.2　数据探索分析187
10.2.3　数据预处理188
10.2.4　构建模型191
10.3　上机实验194
10.4　拓展思考195
10.5　小结195
第11章　互联网电影智能推荐196
11.1　背景与挖掘目标196
11.2　分析方法与过程197
11.2.1　数据抽取199
11.2.2　构建模型199
11.3　上机实验201
11.4　拓展思考202
11.5　小结203
第12章　家电故障备件储备预测分析204
12.1　背景与挖掘目标204
12.2　分析方法与过程206
12.2.1　数据探索分析207
12.2.2　数据预处理209
12.2.3　构建模型212
12.3　上机实验216
12.4　拓展思考217
12.5　小结217
第13章　市供水混凝投药量控制分析218
13.1　背景与挖掘目标218
13.2　分析方法与过程220
13.2.1　数据抽取221
13.2.2　数据探索分析221
13.2.3　数据预处理223
13.2.4　构建模型227
13.3　上机实验237
13.4　拓展思考238
13.5　小结239
第14章　基于图像处理的车辆压双黄线检测240
14.1　背景与挖掘目标240
14.2　分析方法与过程241
14.2.1　数据抽取242
14.2.2　数据探索分析242
14.2.3　数据预处理242
14.2.4　构建模型249
14.3　上机实验250
14.4　拓展思考250
14.5　小结251
第15章　基于Mahout的大数据挖掘开发254
15.1　概述254
15.2　环境配置255
15.3　基于Mahout算法接口的二次开发258
15.3.1　Mahout算法实例258
15.3.2　Mahout算法接口的二次开发示例259
15.4　小结271
第16章　基于TipDM-HB的数据挖掘二次开发272
16.1　概述272
16.1.1　TipDM-HB大数据挖掘建模平台服务接口272
16.1.2　Apache CXF简介276
16.2　TipDM-HB大数据挖掘建模平台服务开发实例277
16.2.1　环境配置277
16.2.2　开发实例280
16.3　小结288
参考资料289


第1章 大数据与Hadoop
1.1 什么是大数据
1.2 大数据的用途
1.3 并行计算
1.4 数据流
1.5 函数式程序设计与Lambda演算
1.6 MapReduce
1.7 大数据处理平台
1.8 Hadoop的由来和发展
1.9 Hadoop的MapReduce计算框架
1.10 Hadoop的分布式容错文件系统HDFS
第2章 研究方法
2.1 摘要卡片
2.2 情景分析
2.3 面向对象的程序设计
2.4 怎样阅读分析Hadoop的代码
第3章 Hadoop集群和YARN
3.1 Hadoop集群
3.2 Hadoop系统的结构
3.3 Hadoop的YARN框架
3.4 状态机
3.5 资源管理器ResourceManager
3.6 资源调度器ResourceScheduler
第4章 Hadoop的RPC机制
4.1 RPC与RMI
4.2 ProtoBuf
4.3 Java的Reflection机制
4.4 RM节点上的RPC服务
4.5 RPC客户端的创建
第5章 Hadoop作业的提交
5.1 从“地方”到“中央”
5.2 示例一：采用老API的ValueAggregatorJob
5.3 示例二：采用新API的WordCount
5.4 示例三：采用ToolRunner的QuasiMonteCarlo
5.5 从Job.submit()开始的第二段流程
5.6 YARNRunner和ResourceMgrDelegate
第6章 作业的调度与指派
6.1 作业的受理
6.2 NM节点的心跳和容器周转
6.3 容器的分配
第7章 NodeManager与任务投运
7.1 AMLauncher与任务投运
7.2 MRAppMaster或AM的创建
7.3 资源本地化
7.4 容器的投运
第8章 MRAppMaster与作业投运
8.1 MRAppMaster
8.2 App资源与容器
8.3 容器的跨节点投送和启动
8.4 目标节点上的容器投运
8.5 Uber模式下的本地容器分配与投运
8.6 任务的启动
8.7 MapTask的运行
8.8 ReduceTask的投运
第9章 YARN子系统的计算框架
9.1 MapReduce框架
9.2 Streaming框架
9.3 Chain框架
9.4 Client与ApplicationMaster
第10章 MapReduce框架中的数据流
10.1 数据流和工作流
10.2 Mapper的输入
10.3 Mapper的输出缓冲区MapOutputBuffer
10.4 作为Collector的MapOutputBuffer
10.5 环形缓冲区kvbuffer
10.6 对MapoutputBuffer的输出
10.7 Sort和Spill
10.8 Map计算的终结与Spill文件的合并
10.9 Reduce阶段
10.10 Merge
10.11 Reduce阶段的输入和输出
第11章 Hadoop的文件系统HDFS
11.1 文件的分布与容错
11.2 目录节点NameNode
11.3 FSNamesystem
11.4 文件系统目录FSDirectory
11.5 文件系统映像FsImage
11.6 文件系统更改记录FSEditLog
11.7 FSEditLog与Journal
11.8 EditLog记录的重演
11.9 版本升级与故障恢复
第12章 HDFS的DataNode
12.1 DataNode
12.2 数据块的存储
12.3 RamDisk复份的持久化存储
12.4 目录扫描线程DirectoryScanner
12.5 数据块扫描线程DataBlockScanner
第13章 DataNode与NameNode的互动
13.1 DataNode与NameNode的互动
13.2 心跳HeartBeat
13.3 BlockReport
第14章 DataNode间的互动
14.1 数据块的接收和存储
14.2 命令DNA_TRANSFER的执行
第15章 HDFS的文件访问
15.1 DistributedFileSystem和DFSClient
15.2 FsShell
15.3 HDFS的打开文件流程
15.4 HDFS的读文件流程
15.5 HDFS的创建文件流程
15.6 文件租约
15.7 HDFS的写文件流程
15.8 实例
第16章 Hadoop的容错机制
16.1 容错与高可用
16.2 HDFS的HA机制
16.3 NameNode的倒换
16.4 Zookeeper与自动倒换
16.5 YARN的HA机制
第17章 Hadoop的安全机制
17.1 大数据集群的安全问题
17.2 UGI、Token和ACL
17.3 UGI的来源和流转
17.4 Token的使用
第18章 Hadoop的人机界面
18.1 Hadoop的命令行界面
18.2 Hadoop的Web界面
18.3 Dependency Inject和Annotation
18.4 对网页的访问
第19章 Hadoop的部署和启动
19.1 Hadoop的运维脚本
19.2 Hadoop的部署与启动
19.3 Hadoop的日常使用
19.4 Hadoop平台的关闭
第20章 Spark的优化与改进
20.1 Spark与Hadoop
20.2 RDD与Stage——概念与思路
20.3 RDD的存储和引用
20.4 DStream
20.5 拓扑的灵活性和多样性
20.6 性能的提升
20.7 使用的方便性
20.8 几个重要的类及其作用
参考资料


第一部分　核心设计篇
第1章　HDFS的数据存储 2
1.1　HDFS内存存储 2
1.1.1　HDFS内存存储原理 2
1.1.2　Linux 虚拟内存盘 4
1.1.3　HDFS的内存存储流程分析 4
1.1.4　LAZY_PERSIST内存存储的使用 14
1.2　HDFS异构存储 15
1.2.1　异构存储类型 16
1.2.2　异构存储原理 17
1.2.3　块存储类型选择策略 22
1.2.4　块存储策略集合 24
1.2.5　块存储策略的调用 27
1.2.6　HDFS异构存储策略的不足之处 28
1.2.7　HDFS存储策略的使用 30
1.3　小结 31
第2章　HDFS的数据管理与策略选择 32
2.1　HDFS缓存与缓存块 32
2.1.1　HDFS物理层面缓存块 33
2.1.2　缓存块的生命周期状态 34
2.1.3　CacheBlock、UnCacheBlock场景触发 36
2.1.4　CacheBlock、UnCacheBlock缓存块的确定 38
2.1.5　系统持有的缓存块列表如何更新 39
2.1.6　缓存块的使用 40
2.1.7　HDFS缓存相关配置 40
2.2　HDFS中心缓存管理 42
2.2.1　HDFS缓存适用场景 43
2.2.2　HDFS缓存的结构设计 43
2.2.3　HDFS缓存管理机制分析 45
2.2.4　HDFS中心缓存疑问点 55
2.2.5　HDFS CacheAdmin命令使用 56
2.3　HDFS快照管理 58
2.3.1　快照概念 59
2.3.2　HDFS中的快照相关命令 59
2.3.3　HDFS内部的快照管理机制 60
2.3.4　HDFS的快照使用 71
2.4　HDFS副本放置策略 72
2.4.1　副本放置策略概念与方法 72
2.4.2　副本放置策略的有效前提 73
2.4.3　默认副本放置策略的分析 73
2.4.4　目标存储好坏的判断 82
2.4.5　chooseTargets的调用 83
2.4.6　BlockPlacementPolicyWithNodeGroup继承类 84
2.4.7　副本放置策略的结果验证 85
2.5　HDFS内部的认证机制 85
2.5.1　BlockToken认证 85
2.5.2　HDFS的Sasl认证 91
2.5.3　BlockToken认证与HDFS的Sasl认证对比 97
2.6　HDFS内部的磁盘目录服务 98
2.6.1　HDFS的三大磁盘目录检测扫描服务 98
2.6.2　DiskChecker：坏盘检测服务 99
2.6.3　DirectoryScanner：目录扫描服务 104
2.6.4　VolumeScanner：磁盘目录扫描服务 110
2.7　小结 116
第3章　HDFS的新颖功能特性 117
3.1　HDFS视图文件系统：ViewFileSystem 117
3.2　HDFS的Web文件系统：WebHdfsFileSystem 126
3.3　HDFS数据加密空间：Encryption zone 136
3.4　HDFS纠删码技术 145
3.5　HDFS对象存储：Ozone 152
3.6　小结 158
第二部分　细节实现篇
第4章　HDFS的块处理 160
4.1　HDFS块检查命令fsck 160
4.2　HDFS如何检测并删除多余副本块 171
4.3　HDFS数据块的汇报与处理 179
4.4　小结 193
第5章　HDFS的流量处理 194
5.1　HDFS的内部限流 194
5.2　数据平衡 204
5.3　HDFS节点内数据平衡 210
5.4　小结 216
第6章　HDFS的部分结构分析 217
6.1　HDFS镜像文件的解析与反解析 217
6.2　DataNode数据处理中心DataXceiver 227
6.3　HDFS邻近信息块：BlockInfoContiguous 235
6.4　小结 246
第三部分　解决方案篇
第7章　HDFS的数据管理 248
7.1　HDFS的读写限流方案 248
7.2　HDFS数据资源使用量分析以及趋势预测 250
7.3　HDFS数据迁移解决方案 257
7.4　DataNode迁移方案 265
7.5　HDFS集群重命名方案 268
7.6　HDFS的配置管理方案 271
7.7　小结 273
第8章　HDFS的数据读写 274
8.1　DataNode引用计数磁盘选择策略 274
8.2　Hadoop节点“慢磁盘”监控 282
8.3　小结 287
第9章　HDFS的异常场景 288
9.1　DataNode慢启动问题 288
9.2　Hadoop中止下线操作后大量剩余复制块问题 295
9.3　DFSOutputStream的DataStreamer线程泄漏问题 306
9.4　小结 319
附录　如何向开源社区提交自己的代码 320


序 xi
前言 xii
第1 章　引言 1
1．1　安全概览 1
1．1．1　机密性 2
1．1．2　完整性 2
1．1．3　可用性 2
1．1．4　验证、授权和审计 3
1．2　Hadoop 安全：简史 5
1．3　Hadoop 组件和生态系统 5
1．3．1　Apache HDFS 6
1．3．2　Apache YARN 7
1．3．3　Apache MapReduce 8
1．3．4　Apache Hive 9
1．3．5　Cloudera Impala 9
1．3．6　Apache Sentry 10
1．3．7　Apache　HBase 11
1．3．8　Apache Accumulo 11
1．3．9　Apache Solr．13
1．3．10　Apache Oozie 13
1．3．11　Apache ZooKeeper 13
1．3．12　Apache Flume ．13
1．3．13　Apache Sqoop ．14
vi ｜ 目录
1．3．14　Cloudera　Hue 14
1．4　小结 ．14
第一部分　安全架构
第2 章　保护分布式系统 ．16
2．1　威胁种类 17
2．1．1　非授权访问／伪装 17
2．1．2　内在威胁 ．17
2．1．3　拒绝服务 ．18
2．1．4　数据威胁 ．18
2．2　威胁和风险评估 18
2．2．1　用户评估 ．19
2．2．2　环境评估 ．19
2．3　漏洞 ．19
2．4　深度防御 20
2．5　小结 ．21
第3 章　系统架构 22
3．1　运行环境 22
3．2　网络安全 23
3．2．1　网络划分 ．23
3．2．2　网络防火墙 24
3．2．3　入侵检测和防御 ．25
3．3　Hadoop 角色和隔离策略 27
3．3．1　主节点 28
3．3．2　工作节点 ．29
3．3．3　管理节点 ．29
3．3．4　边界节点 ．30
3．4　操作系统安全 31
3．4．1　远程访问控制 31
3．4．2　主机防火墙 31
3．4．3　SELinux 33
3．5　小结 ．34
第4 章　Kerberos 35
4．1　为什么是Kerberos ．35
4．2　Kerberos 概览 36
4．3　Kerberos 工作流：一个简单示例 ．37
目录 ｜ vii
4．4　Kerberos 信任 38
4．5　MIT Kerberos ．39
4．5．1　服务端配置 41
4．5．2　客户端配置 44
4．6　小结 ．46
第二部分　验证、授权和审计
第5 章　身份和验证 ．48
5．1　身份 ．48
5．1．1　将Kerberos 主体映射为用户名 ．49
5．1．2　Hadoop 用户到组的映射 50
5．1．3　Hadoop 用户配置 54
5．2　身份验证 54
5．2．1　Kerberos 55
5．2．2　用户名和密码验证 56
5．2．3　令牌 56
5．2．4　用户模拟 ．59
5．2．5　配置 60
5．3　小结 ．70
第6 章　授权 71
6．1　HDFS 授权 71
HDFS 扩展ACL ．72
6．2　服务级授权 ．74
6．3　MapReduce 和YARN 的授权 ．85
6．3．1　MapReduce（MR1） 86
6．3．2　YARN　(MR2)　 87
6．6　HBase 和Accumulo 的授权 95
6．6．1　系统、命名空间和表级授权 95
6．6．2　列级别和单元级别授权 ．99
6．7　小结 ．99
第7 章　Apache Sentry（孵化中） 100
7．1　Sentry 概念 100
7．2　Sentry 服务 102
7．3　Hive 授权 105
7．4　Impala 授权 110
7．5　Solr 授权 112
viii ｜ 目录
7．6　Sentry 特权模型 113
7．6．1　SQL 特权模型 114
7．6．2　Solr 特权模型 ．116
7．7　Sentry 策略管理 118
7．7．1　SQL 命令 118
7．7．2　SQL 策略文件 121
7．7．3　Solr 策略文件 ．123
7．7．4　策略文件的验证和校验 124
7．7．5　从策略文件迁移 126
7．8　小结 127
第8 章　审计 ．128
8．1　HDFS 审计日志 ．129
8．2　MapReduce 审计日志 ．130
8．3　YARN 审计日志132
8．4　Hive 审计日志 134
8．5　Cloudera　Impala 审计日志 134
8．6　HBase 审计日志 135
8．7　Accumulo 审计日志 137
8．8　Sentry 审计日志 139
8．9　日志聚合 140
8．10　小结 141
第三部分　数据安全
第9 章　数据保护 ．144
9．1　加密算法 144
9．2　静态数据加密 ．145
9．2．1　加密和密钥管理 146
9．2．2　HDFS 静态数据加密 ．146
9．2．3　MapReduce2 中间数据加密 151
9．2．4　Impala 磁盘溢出加密 152
9．2．5　全盘加密 152
9．2．6　文件系统加密 154
9．2．7　Hadoop 中重要数据的安全考虑 ．155
9．3　动态数据加密 ．156
9．3．1　传输层安全 ．156
9．3．2　Hadoop 动态数据加密 157
目录 ｜ ix
9．4　数据销毁和删除 162
9．5　小结 163
第10 章　数据导入安全 ．164
10．1　导入数据的完整性 165
10．2　数据导入的机密性 166
10．2．1　Flume 加密 167
10．2．2　Sqoop 加密 173
10．3　导入工作流 178
10．4　企业架构 ．179
10．5　小结 180
第11 章　数据提取和客户端访问安全 181
11．1　Hadoop 命令行接口 ．182
11．2　保护应用安全 183
11．3　HBase 184
11．3．1　HBase shell 184
11．3．2　HBase REST 网关 186
11．3．3　HBase Thrift 网关 189
11．4　Accumulo 190
11．4．1　Accumulo shell 190
11．4．2　Accumulo 代理服务 192
11．5　Oozie ．192
11．6　Sqoop ．194
11．7　SQL 访问 195
11．7．1　Impala ．195
11．7．2　Hive ．200
11．8　WebHDFS/HttpFS　 208
11．9　小结 209
第12 章　Cloudera Hue ．210
12．1　Hue HTTPS 211
12．2　Hue 身份验证 212
12．2．1　SPNEGO 后端 212
12．2．2　SAML 后端 ．213
12．2．3　LDAP 后端 ．215
12．3　Hue 授权 ．218
12．4　Hue SSL 客户端配置 219
12．5　小结 219
x ｜ 目录
第四部分　综合应用
第13 章　案例分析 ．222
13．1　案例分析：Hadoop 数据仓库 222
13．1．1　环境搭建 223
13．1．2　用户体验 226
13．1．3　小结 ．229
13．2　案例分析：交互式HBase　Web 应用 ．230
13．2．1　设计与架构 ．230
13．2．2　安全需求 231
13．2．3　集群配置 232
13．2．4　实现中的注意事项 ．236
13．2．5　小结 ．237
后记 ．238
关于作者 ．240
关于封面 ．240

第1章 绪论 3
1．1 时代背景 3
1．1．1 全球大数据浪潮 3
1．1．2 我国的大数据国家战略 5
1．2 大数据的概念 7
1．2．1 概念 7
1．2．2 特征 8
1．3 技术支撑体系 9
1．3．1 概览 9
1．3．2 大数据采集层 9
1．3．3 大数据存储层 10
1．3．4 大数据分析（处理与服务）层 11
1．3．5 大数据应用层 11
1．3．6 垂直视图 13
1．4 大数据人才及其能力要求 14
1．4．1 首席数据官 14
1．4．2 数据科学家（数据分析师） 15
1．4．3 大数据开发工程师 16
1．4．4 大数据运维工程师 17
1．5 本章小结 17
第2章 Hadoop大数据关键技术 19
2．1 Hadoop生态系统 19
2．1．1 架构的基本理论 19
2．1．2 主要组件及其关系 21
2．2 数据采集 24
2．2．1 结构化数据采集工具 24
2．2．2 日志文件采集工具与技术 25
2．3 大数据存储技术 29
2．3．1 相关概念 29
2．3．2 分布式文件存储系统 34
2．3．3 数据库与数据仓库 38
2．4 分布式计算框架 43
2．4．1 离线计算框架 43
2．4．2 实时流计算平台 50
2．5 数据分析平台与工具 57
2．5．1 面向大数据的数据挖掘与分析工具 57
2．5．2 机器学习 61
2．6 本章小结 66
第二篇 Hadoop大数据平台搭建与基本应用
第3章 Linux操作系统与集群搭建 69
3．1 Linux操作系统 69
3．1．1 概述 69
3．1．2 特点 70
3．1．3 Linux的组成 72
3．2 Linux安装与集群搭建 75
3．2．1 安装VMware Workstation 75
3．2．2 在VMware上安装Linux（CentOS7） 79
3．3 集群的配置 91
3．3．1 设置主机名 91
3．3．2 网络设置 93
3．3．3 关闭防火墙 98
3．3．4 安装JDK 99
3．3．5 免密钥登录配置 102
3．4 Linux基本命令 105
3．5 本章小结 112
第4章 HDFS安装与基本应用 113
4．1 HDFS概述 113
4．1．1 特点 113
4．1．2 主要组件与架构 114
4．2 HDFS架构分析 114
4．2．1 数据块 114
4．2．2 NameNode 115
4．2．3 DataNode 116
4．2．4 SecondaryNameNode 117
4．2．5 数据备份 117
4．2．6 通信协议 118
4．2．7 可靠性保证 118
4．3 文件操作过程分析 119
4．3．1 读文件 119
4．3．2 写文件 120
4．3．3 删除文件 122
4．4 Hadoop HDFS安装与配置 122
4．4．1 解压Hadoop安装包 122
4．4．2 配置Hadoop环境变量 123
4．4．3 配置Yarn环境变量 124
4．4．4 配置核心组件文件 125
4．4．5 配置文件系统 125
4．4．6 配置yarn-site．xml文件 126
4．4．7 配置MapReduce计算框架文件 128
4．4．8 配置Master的slaves文件 129
4．4．9 复制Master上的Hadoop到Slave节点 129
4．5 Hadoop集群的启动 130
4．5．1 配置操作系统环境变量 130
4．5．2 创建Hadoop数据目录 131
4．5．3 格式化文件系统 132
4．5．4 启动和关闭Hadoop 133
4．5．5 验证Hadoop是否启动成功 133
4．6 Hadoop集群的基本应用 136
4．6．1 HDFS基本命令 136
4．6．2 在Hadoop集群中运行程序 139
4．7 本章小结 141
第5章 MapReduce与Yarn 143
5．1 MapReduce程序的概念 143
5．1．1 基本编程模型 143
5．1．2 计算过程分析 144
5．2 深入理解Yarn 147
5．2．1 Yarn的基本架构 147
5．2．2 Yarn的工作流程 151
5．3 在Linux平台安装Eclipse 152
5．3．1 Eclipse简介 153
5．3．2 安装并启动Eclipse 154
5．4 开发MapReduce程序的基本方法 155
5．4．1 为Eclipse安装Hadoop插件 156
5．4．2 WordCount：第一个MapReduce程序 160
5．5 本章小结 175
第6章 Hive和HBase的安装与应用 177
6．1 在CentOS7下安装MySQL 177
6．1．1 下载或复制MySQL安装包 177
6．1．2 执行安装命令 178
6．1．3 启动MySQL 179
6．1．4 登录MySQL 179
6．1．5 使用MySQL 181
6．1．6 问题与解决办法 182
6．2 Hive安装与应用 183
6．2．1 下载并解压Hive安装包 183
6．2．2 配置Hive 184
6．2．3 启动并验证Hive 187
6．2．4 Hive的基本应用 189
6．3 ZooKeeper集群安装 190
6．3．1 ZooKeeper简介 190
6．3．2 安装ZooKeeper 191
6．3．3 配置ZooKeeper 191
6．3．4 启动和测试 193
6．4 HBase的安装与应用 195
6．4．1 解压并安装HBase 195
6．4．2 配置HBase 196
6．4．3 启动并验证HBase 199
6．4．4 HBase的基本应用 200
6．4．5 应用HBase中常见问题及其解决办法 203
6．5 本章小结 204
第7章 Sqoop和Kafka的安装与应用 205
7．1 安装部署Sqoop 205
7．1．1 下载或复制Sqoop安装包 205
7．1．2 解压并安装Sqoop 206
7．1．3 配置Sqoop 206
7．1．4 启动并验证Sqoop 208
7．1．5 测试Sqoop与MySQL的连接 209
7．2 安装部署Kafka集群 211
7．2．1 下载或复制Kafka安装包 211
7．2．2 解压缩Kafka安装包 211
7．2．3 配置Kafka集群 211
7．2．4 Kafka的初步应用 213
7．3 本章小结 218
第8章 Spark集群安装与开发环境配置 219
8．1 深入理解Spark 219
8．1．1 Spark系统架构 219
8．1．2 关键概念 221
8．2 安装与配置Scala 224
8．2．1 下载Scala安装包 225
8．2．2 安装Scala 225
8．2．3 启动并应用Scala 226
8．3 Spark集群的安装与配置 226
8．3．1 安装模式 226
8．3．2 Spark的安装 227
8．3．3 启动并验证Spark 230
8．3．4 几点说明 234
8．4 开发环境安装与配置 236
8．4．1 IDEA简介 236
8．4．2 IDEA的安装 236
8．4．3 IDEA的配置 238
8．5 本章小结 243
第9章 Spark应用基础 245
9．1 Spark程序的运行模式 245
9．1．1 Spark on Yarn-cluster 245
9．1．2 Spark on Yarn-client 246
9．2 Spark应用设计 247
9．2．1 分布式估算圆周率 248
9．2．2 基于Spark MLlib的贷款风险预测 265
9．3 本章小结 285
第三篇 数据处理与项目开发术
第10章 交互式数据处理 289
10．1 数据预处理 289
10．1．1 查看数据 289
10．1．2 数据扩展 291
10．1．3 数据过滤 292
10．1．4 数据上传 293
10．2 创建数据仓库 294
10．2．1 创建Hive数据仓库的基本命令 294
10．2．2 创建Hive分区表 296
10．3 数据分析 299
10．3．1 基本统计 299
10．3．2 用户行为分析 301
10．3．3 实时数据 303
10．4 本章小结 304
第11章 协同过滤推荐系统 305
11．1 推荐算法概述 305
11．1．1 基于人口统计学的推荐 305
11．1．2 基于内容的推荐 306
11．1．3 协同过滤推荐 307
11．2 协同过滤推荐算法分析 308
11．2．1 基于用户的协同过滤推荐 308
11．2．2 基于物品的协同过滤推荐 310
11．3 Spark MLlib推荐算法应用 312
11．3．1 ALS算法原理 312
11．3．2 ALS的应用设计 315
11．4 本章小结 329
第12章 销售数据分析系统 331
12．1 数据采集 331
12．1．1 在Windows下安装JDK 331
12．1．2 在Windows下安装Eclipse 334
12．1．3 将WebCollector项目导入Eclipse 335
12．1．4 在Windows下安装MySQL 336
12．1．5 连接JDBC 339
12．1．6 运行爬虫程序 340
12．2 在HBase集群上准备数据 342
12．2．1 将数据导入到MySQL 342
12．2．2 将MySQL表中的数据导入到HBase表中 344
12．3 安装Phoenix中间件 347
12．3．1 Phoenix架构 347
12．3．2 解压安装Phoenix 348
12．3．3 Phoenix环境配置 349
12．3．4 使用Phoenix 350
12．4 基于Web的前端开发 353
12．4．1 将Web前端项目导入Eclipse 353
12．4．2 安装Tomcat 355
12．4．3 在Eclipse中配置Tomcat 355
12．4．4 在Web浏览器中查看执行结果 359
12．5 本章小结 361


任 务 命 令
　　3.1 脚 本 配 置
　　3.1.1 Shell脚本
　　Shell是用C语言编写的程序，它是用户使用Linux内核的桥梁。Shell既是一种命令语言，又是一种程序设计语言。Shell应用程序提供了一个界面，用户通过这个界面可以访问操作内核的服务。关于Shell的讲解视频可扫描二维码观看。
　　Shell脚本（Shell Script）是一种为Shell编写的脚本程序。业界所说的Shell通常是指Shell脚本，但Shell和Shell Script是两个不同的概念。
　　Shell编程跟Java、PHP编程一样，只需要一个能编写代码的文本编辑器和一个能解释执行的脚本解释器。
　　Linux的Shell种类众多，常见的有：
　　* Bourne Shell（/usr/bin/sh或/bin/sh）
　　* Bourne Again Shell（/bin/bash）
　　* C Shell（/usr/bin/csh）
　　* K Shell（/usr/bin/ksh）
　　* Shell for Root（/sbin/sh）
　　…
　　3.1.2 Shell变量
　　关于Shell变量的讲解视频可扫描二维码观看。
　　Linux的Shell编程是一种非常成熟的编程语言，它支持各种类型的变量。有三种主要的变量类型：环境变量、局部变量和Shell变量。
　　环境变量：所有的程序，包括Shell启动程序，都能访问环境变量。有些程序需要环境变量来保证其正常运行，必要的时候Shell脚本也可以自定义环境变量。
　　局部变量：局部变量是在脚本或命令中定义，仅在当前Shell实例中有效，其他Shell程序不能访问的局部变量。
　　Shell变量：Shell变量是由Shell程序设置的特殊变量。Shell变量中有一部分是环境变量，有一部分是局部变量，这些变量保证了Shell的正常运行。
　　Shell编程和其他编程语言的主要不同之处是：在Shell编程中，变量是非类型性质的，不必指定变量是数字类型还是字符串类型。
　　1．局部变量
　　Shell编程中，使用局部变量无须事先声明，同时变量名的命名须遵循如下规则：
　　* 首个字符必须为字母（a～z，A～Z）。
　　* 中间不能有空格，可以使用下画线（_）。
　　* 不能使用标点符号。
　　* 不能使用bash中的关键字（可以用help命令查看保留关键字）。
　　2．局部变量赋值
　　变量赋值的格式：
　　变量名=值
　　访问变量值：取用一个变量的值，只需在变量名前面加一个$。
　　示例如下：
　　#!/bin/bash
　　# 对变量赋值：
　　a="hello world" #等号两边均不能有空格存在
　　# 打印变量a的值：
　　echo -e "A is: $a\n"
　　备注：bash中变量赋值，等号两边均不能有空格存在。
　　可以使用自己喜欢的编辑器，输入上述内容，并保存为文件test_hello.bsh，然后执行 chmod +x test_hello.bsh使其具有执行权限，最后输入“./test_hello”或“bash test_hello.bsh”执行该脚本。
　　程序运行结果：
　　A is: hello world
　　有时候变量名可能会和其他文字混淆，例如：
　　num=1
　　echo "this is the $numst"
　　上述脚本并不会输出"this is the 1st"而是"this is the "，这是由于Shell会去搜索变量 numst的值，而实际上这个变量并未赋值，可以用大括号来告诉 Shell 把 num 变量跟其他部分 分开。num=1
　　echo "this is the ${num}st"
　　程序运行结果：
　　this is the 1st
　　3.1.3 Shell传递参数
　　关于Shell传递参数的讲解视频可扫描二维码观看。
　　1．普通字符
　　可以在执行Shell脚本时，向脚本传递参数，脚本内获取参数的格式为$n。n代表一个数据，n=1为执行脚本的第一个参数，n=2为执行脚本的第二个参数，以此类推。
　　示例如下：以下代码向脚本传递三个参数，并分别输出。
　　#!/bin/bash
　　echo “Shell传递参数实例！”;
　　echo “第一个参数为$1”;
　　echo “第二个参数为$2”;
　　echo “第三个参数为$3”;
　　为脚本设置可执行权限后，并执行脚本，输出结果如下所示：
　　[root@localhost ～]# chmod u+x test.sh
　　[root@localhost ～]# ./test.sh 1 8 89
　　打印结果：
　　Shell传递参数实例！
　　第一个参数为：1
　　第二个参数为：8
　　第三个参数为：89



第Ⅰ部分　Hadoop架构与Hadoop集群介绍
第1章　Hadoop与Hadoop环境介绍............................................................................... 3
Hadoop简介.........................................................................................................................4
Hadoop 的特性............................................................................................................5
Hadoop 与大数据........................................................................................................5
Hadoop 的典型应用场景............................................................................................6
传统数据库系统..........................................................................................................7
数据湖..........................................................................................................................9
大数据、数据科学和Hadoop ..................................................................................10
Hadoop集群与集群计算................................................................................................... 11
集群计算.................................................................................................................... 11
Hadoop 集群..............................................................................................................12
Hadoop组件和Hadoop生态..............................................................................................14
Hadoop管理员需要做些什么...........................................................................................16
Hadoop 管理—新的范式......................................................................................17
关于Hadoop 管理你需要知道的.............................................................................18
Hadoop 管理员的工具集..........................................................................................19
Hadoop 1和Hadoop 2的关键区别....................................................................................19
架构区别....................................................................................................................20
高可用性....................................................................................................................20
多计算引擎................................................................................................................21
xiv 目录
分离处理和调度........................................................................................................21
Hadoop 1 和Hadoop 2 中的资源分配.....................................................................22
分布式数据处理：MapReduce和Spark、Hive、Pig ......................................................22
MapReduce ................................................................................................................22
Apache Spark .............................................................................................................23
Apache Hive ...............................................................................................................24
Apache Pig .................................................................................................................24
数据整合：Apache Sqoop、Apache Flume和Apache Kafka ..........................................25
Hadoop管理中的关键领域...............................................................................................26
集群存储管理............................................................................................................26
集群资源分配............................................................................................................26
作业调度....................................................................................................................27
Hadoop 数据安全......................................................................................................27
总结....................................................................................................................................28
第2章　Hadoop架构介绍............................................................................................. 31
Hadoop与分布式计算..................................



1.2数据的存储与分析5
1.3查询所有数据6
1.5相较于其他系统的优势8
1.6ApacheHadoop发展简史12

第2章关于MapReduce19
2.1气象数据集19
2.2使用Unix工具来分析数据21
2.3使用Hadoop来分析数据22
2.4横向扩展31
2.5HadoopStreaming37

第3章Hadoop分布式文件系统42
3.1HDFS的设计42
3.2HDFS的概念44
3.3命令行接口50
3.4Hadoop文件系统52
3.5Java接口56
3.6数据流68
3.7通过distcp并行复制76

第4章关于YARN78

4.1剖析YARN应用运行机制79

4.2YARN与MapReduce1相比82

4.3YARN中的调度85

4.4延伸阅读95

第5章Hadoop的I/O操作96

5.1数据完整性96

5.2压缩99

5.3序列化109

5.4基于文件的数据结构127

第Ⅱ部分关于MapReduce

第6章MapReduce应用开发141

6.1用于配置的API142

6.2配置开发环境144

6.3用MRUnit来写单元测试152

6.4本地运行测试数据156

6.5在集群上运行160

6.6作业调优174

6.7MapReduce的工作流176

第7章MapReduce的工作机制184

7.1剖析MapReduce作业运行

机制184

7.2失败191

7.3shuffle和排序195

7.4任务的执行201

第8章MapReduce的

类型与格式207

8.1MapReduce的类型207

8.2输入格式218

8.3输出格式236

第9章MapReduce的特性243

9.1计数器243

9.2排序252

9.3连接264

9.4边数据分布270

9.5MapReduce库类276

第Ⅲ部分Hadoop的操作

第10章构建Hadoop集群279

10.1集群规范280

10.2集群的构建和安装284

10.3Hadoop配置288

10.4安全性305

10.5利用基准评测程序测试

Hadoop集群311

第11章管理Hadoop314

11.1HDFS314

11.2监控327

11.3维护329

第Ⅳ部分Hadoop相关开源项目

第12章关于Avro341

12.1Avro数据类型和模式342

12.2内存中的序列化和

反序列化特定API347

12.3Avro数据文件349

12.4互操作性351

12.5模式解析352

12.6排列顺序354

12.7关于AvroMapReduce356

12.8使用AvroMapReduce

进行排序359

12.9其他语言的Avro362

第13章关于Parquet363

13.1数据模型364

13.2Parquet文件格式367

13.3Parquet的配置368

13.4Parquet文件的读/写369

13.5ParquetMapReduce374

第14章关于Flume377

14.1安装Flume378

14.2示例378

14.3事务和可靠性380

14.4HDFSSink382

14.5扇出385

14.6通过代理层分发387

14.7Sink组391

14.8Flume与应用程序的集成395

14.9组件编目395

14.10延伸阅读397

第15章关于Sqoop398

15.1获取Sqoop398

15.2Sqoop连接器400

15.3一个导入的例子401

15.4生成代码404

15.5深入了解数据库导入405

15.6使用导入的数据409

15.7导入大对象412

15.8执行导出414

15.9深入了解导出功能416

15.10延伸阅读419

第16章关于Pig420

16.1安装与运行Pig421

16.2示例425

16.3与数据库进行比较428

16.4PigLatin429

16.5用户自定义函数446

16.6数据处理操作455

16.7Pig实战465

16.8延伸阅读468

第17章关于Hive469

17.1安装Hive470

17.2示例472

17.3运行Hive473

17.4Hive与传统数据库相比480

17.5HiveQL483

17.6表488

17.7查询数据501

17.8用户定义函数508

17.9延伸阅读516

第18章关于Crunch517

18.1示例518

18.2Crunch核心API521

18.3管线执行537

18.4Crunch库545

18.5延伸阅读547

第19章关于Spark548

19.1安装Spark549

19.2示例549

19.3弹性分布式数据集555

19.4共享变量564

19.5剖析Spark作业运行机制565

19.6执行器和集群管理器570

19.7延伸阅读574

第20章关于HBase575

20.1HBase基础575

20.2概念576

20.3安装581

20.4客户端584

20.5创建在线查询应用589

20.6HBase和RDBMS的比较598

20.7Praxis601

20.8延伸阅读602

第21章关于ZooKeeper604

21.1安装和运行ZooKeeper605

21.2示例607

21.3ZooKeeper服务615

21.4使用ZooKeeper来构建

应用629

21.5生产环境中的ZooKeeper640

21.6延伸阅读643

第Ⅴ部分案例学习

第22章医疗公司塞纳(Cerner)

的可聚合数据647

22.1从多CPU到语义集成647

22.2进入ApacheCrunch648

22.3建立全貌649

22.4集成健康医疗数据651

22.5框架之上的可组合性654

22.6下一步655

第23章生物数据科学：

用软件拯救生命657

23.1DNA的结构659

23.2遗传密码：将DNA字符

转译为蛋白质660

22.3将DNA想象成源代码661

23.4人类基因组计划和参考

基因组663

22.5DNA测序和比对664

23.6ADAM，一个可扩展的

基因组分析平台666

23.7使用Avro接口描述语言进行

自然语言编程666

23.8使用Parquet进行面向列的

存取668

23.9一个简单例子：用Spark和

ADAM做k-mer计数669

23.10从个性化广告到个性化

医疗672

23.11联系我们673

第24章开源项目Cascading

24.1字段、元组和管道675

24.2操作678

24.3Taps，Schemes和Flows680

24.4Cascading实践应用681

24.5灵活性684

24.6ShareThis中的Hadoop和

Cascading685

24.7总结689

附录A安装ApacheHadoop

附录B关于CDH

附录C准备NCDC气象数据699

附录D新版和旧版Java

MapReduceAPI702




















Mapper

Mapper的maps阶段将输入键值对经过计算得到中间结果键值对，框架会将中间结果按照key进行分组，然后传递给reducer以决定最终的输出。用户可以通过Job.setGroupingComparatorClass(Class)来指定一个Comparator。

Mapper的输出会被排序，然后被分到不同的区，以供reducer处理。分区数与Reducer任务数相同。

如果指定了Combiner，那么会对中间结果进行本地聚集操作，这样可以减少从Mapper到Reducer传输的数量。



Reducer

Reducer减少中间结果的值，这些中间结果的值共享一个key

Reducer有三个主要阶段：shuffle、sort、reduce

Shuffle：这个阶段的输入时Mapper的输出，而且是被排过序的。这个阶段会从所有Mapper的输出中抓取相关分区。

Sort：这个阶段会按照key分组。Shuffle和Sort阶段是同时进行的，在抓取maps输出的时候就已经进行了合并

Reduce：Reducer的输出是没有排序的




Partitioner

Partitioner控制Mapper中间结果的keys分区。默认的Partitioner是HashPartitioner。



1、默认的分区方式是哈希取模（HashPartitioner），它会用key的哈希值经过计算然后对reduce任务书取模，以决定中间结果在哪个分区。由于是先用key值取哈希，再进行模运算，那么key值相同的会进入到同一个分区。

2、Reducer任务的数量是根据公式算出来的。大概是<no. of nodes> * <no. of maximum containers per node> 的0.95倍到1.75倍之间。也就是说Reducer任务数决定了会有多少个分区。

3、分区是框架做的，中间结果的排序可以自定义

4、如果指定了Combiner则可以对中间结构进行本地聚集操作

5、Shuffle阶段是通过HTTP抓取相关的分区并且对分区中的key进行分组排序







===================


1、下列哪个属性是hdfs-site.xml中的配置？
A、dfs.replication 
B、fs.defaultFS
C、mapreduce.framework.name 
D、yarn.resourcemanager.address

解答：dfs.replication是HDFS集群的副本个数，一般放置在hdfs-site.xml

 

 

2、Hadoop-2.6.5集群中的HDFS的默认的数据块的大小是？
A、32M  
B、64M
C、128M 
D、256M

解答：Hadoop2.x版本以前的默认数据块的大小是64M，到了Hadoop2.x版本以后。 默认的数据块大小就变成了128M，但是是可以更改的。并且是由客户端在上传文件到HDFS的时候指定的。HDFS集群会一直为这个文件保存指定的副本数

 


3、Hadoop-2.6.5集群中的HDFS的默认的副本块的个数是？
A、1         
B、2
C、3             
D、4

解答：HDFS集群采取分散存储 + 冗余存储的策略，用户上到HDFS集群的文件，HDFS集群会为它存储多份。默认是3份

 


4、如果我们现有一个安装2.6.5版本的hadoop集群，在不修改默认配置的情况下存储200个每个200M的文本文件，请问最终会在集群中产生多少个数据块（包括副本）？
A、200 
B、40000
C、400 
D、1200

解答：在默认情况下，HDFS集群默认存储文件3份，并且大文件会按照128M的数据块大小进行切割分散存储。所以题目中的总数据块有（200 * 2）= 400个。再加上会存储三份，所以 400 * 3 = 1200

 


5、以下哪个不是HDFS的守护进程？
A、secondarynamenode        
B、datanode
C、mrappmaster/yarnchild  
D、namenode

解答：namenode是HDFS集群的主节点，datanode是HDFS集群的从节点，secondarynamenode是HDFS集群启动的用来给namenode节点分担压力的角色。这个三个服务进程会一直启动着。MRAppMaster/YARNChild进行是只有在YARN集群运行了MapReduce程序之后才会启动的程序

 

 

6、请问以下哪个命令组成是错误的？
A、sbin/stop-dfs.sh  
B、sbin/hdfs dfsadmin -report
C、bin/hadoop namenode -format  
D、bin/hadoop fs -cat /hadoopdata/my.txt

解答：此题考查的是命令的目录结构。hadoop安装包提供了两个可执行脚本文件目录， 一个是bin，一个是sbin，sbin中放置了很多跟整个集群操作相关的命令，比如启动或者关闭集群的命令，bin目录中，主要放置客户端去使用Hadoop集群的相关命令，所以start, stop相关的命令都在sbin中， hadoop, hdfs, mapred, yarn 这些集群使用操作命令都在bin目录中

 


7、以下哪种不是Hive支持的数据类型？
A、Struct                   
B、Int
C、Map  
D、Long

解答：Hive支持原生数据类型（TinyInt, SmallInt, Int, BigInt, Boolean, Float, Double, String），也支持复杂数据类型（Map，Array，Struct，Union），所以不支持Long，支持的是BigInt

 


8、现在在hadoop集群当中的配置文件中有这么两个配置，请问假如集群当中有一个节点宕机，主节点namenode需要多长时间才能感知到？
<property>
        <name>dfs.heartbeat.interval</name>
        <value>3</value>
</property>
<property>
        <name>heartbeat.recheck.interval</name>
        <value>2000</value>
</property>
A、26秒
B、34秒
C、30秒
D、20秒

解答：HDFS集群的datnaode掉线超时时长的计算公式为： timeout = 10 * dfs.heartbeat.interval + 2 * heartbeat.recheck.interval，不过heartbeat.recheck.interval的单位是ms，dfs.heartbeat.interval的单位是s

 


9、下面关于使用hive的描述中不正确的是？
A、hive中的join查询只支持等值链接，不支持非等值连接
B、hive的表一共有两种类型，内部表和外部表
C、hive默认仓库路径为/user/hive/warehouse/   
D、hive支持数据删除和修改

解答：Hive不支持随机修改和删除。只支持绝大部分查询和批量追加数据。

 


10、HDFS集群中的namenode职责不包括？
A、维护HDFS集群的目录树结构         
B、维护HDFS集群的所有数据块的分布、副本数和负载均衡
C、负责保存客户端上传的数据
D、响应客户端的所有读写数据请求

解答：NameNode是HDFS的管理节点，DataNode是HDFS集群的工作节点，所以用户上传的数据是由datanode进行保存的。NameNode是负责保存用户上传到的这些数据的元数据和维护HDFS的抽象目录树结构。也会响应客户端的所有读写请求

 


11、关于HDFS集群中的DataNode的描述不正确的是？
A、DataNode之间都是独立的，相互之间不会有通信
B、存储客户端上传的数据的数据块
C、一个DataNode上存储的所有数据块可以有相同的
D、响应客户端的所有读写数据请求，为客户端的存储和读取数据提供支撑

解答：DataNode是真正为HDFS集群存储数据的。HDFS集群的数据存储策略是分散+冗余的策略，由此可以看出，一个节点上，如果存储了两个一样的数据块，这样的冗余是没有任何意义的，所以一个节点上是不会存储一个数据块的多个副本的；DataNode之间是要进行通信的，因为数据上传的时候就是第一个副本节点和第二个副本节点建立连接传输数据，而不是客户端和第二个副本建立连接传送，所以答案是AC

 


12、HDFS集群中的DataNode的主要职责是？
A、维护HDFS集群的目录树结构         
B、维护HDFS集群的所有数据块的分布、副本数和负载均衡
C、负责保存客户端上传的数据
D、响应客户端的所有读写数据请求

解答：NameNode是HDFS的管理节点，DataNode是HDFS集群的工作节点，所以用户上传的数据是由DataNode进行保存的。NameNode是负责保存用户上传到的这些数据的元数据和维护HDFS的抽象目录树结构。也会响应客户端的所有读写请求

 


13、MapReduce的Shuffle过程中哪个操作是最后做的？ 
A、 溢写
B、分区
C、排序 
D、合并

解答：MapReduce编程模型分为Mapper和Reducer阶段，在mapper和reducer的中间还有一个shuffle阶段。但是Shuffle也分为MapperShuffle和ReducerShuffler两个阶段。这个Shuffle非常的重要，而且也是导致MapReduce执行效率低的一个重要原因。shuflle中的执行顺序是先分区，然后在溢写之前进行排序，最后溢出的多个磁盘文件会进行合并成一个大文件。

 


14、下面关于MapReduce的描述中正确的是？
A、MapReduce程序必须包含Mapper和Reducer 
B、MapReduce程序的MapTask可以任意指定
C、MapReduce程序的ReduceTask可以任意指定  
D、MapReduce程序的默认数据读取组件是TextInputFormat

解答：MapReducer编程模型中，可以没有Reducer，MapTask是由逻辑切片规则决定，虽然可以通过参数进行调整，但是不能随意设置，reduceTask数量可以随意设置，但是通常都是和业务挂钩，所以也基本做不到随心所欲的设置，除非是HashPartitioner的分区器。MapReduce编程模型中的默认数据读取组件是TextInputFormat和LineRecordReader

 


15、MapReduce编程模型中以下组件哪个是最后执行的？ 
A、Mapper
B、Partitioner
C、Reducer
D、RecordReader

解答：以上这四个MapReduce编程模型中的执行顺序是：recordReader -->  mapper --> partitioner --> reducer

 


16、在MapReduce中，哪个组件是用户不指定也不会有默认的？ 
A、Combiner
B、OutputFormat 
C、Partitioner 
D、InputFormat

解答：在MapReduce编程模型中，Combiner是可有可无的组件，它的作用就是用来给mapTask的结果数据做局部合并以减少reduceTask接收的数据量，以减少网络数据传输。OutputFormat的默认组件是TextOutputFormat，InputFormat的默认组件是TextInputFormat，Partitioner的默认实现是HashPartitioner

 


17、下列哪种类型的文件不是HDFS集群的元数据存储格式？
A、fsimage
B、edits
C、edits_inprogress 
D、blk_000003425

解答：D是存储在datanode节点上的数据块的命名格式。

 


18、YARN的调度算法不包括以下哪种？
A、FIFO Scheduler
B、Fair Scheduler
C、Capacity Scheduler

D、Stack Scheduler

解答：D不是

 


19、关于SecondaryNameNode哪项是正确的？
A、它是NameNode的热备
B、它对内存没有要求
C、它对目的是帮助NameNode合并编辑日志，减少NameNode的负担和冷启动时的加载时间
D、SecondaryNameNode应与NameNode部署到一个节点

解答：SecondaryNameNode是为了给namenode减轻压力的角色，工作职责就是定期合并磁盘元数据文件为序列化的镜像文件，以减少namenode冷启动时需要加载元数据的时间。在合并的时候也需要把之前的元数据都加载到内存，所以对内存也有一定的依赖，所以肯定不能和namenode启动在同一个节点。否则就起不到任何减轻压力的作用了。

 


20、下列关于使用MapReduce编程模型实现SQL中的join操作错误的是？
A、ReduceJoin可以实现内链接，也能实现各种外连接
B、ReduceJoin的join操作是在MapReduce程序中的reducer阶段完成的
C、MapJoin也适合各种join场景，也能实现内连接和各种外链接
D、MapJoin不会产生数据倾斜

解答：MapJoin和ReduceJoin，顾名思义，就是mapper阶段完成join操作叫MapJoin，在reducer阶段完成join操作叫ReduceJoin，reduceJoin能够实现一到多个条件的各种等值链接，但是不能实现非等值连接，因为太困难。并且容易出现数据倾斜的情况，所以，出现MapJoin，因为能省掉reducer阶段，所以能完美避免数据倾斜，但是由于该机制的特性，只适合用来做大表和小表数据之间的链接。

 


21、下列哪种业务场景中，不能直接使用Reducer充当Combiner使用？
A、sum求和
B、max求最大值
C、count求计数
D、avg求平均

解答：在不更改reducer业务逻辑的情况，以上四种，只有avg求平均是不能直接使用reducer充当combiner的。最终会造成业务结果不正确

 


22、下列关于配置机架感知的相关描述哪项不正确？
A、如果一个机架出问题，不会影响数据读写和正确性
B、写入数据的时候多个副本会写到不同机架的 DataNode 中 
C、MapReduce 会根据机架的拓扑获取离自己比较近的数据块
D、数据块的第一个副本会优先考虑存储在客户端所在节点

解答：HDFS的副本存放策略中，数据块的第一个副本和第二个副本会存放在不同的机架中，但是第三个副本会优先考虑存放在跟第二个副本相同机架的不同节点中，也有可能存放在跟第一个副本相同机架的不同节点中。

 


23、Client端上传文件的时候下列哪项正确？ 
A、数据经过 NameNode 传递给 DataNode 
B、Client端将文件切分为 Block，依次上传 
C、Client只上传数据到一台DataNode，然后由NameNode负责Block复制
D、Client如果上传的时候没有上传成功指定的副本数，则整次上传不成功

解答：HDFS集群在上传数据的时候，请求由namenode响应是没错，但是传输数据到datanode节点的时候是直接client直接和datanode进行通信，而不用先把数据传输到namenode再传送到datanode。这样实现会极大的增加namenode节点的负担。

 


24、下列关于HDFS的描述正确的是？
A、如果 NameNode 宕机，SecondaryNameNode 会接替它使集群继续工作
B、HDFS集群支持数据的随机读写
C、NameNode磁盘元数据不保存Block的位置信息
D、DataNode通过长连接与NameNode保持通信

解答：SecondaryNameNode并不是namenode节点的备份。所以A错。存储在HDFS集群上的数据是不支持随机修改和删除的，只支持追加。HDFS集群的主节点namenode会保存元数据，在内存中有一份完整的，在磁盘中也有一份完整的。但是磁盘中的元数据并不包括，每个数据块的多个副本到底存储在那些节点上的信息。当HDFS集群启动完毕以后，datanode会向namenode进行汇报数据块的信息，所以namenode才能通过这些汇报信息统计得出所有数据块的副本存放信息。在内存当中会有，但是磁盘中是没有的。namenode和datanode之间的通信是基于一种心跳机制。该机制不是长连接。是短连接形式。每次发送一个数据包（自身状态信息 + 数据块信息）即可。

 


25、一个MapReduce程序中的MapTask的个数由什么决定？
A、输入的总文件数
B、客户端程序设置的mapTask的个数
C、FileInputFormat.getSplits(JobContext job)计算出的逻辑切片的数量
D、输入的总文件大小/数据块大小

解答：MapReduce编程模型中的mapTask的并行度决定机制是由：FileInputFormat.getSplits(JobContext job)决定的。该方法的返回值是List<InputSplit> splits，这个结果集合中的每个InputSplit就是一个逻辑输入切片，每个逻辑输入切片在默认情况下是会要启动一个mapTask任务进行计算的。

 


26、以下描述错误的是？
A、SequenceFile可以用来作为小文件的合并存储容器 
B、TextInputFormat的key是LongWritable类型的 
C、CombineFileInputFormat是抽象类 
D、TextInputFormat的key是指该记录在文件中的行号

解答：当MapReduce程序遇到需要计算大量小文件时，可选的解决方案有SequenceFile， 有CombineFileInputFormat（抽象类），有归档策略。默认的TextInputFormat中规定的key-value的类型分别是LongWritable和Text，其中，value表示逐行读取的一行文本数据，key表示这一行在该文件中的起始偏移量， 不是行号

 


27、以下关于新旧 MapReduce API 的描述错误的是？
A、新API放在org.apache.hadoop.mapreduce包中，而旧API则是放在org.apache.hadoop.mapred中 
B、新API倾向于使用接口方式，而旧API倾向于使用抽象类 
C、新API使用Configuration，而旧API使用JobConf来传递配置信息 
D、新API可以使用Job对象来提交作业

解答：在新API中，原来的大量接口都被改成了抽象类。所以使用新API编写MR程序时，都是由实现接口变成集成抽象类。

 


28、以下描述错误的是？
A、输入分片InputSplit其实是对数据的引用 
B、MultipleInputs可以设置多个数据源以及它们对应的输入格式 
C、可以通过重载isSplitable()方法来避免文件分片 
D、ReduceTask需要等到所有的map输出都复制完才进行Merge

解答：ReduceTask在mapper阶段的所有mapTask还没有执行完毕的时候，就会预先启动，然后去已经执行完毕的mapTask节点拉取该reduceTask要执行的数据，执行预先合并。而不是等到所有的mapTask都执行完毕之后才开启reduceTask拉取文件进行merge操作

 


29、以下哪个组件可以指定对key进行Reduce分发的策略？
A、RecordReader 
B、Combiner 
C、Partitioner 
D、FileInputFormat

解答：Partitioner组件就是负责给mapTask节点输出的数据进行分发的。 默认的实现是HashParitioner

 


30、执行一个job，如果这个job的输出路径已经存在，那么程序会？
A、覆盖这个输出路径 
B、抛出警告，但是能够继续执行 
C、抛出一个异常，然后退出 
D、创建一个新的输出路径

解答：MapReduce编程模型中的输出目录必须是不存在的目录。否则程序抛出异常，并且退出运行。

 


31、HDFS的是基于流数据模式访问和处理超大文件的需求而开发的，默认的最基本的存储单位是64M，具有高容错、高可靠性、高可扩展性、高吞吐率等特征，适合的读写任务是？
A、一次写入，少次读写  
B、多次写入，少次读写  
C、一次写入，多次读写  
D、多次写入，多次读写

解答：HDFS的设计初衷就是为将来的海量数据的分布式计算做铺垫的，所以HDFS是一次写入，多次读出的场景

 


32、Namenode在启动时自动进入安全模式，在安全模式阶段，说法错误的是？
A、安全模式目的是在系统启动时检查各个DataNode上数据块的有效性
B、根据策略对数据块进行必要的复制或删除
C、当数据块最小百分比数满足的最小副本数条件时，会自动退出安全模式
D、文件系统允许有修改

解答：HDFS文件系统上的数据是不可以进行更改的。

 


33、MapReduce框架提供了一种序列化机制，支持这种序列化的类能够在Map和Reduce过程中充当键或值，以下说法错误的是？
A、实现Writable接口的类是值
B、实现WritableComparable<T>接口的类可以是值或键  
C、Hadoop的基本类型Text并不实现WritableComparable<T>接口  
D、键和值的数据类型可以超出Hadoop自身支持的基本类型

解答：Hadoop中的基本类型和包装类型都有可能作为MapReduce编程中的key和value，所以都必须要进行序列化，都要事先Writable，如果作为key，那就必须实现WritableComparable接口。Text类也是实现了WritableComparable这个接口的

 


34、下列关于HDFS为存储MapReduce并行切分和处理的数据做的设计，错误的是？
A、FSDataInputStream扩展了DataInputStream以支持随机读  
B、为实现细粒度并行，输入分片(InputSplit)应该越小越好  
C、一台机器可能被指派从输入文件的任意位置开始处理一个分片  
D、输入分片是一种记录的逻辑划分，而数据块是对输入数据的物理分割

解答：逻辑输入切片并不是越小越好，因为太小会增加启动的MapTask的个数，会导致每个MapTask所处理的数量会降低，从而降低了服务器的处理数据的性能。

 


35、有关MapReduce的输入输出，说法错误的是？
A、链接多个MapReduce作业时，序列文件是首选格式  
B、FileInputFormat中实现的getSplits()可以把输入数据划分为分片，分片数目和大小任意定义  
C、想完全禁止输出，可以使用NullOutputFormat  
D、每个reduce需将它的输出写入自己的文件中，输出无需分片

解答：FileInputFormat的分片大小是可以任意设置，可以调整的，输入分片数目不可以随意设置，是根据分片大小和文件计算出来的。

 


36、关于HDFS的文件写入，正确的是？
C、默认将文件块复制成三份分别存放  
解答：HDFS集群的文件默认存储3份

 

 

答案：

1-5：ACCDC
6-10：BDBDC
11-15：CCDDC
16-20：ADDCC
21-25：DBBCC
26-30：DBDCC
31-35：CDCBB


第1章 数据仓库简介
1.1 什么是数据仓库 1
1.1.1 数据仓库的定义 1
1.1.2 建立数据仓库的原因 3
1.2 操作型系统与分析型系统 5
1.2.1 操作型系统 5
1.2.2 分析型系统 8
1.2.3 操作型系统和分析型系统对比 9
1.3 数据仓库架构 10
1.3.1 基本架构 10
1.3.2 主要数据仓库架构 12
1.3.3 操作数据存储 16
1.4 抽取-转换-装载 17
1.4.1 数据抽取 17
1.4.2 数据转换 19
1.4.3 数据装载 20
1.4.4 开发ETL系统的方法 21
1.4.5 常见ETL工具 21
1.5 数据仓库需求 22
1.5.1 基本需求 22
1.5.2 数据需求 23
1.6 小结 24
第2章 数据仓库设计基础
2.1 关系数据模型 25
2.1.1 关系数据模型中的结构 25
2.1.2 关系完整性 28
2.1.3 规范化 30
2.1.4 关系数据模型与数据仓库 33
2.2 维度数据模型 34
2.2.1 维度数据模型建模过程 35
2.2.2 维度规范化 36
2.2.3 维度数据模型的特点 37
2.2.4 星型模式 38
2.2.5 雪花模式 40
2.3 Data Vault模型 42
2.3.1 Data Vault模型简介 42
2.3.2 Data Vault模型的组成部分 43
2.3.3 Data Vault模型的特点 44
2.3.4 Data Vault模型的构建 44
2.3.5 Data Vault模型实例 46
2.4 数据集市 49
2.4.1 数据集市的概念 50
2.4.2 数据集市与数据仓库的区别 50
2.4.3 数据集市设计 50
2.5 数据仓库实施步骤 51
2.6 小结 54
第3章 Hadoop生态圈与数据仓库
3.1 大数据定义 55
3.2 Hadoop简介 56
3.2.1 Hadoop的构成 57
3.2.2 Hadoop的主要特点 58
3.2.3 Hadoop架构 58
3.3 Hadoop基本组件 59
3.3.1 HDFS 60
3.3.2 MapReduce 65
3.3.3 YARN 72
3.4 Hadoop生态圈的其他组件 77
3.5 Hadoop与数据仓库 81
3.5.1 关系数据库的可扩展性瓶颈 82
3.5.2 CAP理论 84
3.5.3 Hadoop数据仓库工具 85
3.6 小结 88
第4章 安装Hadoop
4.1 Hadoop主要发行版本 89
4.1.1 Cloudera Distribution for Hadoop（CDH） 89
4.1.2 Hortonworks Data Platform（HDP） 90
4.1.3 MapR Hadoop 90
4.2 安装Apache Hadoop 91
4.2.1 安装环境 91
4.2.2 安装前准备 92
4.2.3 安装配置Hadoop 93
4.2.4 安装后配置 97
4.2.5 初始化及运行 97
4.3 配置HDFS Federation 99
4.4 离线安装CDH及其所需的服务 104
4.4.1 CDH安装概述 104
4.4.2 安装环境 106
4.4.3 安装配置 106
4.4.4 Cloudera Manager许可证管理 114
4.5 小结 115
第5章 Kettle与Hadoop
5.1 Kettle概述 117
5.2 Kettle连接Hadoop 119
5.2.1 连接HDFS 119
5.2.2 连接Hive 124
5.3 导出导入Hadoop集群数据 128
5.3.1 把数据从HDFS抽取到RDBMS 128
5.3.2 向Hive表导入数据 132
5.4 执行Hive的HiveQL语句 134
5.5 MapReduce转换示例 135
5.6 Kettle提交Spark作业 143
5.6.1 安装Spark 143
5.6.2 配置Kettle向Spark集群提交作业 146
5.7 小结 149
第6章 建立数据仓库示例模型
6.1 业务场景 150
6.2 Hive相关配置 152
6.2.1 选择文件格式 152
6.2.2 支持行级更新 159
6.2.3 Hive事务支持的限制 164
6.3 Hive表分类 164
6.4 向Hive表装载数据 169
6.5 建立数据库表 174
6.6 装载日期维度数据 179
6.7 小结 180

第7章 数据抽取
7.1 逻辑数据映射 182
7.2 数据抽取方式 185
7.3 导出成文本文件 191
7.4 分布式查询 196
7.5 使用Sqoop抽取数据 200
7.5.1 Sqoop简介 200
7.5.2 CDH 5.7.0中的Sqoop 203
7.5.3 使用Sqoop抽取数据 203
7.5.4 Sqoop优化 207
7.6 小结 208
第8章 数据转换与装载
8.1 数据清洗 210
8.2 Hive简介 214
8.2.1 Hive的体系结构 215
8.2.2 Hive的工作流程 216
8.2.3 Hive服务器 218
8.2.4 Hive客户端 221
8.3 初始装载 231
8.4 定期装载 236
8.5 Hive优化 246
8.6 小结 254
第9章 定期自动执行ETL作业
9.1 crontab 256
9.2 Oozie简介 260
9.2.1 Oozie的体系结构 260
9.2.2 CDH 5.7.0中的Oozie 262
9.3 建立定期装载工作流 262
9.4 建立协调器作业定期自动执行工作流 271
9.5 Oozie优化 275
9.6 小结 276
第10章 维度表技术
10.1 增加列 278
10.2 维度子集 285
10.3 角色扮演维度 292
10.4 层次维度 298
10.4.1 固定深度的层次 299
10.4.2 递归 302
10.4.3 多路径层次 310
10.4.4 参差不齐的层次 312
10.5 退化维度 313
10.6 杂项维度 316
10.7 维度合并 323
10.8 分段维度 329
10.9 小结 335
第11章 事实表技术
11.1 事实表概述 336
11.2 周期快照 337
11.3 累积快照 343
11.4 无事实的事实表 349
11.5 迟到的事实 354
11.6 累积度量 360
11.7 小结 366
第12章 联机分析处理
12.1 联机分析处理简介 367
12.1.1 概念 367
12.1.2 分类 368
12.1.3 性能 371
12.2 Impala简介 371
12.3 Hive、SparkSQL、Impala比较 377
12.3.1 Spark SQL简介 377
12.3.2 Hive、Spark SQL、Impala比较 379
12.3.3 Hive、Spark SQL、Impala性能对比 382
12.4 联机分析处理实例 387
12.5 Apache Kylin与OLAP 399
12.5.1 Apache Kylin架构 399
12.5.2 Apache Kylin安装 401
12.6 小结 407
第13章 数据可视化
13.1 数据可视化简介 408
13.2 Hue简介 410
13.2.1 Hue功能快速预览 411
13.2.2 配置元数据存储 412
13.3 Zeppelin简介 415
13.3.1 Zeppelin架构 415
13.3.2 Zeppelin安装配置 416
13.3.3 在Zeppelin中添加MySQL翻译器 421
13.4 Hue、Zeppelin比较 425
13.5 数据可视化实例 426
13.6 小结 434

