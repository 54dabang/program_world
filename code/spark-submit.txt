--conf spark.memory.offHeap.size=5368709120 \
--conf spark.memory.offHeap.enabled=true \



Submitting Applications
在 script in Spark的 bin 目录中的spark-submit 脚本用与在集群上启动应用程序。它可以通过一个统一的接口使用所有 Spark 支持的 cluster managers，所以您不需要专门的为每个cluster managers配置您的应用程序。

打包应用依赖
如果您的代码依赖了其它的项目，为了分发代码到 Spark 集群中您将需要将它们和您的应用程序一起打包。为此，创建一个包含您的代码以及依赖的 assembly jar（或者 “uber” jar）。无论是 sbt 还是 Maven 都有 assembly 插件。在创建 assembly jar 时，列出 Spark 和 Hadoop的依赖为provided。它们不需要被打包，因为在运行时它们已经被 Cluster Manager 提供了。

如果您有一个 assembled jar 您就可以调用 bin/spark-submit 脚本（如下所示）来传递您的 jar。

对于 Python 来说，您可以使用 spark-submit 的 --py-files 参数来添加 .py, .zip 和 .egg 文件以与您的应用程序一起分发。如果您依赖了多个 Python 文件我们推荐将它们打包成一个 .zip 或者 .egg 文件。

如果用户的应用程序被打包好了，它可以使用 bin/spark-submit 脚本来启动。这个脚本负责设置 Spark 和它的依赖的 classpath，并且可以支持 Spark 所支持的不同的 Cluster Manager 以及 deploy mode（部署模式）:

./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
一些常用的 options（选项）有 :

--class: 您的应用程序的入口点（例如。 org.apache.spark.examples.SparkPi)
--master: 集群的 master URL (例如 spark://23.195.26.187:7077)
--deploy-mode: 是在 worker 节点(cluster) 上还是在本地作为一个外部的客户端(client) 部署您的 driver(默认: client) †
--conf: 按照 key=value 格式任意的 Spark 配置属性。对于包含空格的 value（值）使用引号包 “key=value” 起来。
application-jar: 包括您的应用以及所有依赖的一个打包的 Jar 的路径。该 URL 在您的集群上必须是全局可见的，例如，一个 hdfs:// path 或者一个 file:// 在所有节点是可见的。
application-arguments: 传递到您的 main class 的 main 方法的参数，如果有的话。
† 常见的部署策略是从一台 gateway 机器物理位置与您 worker 在一起的机器（比如，在 standalone EC2 集群中的 Master 节点上）来提交您的应用。在这种设置中， client 模式是合适的。在 client 模式中，driver 直接运行在一个充当集群 client 的 spark-submit 进程内。应用程序的输入和输出直接连到控制台。因此，这个模式特别适合那些设计 REPL（例如，Spark shell）的应用程序。

另外，如果您从一台远离 worker 机器的机器（例如，本地的笔记本电脑上）提交应用程序，通常使用 cluster 模式来降低 driver 和 executor 之间的延迟。目前，Standalone 模式不支持 Cluster 模式的 Python 应用。

对于 Python 应用，在 <application-jar> 的位置简单的传递一个 .py 文件而不是一个 JAR，并且可以用 --py-files 添加 Python .zip，.egg 或者 .py 文件到 search path（搜索路径）。

这里有一些选项可用于特定的 cluster manager 中。例如， Spark standalone cluster 用 cluster 部署模式, 您也可以指定 --supervise 来确保 driver 在 non-zero exit code 失败时可以自动重启。为了列出所有 spark-submit, 可用的选项，用 --help. 来运行它。这里是一些常见选项的例子 :

# Run application locally on 8 cores
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[8] \
  /path/to/examples.jar \
  100

# Run on a Spark standalone cluster in client deploy mode
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000

# Run on a Spark standalone cluster in cluster deploy mode with supervise
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --deploy-mode cluster \
  --supervise \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000

# Run on a YARN cluster
export HADOOP_CONF_DIR=XXX
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \  # can be client for client mode
  --executor-memory 20G \
  --num-executors 50 \
  /path/to/examples.jar \
  1000

# Run a Python application on a Spark standalone cluster
./bin/spark-submit \
  --master spark://207.184.161.138:7077 \
  examples/src/main/python/pi.py \
  1000

# Run on a Mesos cluster in cluster deploy mode with supervise
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master mesos://207.184.161.138:7077 \
  --deploy-mode cluster \
  --supervise \
  --executor-memory 20G \
  --total-executor-cores 100 \
  http://path/to/examples.jar \
  1000
Master URLs
传递给 Spark 的 master URL 可以使用下列格式中的一种 :

Master URL	Meaning
local	使用一个线程本地运行 Spark（即，没有并行性）。
local[K]	使用 K 个 worker 线程本地运行 Spark（理想情况下，设置这个值的数量为您机器的 core 数量）。
local[K,F]	使用 K 个 worker 线程本地运行 Spark并允许最多失败 F次 (查阅 spark.task.maxFailures 以获取对该变量的解释)
local[*]	使用更多的 worker 线程作为逻辑的 core 在您的机器上来本地的运行 Spark。
local[*,F]	使用更多的 worker 线程作为逻辑的 core 在您的机器上来本地的运行 Spark并允许最多失败 F次。
spark://HOST:PORT	连接至给定的 Spark standalone cluster master. master。该 port（端口）必须有一个作为您的 master 配置来使用，默认是 7077。
spark://HOST1:PORT1,HOST2:PORT2	连接至给定的 Spark standalone cluster with standby masters with Zookeeper. 该列表必须包含由zookeeper设置的高可用集群中的所有master主机。该 port（端口）必须有一个作为您的 master 配置来使用，默认是 7077。
mesos://HOST:PORT	连接至给定的 Mesos 集群. 该 port（端口）必须有一个作为您的配置来使用，默认是 5050。或者，对于使用了 ZooKeeper 的 Mesos cluster 来说，使用 mesos://zk://.... 。使用 --deploy-mode cluster, 来提交，该 HOST:PORT 应该被配置以连接到 MesosClusterDispatcher.
yarn	连接至一个 YARN cluster in client or cluster mode 取决于 --deploy-mode. 的值在 client 或者 cluster 模式中。该 cluster 的位置将根据 HADOOP_CONF_DIR 或者 YARN_CONF_DIR 变量来找到。
从文件中加载配置
spark-submit 脚本可以从一个 properties 文件加载默认的 Spark configuration values 并且传递它们到您的应用中去。默认情况下，它将从 Spark 目录下的 conf/spark-defaults.conf 读取配置。更多详细信息，请看 加载默认配置.

加载默认的 Spark 配置，这种方式可以消除某些标记到 spark-submit. 的必要性。例如，如果 spark.master 属性被设置了，您可以在spark-submit中安全的省略 --master 配置 . 一般情况下，明确设置在 SparkConf 上的配置值的优先级最高，然后是传递给 spark-submit的值, 最后才是 default value（默认文件）中的值。

如果您不是很清楚其中的配置设置来自哪里，您可以通过使用 --verbose 选项来运行 spark-submit 打印出细粒度的调试信息。

高级的依赖管理
在使用 spark-submit 时，使用 --jars 选项包括的应用程序的 jar 和任何其它的 jar 都将被自动的传输到集群。在 --jars 后面提供的 URL 必须用逗号分隔。该列表会被包含到 driver 和 executor 的 classpath 中。 --jars 不支持目录的形式。

Spark 使用下面的 URL 格式以允许传播 jar 时使用不同的策略 :

file: - 绝对路径和 file:/ URI 通过 driver 的 HTTP file server 提供服务，并且每个 executor 会从 driver 的 HTTP server 拉取这些文件。
hdfs:, http:, https:, ftp: - 如预期的一样拉取下载文件和 JAR
local: - 一个用 local:/ 开头的 URL 预期作在每个 worker 节点上作为一个本地文件存在。这样意味着没有网络 IO 发生，并且非常适用于那些已经被推送到每个 worker 或通过 NFS，GlusterFS等共享的大型的 file/JAR。
N注意，那些 JAR 和文件被复制到 working directory（工作目录）用于在 executor 节点上的每个 SparkContext。这可以使用最多的空间显著量随着时间的推移，将需要清理。在 Spark On YARN 模式中，自动执行清理操作。在 Spark standalone 模式中，可以通过配置 spark.worker.cleanup.appDataTtl 属性来执行自动清理。

用户也可以通过使用 --packages来提供一个逗号分隔的 maven coordinates（maven 坐标）以包含任何其它的依赖。在使用这个命令时所有可传递的依赖将被处理。其它的 repository（或者在 SBT 中被解析的）可以使用 --repositories该标记添加到一个逗号分隔的样式中。 (注意，对于那些设置了密码保护的库，在一些情况下可以在库URL中提供验证信息，例如 https://user:password@host/....以这种方式提供验证信息需要小心。) 这些命令可以与 pyspark, spark-shell 和 spark-submit 配置会使用以包含 Spark Packages（Spark 包）。 对于 Python 来说，也可以使用 --py-files 选项用于分发 .egg, .zip 和 .py libraries 到 executor 中。


如果您已经部署了您的应用程序，集群模式概述 描述了在分布式执行中涉及到的组件，以及如何去监控和调试应用程序。


#!/usr/bin/env bash
SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob

function startJob(){
  startDate=$1
  endDate=$2
  jobType=$3

  if [ "$jobType" == "detail_charge_run" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_job.sh $startDate $endDate
  elif [ "$jobType" == "detail_charge_run_output" ] ;
  then
   source ${SPARK_REPORT_DETAIL_PATH}/charge_run_output_job.sh $startDate $endDate
  elif [ "$jobType" == "detailjob" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/testdetailjob.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_job.sh $startDate $endDate
  elif [ "$jobtype" == "detail_output" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/detail_output_job.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_output_job.sh $startDate $endDate
  else
    source ${SPARK_REPORT_DETAIL_PATH}/testdetailjob.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_job.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/detail_output_job.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_output_job.sh $startDate $endDate
  fi
}


startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

jobtype=$3

echo "$startDate,$endDate"


startJob $startDate $endDate $jobtype

==========================

startJob() {
      /opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/bin/spark2-submit\
      --class com.bitnei.report.detail.DetailJob \
      --name ${inputDate}-spark明细报表 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 6G \
      --executor-cores 4 \
      --supervise \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS} \
      --driver-memory 3G \
      --conf spark.yarn.executor.memoryoverhead=3G \
      --files ${SPARK_REPORT_CONF} \
      ${SPARK_REPORT_DAY_DETAIL_JAR} \
      detail.compute=reportDetail \
      currentInvalidateTime=10Min \
      detail.table.partitionColumn=year-month-day \
      realinfo.table.partitionValue=$partitionValue \
      detail.table.partitionValue=$partitionValue
}


function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')

    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"



inputDate=$($SPARK_REPORT_PATH/bin/concatReportDate.sh $startDate $endDate)

echo "the inpute date are $inputDate"
batchProcess $startDate $endDate


====================
startJob() {
      /opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/bin/spark2-submit\
      --class com.bitnei.report.detail.DetailOutputJob \
      --name ${inputDate}-spark日报表计算 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 2G \
      --executor-cores 4 \
      --num-executors 2 \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS} \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --conf spark.shuffle.io.maxRetries=40 \
      --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
       --conf spark.default.parallelism=10 \
      ${SPARK_REPORT_DAY_DETAIL_JAR} \
      detail.table.partitionValue=$partitionValue \
      input.table.name=detail \
      report.date=$curDate-$curDate \
      report.output=oracle \
      removeBeforeExecute=true
}



function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')
    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"


batchProcess $startDate $endDate







输入参数
 #昨天的日期
 dateDir=$(date +%Y/%m/%d --date '-1 day')
 reportDate=$(date +%Y%m%d --date '-1 day')
 #今天的日期
 today=$(date +%Y/%m/%d)

dateDir=2017/05/01
reportDate=20170501

param_job_hdfs=hdfs://nameservice1:8020
param_input_directory=/vehicle/data/realinfo
param_output_format=text

detail.output.hdfs.enable=true \
   report.date=${reportDate}-${reportDate} \
   spark.deploy.local=false \
   detail.output.format=${param_output_format} \
   detail.input.format=text \
    detail.output.path=${param_job_hdfs}/${param_job_dayreport_output_path} \
    input.directory=${param_input_directory}


	* 根据最近n天的文件大小预测当前文件大小和当前分区数。.

	分区数=threshold*文件大小,其中0<threshold<1,threshold=用户预设的每一个文件大小的倒数
        hadoopConfig.getLong("dfs.blocksize", 128 * 1024 * 1024)

	  val w = 0.7D


	    val currentFileSize = estimateCurrentFileSize(fs, path)
    val parNum = Math.ceil(currentFileSize / getThreshold).toInt
    if (parNum < 2) {
      logWarning(s"the partition num is $parNum,the file size estimated  is ${path.length},the threshold size is $getThreshold , the current file size estimated is $currentFileSize")
      8
    } else parNum

    if (path.size < 2) 0L
    else {
      val lastN = Utils.getChildFiles(fs, path(0)).foldLeft(0L)((a, b) => a + b._2)
      val lastN_1 = Utils.getChildFiles(fs, path(1)).foldLeft(0L)((a, b) => a + b._2)

      Math.ceil(lastN * w + lastN_1 * (1 - w)).toLong
    }

	 def getChildFiles(fs: FileSystem, directory: String): Array[(String, Long)] = {
    val d = new Path(directory)

    if (fs.exists(d)) {
      val childFiles = fs.listStatus(d)
      childFiles.map(file => {
        (file.getPath.toString, file.getLen)
      })
    } else {
      Array.empty
    }
  }


    val sql =
      s"""
      SELECT CAST (VID AS String),
             VIN,
             TIME,
             `2201` AS speed,
             `2614` AS charge,
             `2615` AS soc,
             `2613` AS totalVoltage,
             `2603` AS secondaryCellMaxVoltage,
             `2606` AS secondaryCellMinVoltage,
             `2304` AS engineTemp,
             `2609` AS accuisitionPointMaxTemp,
             `2612` AS accuisitionPointMinTemp,
             `2202` AS mileage,
             `2502` AS longitude,
             `2503` AS latitude,
             CAST(`2203` AS INT) AS peaking
       FROM $inputTableName
       WHERE vid IS NOT NULL AND $whereCondition
    """.stripMargin

             RealinfoModel(
                          vid:String,
                          vin:String,
                          time:String,
                            //year:String,
                            //month:String,
                            //day:String,
                        //  ruleId:String="1",
                        //速度
                          speed:Option[Int]=None,
                          //电流
                          charge:Option[Int]=None,
                          //soc
                          soc:Option[Int]=None,
                          //总电压
                          totalVoltage:Option[Int]=None,
                          //最大单体电压
                          secondaryCellMaxVoltage:Option[Int]=None,
                          secondaryCellMinVoltage:Option[Int]=None,

                        //发电机温度
                          engineTemp:Option[Int]=None,
                          //最大采集温度
                          accuisitionPointMaxTemp:Option[Int]=None,
                          //最小采集温度
                          accuisitionPointMinTemp:Option[Int]=None,

                        //里程
                          mileage:Option[Int]=None,
                        //经度
                          longitude:Option[Long]=None,
                          //纬度
                          latitude:Option[Long]=None,
                          //挡位
                          peaking:Option[Int]=None) {

}

   //按vid分组
      val result = realinfoDs.groupByKey(_.vid)
        // TODO:按vid分组
        .flatMapGroups({ case (vid: String, rows: Iterator[RealinfoModel]) =>
        val values = if (enableEmergency) {
          rows.map(r => {
            //TODO:解析时间为年月日
            val srcDate = Utils.parsetDate(r.time).get
            srcDate.setYear(emergencyYear)
            srcDate.setMonth(emergencyMonth)
            srcDate.setDate(emergencyDay)
            val toDate = Utils.formatDate(srcDate)

            //todo:转成日期字符串
            r.copy(time = toDate)
          }).toArray
        } else {
          rows.toArray
        }

        //TODO 对数据按照时间排序并去重
        val sortedRows =

        //TODO 按时间去重
          Utils.strict[RealinfoModel, String](

            //TODO 按时间排序
            Utils.sortByDate(values, row => {
              if (row.time.trim.isEmpty) {
                logWarning(row.toString)
                None
              } else Some(row.time)
            }),
            row => Some(row.time)
          )


        val cmp: ReportDetailCompute = stateConf.getString("detail.compute") match {
          //TODO 计算明细基础
          case "reportDetail" => new DetailComputeBase()

           //状态划分
              val windows = stateConf.getString("chargeChange.enable") match {
                case "true" =>
                  val stateGenerator = new ChargeChangeGenerator(stateConf) {
                    override type T = RealinfoModel

                    override def getVid = (row: RealinfoModel) => row.vid

                    override def getTime = row => row.time

                    override def getCharge = row => row.charge.getOrElse(0)

                    override def getSoc = row => row.soc.getOrElse(0)

                    override def getSpeed = row => row.speed.getOrElse(0)

                    override def getMileage = row => row.mileage.getOrElse(0)
                  }


                  val windows = stateGenerator.handle(sortedRows)
                  windows
                case _ =>
                  val stateGenerator = new StateGeneratorBase(stateConf) {
                    override type T = RealinfoModel

                    override def getVid = (row: RealinfoModel) => row.vid

                    override def getTime = row => row.time

                    override def getCharge = row => row.charge.getOrElse(0)

                    override def getSoc = row => row.soc.getOrElse(0)

                    override def getSpeed = row => row.speed.getOrElse(0)

                    override def getMileage = row => row.mileage.getOrElse(0)
                  }


                  val windows = stateGenerator.handle(sortedRows.filter(_.soc.nonEmpty))
                  windows




                 /**
                  * 为了减少bug和便于后续维护，比如添加新的判断条件，这里全部采用函数式实现(尾递归)，最大化的减少变量的修改，便于程序推断。
                  * */
                  def handle(source: Seq[T]):List[Window[T]] = {
                    val windows = new ListBuffer[Window[T]]()

                    @tailrec
                    def splitWindow(curIndex: Int, curState: String): List[Window[T]] = {
                      def beginChargeStrict(i: Int): Boolean = {
                        val speed =getSpeed(source(i))
                        val charge = getCharge(source(i))
                        charge < 0 && speed <= 50
                      }

                      if (curIndex < source.length) {
                        //计算当日开始里程和结束里程
                        startMielage=if(startMielage==0) getMileage(source(curIndex)) else startMielage
                        if(getMileage(source(curIndex))!=0) endMileage=getMileage(source(curIndex))

                        //如果当前状态是满电，那么上一个状态一定是充电。
                        val (endIndex, state) = if (curState == Constant.ChargeState && beginFullCharge(source, curIndex)) {
                          def append(v: T): FullChargeWindow[T] = {
                            val fullChargeWindow = FullChargeWindow[T]()
                            fullChargeWindow.append(v)
                            windows.append(fullChargeWindow)
                            fullChargeWindow
                          }

                          val fullChargeWindow = append(source(curIndex))

                          (inFullCharge(source)(curIndex + 1, v => fullChargeWindow.append(v)), Constant.FullChargeState)
                        } else if (beginChargeStrict(curIndex) && beginCharge(source, curIndex)) {
                          def append(v: T): ChargeWindow[T] = {
                            val chargeWindow = ChargeWindow[T]()
                            windows.append(chargeWindow)
                            chargeWindow.append(v)
                            chargeWindow
                          }


                          val chargeWindow=append(source(curIndex))

                          (inCharge(source)(curIndex + 1,v=>chargeWindow.append(v)), Constant.ChargeState)
                        } else {
                          val curMap =source(curIndex)
                          val prevMap = if (curIndex>= 1) source(curIndex - 1) else curMap
                          if (!online(curMap, prevMap)) {
                            windows.append(TravelWindow().append(source(curIndex)))
                          } else {
                            onlineTime += getOnline(source, curIndex)
                            windows.lastOption match {
                              case Some(travelWindow: TravelWindow[T]) =>
                                travelWindow.append(source(curIndex))
                              case _ =>
                                windows.append(TravelWindow().append(source(curIndex)))
                            }
                          }
                          (curIndex + 1, Constant.TravelState)
                        }

                        setWindow(windows.last, state)
                        splitWindow(endIndex, state)
                      } else windows.toList
                    }

                    splitWindow(0, Constant.NoneState).filter(window => {
                      window match {
                        case travelWindow: TravelWindow[T] =>
                          setWindow(travelWindow, Constant.TravelState)
                          travelWindow.length >= 10
                        case _ => true
                      }
                    })
                  }


class Window[T] extends scala.collection.mutable.ArrayBuffer[T]{
  var onLineTime:Long=0
  var chargeTime:Long=0
  var fullChargeTime:Long=0
  var startMileage:Int=0
  var endMileage:Int=0

  var beginIndexInSouce:Int=0
  var endIndexInSource:Int=0

  var reason:Reason=_


  def getState:String="none"

  def append(v:T):Window[T]={
    super.append(v)
    this
  }
}



  /**
    * 计算一辆车的每一个状态的明细报表
    *
    * @param windows :按照时间排序的状态集合。
    * @return 返回每一个状态的计算结果。
    **/
  def computeAllStates(stateConf: StateConf, windows: List[Window[RealinfoModel]]): List[DetailModel] = {
    val result = new ListBuffer[DetailModel]

    val oneValidateRunTime = stateConf.getOption("OneValidateRunTime") match {
      case Some(v) => TimeParser.parserAsMs(v).get
      case None => 3 * 60 * 1000
    }


      @tailrec
    def doGetResult(curIndex: Int, prevState: String, prevResult: Option[DetailModel], prevChargeResult: Option[DetailModel]): List[DetailModel] = {
      if (curIndex < windows.length) {
        val curWindow = windows(curIndex)
        val (curResult, curState) = curWindow match {
          case run: TravelWindow[RealinfoModel] =>
            //如果当前状态的上一个状态是充电状态，那么将会使用这个状态来计算当前结果，否则不会使用。
            val curResult = if (prevState == Constant.ChargeState) new RunDetailCompute(stateConf, curWindow, prevResult).compute()
            else new RunDetailCompute(stateConf, curWindow, None).compute()
            if (curResult.timeLeng >= oneValidateRunTime && curResult.accRunTime >= oneValidateRunTime && curResult.mileage > 0) {
              result.append(curResult)
              (Some(curResult), Constant.TravelState)
            }else (prevResult, prevState)
          case full: FullChargeWindow[RealinfoModel] =>
            val curResult = new DetailCompute(stateConf, curWindow, None, None).compute()
            result.append(curResult)
            (Some(curResult), Constant.FullChargeState)
          case e if e.isInstanceOf[ChargeWindow[RealinfoModel]] || e.isInstanceOf[ChargeChangeWindow[RealinfoModel]] =>
            //如果存在上一个充电状态，那么将会使用这个充电状态的结果来计算当前结果，比如计算两次充个电时间间隔就需要使用上一个充电结果。
            val curResult = if (prevChargeResult.isEmpty) new ChargeDetailCompute(stateConf, curWindow, None).compute()
            else new ChargeDetailCompute(stateConf, curWindow, prevChargeResult).compute()
            result.append(curResult)
            (Some(curResult), Constant.ChargeState)
          case _ =>
            throw new RuntimeException
        }

        //如果当前状态是充电状态，那么需要更新充电状态
        doGetResult(curIndex + 1, curState, curResult, if (curState == Constant.ChargeState) curResult else prevChargeResult)
      } else {
        result.toList
      }
    }

    doGetResult(0, "", None, None)
  }



          //TODO 错误基础类
          case "faultDetail" => new FaultDetailCompute()
          case e => throw new RuntimeException(s"detail.compute=$e not support")
        }

        //明细计算
        val detailsResult = cmp.compute(stateConf, sortedRows)
        detailsResult
      })

override def compute(): DetailModel = {
    val startTimeOption = Utils.parsetDate(window.head.time)
    val (startTime: Long, startH: Int) = startTimeOption match {
      case Some(startDate) =>
        if (startDate.getMinutes == 0) (startDate.getTime, startDate.getHours)
        else (startDate.getTime, startDate.getHours + 1)
      case None =>
        (0, -1)
    }

    val endTimeOption = Utils.parsetDate(window.last.time)
    val (endTime: Long, endH: Int) = endTimeOption match {
      case Some(endDate) =>
        if (endDate.getMinutes == 0) (endDate.getTime, endDate.getHours)
        else (endDate.getTime, endDate.getHours + 1)
      case None =>
        (0, -1)
    }

    val state = window.getState

    val timeRange = endTime - startTime

    //开始里程
    var startMileage: Int = Int.MaxValue
    //结束里程
    var stopMileage: Int = 0
    var maxSpeed: Int = 0
    //最大总电压
    var maxTotalVoltage: Int = 0
    var minTotalVoltage: Int = Int.MaxValue

    //最大总电流
    var maxTotalEctriccurrent: Int = Int.MinValue
    var minTotalEctriccurrent: Int = Int.MaxValue

    //最大单体电压
    var maxSecondaryVolatage: Int = 0
    var minSecondaryVolatage: Int = Int.MaxValue

    //最大采集温度
    var maxAcquisitionPointTemp: Int = Int.MinValue
    var minAcquisitionPointTemp: Int = Int.MaxValue

    //最大发电机温度
    var maxEngineTemp: Int = Int.MinValue
    var minEngineTemp: Int = Int.MaxValue

    //最大soc
    var maxSoc: Int = 0
    var minSoc: Int = Int.MaxValue

    //总电流
    var totalCharge: Double = 0

    //开始经度
    var startLongitude = 0L
    //开始纬度
    var startLatitude = 0L
    var endLongitude = 0L
    var endLatitude = 0L

    //gps里程
    var gpsMileageM = 0D

    //开始soc
    var startSoc = 0
    var endSoc = 0


    //上一条实时数据
    var prevRealinfo: Option[RealinfoModel] = None
    //充电量分布
    val chargeDistributed = new PowerDistribution()
    //上一条明细
    var prevDetail: Option[RealinfoModel] = None

    //累计无效行驶时长和当前无效行驶时长
    var accInvalidateTime = 0
    //无效行驶滑动窗口长度
    var invalidateRunWindowSlidingNum = 0
    //无效行驶滑动窗口持续时间
    var invalidateRunWindowSlidingTime = 0
    //连续里程窗口总元素个数
    var continueMileageWindowTotalNum = 0


    //快充最大时间窗口，默认为5分钟
    var quickWindowMaxTime = stateConf.getOption("quickWindowMaxTime").map(TimeParser.parserAsMs(_).get).getOrElse((5 * 60 * 1000))
    //快充窗口的滑动时间，最大为quickWindowMaxTime分钟。
    var quickWindowSlidingTime = 0
    var quickChargeSlidingNum = 0
    var quickWindowMaxNum = 0
    var quickThreshold = stateConf.getOption("quickThreshold").map(_.toInt).getOrElse(-200)

    window.foreach(curRealinfo => {
      val timeDfMs: Int = if (prevRealinfo.isEmpty) 0 else {
        Utils.timeDiff(curRealinfo.time, prevRealinfo.get.time).toInt
      }

      val timeDfS = timeDfMs / 1000

      //统计GPS里程
      if (prevDetail.nonEmpty) {
        val diffMileageM = getDistanceM(prevDetail.get.longitude, prevDetail.get.latitude, curRealinfo.latitude, curRealinfo.longitude, timeDfS)
        gpsMileageM += diffMileageM
        //当前有效
        if (diffMileageM != 0) {
          prevDetail = Some(curRealinfo)
        }
      }

      //统计充电分布
      chargeDistributed.add(prevRealinfo, Some(curRealinfo))

      /** * 计算无效行驶时间
        * 首先设置一个时间窗口，假设为10分钟，如果这十分钟内里程没有变化，并且速度为0和挡位为空的比例超过70%，那么认为这是无效行驶。
        */
      prevRealinfo match {
        case Some(prev) =>
          if (curRealinfo.mileage == prev.mileage) {
            if ((curRealinfo.speed.nonEmpty && curRealinfo.speed.contains(0)) |
              (curRealinfo.peaking.nonEmpty && curRealinfo.peaking.contains(0))) {
              invalidateRunWindowSlidingNum += 1
              invalidateRunWindowSlidingTime += timeDfMs
            }

            continueMileageWindowTotalNum += 1
          } else {
            if (invalidateRunWindowSlidingTime >= invalidateRunWindowMaxTime && (invalidateRunWindowSlidingNum >= continueMileageWindowTotalNum * invalidatePercent)) {
              accInvalidateTime += invalidateRunWindowSlidingTime
            }
            invalidateRunWindowSlidingNum = 0
            invalidateRunWindowSlidingTime = 0
            continueMileageWindowTotalNum = 0
          }
        case _ =>
      }

      prevRealinfo = Some(curRealinfo)

      if (startSoc == 0 && !curRealinfo.soc.contains(0)) startSoc = curRealinfo.soc.get
      if (!curRealinfo.soc.contains(0)) endSoc = curRealinfo.soc.get


      //计算快慢充
      if (quickWindowSlidingTime < quickWindowMaxTime) {
        quickWindowSlidingTime += timeDfMs
        if (curRealinfo.charge.exists(_ < quickThreshold)) quickChargeSlidingNum += 1
        quickWindowMaxNum += 1
      }

      //计算开始里程和结束里程
      curRealinfo.mileage match {
        case Some(mileage) if mileage != 0 =>
          if (mileage < startMileage) startMileage = mileage
          if (mileage > stopMileage) stopMileage = mileage
        case _ =>
      }


      //计算最大速度
      curRealinfo.speed match {
        case Some(speed) => if (speed > maxSpeed) maxSpeed = speed
        case None =>
      }

      //计算最大总电压和最小总电压
      val voltage = curRealinfo.totalVoltage
      voltage match {
        case Some(totalVoltage) if totalVoltage != 0 =>
          if (totalVoltage > maxTotalVoltage) maxTotalVoltage = totalVoltage
          if (totalVoltage < minTotalVoltage) minTotalVoltage = totalVoltage

        case _ =>
      }


      val current = curRealinfo.charge
      current match {
        case Some(totalEctriccurrent) =>
          if (totalEctriccurrent > maxTotalEctriccurrent) maxTotalEctriccurrent = totalEctriccurrent
          if (totalEctriccurrent < minTotalEctriccurrent) minTotalEctriccurrent = totalEctriccurrent
        case None =>
      }


      curRealinfo.secondaryCellMaxVoltage match {
        case Some(secondaryMaxVoltage) => if (secondaryMaxVoltage > maxSecondaryVolatage) maxSecondaryVolatage = secondaryMaxVoltage
        case None =>
      }

      curRealinfo.secondaryCellMinVoltage match {
        case Some(secondaryMinVoltage) if secondaryMinVoltage != 0 => if (secondaryMinVoltage < minSecondaryVolatage) minSecondaryVolatage = secondaryMinVoltage
        case _ =>
      }

      curRealinfo.accuisitionPointMaxTemp match {
        case Some(acquisitionMaxPointTemp) =>
          if (acquisitionMaxPointTemp > maxAcquisitionPointTemp) maxAcquisitionPointTemp = acquisitionMaxPointTemp
        case None =>
      }


      curRealinfo.accuisitionPointMinTemp match {
        case Some(acquisitionMinPointTemp) =>
          if (acquisitionMinPointTemp < minAcquisitionPointTemp) minAcquisitionPointTemp = acquisitionMinPointTemp
        case None =>
      }

      curRealinfo.engineTemp match {
        case Some(engineTemp) =>
          if (engineTemp > maxEngineTemp) maxEngineTemp = engineTemp
          if (engineTemp < minEngineTemp) minEngineTemp = engineTemp
        case None =>
      }


      curRealinfo.soc match {
        case Some(soc) =>
          if (soc > maxSoc) maxSoc = soc
          if (soc < minSoc) minSoc = soc
        case None =>
      }


      voltage match {
        case Some(totalVoltage) =>
          current match {
            case Some(totalCurrent) =>

              val charge: Double = DataPrecision.totalCharge(totalCurrent, totalVoltage, timeDfS)
              totalCharge += charge
            case None =>
          }
        case None =>
      }

      if (startLongitude == 0L) startLongitude = curRealinfo.longitude.getOrElse(0L)
      if (startLatitude == 0L) startLatitude = curRealinfo.latitude.getOrElse(0L)

      if (curRealinfo.longitude.getOrElse(0L) != 0) endLongitude = curRealinfo.longitude.getOrElse(0L)
      if (curRealinfo.latitude.getOrElse(0L) != 0) endLatitude = curRealinfo.latitude.getOrElse(0L)
    })

    if (invalidateRunWindowSlidingTime >= invalidateRunWindowMaxTime && (invalidateRunWindowSlidingNum >= continueMileageWindowTotalNum * invalidatePercent)) {
      accInvalidateTime += invalidateRunWindowSlidingTime
    }


    //计算平均速度
    def getAvgSpeed: Double = if (timeRange != 0) DataPrecision.mileage(stopMileage - startMileage) / DataPrecision.toHour(timeRange) else 0

    //计算剔除无效行驶后的实际运行时间
    val accRunTime = timeRange.toInt - accInvalidateTime

    // val accRunTimeHour = accRunTime / (1000 * 3600).toDouble

    //判断是否是快慢充
    val isQuickCharge = quickChargeSlidingNum >= (
      quickWindowMaxNum * stateConf.getOption("quick.percent").map(_.toDouble).getOrElse(0.7))

    def getOrInvalidate(v: Int): Int = if (v == Int.MaxValue || v == Int.MinValue) InvalidateValue.Value else v


    DetailModel(
      vid = window.head.vid,
      vin = window.head.vin,
      category = state,
      onlineTime = window.onLineTime.toInt,
      startTime = startTime,
      endTime = endTime,
      timeLeng = timeRange,
      accRunTime = timeRange.toInt - accInvalidateTime,
      startMileageOfCurrentDay = window.startMileage,
      endMileageOfCurrentDay = window.endMileage,
      startMileage = if (startMileage == Int.MaxValue) 0 else startMileage,
      stopMileage = stopMileage,
      gpsMileage = (gpsMileageM * 100).toInt,
      avgSpeed = getAvgSpeed,
      maxSpeed = maxSpeed,

      maxTotalVoltage = maxTotalVoltage,
      minTotalVoltage = Utils.roundMin(minTotalVoltage),

      maxTotalCurrent = maxTotalEctriccurrent,
      minTotalCurrent = Utils.roundMin(minTotalEctriccurrent),
      maxSecondaryVolatage = maxSecondaryVolatage,
      minSecondaryVolatage = Utils.roundMin(minSecondaryVolatage),
      maxAcquisitionPointTemp = if (maxAcquisitionPointTemp == Int.MinValue) None else Some(maxAcquisitionPointTemp),
      minAcquisitionPointTemp = Utils.roundMin(minAcquisitionPointTemp),
      maxEngineTemp = if (maxEngineTemp == Int.MinValue) None else Some(maxEngineTemp),
      minEngineTemp = Utils.roundMin(minEngineTemp),
      maxSoc = maxSoc,
      minSoc = Utils.roundMin(minSoc),
      startSoc = startSoc,
      endSoc = endSoc,
      startLongitude = startLongitude,
      startLatitude = startLatitude,
      endLongitude = endLongitude,
      endLatitude = endLatitude,
      totalCharge = totalCharge,
      Math.abs(startTime - prevChargeResult.map(_.endTime).getOrElse(startTime)).toInt,
      prevChargeResult.map(_.stopMileage).getOrElse(0),
      prevResult.map(_.endTime).getOrElse(0),
      prevResult.map(_.maxTotalCurrent).getOrElse(0),
      isQuickCharge = isQuickCharge,
      powerDistribution = chargeDistributed.getDistribution
    )
  }


case class DetailModel(
    vid:String,
    vin:String,
    category:String,
    onlineTime:Int=0,
    startTime:Long=0,
    endTime:Long=0,
    timeLeng:Long=0,

    //实际行驶时间=timelenth-无效形式时间
    accRunTime:Int=0,
    //当日开始里程
    startMileageOfCurrentDay:Int=0,
   //当日结束里程
    endMileageOfCurrentDay:Int=0,
   //开始里程
    startMileage:Int=0,
   //结束里程
    stopMileage:Int=Int.MaxValue,

   //gps里程
    gpsMileage: Int=0,
  //平均速度
    avgSpeed:Double=0,
    maxSpeed:Int=0,

  //极值数据
    maxTotalVoltage:Int=0,
    minTotalVoltage:Int=Int.MaxValue,
  //最大总电流
    maxTotalCurrent:Int=0,
    minTotalCurrent:Int=Int.MaxValue,
    //最大单体电压
    maxSecondaryVolatage:Int=0,
    minSecondaryVolatage:Int=Int.MaxValue,
  //最大采集温度
    maxAcquisitionPointTemp:Option[Int]=None,
    minAcquisitionPointTemp:Int=Int.MaxValue,

  //最大发电机温度
    maxEngineTemp:Option[Int]=None,
    minEngineTemp:Int=Int.MaxValue,

  //最大soc
    maxSoc:Int=0,
    minSoc:Int=Int.MaxValue,

  //开始soc
    startSoc:Int=0,
    endSoc:Int=0,

   //开始经度
    startLongitude:Long=0,

    //开始纬度
    startLatitude:Long=0,
  //结束经度
    endLongitude:Long=0,
    //结束纬度
    endLatitude:Long=0,

    //总电流
    totalCharge:Double=0,
    //两次充电时间间隔
    timeBetweenCharge:Int=0,
  //上次充电结束里程
    stopMileageOfPrevCharge:Int=0,
  //上次充电结束时间
    prevChargeStopTime:Long=0,
    maxCurrentOfPrevCharge:Int=0,
  //是否为快充
    isQuickCharge:Boolean=false,
  //充电量分布
    powerDistribution:Array[Double])

-----------------------------------------
规则

     vid = window.head.vid,

     vin = window.head.vin,

     category = state,//"none"

     case class NoneWindow[T]() extends Window[T]{
       override def getState = "none"
     }
     case class TravelWindow[T]() extends Window[T]{
       override def getState = "run"
     }
     case class ChargeWindow[T]()extends  Window[T]{
       override def getState = "charge"
     }

     case class FullChargeWindow[T]()extends  Window[T]{
       override def getState = "fullcharge"
     }

     case class ForwardWindow[T]()extends Window[T]{
       override def getState = "forward"
     }

     case class AlarmWindow[T]()extends  Window[T]{
       override def getState = "alarm"
     }
     case class LoginWindow[T]()extends Window[T]
     {
       override def getState = "login"
     }

     case class ChargeChangeWindow[T](correct:Double)extends Window[T]{
       override def getState = "chargechange"
     }


     onlineTime = window.onLineTime.toInt,

-----------------------------------------------------------
     startTime = startTime,

    val startTimeOption = Utils.parsetDate(window.head.time)
    val (startTime: Long, startH: Int) = startTimeOption match {
      case Some(startDate) =>
        if (startDate.getMinutes == 0) (startDate.getTime, startDate.getHours)
        else (startDate.getTime, startDate.getHours + 1)
      case None =>
        (0, -1)
    }

------------------------------------------------------------

     endTime = endTime,

      val endTimeOption = Utils.parsetDate(window.last.time)
      val (endTime: Long, endH: Int) = endTimeOption match {
       case Some(endDate) =>
         if (endDate.getMinutes == 0) (endDate.getTime, endDate.getHours)
        else (endDate.getTime, endDate.getHours + 1)
       case None =>
         (0, -1)
     }


------------------------------------------------------------

     timeLeng = timeRange,

----------------------------------------------------------------------------
  //计算剔除无效行驶后的实际运行时间
     accRunTime = timeRange.toInt - accInvalidateTime,

     prevRealinfo match {
             case Some(prev) =>
               if (curRealinfo.mileage == prev.mileage) {
                 if ((curRealinfo.speed.nonEmpty && curRealinfo.speed.contains(0)) |
                   (curRealinfo.peaking.nonEmpty && curRealinfo.peaking.contains(0))) {
                   invalidateRunWindowSlidingNum += 1
                   invalidateRunWindowSlidingTime += timeDfMs
                 }

                 continueMileageWindowTotalNum += 1
               } else {
                 if (invalidateRunWindowSlidingTime >= invalidateRunWindowMaxTime && (invalidateRunWindowSlidingNum >= continueMileageWindowTotalNum * invalidatePercent)) {
                   accInvalidateTime += invalidateRunWindowSlidingTime
                 }
                 invalidateRunWindowSlidingNum = 0
                 invalidateRunWindowSlidingTime = 0
                 continueMileageWindowTotalNum = 0
               }
             case _ =>
           }


 if (invalidateRunWindowSlidingTime >= invalidateRunWindowMaxTime && (invalidateRunWindowSlidingNum >= continueMileageWindowTotalNum * invalidatePercent)) {
      accInvalidateTime += invalidateRunWindowSlidingTime
    }



     startMileageOfCurrentDay = window.startMileage,

     endMileageOfCurrentDay = window.endMileage,

     startMileage = if (startMileage == Int.MaxValue) 0 else startMileage,

           //计算开始里程和结束里程
           curRealinfo.mileage match {
             case Some(mileage) if mileage != 0 =>
               if (mileage < startMileage) startMileage = mileage
               if (mileage > stopMileage) stopMileage = mileage
             case _ =>
           }



     stopMileage = stopMileage,

       //计算开始里程和结束里程
           curRealinfo.mileage match {
             case Some(mileage) if mileage != 0 =>
               if (mileage < startMileage) startMileage = mileage
               if (mileage > stopMileage) stopMileage = mileage
             case _ =>
           }


         //计算平均速度
            def getAvgSpeed: Double = if (timeRange != 0) DataPrecision.mileage(stopMileage - startMileage) / DataPrecision.toHour(timeRange) else 0

     gpsMileage = (gpsMileageM * 100).toInt,

          //统计GPS里程
           val timeDfMs: Int = if (prevRealinfo.isEmpty) 0 else {
            Utils.timeDiff(curRealinfo.time, prevRealinfo.get.time).toInt
          }

          val timeDfS = timeDfMs / 1000

          //统计GPS里程
          if (prevDetail.nonEmpty) {
            val diffMileageM = getDistanceM(prevDetail.get.longitude, prevDetail.get.latitude, curRealinfo.latitude, curRealinfo.longitude, timeDfS)
            gpsMileageM += diffMileageM
            //当前有效
            if (diffMileageM != 0) {
              prevDetail = Some(curRealinfo)
            }
          }

         def getDistanceM(longitudeA: Option[Long], latitudeA: Option[Long], longitudeB: Option[Long], latitudeB: Option[Long], timeDfS: Long): Double = {
            val diffMileage = if (latitudeA.nonEmpty && longitudeA.nonEmpty && latitudeB.nonEmpty && longitudeB.nonEmpty) {
              GpsDistance.getDistance(
                DataPrecision.latitude(longitudeA.get),
                DataPrecision.latitude(latitudeA.get),
                DataPrecision.latitude(latitudeB.get),
                DataPrecision.latitude(longitudeB.get)
              )
            } else {
              0D
            }

            val maxMileageStepM_S = stateConf.getOption("run.speed").map(_.toDouble).getOrElse(0.1) * 1000
            //如果每秒行驶的距离大于run.speed，那么认为无效
            val speedM_S = (diffMileage * 1000) / timeDfS
            if (speedM_S >= maxMileageStepM_S || speedM_S <= 0) 0 else diffMileage
          }


     avgSpeed = getAvgSpeed,

       //计算平均速度
         def getAvgSpeed: Double = if (timeRange != 0) DataPrecision.mileage(stopMileage - startMileage) / DataPrecision.toHour(timeRange) else 0

     maxSpeed = maxSpeed,

     //计算最大速度
           curRealinfo.speed match {
             case Some(speed) => if (speed > maxSpeed) maxSpeed = speed
             case None =>
           }

--------------------------------------------------------------------
       maxTotalVoltage = maxTotalVoltage,

        //计算最大总电压和最小总电压
           val voltage = curRealinfo.totalVoltage
           voltage match {
             case Some(totalVoltage) if totalVoltage != 0 =>
               if (totalVoltage > maxTotalVoltage) maxTotalVoltage = totalVoltage
               if (totalVoltage < minTotalVoltage) minTotalVoltage = totalVoltage

             case _ =>
           }

-------------------------------------------------------------------
     minTotalVoltage = Utils.roundMin(minTotalVoltage),

      //计算最大总电压和最小总电压
           val voltage = curRealinfo.totalVoltage
           voltage match {
             case Some(totalVoltage) if totalVoltage != 0 =>
               if (totalVoltage > maxTotalVoltage) maxTotalVoltage = totalVoltage
               if (totalVoltage < minTotalVoltage) minTotalVoltage = totalVoltage

             case _ =>
           }

-------------------------------------------------------------------

     maxTotalCurrent = maxTotalEctriccurrent,

       val current = curRealinfo.charge
           current match {
             case Some(totalEctriccurrent) =>
               if (totalEctriccurrent > maxTotalEctriccurrent) maxTotalEctriccurrent = totalEctriccurrent
               if (totalEctriccurrent < minTotalEctriccurrent) minTotalEctriccurrent = totalEctriccurrent
             case None =>
           }


  ----------------------------------------------------------------------
     minTotalCurrent = Utils.roundMin(minTotalEctriccurrent)

       val current = curRealinfo.charge
          current match {
            case Some(totalEctriccurrent) =>
              if (totalEctriccurrent > maxTotalEctriccurrent) maxTotalEctriccurrent = totalEctriccurrent
              if (totalEctriccurrent < minTotalEctriccurrent) minTotalEctriccurrent = totalEctriccurrent
            case None =>
          }


     minTotalCurrent = Utils.roundMin(minTotalEctriccurrent),

     maxSecondaryVolatage = maxSecondaryVolatage,

        curRealinfo.secondaryCellMaxVoltage match {
             case Some(secondaryMaxVoltage) => if (secondaryMaxVoltage > maxSecondaryVolatage) maxSecondaryVolatage = secondaryMaxVoltage
             case None =>
           }

--------------------------------------------------------------------------------------
     minSecondaryVolatage = Utils.roundMin(minSecondaryVolatage),

         curRealinfo.secondaryCellMinVoltage match {
             case Some(secondaryMinVoltage) if secondaryMinVoltage != 0 => if (secondaryMinVoltage < minSecondaryVolatage) minSecondaryVolatage = secondaryMinVoltage
             case _ =>
           }

     maxAcquisitionPointTemp = if (maxAcquisitionPointTemp == Int.MinValue) None else Some(maxAcquisitionPointTemp),

      curRealinfo.accuisitionPointMaxTemp match {
             case Some(acquisitionMaxPointTemp) =>
               if (acquisitionMaxPointTemp > maxAcquisitionPointTemp) maxAcquisitionPointTemp = acquisitionMaxPointTemp
             case None =>
           }


     minAcquisitionPointTemp = Utils.roundMin(minAcquisitionPointTemp),

       curRealinfo.accuisitionPointMinTemp match {
             case Some(acquisitionMinPointTemp) =>
               if (acquisitionMinPointTemp < minAcquisitionPointTemp) minAcquisitionPointTemp = acquisitionMinPointTemp
             case None =>
           }


     maxEngineTemp = if (maxEngineTemp == Int.MinValue) None else Some(maxEngineTemp),

     curRealinfo.engineTemp match {
             case Some(engineTemp) =>
               if (engineTemp > maxEngineTemp) maxEngineTemp = engineTemp
               if (engineTemp < minEngineTemp) minEngineTemp = engineTemp
             case None =>
           }


     minEngineTemp = Utils.roundMin(minEngineTemp),

      curRealinfo.engineTemp match {
             case Some(engineTemp) =>
               if (engineTemp > maxEngineTemp) maxEngineTemp = engineTemp
               if (engineTemp < minEngineTemp) minEngineTemp = engineTemp
             case None =>
           }

     maxSoc = maxSoc,

     curRealinfo.soc match {
             case Some(soc) =>
               if (soc > maxSoc) maxSoc = soc
               if (soc < minSoc) minSoc = soc
             case None =>
           }



     minSoc = Utils.roundMin(minSoc),

      curRealinfo.soc match {
             case Some(soc) =>
               if (soc > maxSoc) maxSoc = soc
               if (soc < minSoc) minSoc = soc
             case None =>
           }



     startSoc = startSoc,
     if (startSoc == 0 && !curRealinfo.soc.contains(0)) startSoc = curRealinfo.soc.get


     endSoc = endSoc,
      if (!curRealinfo.soc.contains(0)) endSoc = curRealinfo.soc.get


     startLongitude = startLongitude,
      if (startLongitude == 0L) startLongitude = curRealinfo.longitude.getOrElse(0L)


     startLatitude = startLatitude,
     if (startLatitude == 0L) startLatitude = curRealinfo.latitude.getOrElse(0L)


     endLongitude = endLongitude,

      if (curRealinfo.longitude.getOrElse(0L) != 0) endLongitude = curRealinfo.longitude.getOrElse(0L)



     endLatitude = endLatitude,
     if (curRealinfo.latitude.getOrElse(0L) != 0) endLatitude = curRealinfo.latitude.getOrElse(0L)


     totalCharge = totalCharge,

       voltage match {
             case Some(totalVoltage) =>
               current match {
                 case Some(totalCurrent) =>

                   val charge: Double = DataPrecision.totalCharge(totalCurrent, totalVoltage, timeDfS)
                   totalCharge += charge
                 case None =>
               }
             case None =>
           }

    //两次充电时间间隔
    timeBetweenCharge:Int=0,
     Math.abs(startTime - prevChargeResult.map(_.endTime).getOrElse(startTime)).toInt

 val curResult = if (prevChargeResult.isEmpty) new ChargeDetailCompute(stateConf, curWindow, None).compute()
            else new ChargeDetailCompute(stateConf, curWindow, prevChargeResult).compute()



     //上次充电结束里程
      stopMileageOfPrevCharge:Int=0,
     prevChargeResult.map(_.stopMileage).getOrElse(0)


    //上次充电结束时间
    prevChargeStopTime:Long=0,
    maxCurrentOfPrevCharge:Int=0,
     prevResult.map(_.endTime).getOrElse(0),


    maxCurrentOfPrevCharge:Int=0,
     prevResult.map(_.maxTotalCurrent).getOrElse(0),



     //是否为快充
    isQuickCharge:Boolean=false,
     isQuickCharge = isQuickCharge,

 val isQuickCharge = quickChargeSlidingNum >= (
      quickWindowMaxNum * stateConf.getOption("quick.percent").map(_.toDouble).getOrElse(0.7))

//计算快慢充
      if (quickWindowSlidingTime < quickWindowMaxTime) {
        quickWindowSlidingTime += timeDfMs
        if (curRealinfo.charge.exists(_ < quickThreshold)) quickChargeSlidingNum += 1
        quickWindowMaxNum += 1
      }

      //判断是否是快慢充
          val isQuickCharge = quickChargeSlidingNum >= (
            quickWindowMaxNum * stateConf.getOption("quick.percent").map(_.toDouble).getOrElse(0.7))

---------------------------------------------


     //充电量分布
     powerDistribution = chargeDistributed.getDistribution

    val chargeDistributed = new PowerDistribution()



    class PowerDistribution() {
      private val distributed: Array[Double] = Array.fill(48)(0D)

      def add(prevChargeValue: Option[RealinfoModel], currentChargeValue: Option[RealinfoModel]): Unit = {
        (prevChargeValue, currentChargeValue) match {
          case (Some(prev), Some(curv)) =>
            val curDate = Utils.parsetDate(curv.time).get
            val curMinute = curDate.getHours * 60 + curDate.getMinutes + (if (curDate.getMinutes == 0) 0 else 1)
            val timeDiffS= Utils.timeDiff(curv.time, prev.time).toInt / 1000
            val charge = DataPrecision.totalCharge(curv.charge.getOrElse(0), curv.totalVoltage.getOrElse(0), timeDiffS)
           val i=curMinute / 30
            if(i<distributed.length) distributed(i) += charge
          case _ =>
        }
      }



      def add(distribution:Array[Double]): Unit = {
        distribution.indices.foreach(index => {
          if (index < distributed.length) distributed(index) += distribution(index)
        })
      }


      def getDistribution: Array[Double] = this.distributed
    }

    object PowerDistribution{
      def default: Array[Double] = Array.fill(48)(0D)
    }





#!/bin/sh
source ExitCodeCheck.sh

opts=$@

getparam(){
arg=$1
echo $opts |xargs -n1 |cut -b 2- |awk -F'=' '{if($1=="'"$arg"'") print $2}'
}

#./SurplusSummaryJob.sh -hdp_queue=queue_1514_01

IncStart=`getparam inc_start`
IncEnd=`getparam inc_end`
oracle_connection=`getparam jdbc_str`
oracle_username=`getparam db_user`
oracle_password=`getparam db_psw`
dataName=`getparam db_sid`
queueName=`getparam hdp_queue`
hdfshostname=`getparam hdfs_host`;


#Job名称
job_name=$0

#下载程序包和配置文件
#rm -rf ${job_name}.jar;
#rm -rf ${job_name};
#mkdir ${job_name};
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/jar/${job_name}.jar .;
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/lib/* ./${job_name};



#任务名取脚本名
#job_name=$0

JOB_HOME=/appcom/apps/hduser1508/zhangyongtian001/sparkjob
cd $JOB_HOME
#!/usr/bin/env bash
export SPARK_REPORT_PATH=/usr/local/sparkjob/bitnei
export SPARK_REPORT_EXPORT_JAR_DIRECTORY=${SPARK_REPORT_PATH}/jars
export SPARK_REPORT_CONF=$SPARK_REPORT_PATH/conf/conf.properties
export SPARK_REPORT_LOG=$SPARK_REPORT_PATH/log


export SPARK_REPORT_EXPORT_JARS=$(${SPARK_REPORT_PATH}/bin/getExportJars.sh)
export SPARK_REPORT_PARQUET_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-parquet*.jar)
export SPARK_REPORT_DAY_DETAIL_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-detail*.jar)
export SPARK_REPORT_DAY_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_DAY_MONTH_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_TAXIS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-taxis*.jar)
export SPARK_REPORT_OPERATION_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-operation*.jar)
export SPARK_REPORT_MILEAGE_CHECK_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-mileage-check*.jar)


export SPARK_REPORT_PARQUET_PATH=${SPARK_REPORT_PATH}/bin/parquet
export SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob
export SPARK_REPORT_DAYREPORT_PATH=${SPARK_REPORT_PATH}/bin/dayreport
export SPARK_REPORT_OPERATION_INDEX=${SPARK_REPORT_PATH}/bin/operation


function startJob(){
  startDate=$1
  endDate=$2
  jobType=$3

  if [ "$jobType" == "parquet" ] ;
  then
   source ${SPARK_REPORT_PARQUET_PATH}/parquet.sh $startDate $endDate
  elif [ "$jobType" == "detail_charge_run" ] ;
  then
   source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate "detail_charge_run"
  elif [ "$jobType" == "detail_charge_run_output" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate "detail_charge_run_output"
  elif [ "$jobType" == "detail" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate
  elif [ "$jobType" == "detailjob" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate "detailjob"
  elif [ "$jobType" == "detail_output" ] ;
  then
    echo "begin execute $jobType"
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate "detail_output"
  elif [ "$jobType" == "day_category" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/dayreport.sh $startDate $endDate "day_category"
  elif [ "$jobType" == "dayreport" ] ;
  then
    source ${SPARK_REPORT_DAYREPORT_PATH}/dayreport.sh $startDate $endDate "dayreport"
  elif [ "$jobType" == "dayreport_output" ] ;
  then
    source ${SPARK_REPORT_DAYREPORT_PATH}/dayreport.sh $startDate $endDate "dayreport_output"
  else
    source ${SPARK_REPORT_PARQUET_PATH}/parquet.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate
    source ${SPARK_REPORT_DAYREPORT_PATH}/dayreport.sh $startDate $endDate
    source ${SPARK_REPORT_OPERATION_INDEX}/operation_mileage.sh $startDate $endDate
    source ${SPARK_REPORT_OPERATION_INDEX}/operation_detail.sh $startDate $endDate
  fi
}


startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"

jobType=$3

startJob $startDate $endDate $jobType


/appcom/spark/bin/spark-submit  \
--jars $JOB_HOME/lib/dom4j-1.6.1.jar \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 20G \
  --num-executors 20 \
  --executor-cores 8 \
  --driver-memory 2G \
  --queue ${queueName} \
   --conf spark.default.parallelism=320 \
   --conf spark.yarn.executor.memoryoverhead=8G \
   --conf spark.storage.memoryFraction=0.6 \
   --conf spark.shuffle.memoryFraction=0.3 \
   --conf spark.shuffle.file.buffer=64k  \
   --conf spark.shuffle.consolidateFiles=true \
   --conf spark.shuffle.io.maxRetries=10 \
   --conf spark.shuffle.io.retryWait=30 \
   --conf spark.locality.wait=10s \
   --conf spark.akka.timeout=150s \
   --conf spark.network.timeout=200s\
   --conf spark.speculation=false \
   --conf spark.broadcast.blockSize=4m \
   --conf spark.files.fetchTimeout=360 \
   --conf spark.network.timeout=360 \
   --conf spark.cleaner.ttl=240000 \
   --conf spark.kryoserializer.buffer.mb=10 \
   --conf spark.reducer.maxSizeInFligth=12 \
   --conf spark.sql.shuffle.partitions=320 \
   --conf spark.sql.codegen=true \
   --conf spark.sql.parquet.compression.codec=snappy \
   --conf spark.sql.adaptive.enabled=true \
   --conf spark.sql.broadcastTimeout=600 \
   --conf spark.sql.autoBroadcastJoinThreshold=10485760 \
   --conf spark.sql.inMemoryColumnarStorage.batchSize=10000 \
   --conf spark.sql.files.maxPartitionBytes=134217728 \
   --conf spark.sql.files.openCostInBytes=5242880  \
   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseCompressedStrings" \
   --class .ifrs17.dcs.launcher.DcsIndExecutor \
   lbdp_corepub_ifrs17_job-1.0.0.jar  \
   env=prd  \
   log.level=error \
   PROC_DATE=`getparam PROC_DATE` \
   SPCODE_SELECTION=1 \
   src_version_num=`getparam src_version_num` \
   res_version_num=`getparam res_version_num` \
   res_hive_db=`getparam res_hive_db` \
   out.hdfs.path=/apps-data/hduser1508/sx_core_safe/lar_dcs_result_ind


exitCodeCheck $?
#!/bin/sh
source ExitCodeCheck.sh

opts=$@

getparam(){
arg=$1
echo $opts |xargs -n1 |cut -b 2- |awk -F'=' '{if($1=="'"$arg"'") print $2}'
}

#./SurplusSummaryJob.sh -hdp_queue=queue_1514_01

IncStart=`getparam inc_start`
IncEnd=`getparam inc_end`
oracle_connection=`getparam jdbc_str`
oracle_username=`getparam db_user`
oracle_password=`getparam db_psw`
dataName=`getparam db_sid`
queueName=`getparam hdp_queue`
hdfshostname=`getparam hdfs_host`;


#Job名称
job_name=$0

#下载程序包和配置文件
#rm -rf ${job_name}.jar;
#rm -rf ${job_name};
#mkdir ${job_name};
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/jar/${job_name}.jar .;
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/lib/* ./${job_name};



#任务名取脚本名
#job_name=$0

JOB_HOME=/appcom/apps/hduser1508/zoubo162/sparkjob
cd $JOB_HOME


/appcom/spark/bin/spark-submit  \
--jars $JOB_HOME/lib/dom4j-1.6.1.jar \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 12G \
  --num-executors 20 \
  --executor-cores 6 \
  --driver-memory 4G \
  --queue ${queueName} \
   --conf spark.default.parallelism=600 \
   --conf spark.yarn.executor.memoryoverhead=4G \
   --conf spark.storage.memoryFraction=0.6 \
   --conf spark.shuffle.memoryFraction=0.3 \
   --conf spark.shuffle.file.buffer=128k  \
   --conf spark.shuffle.consolidateFiles=true \
   --conf spark.shuffle.io.maxRetries=10 \
   --conf spark.shuffle.io.retryWait=30 \
   --conf spark.locality.wait=10s \
   --conf spark.akka.timeout=180s \
   --conf spark.speculation=false \
   --conf spark.broadcast.blockSize=4m \
   --conf spark.files.fetchTimeout=600 \
   --conf spark.network.timeout=600 \
   --conf spark.cleaner.ttl=240000 \
   --conf spark.reducer.maxSizeInFligth=12 \
   --conf spark.sql.shuffle.partitions=600 \
   --conf spark.sql.codegen=true \
   --conf spark.sql.parquet.compression.codec=snappy \
   --conf spark.sql.adaptive.enabled=true \
   --conf spark.sql.broadcastTimeout=600 \
   --conf spark.sql.autoBroadcastJoinThreshold=2147483647 \
   --conf spark.sql.inMemoryColumnarStorage.batchSize=10000 \
   --conf spark.sql.files.maxPartitionBytes=134217728 \
   --conf spark.sql.files.openCostInBytes=5242880  \
   --conf spark.kryoserializer.buffer.max=512m \
   --conf spark.kryoserializer.buffer=128k \
   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseCompressedStrings" \
   --class .ifrs17.grouping.launcher.IndGroupingExecutor \
   lbdp_corepub_ifrs17_job-1.0.0.jar \
   env=prd  \
   log.level=error \
   p_end_date=`getparam p_end_date` \
   p_proc_date=`getparam p_proc_date` \
   src_version_num=`getparam src_version_num` \
   res_version_num=`getparam res_version_num` \
   res_hive_db=sx_hx_safe \
   p_proc_month="20181127" \
   partition_nums="600"
exitCodeCheck $?
use sx_core_safe;

SET mapreduce.job.queuename=root.queue_sx80.1508_11;

#!/bin/sh

./MpIndJob.sh  \
-inc_start=20181105 \
-inc_end=20181105 \
-jdbc_str=jdbc:oracle:thin:@d0lush0.dbdev.paic.com.cn:1526:d0lush0 \
-db_user=Reinsdev \
-db_psw=paic56789 \
-db_sid=d0lush0 \
-hdp_queue=root.queue_sx80.1508_11 \
-hdfs_host=10.25.77.42 \
-p_proc_date=20181105 \
-src_version_num=20181105 \
-res_version_num=20181105 \
-res_hive_db=sx_hx_safe


./DcsIndJob.sh  \
-inc_start=20180801 \
-inc_end=20180901 \
-jdbc_str=jdbc:oracle:thin:@d0lush0.dbdev.paic.com.cn:1526:d0lush0 \
-db_user=Reinsdev \
-db_psw=paic56789 \
-db_sid=d0lush0 \
-hdp_queue=root.queue_sx80.1508_11 \
-hdfs_host=10.25.77.42 \
-src_version_num=20181102 \
-res_version_num=20181102 \
-res_hive_db=sx_hx_safe
#!/bin/sh
source ExitCodeCheck.sh

opts=$@

getparam(){
arg=$1
echo $opts |xargs -n1 |cut -b 2- |awk -F'=' '{if($1=="'"$arg"'") print $2}'
}

#./SurplusSummaryJob.sh -hdp_queue=queue_1514_01

IncStart=`getparam inc_start`
IncEnd=`getparam inc_end`
oracle_connection=`getparam jdbc_str`
oracle_username=`getparam db_user`
oracle_password=`getparam db_psw`
dataName=`getparam db_sid`
queueName=`getparam hdp_queue`
hdfshostname=`getparam hdfs_host`;


#Job名称
job_name=$0

#下载程序包和配置文件
#rm -rf ${job_name}.jar;
#rm -rf ${job_name};
#mkdir ${job_name};
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/jar/${job_name}.jar .;
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/lib/* ./${job_name};



#任务名取脚本名
#job_name=$0

JOB_HOME=/appcom/apps/hduser1508/zhangyongtian001/sparkjob
cd $JOB_HOME


/appcom/spark/bin/spark-submit  \
--jars $JOB_HOME/lib/dom4j-1.6.1.jar \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 15G \
  --num-executors 15 \
  --executor-cores 5 \
  --driver-memory 2G \
  --queue ${queueName} \
   --conf spark.default.parallelism=80 \
   --conf spark.yarn.executor.memoryoverhead=4G \
   --conf spark.storage.memoryFraction=0.6 \
   --conf spark.shuffle.memoryFraction=0.3 \
   --conf spark.shuffle.file.buffer=64k  \
   --conf spark.shuffle.consolidateFiles=true \
   --conf spark.shuffle.io.maxRetries=10 \
   --conf spark.shuffle.io.retryWait=30 \
   --conf spark.locality.wait=10s \
   --conf spark.akka.timeout=150s \
   --conf spark.network.timeout=200s\
   --conf spark.speculation=false \
   --conf spark.broadcast.blockSize=4m \
   --conf spark.files.fetchTimeout=360 \
   --conf spark.network.timeout=360 \
   --conf spark.cleaner.ttl=240000 \
   --conf spark.kryoserializer.buffer.mb=10 \
   --conf spark.reducer.maxSizeInFligth=12 \
   --conf spark.sql.shuffle.partitions=80 \
   --conf spark.sql.codegen=true \
   --conf spark.sql.parquet.compression.codec=snappy \
   --conf spark.sql.adaptive.enabled=true \
   --conf spark.sql.broadcastTimeout=600 \
   --conf spark.sql.autoBroadcastJoinThreshold=10485760 \
   --conf spark.sql.inMemoryColumnarStorage.batchSize=10000 \
   --conf spark.sql.files.maxPartitionBytes=134217728 \
   --conf spark.sql.files.openCostInBytes=5242880  \
   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseCompressedStrings" \
   --class .ifrs17.mp.launcher.MPIndExecutor \
   lbdp_corepub_ifrs17_job-1.0.0.jar \
   env=prd  \
   log.level=error \
   p_proc_date=`getparam p_proc_date` \
   src_version_num=`getparam src_version_num` \
   res_version_num=`getparam res_version_num` \
   res_hive_db=`getparam res_hive_db`

exitCodeCheck $?
#!/bin/sh
source ExitCodeCheck.sh

opts=$@

getparam(){
arg=$1
echo $opts |xargs -n1 |cut -b 2- |awk -F'=' '{if($1=="'"$arg"'") print $2}'
}

#./SurplusSummaryJob.sh -hdp_queue=queue_1514_01

IncStart=`getparam inc_start`
IncEnd=`getparam inc_end`
oracle_connection=`getparam jdbc_str`
oracle_username=`getparam db_user`
oracle_password=`getparam db_psw`
dataName=`getparam db_sid`
queueName=`getparam hdp_queue`
hdfshostname=`getparam hdfs_host`;


#Job名称
job_name=$0

#下载程序包和配置文件
#rm -rf ${job_name}.jar;
#rm -rf ${job_name};
#mkdir ${job_name};
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/jar/${job_name}.jar .;
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/lib/* ./${job_name};



#任务名取脚本名
#job_name=$0

JOB_HOME=/appcom/apps/hduser1508/zhangyongtian001/sparkjob
cd $JOB_HOME


/appcom/spark/bin/spark-submit  \
--jars $JOB_HOME/lib/dom4j-1.6.1.jar,$JOB_HOME/lib/ojdbc6.jar \
  --master yarn \
  --deploy-mode client \
  --executor-memory 1G \
  --num-executors 1 \
  --executor-cores 1 \
  --driver-memory 1G \
  --queue ${queueName} \
   --conf spark.storage.memoryFraction=0.6 \
   --conf spark.shuffle.memoryFraction=0.3 \
   --conf spark.shuffle.file.buffer=64k  \
   --conf spark.shuffle.consolidateFiles=true \
   --conf spark.shuffle.io.maxRetries=10 \
   --conf spark.shuffle.io.retryWait=30 \
   --conf spark.locality.wait=10s \
   --conf spark.akka.timeout=150s \
   --conf spark.network.timeout=200s\
   --conf spark.speculation=false \
   --conf spark.broadcast.blockSize=4m \
   --conf spark.files.fetchTimeout=360 \
   --conf spark.network.timeout=360 \
   --conf spark.cleaner.ttl=240000 \
   --conf spark.kryoserializer.buffer.mb=10 \
   --conf spark.reducer.maxSizeInFligth=12 \
   --conf spark.sql.codegen=true \
   --conf spark.sql.parquet.compression.codec=snappy \
   --conf spark.sql.adaptive.enabled=true \
   --conf spark.sql.broadcastTimeout=600 \
   --conf spark.sql.autoBroadcastJoinThreshold=10485760 \
   --conf spark.sql.inMemoryColumnarStorage.batchSize=10000 \
   --conf spark.sql.files.maxPartitionBytes=134217728 \
   --conf spark.sql.files.openCostInBytes=5242880  \
   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseCompressedStrings" \
   --class .ifrs17.mp.launcher.MPIndExecutor \
   lbdp_corepub_ifrs17_job-1.0.0.jar \
   env=prd  \
   log.level=error \
   p_proc_date=`getparam p_proc_date` \
   src_version_num=`getparam src_version_num` \
   res_version_num=`getparam res_version_num` \
   res_hive_db=`getparam res_hive_db`

exitCodeCheck $?
#!/bin/sh
source ExitCodeCheck.sh

if [ $# != 1 ] ; then
echo "USAGE:$0 SQL"
exit 1;
fi



opts=$@

getparam(){
arg=$1
echo $opts |xargs -n1 |cut -b 2- |awk -F'=' '{if($1=="'"$arg"'") print $2}'
}

#./SurplusSummaryJob.sh -hdp_queue=queue_1514_01

IncStart=`getparam inc_start`
IncEnd=`getparam inc_end`
oracle_connection=`getparam jdbc_str`
oracle_username=`getparam db_user`
oracle_password=`getparam db_psw`
dataName=`getparam db_sid`
queueName='root.queue_sx80.1508_11'
hdfshostname=`getparam hdfs_host`;


#Job名称
job_name=$0

#下载程序包和配置文件
#rm -rf ${job_name}.jar;
#rm -rf ${job_name};
#mkdir ${job_name};
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/jar/${job_name}.jar .;
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/lib/* ./${job_name};



#任务名取脚本名
#job_name=$0

#JOB_HOME=/appcom/apps/hduser1508/zhangyongtian001/sparkjob
JOB_HOME=`pwd`
cd $JOB_HOME


/appcom/spark/bin/spark-submit  \
--jars $JOB_HOME/lib/dom4j-1.6.1.jar \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 10G \
  --num-executors 15 \
  --executor-cores 5 \
  --driver-memory 500M \
  --queue ${queueName} \
   --conf spark.default.parallelism=2001 \
   --conf spark.yarn.executor.memoryoverhead=4G \
   --conf spark.storage.memoryFraction=0.6 \
   --conf spark.shuffle.memoryFraction=0.3 \
   --conf spark.shuffle.file.buffer=64k  \
   --conf spark.shuffle.consolidateFiles=true \
   --conf spark.shuffle.io.maxRetries=10 \
   --conf spark.shuffle.io.retryWait=30 \
   --conf spark.locality.wait=10s \
   --conf spark.akka.timeout=150s \
   --conf spark.network.timeout=200s\
   --conf spark.speculation=true \
   --conf spark.broadcast.blockSize=4m \
   --conf spark.files.fetchTimeout=360 \
   --conf spark.network.timeout=360 \
   --conf spark.cleaner.ttl=240000 \
   --conf spark.kryoserializer.buffer.mb=10 \
   --conf spark.reducer.maxSizeInFligth=12 \
   --conf spark.sql.shuffle.partitions=2001 \
   --conf spark.sql.codegen=true \
   --conf spark.sql.parquet.compression.codec=snappy \
   --conf spark.sql.adaptive.enabled=true \
   --conf spark.sql.broadcastTimeout=600 \
   --conf spark.sql.autoBroadcastJoinThreshold=10485760 \
   --conf spark.sql.inMemoryColumnarStorage.batchSize=10000 \
   --conf spark.sql.files.maxPartitionBytes=134217728 \
   --conf spark.sql.files.openCostInBytes=5242880  \
   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseCompressedStrings" \
   --class .ifrs17.tool.SparkSQLExecute \
   lbdp_corepub_ifrs17_job-1.0.0.jar \
   prd  \
   error \
   'sx_core_safe' \
   "$1"

exitCodeCheck $?
#!/bin/sh
source ExitCodeCheck.sh

opts=$@

getparam(){
arg=$1
echo $opts |xargs -n1 |cut -b 2- |awk -F'=' '{if($1=="'"$arg"'") print $2}'
}

#./SurplusSummaryJob.sh -hdp_queue=queue_1514_01

IncStart=`getparam inc_start`
IncEnd=`getparam inc_end`
oracle_connection=`getparam jdbc_str`
oracle_username=`getparam db_user`
oracle_password=`getparam db_psw`
dataName=`getparam db_sid`
queueName='root.queue_sx80.1508_11'
hdfshostname=`getparam hdfs_host`;


#Job名称
job_name=$0

#下载程序包和配置文件
#rm -rf ${job_name}.jar;
#rm -rf ${job_name};
#mkdir ${job_name};
#hadoop fs -get /apps/hduser1514/sx_hx_safe/lrs/jar/${job_name}.jar .;
#hadoop fs -get /apps/hduser1514/sx_hx_safe/lrs/lib/* ./${job_name};

#rm -rf lbdp_corepub_ifrs17_job-1.0.0.jar;
#rm -rf ${job_name};
#mkdir ${job_name};
#hadoop fs -get /apps/hduser1514/sx_hx_safe/lrs/jar/${job_name}.jar .;
#hadoop fs -get /apps/hduser1514/sx_hx_safe/lrs/lib/* ./${job_name};

#hadoop fs -get  /apps/hduser1508/sx_hx_safe/ifrs17/spark/lbdp_corepub_ifrs17_job-1.0.0.jar  .;

##任务名取脚本名
#job_name=$0

/appcom/spark/bin/spark-submit  \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 20G \
  --num-executors 20 \
  --executor-cores 4 \
  --driver-memory 2G \
  --queue ${queueName} \
   --conf spark.default.parallelism=200 \
   --conf spark.yarn.executor.memoryoverhead=8G \
   --conf spark.storage.memoryFraction=0.6 \
   --conf spark.shuffle.memoryFraction=0.3 \
   --conf spark.shuffle.file.buffer=64k  \
   --conf spark.shuffle.consolidateFiles=true \
   --conf spark.shuffle.io.maxRetries=10 \
   --conf spark.shuffle.io.retryWait=30 \
   --conf spark.locality.wait=10s \
   --conf spark.akka.timeout=150s \
   --conf spark.network.timeout=200s\
   --conf spark.speculation=false \
   --conf spark.broadcast.blockSize=4m \
   --conf spark.files.fetchTimeout=360 \
   --conf spark.network.timeout=360 \
   --conf spark.cleaner.ttl=240000 \
   --conf spark.kryoserializer.buffer.mb=10 \
   --conf spark.reducer.maxSizeInFligth=12 \
   --conf spark.sql.shuffle.partitions=200 \
   --conf spark.sql.codegen=true \
   --conf spark.sql.parquet.compression.codec=snappy \
   --conf spark.sql.adaptive.enabled=true \
   --conf spark.sql.broadcastTimeout=600 \
   --conf spark.sql.autoBroadcastJoinThreshold=10485760 \
   --conf spark.sql.inMemoryColumnarStorage.batchSize=10000 \
   --conf spark.sql.files.maxPartitionBytes=134217728 \
   --conf spark.sql.files.openCostInBytes=5242880  \
   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseCompressedStrings" \
   --class .ifrs17.dcs.launcher.DcsIndExecutor \
   lbdp_corepub_ifrs17_job-1.0.0.jar  \
   env=prd  \
   log.level=error \
   PROC_DATE=20181231 \
   SPCODE_SELECTION=1 \
   src_version_num=1550036477173 \
   res_version_num=20181231 \
   res_hive_db=sx_hx_safe \
   out.hdfs.path=/apps-data/hduser1508/sx_core_safe/lar_dcs_result_ind

exitCodeCheck $?
#!/bin/sh
source ExitCodeCheck.sh

opts=$@

getparam(){
arg=$1
echo $opts |xargs -n1 |cut -b 2- |awk -F'=' '{if($1=="'"$arg"'") print $2}'
}

#./SurplusSummaryJob.sh -hdp_queue=queue_1514_01

IncStart=`getparam inc_start`
IncEnd=`getparam inc_end`
oracle_connection=`getparam jdbc_str`
oracle_username=`getparam db_user`
oracle_password=`getparam db_psw`
dataName=`getparam db_sid`
queueName=`getparam hdp_queue`
hdfshostname=`getparam hdfs_host`;


#Job名称
job_name=$0

#下载程序包和配置文件
#rm -rf ${job_name}.jar;
#rm -rf ${job_name};
#mkdir ${job_name};
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/jar/${job_name}.jar .;
#hadoop fs -get /apps/hduser1514/sx_core_safe/lrs/lib/* ./${job_name};



#任务名取脚本名
#job_name=$0

JOB_HOME=/appcom/apps/hduser1508/zhangyongtian001/sparkjob
cd $JOB_HOME


/appcom/spark/bin/spark-submit  \
--jars $JOB_HOME/lib/dom4j-1.6.1.jar \
  --master yarn \
  --deploy-mode cluster \
  --executor-memory 10G \
  --num-executors 15 \
  --executor-cores 5 \
  --driver-memory 500M \
  --queue ${queueName} \
   --conf spark.default.parallelism=100 \
   --conf spark.yarn.executor.memoryoverhead=4G \
   --conf spark.storage.memoryFraction=0.6 \
   --conf spark.shuffle.memoryFraction=0.3 \
   --conf spark.shuffle.file.buffer=64k  \
   --conf spark.shuffle.consolidateFiles=true \
   --conf spark.shuffle.io.maxRetries=10 \
   --conf spark.shuffle.io.retryWait=30 \
   --conf spark.locality.wait=10s \
   --conf spark.akka.timeout=150s \
   --conf spark.network.timeout=200s\
   --conf spark.speculation=true \
   --conf spark.broadcast.blockSize=4m \
   --conf spark.files.fetchTimeout=360 \
   --conf spark.network.timeout=360 \
   --conf spark.cleaner.ttl=240000 \
   --conf spark.kryoserializer.buffer.mb=10 \
   --conf spark.reducer.maxSizeInFligth=12 \
   --conf spark.sql.shuffle.partitions=100 \
   --conf spark.sql.codegen=true \
   --conf spark.sql.parquet.compression.codec=snappy \
   --conf spark.sql.adaptive.enabled=true \
   --conf spark.sql.broadcastTimeout=600 \
   --conf spark.sql.autoBroadcastJoinThreshold=10485760 \
   --conf spark.sql.inMemoryColumnarStorage.batchSize=10000 \
   --conf spark.sql.files.maxPartitionBytes=134217728 \
   --conf spark.sql.files.openCostInBytes=5242880  \
   --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+UseCompressedStrings" \
   --class .ifrs17.mp.job.SqlConfJobTest \
   lbdp_corepub_ifrs17_1_0_0_demo-1.0.0.jar \
   env=prd  \
   log.level=error \
   VAL_DATE='2018-08-31 00:00:00' \
   PROC_DATE='201' \
   SPCODE_SELECTION=1 \
    out.hdfs.path=/apps-data/hduser1508/sx_core_safe/lar_dcs_result_ind

exitCodeCheck $?
#!/bin/sh

sql=$(cat <<!EOF

    WITH is_code AS
(SELECT plan_code,
       max(CI_RIDER_PRIMARY_END) CI_RIDER_PRIMARY_END,
       max(CI_RIDER_PAY_VALID) CI_RIDER_PAY_VALID,
       max(PRIMARY_CI_RIDER1) PRIMARY_CI_RIDER1,
       max(CI_RIDER1) CI_RIDER1,
       max(PA_RUN_FLAG) PA_RUN_FLAG,
       max(CI_RIDER_PAYMENT_CODE) CI_RIDER_PAYMENT_CODE,
       MAX(LIFE_PRIMARY) LIFE_PRIMARY,
       MAX(CI_RIDER) CI_RIDER
   FROM (
      SELECT plan_code,
             CASE WHEN plan_location='CI_RIDER_PRIMARY_END' then 1 ELSE 0 END CI_RIDER_PRIMARY_END,
             CASE WHEN plan_location = 'CI_RIDER_PAY_VALID' THEN 1 ELSE 0 END CI_RIDER_PAY_VALID,
             CASE WHEN plan_location = 'PRIMARY_CI_RIDER1' THEN 1 ELSE 0 END PRIMARY_CI_RIDER1,
             CASE WHEN plan_location = 'CI_RIDER1' THEN 1 ELSE 0 END CI_RIDER1,
             CASE WHEN plan_location = 'PA_RUN_FLAG' THEN 1 ELSE 0 END PA_RUN_FLAG,
             CASE WHEN plan_location = 'CI_RIDER_PAYMENT_CODE' THEN 1 ELSE 0 END CI_RIDER_PAYMENT_CODE,
             CASE WHEN plan_location = 'LIFE_PRIMARY' THEN 1 ELSE 0 END LIFE_PRIMARY,
             CASE WHEN plan_location = 'CI_RIDER' THEN 1 ELSE 0 END CI_RIDER
        FROM bas_pala_pol_ben_plan_cfg t
       WHERE folder_in = 'POL_BEN_MAIN'
         AND file_in =  'EXEC783'
         AND t.behavior = 'IN') haha
       GROUP BY  plan_code),
--v_plan_code_ci15.EXISTS(arr_pol_ben(idx).plan_code)
ci15 AS(
  SELECT polno FROM bas_lbs_pol_ben_actuary a
  INNER JOIN is_code ic
  ON a.plan_code=ic.plan_code AND ic.PRIMARY_CI_RIDER1=1
  GROUP BY polno
),
--SELECT count(*) INTO v_count_ci15
CI_RIDER1 AS(
  SELECT a.polno,COUNT(1) v_count_ci15 FROM bas_lbs_pol_ben_actuary a
  INNER JOIN ci15
  ON a.polno=ci15.polno
  INNER JOIN is_code ic
  ON a.plan_code=ic.plan_code AND ic.CI_RIDER1=1
  WHERE  a.ben_sts IN ('I', 'P', 'L', 'V', 'R', 'W', 'E', 'A', 'J', 'H', 'G', 'K', 'X')
  AND eff_date < DATE'2018-9-1'
  GROUP BY a.polno
),
c_pol_ben2 AS(
SELECT  DISTINCT polno
        FROM bas_lbs_pol_ben_actuary a
        INNER JOIN is_code ic
        ON a.plan_code=ic.plan_code
       WHERE /*plan_code IN (SELECT plan_code
                             FROM bas_pala_pol_ben_plan_cfg
                            WHERE folder_in = 'POL_BEN_MAIN'
                              AND file_in = 'EXEC783'
                              AND plan_location = 'CI_RIDER'
                              AND behavior = 'IN')*/
         ic.CI_RIDER=1
         AND ben_sts IN
             ('I', 'P', 'L', 'V', 'R', 'W', 'E', 'A', 'J', 'H', 'G', 'K', 'X')
         AND eff_date < DATE'2018-9-1'
)

SELECT  polno,
        brno,
        plan_type,
        plan_code,
        eff_date,
        ben_sts,
        units,
        sum_ins,
        matu_date,
        ins_sex,
        ins_age,
        ins_prof,
        jnt_sex,
        jnt_age,
        jnt_prof,
        period,
        prem_term,
        prem_type,
        ann_std_prem,
        ann_substd_prem,
        ann_prof_prem,
        std_modal_prem,
        tot_modal_prem,
        pay_to_date,
        payout_age,
        payout_date,
        ben_level,
        policy_fee,
        0 et,
        deptno,
        sum_ins sum_ins_org,
        0 ci_rider,
        sum_ins sum_ins_m,
        0 sum_ins_r,
        ann_std_prem ann_std_prem_m,
        0 ann_std_prem_r,
        std_modal_prem std_modal_prem_m,
        0 std_modal_prem_r,
        plan_code plan_code_m,
        0 ann_std_prem_916,
        0 std_modal_prem_916,
        0 ann_std_prem_518,
        0 std_modal_prem_518,
        region_code
FROM (
SELECT  a.polno,
        a.brno,
        a.plan_type,
        a.plan_code,
        a.eff_date,
        a.ben_sts,
        a.units,
        a.sum_ins,
        a.matu_date,
        a.ins_sex,
        a.ins_age,
        a.ins_prof,
        a.jnt_sex,
        a.jnt_age,
        a.jnt_prof,
        a.period,
        a.prem_term,
        a.prem_type,
        a.ann_std_prem,
        a.ann_substd_prem,
        a.ann_prof_prem,
        a.std_modal_prem,
        a.tot_modal_prem,
        a.pay_to_date,
        a.payout_age,
        a.payout_date,
        a.ben_level,
        a.deptno,
        a.region_code,
        ic.PA_RUN_FLAG,
        CI_RIDER1.v_count_ci15,
        b2.polno b2_polno,
        -- 代码行2110 代码转换
        CASE WHEN ic.PA_RUN_FLAG=1 THEN (CASE WHEN a.payout_type='H' THEN 2
                                              WHEN a.payout_type='R' THEN 3 ELSE 0 END) ELSE policy_fee END policy_fee
        FROM bas_lbs_pol_ben_actuary a
        LEFT OUTER JOIN is_code ic
        ON a.plan_code=ic.plan_code
        LEFT OUTER JOIN CI_RIDER1
        ON a.polno=CI_RIDER1.polno
        LEFT OUTER JOIN c_pol_ben2 b2
        ON a.polno=b2.polno
       WHERE /*a.plan_code IN (SELECT plan_code
                             FROM bas_pala_pol_ben_plan_cfg b
                            WHERE folder_in = 'POL_BEN_MAIN'
                              AND file_in = 'EXEC783'
                              AND plan_location = 'LIFE_PRIMARY'
                              AND behavior = 'IN')*/

            ic.LIFE_PRIMARY=1
         AND ben_sts IN
             ('I', 'P', 'L', 'V', 'R', 'W', 'E', 'A', 'J', 'H', 'G', 'K')
         AND period >= 1
         AND eff_date < DATE'2018-9-1' ) hehe
         WHERE b2_polno IS NOT NULL

!EOF)

sparksql.sh "$sql"

