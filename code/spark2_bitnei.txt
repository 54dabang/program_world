state.window.internal=180

state.window.charge.begin.speed.upper=50
state.window.charge.length=10
state.window.charge.begin.length=10
state.window.charge.end.length=25

state.window.travel.length=10
state.window.travel.begin.length=10

state.window.fullcharge.length=1
state.window.fullcharge.begin.length=1
state.window.fullcharge.end.length=10

#state.table.charge.dayreport=veh_dayreport_chargestate
#state.table.fullcharge.dayreport=VEH_DAYREPORT_FULLSTATE
#state.table.travel.dayreport=VEH_DAYREPORT_RUNSTATE
#state.table.category.dayreport=veh_dayreport_category
#state.table.alarm.report=veh_dayreport_alarm
#state.table.validate.report=veh_dayreport_datastat

#single_vehicle_charge_run_detail_table=temp_veh_day_single_charge_run
#single_vehicle_run_detail_table=temp_veh_day_single_run
#single_vehicle_charge_detail_table=temp_veh_day_single_charge
#state.table.charge.dayreport=TEMP_veh_dayreport_chargestate
#state.table.fullcharge.dayreport=TEMP_VEH_DAYREPORT_FULLSTATE
#state.table.travel.dayreport=TEMP_VEH_DAYREPORT_RUNSTATE
#state.table.category.dayreport=TEMP_veh_dayreport_category
#state.table.charge=TEMP_veh_charge
#state.table.fullcharge=TEMP_veh_fullcharge
#state.table.travel=TEMP_veh_run
#state.table.alarm.report=TEMP_veh_dayreport_alarm
#state.table.validate.report=veh_dayreport_datastat


srcRealinfo.table.directory=/vehicle/data/realinfo
srcRealinfo.table.partitionColumn=Date
srcRealinfo.table.format=text
srcRealinfo.table.saveMode=append
srcRealinfo.table.schemaFile=/usr/local/sparkjob/bin/parquet/conf/realinfo.schema


srcForward.table.directory=/vehicle/data/forward
srcForward.table.partitionColumn=Date
srcForward.table.format=text
srcForward.table.saveMode=append
srcForward.table.schemaFile=/usr/local/sparkjob/bin/parquet/conf/forward.schema


srcLogin.table.directory=/vehicle/data/login
srcLogin.table.partitionColumn=Date
srcLogin.table.format=text
srcLogin.table.saveMode=append
srcLogin.table.schemaFile=/usr/local/sparkjob/bin/parquet/conf/login.schema

srcAlarm.table.directory=/vehicle/data/alarm
srcAlarm.table.partitionColumn=Date
srcAlarm.table.format=text
srcAlarm.table.saveMode=append
srcAlarm.table.schemaFile=/usr/local/sparkjob/bin/parquet/conf/alarm.schema

srcTermstatus.table.directory=/vehicle/data/termstatus
srcTermstatus.table.partitionColumn=Date
srcTermstatus.table.format=text
srcTermstatus.table.saveMode=append
srcTermstatus.table.schemaFile=/usr/local/sparkjob/bin/parquet/conf/termstatus.schema


realinfo.table.directory=/spark/vehicle/data/realinfo
realinfo.table.partitionColumn=year-month-day
realinfo.table.format=parquet
realinfo.table.saveMode=overwrite


forward.table.directory=/spark/vehicle/data/forward
forward.table.partitionColumn=year-month-day
forward.table.format=parquet
forward.table.saveMode=overwrite

login.table.directory=/spark/vehicle/data/login
login.table.partitionColumn=year-month-day
login.table.format=parquet
login.table.saveMode=overwrite


alarm.table.directory=/spark/vehicle/data/alarm
alarm.table.partitionColumn=year-month-day
alarm.table.format=parquet
alarm.table.saveMode=overwrite


termstatus.table.directory=/spark/vehicle/data/termstatus
termstatus.table.partitionColumn=year-month-day
termstatus.table.format=parquet
termstatus.table.saveMode=overwrite

detail.table.directory=/spark/vehicle/result/detail
detail.table.partitionColumn=year-month-day
detail.table.partitionNum=3
detail.table.format=parquet
detail.table.saveMode=overwrite

chargeRunDetail.table.directory=/spark/vehicle/result/chargeRunDetail
chargeRunDetail.table.partitionColumn=year-month-day
chargeRunDetail.table.partitionNum=3
chargeRunDetail.table.format=parquet
chargeRunDetail.table.saveMode=overwrite


dayreport.table.directory=/spark/vehicle/result/dayreport
dayreport.table.partitionColumn=year-month-day
dayreport.table.partitionNum=1
dayreport.table.format=parquet
dayreport.table.saveMode=overwrite

categoryDayReport.table.directory=/spark/vehicle/result/categoryDayReport
categoryDayReport.table.partitionColumn=year-month-day
categoryDayReport.table.partitionNum=3
categoryDayReport.table.format=parquet
categoryDayReport.table.saveMode=overwrite

monthreport.table.directory=/spark/vehicle/result/monthreport
monthreport.table.partitionColumn=year-month
monthreport.table.partitionNum=3
monthreport.table.format=parquet
monthreport.table.saveMode=overwrite

taxisweek.table.directory=/spark/vehicle/result/taxisweek
taxisweek.table.partitionColumn=year-month-day
taxisweek.table.format=parquet
taxisweek.table.saveMode=overwrite

taxismonth.table.directory=/spark/vehicle/result/taxismonth
taxismonth.table.partitionColumn=year-month
taxismonth.table.format=parquet
taxismonth.table.saveMode=overwrite

taxisyear.table.directory=/spark/vehicle/result/taxisyear
taxisyear.table.partitionColumn=year
taxisyear.table.partitionNum=1
taxisyear.table.format=parquet
taxisyear.table.saveMode=overwrite

validate.table.directory=/spark/vehicle/result/validate
validate.table.partitionColumn=year-month-day
validate.table.partitionNum=3
validate.table.format=parquet
validate.table.saveMode=overwrite

alarmReport.table.directory=/spark/vehicle/result/alarm
alarmReport.table.partitionColumn=year-month-day
alarmReport.table.partitionNum=3
alarmReport.table.format=parquet
alarmReport.table.saveMode=overwrite


deadlineReport.table.directory=/spark/vehicle/result/deadlineReport
deadlineReport.table.partitionColumn=year-month
deadlineReport.table.partitionNum=3
deadlineReport.table.format=parquet
deadlineReport.table.saveMode=overwrite


cellVoltageConsist.table.directory=/spark/vehicle/result/cellVoltageConsist
cellVoltageConsist.table.partitionColumn=year-month
cellVoltageConsist.table.partitionNum=3
cellVoltageConsist.table.format=parquet
cellVoltageConsist.table.saveMode=overwrite


operationIndex.table.directory=/spark/vehicle/result/operationIndex
operationIndex.table.partitionColumn=year-month
operationIndex.table.partitionNum=3
operationIndex.table.format=parquet
operationIndex.table.saveMode=overwrite

operationIndexMileage.table.directory=/spark/vehicle/result/operationIndexMileage
operationIndexMileage.table.partitionColumn=year-month-day
operationIndexMileage.table.partitionNum=3
operationIndexMileage.table.format=parquet
operationIndexMileage.table.saveMode=overwrite

operationIndexDetail.table.directory=/spark/vehicle/result/operationIndexDetail
operationIndexDetail.table.partitionColumn=year-month-day
operationIndexDetail.table.partitionNum=3
operationIndexDetail.table.format=parquet
operationIndexDetail.table.saveMode=overwrite

mileageCheckDayCoord.table.directory=/spark/vehicle/result/mileageCheckDayCoord
mileageCheckDayCoord.table.partitionColumn=year-month-day
mileageCheckDayCoord.table.partitionNum=3
mileageCheckDayCoord.table.format=parquet
mileageCheckDayCoord.table.saveMode=overwrite


mileageCheckDeadline.table.directory=/spark/vehicle/result/mileageCheckDeadline
mileageCheckDeadline.table.partitionColumn=year-month
mileageCheckDeadline.table.partitionNum=3
mileageCheckDeadline.table.format=parquet
mileageCheckDeadline.table.saveMode=overwrite



login_result.table.directory=/tmp/spark/vehicle/result/login_result
login_result.table.partitionColumn=year-month-day
login_result.table.partitionNum=3
login_result.table.format=parquet
login_result.table.saveMode=overwrite

term_result.table.directory=/tmp/spark/vehicle/result/term_result
term_result.table.partitionColumn=year-month-day
term_result.table.partitionNum=3
term_result.table.format=parquet
term_result.table.saveMode=overwrite

forward_result.table.directory=/tmp/spark/vehicle/result/forward_result
forward_result.table.partitionColumn=year-month-day
forward_result.table.partitionNum=3
forward_result.table.format=parquet
forward_result.table.saveMode=overwrite

realinfoValidate.table.directory=/tmp/spark/vehicle/result/realinfoValidate
realinfoValidate.table.partitionColumn=year-month-day
realinfoValidate.table.partitionNum=3
realinfoValidate.table.format=parquet
realinfoValidate.table.saveMode=overwrite


runheat.table.directory=/spark/vehicle/result/runheat
runheat.table.partitionColumn=year-month-day
runheat.table.partitionNum=3
runheat.table.format=parquet
runheat.table.saveMode=overwrite

#databse connection pool
jdbc.pool.initsize=10
jdbc.pool.active.max=50
jdbc.pool.idel.max=10
jdbc.pool.wait.max=100

#database connection config
jdbc.url=jdbc:mysql://rm-bp191mp5q81b9a9wg.mysql.rds.aliyuncs.com:3306/jldb（10.86.160.111）
jdbc.driver=com.mysql.jdbc.Driver
jdbc.username=jlswc
jdbc.passwd=Jlswc))1

#hbase connection config
hbase.quorum=10.86.160.112,10.86.160.113,10.86.160.114
hbase.zkport=2181

#redis connection config
redis.address=192.168.6.106:1001

#lease repor
temp.taxis.report.conf=/usr/local/sparkjob/conf/taxistable

#detail report
detail.output=hdfs,mysql

#dayreport
report.output=hdfs,mysql

operation.zookeeper=192.168.2.70:2181
operation.group=operation_group1
operation.topic=operation_query
operation.cmd=/usr/local/sparkjob/bin/operation/operation_job.sh
operation.parent=/usr/local/sparkjob/bin/operation
operation.model=run
operation.offset=smallest
operation.numThreads=4

realinfo.schema=VID,VIN,TIME,MESSAGETYPE,ISFILTER,2210,2912,2608,VTYPE,2609,2606,2607,2002,2604,2003,2605,2602,2603,2601,2910,2911,2505,2209,2208,2103,2101,2102,2203,2504,2202,2503,2201,2502,2501,2901,2902,2903,2904,2801,2905,2615,2906,2616,2907,2617,2908,2611,2612,2808,2613,2614,2802,2804,2610,10002,10005,10003,10004,2909,2303,2304,2305,2306,2301,2302,2001,2000

1 2 * * * /usr/local/sparkjob/bin/start_all_job.sh > /usr/local/sparkjob/bin/start_all_job.log 2>&1

#!/usr/bin/env bash
export SPARK_REPORT_PATH=/usr/local/sparkjob
export SPARK_REPORT_EXPORT_JAR_DIRECTORY=${SPARK_REPORT_PATH}/jars
export SPARK_REPORT_CONF=$SPARK_REPORT_PATH/conf/conf.properties
export SPARK_REPORT_LOG=$SPARK_REPORT_PATH/log


export SPARK_REPORT_EXPORT_JARS=$(${SPARK_REPORT_PATH}/bin/getExportJars.sh)
export SPARK_REPORT_PARQUET_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-parquet*.jar)
export SPARK_REPORT_DAY_DETAIL_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-detail*.jar)
export SPARK_REPORT_DAY_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_DAY_MONTH_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_TAXIS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-taxis*.jar)
export SPARK_REPORT_OPERATION_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-operation*.jar)
export SPARK_REPORT_MILEAGE_CHECK_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-mileage-check*.jar)

export RUNHEAT_REPORT_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-run-heat*.jar)
export SPARK_REPORT_PARQUET_PATH=${SPARK_REPORT_PATH}/bin/parquet
export SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob
export SPARK_REPORT_DAYREPORT_PATH=${SPARK_REPORT_PATH}/bin/dayreport
export SPARK_REPORT_OPERATION_INDEX=${SPARK_REPORT_PATH}/bin/operation


SPARK_REPORT_DAYRPORT_PATH=${SPARK_REPORT_PATH}/bin/dayreport


startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi


jobtype=$3

echo "$startDate,$endDate"


function startJob(){
  startDate=$1
  endDate=$2
  jobType=$3

  if [ "$jobType" == "day_category" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/category.sh $startDate $endDate
  elif [ "$jobType" == "day_category_output" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/category_output.sh $startDate $endDate
  elif [ "$jobType" == "dayreport"] ;
  then
    source  ${SPARK_REPORT_DAYRPORT_PATH}/realinfo.sh $startDate $endDate
    source ${SPARK_REPORT_DAYRPORT_PATH}/category.sh $startDate $endDate
  elif [ "$jobType" == "dayreport_output" ] ;
  then
    source ${SPARK_REPORT_DAYRPORT_PATH}/realinfo_output.sh $startDate $endDate
    source ${SPARK_REPORT_DAYRPORT_PATH}/category_output.sh $startDate $endDate
  else
    source ${SPARK_REPORT_DAYRPORT_PATH}/realinfo.sh $startDate $endDate
    source ${SPARK_REPORT_DAYRPORT_PATH}/category.sh $startDate $endDate
    source ${SPARK_REPORT_DAYRPORT_PATH}/realinfo_output.sh $startDate $endDate
    source ${SPARK_REPORT_DAYRPORT_PATH}/category_output.sh $startDate $endDate
  fi
}

startJob $startDate $endDate $jobtype

----------------------------------------
realinfo.sh

#!/usr/bin/env bash


startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"


startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.dayreport.realinfo.RealinfoJob \
      --name ${inputDate}-spark日报表计算 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 1G \
      --executor-cores 1 \
      --num-executors 1 \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS} \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
      ${SPARK_REPORT_DAY_STATISTICS_JAR} \
      detail.table.partitionValue=$partitionValue \
      dayreport.table.partitionValue=$partitionValue
}



function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')
    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}


batchProcess $startDate $endDate



----------------------------------------
category.sh

startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.dayreport.realinfo.CatagoryJob \
      --name ${inputDate}-spark分类报表计算 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 1G \
      --executor-cores 1 \
      --num-executors 1 \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS} \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
      ${SPARK_REPORT_DAY_STATISTICS_JAR} \
      report.output=hdfs \
      dayreport.table.partitionValue=$partitionValue \
      categoryDayReport.table.partitionValue=$partitionValue
}



function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')
    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"

batchProcess $startDate $endDate


=============================
realinfo_output.sh

#!/usr/bin/env bash
startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.dayreport.realinfo.RealinfoOutputJob \
      --name ${inputDate}-spark日报表计算 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 2G \
      --executor-cores 2 \
      --num-executors 2 \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS} \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
      --conf spark.sql.shuffle.partitions=4 \
      ${SPARK_REPORT_DAY_STATISTICS_JAR} \
      dayreport.table.partitionValue=$partitionValue \
      input.table.name=dayreport \
      report.date=$curDate-$curDate \
      report.output=mysql \
      database=mysql \
      finalPartitionNum=4
}



function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')
    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"


batchProcess $startDate $endDate


----------------------------------------------------------------

category_output.sh

#!/usr/bin/env bash
startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.dayreport.realinfo.CategoryOutputJob \
      --name ${inputDate}-spark分类日报表输出 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 1G \
      --executor-cores 1 \
      --num-executors 1 \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS} \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --conf spark.shuffle.io.maxRetries=40 \
      --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
      --conf spark.sql.shuffle.partitions=4 \
      ${SPARK_REPORT_DAY_STATISTICS_JAR} \
      category.table.partitionValue=$partitionValue \
      input.table.name=categoryDayReport \
      report.date=$curDate-$curDate \
      report.output=mysql
}



function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')
    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"


batchProcess $startDate $endDate

=============












org.apache.spark.shuffle.FetchFailedException: Failed to connect to cnjlqcpdn03/10.86.160.114:30381
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:357)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:332)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.GroupedIterator$.apply(GroupedIterator.scala:29)
	at org.apache.spark.sql.execution.MapGroupsExec$$anonfun$11.apply(objects.scala:331)
	at org.apache.spark.sql.execution.MapGroupsExec$$anonfun$11.apply(objects.scala:330)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to cnjlqcpdn03/10.86.160.114:30381
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: cnjlqcpdn03/10.86.160.114:30381
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	... 2 more


生成明细的时候

org.apache.spark.shuffle.FetchFailedException: Failed to connect to cnjlqcpdn02/10.86.160.113:17559
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:357)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:332)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.GroupedIterator$.apply(GroupedIterator.scala:29)
	at org.apache.spark.sql.execution.MapGroupsExec$$anonfun$11.apply(objects.scala:331)
	at org.apache.spark.sql.execution.MapGroupsExec$$anonfun$11.apply(objects.scala:330)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to cnjlqcpdn02/10.86.160.113:17559
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:97)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	... 1 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: cnjlqcpdn02/10.86.160.113:17559
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)




#!/usr/bin/env bash
SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob

function startJob(){
  startDate=$1
  endDate=$2
  jobType=$3

  if [ "$jobType" == "detail_charge_run" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_job.sh $startDate $endDate
  elif [ "$jobType" == "detail_charge_run_output" ] ;
  then
   source ${SPARK_REPORT_DETAIL_PATH}/charge_run_output_job.sh $startDate $endDate
  elif [ "$jobType" == "detailjob" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/testdetailjob.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_job.sh $startDate $endDate
  elif [ "$jobtype" == "detail_output" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/detail_output_job.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_output_job.sh $startDate $endDate
  else
    source ${SPARK_REPORT_DETAIL_PATH}/testdetailjob.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_job.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/detail_output_job.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/charge_run_output_job.sh $startDate $endDate
  fi
}


startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

jobtype=$3

echo "$startDate,$endDate"


startJob $startDate $endDate $jobtype

=========================================
testdetailjob.sh

#!/usr/bin/env bash
export SPARK_REPORT_PATH=/usr/local/sparkjob
export SPARK_REPORT_EXPORT_JAR_DIRECTORY=${SPARK_REPORT_PATH}/jars
export SPARK_REPORT_CONF=$SPARK_REPORT_PATH/conf/conf.properties
export SPARK_REPORT_LOG=$SPARK_REPORT_PATH/log


export SPARK_REPORT_EXPORT_JARS=$(${SPARK_REPORT_PATH}/bin/getExportJars.sh)
export SPARK_REPORT_PARQUET_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-parquet*.jar)
export SPARK_REPORT_DAY_DETAIL_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-detail*.jar)
export SPARK_REPORT_DAY_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_DAY_MONTH_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_TAXIS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-taxis*.jar)
export SPARK_REPORT_OPERATION_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-operation*.jar)
export SPARK_REPORT_MILEAGE_CHECK_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-mileage-check*.jar)


export SPARK_REPORT_PARQUET_PATH=${SPARK_REPORT_PATH}/bin/parquet
export SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob
export SPARK_REPORT_DAYREPORT_PATH=${SPARK_REPORT_PATH}/bin/dayreport

startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.detail.DetailJob \
      --name ${inputDate}-spark明细报表 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 2G \
      --executor-cores 2 \
      --num-executors 5 \
      --supervise \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS},/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/hadoop-lzo.jar \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --files ${SPARK_REPORT_CONF} \
      ${SPARK_REPORT_DAY_DETAIL_JAR} \
      detail.compute=reportDetail \
      currentInvalidateTime=10Min \
      detail.table.partitionColumn=year-month-day \
      realinfo.table.partitionValue=$partitionValue \
      detail.table.partitionValue=$partitionValue  \
      OneValidateRunTime=1s
}


function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')

    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"



inputDate=$($SPARK_REPORT_PATH/bin/concatReportDate.sh $startDate $endDate)

echo "the inpute date are $inputDate"
batchProcess $startDate $endDate

============================

charge_run_job.sh

#!/usr/bin/env bash
startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.detail.ChargeRunJob \
      --name ${inputDate}-spark充电行驶报表 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 1G \
      --executor-cores 1 \
      --num-executors 1 \
      --supervise \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS},/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/hadoop-lzo.jar \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --files ${SPARK_REPORT_CONF} \
      ${SPARK_REPORT_DAY_DETAIL_JAR} \
      report.output=hdfs,mysql \
      batchProcess.enable=true \
      startDate=$curDate \
      endDate=$endDate
}


function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')

    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"


batchProcess $startDate $endDate


==================================

detail_output_job.sh

#!/usr/bin/env bash
export SPARK_REPORT_PATH=/usr/local/sparkjob
export SPARK_REPORT_EXPORT_JAR_DIRECTORY=${SPARK_REPORT_PATH}/jars
export SPARK_REPORT_CONF=$SPARK_REPORT_PATH/conf/conf.properties
export SPARK_REPORT_LOG=$SPARK_REPORT_PATH/log


export SPARK_REPORT_EXPORT_JARS=$(${SPARK_REPORT_PATH}/bin/getExportJars.sh)
export SPARK_REPORT_PARQUET_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-parquet*.jar)
export SPARK_REPORT_DAY_DETAIL_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-detail*.jar)
export SPARK_REPORT_DAY_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_DAY_MONTH_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_TAXIS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-taxis*.jar)
export SPARK_REPORT_OPERATION_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-operation*.jar)
export SPARK_REPORT_MILEAGE_CHECK_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-mileage-check*.jar)

export RUNHEAT_REPORT_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-run-heat*.jar)
export SPARK_REPORT_PARQUET_PATH=${SPARK_REPORT_PATH}/bin/parquet
export SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob
export SPARK_REPORT_DAYREPORT_PATH=${SPARK_REPORT_PATH}/bin/dayreport
export SPARK_REPORT_OPERATION_INDEX=${SPARK_REPORT_PATH}/bin/operation
startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.detail.DetailOutputJob \
      --name ${inputDate}-spark日报表计算 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 1G \
      --executor-cores 2 \
      --num-executors 1 \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS} \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --conf spark.shuffle.io.maxRetries=40 \
      --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
      ${SPARK_REPORT_DAY_DETAIL_JAR} \
      detail.table.partitionValue=$partitionValue \
      input.table.name=detail \
      report.date=$curDate-$curDate \
      report.output=mysql \
      finalPartitionNum=2
}



function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')
    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"


batchProcess $startDate $endDate

===============================

charge_run_output_job.sh

#!/usr/bin/env bash
export SPARK_REPORT_PATH=/usr/local/sparkjob
export SPARK_REPORT_EXPORT_JAR_DIRECTORY=${SPARK_REPORT_PATH}/jars
export SPARK_REPORT_CONF=$SPARK_REPORT_PATH/conf/conf.properties
export SPARK_REPORT_LOG=$SPARK_REPORT_PATH/log


export SPARK_REPORT_EXPORT_JARS=$(${SPARK_REPORT_PATH}/bin/getExportJars.sh)
export SPARK_REPORT_PARQUET_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-parquet*.jar)
export SPARK_REPORT_DAY_DETAIL_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-detail*.jar)
export SPARK_REPORT_DAY_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_DAY_MONTH_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_TAXIS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-taxis*.jar)
export SPARK_REPORT_OPERATION_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-operation*.jar)
export SPARK_REPORT_MILEAGE_CHECK_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-mileage-check*.jar)

export RUNHEAT_REPORT_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-run-heat*.jar)
export SPARK_REPORT_PARQUET_PATH=${SPARK_REPORT_PATH}/bin/parquet
export SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob
export SPARK_REPORT_DAYREPORT_PATH=${SPARK_REPORT_PATH}/bin/dayreport
export SPARK_REPORT_OPERATION_INDEX=${SPARK_REPORT_PATH}/bin/operation
startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.detail.RunChargeOutputJob \
      --name ${inputDate}-spark日报表计算 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 1G \
      --executor-cores 1 \
      --num-executors 1 \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS} \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --conf spark.shuffle.io.maxRetries=40 \
      --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" \
       --conf spark.default.parallelism=10 \
      ${SPARK_REPORT_DAY_DETAIL_JAR} \
      finalPartitionNum=3 \
      chargeRun.table.partitionValue=$partitionValue \
      input.table.name=chargeRunDetail \
      report.date=$curDate-$curDate \
      report.output=mysql
}



function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')
    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"

batchProcess $startDate $endDate












#!/usr/bin/env bash
export_jar=" "

if [ -z "$i{SPARK_REPORT_EXPORT_JARS}" ]
 then
  export_jar=$SPARK_REPORT_EXPORT_JARS
 else
  #echo "SPARK_REPORT_EXPORT_JARS not exists."
  export_jar=$(exec ${SPARK_REPORT_PATH}/conf/spark_job_env.sh)
fi
export_jar="$export_jar,/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/hadoop-lzo.jar"
export SPARK_REPORT_EXPORT_JARS=$export_jar
echo "$export_jar"


=========
spark_job_env.sh
getExportJars () {
  export_jar=" "
  #echo "the input jar directory is $1"
  for f in `ls $1/*.jar` ; do
    if [ "$export_jar" == " " ]
     then export_jar=$f
     else export_jar="$export_jar,$f"
    fi
  done
  echo "$export_jar"
}

export SPARK_REPORT_EXPORT_JARS=$( getExportJars $SPARK_REPORT_EXPORT_JAR_DIRECTORY )


mysql -h 10.86.160.111 -P 3306 -u jlswc -p
Jlswc))1

===========

mysql> use jldb;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+--------------------------------+
| Tables_in_jldb                 |
+--------------------------------+
| base_area                      |
| base_dict                      |
| base_dict_item                 |
| base_log                       |
| base_resource                  |
| base_role                      |
| base_role_resource             |
| base_unit                      |
| base_user                      |
| base_user_role                 |
| charge_detail                  |
| clean_user                     |
| clean_vehicle                  |
| full_charge_detail             |
| jobserver                      |
| lease_charge_station           |
| lease_charging_gun             |
| lease_charginggun_type         |
| lease_company                  |
| lease_station_vehicle          |
| message_log                    |
| mlog$_sys_model_bat_lk         |
| mlog$_sys_model_battery        |
| run_detail                     |
| runstate                       |
| shenlong_veh_month_report      |
| sys_af_dbc                     |
| sys_af_dbc_bak                 |
| sys_af_parameter_model         |
| sys_af_parameter_name          |
| sys_af_parameter_unit          |
| sys_after_knowledge            |
| sys_alarm_push_log             |
| sys_area                       |
| sys_attention_veh              |
| sys_bat_contain_lk             |
| sys_battery_fuel               |
| sys_battery_fuel_version       |
| sys_battery_group              |
| sys_battery_group_box_lk       |
| sys_battery_group_version      |
| sys_battery_manage             |
| sys_battery_modular            |
| sys_battery_modular_version    |
| sys_battery_single             |
| sys_battery_single_version     |
| sys_battery_version            |
| sys_car_incident               |
| sys_car_incident_ext           |
| sys_carorder                   |
| sys_carorder_dispose_record    |
| sys_carorder_log               |
| sys_carorder_solve             |
| sys_carparts_pictureifno       |
| sys_carparts_type              |
| sys_chg_pile                   |
| sys_chg_station                |
| sys_contrast_veh               |
| sys_coordinate                 |
| sys_ctrl_template              |
| sys_custom_configuration       |
| sys_customer_unit              |
| sys_data_const                 |
| sys_data_item                  |
| sys_data_item_bak              |
| sys_data_item_history          |
| sys_data_item_lk               |
| sys_data_rule_lk               |
| sys_days_alarm_config          |
| sys_design_batch               |
| sys_dict                       |
| sys_dict2                      |
| sys_download                   |
| sys_drive_dict                 |
| sys_driving_config(old)        |
| sys_driving_config_log(old)    |
| sys_driving_rule               |
| sys_energy_manage              |
| sys_fault_alarm                |
| sys_fault_alarm_lk             |
| sys_fault_code                 |
| sys_fault_code_exception       |
| sys_fault_deal_record          |
| sys_fault_dispose              |
| sys_fault_dispose_record_info  |
| sys_fault_info                 |
| sys_fault_info_record          |
| sys_fault_info_record_log      |
| sys_fault_rank                 |
| sys_fence_alarm                |
| sys_fence_alarm_type           |
| sys_fence_alarm_type_lk        |
| sys_fence_electronic           |
| sys_fence_electronic_bak       |
| sys_fence_veh_lk               |
| sys_fourzerozero               |
| sys_fun                        |
| sys_group                      |
| sys_group_user_lk              |
| sys_group_vehicle_lk           |
| sys_group_vehicle_lk_bk        |
| sys_iccid_log                  |
| sys_import_log                 |
| sys_info_alert                 |
| sys_instruct_candata           |
| sys_instruct_info              |
| sys_instruct_send_can          |
| sys_instruct_send_rule         |
| sys_log                        |
| sys_manufacturer               |
| sys_manufacturer_product       |
| sys_member_info_unit           |
| sys_model_bat_lk               |
| sys_model_battery              |
| sys_model_mot_lk               |
| sys_model_motor                |
| sys_model_parts_lk             |
| sys_module                     |
| sys_motor_manage               |
| sys_motor_version              |
| sys_operation_dict             |
| sys_operation_docs             |
| sys_operation_filtrate         |
| sys_operation_item             |
| sys_operation_template         |
| sys_order_evaluate             |
| sys_owner_people               |
| sys_parts                      |
| sys_parts_abs                  |
| sys_parts_accessory            |
| sys_parts_acu                  |
| sys_parts_airpump              |
| sys_parts_bodyparts            |
| sys_parts_brake_system         |
| sys_parts_cecas                |
| sys_parts_chassis_parts        |
| sys_parts_cool_system          |
| sys_parts_defroster            |
| sys_parts_direction_m          |
| sys_parts_elecontrol           |
| sys_parts_glass                |
| sys_parts_rearview_mirror      |
| sys_parts_unify                |
| sys_parts_wiper                |
| sys_platform_event             |
| sys_platform_information       |
| sys_punchin_log                |
| sys_rent_point                 |
| sys_repair                     |
| sys_repair_unit_lk             |
| sys_role                       |
| sys_role_module_lk             |
| sys_rule                       |
| sys_safety_rules               |
| sys_sim_management             |
| sys_sim_management_log         |
| sys_site_disposal_plan         |
| sys_soc_realtime_vehicle       |
| sys_soc_rule                   |
| sys_soc_vehicle_log            |
| sys_store_point                |
| sys_synveh_data                |
| sys_synveh_extend_fields       |
| sys_synveh_fields              |
| sys_synveh_interface           |
| sys_synveh_platform            |
| sys_term_argc                  |
| sys_term_ctrl                  |
| sys_term_log                   |
| sys_term_model                 |
| sys_term_model_unit            |
| sys_term_model_unit_lk         |
| sys_term_param_dic             |
| sys_term_param_record          |
| sys_type_dict                  |
| sys_unit                       |
| sys_unit_forward_config        |
| sys_unit_store_point_lk        |
| sys_unit_type                  |
| sys_upgrade_log                |
| sys_uppackage_info             |
| sys_uppackage_send             |
| sys_uppackage_send_details     |
| sys_user                       |
| sys_user_role_lk               |
| sys_user_settings              |
| sys_user_vehicle_lk            |
| sys_veh_annual                 |
| sys_veh_config_num             |
| sys_veh_idle                   |
| sys_veh_info_modify_record     |
| sys_veh_maintain               |
| sys_veh_model                  |
| sys_veh_not_can                |
| sys_veh_not_position           |
| sys_veh_parts_entity_lk        |
| sys_veh_parts_lk               |
| sys_veh_upkeep                 |
| sys_vehforward_dataitem_lk     |
| sys_vehicle                    |
| sys_vehicle_batch              |
| sys_vehicle_forward            |
| sys_vehicle_log                |
| sys_vehicle_operational_data   |
| sys_vehicle_production_batch   |
| sys_vehicle_sales_data         |
| sys_version_manu_lk            |
| sys_version_unit_lk            |
| sys_waring_dispose             |
| sys_work_hour                  |
| sys_work_order                 |
| sys_worning                    |
| temp_lease_weekreport_charge   |
| temp_lease_weekreport_run      |
| temp_statistics                |
| temp_veh_charge                |
| temp_veh_dayreport_alarm       |
| temp_veh_dayreport_category    |
| temp_veh_dayreport_chargestate |
| temp_veh_dayreport_datastat    |
| temp_veh_dayreport_fullstate   |
| temp_veh_dayreport_runstate    |
| temp_veh_fullcharge            |
| temp_veh_run                   |
| test_by_test                   |
| ts_annouce                     |
| ts_appointment                 |
| ts_check_instruction           |
| ts_config                      |
| ts_dept                        |
| ts_dict                        |
| ts_docment                     |
| ts_docment_file                |
| ts_email_record                |
| ts_file                        |
| ts_flow                        |
| ts_flow_action_log             |
| ts_flow_error                  |
| ts_flow_file                   |
| ts_flow_instruction            |
| ts_inform                      |
| ts_member_info                 |
| ts_message                     |
| ts_news                        |
| ts_resource                    |
| ts_role                        |
| ts_role_resource               |
| ts_terrace_connect             |
| ts_timespan                    |
| ts_user                        |
| ts_veh_access                  |
| ts_veh_test                    |
| veh_day_single_charge          |
| veh_day_single_charge_run      |
| veh_day_single_run             |
| veh_dayreport_alarm            |
| veh_dayreport_category         |
| veh_dayreport_chargestate      |
| veh_dayreport_datastat         |
| veh_dayreport_detail           |
| veh_dayreport_fullstate        |
| veh_dayreport_lease            |
| veh_dayreport_runstate         |
| veh_driv_score                 |
| veh_energy_manage              |
| veh_last_statu                 |
| veh_monthreport_alarm          |
| veh_monthreport_category       |
| veh_monthreport_chargestate    |
| veh_monthreport_datastat       |
| veh_monthreport_fullstate      |
| veh_monthreport_lease          |
| veh_monthreport_runstate       |
| veh_offline_record             |
| veh_operation_validate         |
| veh_quarterreport_alarm        |
| veh_quarterreport_category     |
| veh_quarterreport_chargestate  |
| veh_quarterreport_datastat     |
| veh_quarterreport_fullstate    |
| veh_quarterreport_lease        |
| veh_quarterreport_runstate     |
| veh_run_heat                   |
| veh_uuid_list                  |
| veh_weekreport_alarm           |
| veh_weekreport_category        |
| veh_weekreport_chargestate     |
| veh_weekreport_datastat        |
| veh_weekreport_fullstate       |
| veh_weekreport_lease           |
| veh_weekreport_runstate        |
| veh_weekreport_statistics      |
| veh_yearreport_alarm           |
| veh_yearreport_category        |
| veh_yearreport_chargestate     |
| veh_yearreport_datastat        |
| veh_yearreport_fullstate       |
| veh_yearreport_lease           |
| veh_yearreport_runstate        |
| wx_fault_notification          |
| wx_verification_code           |
| wx_wuser                       |
| wx_wuser_car                   |
| wx_wuser_role                  |
| wyw_test04                     |
+--------------------------------+



select * from veh_dayreport_chargestate where report_time like '2018-02-28%' limit 20;

--+---------------------+---------------------+
| id    | report_time         | vid                                  | charge_time_sum | charge_times | charge_consume | charge_con_100km | charge_time_max | charge_time_avg | charge_vol_max | charge_vol_min | charge_cur_max | charge_cur_min | charge_soc_max | charge_soc_min | charge_svol_max | charge_svol_min | charge_cptemp_max | charge_cptemp_min | charge_engtemp_max | charge_engtemp_min | charge_sconsume_max | charge_sconsume_avg |



select * from veh_dayreport_runstate where report_time like '2018-02-28%' limit 20;

------------+-----------------+
| id        | report_time         | vid                                  | online_time_sum | run_time_sum | run_times | run_km | run_time_avg | run_km_avg         | run_speed_avg | run_time_max | run_time_min | run_km_max | run_speed_max | run_vol_max | run_vol_min | run_cur_max | run_cur_min | run_soc_max | run_soc_min | run_svol_max | run_svol_min | run_cptemp_max | run_cptemp_min | run_engtemp_max | run_engtemp_min |



select * from veh_dayreport_category where report_time like '2018-02-28%' limit 20;


| id        | report_time         | vid                                  | charge_soc_0 | charge_soc_1 | charge_soc_2 | charge_soc_3 | charge_soc_4 | charge_soc_5 | charge_soc_6 | charge_soc_7 | charge_soc_8 | charge_soc_9 | charge_time_dist0 | charge_time_dist1 | charge_time_dist2 | charge_time_dist3 | charge_time_dist4 | charge_time_dist5 | charge_time_dist6 | charge_time_dist7 | charge_time_dist8 | charge_time_dist9 | charge_time_dist10 | charge_starttime_dist0 | charge_starttime_dist1 | charge_starttime_dist2 | charge_starttime_dist3 | charge_starttime_dist4 | charge_starttime_dist5 | charge_starttime_dist6 | charge_starttime_dist7 | charge_starttime_dist8 | charge_starttime_dist9 | charge_starttime_dist10 | charge_starttime_dist11 | run_time_dist1 | run_time_dist2 | run_time_dist3 | run_time_dist4 | run_time_dist5 | run_time_dist6 | run_time_dist7 | run_time_dist8 | run_time_dist9 | run_time_dist10 | run_time_dist11 | run_time_dist12 | run_km_dist0 | run_km_dist1 | run_km_dist2 | run_km_dist3 | run_km_dist4 | run_km_dist5 | run_km_dist6 | run_km_dist7 | run_km_dist8 | run_km_dist9 | run_km_dist10 | run_km_dist11 | run_km_dist12 | run_km_dist13 | run_km_dist14 | kwh_100km_0 | kwh_100km_1 | kwh_100km_2 | kwh_100km_3 | kwh_100km_4 | lease_run_km_dist1 | lease_run_km_dist2 | lease_run_km_dist3 | lease_run_km_dist4 | lease_run_km_dist5 | lease_run_km_dist6 | lease_time_dist1 | lease_time_dist2 | lease_time_dist3 | lease_time_dist4 | lease_time_dist5 | lease_run_car1 | lease_run_car2 | lease_run_car3 | lease_run_cartimes1 | lease_run_cartimes2 | lease_run_cartimes3 | kwh_100km_5 | kwh_100km_6 | charge_time_dist11 | charge_time_dist12 | run_once_duration_dist0 | run_once_duration_dist1 | run_once_duration_dist2 | run_once_duration_dist3 | run_once_duration_dist4 | run_once_duration_dist5 | run_once_duration_dist6 | run_once_duration_dist7 | run_once_duration_dist8 | run_once_duration_dist9 | run_once_duration_dist10 | run_once_duration_dist11 | run_once_duration_dist12 | run_once_mileage_dist0 | run_once_mileage_dist1 | run_once_mileage_dist2 | run_once_mileage_dist3 | run_once_mileage_dist4 | run_once_mileage_dist5 | run_once_mileage_dist6 | run_once_mileage_dist7 | run_once_mileage_dist8 | run_once_mileage_dist9 | run_once_mileage_dist10 | run_once_mileage_dist11 | run_once_mileage_dist12 | run_once_mileage_dist13 | run_once_mileage_dist14 |



select * from veh_dayreport_fullstate where report_time like '2018-02-28%' limit 20;

| id        | report_time         | vid                                  | fullstate_time_sum | fullstate_times | fullstate_time_max | fullstate_time_avg | fullstate_vol_max | fullstate_vol_min | fullstate_svol_max | fullstate_svol_min | fullstate_cptemp_max | fullstate_cptemp_min | fullstate_temperature_max | fullstate_temperature_min |


select * from veh_dayreport_alarm where report_time like '2018-02-28%' limit 20;

| id       | report_time         | vid                                  | fault_time_sum | fault_times | fault_level1_times | fault_level2_times | fault_level3_times | fault_level4_times | fault_level5_times | fault_tempdiff_times | fault_batt_hightemptimes | fault_battgroup_highvoltimes | fault_battgroup_lowvoltimes | fault_batt_highvoltimes | fault_batt_lowvoltimes | fault_battgroup_nomatch | fault_battgroup_difftimes | fault_soc_lowtimes | fault_soc_toolowtimes | fault_soc_hightimes | fault_insulation_times | fault_tempdiff_percent | fault_batt_htemppercent | fault_battgroup_hvolpercent | fault_battgroup_lvolpercent | fault_battgroup_nomatchpercent | fault_battgroup_diffpercent | fault_batt_hvolpercent | fault_batt_lvolpercent | fault_soc_lowpercent | fault_soc_tlowpercent | fault_soc_highpercent | fault_insulation_percent | fault_tempdiff_timesum | ault_batt_hightemptime_sum | fault_battgroup_hvol_timesum | fault_battgroup_lvol_timesum | fault_batt_hvol_timesum | fault_batt_lvol_timesum | fault_soc_lowtimesum | fault_soc_tlowtimesum | fault_soc_hightimesum | fault_battgroup_nomatchtimesum | fault_battgroup_difftimesum | fault_insulation_timesum |





select * from veh_dayreport_datastat where report_time like '2018-02-28%' limit 20;

| id       | report_time         | vid                                  | data_reg_sum | data_reg_validsum | data_reg_invalidsum | data_reg_succpercent | data_reg_failpercent | data_sum_noreg | data_validsum | data_invalidsum | data_validpercent | data_invalidpercent | data_forward_sum | data_forward_validsum | data_forward_invalidsum | data_forward_validpercent | data_forward_invalidpercent | data_forward_platcount | data_forward_platsuccess | data_forward_platfail | data_forward_platsucc_percent | data_forward_platfail_percent | data_batt_vol_sum | data_batt_vol_validsum | data_batt_vol_ivalidsum | data_battgroup_tempsum | data_battgroup_tempvalidsum | data_battgroup_tempinvalidsum | data_vehdata_sum | data_vehdata_validsum | data_vehdata_invalidsum | data_gps_sum | data_gps_validsum | data_gps_invalidsum | data_limitation_sum | data_limitation_validsum | data_limitation_invalidsum | data_alarm_sum | data_alarm_validsum | data_alarm_invalidsum | data_platform_sum | data_platform_validsum | data_platform_invalidsum |




select * from veh_dayreport_detail where report_time like '2018-02-28%' limit 20;





select * from veh_dayreport_lease where report_time like '2018-02-28%' limit 20;



select * from veh_run_heat limit 20;


--------------------------------+
| ID                                   | reportDate | lon                | lan                | weight | areaId                           |
+---------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------#!/usr/bin/env bash
toParquet(){
/usr/local/spark/bin/spark-submit \
 --class com.bitnei.report.parquet.ToParquet\
 --name toparquet_${startDate}_${endDate} \
 --master yarn \
 --deploy-mode client \
 --executor-memory 2G \
 --executor-cores 2 \
 --num-executors 4 \
 --driver-memory 1G \
 --driver-cores 1 \
 --queue spark \
 --jars ${SPARK_REPORT_EXPORT_JARS},/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/hadoop-lzo.jar \
 --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
 --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
 --conf spark.yarn.executor.memoryOverhead=1G \
 --conf spark.memory.storageFraction=0.1 \
 --conf spark.default.partitions=100 \
 --conf spark.locality.wait=500 \
 --conf spark.locality.wait.node=500 \
 --files ${SPARK_REPORT_CONF} \
${SPARK_REPORT_PARQUET_JAR} \
 input.path.startDate=$startDate \
 input.path.startDate.format=yyyyMMdd \
 input.path.endDate=$endDate \
 input.path.endDate.format=yyyyMMdd \
 srcRealinfo.table.schemaFile=${SPARK_REPORT_PATH}/bin/parquet/conf/realinfo.schema \
 srcAlarm.table.schemaFile=${SPARK_REPORT_PATH}/bin/parquet/conf/alarm.schema \
 srcLogin.table.schemaFile=${SPARK_REPORT_PATH}/bin/parquet/conf/login.schema \
 srcForward.table.schemaFile=${SPARK_REPORT_PATH}/bin/parquet/conf/forward.schema \
 srcTermstatus.table.schemaFile=${SPARK_REPORT_PATH}/bin/parquet/conf/termstatus.schema \
 tablemap=srcRealinfo:realinfo,srcAlarm:alarm,srcLogin:login,srcForward:forward,srcTermstatus:termstatus
}

dateDir=$(date +%Y/%m/%d --date '-1 day')
if(($# == 1)) && grep '^201./[0-9]\{2\}/[0-9]\{2\}$' <<<"$1" >/dev/null 2>&1; then
    dateDir="$1";
fi

function batchProcess(){
  startDate=$1
  endDate=$2


  echo "srcRealinfo:realinfo,srcAlarm:alarm,srcLogin:login,srcForward:forward,srcTermstatus:termstatus"

  toParquet
}


startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"
batchProcess $startDate $endDate

==================================================================================




















#!/usr/bin/env bash
export SPARK_REPORT_PATH=/usr/local/sparkjob
export SPARK_REPORT_EXPORT_JAR_DIRECTORY=${SPARK_REPORT_PATH}/jars
export SPARK_REPORT_CONF=$SPARK_REPORT_PATH/conf/conf.properties
export SPARK_REPORT_LOG=$SPARK_REPORT_PATH/log


export SPARK_REPORT_EXPORT_JARS=$(${SPARK_REPORT_PATH}/bin/getExportJars.sh)
export SPARK_REPORT_PARQUET_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-parquet*.jar)
export SPARK_REPORT_DAY_DETAIL_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-detail*.jar)
export SPARK_REPORT_DAY_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_DAY_MONTH_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_TAXIS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-taxis*.jar)
export SPARK_REPORT_OPERATION_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-operation*.jar)
export SPARK_REPORT_MILEAGE_CHECK_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-mileage-check*.jar)

export RUNHEAT_REPORT_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-run-heat*.jar)
export SPARK_REPORT_PARQUET_PATH=${SPARK_REPORT_PATH}/bin/parquet
export SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob
export SPARK_REPORT_DAYREPORT_PATH=${SPARK_REPORT_PATH}/bin/dayreport
export SPARK_REPORT_OPERATION_INDEX=${SPARK_REPORT_PATH}/bin/operation

startJob() {
    /usr/local/spark/bin/spark-submit \
      --class com.bitnei.report.runheat.RunHeatJob \
      --name ${inputDate}-spark行驶热力图 \
      --master yarn \
      --deploy-mode client \
      --executor-memory 2G \
      --executor-cores 4 \
      --num-executors 3 \
      --supervise \
      --queue spark \
      --jars ${SPARK_REPORT_EXPORT_JARS},/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/hadoop-lzo.jar \
      --driver-library-path /opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native \
      --driver-memory 1G \
      --conf spark.executor.extraLibraryPath=/opt/cloudera/parcels/HADOOP_LZO-0.4.15-1.gplextras.p0.123/lib/hadoop/lib/native  \
      --files ${SPARK_REPORT_CONF} \
      ${RUNHEAT_REPORT_JAR} \
     runheat.table.name=runheat \
     geoHashTable.path=/usr/local/sparkjob/bin/runheat/geo_hash_table \
     runheat.table.partitionValue=$partitionValue \
     report.output=hdfs,mysql \
     finalPartitionNum=10 \
     date=$curDate
}


function batchProcess(){
  curDate=$1
  endDate=$2

  while [[ "$curDate" -le "$endDate" ]] ; do
    inputDate="$curDate-$curDate"
    partitionValue=$( date -d"$curDate" '+%Y-%m-%d')

    echo "begin execute $curDate"
    startJob

    sleep 1
    curDate=$( date -d"$curDate +1day" '+%Y%m%d')
  done
}

startDate="$(echo -e "$1" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"



SPARK_REPORT_EXPORT_JARS=$(${SPARK_REPORT_PATH}/bin/getExportJars.sh)
batchProcess $startDate $endDate
cnjlqcpdn01 10.86.160.112
cnjlqcpdn02 10.86.160.113
cnjlqcpdn03 10.86.160.114

crt登录   ip：47.97.187.36    root密码：123456.lgxy
登陆之后   ssh  10.86.160.114   密碼:123456.lgxy
任务目录  ：  /usr/local/sparkjob






#!/usr/bin/env bash
export SPARK_REPORT_PATH=/usr/local/sparkjob
export SPARK_REPORT_EXPORT_JAR_DIRECTORY=${SPARK_REPORT_PATH}/jars
export SPARK_REPORT_CONF=$SPARK_REPORT_PATH/conf/conf.properties
export SPARK_REPORT_LOG=$SPARK_REPORT_PATH/log


export SPARK_REPORT_EXPORT_JARS=$(${SPARK_REPORT_PATH}/bin/getExportJars.sh)
export SPARK_REPORT_PARQUET_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-parquet*.jar)
export SPARK_REPORT_DAY_DETAIL_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-detail*.jar)
export SPARK_REPORT_DAY_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_DAY_MONTH_STATISTICS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-day-statistics*.jar)
export SPARK_REPORT_TAXIS_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-taxis*.jar)
export SPARK_REPORT_OPERATION_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-operation*.jar)
export SPARK_REPORT_MILEAGE_CHECK_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-mileage-check*.jar)

export RUNHEAT_REPORT_JAR=$(ls $SPARK_REPORT_EXPORT_JAR_DIRECTORY/report-run-heat*.jar)
export SPARK_REPORT_PARQUET_PATH=${SPARK_REPORT_PATH}/bin/parquet
export SPARK_REPORT_DETAIL_PATH=${SPARK_REPORT_PATH}/bin/detailjob
export SPARK_REPORT_DAYREPORT_PATH=${SPARK_REPORT_PATH}/bin/dayreport
export SPARK_REPORT_OPERATION_INDEX=${SPARK_REPORT_PATH}/bin/operation


startDate="$(echo -e "20180301" | tr -d '[:space:]')"

if [ "${#startDate}" == "0" ] ;
  then startDate=$(date -d'-1day' +%Y%m%d)
  else startDate=$1
fi


endDate="$(echo -e "$2" | tr -d '[:space:]')"

if [ "${#endDate}" == "0" ] ;
  then endDate=$startDate
  else endDate=$(date -d"$endDate" '+%Y%m%d' )
fi

echo "$startDate,$endDate"

jobType=$3


function startJob(){
  startDate=$1
  endDate=$2
  jobType=$3

  if [ "$jobType" == "parquet" ] ;
  then
   source ${SPARK_REPORT_PARQUET_PATH}/parquet.sh $startDate $endDate
  elif [ "$jobType" == "detail_charge_run" ] ;
  then
   source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate "detail_charge_run"
  elif [ "$jobType" == "detail_charge_run_output" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate "detail_charge_run_output"
  elif [ "$jobType" == "detail" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate
  elif [ "$jobType" == "detailjob" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate "detailjob"
  elif [ "$jobType" == "detail_output" ] ;
  then
    echo "begin execute $jobType"
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate "detail_output"
  elif [ "$jobType" == "day_category" ] ;
  then
    source ${SPARK_REPORT_DETAIL_PATH}/dayreport.sh $startDate $endDate "day_category"
  elif [ "$jobType" == "dayreport" ] ;
  then
    source ${SPARK_REPORT_DAYREPORT_PATH}/dayreport.sh $startDate $endDate "dayreport"
  elif [ "$jobType" == "dayreport_output" ] ;
  then
    source ${SPARK_REPORT_DAYREPORT_PATH}/dayreport.sh $startDate $endDate "dayreport_output"
  else
    source ${SPARK_REPORT_PARQUET_PATH}/parquet.sh $startDate $endDate
    source ${SPARK_REPORT_DETAIL_PATH}/detailjob.sh $startDate $endDate
    source ${SPARK_REPORT_DAYREPORT_PATH}/dayreport.sh $startDate $endDate
    source $SPARK_REPORT_PATH/bin/runheat/runheat.sh $startDate $endDate
   fi
}


startJob $startDate $endDate $jobType
a62287f6-10c8-40a7-b156-b2a80be9fda6
e0285647-6674-403d-af48-091dd2ffcce7
d8c58624-1ca6-4f61-8630-529b988f8aec
7b315b07-7012-4577-8877-7512df4d8e3a
cad7422b-9c8c-46e0-acb0-2694b13a6894
4cb36112-8cf8-452e-bd0c-720a561d5226
a0cabc2a-4211-4470-b2d2-304780ee6eb7
87b88a52-3b9c-4099-b0b1-29c06dc08736
fb2ccf54-dc43-4871-b3c6-15f2c4294478
5baef2c7-5ed3-4cb0-9c03-7edf43cf6dff
139aaba4-4f27-40e9-a102-6b2fd4be4775
f7c44e74-ad8a-4a92-856b-939bd8960f8f
f2137fc5-128d-4948-b9f4-13927f11a91c
ea306e01-f116-4257-a2b1-d390b4a2a122
9fef5f01-eaa9-405e-9e62-42f4711f9a72
6e2f74ee-3d9f-41df-8a45-79dfc3cf44d1
7699b908-f153-4d88-ac44-b9cac160e0c4
bf1c8be2-ba22-42bd-af55-521eb47ce306
a2841916-16d5-4767-854a-e4cd8cbf16fa
bf97fe29-5cbc-41fa-abbf-12fe2a2e6f1a
3bfa7c18-e56f-4d19-9be8-f834c98cad04
c3f33e10-496d-4abc-8b01-11d0862e496c
1e9a904a-4f09-4527-a0bf-9b5866b6b348
dafd1169-c233-4b01-8761-47d36a0c2a4c
58c58b07-4e04-4064-b608-a21b991ad56d
41fad3c0-cf40-4118-a862-c02752b78181
6eb8a9d9-371e-4a45-a717-c8a811f58822
3f77215a-c620-4501-984d-e4823133e13d
045761f4-002a-4540-bb78-326517d40349
27fbdd5a-d755-4976-9552-7cbe17f3a1c4
de83c7ef-ef56-40d8-9b2f-e160eb0adf68
6a797263-00e1-4c3d-838e-1bdf604087df
87da43f6-6209-491b-b5a2-5bb11c0a5db4
0e785b6b-57ce-4180-bc4a-5b74181e6317
e9adeca5-6644-43d4-a71a-dc803ecf2df9
70679047-2a30-4950-8136-fb679202dff5
ad230cc8-29f9-41f9-b82d-092d6c5737e6
e26c5513-e4a0-4e0d-b97d-addb0bc820e8
b56791e8-a53b-4e2c-9f24-55f3674d4577
fabe1cb3-3454-4d81-8d83-831ae073818ad5d00a38-7597-4693-938e-137b7f87a296
7cae0911-53ad-4514-af84-bee19d4a8d69
9fb0e437-3c24-4bdb-88c1-ddd825b07bb2
4106ec06-0271-49ca-b64d-f88992d8676f
e14207fa-1b61-41e4-890e-a485f3f60f67
8c552b18-7e74-4207-ba93-38043e297066
1f58d05f-35b5-40e7-af90-6510195d0e3c
88b266f3-98eb-438e-8826-f71254b1eb1b
c89edfc1-091f-4635-8ec4-39f29e8f97d0
fc0031b4-b9ee-4f2f-b6a7-da75b15338d6
c71398f1-20c1-4765-8998-7a58e274312b
c5731759-ae78-4771-ac35-da70c890b45b
5a0051fb-4bf7-47ef-992b-75c3b7091a14
0736514e-90d8-480e-a806-e6152e3b38de
2347d006-5f88-4cb8-9bb0-13b4e830e618
e4eb3a86-1090-497a-ae3f-38627cb8b9aa
17d71900-5ff7-4d31-837b-2388ccec2bce
e93dc24d-4775-4f60-a228-c8d70980d756
c952100c-d43e-40b2-84eb-ea26c8c2e2bc
bb6b2620-3e31-42fc-86dd-fac0351b57e7
29343697-6312-4988-839c-808c2ba73044
6157a6e0-697e-4fe2-8021-9cd40ace6954
50be0a93-1154-4e1d-8aa8-fb0673bfee37
0dabdeb8-829b-42dc-a12b-fb88a7b41b28
dd1c6021-6ae4-4616-b5d9-6791512f5c0c
a5bf1e14-cb08-45fc-934a-494167b88549
d01123f4-d42b-4d5e-94a7-29023dd83d53
f9945801-d066-41ba-b511-d62478dd4019
e35ef23a-4bb7-4c98-83dc-ccee463306f8
0911369c-8d3a-42d0-a69f-3389351856d8
78c99f6d-1685-4812-9faf-d21de1716e42
6211b3a8-b8c2-48a7-b8f4-440b7f1f2f64
aebd4ca3-a729-4396-b219-ae50d9918886
8126eae6-7cda-4455-b2b0-23171ceb89f9
ba2fd5f6-da99-40bc-ad7a-2b16a2b373dc
d0515ff1-d43c-4054-82e4-f27a0a0467ff
48dd58a1-bf47-4ccc-8261-26cc0d510f80
354ed2e5-a396-48e7-801c-7eeffb243fed
571a318b-4afe-4348-a7bb-0ee8b7f2f279
7a4c0bad-345d-4c1d-b172-5c9132c53b69
5babc01b-1f8a-459b-ae27-4eeba48dbb71
b3137383-6399-4b7e-89d0-06a9555139e5
458dd5a9-784b-4a26-bfeb-17861e53aa84
a0547ab5-083b-4ce1-9358-4b57a98b7c9c
1b17378b-0903-4817-b751-53c9522eb061
4a4ec34a-54da-499e-9bb2-f73452c0a662
ba1b03ee-d99a-4cd2-8cce-9a1741244ae6
d7520210-b540-4063-9081-17247c9b2441
2f312ad8-1e95-4150-a168-78879867ab0a
48bd9500-1bca-4f3f-b4c6-941375cce149
2215d2cc-5471-4bb0-9542-36d0dc32f466
bf02d471-daac-4205-aa74-185df47d4c9b
1bc9df5e-4232-4ada-997d-c4050902e26e
f3fdd864-d5cb-471e-9985-a7942d367053
a9b29bad-e97f-4aab-b5fa-a45476079a23
3d4c153a-b98a-4eca-8d25-b2acf10d4486
4cc6fc92-fa75-416b-9938-e9ef014d1963
574c0f64-3894-4ec3-b767-e388b8fab252
595a3fec-fa97-48aa-931f-b053c959dbe1
68131dd0-5eeb-465f-9d3a-c9d99ac283f6
37b1c9d4-d312-43b3-b2f5-550feae431e7
bf684895-8bad-4067-862b-97b59fa8339d
8d1b7e28-e1cb-40ee-846c-2a842680fff6
e96917f8-c9e3-42cd-bab3-6dc712b4b428
d9ec2339-dcce-4bba-b313-a150cb9f5c8c
99ecb7e4-5a99-4678-b21f-cae8d7437652
45bc75be-4d25-4b1c-ab7e-495cf94fe749
43974ea5-c80c-4fc9-acd1-0ef8900f3b68
984850cb-0a55-4020-bfd2-5dcc0a81c18d
fd08c59e-246c-4bd2-a69a-6f4cd4744f37
aad4117b-980e-4001-aa10-22ba600097df
7f75dab7-ce0a-4242-b6d0-4cb78f783356
62a59018-9a9f-4864-b464-980037513a11
0f9f8d6f-ba3a-4700-aa3f-74c55cd7cb58
c8eec960-a0c9-4edd-b536-5fcf9c50b91b
42991111-3393-4b6b-8fd9-c5833fc7dbab
c148b2bc-ed90-4fc3-8309-d2c2ae991757
f1a9f170-ff28-419a-abe7-5b648a99e0b7
e87cbabe-3459-4f86-b3dc-5de1b0dced13
03f40859-eea6-4dc6-8ac3-2e925ed0f552
4b37215f-4977-4037-bec3-11618ad458fa
14324b2e-2d18-44dd-a3d8-e3aeee115b8b
3b26175e-2d9a-47af-a248-54055b558bb1
87a9cdb8-6eb2-441a-b37e-0b9e63c8bc0c
2abb2557-93cd-4536-88ee-2d522c2560aa
49770ce3-e778-4115-8473-05731c7ab8f1
b3ded535-0bc0-4d5e-801b-9a072b5626e4
a8c64008-079c-42c1-82c3-0aee2ca07da9
b5297b95-be55-4aa9-9e3d-5e010a65d687
13c2c2c9-3dfb-4dec-8a73-e6b07246814e
22cc9c4f-fd66-4165-a5c4-977532d8cad3
a3186806-1011-4f3d-bd98-cb7791395eb1
fceddd77-fdbd-4b0b-ab30-7fff15d25c7c
4b9d2a22-f060-426f-89de-f08cb448878e
7d983c25-790f-4433-b83e-c22d440aed34
fe2c31ae-04c1-4ece-9266-edfff7626329
e84c7dd4-59b9-4406-8ae3-c56546ca462c
d34776d5-8b82-4e7d-afa6-4e7669ad7a82
9fdf7190-a6ce-4299-8fb6-338a9f85d6b1
a1ee9458-d8f9-4967-9257-a4f419ab164a
ee4e77c7-dc83-4863-a75c-d3aec50fd021
d7c564ec-580d-4782-b25c-8b627c2e4b3c
d9b81541-380a-443f-b100-abfbea84aa98
d4684b3b-e150-42d3-b87e-feb3ee543acc
7ff20de5-afa4-4e4b-938d-85e580afda6e
86f91e3a-3361-439b-ae2f-9b173052eb70
48092c7e-5cdf-4ab7-9333-21915800918c
d5600c60-c372-42d5-946c-3996f73bceb3
658b2605-f813-4872-974a-96617531ed06
d329a612-980d-4928-afb8-fbeaed0915ae
7a45a75c-7a1f-4356-a728-b77b38250083
c645b2ee-05ae-4ece-9a38-0d029080af65
a4fdebc8-826e-4464-8b91-67e5dd04856f
88041a73-8e52-4bfa-96db-d54ed451f9f6
a7cd291e-058f-4a6a-bc4b-afb16032e1dc
6eb6cd35-4420-485a-8357-a25ffc356c2a
f8e485a5-d590-4462-918b-e50fc7b32493
42f92ea2-1290-47f7-91d4-0fdf58a67933
a9f8801d-e474-4ed5-88c9-785294f3dfd9
70d116ba-2530-40bb-9565-1b8f4f5f94b7
d3b0be76-e881-4a87-a135-79f75b098c6b
77a6de3d-ee93-445e-9583-a6bbc7f22c0d
7dfa7c58-25ee-47e6-99d8-1b435dc06349
475cb22e-048a-4820-b70d-3a69551957a0
cd33565f-be8d-42ef-97f5-7719d53b407b
78777e93-dcd0-435f-9ddc-6d5b35c9f272
5fa8d8c3-79d1-4f31-8587-79c81f1eaaf3
7fc88da3-9057-4b8c-b870-a7ac926838af
92b614ea-0eb5-4497-9c47-5612e3b923e3
4c86a840-44c3-4cf1-9317-88165aadb000
f236606a-a8f5-443c-8315-7db30f19ed62
a3ea1572-8f8f-47e2-929c-8d0b1281bd5c
d37c652f-a322-4604-8a59-18d2d7a7e54d
874fd7a7-27f9-48b0-92af-a334979e0248
8800c646-31b8-4267-8758-b10334aa156f
408a61e4-7946-4a81-a3eb-a8706feb6fc8
637340b3-2084-443c-8b50-8704feff3bbe
f0d174a4-8dbc-4675-b41e-9532452e9bea
e00c0808-1ab8-44a4-a27d-d7203f66fd19
e4805d3e-9af6-4d72-b0df-9ca3c2586042
b9c3ce90-a434-426c-b21e-48454bf1123c
236a0f3e-232e-453b-bd09-3dedacedffdc
0e08c8ee-013a-4f07-a22a-f59d5089a20c
27e2abc0-4698-498d-adcd-a5a2c32e95a0
19c73dda-401e-4f49-a629-47be21c19f2d
2ac72849-787c-41d2-bd63-9fba6fada1c5
1183daec-5aa8-40ac-add7-0c84024636a6
939a5518-88ae-4fa6-b348-53139ddfdfc0
283a4722-1329-448e-a1df-f09dd574a112
e7d0543f-90de-456b-a76f-2a8eaf857502
798b07d0-f281-4032-8b91-bc50a164ea4c
9c417bbf-7c47-41a0-bc30-294aa93ad177
f2cfec13-cc02-4beb-8230-051d3d4251e2
96283730-598e-427e-98f1-bf6b3a06a1f6
3fa3629c-48b0-47f3-9a28-692a2d21e83c
3627aee0-fc42-42ee-887a-470a56babf27
ed459184-08d2-4a9a-9cf8-6e9d305c0609
c081ff45-1156-421b-9ee0-4830f1e808b1
1cc9fe76-b599-4eec-8975-4c26d12cf1b9
1597d16c-cb6c-430e-ae66-fa6c3c92e288
ec184e1a-ff31-4dcd-b775-2b8aa34c9553
f0cf3f75-1d23-4fec-bf37-8bca6c7ef45a
d6b4173b-c450-4dad-ac8e-8ec681b0d94e
54227748-413f-47e1-af60-a8c6ce98da03
b887791e-6ac0-4ff6-ae4e-2707b128ecb8
cca7ea48-c305-40f5-b044-d194821c5bad
470c7ab5-4041-4957-bc8f-f519aa8481e8
703d4ccc-e605-4ceb-aff7-ae996ef79a9c
c9db310c-d7b6-4a83-a03c-c225e4cc4d9c
29b67948-6f5e-46e5-99a1-100d84d49ad4
88f873d7-75be-4cc1-9019-af1155e55c37
e969a02f-d102-4678-b37d-d842bd8bf7fb
30929b28-cdc6-49cf-8a1e-15c4ff44d8b0
cc5b619c-0607-4c37-8ded-709d8af906e3
90b80df0-38eb-41dc-8876-cc2f3b2479e5
c17b7df1-12c2-44d7-94f2-0edaf7fd01ff
6a022349-4d8c-4819-8cc6-5f41a40ab801
f868255c-79f7-4792-8721-ae9e6bd35ea1
85953afc-8163-425b-889e-70ff8a83c414
9cf36d53-7316-4781-a4f5-bf073a817dff
73b0b63f-7ad0-4236-918c-6fe7032b8dcd
a18e182a-134d-46d5-821f-8bef123aac18
be701740-b224-43a5-ade4-c502a1a1843e
819ba603-c520-4a03-b28f-ecfbb5d85b90
d63a608e-814f-4430-9d17-1c16f1bd1707
c520c3d2-134a-4c59-a794-0fd6810efd62
9a5dc958-5d5b-4b18-a01f-967f502a86cc
6e8081d2-da7d-4ed9-9a9b-70c2cfc94cba
4624ab6d-10af-4397-81c6-57bc4c123cc5
20ef8a00-a8c9-439f-9032-996780f36e73
611d2298-fbb5-4677-9d41-d709efc31122
f2492675-9137-4367-b645-db0cd01ada11
f8bfd2e0-aab4-4cdf-b923-54e68027273e
ca6cfed5-2ca4-4e1d-9003-d2043d817719
e3e4586b-2018-4510-b111-a9e05acda586
6655fd7a-9e32-429d-9b52-6f9651eb889e
4e597b11-123a-4ef6-910e-56e07ca600f1
4129f919-d7dc-4ac0-aa6a-7b5b05aede20
6239080b-568d-47e1-996b-7df661f8e7ae
c9f192ab-65e1-484a-a107-80ce236d5201
59db595f-d85b-49e0-b9dc-0e23b4a5ff2f
7de94930-2c25-40b3-8a0f-0017b5daff32
bdd3902c-a71b-4099-8a8c-10e709fa345b
e193f05f-d8a3-4733-835a-9aec90c330b5
34f81529-a57b-4533-9282-89af04fe27bb
0937548d-84ff-43cf-b55a-a3d760e6c4ce
fa7c344a-4da4-482c-ac34-09eeb09853cc
b770963d-4044-4f0d-8ed5-b5704ecf951e
b6777cba-50b1-46a3-94a3-823a93ffe48c
2e69748e-ba0c-4c08-8b08-7c1956c3f63f
d20eb86f-0748-497b-8038-d11372b26afc
6a0ea426-f530-4a7e-a0f5-c19b6ea7a0d8
6520d38f-4ff7-4c97-acce-3cbd1658b0ed
a036bf5f-a2f4-4ac2-93e5-689761baba6d
759bf9b7-cf00-40b3-aa0d-19edd1179d8a
5299e45a-af62-4fc0-af79-93f892454f52
e4d57033-0406-42ae-8192-c394a4c2b6b7
338a0986-16bb-495c-a8af-225bfef2de59
5a1cd83b-d842-4b8c-ad2e-14223919981e
89638246-60f1-4b4f-951e-d84a7ebaa7cf
2661bf26-58a2-410d-9714-acad034a7492
0742d0f7-f8d7-4195-b3cc-b0ba4465c459
01b8d979-4efa-472f-8709-2963cb89d274
d26e9891-6e95-42a3-9eea-2a7aca758990
82ce20e7-5783-475b-a0ea-8b29616a110f
19a4b87e-9614-4fc9-9721-dc17a18f1a15
619229ed-f670-4ee1-9efd-fd69efdd4ba5
b21c3db1-2232-421a-a84e-7ac8bc8a82e7
1fe9c065-8cf9-4e15-b171-324ec33f9e7e
4e77dec7-56f0-47ae-9abb-fc5ce511acf1
1e2e1214-08b1-4284-8ac7-7f74dc436acc
724a2ecd-600f-42d1-9dab-5f1ff911db7a
286f47f7-c94d-439b-b60f-e3ae9b095f53
43152522-959b-4854-9e2c-c42823ef4bfe
e68dad71-d622-4044-8944-e4f52d852880
ff076956-6bd0-4330-bf10-6c360f8fa798
c0981bc0-5156-42b5-a787-d5395316832b
910cf897-cf83-4c99-aa60-0094f2e0bced
aeaa381e-b685-472a-a4f9-6a8a011dd282
4299b72d-d127-46de-ba88-dbb713db8e28
45146c77-e0e9-43ce-9206-0d5a1f604847
360f0dcd-0a33-4f49-911c-1a36d38394ca
9ed8b860-8fbc-4652-b659-a34a6db39080
62db72a2-1a6a-4f5c-8995-693f46f9c5fb
d83f228f-77d5-4d45-9239-a413a72a9c2f
e9cbb51f-7f2b-4a06-beb4-cfc6dbf70fb5
5477834b-fc78-4d26-aa46-01b14366a4e1
d3caef46-4994-4035-adc3-d51ea8da2e76
ebcd131e-5e8f-48d4-af77-65c65493767c
4255db20-d981-4a62-8afc-daf6c9b2a283
d0d990cd-a381-40a6-ac9a-bf3f0c26ff86
b3e9221e-b3c8-4547-9686-8471bee90683
b135b6f1-d911-402b-ac22-c2bca42fb480
c4e3171e-0a46-4f1c-a18b-8f4429619776
ddc25bc5-d6ff-446a-b2b0-998da1cdf246
933a29bf-4e5a-4c01-b99c-6d1c8b7f4883
a96e0d11-0317-4596-9244-2e9ae2c5e1f4
4233c45c-3ed5-47aa-a449-d3c1dfc10216
e0bb1c82-d9e2-4a92-9ead-942ac4809f91
e6b6f341-92d9-4ca2-b9a4-db2accdfa461
ef54566a-a0e4-447d-b809-c465fbe64ae6
a03afb28-2285-4181-9a93-894580d6187d
d656b2bc-bffd-48ff-9fa5-226e795b9863
49d8a0b5-ec4a-48c9-93d1-99176724ac98
dbac0dd8-9f7d-44f0-82db-d5d70c78469f
96df9617-a021-484b-b621-0d8bf0564ee6
40b939fa-1dd5-44dc-bae0-68f3f4c5626c
e2de99a9-c7bc-4822-92eb-20d2c7a79ce8
8269b476-29ca-439c-a5e2-9a559b595c2b
358d8b1a-4d6c-4761-a585-c7fa5ab0c304
3f5ad557-c05a-46aa-8137-87d9ae7e129f
afb4d85d-6b97-4d96-ad8e-8f3c494e65e2
e5359218-e0a9-41c1-b1e9-a72aafd7a048
253021a7-4eda-405c-bb17-fffeeffdd2d2
dcd8b8de-37b8-42d9-a5f4-351505b1d927
b0f246c7-d4ac-4dad-9afa-354c3855b27c
061e71f4-af5b-446b-b6c0-7f5465ba17ba
a67df4ac-5d0d-4974-b0ca-174defc995ed
ca7c7714-84ee-47ae-a611-e675293cc32e
3e268775-c156-42cf-bd5f-16ba6d41c1b7
9f2d727a-ec3e-4528-bf31-de37fbaa80bd
54f68186-7fe9-4160-a186-2c5437d899de
bfad9b96-998e-4826-9253-bf1fecfbaa85
d046e234-2629-4dfa-ae48-664f789045a6
47019994-fc07-4306-baa0-13d3d130c4b9
574b1497-b537-4224-a8b7-3ae412a65ce9
b3d471d5-0a4d-44d6-8750-cac6cbb65872
16494554-8d13-4e11-8fe7-7743b6df9e02
0b56c961-db57-47a7-806e-4093a861c772
d713a1e7-c2ec-4493-8586-ff72dc0fb879
c9a6ac9f-7ba1-40f3-a6bd-43a0a2ae8514
b320d256-ee10-40ca-aac8-d5957db335b0
24b479a1-0e60-45cb-bba9-b77d92d47dda
0bc08487-19fb-4cfd-96b2-6bab0c9ed7c2
5af480a1-84b0-47af-bd0b-b945418269e5
a3b8eb80-0301-4e06-9df3-0caa507fc757
3a437774-06c6-4460-9c04-74e6246601a5
cd90580e-9300-46ac-a062-197d90fe0c22
2946fe24-4ca8-4acb-9494-8b798de782a0
7193898c-c7bb-451b-a85b-9ee7bf835b44
562ed0ac-1d60-4317-9986-5ab9fae84bf0
f05fa391-837e-4b9d-ae91-ca9683e6f8e5
65402759-6249-4d0b-bc6f-452e62e8cbae
f2d5b2fa-19ae-4aba-8676-e98c4dadea53
11b608ae-fdd5-42d7-b381-9cd2cacb4554
973956f4-308a-4f88-9a11-dd5bd0cc06b5
442c6a67-ad17-4369-bbdc-212a23089d72
84c96ddf-ef0e-42ee-994f-6f2498854854
d56d92ab-1dab-4c2b-b57d-6017aa64df4d
856fe9eb-f2cb-4aeb-9664-26ec5a0707f9
21a4379d-a258-431d-b212-07f26f0c9354
4686aa86-eb58-4ac6-b3f3-882a7c87d243
6593d907-8e7d-4127-b4ae-69a39cd7bddf
7171083e-464c-4adb-aadd-381af0c35bb7
820d70bf-42fe-4fd3-8f1e-c3d93de2ae0e
23ebc718-7b63-46f0-a393-fe4b0871451a
9df5b4cf-2182-46be-ba93-a97c6fdc00b7
73ed418c-0796-4984-ac82-b99193a39766
1a813bf0-fda5-4a00-9001-8944df75db5e
10e134c3-1c67-4d7d-a116-343cced93d2b
d340b195-40b9-40b9-be02-c1df5a1fe775
2b31e1db-6988-4372-92ff-1620289aff9c
3d8a4cab-f463-4c6d-9cd9-1cd8cae21131
ddcab515-eb2d-4d93-9e8f-d5a514eb951f
e3141710-1b35-48df-84f8-eea646519959
97e157a1-d7fa-434d-93cc-f1de8653e7b3
528aee14-30cf-45ab-97eb-241e4e1f2468
30062c19-fd27-42b8-9de9-3ee9456fca1a
25fb5a6d-c06b-4394-8425-75a4d9f44d59
ed3d9f6d-faeb-4d01-8287-ffc90a0a6e05
e2bce663-de47-44d1-acff-7e2276879628
48ba92a1-ea54-4ec1-b09b-756eb6ffe23c
62da10c1-af0e-4c40-b99a-b8fdbc1f494d
4af63251-3a3c-4a88-98e7-5ce1e19993e5
b12f17c0-54b1-4bfb-8f9f-a374f09dcd44
7a139024-ef44-4a7f-a220-44e52e028440
42bbdedd-5fb6-4687-9da0-dbf51dbad88f
4dfaf494-f65b-4123-af01-af309519eb04
0174324b-5c71-40d2-9068-a497f9eddc48
d3bf46f1-9fc4-4d2f-824f-aef52e98ca0b
e0f81dde-d7d9-4e74-8789-7b6899c609b4
e750d1b6-71ca-4265-88c6-6c9cd6beef9f
1beb3fa7-f4ba-435c-94cd-d1a8967e56d4
a54ee108-5b25-4b99-8c43-fe63d3b77abf
732650c0-abed-43b5-8aa3-daa18b1fa4a4
19fcee64-6ae9-4456-9cca-21ce5536acd0
91f695f7-bc58-4b0c-8df1-470b39c610d6
ea605eea-9153-483f-b534-ea547d0616d0
10fe7cc1-4c86-4192-83e8-59239fa8ee31
0b1de43f-7c24-46f1-87c3-cfc12486a0ec
f809a136-0fa8-4239-ac86-80ae891e583d
89d78ca1-fdc9-4970-adb7-98557861e64c
50a659a0-6c61-48e2-a387-47997b04d340
9ae50cad-f43b-4dca-bc8e-24e7888efede
fc19fe83-f634-4ac8-85cf-66267b8b062d
28b0aa1d-a29e-44dc-b597-99819eea64a6
6ca210ae-e765-4ca6-9858-b13ec1443b2a
e15ca175-31ee-4a81-95de-f0d5f1cf8430
e8d38c80-00b9-4fa7-b45f-01fc3b5b64ca
0dc9f4b4-2682-4199-a8ec-6ebc43bca524
f39ccc5b-db1d-4585-bab6-22d1d604c8e9
45dcb5b0-0ad1-41fc-b628-1d0fada7855f
fe2be15a-61ad-4187-9233-2b2f90b3d818
0aceae4a-02f9-4403-9b01-e385ebf2ad9a
77cf80a1-a450-48e8-b818-0fc3535c983f
86ef9610-4aed-4954-822b-f627b31d427d
979abf45-9594-455c-a553-df8f01222338
92a4215d-add4-42a9-990b-6db0887a655d
54cbad67-ae7a-418b-92a9-70303c068ee0
14cdd884-2e0a-41f0-b56e-57d4aeee430c
ffa759c0-0e5e-45b7-91e8-710cb1d7c515
d01599d3-4026-4d4c-bcdb-d7b63a18c73e
ac234b02-41ef-492f-894b-eb726701d05b
3fa8cc51-9354-46d4-88dd-839508ff9cf7
f9a78712-5854-4113-9460-965cd542ed78
b1e2b127-3710-41dc-b033-d4cfd1083af3
3f02702a-aa29-4659-8a1d-2136dc59f0e1
9c77e73d-b37a-4a6b-b2df-6b2b596d8e3b
dd981b2f-ee20-4c5a-aeda-4b7dc2069049
5b93ad74-14c3-458d-9cb0-2e4eaec337c5
60122551-72ef-4c10-bf80-15608b208666
a4ca4dd1-782c-45f1-8349-2df966ac026b
25d9b03b-97c6-4341-9437-1781c5b80e21
12327f88-b358-4819-8241-654803a4565d
fd204953-c8b6-403c-9b88-08668aa61880
e3065033-752b-4094-88a8-04e22797b47b
8b9aca62-3196-419a-b460-cf47e2a7570e
378b3fac-e25d-4ddc-949b-7ba9687476f5
624475bc-a0be-4041-8180-2459b0f1ed4f
1ea79a1f-64f7-4054-a1ad-c5c772eb0fa9
0e2e1540-2b6e-4361-a519-62d0f21c9997
037351ed-212b-40da-b761-b6e18f958483
72eee288-d163-411b-b8f1-47b3a24764f5
cf4d25b8-48f8-48cb-a322-42d4dd3fd468
dc489d4c-5116-4e00-9628-5cd8b5f754a3
589d4d2d-3bcf-4d81-8777-34e564fae835
09a2b768-af28-4775-8e1b-f44e59a09091
d35f3f95-7a6d-4240-9ecc-9b83c70a238e
27f6b563-24cd-4e75-8fa9-41c69899200e
5df3c0d4-fa97-4ab6-ae6b-75b4a595a020
5f291ca1-69ac-4d1b-bfc8-ebf4e91895d2
c5631a98-d4c8-4a6d-add1-b108f0a5bdc4
52ebd597-13f9-4b0d-b3b2-a8a7dad89a4b
5459eb40-653f-4fe8-9804-5bbc8321a78d
668ca87f-cf4f-4692-a768-ca3dadb10d9d
2eb73323-268c-4876-bd40-e026f285f946
61f7079f-e790-404f-8d0d-09c96ff5c792
67499fd7-59ca-4ed2-ac80-db66e4257be6
4aa7ba87-3d4e-4a72-b8bb-e2b4340d4a30
01e5f694-f7f6-4fee-80a5-971760ba6835
49f01fb9-3f75-4002-a512-0a8670bbc5f5
e3d605e8-f6fa-40cd-81e9-52b91ea08d0d
a77052a6-5d71-4a6e-ab0e-1a823a0ae91f
5efd1ef1-9ef7-4315-86b6-604c57537324
e704db93-cdba-4617-b38e-b3ba5a447ddb
00f67342-d4ee-48b1-b17a-a69ca34b1270
75043085-be24-4c95-acfd-d80708c9e966
76211eec-7672-4ed4-b9be-7daa7aa8fd5a
90c08de1-139c-45ef-a756-53bca8614aec
804c8daf-c926-40c1-b91e-2e797d9712e8
478503b4-fe0b-4957-b567-1c8b14c99ce3
8302cd33-49d7-4a56-adb2-d053a1593005
307fc563-23d9-4ed5-a9b6-1b34eac3a174
fa6cf3fa-016e-4dff-b576-6de4b68e8a6a
746d85e3-a7e0-4595-b7f0-b66ce74508af
b819bf92-960a-4058-b4a0-3627f1de1fd3
02dc2649-9ca7-4dc5-9c46-351bf58c7f68
0b2137c1-aa8a-4b2f-bf70-f11cb6899b5e
5064e8a6-1bc3-4360-b64e-a6cf217a7933
aab5592e-98a5-4772-8081-2f17ef3ed0c7
aa698fe0-39fc-4c45-9e05-3570c9e4a60b
4a00aed4-1135-4c98-90ee-8441e22cb0ad
60d5b489-e67e-4873-a8ae-e115258504a2
3ca69f6c-dd3c-4575-9b4f-66eab7779512
316e8768-b9e4-4369-8ef3-e8bed9163239
13984ab3-d2d4-4e7f-b1d9-3659ff68f7e0
2a903100-71e3-4e2b-b5f9-b7f3ec42928f
3956675e-8ab9-4a8d-9ec6-06e1c4230084
be4d076c-58a9-4f3b-8888-c7fca1e4c278
e683c492-e02e-426a-87a9-98296bc71b5c
706332f2-b0c8-446d-bf73-196e9e21a212
e779fb96-85ef-4723-9a63-1f8f029b2ff6
a4b283be-d03d-467e-8ff3-53a5a4429ec3
87aae479-9dc5-4e12-9b96-ea86442f97b6
d51c6c2c-a5e2-411b-8bf7-cc677a55077a
f6c25b2f-c9f8-4c63-ab8c-2ff308483eaf
47138c76-e200-4943-9380-4d45c51c4a24
51b339d6-2ba0-4a54-abb0-b984aaf442a4
34560dde-da3d-4d16-88a6-927a139d5dd4
08c6b7ec-a051-43c3-8da0-187f9f386e31
b79475d3-790f-42d7-a76b-fcb937abe2a8
621c8531-0c60-4941-9492-e62ee5f24e27
c5f0a782-1865-4ff7-b28b-f55db7f839df
23b9887e-0175-4691-aaa0-386e268276de
add253bb-463a-4bd3-a6c3-773c484c95e0
0a29c029-5b62-4f95-9803-b8a40cc974f2
0a798d92-0191-4e26-9a39-2f3244069dde
317e635e-e5d6-4c3d-95e7-2a68928c1a1e
fb072684-57af-40e0-aaf5-2b04f8e9461e
e4a33987-0020-49d8-8628-6e1bdf5f3b71
ec8131a8-981d-47b9-a20e-25baf671396f
6e4d5243-9166-4610-aad3-5cefb78ecd62
9ed40702-614e-4f3b-93e0-87664dd62002
6da65122-6d20-4eba-8587-4a336c4436b3
aef300b9-7b8d-4ceb-a6e7-89952108091d
1c22daf8-fe1e-4675-9cec-d639895f65ca
75966e7f-af35-4b2e-83d4-88f8e4eee6da
6078e65b-a5ef-4dd9-bb5b-bf177affe39c
043f0d92-27f9-458c-8eee-47625103449b
fd19427a-4164-4522-b25e-030a4fc6e768
7b45ccab-fa98-47f2-87f2-9b76de3778ee
e3ebf2bb-3356-4dd6-8005-b4bf7165cf85
b4d178c3-07fd-44c0-8b44-adfa0caae496
c05ea608-59c7-4503-aabe-ffc8b95674e7
53d78124-ffb8-45f9-8ec2-8401e62dfe87
c2a43163-bfe7-4445-bb67-9f987b105132
c7742eb5-f0d9-44a1-ad2a-0f4d25ed2702
c793ca40-a365-44b6-be0a-7dda94672a74
b5b34aa7-9167-40f4-9d41-c34b01adeddb
ac623b04-7101-4a16-83d3-f6e148a6fad6
443fe5da-26fd-4c54-ac14-b5b7936d4106
60cd5ee2-91be-412b-baff-2d230c893ad1
cb995171-5034-48da-bf49-3f42028753b1
dd4f57b6-b51f-4a7c-bc05-8faf0cac444c
4e021fc9-7679-4a2d-9e25-e72977c7efb7
4b8df4e7-b8f3-403a-8cd7-07e83fff48fe
9e92ef58-7ad7-47ae-9c10-010f303bd553
0c7504dd-bfb3-4999-9ad1-9e453d43d2d4
cafe37cf-b3c5-446d-9c8a-09bf583f4881
1d91a246-2a67-4907-9e92-f82e32252def
623eae2b-393e-4ae0-bb41-c1339d7810ec
a3df52d7-5d85-4022-a04c-5e09962dc53d
0d7dfa62-615c-4cfe-ba19-6e4c67e8b05c
69d94cbf-3cb1-48b8-b840-c4c9aed7a238
07a94903-7c0f-4741-94fc-c7cf953b3a5f
9cb17ea8-e371-409a-b420-10176b01f656
55e09466-5b8a-40b2-8635-b3ddea36a01a
71245e0c-2e3d-4cac-a557-a5753247466b
f3cd73a3-31fc-4af6-a51c-b7efd2f34f03
02c921fa-3b27-45da-b317-6de51bec3ed0
1b3c0673-67a0-4579-8f28-aa237d660881
fca5b689-0466-4fb3-ad6e-3524fcc85abe
f89a2bb4-7b4e-4b2d-a2f3-5ac4e06fb3ea
74e6a50c-fe39-42ef-8dfb-e68513e165d2
cc936892-68c2-417a-83a8-2b31bcd9669f
e1dfb3a8-e3c2-49f4-992f-1f6a472724c0
82c2362a-0cd9-48a5-b8be-7b39031a259f
a6198a02-0bf6-4673-bbd5-1e27c054ad91
deeebc29-39a7-4bb2-af8f-ab6dfb2e1515
0cf0db38-f560-4611-a768-9001340f5d02
c538b174-e76c-459d-b676-4d69e37bdb13
281ad171-cc57-4e55-bf30-24d7c87a5670
5708f3b0-bc2d-4de2-94b1-8b5bcc9b3049
891c1590-e205-403d-97cf-d36845a4aac1
e78fcb57-0723-4e25-a30d-c9f31efe7a04
fb64fd88-c1cf-4467-b5ea-990c3266ed85
17c8bc74-c115-4fbe-90ba-cf20925eec4e
c47a62ed-63eb-4cd3-9bbf-f9692f70d36a
76c330e1-c2b8-408b-be53-67e016bd835e
e59202d8-5836-407d-9997-ece3d0b76634
87a3bcb5-93e5-4bf1-b1bc-de628bebb5e2
b4c22db7-24f0-4387-bfdd-c69d38c42114
191aa1de-0ce9-4201-8ce5-4e15cf9ad3f9
44505896-b955-4a60-a073-785752b80deb
9eddcab7-e546-47fa-be54-9d29a29d3e6c
260a26c2-a088-4ea9-ba42-39cb64658c46
db0d7104-1b63-411a-abee-e229b7fe17a7
086c1882-458d-4c6b-9951-25cf6b6f0caa
b7ee769b-be6b-45c4-a4c7-e477731ac014
acb6ee1f-4264-4bef-aca3-fdb56c7007d0
b538564e-c3d4-44c9-8a17-57b5f4f4296f
5dc158c8-5f1a-43d3-948c-e4de1d2d4faf
40a65d7a-cd27-44f6-a122-65edc68b00e8
c0aa6a9b-858f-4158-988b-09061e4340c7
54c5508d-522e-4c69-93ea-7e720c07c9d3
ce6ef822-d8d1-40a7-8fe7-20d3008850b2
a1173836-48fe-4f10-9e6e-63c5fb176848
ec4397bc-1e5f-48bd-80a8-e7c5b8d8218d
c50b5cd7-9b4e-4823-b721-2cd01a58d29f
d981b846-db48-4b26-88d2-0a1b804beec1
18070190-bf50-4f3b-953b-7c2a714bdd7e
0c1e3f23-f9c1-4da1-a026-79dd37a466bb
75b73450-9b6d-4403-a637-562493b55e8b
231d340e-9163-4c30-8870-321b162e5de0
c34d3bb0-53fe-4b76-b2ef-6cc06c378226
098dd088-eebd-4596-bef9-5ce8bf44cb86
27af1c30-e1b4-4bca-a6dc-7f8394886467
91df0b19-0c07-45a3-883e-33a7a64ae500
8d9473cd-fe5c-4400-9620-1bcac63bb085
281bca43-0564-404f-b498-922c07525c2b
85b79ae9-6797-405b-b5fa-4ed247cce844
28ce03cd-ef38-41cb-848b-fc9fc7109bf8
93ecc33f-19c9-464c-805d-e36935840bb2
d8d15a22-5715-4365-87de-89b0605aa808
33ad4b72-997f-41cd-8ac8-0556f85a2030
ae9314e5-0954-485a-9460-9c422d0b8d8f
128db107-0ffc-404b-917e-cd234847a7ab
6839f58c-46b8-4662-9341-7c2952582f04
0e3e9905-bc3a-43a4-b4a4-81bb9e9ca2ff
2cc638e4-819e-4935-970c-8734fa6a4e19
5e2f322d-c372-4c14-81a9-593916dcba48
878f24d8-d7d9-4820-83a6-5c5046654f24
de4493e6-fd95-4a34-9c52-ff93c8d1e936
61d89354-18d6-4a63-948b-8d444c9dcbe4
42aac262-12bf-4215-a970-4fc6899ff985
cc329157-9bb8-4f6b-802c-ecd28568d39f
db3ce4bd-7318-4b60-b628-dfeb8c6c9d71
3c625798-c232-40fc-93af-e99509450844
6a22a32e-8caa-4bc4-baf5-9f28983aa7f0
daf11e6c-6597-495d-bddb-fa54a51ab5eb
b087caaf-e282-4447-a8a6-dcf364db694f
73e68394-3da6-4985-89ff-1f71dc3e908e
fe5d84fc-542e-4657-b439-39f0de4f41b4
f6f1dadf-2ea5-4340-a671-c3447ac1e676
280abbc3-6de1-4f44-8c29-6868341c5e86
c0a92c2d-db8d-444f-8023-e4b60ca4bc58
ee0af0ab-f1e6-441f-a08d-f4042032c78f
fa363077-fe3e-4ef1-962d-da6d07cef213
a27ec3e1-f051-45eb-baae-49966edf18e0
573c5b5e-8cae-4be0-8534-b43297232b88
8158b7ba-c189-418a-8877-34ceca937b6c
29b5d3d4-ed6b-4b13-a9e6-f98550e82247
7e0dc9d6-3258-469f-8a74-629784b599c2
b2725a9b-199e-4d27-8ee3-41186659c912
0c41cc01-ff8e-44f8-9da1-319b71087510
b1fd87a0-e7c7-473a-a49a-1662449cd59e
16a5d04f-d0f8-45b3-a43d-6ebe45360cb8
8f4bd680-7a34-4655-a761-9ec91fb7c2f1
ca182869-2134-475a-a4d6-e2ec9b6bc7ef
3bbf8fb5-8744-4bc6-8b90-ae10a542e02f
fa4fc68d-4802-4d48-a168-4349243f1c0e
b0f4c7de-7f4e-411d-b81e-219a2daae557
c36a9040-98bf-46c7-8661-365983fa86cc
727b9e1c-2715-465f-bdda-852ab2a65200
5efa171b-a367-4d8a-967b-9c551bb9bdd7
392e9f7a-7a33-4b59-a011-34243275906b
c8166355-8980-4b95-a304-bcfddf12ecc5
a9120e32-644f-4d8c-bc6c-1290abf64da8
b19703e4-2665-4bf8-9e52-706b2952f7c5
c2143c90-a01f-43c5-a233-da4bc2b41808
58cb6207-07e1-4216-a0e5-5926609ed6f7
eb3f8de1-2b77-4e33-8413-7abd4acba0c3
796c19cb-3dc5-4084-b75d-ebf194232f82
7f62ab29-100e-4b77-b2d7-0398c91ceb3a
1c8fb53b-83ad-4af9-9956-efa2074f68be
3000cdf3-424e-4354-8ccb-ed7d0902c40c
eee07273-a06f-4d39-ac09-49181bf98b06
255a7327-4269-41d2-9e64-f746345443c3
ebb0bc36-b7a2-4f53-81bb-74e4d99c9017
5f31fb79-bb51-444a-ba37-6e7fb8cfc593
331ee99a-a0ae-48cd-8eb1-7d58b91a3012
4efae354-4e1d-45c0-9ebb-a804bc5767ff
f4b5dbf4-e3bf-43e6-8928-3d8346efcf24
e3d00353-18e2-429b-8d99-fc975e35ec7c
4165de57-685f-476f-8141-4f47e3f93a51
ee2644ae-c02f-4620-bb7f-956906b95708
ec140112-8193-4eac-8770-c4ce880cd7df
016ad74d-84c4-4f70-a9b5-34edab51bc82
1d7f1200-3dcd-4fe0-856e-fd2737e8a081
e0a8b390-154c-4a3c-a8c9-c5eaefbb0d26
a9bcd48f-28be-4230-b817-a397ddba27c3
2b32bed1-cefd-421f-ab0a-b9d8a466177b
270e02c5-afee-45fe-8bb2-94ea5c536777
fe7bd7ba-a059-487b-bcac-95e60148ded6
6b110efe-6cc4-427f-ad5c-757c8500d1fb
004298bf-4778-45a9-92f2-c378e6749fdb
6c6558ec-d9b1-491c-9b1c-71fc457c1aab
112e80f1-c13c-4a3c-8f55-dfb5a3731611
1a712c4b-4668-4c2a-972b-eb52bc572548
d53713d5-d282-4486-93ab-00d7f0bd221a
d0c9f7ba-e40a-483e-b0a0-0614578b9a7e
afefb19d-063d-4865-b682-c128b0ba9971
30591303-3b6f-46a6-b034-c956860d9dcc
e41d8ed9-2de2-4885-b309-b3576157b38e
3e3c0cd2-6523-424a-9c91-0d743f6ad347
e124d644-55d3-46b1-ae84-515a184e8158
6b9f3287-8dc4-477c-b857-def517d655ab
e430d95c-23a8-4ab3-9b9e-427f0b23b573
a1e803e1-7ebf-49a3-b665-2bf0a5d3b084
c6065c1d-ecd8-4cb6-be82-c3c5ddd12de8
d5322517-64ba-459f-b6ed-5180dcc43c38
6564a3db-adf4-4427-9977-28a80c3be804
66c2b5ee-6ac0-448c-ae67-41d203f2e0a6
b5d142cb-b69b-4155-96a5-26d3dd842e7d
674bfce9-dce4-4bcb-b3d2-e25ee35261f3
f4449581-4dfb-4387-912b-7ed5604653bc
1e227dea-6f5a-4d07-b090-10403c3837f8
6d5d6aa0-180f-49cd-8f59-83b8b4146b71
e31e1d09-0c55-4b59-a3f5-e7136d46dc67
aac42204-ae57-4bcb-8dfe-44b41dd92009
4409f08d-2c54-43fe-bc0e-510b98bad3b0
ae6110c7-ab02-4856-a8d4-7b141f65e54d
ff832fe1-3520-48f7-88f5-1873a2fe7fb0
9abb4499-7d6d-41c9-bf15-bebdce9f674f
0cb1ab79-8340-459c-9917-9ac7a6f60839
72fb0bd2-29d1-483f-a338-92ed5f366032
ccbb5121-3395-460b-9888-74727b2a561b
9cddb60d-b467-472c-aa1b-8c7819e93b03
230f1c66-d162-4cf0-bc80-a507e40e03f0
14a520be-3a32-42e3-bd25-46b3ed332951
b8a2491f-32e4-4aa3-bfc9-a14ec542fffe
aedd1d44-dde3-42c9-a6dc-0dc8f9b60a3d
3feddc84-762c-4fb9-8d83-6a4bc7a19bc1
5727e081-9f95-4032-95c4-c5f8c6f23779
84155336-61ef-4a1f-8d61-26cd936db174
0779eabb-7f2d-44e7-9f3b-17539629ee59
1e8d9f94-d8ec-400c-82cb-8871f13cd579
d5238887-5b9c-420a-a4ec-98d18e793055
3c1aaee2-3ef7-45a9-8d64-7f1ed4b3cb92
a6e81a44-7def-40fc-96f3-2cb27856675e
37ce6876-38c7-473a-9308-a2bf57d0fdf9
4e8cb2b1-53db-4fa7-9de0-ccee850862be
9950cba6-c7b9-4ab6-8580-8357967c35bc
5d2b5d4b-0f4b-41f8-a877-929352790c63
3c03ebd1-322f-428a-a788-5796a52bdc87
1c469cc9-3a3b-4e6b-8c44-19ab659dc2f9
0cd7edaf-7a99-479b-865b-ae0db3506785
bf984dd7-ef86-4d9c-a40f-38f7d6a121cd
380b2f09-8a1d-474d-9a6c-6f2474077ad1
1673d719-48fe-439e-9145-8223b8c99c6c
22185e1f-9c80-47ad-b7c8-a06045540f42
6e240159-ff6f-4e91-870b-de2591a25a1b
8ed83d68-9b95-417c-b21a-060f96ec5e17
c05da806-5269-4d31-a413-d51a450501f3
88e70bca-86d0-407c-a0f5-4737e3ebb334
0b661fed-3539-4514-87ca-10aa3fd4a2fd
0c147837-2851-4d5f-883d-e80d53ff96a3
8f2eda86-ee6f-4cd0-a6c7-3caae8e9af36
5fcbe6fb-6347-444e-bf5e-41f4ecbe225f
0d9ddbb3-804a-4a9c-8280-e0e8a35d7268
9606869f-f0fb-40ab-8999-e103278bf1cb
92bd9147-78da-40f7-8e5c-e96dd7fa31c3
9d76c448-c324-47c3-a8e6-b6f43ed29b89
576e3b8a-45c8-4279-aa91-bf6d578b5de0
b1cae075-2a18-4792-b348-1df9bc3a1cc3
38bb0906-0137-4832-b041-7f94384fecbf
bf855294-84e3-4384-bce8-694ed87f577f
82d11b28-3ca2-44ea-9e42-ca26fb49df89
b8493f0c-70ef-4452-8ecf-97cd85ccfc7a
052a71c9-b233-467d-ba9c-64e2208857d5
5949bddd-b5a7-4666-b09e-8719d4dc5de6
16676fde-9436-4b93-ac39-cb9716608c20
4e14a7fd-681e-4865-8075-5a0bab655e05
d917630a-112b-47bf-86fc-7d0a8f906af5
fe2a232e-d61e-49c7-8a8d-88c786a49699
38652b31-b573-4716-91e0-9a26903384d2
22e6adc6-9203-4e80-968c-4b3d1262a927
69062ead-f13f-49a4-b541-24e8fd76220a
f39aacb5-d78e-4962-92ec-6d1150e68f7b
e39dc4fb-9cbf-4353-9088-b7b3ce58d734
a30ce62d-6bfa-4d98-a4ad-31f298334e96
7dcfc7ca-12aa-4473-835b-501a66221c8a
77fa653b-059f-4a90-b9bc-f81630260296
46bc726d-8e14-493a-9fa2-b4a1a452f86a
41cec485-5905-4b65-ac16-737f8345d7a7
d024fe10-910f-420a-be61-e35498920c0b
ec9942ef-4e5d-4d2d-aa1a-880a0f0e0953
9ea62798-a965-4111-b6dd-82c6b199adda
4604fd73-094e-41cd-aeb4-6ced8f1a8f13
e1630c09-1406-48fa-8181-e7d1a94d29d2
f4aaeb65-1a9b-44eb-ae17-75c63f76809f
aa973de0-56e0-4799-aa58-19a04bdba1ae
6b6bb350-48d3-49e9-b292-6c7698108630
82d5c810-1301-4b2e-8fb1-ed471f083350
227ff52e-ee19-4771-868d-3ed1a48bf1c5
6402f098-d570-4676-ae92-1906bc75e739
6a3d9684-df45-4b0a-b403-7957e45a862b
95ef9d13-45c4-490c-b964-9c015ae653f0
f3914c1a-e11a-41ec-a6b5-55d805e7c9f8
4f2fc3c4-8872-4567-9baa-2069ad8836ec
3c5214b0-4e0f-4e40-b17e-8b6aead349d5
b33d2a12-9519-4981-b9cb-d3fe582fcbde
7835b394-f859-42c0-beef-5da69aa4bff6
350059e7-b31c-4cd0-9b6b-7e69270133aa
1b9962f4-0f17-4de9-bd56-038b13124e9e
a9702445-8d98-4d79-acf0-5f3273b46e72
a0556a7f-5c6d-458a-b561-c52b8376c073
f70ed8fb-7163-4cef-a894-ff9a827711c8
fe30f722-cf9e-4fd5-a688-85f31a0c4e57
7105ed1f-d5d9-4aa7-a835-533bbd5cf547
c09fa349-1820-4522-99be-5210c7f67313
1cd7ab46-6f6a-4c6b-8317-26b0cd55f21e
70c51336-15bb-4dba-9326-91365c143cc4
683f8b3f-227f-44ad-bf6e-e7fe29c0a4e9
94cdbd1b-9096-429a-8c67-32320c5161cd
4a244f00-1cdf-40e6-bfb9-6ba9a80a1394
e9cda270-94f6-49ee-8e66-dc3e59cd0f02
325e563e-d462-4c61-8fad-782aa524e4f3
4c5ac1ea-9acc-46d6-b02d-b763007bd4c1
37fb3b44-9978-4c23-9f94-4a8ca8d45770
7ba63f05-05fc-401c-8a5a-bc879fce6770
9d61b3d5-1020-4da3-8ad0-fa46caf20871
fdacefc0-4ca0-4ff4-b830-df8af4555e94
54614314-d8ee-4cef-aa51-d139b80d5134
23bf8516-77d6-4f46-a7f0-657e9e16f974
5a8c66d2-0c59-4269-a8dc-d79a6ae781ee
93ecd704-f3f7-4e0c-a1eb-81e99d42870b
5fe87f88-a1c2-45a7-9358-e428e4df366c
8fcec8aa-d0c7-4eed-9415-d4716afa33f4
f41826f9-198b-48f6-b44f-da2249426a58
f0ace8f9-0cd7-4ea9-a360-2c1cf6f2300d
8495bc5b-832b-4c3f-a921-e74d7ed97608
639d5dbe-7584-4578-934f-0f8cb4ba1575
3399f16c-0f14-4eda-a197-4c0c80324a0b
cd0fbcad-3f60-4263-98cf-51b8fff039c6
248c29e9-cddd-4424-9725-03a2131d3223
b66e8f37-456e-4e1d-acfe-881b7e71ebbe
e74dc4b4-b818-4883-8a5a-c063805b4464
0a2ffa77-4f7c-4f34-84b6-6aa89b28bdb5
278fb53f-608b-41e2-9bdd-58fc80e3be44
b0b23727-ed9f-4653-bc6b-303f77fe65fc
021b8049-e1ae-497e-9227-22cf5867467c
5a3763c6-399b-4823-bae0-e43099c20748
53ee39d1-42aa-4cf6-8755-e47384f44947
c6fe9222-88a9-4dd8-a8b6-9ea4c3e194e7
2215ff2f-8d52-433d-a939-df295949468d
943998d0-0159-40fd-ab50-6be7dce85b92
063da5eb-6441-44ac-97dd-2b363604bc14
99570aa7-8275-407f-af79-dc26bb64519a
a51b956e-13e7-46e6-9cdb-fe48f93ce9ae
9e0c133b-aaf5-4b06-85cd-254b844e90c2
d626e850-ffd9-4169-94ca-1f4a7eaca399
631f3258-a984-4925-9849-e4c6012eba68
cae1ff29-c883-44f9-ab94-ca90b8b32924
8d9ef9d2-b8f5-4528-ac88-614d23a4f899
a43e41b8-e508-49f4-b664-4080201dd1d7
89bbcc31-b2ec-4c1a-aacd-13513b8e5329
d14b81f9-d3ce-4364-9120-1d51ac30ae75
b9352f79-6cc7-4db7-b9e8-952be9ffb349
55387e09-c703-4739-8149-a67456efe653
922f9336-65c2-4cda-9db5-b38e96429a65
77871af8-838d-43a7-b503-b908e634fa1b
27a47d3f-1a43-4902-9854-9d7a29383abf
eaeed0ca-b861-407d-9d77-38e62fac263e
7c1bbacd-acf0-4503-a52e-3a1780b5f71a
c1f8ed40-d985-44a6-a6fc-c76fe2e4e1e5
912945d0-7228-48b8-9471-600a2195e74d
53c6be3d-674c-4842-b58a-9763cf72cc51
fe08df45-4a02-42a9-abae-65e3a5098ce3
412e3b33-1325-4429-92ef-95e0c758e161
fe80f041-f386-4176-8a7e-29555dc09154
d51f8055-74a9-4802-a7ba-878fead313b1
7df8a2d8-9c0d-4bdc-a466-fbe21be663a0
c17bdc7d-cb8d-4c35-8b14-f645a52477a6
4a1f41e6-088a-4678-be9a-dff100c68d52
4f514971-7e26-4715-b2dc-fafb94871513
a3f29436-f98f-4e2d-94f8-62149ea9e378
63a896e9-454a-4831-9020-0d2575d26ee4
46331af7-151b-4358-8b0a-c03ee824571d
3a76c677-cd4d-49c2-b8b3-6b8165d579af
15b07863-778f-47c7-80b5-f1fa45e90b66
5e671769-2a02-4d8e-b6f1-4bb28bf487f9
545ad2ae-db83-431e-9225-eaa3dca9e5cd
f6f7206c-692a-4dc2-80ca-54d9d56c5840
61bdbd9d-c488-4667-baa9-2757f0160200
9b28b94b-64f2-4404-845c-8bcd1908b9fa
eba320d1-ae00-45d3-b7d8-d376278472c8
8eecbd2f-4133-4a76-8d01-45d2f2c32fe4
a6b82fd1-985b-44fe-9313-24678d885462
2474d892-9d54-4d1b-a540-cf75c4a4c140
c408238f-7939-4cb2-a29d-0cadb74ff455
0262cc19-280d-4603-87f4-104ca7643e19
9860aed9-2c28-4905-bab3-eb9982b25159
7d2d0cb1-c116-4eab-abc7-a218bcaa0656
9a22c9a4-7f68-454c-a95d-959dd32c0172
5aec6ede-1938-424c-a309-6d1dc1b007bb
af10b89a-c42c-44c4-9def-a8116fee62b0
e9fc7ae2-9cae-4373-b0c1-f4c3683d1cae
48b9fa94-b91d-429d-92be-1d8cf5f094a5
611cf482-010a-4d9a-935e-ea219cd15742
ea60f9c8-1408-4ce2-ae91-de02e2dedbe4
3558d5cb-2be0-4058-b335-9e78cf2d9dc1
727141f5-3555-428f-a7ce-2c5c8b80239f
f8ed7569-32c3-459d-a36f-3a20825bae8f
9e9ce0b6-9584-4bd7-a659-36bba6f86be7
821b335f-cc8f-4ff4-8386-c39e836f12ec
7f2a0402-3fa5-4d98-b06c-b2ab322ec58d
4268b814-392f-433b-94a3-4339f8c26d1c
c070c5b6-17a0-4343-b4dc-0f872338f108
a6afc69c-fe51-4ff1-8569-50a94c6ecba1
57e353a1-9e5e-4211-828c-b10be7a36737
c2d9233c-97fc-41b3-98b3-79ff7876f540
c11d1a2a-777e-40f7-af57-66390f127875
fea38f4f-8bd4-4902-87f4-b7e30f76a1fc
535edc7a-5628-4b01-8838-00053015386f
c0e6da8a-4164-41b9-8025-e595c07b991d
72506650-4ecc-4672-8c20-41bb2d45b643
69dc6108-f708-4bd5-98a8-021aa944bb76
fbb30987-baaf-4f9c-89e9-f395777137c2
4ea6ab30-7090-485b-879d-f763f963d86a
0074241a-3eba-45e5-b322-abfb2ec85f37
4f0e562e-d378-4405-8ea3-f4123d5f5c32
8d4b5054-f4b3-4924-b01f-cf07d162b2d8
52c60a64-d6ba-4a21-abde-f8f676d02d6d
dbcf582e-0037-47df-a02e-ac0d837b124c
11308864-e6b8-43b5-83e5-653f5f87e01b
cc4716d1-1c78-47eb-821b-4fc96d013d12
e3f13964-09c0-4e80-b028-44920627b6c9
32b7166f-9c75-46fc-8439-9d94de875d8e
7dfb7881-2388-4869-99ba-8635497cbbe9
49fcf0a3-bb72-4fea-96a5-59efd3a4f747
ec6cc611-728e-4957-b406-51135e4002b3
b0c07ba3-b4c2-4d66-a620-72cb9dc9b051
58a67eb6-8555-4672-8934-14f3bd899eae
03959d0c-b3fa-47a9-8b86-6b71f1c45e33
6c5c54b1-2bdc-468e-9409-04b5c3b04456
a8644319-a85f-41f6-afe9-70d2be5a0f17
ea33aa1f-e525-43b9-ba74-b4ac2ac0b216
9faa1c43-5e79-492a-b0b6-c208e20b6c5e
8b12665c-5938-48f0-8d97-afed53749af6
4df9ed7b-7bf9-478c-a957-ff24b061252d
810eec1b-7e9c-4456-86a2-f16bb825056f
fa99344a-f27a-4f94-99d1-a6635772ffd5
e881e293-97f7-4264-bc9f-5b91f5aa899c
b9bee65c-b8e5-4cb1-b9df-436c182513fb
8ac63d24-a5c0-4764-a4e7-c7779ea82f5f
4edcb4e8-722d-4fed-9ee6-3bd9b6bf6694
a431abd2-6904-4225-b89f-3a963a951772
2a2f9a0b-ef9a-43a4-937d-21841613e48d
256dff62-4d4b-44e7-ab53-b54bf235ac7b
62740eb4-a36d-49de-a41e-1e9e03da5d86
c1d2b54d-5462-4054-ad50-76f6d07685f7
74dc37ca-2075-4fb1-80b8-3ec739cfce3d
711f38e1-5cf0-42e0-a8de-b20d0b2915ad
877392a8-6bfe-4290-bbd2-e05c2718aecf
815880d0-ccc4-471a-829e-d9a916184027
b7fef112-8766-4e3e-8ee6-042084f6cfcb
e867759b-fbf3-4a85-8c7f-be03e0373750
09fb7bbd-a00d-4bcd-8a33-ef83a8d54c03
f5e1af8d-44bf-4c1f-b80f-9c4b68676e49
5c2ed705-6a5b-4670-8cf6-ed950896109e
882b6017-44ed-4505-ade7-eecbebef27f8
f8325310-f0c2-4047-a20e-f4ce49be9042
1e9e27bf-d133-4fda-abe4-4728607e9ca0
7af269f8-c25b-4137-9e6f-cf2885a53458
c8ca5bab-a0f2-47b0-bb18-c249a62be185
99153e16-dea1-4058-a10f-c88bd0285735
38939a4e-21c8-4f54-8235-3fc0f501191f
385bb460-d6db-4655-ae92-87d217153875
046da48a-8c60-44f1-a7aa-63ed6fc8607b
b9679059-d627-4f08-b197-992bfc5da751
43ad5aff-1c02-4b56-93a3-becf6e3a903f
8f6462e0-9cf7-40d7-88f6-cc100735a55e
036797ce-0802-402d-aae6-ddb7b372e906
7c6dbdea-e867-4345-9d03-bdf5b3f45936
5726bd4b-9cc0-4087-a1f4-39a50925ef98
f55d2107-06eb-4992-b24d-23e35efdc0bd
063d577f-f57d-469a-8ecf-f47f5b839d10
cb396012-4148-4e92-bad9-04c7b3426cda
7ef889e3-377b-457b-a543-638a49716137
093b0c1b-9966-44de-9da8-ffc724788dba
e9d45080-18ca-4c31-abcb-65873a345df8
fc494497-9623-404f-9549-15da0a0d2a9e
cffc9bb7-c755-4c7b-9803-1edd990d1e97
524d6626-b2a6-4b28-8495-90f7f4d8102c
f0d42881-819a-4a40-a5be-4032fb549022
6ed7c6d9-fe9a-442a-8363-c9deaa8067b4
250e640b-0715-4c2b-9408-668182d5e264
e6b49dc3-15da-4ab6-bb6f-42158d7756af
c03bf87d-f957-4a4f-b231-30251ea7093d
43304528-a344-4009-b4c8-65b1642bec81
5754dc5b-1bf9-4191-9497-7207bb8de5d2
6f266fe2-7b01-4e79-8995-e7765acea6ef
536f4d6e-89c8-49ad-b1b4-8592dc4d0d19
c5f3a78f-4dc8-4525-940c-ca2c785d8ac3
0ac7fd87-2080-40f2-8366-b429c81f51d4
42e3b341-efa4-485c-b302-5f0a690f6938
ba47bbd0-cd7c-4a58-aae0-9dfbc1081296
a97d9449-1a66-46a1-9dc4-d320bdfa81db
27b67759-aec1-4ae6-abe1-10c4080c334e
d681f5fc-5692-44b2-995a-d51ef01b1693
b17ff137-7fd6-4c61-a187-34acb230ce19
a5559b27-d71a-4efb-b2b0-21de7d08b2e1
8e287fff-8f61-4d8d-9ea7-7cdf235676db
117362fb-c829-4b2d-b0e6-f98675702ff5
9b87ab88-a4c4-4f9d-97be-81dec47fc391
e6273be9-95ee-47c2-b699-51acd38c7663
54ab0476-7f33-4d59-95b8-a2a62a3aa88b
29be710a-0fc3-4f10-b299-f6fcd4408108
9a6444d3-3967-4906-9ba2-8e0a0b6dfc18
cea73127-bc45-45d3-af1f-0fe6707792fa
b72924c8-3ca2-4ee1-8075-019fbc11a5c3
cf492312-85cc-48a2-9839-13c61591b484
1348f9a5-f35b-44a3-a473-df17576adb18
93de51dc-0654-4ee1-81cc-38ca4016e763
b2e1acf1-50c2-4310-8b35-a5217a6cbcae
d50cf11d-b5bb-426d-8885-419519b8935c
4c727eb1-9e09-4fbb-8ad3-934f76a493af
c91f616f-ef58-41e6-abb4-4c9c2c9a30f0
6c47e8dc-c835-4f45-ada5-a8a1f18c032d
efc5c07a-b66e-4233-8de2-af876d431db3
fbce7bba-38bc-4652-a65b-49660ca0062e
44916fe7-474f-4aa8-ad5a-cf710d6efc36
3fcee6a5-54d4-41d2-90f3-8c229f9812cc
b21a77df-b7e4-47d9-aed7-950205086df6
221f0579-f67f-45d2-8ca0-c5482bd6d579
4a020b8b-853b-42f2-9b42-5989958a77fe
78d7f8d3-8e5e-464e-bd9e-0fdbadcefab1
ce4ca766-b0ac-4f9c-a08a-c1442023bdd9
1d0878c3-6bab-4943-b914-2490c4cc801d
9bea3c35-adbb-4270-93ea-9e4ddbd97d5b
2e9898fd-27d5-4f9c-a887-28aab8c7df3d
e29d2922-1126-4ad7-9c87-4065444e8a9f
f42a3af5-9a35-4c9c-ae27-e6600764f4d6
a8445c76-1bcb-4ebd-ae0f-8c4b7d322fbf
7ee57451-de2f-4e62-887f-3df945b0080b
bf8635ca-2760-4c62-abd5-ed5599835c77
872c814a-9363-4f54-9484-8b9f19407fba
9563327d-ba02-4b73-8699-748fa1e3a0b7
887e8346-a76a-4756-b093-0261c10e14fe
d5114100-c60f-4671-b562-7cf04acdd3e0
b596c22a-0904-4a12-88cd-64334af8ae0a
02fdd8da-5d80-44c4-b139-1d1d04217380
d2983d72-4861-4efc-8739-03762a2aed77
3ebef8b4-54e1-4468-9e9f-1ec702c4532b
03c32302-4dc7-4596-8e3d-d6f1b170e4db
000765d9-4ce5-45d3-bd3c-5355e15f34cb
663e3460-97a3-4b14-8fc2-8259bf57b6ec
7694dd85-23a7-454e-8771-5153daee7e6b
e66725f2-e0c0-4150-a0e3-78dbbdbd9a9f
72e6e47a-f5b5-4587-ad77-50d6e1552d12
ea3c6b7b-df54-4f9d-a6fe-2db835deaa5e
d93f2b81-c6a6-4a7a-ab4c-5f210b192751
7608e870-37ab-4617-accd-f92fe90b7151
675afe10-382b-4b43-85aa-0506ec9f555d
22dfbbcc-d2d8-45de-a721-7dd5ddf09415
651394ed-4242-4df3-9a69-b76ca3d36214
01544a58-58aa-4594-b8d3-4d7f58ddefd4
6fd3cb33-9e87-49e3-baad-557964d2f64e
66b4f79e-7b6a-4e4d-9822-22b608be0930
fcb22a69-2e6b-424c-9af4-88f00e9ae999
392e18e6-6d49-4817-891b-788ba812b5e2
0e2f44af-39be-4e6b-8878-e1bbd44b59d3
97de39ef-5f96-4ca6-b06f-456669c1ec5f
9b848339-9866-4adf-9bfb-0cf90db9f8dd
09f8fda2-e43b-468e-b184-1d0ae2759002
b5288c5d-1cf5-4f0b-8d1e-d0b13a8fe319
9723621e-0012-4904-9c51-f2f8190fadd0
6b769ed0-1b1e-427f-b199-dc9dad29c6b0
be74e13e-5bec-42a3-ad4e-93279d92fb87
423feb79-7eb8-432b-bdbe-69db0f231c25
c59476b1-4927-4566-a516-71ee4afe002e
cd99e285-852a-4f0e-9e57-196f2bd4f6f4
4f53e035-8417-4e6e-ac69-417baad593c4
db403ef8-498b-413e-90ed-cc4365aabe4a
8e252abe-0fb7-4061-8c5a-6ecd20ff3606
9c701e14-325c-40a4-9d85-7fffbfe5d62b
d73868d1-9492-4beb-8816-bcf672d96071
7e69e71c-cb13-4b01-bdd6-1e7ac0575a46
6cac5bdf-ebc8-4b0c-a4ca-2bd422a7ca2a
f2150316-6f42-4fd5-adca-b2290dea74c6
9a9c52fd-6123-440d-a6e8-2387f62177c9
de98655a-769c-405c-af59-62a5a9363dc4
0ebf9d05-0765-4361-b5f4-25b2957b71ce
718a4e3e-799d-4a49-90aa-2b256c971ef1
f0f7cf9f-8c48-4266-bf41-24d271ac39b4
621a7933-1901-4479-aeb9-cc0d00ff8430
e49ee095-ead2-4293-b05b-f85c8782e137
26727e6a-848d-44c8-b812-a07c2ef548db
b50e4677-df1c-46c8-a1f4-21ae69bb2e83
a9b3411e-f772-40c5-8b0c-4b698eca4bfb
79db10d3-bd67-4d1f-aa76-1b22edfa7b6a
cb34fae5-248d-4217-9b35-39c4831725ae
2215ffdc-090a-435d-b3e5-d273f6ab6375
927ffb63-20fc-421a-a82b-79d072b8493f
dd01823f-c39f-4f7c-b308-035d17a669f8
39061a19-2bf4-487b-9b9d-62fed4eb9ad4
1bc4b405-0bec-44d4-8f54-6d302557f579
8c356d09-4c48-41ef-a4d8-1ba73a6c5b23
57009282-d376-4160-9a8f-00f293365e7e
2db06b82-034c-483a-b58c-61f97d3446dd
847130d3-61ab-4a74-954c-d3644cd5aa3c
1ea2bd3e-02fc-4c01-9e50-6d77aef79c79
12e700af-7d36-4ccd-bffa-38b7330cab89
c9de147c-6784-4b33-b037-9968905916c8
702fe2d6-e4f6-453d-944a-e3b668da29cb
b45b57c5-9328-4fa9-a9e8-bfc755aaa04a
9da829bf-e121-4778-b7e7-0ad502072fe7
ffa32e59-840f-4719-a4b4-9b6617c5e094
27469f5a-9cf8-4f50-a347-cc967c1146df
6aeab91b-aab4-4fe3-9007-56452ea5a547
22e540c3-5f90-47a6-89a2-d092e6183169
05b10446-8795-4ae5-a4d4-0b5327193560
6a895aa1-7741-46a5-91e5-8ad1783b3633
6a9694a1-7321-445b-acbb-3234274e4149
9d067754-277e-4a9b-98af-8ffadd4d5c61
89629991-7b66-4b61-8e79-0ce397fa497d
e5688aa2-e344-43d4-94cf-29cbe0cf2408
bd5f507f-e138-42ee-9e1e-1542251137b4
d2390a9c-20f2-4934-8547-b9113251379b
a3ca18aa-5a4d-4e4b-9045-0b7530da24a5
67b25cd2-390f-4a6b-9f99-bfd59176ddd0
efa4813c-7a04-4748-a532-617cf22b1221
1f668d98-f463-4687-bbce-2390ee4ef113
9d9d4843-9f29-4dee-b0a9-9a127da64f09
822af2fb-6ae3-49ef-ad97-31807bfc4305
600cac76-e5fd-4740-b903-a9f369e07353
776295a9-8218-45b0-8dd8-d653ca3a6521
35f2d833-fdd1-47b7-b10f-1f9a030e6430
b0747543-32e4-4cff-a857-f31f8cb681df
d5c65099-983a-4519-bcea-91924e0542ca
4fac3f2f-796e-45bb-b97b-72b99ffd0caf
76da4c30-4ea6-4dfe-b3f3-7c7bfe72dadf
1fec9929-057b-4e08-9cf9-94e81f0bfa61
5b6b2224-b312-4afe-9dbc-bb659b4d8e6d
63042302-e182-44c2-8078-e5006eb21645
b5ad1ef3-4b5b-463f-ab55-481d5d1798c0
fdd1f6dd-bd04-4090-95c1-e7a7b7b7daa7
21ecf720-2c38-48dd-811a-42bd1ee0ae22
f1f6e362-7bab-48a6-b2e3-8dbc225a5b99
29eb762b-ed07-4f80-9db5-b0980bb035b9
5d57884b-b3ef-4bc7-b149-e6a96ed8cd36
71e6f452-0687-46e3-bc7a-95bf73b49715
f679b49d-8543-496a-9691-137a26fe569f
980c32c2-e074-4aec-a579-cf13056153d6
324bfd4e-caea-4059-bf11-fe6f284e6cb6
872782eb-d7c3-46f5-ac2c-4dd4c2827d39
2bff710f-a12c-492b-a01a-c5a961ce9bce
1fda4157-81e3-4eb0-85ee-d5b479d7a408
23c0089d-d362-417c-8bf4-d9e6e4c7ff74
75b531d2-b9be-4b69-b26e-585f6a26b3ff
1ad30e66-c7a9-4aa9-b7c5-747249eb1f68
ccdcb679-ff4c-4ebf-a8c4-4e67e72cc887
15713477-9102-4c40-b0ec-8d6cd2c4c54d
913d8526-fa18-437a-97c6-49afc4de8776
f6be09df-c4d3-4c7b-949c-ffeaa5b1c9d4
6f4327e8-d208-4607-9e76-9e83ebe0583f
f46aea9d-e9fd-4e55-baf1-3a9148e518c4
3b1f1186-12b7-4674-92b9-692f067c65a7
9e287ef5-cb5f-4c84-b94a-74e37d5b93ea
fbb274dd-bc32-4385-91da-96447b82e776
dff692fd-0be5-4fd1-8e45-d103147e8279
1b8ff4e5-006a-42d1-9c80-565293af523b
0a2a852b-e99e-47e9-9e18-242d24145ca7
9520a0f6-fc02-4a8a-a9c2-4176691b5485
c8db6de9-78d6-45bf-80cc-8d4be73e6ce2
3c0e0d31-9b89-4ce6-b25f-d1d6fd916102
c4a75c90-df2b-4199-a897-3818b1a876bc
ae993569-2b20-479b-a767-114a6cf9310b
b4ed3a93-25eb-4dfb-b348-e2fdf6293bd1
f6ff199d-9b08-4503-81a4-47b41bd2df79
aac2e089-b872-4444-bdaa-d88f9063434c
59be1ebd-832e-4781-8ac7-ba7f7cc91ba2
d2ea93f7-72cd-45b8-b133-1078bc553f3c
8e65a937-d251-450b-a14e-ac8e44f15fde
d7023a15-0b77-4be0-a08c-441679610877
8987166e-9699-4fd4-aaee-72be13e71d9d
7701969e-8468-4b92-90ed-968d9d083ab5
dcdc8703-64d6-42b5-8ef3-06e0cdfeb0e8
82c9f406-8206-4356-aeaf-4fd2f7543a4f
d8734dcd-b693-41a0-89e5-3d9a7ab3e347
879e33ca-1d9f-4404-9452-fa8810c25d23
c95d8d98-990d-42a3-8be7-86f62c939e68
f60f6ae9-ae1e-4b68-ae7e-34465d6d32e9
14c93e7c-b8a3-4a06-b7b7-21039001d159
7537206b-79f3-4b1f-b17d-df44297c4ed2
046c86a9-39fc-4f7d-baa2-75c5c8e3657f
258615a8-9379-48ac-87ff-1f9ae9fc37af
b8856bbf-6b4f-45ef-abc5-b67ba2ea209e
6121cd6c-e6fe-4dcf-ad4f-14c3f05928a7
46c2a220-1f90-4244-9084-c9c33cae417c
e5ea9104-c7be-4fbe-b071-0bea2e78b974
919c631e-7e55-46ec-8636-23d9c010ad06
14527cbe-9908-455d-b35d-dd8e42fb61ad
a65168cf-12e8-4f55-a2e3-c0d7284f4b60
f1431dbb-908a-426a-bfba-24319ab44b29
2986dd7f-cb7d-47cc-829e-92d307d3db17
7bb62c8a-640e-4f16-8a4d-6d843c10ac55
022f08d1-f5a8-405e-ae2a-4af850ab990c
362b40e5-655e-464f-9619-281c02de217a
382b55bd-0b33-43cd-b1a0-e287ba791d4e
a84fe643-d2e5-442e-8016-c2a644724958
d9e85bc8-289e-443e-91a1-1f01662e612a
5ed5687a-9ace-436a-9f74-56913923253d
3f4d12b7-fd67-49bc-9154-357218ef4856
69002367-ff2c-484d-82e3-b20dcf554a36
35122008-79c3-478d-a570-3ca948fa8293
b581a464-5534-41af-97a6-2af33b117cd1
a6d99f1a-a5f9-4c23-b628-8afd913cb70e
c121acfb-144b-4b2a-ac8a-258705c67437
d42f07bb-2d3f-42d2-8d64-fddf79db2f82
b185474e-fe8f-4218-a266-6f1faa7086f7
70d65539-5135-4b7c-995f-957ff97b92c1
f732280a-1d8d-4177-8432-92a73144b047
b93458e9-8c85-4b69-b5b1-36b7834de447
a1d39e3d-bf1e-46ed-8bb9-c1165f67127e
44b06049-00a4-4559-90b3-7805ac6d775a
20247251-c1fa-4404-9bf2-6b927949173d
f9ef156f-78b7-43d0-aa34-cfedc43db6f5
0efdd0e8-8641-4953-a0e6-1bc623a22293
8728c724-6066-4f62-9609-7d70a3a4d4a0
8af50ebf-a549-40a0-84a2-3a6820e5d666
c64d23a3-267d-49d2-8234-fa4426c6e1b1
8f50ab60-095f-454e-8077-d3d316dfb81f
6f27f30e-c21c-43eb-8d71-1e9cf4d17d16
53ce95b0-9e47-4c75-b647-8982b80ddc8c
3d9ef7d7-bca0-4eeb-8550-eeb473724815
295e8241-848d-4062-8c47-4a81b8611d86
aa21195b-2153-443c-a3de-76c7e130a7b3
b54ea94e-1d4f-4a5b-aa3a-658bae00b059
7f90a117-0473-44b3-90d6-2e6daa50a6b9
a607200a-ed76-45a2-ba63-9995187f1524
21fca985-ddc1-49e0-ad30-4aa82ad73266
758cb013-50e6-4710-8e3b-ac39715d2ec8
567224f0-7590-4dc5-a04b-e862fc7f7ea8
e19ca319-2e67-40c6-9d05-89fd87f9d620
caeaead0-43f7-4c6c-906b-5abfe612ffcb
fc0ab996-b6a4-4f5a-9ef6-a1b90d2758d9
1c10a317-1905-4178-9374-99662a87363b
9f4749a8-3aa7-4a54-92eb-8411e2e20c9d
5fc40f9c-6028-4499-8aa6-5993393cf19d
b08a3846-6563-4b78-b5c2-8648cee33795
81e32b37-3cb4-443a-b0f0-20729bc462cd
40fc2529-c4fd-485f-81de-45ee666ff4bf
e850a141-f760-4fc3-8616-d054c8bccba9
eea95a54-d056-4a2f-a4f1-7628881f4a08
e9124db5-fc0a-4087-abcc-277a7d16b67f
596d5863-a350-4acc-bb2c-f3b06d4f8ba2
c1b8284e-c59f-4fd2-9201-6d067022a916
d47614be-873b-484f-83e6-d0025ffda75c
37e21a37-6ab2-4995-b3f0-1c385eb3a678
6cfae7f2-f67a-4307-9e51-290450073f67
9e0ea57d-6906-479e-87bd-1ebc521d8698
7c119f03-a737-40cd-a384-41721cde15c5
4e6fd40f-9c49-4fef-9826-eadbc34be337
a19e8a46-a8c5-4653-bcd7-de58ce34a5a7
5c65fb2c-12d8-4bf5-a06a-19604e634675
ee87d0a7-5d1e-4216-bdf3-de2ab91959e6
b70c093b-046f-4bbb-abe1-4f8a8d0113ef
e73c1587-1453-4257-97ea-d88e23de672b
45eb5ace-e839-4c83-b5fc-06e28b3cfd99
10a793ca-d348-4b0e-96d6-3cc4ef46bc47
c6c0854b-5374-406a-bf39-e4f097041857
b0c703c3-c120-4840-88e2-8ee544973a0e
e2ae667f-9730-45dc-9bb3-5d7491a9b6bf
720b4322-2ad8-42f5-b04d-f3722cfcbe04
4b1a8eb6-3ff3-40d8-8170-cda909e0523f
1112e2f2-93d8-465b-8a37-e1fb9c2c4acd
3d20c854-ff56-4b05-a56a-4138d278388e
91a58008-cba0-4d0f-bf4e-2625b7aedc5a
47135032-3a79-45b1-871e-e5e99e2a1505
f0d61255-9bc6-4cb9-9920-4b288c0e063c
536eb209-af8f-47bc-b555-82c29c037958
cfce97f0-a1bf-458c-8c10-eac314f135b5
3afda381-7b2e-443a-9ee8-aea9e2e63f37
fdb4afca-9670-4336-b7de-51aec827a229
6c243b0e-e156-4931-9d27-d0738f1dc0db
f410fe34-e5b4-45bf-b3b7-6a5e8fee50d1
f5dc56b8-b124-49c5-8bc9-e2e4222d991c
2e6c07af-9b4d-4dfa-a416-61705bc7f0a0
b02a0e8e-7136-4243-bcb0-76156fa411ed
3e41fb83-e05e-458d-a96f-6b5fa346b0c8
866d7189-a13a-4256-a2c8-ded01cad71b7
8600338d-6093-4833-9b10-2b06bb6d63e7
1a112200-d28b-4a26-9407-380e1979b395
798acacc-af8f-4656-969b-648bef717841
8ba6ca5d-4d53-431b-a998-2c4b571c615c
75ea2ac4-6a18-4bee-b06e-f8d5fb91225f
e9d39670-d737-4e96-90f3-b29d158298b7
df9a9cc8-3c0d-42f9-a5a0-ac8d50c15460
469e111f-9034-48d1-a242-7d9ab347119e
126e596c-70d2-4b8c-be7c-de724a2d242b
5a1ddf4e-b336-47b5-a0ec-2b8fed25f775
a5682d72-3bc3-446d-b7bd-6fce2b139b2d
8b72d475-6dc9-4efc-b7b3-457aab59b782
f2efd917-657b-4b1c-9372-e114c6dd3a87
5ad3b5bd-c906-4784-bae0-fe046c81f7f9
10b82a4f-5ec9-45a9-90f2-22c2f5c9e94b
5b507274-ef0e-4d6f-9d2a-61caf9138ea3
7883be03-ede1-4860-a6b5-0aa89c9aa6e6
18dac950-1431-42a7-9110-ed47fb2cc6ed
49c1de4c-14aa-464b-aa4d-c02aca167e02
c9e8f2b8-f6a1-4c7f-a2f2-ad8a5b80cf29
3d6b295c-3ca7-4b8a-901e-bbc945f799f1
bd9e0d32-6642-4eac-80f4-9415749a2771
5686578e-d097-4b30-af29-91066b45d036
bb354908-3afd-443c-990e-1ffe185543af
d4461e82-5a87-4491-9ebd-2668f7c307fb
ac39893e-43dc-4882-b962-4d01b4a4a0f0
f6bf498e-b35b-4d00-82cd-86b06e37ba6d
ef36964c-ecb3-471a-aa44-58b7c73be3c3
5d8d4de2-abf0-46b7-9220-376c9815c643
1f180e03-4f02-4f27-86ea-a35344fe9565
9a8dc902-b509-4ff5-bcab-270f998c3845
5dbd5e06-f1cb-4c75-9d6a-8fa1cd3327d1
f10e7eb8-842a-4101-b6d3-4297c9c67248
a44d6692-d956-409e-bb14-01695d29b3c1
2844f18f-265b-473e-869e-b868a6c3d852
7e2cb1ea-2258-4ee3-bb51-d0394513fda3
cfc561a0-6b5b-411a-a630-89a187dbbb95
4564b261-0b9f-48ce-8353-26f57f866dcc
a60a8020-bd8b-4247-80d7-44ab55ac248a
79ab362e-873c-4295-96ec-db201f9732b1
6c9c24b5-5757-4295-9bd0-1d47e66f35c7
e3b5fe8c-e23d-4bc5-9b13-233aa12e52d9
14824e1b-7d4b-4bcf-91ea-52b1d5222dff
a00098c9-4ff9-437b-9e1b-0695703517a2
ffb863d3-e8b8-4016-b969-181ccde1f772
c0f4fcc9-2244-4e93-b2a1-984ac9b89312
9b429f0c-1a8e-436e-87ae-6831aae43d16
b5563aad-2cb5-4667-82b7-89b570daf16a
4eddb41c-18e1-4e3c-928e-3002806affb6
18cf4863-fde0-4373-81ba-321d00669b9b
6c492021-b4cb-4183-a458-99e473392ea3
c7bb809a-7974-4c0c-86f2-515d89b00278
64463291-e766-4458-a59c-ae369af0d6d8
132d5338-acc6-4995-814e-321c7c6fb97c
e69b5e94-52c8-49e5-a223-58c82caad85c
33cce9e5-c661-496f-ae85-38bb32d4505d
95d3a9c1-fc7c-4fe2-a8a1-e67670b8bca5
e4146325-e58e-41d2-853e-42cd81a651fe
95ca965c-8828-4dfb-91fb-06684a17fe80
39120012-8e7f-4a72-81a8-4cfaf77e080b
f975a69e-ce32-44bb-8ab5-4bb774fd6211
f3601d06-733b-47cf-b076-48db7b5bd4b0
7e7b3821-d6dd-44cb-8dc2-437969deb755
171f139d-4759-48c2-9adb-24b43d465b1b
2fb9d4da-b473-4e93-8fc1-e01d375a61ad
692c432d-19c7-4d4b-b9ee-3119449df8c6
26a9571f-6670-4a72-bf6d-e687e130c534
853d04a5-a6cb-46b3-b21f-b4d1708f4794
4bc6ab28-f44b-4d89-bfb4-81c4aec01c3c
90b76f1c-c47e-4699-b466-1e5f869d0d42
96f278f7-cdd0-43a0-95c6-68d5abeebeab
bd4368d4-d8d0-4309-8fbe-f58bde5f365e
5126c319-a569-4dc0-b2d6-63fda61180de
62d1be06-d9d7-4979-b92e-6ce4591050b1
a27e2e7f-a27e-4351-b5ce-dc0ef8ec53ea
4b35f36a-6bf1-45f6-87a7-fce2e3f5dc23
cce267b6-a5f7-41f2-8cce-cacc003ba0bf
e41c620b-cf08-4dfe-98d3-d89f97ce38c5
ab856835-5ce1-4c7c-8f3b-9229eb55919a
1eb58bc1-5242-449b-82d2-d98ca3612b43
3a16c6c6-be1d-4025-bb6d-1749b1418661
cfbd0a03-ee7f-4d94-87a1-a9fdca69db51
c3dfba75-60b9-4631-ba62-b9729ccd9b97
c6c55288-fd1b-45b6-b013-01c4e4a867fc
8def03aa-e077-4ad3-9e55-b3b22546ea2a
ca977f00-d439-485e-b067-86f570ef2032
8cca9739-6052-4e21-84a3-a6882f95ffa4
e747875f-be88-462e-9dbb-49c2448c969a
9b7454fe-3dba-4aee-a7b1-64aa7333e338
18b06993-1b07-4c4b-b2b6-c3003f07e2bc
97ae9f7b-4cc7-4361-924e-08fb33945706
5edc75e6-ccc0-419b-bb91-3f0f95f1f50e
db9e785a-1904-4489-8c37-8a9488c7293a
1b93d32f-43ad-4a13-a459-b9f88290e9cb
e5a78264-c6ea-4d49-bc5b-28c18eb6629a
b27db2d1-be8e-4c06-8c99-c1f61d0b41b8
2c38f630-9b24-4a32-88db-dd9785a29780
89beac0b-6a1a-4855-92df-1b3e348613d7
dd27eef7-0f71-408a-add3-4f6a612264ee
b93955f4-dfb4-4f2e-9175-212e63146478
b707f4c2-0c33-4840-bae8-491634102df3
0059019c-15d7-4458-8a43-1c91c5d86823
17d89936-51c4-4759-8ed4-db26c435b854
42bd73b0-76e0-4725-b8d1-a05673499ef9
82643c31-62a5-4a43-8860-491c95025403
c9da2641-15c5-41fb-950c-acd5966f995e
4deaabb8-3435-4360-8392-4cd9b778d4c3
9eee4f4e-f6a3-4dc4-8913-376741858b01
2f0f035f-a0f0-4191-9737-91f35f7489bb
0b06ab9a-db6e-42ea-993d-72a2ead21040
04038d0e-78dc-45db-b3bd-463983baa003
2af6c78c-1a89-48ce-ae6c-dc9e7c5b6757
da1a9269-fc60-406e-b54f-80a0d87d3350
4e6ff67f-28cc-47f3-bc8d-205ce0fa7be5
cc02580b-0159-492f-a825-fad5891c943e
02805605-4d65-4a02-a933-915b9828c37b
20f159d4-2c73-4ce0-a358-7572de8794be
540343eb-063c-4380-8577-8f5ec65c8a0f
159371d3-51a4-49f0-b68b-5e89d559d418
a143c448-cfed-4e66-901e-3574b9e06eef
d88ba1b7-7e5d-4498-a486-207f0626f3c6
c96ddc80-b5c5-4ac8-8949-746b9c3e573d
b5b182f4-518c-4351-ad87-84baf404a51f
af4b1271-cca7-4f2e-b9aa-1ee2b249b0ce
71370d97-e601-473e-97e1-451705a1197e
8d402420-cc17-45ad-8d65-21c1ce8b1297
0887f892-0810-47a7-aa48-4af7b40232c0
2a76cf75-6aa9-4b02-b536-d34469b22c03
2f2e3b3f-6e0a-4f4b-8011-2ec2f6a7943d
8bb9473f-9c07-47e2-9401-dbadf5a76b2a
7bc07808-6aaa-49d4-b3f9-645bc868cfd5
df274a38-fb03-4169-9dcd-74c6306d8dbd
8ba455c6-2946-45be-8410-7bc840ae0bb9
c8d837c3-309f-459d-8c7a-69062903590c
04ecc339-37cd-403d-9d91-f31080cb5cec
935ad674-6d42-4fe5-88d4-443ae7a4626a
47945a96-f130-4f1c-a6b3-bf4ee38d45bc
b894cfdb-9bbb-44e3-b80a-4e347b14564f
48a36e32-8bbf-4a3b-b9c3-6aac14c9f6c9
8e08311e-d626-4cc8-812b-e4003699285e
c0fe5c45-26fa-40e9-bcc6-60554a0228ae
da746754-f2bb-4253-bce4-e4051d803d32
17871e92-6551-42b9-87f2-bb73c4b0415e
c429ca13-ef8d-4c26-bb18-f472d6d3c4af
4c8801ba-f17e-4e07-aeb9-6713e296700a
f0e7673a-c79c-4a7a-aa42-098541f66b9e
30fd07d3-a4d2-4b53-91f6-af02b27e710b
2fa09961-8913-46df-8128-2b4a7a0f51bf
fea83ee7-98ee-4785-862e-9c9fc9cdee7f
e713e54e-5d8a-42c8-ad09-1d20aa8feaa2
87c0fd12-c0cd-43ec-8dba-16dad0588dda
5be47b52-5d0d-444f-9cfe-967b3cb6de9a
d1d78fb2-f679-4b92-9e1a-f0e6bdd1cae0
1b06c5e4-4be2-4fe8-8d7c-e34dd85e0ccd
9afeabb8-348f-4f08-832c-60619ffd9359
73e1cbba-2271-44a0-92bc-ce142cfc17ba
70a2de76-bdd9-4624-8d97-a02e84dd47c1
56e46bfc-f0f0-42e7-9d9e-7c0556b6d913
046aa6a3-27eb-4303-9c1c-3cb4892ced47
bf139bb6-de65-43bb-8026-d332a8481db5
2bad9d8b-d603-4d37-b04d-a95cc95656a7
4bb9744b-3027-4f2b-9f8b-a0c9f52c49ac
05922aec-9a10-42d2-ba30-78d405da7cf2
e55e9150-0072-45cc-8c81-bd0ce2adb269
f0ac31b3-8fbf-48e8-bd5d-2feef0e56c48
2953a881-fb63-4757-a5ea-e856a07c29ae
06443333-f983-43bd-a43d-4a533e34661a
0fa8599a-fa44-485f-bde8-4ce6b502fa8b
0ccd8784-b3d5-4cd0-b6a6-8d3c6fc4a95b
243f9a74-8733-4419-8642-cbb541435377
3f2bdfc9-8be7-47b8-b63d-d83e8392901f
03941621-bb9e-4d01-888a-c6a4dcb2be34
e9e8a109-d444-4a97-ad8a-2b2c22063464
616d74b9-e5c5-4995-9d9e-55d8c71bb58a
ae25f2de-d4da-475e-b741-0be9cc663f17
967f048d-3929-421f-bd56-52d8dc721c95
3ff6f3a1-14e5-4368-8b27-0b0af54afd1e
5f54c9d7-bafe-4d18-9d9c-9002262d3f1d
decc8fb8-d71e-4856-b439-3e6dcc04ccd6
4f37adad-6523-4e42-983e-b74a9f8b1f1f
86e737ec-ecaa-4dab-96aa-a377405ba0b8
1e54d7d7-2e00-4f46-bf9d-ee94b124dab1
c0dcd6b2-9a6e-4f28-b688-0196b5d74d2c
913ee801-516c-43d7-b6eb-e0748c283c3c
2517e0cc-1790-4f0f-911f-27a1940f22ef
55e0e718-73e8-474c-9490-94b0bca0b7a4
b9356a25-3081-4d22-b972-c4e81441ed0b
f0a04375-a747-413e-a1c1-6cea434eca94
fea08e6c-f3ba-48a3-9696-d814b0306f76
d263198b-eced-4c0a-8f89-f84fd2157c18
bf550418-2612-4af7-8989-5462a94448c4
5692fdca-50f9-4bf7-8e86-fe021112cf9c
de910156-10e6-45fb-935a-0011923caeba
ddc19ac4-db7a-4ba2-b662-a61fc8e30bc8
81275c68-b33f-46c3-8a09-513c78c18e93
b40d5db6-1e60-4ddd-8c56-9aeec175e0ec
2b0658ca-cb02-44e3-9db5-82866b24b5a8
54f12ef3-9761-4671-aac5-609a62b59df1
6bce4cc2-9e4c-4510-bca0-db853c3763fb
a3183b28-affd-470e-a4be-48555d51094b
03c2f4b7-2ece-4366-a5b4-737214d90f09
e12f68af-c5f5-4295-8273-d1c20d54d181
e30a1353-884e-44f3-a9b7-dbb88417cf42
3a37c16c-21cd-46a7-845b-8f631c317f1e
4876a466-02d3-43bc-9f86-74e407129621
510463eb-eb0c-4721-9cdb-21e9d6be98d2
20999826-6b1f-466e-98b4-84a95fe97af4
f361c17c-5250-4a6e-a1fd-160d86d932b0
5b7e5bd8-946d-4a49-b8d0-28586624d2d1
ff96a954-67a4-4eb7-8846-d2e4713fd3f4
3e33bc6a-3295-4d15-af8e-d631a603754d
fc0fc459-12f8-4fa4-b01b-a56b55c7fd2a
be1de3b8-abdb-4ffd-9289-2a26e3013b01
3ef787b4-0961-4654-ae65-11bde0c2c3a7
0dcd3f78-cfce-4a8f-95ee-85d88e6193cd
93dfd520-d6f4-4c9f-9f3b-10341b816acb
69852520-a2ff-4d38-a08f-6c0bd3f8d536
90ebf578-792b-46b2-ae62-3fc762728fb9
2bde9408-3bd6-4020-9818-34c4b1d8d5d6
1f0e04db-2229-455b-838f-9261a1185330
4ab29b2d-6aba-4f8e-b366-b78eff66448f
c2a85da4-f934-4805-b9e4-100cfac11ba5
d13fea5c-aa0d-4355-8550-b6409ec6d933
47f7330a-a7bf-4b18-a625-efff09ebec2c
7eb97162-3b1e-49fe-9037-822988c72d59
f52a1229-5d5e-4eaa-87e6-fe9e801e86ad
b3acca05-5f45-48a1-9e02-bb1bda79136e
f3075319-6b22-4651-a602-e4af55977cb6
5607331b-b86a-4317-9b0c-82570a9c931c
5f463c46-7ffa-46da-94bc-2d54f4749a86
f03d4793-ab9d-43bb-a13a-8b72bdf967c3
3c812531-6f9c-44d2-a1a8-431e7780f703
c5d520eb-1c14-4206-b907-e3a4d9fca3ea
150c7325-ee2b-409b-9c1c-b758e861714a
4fa7b9d6-9138-4f97-bad4-adb216112928
8db8688a-8f18-4a75-8fb7-a68c9a7a3bab
fe025171-97ce-4072-92fc-fdbe63b50723
de68e5e1-489b-4efe-a53c-be87797394d1
b437be5e-9891-441b-9230-6b2b2af95eeb
0b547751-9dbc-4583-8455-55e181c310c0
27ffc3a7-0999-41e0-984f-4e95ccb6bcee
43d2ff5e-aae9-4eb1-81ef-0ac7fede0013
ee34c12c-c96a-4502-89fc-c82ecc50db6e
115756ba-9760-4042-855a-e32509555c01
d6f1658b-f652-407e-9e5a-8b6ca0d499d7
b9e0b92f-53d1-48bf-a817-4b754b5e1c8d
b7b9cab5-c056-4311-8431-dc6e7eafba95
53ec4e6e-d548-45a0-b001-d93f8dcd80ca
795e1a82-3da4-4480-9d38-e2790eb8bc23
d3c4be9e-443b-40e8-b03e-ce83e741e5d3
53cbc173-8314-48d1-9043-7fb9faebd338
5cc4392b-f7c0-47d0-8362-2177ba830b31
1e4f32d7-9587-473b-a870-c078df519b38
30f3c795-4727-489c-a2c6-322ffd82362e
6e22596f-b83b-4758-a207-9c914f0fd544
0efaaa53-a8e3-46b2-a185-79883e994902
7da41132-ce1e-457f-bf42-a42e095efb72
d3160c3b-8699-490b-9152-378892932f1b
95a874a5-fca2-4d1f-9255-2d9eed7a1f60
5f311b56-0739-43d0-a5ab-ea3b65131b5c
f9133f2e-52dd-49eb-a625-bc504c87e308
0d623e53-2e02-4c7e-bf65-cbd1efee01f1
5d713c81-854a-4baf-a556-5212b8f8c60c
35f429e6-8d88-4ca1-8a68-a75d84700729
ec7f8139-6dde-4252-b759-04ac38056182
3ccf8ff0-5db7-4aa3-aa05-5e0f04406deb
f08cba70-dfbc-493e-a9fb-18a8b3cdd56f
85b1c399-902f-42b3-bd6d-ab188168e99c
a86cf4dd-9819-46f8-8e2a-d94b476ef606
68e84339-5078-4508-a879-1521674960c1
23e40953-4f5e-4ebc-b7ff-bab563d889ef
4f89b313-3ebc-479a-8b24-d9805e24418c
e7974101-4ca5-4198-bd04-76e1282fe581
6977509f-bd32-4b4b-a85e-db429b921983
e62510ff-4a3c-4fdc-97b8-db480c62687a
1b204589-98e4-4696-aff3-437e303b8961
f4f31b61-52a7-4ba4-883d-ab9645c97024
49719bed-b760-418d-b40a-583e9cf20e77
80dc5590-92b8-48a2-ab85-ad26ea97d92c
f1bde3dd-b15e-4668-83b1-a0025ced2577
6c7f69ef-de1a-4976-ab13-c164c42b2c31
b01cf258-e9df-4b53-a0da-49c4c91d5431
6538101e-2000-41cc-8f0d-139f720d20e0
1e7d8db2-90fc-43b2-9388-bf821ec3cb6f
20e0164d-62bb-41f9-b141-ff8ef7aa1ffa
ec77c0a1-0812-44bb-8a79-82b3eb6011af
7680e27d-03b2-4feb-af31-29791b36fed9
0a7b1d60-f2de-4b6d-b79e-13bbabb0f3fd
ccf765ee-55fb-497f-aadd-5a01bb7c8d38
4d769168-9463-4eb1-9e47-019457d7091e
3ba118b8-6db7-4943-a9f5-5efd223f64f9
f412ea14-2a98-4d0a-b8dd-99c0be352423
2c243f91-f25f-4972-9201-de3db52f33a9
8d893df5-3a1e-475d-9a7f-174856dbcf3a
f3b55dba-d677-4757-8b3a-24c71c03de16
bfe64db2-85e1-4381-83ed-289669382698
52f0a058-86fe-4ff1-b5a1-af1aadfb28d5
c52473d0-31ff-49e1-b830-5e33c5053932
12a0aa17-71a5-49e7-a545-ccefc529fd62
503f0c40-3c24-4b81-ba2e-de745beeba5b
fb105e3c-20eb-4510-873a-b6b23f8cb480
74338f4b-bc29-4f8a-b8d9-551d656d45a8
9957bcb2-d27d-4c41-a1fa-95ebba96025e
3c2ae0cf-2b74-4c7e-8193-e877186f380f
2d283513-9a0f-4a30-b5c3-8c78e54ae741
dc57a59d-41fa-49d0-9724-15986d8cab55
e0b6674d-a5c3-4c41-a9a1-ca3c221c08f0
28ed0324-2a35-4bcb-83f9-fa75431a2062
c6b300cd-68e4-49bd-88b5-8a1fb728fbc3
a2be26c4-9068-46a0-8be9-803259316ed6
009f2e84-a479-4fb0-8556-dc16b5983c4d
3bdeabb2-33f6-4e40-91c7-1b6981ace9a9
cbe51fe9-3f44-4320-b0e0-16be56d1cff6
6e6cdbfc-f46b-42da-8d5b-0498c4f5fcfd
7866538f-ab52-424d-a35a-6880f642f85e
4fb1ba95-b59b-44e9-8e88-e5e0ade2b676
11804385-0a16-4d21-8cc9-3770cbe809ae
d8d06573-3d44-445e-97c1-9c8eb6b49044
bdc76735-68a9-49cf-8661-574b3be7a076
2f561521-4bd4-4bba-be32-731204017a33
594d7154-225e-4eb1-8e5d-eb57d17bf101
62a01de6-9e01-4960-9f01-854124255ad8
26c4af95-06b4-420b-a7e8-f9979c653e67
58b735e0-2437-4a06-a029-c95fe6cda5e3
4c9f7b07-6f2f-4ad5-b71a-1f54184fe098
d7137897-dbbf-4043-9c32-f8682834945b
941c5dd3-975c-4123-804e-d925c21ad21c
4724a15f-3650-40d0-a86d-5a05c37afb38
e82beea9-a8cb-408d-87ea-35c6a978694c
78ba1320-8791-4d3e-8481-0b5e04352910
4d7a4e97-b19d-4a78-b92c-1d865db42399
adb51a46-f552-4051-8365-790cbe8122a7
a6455702-c4f4-4d71-b2c2-c6c0d7fd43fe
5ee19a90-9789-4ce9-a2c6-d4772903ab40
1955c137-3c76-4261-b3cc-c32238b40f14
2bce8c0a-29b7-49b3-b494-673a19d7a620
b48f2204-451a-4722-bda2-41832f5f051f
184d29ca-6cc5-4a3b-91c5-88c785b803f1
266b1f0a-98d9-4718-a905-3c80919e633f
7990b983-fc76-405a-9a76-02abf80dd7d8
c2304192-6a12-4884-a226-27d32c768f01
8669141d-1a50-4716-b26b-7ae9075ba289
d8d3fb41-57c9-4c40-b269-75b292ac01b4
4223c625-dcf9-4803-9941-8956cd3afde0
09e326e5-cbe6-416b-bcd0-7e66f2f7b91d
54472878-219b-4b52-81c9-fc49aa6d4ca6
7ebd8188-fed0-4395-a539-4959c058ec6f
87b6cc92-c56f-47f0-9c70-0b7c30ff31ce
c187594c-8dae-47eb-917d-11dd11c38199
5e2770ad-d4ea-41e6-85f1-f6c12d9333cc
e2b3a234-eec2-4990-bc85-61e3e9787d0a
bb0c6ed9-4f9f-4a04-a9dc-900f1063c78b
cd79f21b-ffbd-4b38-99b3-80730a07943a
a9b52bce-bfd0-43e4-9db4-1d4952ad9880
64b310d3-4479-444e-8c52-f5797b90792c
434665dc-3658-4770-917b-fbed54e473d9
8c11b151-c72f-401d-86bf-6274cbce85dd
f8893431-baf4-43ac-abf0-50174d2158bc
7985e6c9-43bc-4457-9098-9f7bd940d715
afa0513f-5b65-4f10-ba52-1a35f94fdf0c
972aab21-c181-4ede-865c-f1593b34a92e
729fd450-c374-4025-807f-f8771d440ff4
bb30b6d0-f77b-4e26-a5a8-dfbec93cb1fb
c7fc9ac7-d3f2-4f71-9caa-be06678844f4
1a0ff5b4-cffd-4d14-b3b5-0af957173658
dbb6b35c-eea9-41c4-ac60-625a4aaa3361
793b6a7f-b5bc-43af-96e2-6f22536a9830
ad69dd7a-e6f6-4502-96cc-47ef2b72a4bd
b83bdcdc-4fd7-47a6-a590-a001e7e52d29
9ab7216b-88c6-46a3-aa49-9be9cfe55957
c6afa040-a505-42ad-a061-edd965a86417
2acabfe7-1aaf-44c1-a0d3-33a2a904872a
6db2a1ce-5d94-4288-ac58-51eebf4efff0
be22bc63-33b2-4abb-a448-3f70dff8fca9
3bf5dfab-e1ad-4a52-8665-542e9dfd098e
3f6c5eb6-f9ac-456f-90f2-103b21eee7d6
f78856eb-1856-4e92-9357-2d493e7ce56e
36490f05-de83-4b96-ae78-cb08700180d0
382bba81-6e94-47b8-9dd0-83754089b464
425f87fa-5e4f-4aa9-a4cb-e9fc59541edf
7acf03de-5e07-4453-93ad-ee770b862092
4dbd893e-9b58-45b1-9032-d2cd8859063c
db639b9e-6a35-48cd-bc27-c693e00fae39
5303644d-b2ba-4d06-bd34-9c6dadfd13f5
ddcfc198-6229-49b1-b8b7-b617b0c5a57d
0c1826d2-3b7f-42e3-9c55-c8243a97df36
8e81abb4-f10a-4512-b458-b812cf8a7f8a
94a5217a-1b13-4f82-b39f-36fd660559d8
43b27b4d-ea56-4e02-ad2d-3d6255232908
5443c554-278c-4110-8530-e8bd3332b16d
bcde7d24-5fa4-424d-9611-4a10397fe69f
3bc1c039-cedd-4581-a1c9-b0aa8e6559ac
aa69636a-89ec-4c6d-9d5a-35db0a2eb365
18d442e7-df2c-4059-9ff5-98dfd8f8603a
6a0ef444-59e3-48f5-a16f-19f59c3dda59
0d397a81-85fa-4025-bdfb-df415bdc027f
b112c2c2-3fb1-4a85-9a84-60b3c9fe88b2
2ce9f39b-ea9f-46a8-b76d-0d4e84582daf
cfa202a7-0aec-414f-b21c-ff5e224ec895
934518ba-16dc-4da4-9d12-b81c4d8e5f4e
767d2d06-c94a-4efc-9eab-3a75d1837dd2
09418001-0475-4ee8-9955-c029b120d962
cd43b143-7d02-45a2-ac84-975fd0118e29
e263e026-af8c-4a9d-b480-53a7850d82a5
ecbe6f8b-a71a-4984-b9f2-18cc64568f56
7f18148d-e389-4b84-9705-baa304c20b90
585cfe52-f86f-4f98-a2dc-68b2d921d487
f8754822-ab2e-474d-a152-5696281b520f
c3991da0-de82-488d-a4cd-787ea22f33a2
983604e1-4db8-4cf7-ba53-e1271975399b
6358beb1-5755-487e-be60-cbf501f52fd9
2cb09f90-7bab-48b4-acab-c8e444fc6de9
4ee1861a-827c-4658-9e65-9a90eff050b2
9496e6fb-fdba-4eac-9494-5c1c81699308
a2e40bb0-4cfe-4726-9874-a3f845b04269
aa269033-e563-47a9-aee7-61b79c49d716
31190076-e82e-4913-8a28-e45142d2d66c
87ba3a61-5dad-43d8-8ba3-7efded2a7fcd
0dcdc7ab-0fca-4422-a4bf-e99874b69425
49e3d1b0-d7e7-4757-93a6-e6a45faeff46
cba3c175-f856-49e9-a289-90a43c14877c
bd0fcfec-cd1d-4ff6-a975-1c42787ee736
c958b7b1-07b5-4d5d-a5f8-182798175484
70bd2aee-f3c0-4362-9070-015f632cb6d9
57f1bf3e-6f59-4dc7-b721-85f2fefca20f
bc4a47c7-ba69-4456-983f-4f67a0e76ecf
bcfad574-7bad-484e-b920-1c910530d3f7
6e455e47-6242-4656-9470-9d8ae32ea28b
8a1b4a25-72a1-4da5-b87c-1cfd2f0728bc
f230ecfd-3469-4724-9cbe-80473006ebcf
a9498a29-f58c-449e-8f0d-b4dd9693b1cf
9e5f668d-46c9-4196-998b-e27031db35de
0d62ebf9-c234-4071-bc0a-786badf2b053
0e3f5e04-3a62-40ad-8b0b-d8beb2d5035c
b028fd39-53c0-4d32-b89c-3aad49254d33
ad99ece2-e80b-45eb-b71e-0689799b6a70
8ab455f6-4f42-4a56-8f87-ade650fc885f
fa055aa8-d87a-4fd0-b010-b6883d35de27
022a7a6b-34fa-4acd-a23c-58dfb9c4f602
dbef6017-2c47-4fc0-ab24-88da447a65ee
38bbd7b9-404e-4151-95eb-a2ec7290fdde
acca15b1-c734-4bc4-b72d-27b6d9e761fa
f77b338e-d7a5-4f7a-b69d-b6f5a622caba
2a93b1ef-2ba1-42f8-9542-b94f7aeba088
da42968e-d49d-41b2-8697-671289870d10
059b22b3-d70c-456b-ae41-75d1fe8854fb
f6147b4e-d47a-4fa2-beaa-d0bc3432066b
5574fb55-0aae-44fa-91e8-22dae2c2fd39
b0a07fc3-4c80-4c81-a455-de9a0392659e
84a802cc-f45c-4908-b4c8-214f16899334
14d4fd89-e687-46d6-8045-c0575362d37e
71f97c1f-66cd-44c6-b633-c7fc7081fd61
7902a316-0eaf-4096-885d-b45d85269669
415159d5-990b-4de9-abfc-afcbcd29023e
e91c18f0-2ddf-4848-a4e5-da6428ecd4ac
c24ca568-1618-47ef-9a99-2bba778b9adf
87cc9661-a0e2-445a-a5a2-ec680e1bc700
5fb16568-07ac-4a0e-93f5-095b4708a596
3fb5f141-7d50-4324-8d70-7023a010ff1e
351da37a-da97-4053-93fe-7871262dbe93
7d7cafa1-0a23-47e1-a688-73f51a045544
b71bc59c-7b8c-4169-a4f5-83441cb646bc
527e4dcf-001f-430e-95da-dc4a5253bafd
2b3173e9-f29a-4f82-9f84-6f09774de83b
36137b2d-e9c1-4f02-8ba9-cc4c2c246be7
91b1fe5b-d34b-4dcc-84f9-a8bdb3e3bc21
4ec4e235-ce66-4656-91c5-3af379627f4f
c16e3218-356d-4fa6-a5a5-39e1728a5ade
30b5f76a-c268-4479-b0fa-01c6beaa7f53
571228a9-c189-4ed8-8030-394008ca3e3a
bb1f55cc-fa95-4552-8ff7-fdffd461619f
a5d11b32-6210-484b-9c81-827e70fc84e3
6193b331-806f-4e1a-ab16-c869eab8967e
2b61c89e-027a-4cf8-b275-7a11e2336d4a
1333979a-4dd3-4eb7-b33d-b7cbdc8154d0
5c6fbc15-b47d-4997-9318-5ec4380e6438
76a60dbb-8d27-4cf6-ba98-02770fed4ee8
8ffc215c-e9bb-43e9-b433-660df53c0259
b85332a7-203f-4960-9a1d-2a6b086b0704
445ea99c-2466-4d97-9fb5-e29d3c7bceee
4396bcce-6b70-4237-ba92-04363538aec2
63bfb63b-fe23-443e-842f-00d26c5e34f0
608d891f-ea2d-4ea3-b5d5-d9322e532bb9
cae3fa87-44ec-426f-9bb0-993d4b7305fd
f1964466-31a9-4b57-837e-8c340b5822e3
94b6a9b0-ccd1-4ecd-9248-fb3d2433753e
de74e30e-be70-4f24-9536-65853db84673
5d6d05ec-e1e0-4b90-95f4-62340221b5bc
8aacb8e4-fe71-4809-a45b-f6e9d36af13b
6074d471-c0cc-4e5e-a319-f37c63d1e1f2
904f0a41-d7a8-43ce-8b8e-dcd49eaef582
9e608abe-7586-4966-ab63-23d91c1573a3
67ed32db-9712-4b8f-9f2a-897ed84fba0a
914cdab7-d9a5-43c7-bfaa-3e9a11c3f54e
8908bded-407f-41be-b382-eabedfa4c6ce
2dd52de1-df08-4da3-bc9b-3a88bd2842be
c2d49d9b-bf5a-448d-9f87-c1129f4d8ac8
04635af7-8140-480d-afe2-7812005af171
b4767647-fa2f-4756-812b-c74ab48ff58a
22d5c6e8-e768-4553-b1ff-5eedad036ac1
d9f584c8-705b-47b8-84a5-baecf6ff4ace
06d6eb2b-76b7-4317-9671-0b99e56fe236
2a2cbeb6-5b66-4a83-926d-df205ee339fe
f513a8ca-402f-49ed-a853-ada2b125efd6
6f6f653a-baf9-42df-aa7e-7593356797ff
9d24fa73-eb37-4dae-be9a-811dc6698fb7
c7b798fa-12dc-4937-b771-eefd09bd471c
eff960f3-4d1c-4fca-8ef4-c08bda7be5ab
5ec514cb-b28d-4480-87a8-b9ac41ed01ff
4e4976ae-b3f9-4ea2-af1e-853d911f95f8
b1e1826c-3b5d-486b-9e5e-0ed1d8517222
60296e2f-2ab4-4f7d-b7ee-23465883540f
3090b7c9-0e36-418d-855e-01ca7b59c5c3
32e7b27d-8fa3-4cb9-940f-16d2949b0a17
18b24ada-0d44-4806-ab76-7fa9e8cb7442
c8864c65-f047-4b1b-a99e-d68d5c584419
02b691bd-4b38-454e-af01-f74eb573aa84
dd857310-9616-48ce-9a95-ee4c1374de00
eb342714-632e-4f6d-ae13-5ba72dee7c76
5a57f453-62a6-42a7-a2fd-703b0ef78e52
3eadcaa9-2e0f-4d8b-bbbe-ea7b8e913d61
053597d0-b40b-4e39-a168-344189ba34e2
cc9e5389-b970-4b4b-9263-e6f4bc895732
6f80bc8d-9437-4639-89a5-404cca1d08cf
51cf35ac-5645-4a62-b428-85bb5d424337
28028b0e-9643-486a-96ea-94147d0835ff
b99c828e-00ee-41f3-95c0-766f397ded55
7528feeb-cd45-41fb-b039-3c0f4f4fe11f
885b6dc2-5ea0-4b75-91c0-0afc5b403fe8
359c2398-8425-4053-81d6-35f9ea053198
8a666505-4e6d-4554-8a4b-d40980639138
1e3dd9cc-2300-4a45-9bbc-d28488c9260a
c8ae6f7e-5472-47a8-a424-6ca3ee0e8112
36a8458a-4871-4bbc-b5ba-f0898654e72e
ce2e8d2d-5f93-47ef-90bd-cb087ef6be7d
2bf38417-87e3-4199-a23b-626c11f75795
a4bf3461-fcd6-4c1c-a517-ad577d668583
953f5977-a447-4ff7-97d8-cc46de2a67d7
f346593e-b251-469a-ba42-649f22d3ebec
c7346eb1-c47f-45fc-8d52-7b41fae262aa
339a18dc-dfe0-418e-ba6d-f0a2dd631a8b
b4b0fc01-44b0-4a97-ad6c-d9094e10fcc6
08719539-79d2-4e75-ba52-8d6439055b7d
d602ec46-ab9c-4a1d-8ecf-c761ed14e787
6494a12d-33e0-4066-91b4-be02e0c9748f
83784bf5-67dc-445c-bd07-d5a54ed54a57
faefbe7a-cbb9-49ca-ac61-a6d3e0fce940
0d79306f-84b3-45cd-aee6-06c897a825fd
b30444fe-ffb7-4993-ac73-e93a802d5026
f42b5d3d-a540-4c79-b33a-feb107e91e2e
7252526a-4d3f-4472-8f87-5cd0d3c05b1f
2dbda36d-a3f3-41ff-9878-4e28292b3f07
19f958dc-5c70-4d1a-93e1-6291cf5a09e8
9f54648f-9221-4600-8641-a94dadc1d228
6b5ffdd1-c19e-4e44-9d84-ee816971c3ba
1d30e3b0-45f3-4533-a7be-1982e0f5ad05
a167f1e1-e78d-4492-9450-308a76155a3e
079c80ad-2cfc-4ccc-a02b-4e8f1bf814e5
11bee255-c17c-414a-8d7e-b4ca6afc2214
ccc78d7a-017e-4bc6-9278-d46a921a9bd6
ee7339da-d912-4c5b-a216-41f1a595a818
efe6f367-377f-4cd6-a52d-75c1a4ed1fbb
1c191efb-30eb-4735-8d01-55c31b518d20
36f10acd-8dfb-4d93-b0cc-d87a556a75e2
e65b1a6c-ebbe-4f66-8937-e4306de7d767
47b5bf7d-b296-4b38-93e0-25e156a0d801
1c2e856b-753d-4601-89c1-e471641b6965
e002d174-8032-441b-9647-fe72dee818ba
c8c3d48e-502a-4030-9eb3-b78deff858cf
36531c2e-0e0d-4633-82fa-d1528bd79982
7362ef38-1928-4bd6-899b-e1f74bacd4d3
34493d51-d88d-49e3-8071-7be302ba30a0
3007175b-ad5f-4a86-89cf-3703d0e73bce
94e42741-ca18-464d-85eb-8156bdad1e63
b0809297-fff5-417f-a5ed-dbc01fb68c7e
fd28f5d2-d59a-4a6c-803a-9a410608a8b9
e713c3f7-d329-4a7f-bbb9-605d9759f4c4
ff36602c-2808-4d53-9858-0ee6790a0ba8
5c7adb5b-8856-45ed-b281-b4d3cc7b3a96
cbaa5d5c-70f1-44f3-b6cf-418b29f1ca3f
148eac3b-8f05-445d-916c-c453159b82b0
4634fcbf-133e-4b72-9de1-f497e1fa820f
57cb7cef-e7b9-4238-ac75-deb821af1c7b
2c236f2b-7d49-4c33-a1fc-983e88707f6d
8c6bd4af-0f15-426e-b172-c583d7df14fb
34b88655-eb64-4211-b3dd-0152b31eb0f5
0976db4e-eaea-419f-9fc0-b67ec2a17142
266a5919-be1b-4648-83e7-8d0a5c163469
9d90294a-06ad-471c-a94d-b621d5d355f1
4dd260e9-9562-4065-9573-51c0d51da8be
3356de07-5db8-4053-8d50-199567a5a9fe
8abe4be0-31b9-4732-bc88-a98d00cd6c07
6ec1e918-b05b-47bb-8dff-7530edabeff3
206edefa-04e0-450c-9705-5d18b75d84f4
b8e68973-f710-4d16-8342-db8364131a07
a7babaaa-6dbb-4c66-8e9c-b75f1a96e9fd
d900a005-4d47-40e6-bdef-8109dc07cadd
51620e79-a7d5-4e09-9b57-0b96afe94f11
3d13235e-0f11-4ee1-8598-63a8daeb5080
5123c743-9a30-4f1b-80b8-6eeb70bcb1db
ae8c4587-ac1b-48eb-bd5c-8cc63c58e01a
51f071a8-e030-4069-bf96-f437b85314b8
21a90ea9-f95e-4229-8e0d-a1e2c47405fb
ab6d148a-3f43-4a62-9188-7b420612272d
17132fc7-ee4b-477c-a0cd-ea5efcd9c1ec
9ae3a6fb-04b9-4949-94d1-8e6e606e432c
4defc375-e78b-484c-b610-f4150cc057b9
e5e46025-c3c3-48d4-ad83-4f572f94ca7e
77012b5f-60e2-4abd-8a87-9d220bd8c70a
cb5d0a83-f582-44b9-8ab7-80e88475f0df
6903029d-e2bf-4bf7-931c-b56390663a05
1de31e56-d32e-4a3c-b3d1-914b7b8e4833
7ca6dc4c-906c-42b0-80c5-e856813c65d1
b81d51e4-58ad-4f42-a272-255909b3ab56
36711cd6-ad66-444f-b913-7c8c933a7cdb
d34b101e-500d-4b1b-b853-f91043098439
b7d25808-b383-43a9-9fb1-5b01baf36ae8
9456e245-b02e-4cf5-9cdc-03520e610754
eacd3023-b960-4e9c-aa1c-badb09ba1501
dcb62fa7-46f8-4dde-89c0-adebfdbd017f
9508796c-79db-4149-a121-d16670f97358
df0b87ec-3daa-4ce7-b13f-b09ad39e2a98
4a427133-e361-49a3-af38-3a3b090dbefe
4a1563ad-56ae-4b1f-a237-6be89e5882bf
efc20e2d-a8b7-4aab-9611-a18ac73453ff
50a3f370-4b85-442f-8b06-d52b3a3f80f0
7f93c560-9706-4eb5-b8db-43c329c69124
6e11d1b6-33e8-49d5-bbbf-b6bda3daa836
bbc43b36-9c8d-4bb2-99ba-72d82a13c968
bd7fdcdc-bf1f-46c7-9e8c-cc880f26432d
9e65f4f6-824f-4ee8-b237-35cdf330ed10
8feba185-2848-4a84-a5b4-08ddbe140c6d
0a82456c-3fa8-4791-909e-22b7132384e4
7ab5eb02-6697-4d3f-ba17-f7bc88b7128c
b918eba1-ae91-444d-9885-ef811825fa1c
83b74dee-4973-42ac-9e1d-894d884bfcbc
b11755b4-6601-4400-8ded-4b7085446154
aa1d2dfe-82b2-4017-88bf-a74d38041b34
85dbca8c-35d3-4a52-95ed-890a4f0c88ed
ddb4bb00-fe53-4943-a954-4f92ce48514d
4ddb6aee-5192-4049-bac8-109d2e1732be
7b9b073e-76eb-4bc4-b0c4-587f3985564b
c425093c-f6f3-47c9-91c6-d540c807228b
7c319871-b83b-4f5e-9f73-2ac2c5f42a3f
b2df018b-b305-45cf-bf02-04ac3b691877
5faa9eb3-3268-41be-9f86-9ec3727d8a6e
7cf3bedb-4357-42ed-84b3-3641720f4809
7d6da959-0bf1-48b5-a327-63b28626fda6
d73e8881-c512-4d03-a09f-74cb34b1f79b
242b88a0-6eff-43b1-89cb-8a3531fc9e52
c041fc30-512d-49dc-bbf3-8170cc8729cb
3f202fd6-da2b-4409-bcdb-15935c92a424
fd01d3e8-c204-409b-8817-f0cb5b678202
de39f393-c050-4481-baaf-334c58eb2e57
07dbafe3-8e35-4c16-aa42-99f6d9c46a7a
c316bbe5-23aa-4ee4-bdd2-e2f729f2f3a4
0e7652f0-2906-4ee8-b871-781f50910e20
206f5634-ea5b-4f69-8dc0-fe4a986f9708
b37ad04a-7fdd-416d-8661-9f0d8132900e
59f428a6-ebdf-4c43-bbd6-d8bef6f65c7c
b1c315c8-8538-48b7-b0cf-3b262e31e989
3284996a-de42-4609-9861-3250a0927c11
ce05a064-11d9-4327-8ab9-4caff3c2a8e6
2ad8caf9-eb3c-4eea-8520-84824473a466
62a92f38-01e8-4a85-a2af-13b4e6720ad0
6f26dc41-b8aa-4673-a478-2363bb6d1a04
67a3d775-67de-4f0a-b816-6c90c58c1f0b
e1dec79e-fef1-4e99-8b8a-3e6b71d52335
64b12730-5f53-4541-aba0-87839093ff44
5e50b981-a410-4c9b-8440-04c7a111a681
77ae3728-eab7-4942-990e-6c1bb87ddd9c
47b9d01c-bfe7-4823-8da7-cb79701b0366
8c7fdf35-bb34-4bdd-9a17-0307be6d4e34
1988b2a4-6d9c-4a16-b52b-5a30acec98cf
8b3a6afa-9223-42d2-98a6-0cbbca7c006d
7eb9e464-dfb3-4072-8f8d-18dd8f0a1f5b
131a2547-0098-4681-a1ba-3f36bb533a85
a394e815-78a0-4ff4-80da-669a8ca02d39
de16db37-57b9-4cae-85d1-4221d5cb442f
2b1262b0-e2db-479b-b63d-3c50b97f34ee
8d9cba28-268a-4cb2-aa49-bb91c55d81c5
961d6209-ad52-4a0b-9dfb-f0db697d7eee
ea60d7bc-3e83-4236-81a4-b379c5f05139
f2c99fab-8715-475b-949f-7ba665a4054a
52032e5f-624a-4e3b-b1a5-4ea6234fc6de
f01ff2bc-fbed-482a-8c05-5a757303dbb7
cbb34736-225b-4959-a16d-03eca47f274f
e28e20db-a070-4352-964c-55b7fbc2fa20
d6e2bb31-e373-4795-b649-20beb8e4f085
d211db56-6030-4038-bad0-d582da847a93
9e54f7e2-1f7f-42ea-b82d-7e4714b1d6ad
383eb057-d759-42d2-9687-fb0471e0ed15
6f1938d4-38a2-4ffd-b9d1-5c3ac41d6819
0f74d259-e30e-4255-b591-2efc5912ab87
1293e8ca-391b-46aa-a8f4-6f0fa5006510
2f90c122-efa2-4102-b7ad-74307c7a7b83
5f1e2298-567d-467e-992f-52a040fc8ca3
ab7762c5-3167-4c6d-9f8a-9b5b06890069
19691f29-88db-41f2-b316-751fca4cc05a
e823548e-69c3-480b-b335-fc8fc437e6bd
1d865ae9-557b-43a8-8a0e-e361afed137b
1eedda23-0e1e-40ea-88cf-e9299e89a37b
cbee620c-43b5-4909-be3a-e0367bf63760
dda6c2e2-b19e-4d38-bb13-06a3cedeff27
21087b24-184b-44e3-b482-2bfb07675209
01787b4f-3f42-4363-ae06-151a26e97b09
2cf2e629-b50d-44f2-9ae0-e5ff16ac79be
cc86bfbf-c2c8-4db4-bf03-304e12ea9b13
93a3864a-abd5-4ae2-bec4-8d1a00e0ebfe
7e8fb0c8-8a7d-41f7-a5aa-059e0165401c
0aaf9442-ee2a-46f3-9c11-8a585a1fa143
1748ec44-5435-4240-b1d2-beba6606cefd
d7200326-2e8c-4b3b-b0a4-8ff273faabe1
4fbcc155-8d61-4765-9789-45998ee04f2a
7edaf651-f3ce-4554-a815-579d8a4c345e
0dc8b544-63c6-48e2-ba46-e43a5762851b
9952b790-3ecb-4f15-b078-4ab2dfc98343
8561178b-5227-404d-ba39-98a727d62112
97356ef1-c1ff-4abf-bb0d-792fe7d5f508
e32635ff-2d26-4d6c-b66f-a6152b674b50
b2c0d90e-a404-4e7e-91c9-6d078ae4b4db
0b335246-8ec6-41b3-8885-c88c5ea2e23f
7c59e753-8724-4fc6-b987-a731bfb59bb5
725b2e6a-8223-497d-8510-f79029458acc
f36f4de4-cb5b-4783-9326-958d7176b26a
1d406d6a-34ab-4f02-a213-e35c7950d60c
d84298ef-3930-4f78-8e3a-6ff869f36fc8
d4646589-e1cb-4958-a9c2-6b2a7d2d3261
c15b6ce7-fd5e-4891-a911-12db849db73c
e07d9174-0edf-4376-a9c0-10dee7f1aa33
d6dc50b5-2742-43ee-bdb5-c3b093635198
679e663c-c509-46b2-8f1a-5db22a219de5
6a815121-55c8-46e7-b93b-d2aac82f3166
787d04a8-9e51-41ee-b345-01f748c555bc
8b4f48be-b888-4d4d-ab3f-857a8d026d56
ec4fc913-164f-4f81-9b99-68ec5e535df0
a626cb6d-c4ad-4550-989a-65336af7335e
5428d880-9bda-4eec-9362-737d8271514c
b57e6d2c-5a15-446f-bb36-ac0c9bca3735
3e3f16a2-8365-4e92-b0f8-7f950cd5e736
f8a34125-5de0-43fd-b3c1-77feac12bf38
728c9f92-6b65-446b-a995-86d26817d1a3
3a6860ad-9c53-4710-8406-07548f2141bd
008bba91-fb30-496a-b675-38087b6cdcc5
a129ce43-415a-4dd8-b706-b14413d98a20
776059dd-7ce9-413b-a911-4e14c8309112
24c987d1-4a68-4b61-95b6-ae94a3c8d20d
5c10169a-1287-411a-b1b2-3303c3f4936e
d4632cd8-72e0-4f55-9070-fb93ee9f7089
e5d90c4b-5942-48da-a362-b9ac918755c1
a7e6929e-ec64-4f4e-9dda-69383b952866
d51aff23-ec67-4972-a95a-1f1bbbcd5397
61300287-ac46-403e-be12-f5e436e85d74
d481f3c1-6e15-4501-b3a0-3b6dd1f675f9
18702247-45cc-4bc4-9bac-c79f2fb80445
599c9f58-6c34-4c0e-b6bc-467a726c38d9
8a2b6073-1737-4920-bc1f-963f697bb584
3d31e7ba-0816-4fde-8b6f-8719be95527b
1fbda870-8935-4054-8844-4b3cb3198ed3
65a9df35-45e1-4283-a269-39afe56afdbe
3066d8f3-462c-4223-8c7a-2ac031ac4d8e
aa21ac89-3d35-4e86-ae18-1b57df40124c
629d0851-0084-49c6-864b-f443f8d0d127
1a20e76a-7252-4ed7-a75f-aff723ec1ce5
9610c57c-7776-4b05-a506-c7b11c10cfc9
2f0c167e-a13e-4d58-930d-83ebadb9feaf
e7ba152d-8c59-4dbc-8197-2e5decf375df
18a7aae5-5ada-462c-b871-b84ce6aa1890
32abec7c-733d-4c95-821d-1effddbb27b6
bb2b7845-6682-4979-a92e-dea5df93622b
453fecfd-2898-4f89-af96-4a69ac286d21
75b8442a-4832-4c69-9ff1-a7d743fac135
ce594815-ac95-4bdc-9728-43485f29814b
8695a0ce-aef1-4463-91b3-f53b6fbb8533
f8308146-0770-481f-9ac7-a5fd64802728
f51155e1-13bb-482d-bea6-372d82699dc5
41927cee-1641-4155-bae7-0099a3b54b48
c7be6fd3-3dd4-47d2-a1a7-f09618d95b46
8e71e198-cc67-4397-9672-f0ab2e5cac1b
d3f93c71-f4b8-4e35-ab87-191bfa5ac99a
959701d6-424c-4ec1-b90a-42555cb666f1
7935c7cc-8486-41f4-b68e-8752165e87a2
c241ea38-8146-4f3a-8e33-000676a19d46
479a93fd-cfef-498d-b224-9176be3e16d8
8da84d87-81d0-4bf8-891b-cd9f5a0f138e
37f905c1-f5aa-42ed-a07d-43add88f7934
ccfee221-7fe3-404e-a5fe-c45c84ae88d0
e791c394-e3d8-4d70-aeab-3734f2ad3a81
0ae31935-06b6-4c6e-85b8-e92b01279178
21081cf2-84f7-4c4a-b66e-ce2a0da4f249
26ff16f9-8470-4448-bc83-93d96e01024c
c16be32e-b30e-4463-b6e3-417defa5193d
9f20162e-115a-4f19-a2f0-f96d477aa55b
18a489c4-5a0c-4b9d-b0e3-2a63975d51c9
7fec62e4-2b6d-4953-ba5d-895dd413193f
0e606559-95ec-47e9-a7fa-85aeba4e1354
0f14320c-a96c-4b8c-82b5-3927d911b8e4
18b97b0a-ea03-4a86-8c68-b0438d9f84b8
430eedf9-06e0-4443-9b19-8bad2b9896b2
b6f37158-4bd3-4502-b5a5-765b2657f0fa
dd0dcd75-5e65-4d58-80af-eba09ef478a3
d7ce537b-3feb-4936-95ee-18ac20f8bfb7
fc95717b-80d6-4b7f-9c45-3eac8add4fa5
07e52d1b-203f-4db1-a2a6-d7976b92c007
bc3638e5-c588-416c-979b-e5ed5987a714
6d14b3c2-43c5-4fc6-9944-40843c9913c7
5f029a39-f78e-4dcc-b21c-a2ee755bcf39
bc6367fb-417f-463d-b560-0499101174b7
71d8d619-af95-4c7a-ba92-c6a29df3f297
226d661d-31e7-4644-ad00-cee1f9a8b815
9ba1d3b5-14fe-4930-a08f-25d6991eb062
5a895f33-2f08-4c14-9d18-98e2e7ad3c31
782a57b9-2521-43c0-96e0-3926f30bebd9
408d4f7c-d443-4f7b-806c-f17640b91d81
a44b65e4-946d-4e60-b890-d06310f40db5
99151aff-69a4-4364-8ab1-4425abc34afe
b3da656f-d352-4322-85b6-402f91b02e9b
3a09e1a0-65e6-4645-b217-4b9525a38e77
c469d667-5de5-44aa-a35a-3a229963784e
ca0f1d6c-35de-48d2-8da7-085445f8c40c
3f35c2da-7d14-4239-aac6-d2e6650a4f8d
fbbf3d9f-a3e2-4e2b-a380-2c7c6baeae94
b037faa5-4a81-44f6-b904-04eef5e7adb7
d40384e7-0264-4221-9829-18359a867411
b971984a-4249-4ea5-9240-e5ca97836b0b
9e330735-bb14-415f-9380-b0a420430eb2
4e81ebae-69f1-4e46-b1b3-0ff47184b93c
698b5133-0074-4c68-a811-a0e857fc5747
72225299-8046-4800-9835-0eef93e75211
ee35ce39-42de-467c-bc28-03afdbe0b3c7
35513b61-10fa-4078-8ff3-f466a3a59910
bddf1b31-35cf-4b10-bc0c-04cf188b9421
490a6ef9-a4ce-40e4-b57b-b852ab83a439
e1779264-9489-4275-b226-544fe2c53dd0
d69f7f78-de60-49ca-a045-e1a9affb8a3f
445760e8-4136-4922-9e64-8ba4f0191b79
7f59da44-9193-49f0-8887-f02132b798c4
f5190eef-d74d-4137-afd7-a5615a70126c
697afa71-e07c-46d8-b687-5e53731738a8
3f3b0df1-d523-47fc-a7de-1e77a0581b11
01171e29-1316-41e1-8bdc-401becc89366
bb236fa8-327d-480c-8840-cc2932b8c468
684b4040-5ed5-4b52-a5be-51748d35b727
af98a627-d508-498e-b759-d023fbce5efd
28935c46-c011-4053-9097-03fba04e7aca
c8f1159b-e4cb-41cb-8f44-4eabf8831171
93e3aca9-e942-4928-8aeb-713d2ddb9a48
93ff18e8-e912-4aa8-88f7-908a552085c0
1bd875f5-5f38-436c-849d-85ba2af6a33d
0deb1f18-2bc1-484b-9ed5-dca299c4eccc
21b5ad31-a2ed-4428-bffa-de800c076e69
ac7665e0-d618-4b23-8b7b-743ca2f70342
1ebe55e8-8ac1-44bf-aa20-48d97cf7ea27
ea171ea3-9ab2-4cc4-9c36-406ac39bff6f
1db1f59d-7685-4da6-946e-180d8380f8c0
bcb4f239-3f65-4f5c-8d80-f3a2d9d3d0a0
36efec76-3c51-41a1-b632-9cd2daab5602
7a3aa4f5-c001-4563-a357-c2a57ddb66d2
94639825-de88-473b-89b0-9efa965e0400
effad060-7511-4091-96b2-43e59bc7e272
95b62665-b8e1-4979-a562-536e3538f5cd
08d2e475-43dc-4171-99d7-3ab615677e49
c35f5ad8-49eb-4072-b83b-cfee8c7bcec1
0f7769a2-9034-4001-868d-418654811d89
cab20836-beeb-4b1a-913c-164997dd16a2
42530ea5-d478-4292-91ac-3c5f4f4fe27b
268d8c89-4254-4869-baba-f58189be9d80
36117c3d-2a62-4981-a302-87b1b380ebf7
2f0943e1-1872-4a32-b4c3-6c7d221a981a
9e0346f7-7ffa-4ade-a552-22e1f0da99cd
5639b876-405d-4310-9940-7d1ffdc16984
b141a9e4-e655-426f-8b4e-a9f6b2984ff1
85205548-9847-47d2-b5ab-68611c0c4e20
6dad694c-119f-4182-bee6-98775836d030
ea2a23f8-a2a8-4385-8ec7-acaf14edcfd7
980e14db-92f2-458e-a874-c6580651e0f2
a59631bc-0124-469f-aea5-e8e57ed7cf91
77b967a3-7a88-4541-91a8-a78ea7939e93
87a978db-bc51-4d6f-8d27-e423592fa503
b6432470-2cc5-4f23-adb8-ad1b5f3e96da
619a0d49-23db-4d4a-a187-cb39242bcc41
f5d97a2d-cd6c-4c39-8ab7-3400c39ce484
8adc6cf2-c813-440c-8fff-f45dd00d20cd
a0a3be4c-b05f-4801-b1fb-50356f335d7b
e4ed07a5-7fd3-48d5-9e7d-0b15481c370e
267c712d-3fc7-4dfe-82d9-22d80a6c7811
b2f9e2c2-bc8a-44f3-bb1e-4784c8cabb3a
e6ad2890-e2bb-491a-a671-618e6742d9dc
e74a3d4e-9000-48de-b330-b47987f6babd
85e27a92-e5f9-4891-b645-659d3106f9a8
08ffe72e-0ab7-4b0b-abc5-a3bce58267b3
f1771c0b-f8b5-46ff-9b3d-f54d115fabc6
e03f0a48-4d50-46fa-a0b7-bbdb8d511fa5
cd0eb219-53ee-44e2-ac99-25c8dd225fb8
14efb562-cf34-490d-9be2-92b92f25c770
a9e58919-ab31-43c2-9255-2ae71d75c889
854c274d-98e0-46dc-8445-48c939c0f53b
1857bb6b-b041-4959-802b-48573a2b9367
8827def7-1b4d-4057-9e1c-b6d6f61877f8
1283456b-69e2-43a8-a027-906b8aa5ff04
36201aeb-515b-412e-ae33-7ff21e46fb06
0a712682-b3cd-46da-afa3-907f72816819
7138374f-f72b-44a9-baa4-0936a4104be9
d463d0b6-ddce-4fde-93a3-309f92057431
f1eb1fc5-337f-47a9-941c-00ebae6829d6
61c5eff7-b30a-44b6-b6ed-9d4ae8370682
c231846d-8cf1-4f70-b43f-ba0ce8fd1edf
ad61b93c-99ca-4f27-b1bb-d6cbfec732ba
1655235e-fe2e-475c-8ac2-0f09778adcf5
3bb1ea0e-f048-4572-b3e6-e7ad9925cc0e
19bb5c00-a89a-483a-8a10-ed30c0d81381
762e4012-e287-4983-8a5c-c4dafc8c3ac7
0e87cf2f-c44d-4dc2-b7c6-4ead9728cd3b
741f9ae0-2907-4465-b714-0f9e440049d2
97adf156-a6cd-4778-afa7-6308c09ebe68
4bfc4dd2-4070-4fe9-821b-015fec5110ea
cbc45596-9e82-4783-b61f-cf50ccc43038
4914c263-7725-4edf-9f75-2db04f7dba24
1cab36be-f6aa-4b5d-9afc-0d05d2fb73be
b78cd879-50d5-4161-95da-807407ad8916
c9877481-8e85-4c36-a0e0-f25441923464
a192111e-9f73-494c-aa11-a6f7ed4a3621
ef79d4a6-ca0f-45b4-8530-4653ae40a72b
0f2da5d4-5123-425b-8de9-75f87ff1ef6d
9bcd5b76-c18c-4665-b433-3da9aa3c6391
b64faad4-80bc-4797-8907-674834edd821
1bb69e81-3aff-4a69-b821-51de76f4b28f
e315c70a-d880-424f-80f5-6d297ceadfb8
dfdd69d3-69c9-4852-bccf-46fa9dc19181
cb97afbb-d43f-4dd2-9574-48b10469ae28
d3d78458-0cdc-4937-aadc-31dd84d1a350
3a10567c-113d-47a5-91f8-202c1d1ca22f
d9b7f707-24a0-49b1-9f6b-48d700b8d453
ea54a26d-0539-436e-a725-6a25ef71d69f
26d6ea3c-68b9-4bcd-b713-0fee0b878b10
e2ffe0f1-c72a-4c04-a7dc-f0b6fc879af8
b3423aa9-423d-4021-9daf-0a51ed81057f
ee34df6d-1641-4ea5-957d-bee17f46104b
49234d16-0659-4c73-9e75-45946c76adcd
8bc62cf1-6ff4-4950-90d0-e06295856d59
64cf279a-1fc4-4e8b-b095-7d7651c8b993
f2b66316-7229-4c57-a736-9fa453b17120
613d233c-5b5b-40bc-bdb5-7602b93fa833
6bb3b248-6e67-48eb-b51b-355e85e138ad
2fc4a65a-579b-41e2-9cc4-e3dfbe100ad8
36188c8d-13b2-42b3-beec-6f82453b63ae
65a71eeb-58f6-43ab-8547-d55322bcfaa0
a1177329-b168-438e-ad24-8d706c3a082f
8dda1490-6a49-4774-abcd-67253427682b
4fd85874-0824-4f87-a8b6-132eac818e1c
b5bc9c2c-4b4c-4edd-8a75-198033e0dc9e
1a90325e-d1c0-43a7-95ce-506fc42c6048
bae0ee99-dd44-49d1-aa62-f02991e966da
2192e252-dd40-4c79-bb72-805ad5494ec5
6a60b8f0-f630-44a4-af2a-f0dd6b71f9eb
1b20efb4-a591-45b4-ae80-de910f9337a1
3e6e29aa-89d3-468e-91a9-7c7b46becaa5
dbfda996-dbcf-49ec-9f36-9316bb5b417b
7b90b0d2-5927-4daf-a32e-c946119937b2
66a8886f-b64f-4afd-bf9f-4c7cb4f77096
881fe70f-4d9a-4e54-a8a4-e2dddff5158c
f65870a2-9e5e-49fe-849a-31c0c076ffa1
2ea587d2-054f-4f73-a142-4e124e5b3125
33f7260a-7d9d-47f4-b6bc-322f89633872
9e66a571-0718-4b33-92cf-aeea88e5fccd
8f3f46c2-6f47-46e5-8deb-66aefe60d72f
3b2af254-4b4d-40d4-9b16-cb74603c944c
6256b3c7-1152-402d-8407-0eee5fbcf313
1987fb2f-6774-42f0-9acc-68bf892638a6
ce751869-4596-434a-909e-69fa37af1e83
9be8e700-7523-4747-81bc-567afbfa348e
6b9fe80b-6296-4a29-bb8e-1b54dc33acea
62662a91-c11c-412d-be4d-6befaf08b08d
26db67a4-2370-4e99-b69c-373084cf670c
d812a2c6-98cb-47cc-ae3e-b693f7a7bcfd
ba8229a6-e13e-4ece-83b7-3ad76d24426e
f5c89d8e-d2b6-4e54-8fa5-ffa7dc0fdabc
dc584b4f-c37b-4d09-bbcd-b61fe475db3a
93dc8e1d-b5e2-458d-b0fb-f5f78f2b0605
b93deffc-b628-44f8-9114-7d8d9ac9d876
123a39ff-c948-4279-84de-cd6c6ee697a4
ba47508e-ca10-421f-ae08-599ff96eade5
2a33f14d-f597-4c4b-b129-bac8e128ab84
466f6235-8180-411a-9de7-a416123b659e
84572c70-f497-4c9e-9ab3-68e71500e53f
02cc33e3-9aab-4fbb-bc37-2e767101d0e4
084ec0f0-6c93-430b-9335-9b1671462c29
138b52fa-7f54-457c-84bf-d34178f069ca
c995f0c6-042c-4148-b04f-a8c7e9963b96
634ceb72-8028-4f4e-b25b-e1729169fdf8
140b9260-c0a4-42aa-871b-63e1f04a74b1
9f763962-7b26-40c6-b495-845ad67b0d74
811d6b99-d664-4630-aea9-195f5092ae4e
c9aa4524-2ccd-42c4-b736-0074c2d9cb80
67b36314-d88d-4d28-9f44-a78b2a60b537
4d1a2c58-6b0a-4e2b-a13d-33d8aa89c9e1
2af7c301-8b3e-463b-979d-9fc5a4d58d33
fbf4e4f6-2f78-4339-9c72-a17984a3a004
719347d5-c5cb-4a0f-b922-982d5a2d0855
e20e2761-cc1c-4b44-ae8a-7963c6cfed0e
5b2482dc-931d-4219-8b6c-8921e91b0595
303ab659-8402-4a2b-8ce5-7c23d52d2d2d
6b567e07-8011-42c0-a256-90cd5a32e3d8
cc8164e3-d9b0-4b61-9cf5-cb816633e78d
748e3792-ef44-46d8-b5cf-e1d0c4b07db6
efac8516-b3c4-4b06-b554-bbbe89b51e04
d066257a-7db2-4ecf-ae9e-83a84d98ed2b
93ec760e-4b7f-4a86-93db-8fc557ff5291
f3cf37c8-0999-4689-8607-86ca54e19821
005ff250-b7c3-4df6-9c15-51e4fa187c62
a4441f49-ad37-44ca-8f9e-3ca7459d06e8
e83dbd60-7b23-4342-9750-e6eda0a67c32
41c1e81d-991d-4356-a411-d7d1fbaa1784
848002ae-87c1-4bb2-951e-910a53bceae2
55a67ec0-c741-43ea-9e18-f38153631726
41a14f96-edb3-4065-9a7c-88aa605a68a5
76556df4-de8c-425c-a6a3-c3f37ec05faf
6ea7a814-48f0-4049-b36b-4c38861a03c7
9e0d6178-c967-406f-8411-d234af9b4656
4c36fa7f-3a64-434e-910f-ac2a9748adb5
255b6a17-194b-443f-97d2-7fcaeb9a14a7
9e2832ec-3691-4426-9eec-7e0a2560c4c7
8ab47aa8-8be6-4669-bf1e-e73946d6e1c8
ebb7cf4e-29e6-4af7-b989-518d6e522545
b486e2df-2c9f-4aa1-b487-28fc96a5a4f0
572c6d97-e927-4891-a9dc-ece56757a15b
c88fb4fd-9f6a-4269-8a0b-d8ed1901d4b8
b89ed4ba-bee7-4d0e-978d-df5c1720a121
f00e3134-e531-4bc3-9916-0acf236ce553
c9d48b05-6e95-44e4-a043-ee2045473082
2301f7d0-515e-41d3-ae45-1a6618fcbdbc
c23c1f65-aadf-408d-8700-03dc4d9945ca
7283643d-8dd9-44b3-b30d-87dfec36714c
c3d74813-dc3e-427d-a0fd-9fdd47f3cf39
0796b73e-3149-42f5-ab02-f41a4c5810c8
7d4eadbe-5fb7-4b55-ad6f-9d3d29b8e8e8
8f610ad0-ccfc-4c1c-8113-04dd0c9ae641
437e2b5d-ae68-40d2-826b-02782e28fb45
d68dc960-42ba-436a-b905-564aabe5bce2
b3c979af-5da9-4098-ab6a-d00171304bf9
52e9d52e-2a7e-443c-8e3e-e0bd4904a001
1746e098-b982-4db3-99ea-8db0ccafd317
2a686509-bf9e-4cdd-8ba4-b6354904933f
cf977a67-88d3-42ad-9d49-9767fa5734ac
d87158ae-4a3a-4b1e-a460-109cf5c07f26
dadfe70f-fb43-4748-8fda-125e0bf859c4
3cf7a234-6b3a-45bd-a2aa-47355573a93c
58c992a7-04c6-4489-a1b1-b6368292d260
02179b70-bae1-44b3-970f-6957478852f5
a815a362-f56f-4198-a609-d697adf9e1ed
11020406-8adb-4d52-8787-c7250750c493
d1960f50-8de0-4625-9efd-0c0325397853
28552ebe-7150-483b-95ff-8674ca2d4d6c
854c3c9d-67a7-4cc4-af9a-5b0cf56879d6
628d92f9-e77e-429e-8fc4-de5319da5b9f
c4bf2a3d-3d41-4a2b-b8e9-c5125e482c70
448deaa0-e0e0-4bad-9fb4-787179fc00df
64fc0c23-5044-4f8d-a464-cd2a45603102
2e6ef437-39b5-4c58-9a48-eb3152b54967
ca4e37dc-bca2-4721-ba79-1f917b14c54d
1d349ffb-5708-4734-a468-dc1ccb93b217
6961d215-728e-4c2c-8eb6-0a0d478c8248
a18a77e4-eedc-48f4-98ad-68d272b952ea
a3d74417-1f3a-43e0-a5ba-90d953264b58
212aa2d3-02d4-40ba-bac0-ffa9d121e69a
69344dde-f2fd-4bb5-945c-7eef5a1f2658
4cbf749e-5076-4663-baeb-20bbb8abc55a
aeea17ff-8312-436d-825a-f84a1ffb0c10
9e6a561e-011f-43f8-962e-3fb782808593
a8c8e536-91b5-4842-8142-c29bc7ee8288
d29134f7-1080-46ac-b0a9-23b13be94eb4
fc70fe97-f441-4f79-a6f4-4993e25515b7
7d926607-ea85-4ce5-aeea-5d40888be7c7
675f343e-ab08-405d-8d47-2b8e8c6020bd
91d43f82-413d-4c9a-990b-ff5b2d217200
fa9a67a7-f336-41a3-95e9-b34417fd2629
0624206d-90a1-4622-96ca-19f7d4e99daf
8096195e-1b54-48a8-9406-c14e092a2636
eabd7582-f0f4-4779-953e-28be281fd6af
aac8180a-7c5f-473d-9b35-b95cb11be7d9
62c9509b-f6b5-4036-83cb-c7b71a8f369b
89489279-ea52-4631-9c87-4a9c94672cb0
4aad1929-bd7a-4f92-abaf-9eb12931d492
a93c5014-e5a1-4476-a9b4-cdc5f33fb041
29a91e05-a2ed-44e4-b399-6f9124e6d197
891ae793-a155-45d9-b930-9fb999ef37b0
96042d5d-b3a4-4580-b02e-b63a8a1248b1
27a91d2b-5220-4e63-aa4c-0937b896a107
8ee9368b-cd79-4f0e-b3c7-a4787941b4ce
f97989d0-c712-4efd-ae2b-e3dfcafca8b9
db837650-daee-449a-a0a2-050a93e1ba58
0a8bd5ec-2f00-4616-b025-b73d971ebc33
53b33674-d371-4583-a3fd-db81b52bd352
ff5812f3-b062-4865-926e-44bade1e1e0c
fa78f8eb-71d7-40fd-a66e-7e9f512a35e1
b5ad42bd-c800-4acc-a2d5-4e118c17dae6
ea6c5c74-2439-49cd-aa75-fcab00005e14
6794626e-3e5c-4ab9-b660-5d31338cfd3f
e9cf3da9-a2a6-4182-bd70-6fe555bae722
e98e8d3a-d258-443f-bb63-571c6a4001ec
fac685cd-a946-458d-b6fe-5fd00ef7eca1
a248a47e-ae88-42fc-9c4a-601672b58f84
6997cd27-bbb3-4883-b562-93d5f43122c6
b57cad88-dee1-4fc4-834e-2609cf6969d5
5e13b367-88ad-476e-b1df-1a18c7886e48
65b82c55-5c6d-4e7e-b260-8512a560cb90
1324c84f-fe83-4578-9ff0-6cde3e810d7f
b6ee72e5-e08a-432d-998a-81c3d44dbe69
39bcd11e-5ed6-4a05-9cec-790633d41a9c
a56a0966-8e3a-432d-8867-b702020a476e
0886fe28-16fe-43b8-9525-820fb8c26ae3
86660a5f-f09a-4650-9586-1573f6f71dbf
79e8cc20-416e-4c94-9c9a-6dc80480d4d1
ba33a3a9-7545-419e-b706-79d773da35c0
d31af797-8f6d-4309-8c98-d96377c064c0
7fc4acce-e65f-45ec-ab77-f84912259660
43e65816-0be4-4e65-9152-6860f676adf7
60320e19-1ec6-4386-a8e7-7af3305ccaa1
7bb11fab-ee0b-47e6-818f-7612ddf551e9
2f6c6b16-ef5b-453d-83ad-3e19cf3e67cf
834ee9d0-0b81-48bd-9e01-ba9d935e29d0
3af4e051-b526-4ce0-ba4a-eba79d913a4b
69de3aa1-80d5-4d88-b7f6-e34edb143929
053b87ce-85ea-4270-889f-8ed6b94e6b20
b6ce7723-eb93-433a-8a51-7cc27ef6b46f
9fc89826-b62b-4ead-8212-c8b08d3de495
72bf7140-e482-4849-bcbb-81ac2745e4b7
af813440-39c2-4b9f-be10-8b626a9d5970
fb26b12c-f33a-4571-bdc7-d43292483e26
bc3843e5-d9d0-48b8-891c-f4e36062ef61
5b9fc2f9-795a-46ba-bf8d-bd2f8b3eb198
cd2602ab-8ec2-4abc-804e-56d1e9f72e85
03ab834b-d4a1-4338-9291-3a6b689ac768
ee794f34-12c7-4f57-8c4a-7385a286208e
80bbcdf1-7754-4f7a-a3fb-90d21b825932
0d53040f-8d29-43b4-8bd3-eea253fc2c54
7ddb9237-8ff9-4273-acf5-a6c5495729ae
045c82ba-8740-4377-8e05-ce885bcf5edb
73ad122e-047d-482a-972a-02d829b484a6
2fb3f7be-ab0a-4983-8e38-cef7439a8d28
1274fe27-f6f8-4499-9b9d-8e9ec05769ed
d4adc2b3-7b91-474f-8454-a5095ef4f298
c6761e05-85f6-4940-9243-dee5eb0b8416
6ce8a52c-cbcb-4bc1-b2de-7796f2a8841f
e4ee7237-f21a-4518-b847-b99940001f89
84aa1244-5e40-42be-baff-ab93a9a65380
94c60ee8-e51b-4b9a-b7c2-3d59eb4f4c99
6aa4ab94-972b-42ef-bb87-b18ac692694a
3d9bfadc-d550-45c7-a7e3-f9ea1eb2cea5
20cefbc8-5277-4dbb-99b5-ab07229f5bc0
c5d4ff0b-8a29-460f-86e5-7d22d48925ec
496af3f4-5f0b-43f7-b06d-e2dbf521bf85
d741dea5-d8aa-4e3b-a9b0-f9f669c5adfb
cd32f805-6cb6-4e6f-90e1-f712e5ba0ede
dd58b122-b44b-4ac2-82f3-431bf57c993b
93a8ab2d-6cf5-4c9b-98c5-074431382b6f
62d03ab2-c416-4060-b6fc-e198b651d2bc
5b386a77-5e4c-4beb-bb5c-e2307a410825
37805132-93e4-4725-806c-2dff6dd3b70a
f3947305-6e8d-430b-833c-bb6bfc3ad27f
396e8793-f84e-4362-85ae-1009e4fb197d
4b5f0cfd-43a4-4fbc-920b-1cdb1b7c9c91
d30a27fb-1a5f-4850-af5c-3989e1d44746
7bc9d149-7de6-45a3-89cb-2a84901aee20
3025f1b5-38f1-4d07-953f-05282fe31cdd
6c63c8b5-0aea-4348-be96-2f83c581566f
971a3208-8fcc-4bb6-9bb9-79d0de4ed10c
ccd11159-a021-44d5-b7e7-37f77ea9cc88
67012f3e-9f5d-484a-87ec-1b2aa1d08601
6eea771a-0ce5-4fc4-9b49-1bc8ff581013
a0d06b2d-82cf-438a-adc1-00e87b42e06b
d3ec6560-3d40-4569-8ab0-90f8b72fa2fd
72be7a40-1b1e-40fd-a7e0-39bc80f247a5
bd97575b-a62e-49df-98c0-4fe5a396f0f2
1bb9ba4a-4e73-4fbe-b7ab-6af2a460700e
3e1ce8ab-c315-468d-8d96-6596b79fa226
703b8061-6632-4e1c-bce8-5c160baa7b73
b0f896ab-f69c-4179-aac7-152c99ed0217
50c60106-d223-45b2-8e44-be3d06603786
fe092c13-814f-4664-8520-682591cb0b04
fe2e1de0-33a2-41d1-89e1-dd5b8a4c443f
2f9b0584-636a-4e55-a324-33ad84602ef9
649aae1d-808e-4faa-a2ef-95628f3afc2b
dfc3a9a4-e608-40c1-8d3d-7ca6c6e3ae06
c8f2178b-72a9-4cfc-b4f5-753e2890b668
66f304a9-30dc-4613-bd83-0b72a1836f9c
1a234ed2-c32b-40e8-8c69-7f200b9c5cc5
cf0ba12c-0324-4cfd-8048-3204c054fce1
2e76f67e-7f6c-4215-b267-aa1d153a543b
f171667b-75ae-49cc-a49f-1334f2edbb4b
c5121f94-11ea-4162-a5fb-af1bcd1a2551
43efdaf0-c7b0-4652-98c2-e1ce86635685
ebcae0da-2932-4109-8e11-ad66f3a0e801
d5e8c665-307e-4d1b-98b9-dd7f208294fa
7b80c454-be63-4170-841e-802a63c7377f
2ae77837-f46d-4aba-8b62-5b2977d07cc8
69439123-767c-439e-bf1f-c5cecdd932ba
2ab0448a-686e-4fc3-a99d-ce38a61512eb
4249b616-727d-4c42-9aa6-128adb1909c6
21ddc5c8-2936-4c38-98ca-8db0b43144d0
e541c7fe-7699-4a11-8ac2-b8cc7c0a0883
b8c0f588-c67f-44c2-92dc-8c89e6278b55
eaf58771-3d52-4222-96ac-179663df723f
252868f1-c3c9-4eab-9fcf-99e4f06b4741
4617e551-e8d6-45b2-9d9b-095743c9586c
6296e667-381f-48f6-8dde-9a04e70376a5
884a3723-ba0d-4e23-91df-444b4ed8c51a
88bdcdf3-d802-4944-a8b1-0551af0fb3b8
102797f7-2f06-4b00-88e2-e448a78d3927
3bdf096a-6ade-4bcd-8a9c-be40e08c3365
8082be11-0602-42b5-9d73-01c06b22419a
8a553ae0-b343-4906-92be-79ba4bc68552
4295ece0-53b4-4904-a8fd-6799f286aa39
f3b58260-3eb3-433f-88c8-850d17d46749
7b256e21-8595-41c8-a180-43676daeccf8
dbf34b16-63a7-46f1-ac81-ee0e7c1de6c8
edf65ac1-9059-43ed-8f63-281f2cd5ec80
020275fd-101c-4f5f-9d1c-989b5b31293d
71e5d80b-0b66-414b-bfea-fc2bda423aca
1120d6f7-62ce-46c2-8b4e-0eeed27fa353
03aed689-7821-45aa-8986-4d0a21372f88
bf7dde26-c2aa-4311-bfe2-6ccc4ff74f4f
24745505-1fec-4136-9c3e-89d2899917c6
95145651-1054-430f-93c6-a2eae3ebfb71
811c8bf4-bbb3-4ff2-b2ce-2ca27da38d76
3c82d591-386e-445a-b152-65841fde428e
9d0f81f8-4b58-4b54-9487-087e8d184377
e4a3803a-f8c3-4baa-b5a0-8ba145493bae
c3284d4c-c944-4dd1-ae82-1934f56c98b7
d6dab7cd-1468-4a06-8cf7-be1e2f5cb875
e21982e8-8403-4744-ab77-3b909387aaa1
9d85d08c-0ede-4683-b2ee-feba1708f01d
73da4482-1ccb-497e-bf9f-962cf798d8c7
9ee2f934-dcdf-4877-91f6-f61a1bae0dc7
b7a04616-0e57-4aa2-9f19-3d4f58d61647
54dbe61d-3989-430a-aee6-f300d3c4f6a2
86c04f82-b9a3-46bd-9e78-c987f7b086ff
c296254f-26b1-4369-bcfb-2397c8a7a7bd
c2b00d18-210e-4231-80f6-be18584f0cac
716709d7-1d8d-4aba-adfa-e1b847272935
2d42d0e0-2edb-4e41-9d5e-d272868d2d18
8c643d13-3df5-4533-b60c-11f592b3bfb2
2ca6ee3e-f752-40c8-9ace-f362babd87fd
a65708e6-0075-4d59-a79c-8aa71c8d6fb2
cd26a75d-da51-439c-8b70-3ec91bfc73e1
d278985c-a51d-4807-a020-7dec26b549c6
6ab99683-9304-41ad-b889-17cac1230b0a
bf69f36b-9e50-472a-a8c8-af94d11aff71
31300d8c-108b-4471-8541-7c9f7b7bd76b
554c2c7c-d877-4954-9caa-91710586575b
3676f194-4baa-4270-9108-bba3b78eb549
3654951d-4669-4587-93f6-6637241bdb7c
bca1999e-a67a-4a2d-9cc1-eea0de6c4196
52325176-7e9c-4ff9-aee3-54f7afdf6ed8
b4e110cd-0ed6-409d-9b64-d1be255b2782
5b68b391-b255-4ba1-aa33-bcb8e7b96603
6d0c51f5-158b-4335-a908-ecb7f74f074d
90c9b4c0-338a-4233-989d-81e385bdaa3f
c1507194-181b-4561-80ac-746399bd0c28
4e0ec98f-134a-4422-9d61-8eeb922be83f
a9893efb-45a4-438d-9b92-a611f9acbf04
5a46e1d0-1c12-454a-8ccf-940bc13d314d
48b6c101-7075-48ce-9455-033da0af5e79
1a72b1af-9e7f-40ed-b1fc-f35e2e24c81c
32c45fc8-d7c4-4282-8752-c70eb760fe62
5d9bd62f-1771-4269-8ba4-bef2c3572f98
57bd493d-8776-49ad-942e-5301e9058b3f
8c1d0c83-e965-494a-abe6-cd23d4b195a2
e652dd7a-01a2-4406-9f72-44aeeda57410
f1a97fc9-36b5-479e-999d-f6a7a3c1bf7c
373d6fe8-5268-4d1d-9600-ff2aff641c9e
2ee233f8-6ef9-4d65-99c5-681b43a6bc12
9fad4877-cc60-4703-bc7f-c6994932ba56
f3c73aaf-07be-4239-9bc8-3cb92234a898
821167b0-aad1-4758-95b4-e25bf265cff3
bd7d6f85-28a0-4bde-80a3-01852fa2cf63
9980a830-4541-4930-9c09-e74f8c2d39c2
df08de19-3aac-448b-a9db-0f9185e6dc89
2ef09886-2704-4393-9f71-7e31a8b5f273
2772ebc6-d174-4d8e-acff-21dc37c2b399
fb217cd0-ae17-480a-a4b3-6d6a8079f53b
c0f2b8a8-0446-4cda-b46a-5aca6b676415
1d5dc82d-9650-4800-b076-fdc29f65b173
bd60fa49-ec16-492c-bc0e-e2fee059c0ef
2500ad3f-3704-43c9-93b7-c086a488cb2c
2bb53aa3-3f54-41ee-9c26-368dc8f7c537
6852be23-5aa4-4560-9f92-b5ecc24f5f0a
c2dae3d0-cf9e-493e-87f9-a953f6fe98b4
ade3b7ee-30ec-42c0-8d74-c8b80f4f0617
c0c4cf7c-9175-4eb4-822f-989edebc0030
840a873e-2f9e-4478-8d08-334ca9c7ab98
3eca8247-2af0-44d3-bb04-c59d5710377e
f152aea2-cafb-4e65-a6d3-f03e811d516b
7c46e565-e497-4393-8a04-c292adc0997f
87498aab-a524-4ef2-8629-7cdaca971173
192f64cd-3673-4e1d-a75c-d81f93802a57
b81bd678-8ea2-4523-bae9-0cee9cda5350
946b9a87-8748-436b-85dd-eeec4acef5ef
b651ac06-114e-4946-b5a8-95942abf4c26
55cb4f11-900d-4157-bc47-d8642f4d0dcc
85dc8d24-98c6-4959-b6f7-6bf4ff146dc8
11d0479d-c3ea-42df-8dec-4ab33682bbfe
af3c0f9c-9135-401f-9394-16e7d0b38ab0
1e3f7c30-ecf4-45ee-8ec4-c8c0b773055f
ca3fe1d3-f7cd-43fb-8c45-4bbc63a341de
83e73481-1e36-4625-9240-f63355efd570
ae17860c-6edd-444f-9134-25d44284d668
59cb0a69-74e6-4899-a043-6e6d02c155d6
6c32c49a-57c0-40f8-a087-b7344ccc3f70
7ff78b3e-967c-4609-b1c4-2421175ce525
e0ffe973-f741-4c09-8fd7-2075e69a4db6
022d47fe-130e-4ee6-967a-33cafab0ac15
6b7797fe-816f-4b88-810a-ee51ccbb8fe9
82849651-d222-4137-8e91-7b780d9c33d7
1e57ed71-64de-4475-a292-29d7f2ebb1cc
22c9b470-0d3b-4f3a-bfe1-85931c9be34c
83e013b6-ebcf-4e0b-b986-34c6c8252f47
b7a5e518-ac95-4d27-a7ff-f26d0edc6ee9
c812d1ba-6844-4d24-9403-1be4ee5a5e62
f729b196-082d-4450-9777-24cc9753f97b
9fee73ef-082a-4c3d-b73c-25e3089ac109
320d6876-3a76-415a-a9d0-7d2e77794a8f
03dbc201-2498-4625-a7f6-82abeedf268d
69bed307-ba30-462f-982f-380702fe412f
1a105f90-1130-43da-9664-9e8cd9a4e3b5
cf9aa6cf-b4f2-4fa0-92e4-bac83de60947
beda5090-2752-4587-8290-cd8d83c18130
84ab34f5-ecf3-4c4f-9083-fb9d5582309a
b3522478-02c6-4623-844c-bd882d213e5b
2fe73c82-0710-4213-aca4-9bc38031290a
14c01466-dbcd-45cf-9221-af91e6169544
75d1b6af-6cd7-409b-be90-352a11c8efae
932653d4-a140-45a3-b279-b76baa0ab347
b7d344aa-be36-481a-a590-8148d46ea5bb
015ee9ba-983c-4c68-a6e3-76098a547436
bfcdebe3-9089-4122-96ea-2866d816fbf1
9d41bb6e-edf2-4484-b944-ad25fbc6d7e3
061bc740-fa32-44a1-a765-c6ad0a1bc7e2
98c83fda-b1b8-4c23-9f11-7e70bf6fff1b
f52f0458-ee25-4dea-93be-dbba008cd279
984ad652-666a-46ee-b712-41a23a7547d2
92147b39-5c98-4573-ad14-bb3f82e0dd40
f33c01d7-15ab-4a55-b125-72ec5c33ae22
4a68be78-9540-49ff-b17c-27b445458e29
01856d6d-53b8-4e6d-8a83-8ca34cf974a9
d4d7a497-4983-41e7-8a96-62a081d3db25
93ef2feb-717b-4a7d-a2e9-d1df6057b5e4
0aa43836-2d84-4dde-845b-003ac418f375
a4e6e765-8015-439f-a38d-9d45810748fe
376809be-019e-4ef6-b1f3-951964800b45
d0d57a83-12fd-49ae-ab87-f9f9149668fa
81996677-62bb-4638-b218-a86de15ffc69
6ef024df-fec5-4fda-8d30-1787d046c420
79f290ab-9af1-42c3-813a-7dfa8cbce916
c5d1dc14-2d26-481b-964d-e8e01bb4a894
de342df1-002e-4c84-b018-6463f8b8a56d
02fca1d6-71e2-4fb2-87d8-e48d6f4655e5
3b7073aa-5ff1-4c31-abf9-56839126cf7f
5913b67d-9562-49f4-a181-92374ba899b4
2649348d-e069-4a7b-b499-ccb6b64cc3b1
7d6facef-4576-4e8d-85f2-1852a670a291
372e9215-4902-4748-8ee7-e623fd82d54a
c7b27981-4ace-4188-bcf3-b50681757c3a
4c2feb1a-fb01-4fe1-9750-25049901e063
3ccad0d7-f5aa-4f1f-8a89-9009d66e1695
4c33a923-56e7-4c27-b677-02d7aa7f6a82
6982bed6-e4d0-4e4b-b7f5-f3a5f7a277e6
2cd3883b-30b6-41b5-bc3d-10b750be19ea
9ca507db-5d86-4000-9e94-f7b196504e69
bbbc5d3b-8cce-430e-8917-80a5b1ee993d
132b31bf-5532-46dd-9dea-937a3bdf6861
da60d020-62e3-4720-8211-68a7a6e64045
94036522-c4c7-4707-9c4c-cd58b029f66a
cc809668-9137-4ad7-bb4d-558e5a687b6d
ee8d0ee6-404c-434c-be81-7edc2b057eb7
4bd3212f-07b1-43dc-bb61-5749da7a10b0
52f77d80-9f3e-40ad-b9ee-cb7fb6daa971
4535c8f1-536b-41b4-8300-2c1c2b8366f7
d975c24f-eb92-483f-b3b9-d520760dd282
37e09ba2-22c5-481d-8740-52b62b1f21f7
950d5f16-e717-472a-9595-904b9ab68bec
71e74952-3a70-45ef-8338-34652d304d5b
cb45ec56-3c03-4081-a576-67ff3d87ff23
2454a180-407c-4262-ac12-02a4dd8ddcc0
34f04fe0-b26d-4e20-9c52-c391ffa0ad84
dde84873-27f0-4f16-b95a-9f1a913ffed4
0ce61432-de63-48fa-b4e5-90bd1efa8132
583f2772-19d7-43cb-9ffa-7cd66f4c9ce9
af4408d1-b0c6-4298-95b1-33ede8d26764
b2d1540f-2e26-46d1-ad26-1f4e5a75602c
b31df6a3-c91a-4511-950b-25b6026bbd17
14201f34-ee6b-42e6-affa-4033a623ecbe
5fb73b6c-18de-43ca-8056-2d75db6ef776
15afa4eb-4739-4851-adc7-bdd66d862261
b624e332-dc1b-4c07-853c-ba0d681fdd7d
4b35e9ac-3f3a-4cf0-a406-b67ca011d24f
57c01ab6-324c-4100-bce1-8c6ba099f4be
0281adc6-791d-4582-bbe4-645dd704650d
28d11bc2-4e7e-4b2c-aabb-d75fe2d89b5b
cdce9ef8-e1fd-4277-97c2-f568504b70ef
83ff4e05-74ef-45c0-bd86-137c725de5ec
7964e83e-e811-494c-aab4-a756e610caf7
ceb5fbca-05ec-4720-bca9-c30dcdbacc2c
976ad93a-beb3-44b2-bd99-f131ec426bd2
8ade1165-c1a4-4112-8887-3a52bc054045
2cbcc445-56e5-436f-a4a6-6f4be8dd72d5
ff716800-1e3b-40f8-bf1c-055bc824c983
5d8fc2bc-5770-4de2-9a9d-dcd070638406
2d6677d8-6575-4d86-a0aa-ea2c0c533009
e04ee068-408e-435a-8949-62b5d26baa60
fb1d7d01-d0ac-4809-945a-623f274d6547
e6a8257e-f6a7-483a-86ba-92b570bd3292
d669cd49-3b7f-4a0a-97ce-2804ae017d54
ec2118b8-4314-42ce-91c7-b40a42d88731
8906ff3d-0458-4131-ba2d-4b050f5396fc
0f4da203-3c68-47c1-b271-363b09b5de16
ea32eff0-cae1-4f25-8e70-ecc572f3b5e4
e2389f26-aaef-4ac6-a738-c3e962a1c9c8
5d83ac71-2ed1-4ede-a037-fc7f48a6ca8c
338808f4-a72c-4401-9961-45f2abc999d2
1845adfb-04df-4b01-83f0-7863452cc6f0
5eae4eb4-c93c-47e5-a61b-0f37fbe05d8b
d621d1ff-22aa-40de-82cf-5aa97c7513e4
9a87ba4f-7054-4431-85aa-4a866d247c11
8a809090-0ba6-4e14-a495-370ab7f8eed5
278cac23-85bf-4619-9efd-516f70bce182
5695160e-6be0-4c04-beb5-fd7c5c908ab5
eacd9541-7932-4001-ad6b-9821c0ad0104
9cf063c8-b7d5-43f5-a809-a4392a4ad7ab
e34e416c-2fbb-4120-8521-fc21fcb7a6c8
447988b8-b14c-47c2-9664-0ab8d80e86e6
ced45741-3931-4acf-8fe3-1d5356649485
e032734a-c3d1-4a53-89c8-56285ffbb49c
d8def336-71a6-4bf2-9c04-15386e5ac42f
f0cd5077-a06d-42d5-ba30-31e67dda38f7
63a6b76d-ac50-4aca-9e25-c5d62c2da4f9
08744645-f525-4bdf-9f94-744b35922e8f
e0495bcf-b240-46c9-8ce3-97701ec3f0c3
50839983-4164-42b1-afb5-274e2941c2ee
64bedad9-ebeb-44db-9947-93160866a8b3
16d96e43-89a0-434f-8c4b-aaaed9e9da01
4be1693f-96cb-4e8c-9b0b-cdc5a64cd3f5
662e544a-3e24-4c3f-925f-8d6b8fb6a901
24283972-469e-4512-8ee8-3b029ef6bf0e
a0210358-ac23-4b14-b1b1-1bdbe67f898e
06e3882b-5132-4e15-b17f-4fe57fc2df2b
2b177abf-d266-4395-8365-dbfcdd7dfd84
906248cc-9f7d-4ae5-87e6-b583af1e9990
bbdeb224-5363-4a1a-bb00-6c5a6eb49578
c3768a35-87e6-48a0-b873-21e93f3030d8
f4347b2c-8108-4856-ada1-915e1688a599
e0971a54-efb6-4bd0-82a4-e4b185b44832
18fb1700-9bdb-4df4-9cd2-dbd31101db29
7c38127d-8827-4664-9c3c-3e0cd953b1ec
f953f9f1-2923-4ad8-9cdb-8442fd49495f
dca37a9e-fca6-47e5-8b88-fc87c099d296
7239c54f-1e3c-4788-8c0d-b05cd2257f24
242e4360-b2cf-40de-9473-79e51dd318af
72d2e208-bc41-4615-b9df-4b8ad2c375a4
d9906d5d-0546-402e-a3ff-7171cd7088ff
186cd9e6-6948-47dc-af31-da0ad412a2ac
adf6dda1-76a4-4dee-8020-4c4674e422f2
470e2e50-39a2-4082-b10d-92e0174fb866
b7025f0a-47b7-4a5e-b6fb-39aa67a6831d
cee43f2e-606c-4e9f-9d1b-b9038d604b5e
75611165-f02f-4d67-8bfb-f12d1aa9f577
ba043aef-e6f0-420b-b152-e6f38c496a4a
ee99990b-5b64-4ee2-ae29-3721496bd1ce
0da8cf5d-04b4-4814-9744-bae717244658
256271a2-f3e4-4428-8d34-5407f8a58487
8600e118-ffc2-40c4-9346-b80f5750f781
78f226ac-bee2-470d-805c-8ceabfff55d1
710a5f2e-18aa-4c62-ad2f-a0c9ed59524a
15a0cea7-f793-4546-9838-be726d31c38a
dfe36cd9-7596-4e34-a332-9c97d7fd1905
5528f0ec-8ef4-45c2-ad65-169e32acd637
7dcd13fb-1489-4bb0-b27b-a8b3e96574ae
2b090e76-8bb8-4de0-be2d-43d05fbc6116
5e1a69df-c6f7-465f-b8e3-6024de21362c
fd3f3aa9-a13e-4670-bdbf-199866748824
d2226a06-5bf4-4018-942b-6849c7b958ab
6e0c9228-2da3-4779-9bb8-f3c2eb509552
c904e5cf-c572-4dc8-9e22-d78332aa2d25
d20646c6-fbc3-47d2-bf7c-acab85bd73ba
0d16a466-8836-4621-8498-93ea84a0e221
4e501b11-b641-4a1d-a17a-4e8c60f3feda
f8f4ea33-8875-415c-aeef-4a7c933021e4
11a54359-8531-4627-82ef-325da8b13130
4ae1579e-a6d6-4530-abaa-7c54ae7f68af
c4134954-ab4a-4c1d-b793-d67b6d2dc83a
2e6771af-db8c-4400-a7a7-f9ff9792dad1
ced1a5b3-0132-45e3-8a07-2afdd4dfe97f
3fdb8d44-30e2-4120-b995-d8f2c101d044
24669d30-7162-4250-961a-b9e42657b4e4
58e85174-ffcb-4d9f-a093-16306cbe60be
5dda01d6-0bf9-4e48-9ac9-4f453a3449b2
6b21d82e-e516-4c2e-9c04-62f332fbdb0f
4867688f-1084-4a16-8b62-28a025f33ed3
1162cc35-f9ca-49dc-a5ce-d92beaa5a19b
ce8a8502-2048-49e6-ab0c-df2997973a13
d9d2ff2b-5942-45c3-9449-bd10a7d3bc00
b1e79e58-c556-42d4-b3c7-872525cd9317
dbf86820-f19e-41ca-9c78-c16542f5efd4
6ed90c94-fbf5-4739-a616-79f01db8834b
01f7e0ab-b39c-4676-b9f6-2252de3eae26
f59b0299-32c3-42ae-bbbc-67fb71be9822
7f7bb340-e2c0-45d6-971a-604923d818d3
3433f34a-3efd-427e-971b-a9a0dadf0c84
b0b08797-3bf8-4896-aa9f-95f57e846010
8da08bab-46f9-4382-a33c-0a857e113e11
c3d45943-f246-46cd-a4d9-ea9b9e8bae36
8d62e869-2161-49b1-b6a8-c92d3f3ef9b6
5d15dfd5-e797-4347-9b82-a2d8a564cc54
8a448049-fd72-4b5a-8375-8b93db4c7b2c
b5609644-cc4a-42bd-837f-532cbba028e7
a457d1f3-5206-4b1f-aadd-049aab1a7953
21e8a224-032d-4c93-ad67-a7bd17a4687b
ca4db4f2-003b-4e8a-8c2c-edf819d00f6b
bb0ea636-296b-417a-b18e-e2899894de69
8d7d6b84-3d2d-4b18-8caa-b5754f5734bb
67a0760e-e746-46b4-8167-134aa8736bcf
168e40a6-3338-422b-b430-76d271c1b0e4
9e3460b1-8f0e-4514-8e02-dc57e6d454a9
13a78647-0989-4d81-b445-420458fb0c13
846d175a-1561-44e4-b339-adaa4b076ba8
2ec1ecc1-36f8-4879-9aed-24b7019f6847
b0f4e072-c45e-4079-bb83-fa5af584139f
adf69cc8-85fb-4eef-a09a-54b210f8c6e7
3793a340-c231-4832-83fe-921b501e76a1
1dc0417d-6a06-49a5-9785-24e4c1fa7f74
3ba58300-c446-4887-bf6e-e742c1e57c9b
91d77a00-af46-433f-874e-ebfb0702f769
1b2cda0f-7620-464e-8cb4-894847f77ead
254c4998-732d-4c21-bb45-08e5445d795e
3daeff72-a904-4858-8581-bf1b56917cf6
ffffbe76-1683-4669-8d20-b96981a8735f
00c620aa-6696-4384-81ea-37a0af0cadf5
b4b69440-5f00-4455-aab5-44d8a5f43de0
3751dbf2-14f9-45be-a785-75e996f2c4bc
92f6e54b-c228-4b8c-a764-05c0b2b8da03
6e3ceebf-a05f-4f3d-8e24-a37a11a08d8d
3cc896af-018c-407e-8cbd-77ef2c6d9196
898d0bef-1a76-44e8-b5d4-72573d300a60
0efce8b7-3a21-42c9-b387-29c94ed19fe8
03beb33a-221c-43e5-a447-2fc165b0e7c1
72e97beb-5a47-48e5-8a85-ea660a84af90
27c5b7cb-10b2-464f-a95e-44d02375ac7f
53324b9a-16e7-4592-b16b-4d63b84d62cc
2aa5ff8f-032b-4fa7-8691-21865cbf4357
34201d44-2436-443b-a639-ae23bcd6e33f
3efc77a9-6264-4145-9007-32d485e23c76
1a4e5be6-522f-4582-811d-a23d4dc82079
2cdb5a1d-504a-488e-a22a-70abc6e4423c
964f56bf-6f19-42d1-9d84-51ac0febb560
4986b189-0d48-438c-bbb1-8d2b52ac2212
e17dbd01-f214-4f72-b017-b945dfc5b53a
480b7fa8-07fe-4365-9fd5-813d2a88d0f6
058395c6-0898-44e1-b19b-ac562147408f
4a54676f-0b51-427c-ad61-94e61d2ca725
73f1a4d3-934f-4197-9ea2-c5fefb3ac1e5
b3d1c41b-a1d3-4d70-901c-f4ed479f7888
a37c569c-8cd6-459c-8d95-a5ff6eff6ed8
c44f7728-0bc5-4019-adbc-696a6a7bd04f
db9d983f-4ae1-451b-95ff-6a8774a0ca41
1fc83b7f-e8cd-45a5-950d-a032adfafb7e
3e2e834e-569e-4cd9-b859-1c7448293bff
55775093-fb62-42ce-8166-05c4cb3a1c46
b8bfb6f0-0fdc-4614-a275-8934c0238727
14350586-9eb3-480f-bc32-7ba798a4af68
f6ff5b5b-e4c4-4153-8665-bc7c00c3501c
664d7f95-d1a6-4bc6-9f3b-57193869dad0
60f0e27d-7b48-49c4-8e57-9d71b3d9fbc4
509aab18-7237-45f9-bfd1-db7cf090af79
6998bbfe-ec59-421a-9248-c4497e515681
71483d12-4965-408b-b229-254edb92fb2b
25024021-b3f0-41c7-97bb-0114e8bf4497
e0a86f61-afe0-41fd-ab4e-820d63171b3a
f63daab1-a970-4ff3-96a1-1464862ba7cc
c128ee88-9fc7-4262-bf61-6c5051fe2aac
ddc626ff-eb1b-4666-b2fe-570ae19241b6
b79c6951-6c9e-4ce3-ae23-3f00ab7ba7b3
fdef6c4b-edde-422d-a8e3-ad6ffd2399ff
caab41c0-51bc-4ed4-bad0-06b5645fff2e
10e2dda2-3f27-494d-b35f-b20bea0388ed
cd293b0a-9d27-4fe8-8423-789afe018e4d
63a64f2d-7f46-437d-9786-dfdf813a8924
042393cc-ca1a-4323-ab79-2fd02b908e09
f6b44d2d-2d88-455c-beb4-b23cf5b0d10f
b3b6bcc8-b441-4a7b-a969-38d5b88834eb
4eedc07b-c421-4b98-8591-b06541181301
59e0f787-1748-43fd-adb2-b49cb0d65cd6
ecee82b4-1571-498c-8bc9-c1cfc615dde0
57c88679-0e5b-4e32-9279-a0b9ebb4c42b
b74c3940-3ba5-4187-ae21-c39c233617a0
d37f871d-2608-453f-937e-585e517f2c1e
08ffe6a1-af83-448d-840c-e421216372a8
9bd99606-1b75-42e9-9b73-fe499f1f8d2c
63e9b7b1-f371-44dd-a976-3132daacd4e3
69ec50cf-0e1c-45e7-9841-1deba62f19a0
6364d7ea-97fb-41cb-a628-f7e8b10d4db4
074a0404-4fba-407b-a167-9fbd266b8974
209bbb9f-d3f2-4878-9bc0-108e1611f4fe
5c3a8119-26aa-45a5-97f2-187672364dd6
2125f982-5f7b-45d3-8bd0-1b9c5ed50f9e
766fe412-a463-4ec7-9c27-cdd326f9b4bd
46d52f87-2e40-4b49-a0f0-af6836728c0f
7f4446ca-ac8b-42b3-89bb-a091fec69cbb
5fcb24cc-d24b-47c5-80f0-a8630a39c587
c41ff7f4-c8b5-4b80-b091-4c3d23f0cd9e
706baa53-2d82-45fb-878e-0fe29107aecc
cd395729-198d-40bd-99b2-78f8253d5b64
382df327-3a21-4b2c-9a7b-da99936bc7b0
9864b70d-5e5b-4886-adca-7a28faae0b0b
49624ab5-6666-4a00-8b8c-d986ffedce4f
864508ba-beb9-4d83-885e-f92129bcd499
c9bb8f7b-225e-441d-bcea-2a7f4118b816
560e4d05-ec25-4407-b29a-016ce0da1291
affcf0d7-140c-4d34-aac0-3e9273806086
89f82675-4785-48ad-9fe4-70e78a2a318d
700f0339-2d6a-4aa6-966e-03a451fd3fd4
2f9ec3ec-d599-490e-a83a-6215db61fd08
4698c0ef-ae8a-4b95-b57f-e8af4f4d7fff
b6d95194-d02b-4c90-8bd7-b44e736cca96
07a5a414-63b6-4f38-9356-3a379996d2c1
eeeb246b-68bc-4839-9e5e-ba617eb2edf4
223ef5d5-427f-4e35-85b6-27151e9078a7
1688bbcf-05ba-4fe4-b2d5-03a352afecc2
a4421d89-94b0-4509-9cc3-2e7b0e46a709
c0cfdac1-65ea-4975-b83e-8497fd6b0eba
4ea737f3-1af1-4f34-b4b9-dbb20f3f4684
72c7c8c1-a8be-4e80-b93b-d9fdca7ea68b
4da2db12-3625-4f9c-bd6c-b5d207f2583d
7e00812d-87cd-48a2-8248-6209bba6ca84
4154ac2d-8529-4d66-aac8-6b2b67f69f03
a247313c-f40a-4450-bf30-f0b30d47c9b0
30077b9d-32d5-4be0-b3ff-67c99a5b56d1
86c9c2b2-133a-4d41-b3ba-0b173b91a292
36fcad3d-627e-40d8-9174-6ff3f39d5ca3
943134a0-7620-4215-b5ab-7f3a4ecc3a1f
1fd5598c-69b9-440f-82db-a2de53d97604
98d5dc95-11c1-4caa-ad29-aae6cb5d4403
8749a61b-bcc6-4be0-96c7-70037a4888b9
5f602dc5-6b7f-4389-a0bd-4cc68bac5b9f
1c709891-ba62-4271-93d8-74b8e55ba968
0d18361a-5f03-4b67-ab53-cff0cf6fff8b
b40e73b5-001c-463a-8476-a2f3fb480eb4
e8a05924-2c0a-493a-9c44-fe6451004fa4
57260cb3-f218-4492-b05d-375a6202fb7d
be0262ab-176d-4f1b-a19b-cecc11d8c1c5
091f7b26-d9e0-453a-ad77-b20aaa295a89
950dc356-4db0-444b-8be3-249bcc2f4f00
b65fa76c-c2d5-4c00-aa49-983daa62ec1a
325581f8-e2ba-4d64-b7d7-f8c330b9e2e8
3124b7e9-7013-45e8-af7f-f50653608149
83c3aa1a-57ce-45ea-8be3-3dcac827a11a
63c1045b-2720-494b-b2b2-1f971f380892
b33478d0-3e43-48b2-86b2-8fe2e629d234
9abc1305-5b38-40ff-95c3-e6ef52393f94
10915f03-811b-499b-981a-64ae76a6f402
0125c0b0-d07f-4606-8898-0dd14a6f6018
9c714cfb-db16-4920-ab9e-d6fa2ff4bdac
4f969d79-f5bd-48be-9a93-15516ef784a6
54697ce2-5150-4708-b625-0a3f5e7da446
6b30837f-99f9-4650-8652-81482f66b526
b677302c-1b1a-4b20-a70b-9268b00107ff
b0b9b0c9-46ff-4cc1-a83f-0c9113e596b6
62644a3c-bed2-4ba5-b095-fc411aa85f02
804ff758-7661-4e08-9932-84fccdd52084
4b82eee0-866e-4b36-997e-93e6d955738d
0a4a76a0-62d7-4764-b1ac-447763919981
d26b246c-909a-4011-a44c-7884ca6003f8
2fe2d5e6-2297-4ed0-ab3d-9c27cb7a6876
b5101dbe-5b38-43cc-8ab8-5cca05f0e493
3340b195-bcf1-4bd7-8147-e05111da2a62
29ca3fe6-c497-4f8a-8540-503b208a3ca6
9753a20c-9ee7-4689-91c8-08ac07736736
b2868fdc-0f75-489a-bdba-6d9d7fc70868
82789c0e-789a-4575-81f0-017d61d86f94
635e23be-c727-4608-9ffc-2cbd8eb1c2b8
a914800a-7b87-486a-85e9-6c5265756a18
a8701af7-80d2-41f7-b67c-45663802dbfa
744adaa5-6d90-4512-aeb9-b37d37c13010
a76c40ae-90a5-48ac-bd07-f1f34f8e8109
3acf4883-0ae5-49b7-91f1-bf19848e8a68
0ee20d69-95e7-4303-a432-4143b7581095
2de8f859-26f7-469f-aacd-1a28d2422d7e
c24a71d3-5acd-473e-a019-805d5b7f3cc5
d2c76a0c-389c-440c-941e-4a284c3dddb0
fa2b7593-e03a-45d1-b8a9-3870f6806e5d
8e1460e5-dd61-4411-b992-7840a3a788ad
4f7d0270-8b57-48d8-ba99-36218e6720b7
aa7804e5-5177-4f64-8e90-6363d9fe7b9a
d77094e1-4fc4-425b-9289-b977f81aa83e
c64b6864-1c94-4496-9f2b-17ac9614dafb
84d67bc1-14e0-471d-ad23-11017f49b29a
29d8db4a-5dc5-4040-b5b9-83c546fe92d7
a8a9a43f-93f0-4e4f-a3b6-1f1219a16d07
c04254db-2309-471c-a0a2-df700eb8e715
f5852f11-e6da-413c-a05f-351cdcad5180
eafbdc7e-a960-4db4-9c5c-c480c08fa803
1c34f9d9-cec7-462f-9c47-e2303661a6b5
6b94edc3-f9ac-4b87-801a-0a70570ce732
0116daf9-e749-462c-92b0-78280d68da40
dc2a9ec9-be77-4042-8ca0-7c9b566eb6bb
2495aefe-78ba-49ba-868c-2bc306b1f2c8
591869eb-3ceb-44de-9708-3ead104dc579
3f102748-9987-4fec-84e4-21360684a06f
b0e770f1-e66e-4298-b5a6-69f121a3f06e
b87c4550-7988-4ea5-a419-e2fdb7d3d9ab
2795ffe0-824a-4467-af41-7000dd5c0f26
5585854d-e741-4096-b4d2-26233f762d5f
c4adb011-ac95-4183-b817-85474db1b5e0
bb74213f-0060-43b6-963a-f758ada9b504
cf526db7-9e08-4512-ad37-a857480f54ba
66901ea7-5c60-4f79-b1b7-a15c6d2b9947
ec8d4f51-c334-4d50-9b4d-aaa1979219f8
20316f51-ecaa-4048-9416-1f9994544454
0dc88c21-c7b6-40cb-9a46-6a3c0e7900eb
f6bec5ca-87b8-44be-a135-48a326572152
6c2a1c00-fde2-4693-833b-88aec4937447
f4048efd-8579-4401-9326-a293fa39d8b8
90c1297e-d74d-4c79-86f7-19554ae72ac0
9e5bb596-ffec-42e7-a42b-7f10b1dcad3b
ec3c3d43-eecf-4ed5-86a3-ffccc82f5450
57aadcae-9558-4d48-b323-540b7b39a6c3
cdef1640-3ede-4b14-8c45-740108506bb6
5a9bdc84-4b6a-415f-8892-4703f57829d3
d7dd7edb-56e3-4c11-8bab-328098b3755f
4f73b3c7-834b-4e83-9357-7d6da5f9af59
4ed2bede-ecd9-4b04-89bc-8898f8328d4b
fa36fc31-42c7-4d50-b9e7-45a02400b0ed
e7ace5cd-102c-479f-b183-439813a086de
f3157aec-c67d-4d3b-a87d-75c72af22aff
aaa6e8a0-cde8-4906-b162-297cc2038beb
bc0a0887-31fb-4157-b334-be296d58dfc3
54a3c22a-224c-45bc-b834-94f84409b7b3
4eefc14f-876d-4338-bd3f-35b234c196b5
3a574d97-6421-4218-93ba-b9b9c2eb9300
6eb1ccc1-b52b-40a2-a1f2-00dabfe4481f
8cfa46a3-092e-4348-9074-4831d225d3ca
dec9c911-4e96-4f48-9de3-79485ccf076b
d49e3e65-f4c5-4a3a-b03c-c507e1efc3e4
1c55a287-30c9-470b-a6fd-b73b12bda745
69eaa735-983b-4d94-944e-5b0ff0bc868b
3015cc74-471c-4bda-b32d-3a4e806874d8
fea1efd9-269e-43ee-af5a-8a213bebc997
fab2a2b5-e6cf-408c-a00e-17fe0cc04549
ca9c3186-da60-4b1e-94b7-ab4355f3e33b
7f16613a-6de7-4175-aad4-6d4990cdc832
ad7cf020-352c-4712-96dd-67a2843fa9c3
001c05dc-8326-4fb5-b4fa-083fcbf0c46d
483678bc-b05b-4a61-8123-9cf8dee347c8
46edc4bf-83ad-497f-9967-985668f14987
b30a48ed-7ab8-4709-b44c-c9557193f172
e6a8a6bc-ed45-4f97-bc8e-d50fa5f5b58b
45baee4a-86bb-4ecd-82f7-d17cf6064a89
b095bc30-a393-4284-864d-be2415551da2
cbce4d39-0e16-4c27-9e60-9432737ab789
a66c3070-d52e-4a5b-ba74-60c6bf4cbd33
70bfca86-66d5-44e5-9959-17498a595ef9
68a5f4b7-75ee-4766-9931-449f367a98b7
34745d4a-95d8-4b65-ade6-b2edd8a06eeb
7226792a-a6c1-43aa-bf47-2d61739947ce
64149c67-ace0-4c10-ab94-d79cbc1a630e
9d1ba985-9bbb-4d1b-bcd5-dc5338146590
f39f6424-3bf5-463a-ae45-385171f2c413
cff00be6-b309-4938-91ae-bd03e9d6dada
3ef264a1-ebeb-461b-a6f8-3f364b592657
5c1621ba-6c20-4951-ab52-1edec87b59ef
6344e2e6-50ae-4526-afcb-6a28a54de676
a27cb10b-6ce6-4606-8937-296345144896
59753ebb-6e60-4ef7-a25d-2bb4ae447fbc
2e676afd-1667-4da3-b311-e83dd137b1f4
c0a3e492-8f83-40bd-9c36-e2159e8f3fc1
5eb1b375-0752-4b95-a3af-ff4ab11b9ee7
e2328685-5f19-4f3a-ba11-f66a03bb5a0e
989c0486-56a0-46c8-ba18-30cb1a6a0df9
f3d67738-5485-4f2a-a63a-ff8a1ce8344d
b778103e-90a6-4f4f-8151-78d3feb1fa79
618889dd-9f82-4c5b-b310-d596a45eddc8
f67adc99-6585-4ce3-b292-25bb59f095f1
1bc46468-9bfb-4d50-bd4f-00e91f0e7143
a51dba58-7e50-4fc0-98c3-1a78d50a8d23
da9e63ee-9b25-43d7-bc58-0b206944dc28
ae1bb5d4-fc22-49a7-9470-867fff2b1d20
4b108751-b62c-4124-bca2-fe1a148c3c18
31faee52-e084-41a5-b3a3-09c097dfb10b
ab956d87-28ff-4010-ab3a-e9c9c9a24f78
466f09b2-8ccc-4253-bc96-2e1d8117414f
9a298d90-7dcf-4343-8a3b-185bf313d611
b77d08e1-6e8c-446d-8a5d-041983261d6c
5ced51f8-90c1-48b7-8580-b11d3392e3fa
aa3ef183-8f64-4177-877a-3062a5a2b8d7
e69ec06f-a702-498a-b084-f71b2842c18c
6749db21-29d4-473d-b137-b8ed1d2ee84e
5492f9ec-1922-4e66-9f2b-03bfcb5c5e47
2e5fd7bf-467b-472d-abab-a868983544c1
aced97aa-7f39-4e53-8528-162fdbdbfa97
322f300c-d589-429a-8170-4f791f134dde
e07c9cfa-6fa0-416e-a70e-682a5a88786e
6068c505-216c-4dc1-8817-70ca6a1a8ba3
73abd061-9295-4a72-b5b3-88a4a459ede9
45c0e544-b5b6-42b6-89a2-c0891b6fd447
db3b3cbc-39b3-4860-88fb-cf0d6f226a8e
8af91794-4746-4a23-a4c7-d7272e5e368d
3f7cbd03-45b2-4540-944c-a08a035a1351
c9452936-3333-4e9d-8a33-f6d3f9d3b4bb
ae0c5034-a009-48f8-abc3-055b07da3f2c
3684021c-b5a0-43dd-b9f6-1fb8be66f2c8
86d73e49-c2b7-4e2b-9073-ed03f657df99
095e0d74-94dc-486c-9cbe-4b00e4c44e33
93c2adac-ca97-4d1b-8d68-53ce5ef27e91
9edb6314-0931-471b-b4e5-073546a80487
24b71f4d-0622-4a2f-a044-0a3db58049d3
cc38253f-7606-432e-8a3e-c3f9ab1e080b
ad4eb638-e15e-4801-89af-3d78df89cdfd
94258ca9-f9c4-4946-b123-7c729034b339
d09ac74d-e8f3-48ac-8b2d-3e69afeb143a
89446367-eb2e-4ace-b1a3-f6b58cdd0b0c
5c043d1a-6986-4ef1-baae-93a88b74ee28
9e546fd9-96e1-4931-a526-0289f50abc40
5b3315e6-e96d-4fce-94d9-2a6cc8944f66
3c90bf65-3411-410f-8adc-56a0d5b61765
551eafb7-03ed-4957-9ed8-3f2b48a357a2
aa0fbe6c-e1c0-4a10-a359-3f18aa4f342c
eab74e03-ff47-4a38-9f4b-52f3bcfc3ec1
fdc00424-0e8a-4c60-b796-37bc4524cb4b
36cb48c2-de35-4300-9a77-75a4c3b21aa1
0a76bd9f-07ac-4d19-b98c-916c86a08653
9becbc69-ebb0-42c6-8573-e450be470af6
268597af-0478-47dd-8ab9-dcffb2b7d350
01c16cd1-7ad7-4e36-9b6b-75c3bf5195ca
352b4946-372e-42ff-b743-152607270f56
b37b83b5-a84b-48f7-8bce-478687fc0578
af18cda8-464d-4f55-8e58-0cbeea14ebfb
eef46ce9-db4b-4694-97c6-eef71f6cbd70
f0b5047d-6be5-4077-a4ec-1b3f13bedc80
f6dee139-0037-487d-aed2-2f5c104a614c
49cce6b6-f869-409e-8b8a-6bd10eae00d4
9e1a427a-1fd3-46de-9876-fce990d66c1c
7e2b05b2-88e5-471d-913d-b8c2723405b1
c8927cb9-1bd5-46f2-8fc7-fbd2a6fab0e6
f8735237-a814-4c5b-8919-969dca974882
69bd5075-7552-41eb-861a-3776b050630c
e184cdf4-d2e2-47d9-b69d-cafcede0afcc
8050a645-63b2-4eb8-a3c8-24bd2f594cbc
8d4bfda8-ea0a-418d-afd8-9f79edd93d8c
482296ca-71e9-4b49-b61c-d214aa546bd8
c1e34346-7682-4b18-aa39-d0fbc05697d7
6c9a9036-05df-4bfa-a23b-ebdb51cf1ef1
ef88d2e9-240d-4583-ab00-df23d15c78a3
913eeaf5-e8b3-459e-9d3e-ddfb9395461a
cc43e72f-0f4f-46e1-82cf-5c942de2a02f
9941d674-9078-4a40-905a-4b8e30fff402
93cf446f-32c1-4867-a8e5-3ef4460418da
4b869685-3518-4a44-8fe2-143e3160c740
7a182532-52ad-4edb-bc18-60452f5f5c7d
d5134681-b39a-4828-9a4b-e169ffab1b5a
21e54fab-eac7-47a6-affc-ed7100556cf4
c4b53d6f-10b4-466f-920a-ca6b0647b07e
033eae3f-25fb-4c85-af94-a9ea78d6ee88
7faa587d-9bd0-4122-81f9-e34441478c6b
323b148f-a8ef-479f-a7d0-92284555d654
0192df2b-953a-4591-8f3a-9c9425d2cbbc
d2ce7e86-85d1-4fe4-8eac-9e68e3cc6fd2
90b6fb27-60be-48bf-9286-d717480eb4ce
b07c7ef1-a00d-4152-831d-4422204c3949
09f7ba4e-5370-425f-8e6f-ae71932f627b
d87c5645-b0c6-4da9-989e-cd1dd315a8e5
a738c072-9e04-4b2a-85cb-38fc972ab80c
1b432bf6-05ee-421d-af56-feab8232c854
a6318783-947c-45c4-b4c5-d14c9e51411a
10d4cebc-c2f6-43f4-8434-7721e1959a4f
5ad4ca9f-efbd-4578-812b-fd276d6c7273
a54629bc-0d85-47ff-a9b8-791ab46e7d0d
c76bde0e-e08a-4cb4-9222-9d295db18e68
e0008b7c-64eb-4dbe-a957-8fc95421dbbc
09c7d138-931d-48bd-8af5-4c1bb93cd266
3805c9b6-75a5-42a8-ab16-09114a5373d3
24211099-5db5-4928-9c2a-25a125548b82
a2db348e-0b76-45d6-9f8b-a8872b2a744c
0f34990f-5c4b-462e-a6af-c9c104ce999e
1342ed7a-71ce-4b99-b0c5-598d1b3a8709
a40254d6-6ede-4aa1-866e-b7a979dcaf26
c41ccc93-ca68-4239-893f-f0568de935f6
a69421dc-d643-47af-b814-428a9521f7e2
6227418a-17a9-4df0-b7b8-e0f42629cc3d
2e46ed21-11c5-41e6-bb96-36d05fd18aba
667b4403-a6ad-428d-bd67-e7d4976c0e25
9868c75b-8765-4b93-8654-b3aa9bb4005d
6ebf2b2c-3b08-4896-b61f-a44c80ecd068
00f678f5-7e38-4d20-9cd9-7a93b1a5cc35
675c0db1-8afd-4071-b9de-752fce0fbb3b
c456bde9-c660-485b-b794-2dab2a43a50f
dd9a0249-8ddf-499e-90b3-464e29cf67e7
6585300e-6f5d-4124-88a4-0a599d16586d
9e648256-62da-4665-b750-81f7d0fb38f7
98569c3d-373e-4421-a6da-4a3250ed2423
7b0b997f-a89a-4d95-b0fe-5792775d9a58
da999572-84dc-4fd5-b9de-a278a3526979
554a4ff3-89cf-4cec-b2d1-486356305c3d
6557c4d3-b7fb-4cf0-8c64-90af39771f3a
ff4decb2-f1a3-458e-b63e-f9fdae8258be
d250bbd1-2c1f-47e2-8f6f-e0f623c572bc
dd7e4688-d15c-4583-8e05-c937287e68f6
a332da91-3480-45de-b44a-4b4dcb743053
f683d4d6-a15e-467f-bfad-9fd7c4c27d6d
78f89635-8d5f-421a-8754-eafd9560ebe0
dec23716-b7d0-4ca8-809f-64e01fdb91bc
71b27ca8-8a96-45d4-8422-52d8b24a03d5
5ba0a892-c85a-4e60-88b7-867d7dd765cf
88cce1ff-52b7-4db2-aed9-cedcfeba83a1
34522995-f616-4d2c-a04d-6218eac21138
8d1bb32c-4b53-41ba-875f-19af026fb26c
832b2c92-372f-4840-ba76-ad812a026aa9
23889a79-cf44-416e-90b7-be56fee35193
7e5edb8c-d62d-45af-bc64-cd3538353330
27ebd534-dfa4-4d45-8e03-126e1274a1d7
a2bc587e-a8d1-414d-a30c-eb26d537dff4
21afbe32-53b0-4596-af72-58b10e30994f
580aafcb-3b52-4943-bb9b-e3b4c3aab063
6746f6e4-799d-496b-9929-4c46f8e46325
f42c6707-042f-456b-91dc-8beb0c28372b
a0e2c56a-961c-4139-b11a-96a0145fe147
a4f42538-013c-4edf-913c-3c91186721a7
5a0ac348-2876-493a-827d-9bece3b6e5e6
2dbe62d0-7e7b-4c0d-9592-035814b2801d
ef4a151b-5d18-47ef-90af-29a90bd77f51
847dbfec-6742-4528-a92d-676dbe128ad1
7deab527-2496-4fb1-bef7-19208091111e
4eff8158-6edd-4622-9826-ca72db933962
109497df-2605-46ed-a920-adaf3044b9aa
f56f5608-e79a-4179-b44b-72b69eaf5b87
e6ea9e94-f676-4179-9740-b4fc3ba1933d
2ac23014-5af5-4918-8275-4fef82147ab7
a8190f73-1913-4a58-8f30-ed7d6e26d8ae
6765d591-b060-48e2-aee9-39078077e651
8a8c43c3-61fd-4913-bc69-7809e204eebd
bfedf802-abfc-4742-8633-317fda8fc4cd
3fb95d62-7bb9-45ea-80af-388178e01407
4eda7d8a-ddf4-4a83-8643-16ec688a0f1a
38bf781a-cc2c-44f8-8da2-ee2e99d7a660
bfd0fa6b-f9d7-4053-83af-8c4c2a0cd138
f358b3de-e17a-444a-b6e4-988ad6f09c59
d49e97a1-42d3-4856-8f41-e8fd1f5647f9
b968fda4-2f2d-4886-8328-316f46b19152
647167c5-61d6-4a88-b1dd-a94f0ef09c58
c1fd239f-30b7-477c-a5f5-a84bbc81aad9
a37f1d86-79a9-4ce2-b626-3f941209f9dc
dddbb261-fb3b-4474-b9b8-960141e9fbdd
995b2160-2085-4f62-9bdb-d22ad8ad3847
c6f06913-1abf-4e01-9a09-1645ddabf758
d243ca4b-99a3-46c8-8803-a18ff3083fd2
4dae262e-30fd-43af-95d2-f2d7666e599b
a3ccf6c3-7a3e-4b0f-b576-f09a37c870d2
78e4283f-03f2-4407-a5a8-db81047d93ae
c1fe8424-afd0-4b00-bf86-ef8d0923e88d
7ed09c80-87f6-45f3-b2dd-e71549b6537d
4088c245-b9ee-42e0-929e-455968f19361
5c334d30-7fb6-4b48-a3a7-79a336874078
1befe15a-3f2f-4e24-a2b8-e34fb64a4564
b33523db-faaf-4565-a8e0-626957b91aee
d3892109-89f8-4633-819c-27443498f699
956cf083-79a8-4472-85fc-9e1051cf849d
5b1ad403-9e8b-48d4-aac6-c9d31e041a5f
3ae7fb45-7738-43e3-8a2c-a2d2d1b931bf
51dfbdc8-224d-4702-bd4c-a8e93d488854
fb8af990-8995-432e-9726-09d23a0af650
7553619e-001b-40a0-8926-e82dc7804087
1ce1f1a3-58e0-49a5-817d-a35da1615c01
01eec53d-a916-4dab-97fa-8facf114546c
554c2a7f-1aa3-44d5-88cb-5023803eb8b8
f4508fd2-b881-476d-8db0-e861f1faa725
dcde698e-21a2-42fa-a309-d355983aaac7
d1f431c3-2ff2-4829-a81c-c950196a5c8a
a640e72d-8bb5-40ef-8ffb-4de981a1b8af
e2253f2f-62ed-4191-ac28-5ef7702e82cf
e16197f8-9f26-4c51-b6b9-5432f7e2071f
325c69a5-6fd2-4866-a5f4-d72744dbade7
d1b86bbe-9fc7-4ca6-8e03-ba23d4bbfa39
6e77c68b-ca11-43df-8ff5-aa681276be8d
6f34f366-5a89-4d58-94dc-ed6688cf4de1
acb98cc2-8a34-4925-b41f-c6d98fef15a4
4706757e-05e4-48aa-bde4-93cfbb9ec6a3
468e924d-893b-4938-8191-c63ffe07876d
82dc87ad-cb71-4d5b-a180-628af3df9a4c
41f06e3a-0291-4869-aeba-0311a241bd8f
c991b1ae-a0d4-45b7-957b-ec3bb8d1d88a
6f9678ea-c318-4112-a668-0608c05f0d5a
6489fbd2-f10a-4f0b-a3dc-e6408055a9f8
e9d5686f-59fc-4e56-bfe5-00c6f34796e0
32dcb0b3-fe2a-4ef9-9199-4455c4feaf6f
ebb83a6b-8f9e-4dcf-94f7-90e69e104a99
195894ba-f80e-4789-8093-8f2af46457ad
3c65d11c-bc04-40f2-9b5a-97dad9089ac6
3e02088b-c1cb-4df1-86a5-6f6a83fbbd46
2ff806cc-0a6c-43df-8dbe-0c3be6510939
0a4246be-5dc4-4f20-848a-1095c9b27a7d
1d85d0ea-c70a-445b-864b-9ab3d6013dbe
87274917-afff-479e-99a7-0a5807d8ef88
56b5fe95-06eb-4736-a2f9-fffa79533c6c
6af28440-5e1b-4413-9b00-f0c1b916156a
d4c77b1d-2c12-4a5c-a5ef-e005ae81b716
45d12700-706d-4194-9044-57ca171fc1a9
b8dd3d78-3220-4efb-9fb0-6d173604696b
6ac9d2f4-f756-4ab8-b2b8-1138b7686331
bb214e4e-8d5f-400a-8107-755e9f29eb9c
de4194e7-b3f0-44b3-bc3d-deeec5aeb3a2
373451d3-5a7a-4e2e-8f85-388bec2c6d07
6efcfaf0-4584-4f48-8fc7-79a378227ca7
3cb7c2a3-bc86-462f-85af-9f6bf46bbdab
4d6546cd-41fd-43c0-9f99-e12cf3b1ea57
aa2c2127-12f7-47e9-9100-03d92c338de0
0faccb1f-baca-4e12-b112-0cca42a012da
de906a13-38b5-46fc-b6f2-e593ed74835a
f74c4449-9ffe-4ce5-99a6-e805f545c253
a3fd4f34-a7c8-41cf-946d-67d5297e6d7b
4cd0d39f-c443-494e-ab4f-b6bf4aee7587
41be375e-a40f-46a6-a863-10b63d285a22
7b24c0ff-05b6-4c4e-844d-2e6c8b26cc83
894ffbce-8f75-44dc-bde6-1b235cbb4c7b
a1562fa9-f45c-4b63-a0b7-c506021b9481
3685dff0-3aba-486f-b535-76a7c9d608c0
ac962598-c223-490d-b015-6f41cea20f14
96699836-2efb-4bca-93cd-69bf63662e2e
22d8ae01-e5a2-4600-b0c7-8e53ff55cd2e
6ff64f10-1ae9-4d18-95bc-4f2172d96d58
43392841-7f69-4949-ad0d-d95c505beb47
bb6adaf2-02b7-4499-8f3d-c101b46ad5c2
7903134c-9b78-4a3c-8d3d-064bf4ffee99
5acfbbf2-e8f9-43c7-9a79-fd6a4886d59c
47933c39-888a-491a-97be-354c71c64a59
3e2e027b-8247-4cc1-8e74-01da9589d8dc
1aa30830-71b9-4d2a-b7cd-798f4f03ac07
35b97161-8d2c-4572-bd06-b1d7e32bd10b
f4553756-56b8-47db-ba02-5760abb06b1b
7a4f6182-6397-49c2-991f-18bdf69f8b07
08370006-dc96-40ca-bbff-fec01156aa7a
dfee94e8-f993-4be6-9259-360119f1fcf5
c7d2cc99-2e11-4995-98aa-7c837595252c
0554a0e4-3269-42dc-8bc2-768d4976dadb
07d823c3-c98f-4b01-a765-2f29be0ccfe9
f4d46938-1943-4370-96a0-1b2a285394be
b88a964a-afc9-44d7-aafe-3147f5b42ebe
7c747e31-ec60-49df-9e45-ec09a887b2f6
289d263f-ba40-47c6-998c-0b3c37c5ab7b
935ab5cd-4053-4ba9-bf23-6ffb6c7c7667
7be6f663-260a-4e37-902a-9fdb047cff09
b72eaae1-4908-49ae-ae42-9247690cefc2
89b1609f-14d7-4e99-9dc2-398032a2b264
9634b327-63bf-47f2-91bf-6764712539cd
feea2f90-73d6-4b41-b730-9d442dcbb494
23f70ecd-c7aa-42c8-96db-6167194b8721
fe080ef3-0feb-4bbd-ab85-4aed2a4c8df6
4ddc3b59-744c-4cde-a11b-71a0342b8795
f06b3f01-98e1-462d-9b35-904b504c4096
1636bfc0-669c-4b5c-92d2-e07fdf961952
7169be58-097d-45e7-b2df-8f58e85aa221
62d2e7b2-d92c-43dd-ad96-90261f7bdc74
41570753-5820-4295-9ab3-7520568202ef
8a8ed3cc-604a-4a7f-89b6-09e77ed1254c
7f724dcb-ffce-4e89-bd6e-6a8a663d967f
b43d59ec-e64d-4c67-8c35-93d1b1ceccad
e0e734e7-292d-433c-9434-c3dc38adaa31
2f9795d2-3ffd-4e85-88be-2eb8e990d726
7ad468dd-b92a-44f0-a860-95863f4dce67
00525eaa-b2bd-4d93-8e5a-21d3d027b0f1
24ef5019-5ac7-43c2-b5ba-5093a360cf34
703c2208-d569-45c2-8c7d-e38c9869684a
602f8005-6830-4b77-abfa-6fa568883a59
4b06247a-7487-42de-9eb3-c94801ec88c5
d987b61b-118a-48e7-94ff-3dfbd228790b
fc7bf5c1-7150-43ea-a596-edf110e907cc
c83eecf4-bfb5-47bb-aa34-712008eba566
432441f2-84a3-4cc3-bbfe-c22ef2d5e89b
a02c054e-18a9-4445-aa06-2cc3aa1ee833
2588e573-c711-4fb5-9fa1-87b56c027e4a
a48b1e20-f823-439b-a839-0e23be4d768d
dd3da72e-1886-4577-9b5b-fd86eb24a27b
61484e55-9f6f-4ac1-8c05-e1724c623138
524dda40-4712-4951-a84f-44dc93533b2a
0e9c5b51-d73a-424a-9f1f-542402e6a95a
99637d6d-982c-44e6-8a46-108109269939
baa0344d-e4e2-4785-a5c9-26b5cccee3eb
c38be00a-5631-46af-8f12-e426c5625160
e4fcffaa-249c-4065-b721-021f57f017ec
deb1c596-aa23-49d1-ac24-d636fb8cf487
f26d6bd8-ea96-4744-b12b-9c6195b28b63
a915fe3c-8160-4c32-9865-1604804a7901
99d57eb0-ca3f-47c9-b7fb-feb2e20c6828
5f3a3b3e-be86-4f0f-89b2-d26a9470b954
461388e8-a3b5-4b11-b18f-19f396cb5b34
2db79822-8d6b-4655-9a0f-576c9e4f2c54
d2397c9a-9ad1-4e86-adb0-a807b44d8e34
18b5521f-2f9f-471d-b9e4-f4c6a7df29f7
a04765a3-8c1c-4d3f-87d8-ecaacd4cc3fd
80a00a56-d301-44d9-b99e-c5aae6c5dc9f
d09cb01a-f329-4acf-b960-a6294382a4bb
dff322d1-283a-4773-a662-46f20169e582
59b2b0b2-cde6-41a4-8188-c80e22083f2d
1d4511d8-e5b9-4755-98d7-96022de90f2b
cae56218-89fe-403e-a3b6-39dafd268040
42941f9a-71cd-4003-8017-38b3d7577933
b2a1ff69-604a-4121-ae5a-38aeca99b6be
a78eaa9e-5876-4618-b46e-77e628f3a281
bd058292-9efc-41d8-af40-152dbdd72deb
94f638bc-4a4c-4404-978e-27819b20b785
a3382d82-e485-4f6d-b5e7-d87883a549b6
cf9b992c-8174-452d-b3ab-95d415acd83e
05c80ac5-fa6f-4bce-90a4-4a29cc4b5569
dbf18c49-241e-480b-9fd0-bd0b9681945d
94c9fd1e-5251-484d-8581-e04730e91860
b367d1ed-4b17-42b0-b750-5cf0fd60a431
587ab22f-fe1d-4a57-a07a-de82eab9576c
2a305274-6044-42c4-8795-9e2efa564015
1d82e55c-770f-4fd4-8a99-4280ca05d1f5
cfc73742-1b22-4b5e-b276-ac83db6f2506
7bef34e3-3454-4567-ba8b-f25cdde3ef25
12aa3d3f-ecf3-4676-9a8b-c86ff3a6ce09
9ac38fdf-6037-42e2-a958-d2f470b49215
d0fc173d-8f6e-4d81-b6e8-f567d6a2ec3b
7ad9dce3-6dbd-483a-9fd4-710974ba1317
cce6d3d8-9909-43ba-b302-827519b8f4a2
742fb46c-acdd-4b27-a4ec-f025e1eec538
914d4421-decf-4b21-8e63-6737e19abf16
f79afdf6-7001-4226-a1f2-59a3d55473bb
7253f88e-518e-47c9-90ac-2809588e948e
97b22c37-8747-4167-84ba-0cf9ffe5a249
566e4fff-c942-4914-8531-fa1bc2bed367
ad04e8d5-e2c8-4874-a0f0-00e83eb8dbf0
bdc4b95f-e2ae-405b-8ffb-6a8bece85ce6
4056c815-a2d5-4add-b23a-b4e177a14207
70abf7ec-b94c-421d-86ee-bce29f08e98a
8e944490-d63f-4dfa-9325-40e7f6fa0d5c
ffc1f0a2-b161-47c9-a797-849739f0eaba
d0cd572b-3caa-4e56-8cab-a94fc9792f8b
f6d3a6c5-6af9-4edf-9e64-8c826e4b05fd
e1b8b03b-5da2-44cf-87a6-946fcb62ab27
12207c33-b45d-4b20-85ee-7d384511b0c5
92cc3c6f-5e7b-446e-adde-cc7081747f30
411626df-ca4e-4117-a1f5-ed8fe2de3d33
47035ed7-909c-4f6b-8350-92c9c533e0df
3b23ce87-9ed1-41a8-81d9-725c6b9c8fee
3206245e-2a6b-4512-aa07-73a514cfe4cc
5ecb98df-b15a-469a-841a-f82afb8663a5
121f6cb5-ff4e-413d-a2e4-d4b693552d6f
27965359-780a-44c3-99de-468a533b7b06
4cc37e92-e147-44f6-9e79-66988934252e
b8482825-63b0-46fe-8426-4332a608d691
281a8cc0-dd4f-4372-9bd3-27d0ec900db2
6d56c636-a5d8-4dad-b2e5-a82719d43e40
1193d5ad-8ced-4952-94b2-64cf86c19e0a
11bcb344-cf59-46c3-aed3-6eb9720ded83
83f47484-17bb-4f0a-9a6a-35f0c9387ee7
d09f92f7-dc8e-4049-a0c5-d62e694f7e8e
a6429783-b782-430c-8089-342456df4091
527defcd-caca-4cd4-af52-80d7be04618b
96a04d3d-7eaa-46e3-96f2-7c538a7ba6fd
e87e5843-5d5c-43d8-9b3e-9987aa16b6ea
84b94990-7685-4bd5-a50b-54e34bedeee1
404ccad6-366f-4c73-bed9-682b30ed54be
b08adfb7-03ea-4cd8-91e8-0d0dad4f26d9
a23daa39-9546-4ee8-ba00-7be4fb570df2
e850dc9e-47aa-49d3-b69c-809c6702f8eb
578159bf-cae9-488b-847a-66d89acc1e7d
b4b1504b-d780-457b-bbc8-2c28216e5f8e
9f199ade-0684-4071-a7c4-c1d37ca78329
b9aa9917-c663-4ba8-8805-809a9ea518ae
8cdaceb3-953e-4ec7-a1e1-5197abc81a9c
c9b63682-b8dd-4bdf-841d-a6f8716881ed
8330fb27-a16c-42b8-817b-d6e01ccf1fdc
9228dea7-94a8-429c-b7b1-7646fddb257a
b6d1e3b9-79d6-4481-879b-b4923019b2b6
a4e342db-ba37-4fa4-9919-cdcc6e9fc03c
9887e0ee-fd02-4700-9904-cb80cc65042d
1706e5cb-cef3-482d-b404-7c41a46a37bc
f8062f9d-61aa-43fa-af14-3a5135c841dc
1d5a520a-5f7a-42ca-8999-a4e7ac488544
8eb5cf8f-c2c1-409d-b262-c2ada1123012
1959841f-8651-4547-8d2a-39a54c10bfac
bffccdbd-7558-41d3-acb2-a2c386218981
38b46297-82c1-4e47-a8dd-3f717037323e
daa53822-ff72-480b-b337-aced8d8e29b6
301a6196-abb7-4b1c-9cd8-256cc76967a8
e7ff401e-61dc-4b76-bdb5-6d4d07e0ea0b
421090e9-e1e1-4b92-ac0f-186c57f728cc
7df1c198-4a2d-4b98-a3d5-48ace2ea2e7f
26385360-2588-4013-a7dc-a754e852e25b
d85f6dcb-60f0-4d21-80c0-a9ed752df6f9
eeff6da5-d985-4f30-be2b-d9d292716526
182b6669-dd38-43f3-b680-414e306e6d8b
0995d1f7-82a9-483e-8151-e9ad7b89aa00
95d218a1-4333-4682-a096-ae7e6ff1a364
3e92e66e-dc60-46bf-8369-d369bbb63491
e2451a38-03f4-49c6-ba11-6d52f88b67ba
97d4ac67-92be-4f04-b3c0-fe4c7a3b7cad
c4cc7949-77a6-4555-aeae-8b02bff800da
be08c9a8-cbba-443c-8c04-1acdc2a84804
4d17d683-2dd8-4ec2-83b5-4db30423d79f
78f081b5-6606-4420-bdc8-a5232f35444b
9a171eb2-c5de-475e-beaf-77d03f1699ca
d97aaf3e-aa18-4a2d-8273-c68ccce3a769
459c4192-0ef8-43e0-bef4-05fab5ab7d01
af654dbf-0260-479f-b020-2c8e9252aa39
90303df7-0d96-40ea-80ae-fc7154a683de
87af460b-68b0-404e-b23a-9702b0eaf0fd
3f4958a3-b346-4198-a59e-4385d8d5e58c
8d41d0af-c946-47ab-88ab-002a73935ee3
c754bc7c-c8e9-40fd-b719-d721a8a4cc3e
b5f1137d-7e8b-4f51-9309-c351f2a5efe6
ba645a0f-6847-44b6-b929-52954c9c7798
2dbc0cda-f0e8-4f76-955d-0f9c457e7b76
92ece8c9-99f5-4020-90a2-b05008fb925b
f6fd2cfd-8720-4a72-b127-0cdfbd67ea0d
16b5bf16-60a3-49f6-8eb4-6eea28b98bc6
89d46ccf-d1d9-4ed6-abfa-5b57992428fe
713ae9c0-adf7-4f8b-9c63-05e2d857486b
b734cc2d-fcde-4197-92e6-e3f45e9c6ee3
33337f6a-7100-4836-bdd9-fb0b7009e2d0
6a32ce2b-e6ec-401b-b902-d3bc0701ff4a
d9f35711-1f51-419e-95b8-fa43ce40310b
ae0d4426-3c3b-4edf-b876-7d1c6c8e2f6d
84ed10b5-821b-4342-bd39-4acb9d93c712
0c928676-4fa6-4a9e-bdf0-1b1e37bf0100
7d6d76a0-1d66-422f-8d13-51bedd0ac15b
b9d09a5b-5e86-4721-96fe-52b1f6fb78bb
11dfbf1b-21a7-4f02-a9c7-80a74aeed977
856ffd5c-0e38-4a65-9424-e0e0e08a333e
f93fe2bb-3216-49ef-9776-44fa1f466a39
746eb726-6dd2-405f-8000-303e4623bc62
dc7945aa-2510-44e9-ad2c-8acb43c1013a
b9ceea27-77f1-48bf-813d-155670379123
e6543396-0781-4c7a-b6db-9f5fef166863
e06663d6-84c0-459f-b39b-291327d50233
a7bd311c-af56-49fa-aa9e-cf08d8ca756b
712fb553-9d04-42d9-be4a-0e6b6e750a60
0f845caa-8498-42e2-b2d5-8acc4510bf58
c74fb601-19a4-45cc-a2fd-3bfca683c5ec
5f70ea52-74a8-4d3d-b10b-80319ca15eb4
1f1a0737-ac6c-4eb8-822e-055b5ec6b988
b0ae32e6-7983-4f34-b48e-ae3fa81178e9
42a5b08e-5bec-4430-a8da-c22c0751fc7a
74c504d3-c6cb-41c1-9358-db76089cb9aa
e7a4da5d-d063-4048-85b1-b3af50bf6a84
cec08ade-ce1d-458b-b6a0-496e0c99f27b
539201c3-2d6e-4d4e-8190-4adc7c59944e
b85669d0-1a68-45dd-ba84-a36c75029505
84281e1e-330c-4461-8e92-800851760854
b44745dd-7ccd-42e1-a1d6-c89884fcc36c
6aab95fd-727e-4f11-bcea-aca1b88d85ed
3fdbaa05-7e6a-493e-aad8-1e37658c233f
391e71ab-7725-462b-97de-bff19be1f9bc
8959c374-e75c-479d-a019-f97305e85751
a13dae6c-f50b-4b5f-8e9e-c228fadc8f7b
0bf3de48-5df7-426c-a27f-a0230105d7de
dc7f2724-5f7c-4da9-9b31-4e025daf0f7f
0312f950-3429-43ae-8497-175decfbf9b0
ba16cb08-6f91-49de-9cd3-c8097fb3c235
1a4dd719-8ae7-47c7-b126-dba95a8634be
2ed383b2-7a08-48d3-ac06-d371743d1a87
d5c2622c-d362-4947-ae61-bb1dd3f4ee57
d12a92cf-3a74-4329-80d0-b57935779992
3cca3fd1-4642-481d-be85-24cddd95a958
909d5101-f20a-46bf-954e-c9249df5873a
5fe1c810-cb67-4879-b4b4-5e2e6226d9e5
dae92d7f-7c84-4fc4-9b57-508f302462fa
2161febe-e459-4508-adab-9ed5b16e5d9b
28e452e6-2d9f-4169-80f0-cda504508478
974b9064-625b-46ca-ba52-619ea1e79833
de1b7aee-a5af-460b-93cb-740cb1aecc04
918b79e7-2781-4d57-bc8a-f6a194b17994
54c9ffb8-6954-4237-9218-939feb33836c
1334b734-21ff-4fa2-b0a4-374a166921ad
3841648f-def7-4c23-bd58-890890121643
4a7f1606-99ca-4d2b-989a-35f76782d868
5e9f6ee2-e69a-413b-90bf-98410e2bf322
e2f3663a-4e72-44ef-a7f5-dd37f7362a69
11bb9946-0c76-4548-972c-d939ffea67f4
1d9ac400-7c8b-4255-8dcc-c72343e14fd9
e1160879-f4df-4690-be6c-115b0c48d2e6
066fceec-9806-43c8-bbb6-8189fa07b56d
e4b5596b-fd9b-4a6c-bb29-b6aa45b0089c
fcd029aa-f68b-42ae-ad24-5658a4aa3868
70c584ab-2d2e-4dc3-9948-cc9ad78e32ef
4daec5db-3bbb-41e7-9cfb-6e2b500573c7
681381ef-49e6-47de-a87d-277c9d17441b
368e4064-d97e-4ab4-8a07-5f742b0d24f5
1513e583-4afe-4473-80c4-2b34a2a1b494
dfa12ac6-3dbc-44c6-a282-4beacca9a30c
8b49d76d-cf6f-4e45-a08e-18df6306d85a
e4e1b2a9-935a-4523-8151-66e7a0ee7ceb
1dd3cb0d-8b14-4cf6-99ad-9e1961389106
ed6a802c-33e2-4741-8bee-c00771718524
957318a2-8dd7-4df8-ac0a-362fc4fb5bbc
2c0819b4-b696-464e-8dcf-167e7e1f5377
bd725302-8cbb-4dd0-aab2-3f13eadd986d
c4b49aef-6a0a-40aa-8df3-ad57db3f342e
151a2b3e-952e-45ce-ba24-c4af5bffa824
c71b47d0-1d80-460f-9e5d-d68c8439e7aa
a9da6b1c-7e33-4c8a-83cb-bcb17a688e21
f4ecd373-29e4-4281-9784-ca361148893e
c25a1d36-7b5e-44d8-a8c7-27a3ac185f29
0b7b2062-611a-4d54-b503-87f291ef132a
170dbe95-5836-4232-9064-751ac8d5bbe6
cf0b6cf6-7d94-4011-a0e9-7eb1119411c0
117c7f9d-39ef-4821-9633-0cbcf4e1477a
6cbb1df0-7373-40ae-8ecd-473dd957b413
08e3932d-de59-48d5-bdbc-d5c5dad9f5b2
c3f34943-e600-4c62-b0ac-9cb8d0b6da0a
d2937630-16f2-4876-adea-c2141bb43dd0
7159cbe9-e1cc-4d8a-9345-93991cbffc9c
6b79e2d5-43b2-4830-9a63-a3046e2bb44c
f459f906-e6ec-4d1c-b7ba-f534a2ccc85c
ed35ef67-a272-44bd-9eca-9519c8da2f6d
4b05393c-0bbb-4ac3-9de7-73b98e937921
63c9d107-fe3e-4177-b984-794ddcb508b2
cfcea824-a5f2-4d11-8da2-0ca3fb0879ab
29e3b3ce-a608-4e3f-9367-d5de3b9ae39a
8e4bfd6b-c70f-40b5-a56c-6e848bafbb93
65fcfc4d-fbfd-4119-970b-4c0cdde87f7c
942ec422-f19b-4ee0-9eb9-898ff5949279
0d5a99d3-14ac-4f7d-9b1f-36eb90dcb71a
7f46d92f-aa32-4f1b-8269-20245ebcbc8f
24b259b0-5073-45b9-9c59-cd499329ec3f
ed700b3a-cc27-49c5-93bc-138f6b69bc37
ba8a66af-10ab-48c0-8c99-5c7517de4b40
6ecc7e48-662f-4956-aef6-3b97ad1293ae
4a5d0187-59a9-4391-9e59-a4713708112e
b1eb97fe-61c0-4b30-adfa-ce97a68dad8c
002b5604-7143-4484-a232-d3262f3a787f
c1151970-7b9b-442f-9dc1-d34c9c0e2ef6
f3651773-35a6-4cfa-b5ca-3b0b97356ca6
36c1bbd3-2e90-4527-96ba-7998e8ca47be
7c2a954e-1977-49fd-833e-bb4772f5605c
c83a072b-b0db-40ef-ae4c-a8a940d4209d
d51fec58-2b91-46a2-9017-316b6c7d0aa6
6497db32-8a7f-4841-80c2-7394d7949184
0e6664db-0380-46e8-9056-607db4351e80
1b0ada06-ea35-41d6-a1ec-ef289aad6067
9e0def2f-e09c-4a95-8b49-3ea6c3768b06
1cdd92e4-2487-455f-83e4-663d9be5119c
6e1e0663-def9-47d3-9065-82cad05d035e
f4746063-882c-4ac1-97d0-a5434e06d251
d12ae4f9-c9cc-4012-a7d1-ee9fc554f62a
7876f1b7-b80d-4733-8a31-96d8a28861fa
1a51d777-63de-4e89-9b78-034917aa85c6
e112f0a9-ca6c-488b-8f04-e3dbbe5d820d
4b2ff564-05bc-4cdf-a324-1d4bd649696d
7bedac1d-9bb7-40e0-86f2-618eca836459
606a4427-91bf-4c84-8034-309a6a4c6544
e378011a-bad1-4c32-bcb3-4ca6503c36e3
bbde37ed-3a10-4211-9bb1-66ea327d8274
26159e6a-41b8-4db2-a20d-b74f7d6a19cf
e35d94a0-b399-485f-8252-62195253048e
f65e888f-c863-4370-9997-3bceac8aa525
8726947b-a4b7-4c04-a0ec-12b498aca265
f85dbc17-60fc-44a4-a247-3683dd8f0692
c97557d7-318b-49ce-91b0-e2ad45896fa6
98d64f9e-b3de-431c-acfe-f5b13c64c15d
311c5847-23f7-45c8-8896-b726a3b3488a
a88ab097-1b90-4fe6-a227-8ed0dc8a2801
5d99f9ed-0437-4d20-9c3d-c268810d5287
b55b9baa-5fd8-4ddd-ae0e-c96fe8f08aba
f8147676-ab10-41a7-b7a4-d776a9c60b6d
32fcc19f-551c-40bc-bc59-0b5268842301
13228119-c9b5-45f1-822d-d395457eeff7
1b92cd65-d8a4-4bff-88d3-35f6ebf48987
199fef75-5f6b-4e95-9dc9-089619ebe001
44023dfc-3b3b-4f02-8aba-02c80c97db08
0f3c5f60-3003-4141-aaf6-cba9769e1ab1
369cb978-0652-4a25-905b-de0c7ca21100
255635e9-1239-49f0-8d6e-dcfe52635df4
75cff09e-e1b8-4fb7-8e32-8dacf2ec04ba
c054ca9f-4c14-4904-8d3f-0af347fdeeb4
8d855ade-1621-4098-8276-e5e52a7df276
6ab11083-efbd-4627-8361-5c16151bc223
1ab2c117-7eab-4451-9b94-b3453168dd99
59665a6a-7f50-4e09-a550-ab280fe9f550
aabb7cf8-843f-4077-90ef-84c8c341676a
f4ac2343-2305-478f-96f6-2dd6b2b1eda4
b3dd02fc-8fb2-4c6d-a8a6-6882e84e3f75
009f2c8c-33bc-4c71-9ce4-6908452896fe
a3461a95-147b-435b-b73c-32490a1f53ca
158ced8b-0bd1-4e35-bcd9-c6d647f53575
bf22af7d-01c3-4226-b586-3a30acc81c6a
724a1ae3-10af-49b0-b51b-b9a3e237dfe9
0ad83477-e5be-4473-a062-df0713a14058
84606526-742a-427e-ac08-29c63767f683
340d6390-3795-4870-b2ba-cf4704335190
917c7da8-fd36-487e-9321-f8956c6cd954
92272498-8ccf-416d-81a0-f3725e8baa6f
c2be4303-2a61-421c-808e-d8bf13ef9bd5
9bd73781-6912-45b1-aa94-95fb4dab50ef
3831bc20-e1a7-485b-a333-514798afa2cb
1474aa37-a3ba-4fc1-b4a5-37931f85cb15
ac93a37a-9fc7-4399-92ba-856c1471a0bf
f24285c1-3638-491c-b717-103554a3c9e3
61d8c7e8-2868-41a6-97dd-4045fc1ecd75
81ee8474-07ac-4215-bb07-2143a4ff3bc3
8d9fdbd3-a242-41c9-99f4-8193d22c4d24
77ef61c3-fde1-4dac-aa82-9673f5d31f7f
8e88e326-2d05-472f-8daf-dc395f352791
0517d2d1-a395-4631-a8b2-aac564820a73
a453bbd3-0209-4677-afcf-19c148c5e45e
c362845c-7072-4888-b72a-d56a66334714
67876fae-76fd-4bfa-b443-0a6d0ca78480
a90dc545-3a52-4399-8c36-74a56c3bf79a
ba6e526d-968b-4385-aa93-39b0947776e8
543c5644-ed33-4dfb-b333-b9126578f584
c77e8f70-f876-418f-8419-6dc40f38ceac
08850ff1-6165-406d-80ce-37c6682b5d06
1919552d-41f4-4b54-ab7d-74ab6381abe2
6b81aeed-b4d0-420d-99d8-14b156d6e6a0
495b878d-64e4-4617-9248-a6d5ab03a5fd
933121da-1fd6-49c9-8ff7-8eda2d8aaa8d
2045caa3-7190-4552-aac0-ca1c34e082a4
330a17f0-d639-4dd1-8496-6127299ba0b5
5dc1cd63-61ec-44d4-a185-0b2b0dacd83e
b6f696e3-ddf5-4d89-86bc-32e4dad6f017
c39b4fee-144a-44cd-97ad-07772e9ca89e
d24e47cd-9821-41dd-a61a-06b57a36bd38
05aa59db-f53c-4d95-8d12-10326752de87
358d8606-75ae-4319-873b-58cef459d6f0
875345a8-3aa7-4ac3-a806-0518eaa09000
d0c0a130-7ff2-4247-9165-6bc31b99f7ff
c0934ac9-b605-4b5e-93c2-89cb2dd32413
e6f49991-0dcb-4067-a38c-e948338d1207
8234373a-1064-47b5-9167-5bd96f3ac1ec
d4104759-6474-4d4f-a78b-243fcc780199
519e890f-d4b7-467d-b77a-747ed55826eb
06b249a1-3243-4be3-9197-d6918f6622d1
005442a9-230d-4915-b658-af27319f62b3
8514e660-2e03-44d8-83d1-7caa01f16f57
bf9f74f7-b74c-4361-ad94-7c9328bf904c
3985e053-6176-435f-b82d-beffcb4395a0
80ab39be-d28e-4caa-b1ff-72995f7da356
d7e7d89c-0142-404d-93d6-6de342b86356
3cf8bef4-2336-4fba-9172-c48b7f60e2ea
25360359-2ed2-47f4-9f8d-e80f6c1498a3
939bd321-7ea7-4d21-949c-24e43a40113a
a7870609-59db-4f7f-aab3-014e80cb2e9d
3ae03557-cb80-4112-9e5e-1a65afccb493
2be03c60-5195-4dfc-8b02-19d207ff6989
2563896b-63f5-49d1-b111-9497933b08f3
ad5b9588-01a0-440b-a5d8-b26198a1fe4a
5c9e0dcb-f802-45d0-8530-63f39f90e112
032996f2-9665-460a-9acb-e36187ce1f7e
2d777d22-adce-4ac7-af74-ae2365faa7e5
bb0e4991-b693-4bdf-b1a2-6b54eceaf5d7
e253267c-e05c-4412-a204-38e5e797266b
9ee449e1-ea12-47a8-820c-471275c8fb13
66fd1991-8c38-414c-8919-d34adcdd30bb
103a0725-7a1e-4a63-980b-c0589d4da0df
aaff6c17-2bfa-4072-b825-2d9c07170b5f
b3c29956-e261-4e87-91c7-cad1dceff073
8c929c06-f771-474d-9059-0e7362a04633
037d1b50-63b9-4314-8e30-89c9cbe87f25
a0732d4b-75ce-4c1b-bdce-2b441ede588f
4b77e6e9-78c3-4f44-a878-8b8dfeff2d43
82ca7dc1-f2cd-4ea9-b4f4-e2ef083db3b5
b3922423-6770-4815-8544-820b3fefcccf
0edb4085-0e9b-46e3-85b8-2aea7f50f71c
2f64d3c4-fc41-4ba4-a672-d21b3a24ea11
939dba63-9021-4edb-bafd-6972d0a0e00f
eaa1ff2a-6d43-42d5-b538-02810b30d9a6
bce23668-3f06-4049-8446-976bc0a07180
5a2fb41b-9452-4fc5-9a74-55bf7da46e65
8f1e0f78-3bb9-4025-97cd-73dfe5d75814
3b299392-d5c7-44d0-80b8-404afac6f6d9
e08e3c60-b9e4-474f-a930-808b1c381ef8
9d730ec0-7d39-4a7e-8d7f-fe94ea59fdf5
186d7771-6097-4cc7-b970-514f2ad5c1d7
c83ffd6a-df04-4c6f-a0d8-84fb60d2754c
15677904-ea3f-4321-8389-5ec7b82c4dc1
55eccd28-268c-410d-a531-d567824d90fb
51f28b0b-1a44-4e10-b194-bb5f0d309a69
65f97348-1b57-4f76-ba0f-2eac64f72c8a
afe49d6b-310a-4c26-9c52-052a7ca9dbf6
97ca2797-400d-40d7-9b51-baf85b5a9ea0
32119b31-0622-4386-8bec-f1a5f7648d1d
20cb2cdf-aeaf-4784-a0ec-e72d30775136
7379af26-1220-4728-9067-786b82755a40
38f8f01d-1f2a-4577-a09c-760274ffaf69
05194708-861d-445f-8cc7-aa6ba5b6fcb0
aa717e2f-fa01-406e-9028-3cbf36742c2c
29fe6438-165a-4676-be51-7d76f1b3cce4
4bb4d350-7330-4556-82fc-57d953a842aa
1e397300-f931-4b98-aa1d-12493549d8f1
2a8ccddb-854d-47da-bb5f-06eb9058ebc5
9cb89b17-53d6-4f3e-aa61-104626e066d9
7f968dbb-fef3-4072-9d87-59f15a4978c4
1ed4e250-23af-4593-a0e7-4c13f0957eb7
b71df9f1-4db8-4f5e-b1ae-02f3b8877fcc
788876ad-e57f-4bd8-a404-ff4a397bbb80
d8097713-36b8-4569-8145-0b77741781b8
935cd059-fd75-4f19-8e87-cd4a3c1733cb
dcd6501f-bd04-458f-b249-d9d3aad5a5e1
9c291fa4-f716-4229-8918-8cc9334aba95
85c2bc0e-eabf-42c4-9514-5b68506223f0
b142bf25-11fa-4e38-928a-f3e2d628b2eb
26fd15aa-dac6-492b-ac73-dea712e50de7
3b574012-442f-4894-abb1-56e90b4c58b9
93c15084-d029-4d85-a224-700b0dab1dd3
700d03f5-c4fc-4723-b6a5-bba654869c13
fb8157ff-8278-4504-ae2d-6435652865ab
75951206-1469-492d-a012-185700598412
9a8de978-3b47-4cba-a009-9269a2d97050
443be389-d918-46c1-8f00-35bbd521751d
42186333-05e2-4ec6-9ce0-b44722fcf4bd
83eddbc4-a42f-4c8a-bdda-ff483ca08b64
ce2a4386-b3aa-4c26-bc32-8e47ad52b976
d083630f-d9c2-4742-b674-4840f730a54b
671449d0-9afc-4323-a132-3cf5ae423567
ae724605-966d-483f-8b58-75a83e634e5b
22a52a2f-beec-4e04-a311-a0fee933b854
627fa6a6-cae0-41b3-9b4e-960db9048ce5
81e8012d-b934-47f0-a4ec-7c92bb80a7f1
c9d02e52-96f1-4e99-b16b-c40dcca9c5c8
a996cc5e-ceee-4b99-acf4-19e621d619f9
bdbaf6e0-e4fa-4285-9475-615c49e80c0f
f6b9bdb1-98d4-492f-ae02-cde208f30fa5
052dd4bb-5bf6-4062-aaa9-2920e0106642
419c88dd-9c6e-41c1-bbfb-6b4a56517d19
0b48e45a-b295-4db5-a5e9-4af78ce3e49f
7c7c34c5-bd0f-49da-a8f8-145236ad6472
558cc27f-0465-4592-b2ab-3478a1c6e4e4
c5518125-fe3d-41e1-86a9-cac24fe850f7
a35cabba-6cd3-46e8-b8eb-f0fccea25cc8
8a2ef820-5080-4afc-8133-e8213c2ab03a
af6e5fe4-d4e0-4ba5-9502-997099d22bba
86007451-606c-49ae-8521-df734e4461b3
0ae15065-539e-4e57-9251-f08b15980ff0
3f756259-f07f-41ea-ba0c-171a4bfe3012
dd120ed2-9e40-48ec-bac1-8dde228d0a9c
da4abd70-9cb9-41ca-b655-a0347c4bd675
72cc46b2-42c3-413f-96c0-191427bc48f7
1e44611b-7cb1-4680-8e03-6dd2dca7b525
9c5b3357-c77d-4af8-8c02-9d22843d87e7
382df2e1-2a8c-44f3-b697-7b432e3748cf
de76f0ab-bcd0-405b-9873-e4b17f7b43aa
0211c51f-34d7-48b2-995f-d8e1d438d6e3
b618240f-cafd-4ede-8855-3b6a915c90d5
802d634f-81a6-40f9-bfbb-280be96c4ea8
4a891e30-3eee-4186-9be4-3b12bec1903b
d9a8f9e4-e764-4ae8-94a5-b610e32867b4
f7745cec-76bb-4603-acff-8dcc4002a883
236b8b1b-8afa-4bb6-ba6d-a45a3c5c162a
65ee8460-eaad-4060-925e-0fdb9e66b39c
15efa840-16c4-43e8-9ab5-62e6517babe8
5de691d8-7513-4c80-acad-3b5d0461dfa5
0c4adb87-55a3-4072-9232-6d7d69e04926
777f1667-3b16-4485-babb-492650e69409
5f6bd688-d476-4d39-8893-2604b9572185
cdcb9f49-dd11-4bcf-9e1f-f5eb80f19aac
9791e0fe-4838-4cab-aa78-74243e6b552d
a215c3ec-827e-4e15-93c9-00492a8a9ef2
d1665c23-d8e1-4a35-8cd3-982c0a506918
e14a8a79-4876-440b-adf9-3fdf18e00385
329f731f-e502-423f-89e9-5b75c79a3783
f8fb8037-a209-4e24-996c-04a9122f1634
850f0d48-6292-4f1d-9481-cf762ead060c
a1004a5c-115b-4d7a-b97f-a4500cc2e319
903e8acc-19ef-42eb-a6c7-8be360236a0d
dd4c0dd8-f3fa-4fc5-a793-bcf04c2c1a8f
2ab9c368-f249-4af7-80ea-3f5eda0dfd85
60281978-7ec0-4103-84c2-56e06365cfe4
4b6fc25f-3d40-49cb-bced-0cec9383f138
8be72c8b-a09e-49c7-a891-44e1d73e9b44
695058c7-0d9a-459c-bbe5-12f1213cdcfd
1c7e2bfd-f0bb-47ef-946b-769c98eb52c5
6ae93a1c-99e9-4510-9c63-ba7daf5e9e00
3e54a340-96f3-4b9b-9b4d-65cf5d1fe063
a6ae4527-99b6-474a-966e-4a65cf071527
f3563aa8-c4a5-4fc4-8e7f-aea616fd7726
ecf6a255-839a-44e7-ace8-2c2164901ad7
e586f7e6-c6bb-4f54-ae0d-be926152d8f1
4f23f7ab-5bfd-47ce-8132-c014f0711e46
b1cd13b4-8885-4f68-ba0f-d3d177521f08
ec0bb8fd-b26c-4c45-9804-ad4a264e6441
61aeea18-f040-4314-af32-5c58a04401dd
56d8c7df-5900-4362-9e4a-5d11d4a8b884
3b3b6906-ea39-45c0-b6ad-1b1fcce87201
2afc2e3b-44e9-4ea6-bb62-fdd73954d4b1
e2549530-b706-4ff9-adee-048ec775d8eb
dad0ee17-bff3-4cad-b074-4b06468a46f2
71c7c9b2-5af0-4271-a0d4-fe6f9374186a
41979d09-98e2-4d7e-bb93-c2930f0de774
1e5eaf61-3b26-41e6-ab4c-ace068eb36b1
f219da38-d085-4f3b-abc9-3c5e85c2a882
c32d92fc-f1a3-40a8-86be-1075c5292125
d3c53f48-7b7e-4080-969c-f0a139172aaf
716e9bf8-27e0-4a1a-9660-b52b6f475aea
da7826d1-b44a-4800-9f7c-edc2febe83a0
3f412d1f-6a18-40f7-b235-72c88b41dd9d
99b81acf-d52d-444c-9e0f-d3838d617f45
2f3c0822-892c-4be7-9dd7-17d64ddbca00
7154f7bd-abe9-4176-ab57-32468bb63453
e99d64b5-983b-4ddb-b137-e538e6576d44
3031c665-9c85-4209-9552-1fcb98e78d26
35d32b8f-578e-4250-a275-2995444f0047
ec608442-027a-48cf-b9e6-a216d3886bda
b92815e9-162c-4bc5-a908-59ea74826315
e89e8681-b5ef-40fd-aed4-eec87167b0c4
ab69c0f8-a311-4705-8a74-68af7f4efcdf
a53b1f8f-badf-4932-a197-287d816bdfb8
25c1ccc6-7326-4d2c-80ae-885dcbd110c9
0a6298dd-ca94-45bb-a766-6fefd248a545
e4f01428-626b-4fe8-b81f-4eb8c495def0
d8663b2c-4c3b-434f-b749-95eee71413a8
f4bc13f0-24f0-4067-8fc0-08d9cbbf8a75
45b73a55-f1cb-483a-9ef9-5844a2552e93
52b1557e-0b2a-480f-a3e2-a6dd0c87598c
22cb7e6e-3ddb-4d41-9c78-fcdaf056c26b
3fcf9879-60ca-4888-b8e2-dd23806a1c22
733f4454-307e-4898-9169-42e2a68c3b88
58e1664d-dee7-474e-9a87-9b09b07ce0d4
e0c98b63-5988-469a-8a1d-dd80201b6937
6fe99069-eaf4-4791-afbd-e2fa2b7eb8ec
22cc1ed0-d078-4830-85a7-b5ee5111ea82
da0030f8-4373-40ab-ac72-1a2700e88e41
c06678b4-ffcc-4e7e-bc11-de21c5ff7cf8
0e33f0c2-2d15-452b-b816-bc55dd0c383d
f8edf561-3926-43d6-a8af-3eb2e7b3fc81
542bec64-187c-47b7-af3d-b26f27c42f11
e7c4a55f-d915-4b4b-889a-a679e142ca38
40715cd0-8e9f-434e-8227-946e29b64e47
2965f8d8-f53e-48a5-bbc3-679b73f952d7
5c0579a5-aa45-4a7e-9cd8-4544fbfc4102
6ebfd416-a596-4be4-b378-c396312e01ed
6c4b57f8-b76a-4584-81ec-c83b17718b51
5f5357dd-55fd-43cb-84d3-5c740fc5f418
69c7055b-87a9-4acb-bb28-e61bbfe67b8f
ee8e0afd-7334-4004-89ce-a7d924ec4913
9944de09-dcad-4c03-abca-10760d324e64
e2638c04-7453-4bce-aa53-e190e2a3aba0
8fbab890-c8f9-42ef-9747-f6064dc047a9
066b4618-8f22-452a-bbfd-6a1c70c722a8
7d0c41ec-7155-4da3-b5c2-5d59cfccb7a0
e191e39e-b898-4445-8cae-48501e151cd1
73f06690-1637-4feb-bbeb-f418954118b4
13a97177-5245-4c60-b2fd-37ed01fef3e5
d67522e2-6041-4e95-aa73-643a0ef25eca
b0cafc0b-f1f9-4847-b908-365ad318c87f
e8b95e5d-274c-4a70-8b4e-b0e1c63ce860
ccd4a578-0c2c-48ca-b6aa-b5e5a28cab41
095ab483-3578-4df7-adb5-ee476de45215
295c2cc6-abed-4577-84a6-a5674e812128
6f7c208c-2c36-498a-aca6-093bb3f6af08
b9d76114-2a89-469d-ac86-df1eef201369
1c800bf9-6b60-4031-b492-907a3760b29e
0b064f3b-ded3-46d5-87c4-074ed6d4b26b
ce9bbf16-5d92-47b7-8460-71aa63cc5c0e
278971e2-18e8-4fdb-ad53-011bec41bfd7
f293251f-906c-44a4-aabd-dce93757f542
315ab0cb-4730-4e93-83e9-256e26280c8a
05d3db52-eb72-4ca0-977e-c11f395a812f
98577af7-6ce2-466a-8976-1edda3053ed0
892ef1af-5e99-4ed5-9be0-7c8b457cb1c0
f55c49e3-eb7d-46b7-a7e3-0f1139217e19
e2b22a00-2dc7-409a-978c-667c9b71238e
d6ac7994-81df-4d0d-87eb-3ec1c6a4f84c
76ff3806-19eb-43f8-8e9f-57a1961ba01c
365789d6-e1b7-4b78-ac89-083eb1079c65
8bf71fe3-320a-46d1-a96f-4fb13362c397
c7689fe3-9b5e-4780-ae28-6eec8d62ce35
e5c2fae2-9d56-481d-a143-32f39b6fed85
7d383617-abe1-44f3-bf61-6e44be341f1a
b2e3c44f-f78d-4174-95fd-a6509481e0a5
603b9fac-68b1-4f34-a670-706be4a57d00
5c0723d7-dd40-4f0b-ad6d-4a6d2d8dd4c2
75c23a26-60fe-493f-b1a8-f6d2fa60ebeb
3f2f2fa4-0f86-4a0b-b1f5-09325e39484d
7f13e7f4-26c6-41ae-bd21-2c6f9c3140ca
d4f5c266-2ccd-4f2b-bdfc-4fddc1c2632a
aba21757-6d8c-4dc8-ab34-2e4496ef7c41
eecacbfd-9838-4297-a284-dd00deaa48ef
7bd6649a-de1f-48a2-9675-30436a211d5c
4a3860c2-807b-4838-8e2f-2ae28138e654
f79d5abe-4d39-4400-8f9e-c4121f478428
81957629-c61a-4ea2-83a3-461bff1b179c
88c12d79-92af-4abb-b17c-8b2a0dab3977
861eeee3-28a2-427d-9e3b-9a582d4344d9
e2e1a2cf-d3cb-431b-aa09-d634f0fca528
83e3e58b-b234-4205-bdf6-6ae52564e707
5642b5fa-fbb5-4cb8-93c1-143f9c65875c
f9b8fc42-976a-4d89-aa50-aee0801a74b3
8f700ead-b896-4c64-99ab-ffd3b087c6d1
c2b9024d-91a2-4b50-bac5-0141e5f43703
924a0432-3691-4cc8-8dd8-1c88579d9cd8
3a27c058-6a15-492f-9df1-b485e0487a38
19c5cd27-2d1b-491a-a1de-0b0d5f37cfe9
9ebc8791-dc32-4ed1-8819-761e14bc6bc0
a457c1af-9547-470d-9428-c6088c12ba70
7650b85d-7765-4615-8e26-0da7c809cc90
7854a35b-12a4-4f5e-b1ef-c5e94b63aadf
417bdee4-a764-4e1f-afa0-694c497ffe7c
65118a16-7a9d-4443-b913-0ebfb611e635
f6ecf2df-af4c-4257-a943-e422c1588800
4ec09af3-28db-4c36-980b-0432c0d870a8
ad0bed77-7655-4636-a61f-892e3daf4175
f5a6a7ab-3596-49ec-be93-2ba09397cc7f
0c9410e5-10da-45b4-96a4-d7ae373adba2
b72d8640-49e7-492b-adb4-c5af68c5a624
59aa6c45-0dc0-47ed-8469-c45bb0a6a308
b34c332d-4daa-4b75-9509-698d6e4f0e57
24d92477-cd84-4046-b532-9b4496428112
5c97d9e1-61bd-4d9a-ab04-8382e0d324d0
121eb483-5a10-40f3-968f-8e1da5549868
dc2e905d-402f-4120-9feb-7a2f6edf3ef2
ab1fa7e5-c071-4904-9c81-a1a0dd979adc
884c95c7-83c7-4556-9d58-528bdf3a43d3
a74127af-45e3-45f5-9387-2c90d127af5b
5186965f-b7d0-4be5-86b9-6230f915024f
a7469f75-af0c-4212-b7c7-0fe86e2ace04
63f929aa-6b1f-49f9-afd1-fa94578187c5
c8694e0c-e731-46a2-bac5-de836691d695
2c02771c-0eee-445a-8d6b-cd093efffe33
745c192d-a3f4-461d-9a68-344fe8bcfb05
d2690e98-89a4-4497-b0ee-1fa723797d96
34422a7a-bf8f-4a5d-9510-c732ec052f55
3aa5fd42-285c-482b-98c5-856349b96978
e55f0e09-943a-49b1-8014-c07aba3cec6f
3bdf711e-8ab9-4647-aa41-17b76168bebc
84cb45d8-e983-4ec7-b9ad-3bd39c19252e
79500ef2-802d-48b4-bfa1-b733c2520f58
411fe9a8-e93b-40c1-a8cd-f26edd6743bf
551d7c7a-91a4-4f39-a92f-c16f889e064d
7bdf16f1-9a93-4a6b-b41a-cbc9e3ba8d02
ba25fca1-ac6c-4bd7-949e-5dbf5a4df9f0
db7b5e7e-a526-4d43-bfc3-74cc3c3402ab
94d86585-40ca-423f-b0eb-467c5e70a12d
fd97370a-7d17-4266-a2a3-885b5e4990a2
93f6a533-b971-4705-bf54-f9890689b466
089cc452-c1b1-4018-b08b-05a56faaa44a
8263e969-5a30-4ed9-b5c5-95d21c643b6b
da9a64f6-e7b7-4864-8f9e-84b0d129abe3
f37c3be4-ddfd-4ec0-a8dd-9593e6c3f7a2
e2113f94-3800-4f32-a6f1-26d0b511a11a
341fde5e-452c-4b86-8f50-ae331b749868
368a08fb-ea5b-4b56-8c2c-9cd32ee3c5f5
a670a3fa-deb3-43c8-8a7a-a36b976397c7
24f19f43-9e4a-4457-95cd-e96fdff06ed2
a463ded4-2bea-4ffe-b23a-a7ad4cd8e692
927a16ed-b334-4028-9bb0-9db2a739c8ed
c4071cbb-132b-4e89-9ab4-4a3bea3af438
9fca55b5-d6ed-4095-8e59-3bc0d69ea039
1c172277-38ba-413e-9822-47f69295e3b8
181d4370-a071-4375-83be-144f1eca8813
fd90848d-e260-4cc7-8bd6-db38bf588500
b57ef0b2-9d85-422e-b7e6-cf63cd4b2c71
39f5993f-30ad-4dd7-809e-fd12644a4cf4
144539e5-af81-41bc-8778-526137c56585
699e2fb1-17d1-45f9-aebe-49a3bfaae093
4525f283-360d-4feb-9231-afac6f67e379
40071915-3ca4-4574-b25f-15001842c8c1
aeac197d-34b3-4bbf-b8a2-48f9d569d6f0
66821d43-fd06-4799-aa41-8a4642a563dc
5e58d5aa-e8c2-496e-aab1-6db17d5f4fd9
1ebdfde9-b908-4109-9cbf-a0d1322a1adb
2c9a2e6d-a317-4b46-93d7-4339c5e871ed
a90935ed-1687-4cc8-8397-68622bf95e3f
0e87db96-4b34-45ec-b0e2-c0bc99c47d95
dbc9beb5-58eb-4469-ba2f-5448914a68f5
2fc7236f-943d-4d8e-aa81-73f10e1c5e0d
79551bef-4c73-430d-a5d8-710f64f13b1c
a69e8ab9-7897-4767-80aa-0e2f8cdcf1d2
235104f5-46c1-42b6-bb60-064aa090bcc4
4cd7232b-5729-418c-bd2d-f22439f69dc0
18da7544-0396-4b2a-9769-383e6fdf82f3
e0ceb921-f2f9-41d8-8217-bcd5699fc80b
8d4f3951-cce7-497b-a4d8-ae047340664c
aa9cba4d-635a-4344-8180-401a45b65a2c
89819fc4-4104-4a6d-be13-d3ec2e1b3c77
949c95bd-cbcb-4306-8dcd-04a2eeb44beb
fcf0c35a-3ccb-4530-ba29-887d0545c665
269dd058-1eea-44ab-83eb-3a9ef10d9e34
6d25dea1-7688-4950-b6cc-553933cf7ffc
d178c647-d0e9-4478-a5a4-be5efde6c8d6
4fa294e5-0b19-4293-acfd-a69dac8ab0fa
681d7fb5-1a5d-4bab-b621-3810c32e6573
ff30ca88-10f1-44d1-bbdc-f50a73e13873
08f718bb-05a8-4fe6-9418-7bd012c416aa
54f8db0e-b322-4e0c-81fd-9a4eeccc227d
93d42339-2863-4fab-9f0c-dd555f97fe28
0ea02168-463c-4de4-b71c-803445e3d12e
38fdee7d-af41-46c8-b532-a8b2778de200
5f090b7d-e18d-438d-8e7e-48bfede3b64a
4030f474-5300-48d5-96df-f11fba0b7c89
459c1940-3039-4e0d-b9a1-ccd1a61dea62
3fc55ba9-3708-46b5-a37f-f25a9695e700
99abdc13-7126-4407-b8eb-f87180518ccb
7b66c2c0-ffce-414a-825b-446a02515d7e
c156cf91-7aa1-4ed2-a8e9-2d9539b6fae2
49860e91-9314-4072-a977-80ad88a67be7
c16230a8-5a96-4856-80f1-9c39ff230f20
ac12c98d-9e87-460e-a371-694782b20a6c
99241e62-801e-4464-a6cb-ae5d2f8a4f46
bb07c304-fac1-433e-9c75-e2b1b81b3010
262d3ab4-61de-4add-80bc-6c11334f8020
1176a7a6-6c74-449a-8302-d62df841e9b3
52f92bdf-43a8-4ba3-99ea-bb3d980157d2
468f274c-4c61-4fb2-8a83-3fd53d65d2e5
50b83364-bbdc-415b-82b4-06d918526e45
8dff6c05-5576-468a-8e1b-619cb9842647
5e1d8acc-a842-49fe-a4ab-0ec8927a77e8
62e37d25-0f1b-4df2-902b-612edcdfd664
68be9ab0-503b-43c6-bc9c-e5eb262c3f44
1c13c754-e761-41fd-92ec-2ca0f44ca29b
cbaffdd7-0fec-4f5b-8327-ab7e6089c63b
362307ed-6f5e-416d-a002-c71447147d8a
1255bf8f-061d-4dce-bc13-401a0fe0e43e
f5d37a40-1290-4618-a9e6-c70296465fa8
3fbe571d-9fc8-4fa5-ba1d-ca7371113bd8
e340ebca-6751-4bb4-b911-f55a0df7062b
85694ce2-cb7c-4618-bb28-982ba75f52fb
71ce350c-b3e2-4366-8ce2-447612da3e02
af87026e-b590-458d-af48-c100e3431fef
0a74fdf3-e9b7-4329-8f4d-76f2ea932de3
a9f4dc8c-46b7-4738-b187-0ec92d98d648
9edad0a9-d2b0-4814-94aa-58924b51cde7
0b4c0dff-fe31-4dce-b0ae-a6dbd7142cd8
542891fa-9d5f-4429-9ea9-fff8834379a2
ba9a51f0-2f2a-4be4-a6e8-c54e8d437ff1
6ee3f8ef-aa5b-4f17-83b6-7e3deb84760f
7b4bdd85-20d8-4eb4-8418-3184a78f8771
ef528598-15fe-4590-984c-b1c1827d5f7e
4df05fa7-8d4b-4763-b6bf-073df0d07eb4
cb83dced-cc40-491f-853c-5d3bd9b235df
6fd09cb7-c7ea-43f2-93a0-2650c79a695d
1fc81713-ebb5-4c94-9181-a7f9e4e9bc32
741cca05-7856-48f6-9cd7-6ffde6f6df9b
c8a0d816-aea3-43be-a6e8-e2cb738e71b3
56d51e80-085b-4f04-96ba-ec1a824edc6c
ca735f95-1d43-43a7-b8f8-c98dad0c366a
4e4fbe7f-8e87-4b38-afe0-19b8a20c773c
22fc5b47-9ea5-4d50-8c8c-ced0a678d7a0
13ecaf44-6cce-4982-bf8c-1fd8db7cf905
6a75ae85-7a8d-4294-9e7f-60231db4ab45
eba3efac-36b7-4035-8c16-c6a16bc3faf3
ae587c26-9db6-4ed0-b967-493961f46ec0
2a7a9c32-13e8-41b8-a611-0639716eb7d5
fb4bb2aa-38bd-4788-b561-4f949021c0d5
97e8c824-e852-4d21-8df7-649bcba68267
ae54f5c1-60bf-495e-b02b-78536712521b
c827a37d-397c-4b19-8f4c-37dd2b3a69ba
739b485b-aeeb-45f5-bcf7-9c9455b3af0f
bec875ce-f3a1-4dc3-98c6-ac3f8ffa1751
dda24902-d18a-4ef7-83f4-72f11c482e2c
c58ba9d7-2dfa-47d9-b76a-5901144e707c
fc8a29eb-2a8a-462e-aed3-ccd9f713f9a3
1f88e56c-6e1b-41d0-ae4a-8e15befa9ff4
d1eefb36-2344-4ac9-bf9a-54daacf44735
14591017-57ce-423c-9f22-ce0ff4ab7ac0
afbe7edc-62e7-42d7-9f65-7cad34a3c313
9b6ede92-37a2-40d3-8d51-a54dcbb22431
16b7dfbf-256e-41f5-b536-5e0d27a74e8c
8c828a50-a540-44b6-ba6d-b2bd24bbd778
412dc83c-0443-4a0f-9d70-d41eae578e0e
94d5cded-6bc1-4be1-b4c4-9abfb6f87e8f
5cd53dd2-b275-4828-b4ab-6bb58bde94c0
fb8d055d-9572-48ab-baf3-697048eb7e15
650d4e3d-10e0-40e1-894d-d3598907477a
8391f938-81aa-43d3-a1e1-6a0a03996589
9815b25f-0c5d-4edf-a470-9ccff6491292
4a079ddd-26fd-48b2-954b-dbc772172783
a135c4f4-ba18-4f48-96c2-ba119ec01564
bd48a87e-f765-4555-b547-84043e31d449
3ee194bc-1768-4e10-b9f7-9940f2b0bb7a
2b88eb62-53dd-4b46-ad29-1a9601d9c35f
8b5f8101-cd82-4c4d-a54c-1357cc264437
e67b3221-a8cc-4089-81cb-7669b1c056d4
63638eb8-7e49-4e5b-a425-579e0d81769b
3d4b55da-d830-4725-8ed5-ba5930e38c4c
0077eda7-7382-465e-9829-0090f30f0eb9
57572912-93ec-4d2a-832c-c2cec634626f
c0b0d17a-f4d5-4a9d-b550-cfb5b5ed0b5c
95902c34-7805-456d-8f03-1ab9686a61ae
2b8b94ca-2d01-4eb5-a4ff-1d2ead8cff9b
a10d2163-abda-4893-9589-10e4d4f71c2e
d2c99002-9ddd-4301-bc07-adfc8297912a
ada81542-7ee3-493c-bcc0-5c839a40caa3
b187950d-4a4d-4479-8203-16147286cbb3
eb1c30f7-24da-4ad7-b040-0789f9dc213b
eaa3315d-27a2-476c-8d33-a779d864386d
a7ec1ca6-13a8-49a6-9ed8-e5b7e32c3c9c
01a1528f-d23b-4022-86c8-4242de1e0fd9
c82794a2-a8d5-4978-9a88-0009ecdf3190
819edb2f-ae81-4152-acd3-97f593d0c985
99e58e3e-9368-48f2-ab49-d30b5eb500a0
1e1440c1-7c6f-40d1-86c7-a8750f95c161
d6aa3850-e0d8-4a5f-800a-49f57f574f06
2bc66103-dc3e-465d-b869-cfd289cdefdd
7d68b523-292a-491b-b2e6-e7fc6869f3a3
56f6401d-cf0d-47eb-b261-dc979aa2d34d
9ce71965-2642-4efa-89db-84a99c448702
1b8f1c09-4a43-439f-b36d-75bc31b0a48b
7b36c79f-2d99-4fef-bd45-421d53a873c0
b91acfa3-6a97-48c2-9281-ace29e06078e
59a74481-33cf-415d-bcc1-11e846181d8b
9b71c78d-f1b4-4b77-95bc-70c49f826632
7da99418-c483-4d46-81ec-1309c40abd8d
2d91d88f-12d3-4db7-afe6-6fd4058ea185
d4415607-8fcb-48b6-b2d5-7453f29610f6
77aec155-47ad-4a31-9a27-c0075f401c85
d29bc0e9-185f-44a2-9d61-71e1c005b0c4
12d0fb52-48d3-4a26-8fc1-107b44953343
afbb3d99-457e-40f2-b392-e3a979f20f25
03962959-c813-4434-aabe-746b25a8e773
c55c571c-2a68-4839-b22f-5d67e1d35dee
551a675a-4f66-4ebf-90ad-300b55c3f0a4
ad802e62-326f-40ee-8ccf-b495e9d54d97
3c5bd9bd-b5e3-427a-9fd2-b3330a653d6a
6b4ee44a-aa44-4e4d-bdea-123c393441c1
cccdf614-2a0f-4947-af36-828572cc7dbf
03cad29a-6878-49f5-8d75-6981a0058780
d2a91184-819b-4e86-a18e-b8b178089c07
36ba68e6-f091-44c1-8cd8-9b3dfa32de41
d1621056-d9aa-4c66-83ff-da11404d7262
e65743e9-8923-4a41-b823-c5d858d4da8a
23510930-8f1c-43c1-9178-db47d39bc74e
0b73b6d6-6119-464d-bbea-7241930c0b98
9c196009-7086-48a8-9ded-dd6a0d3764b4
45459855-ae76-4dcd-ad43-1050d6c3dda9
d4a2a63c-d010-4345-8233-a0078786972f
31ebcb44-d238-4af0-a478-165cc9fa546e
66525472-839f-47fb-ba2c-9726d6de8160
dadab95c-4e86-4e76-ba65-1e5b5d5157a1
11c0640a-cde6-4f74-b2ac-811bd3715f89
2a279300-84ed-461c-9bc9-234242fe02a5
1df03c1e-26e4-4f7f-8659-d3c88d3e32de
e048aff3-fc21-42ca-b104-643c19c9f1a4
9325727f-fb4f-409b-9b05-ca83a72d480b
69b73863-764f-44fd-b8a0-22631168a7e4
9f22017f-eeee-494d-bb4c-3e740d4b9f95
a3912f3b-7327-4701-b2c5-5031f4b4387b
b79e1fb7-cefb-4cb2-a23e-a66bf88edcd7
5f5a432c-4caa-4a25-9b19-5195f72de950
026b8ad7-206d-4c7c-ab3e-5bdca8def9f9
57753808-decd-4da1-b91c-860633555fa0
f1c0b051-233d-4f6f-9033-87b2e40e6a13
eea65c90-e0c2-4574-8972-708b52eab4a7
c0f4008b-f904-43dd-8db9-9610a355a165
eb8579fc-6942-4951-884d-22a0638d3c83
47b0fce4-5f58-4a3a-8f70-0cd44103038d
803e3c7e-b16f-444f-bc28-4659cd1a56cc
7f7508f7-891a-4d0e-8b86-0e0b4b1cf747
523f4b46-7abb-411e-bfea-9329442554fe
feac9961-f4f0-4ba5-9353-786415d42f84
d7f70501-2544-4a90-9572-88d02810640d
9886a9df-e0b1-4382-976a-ebb86da93471
42e3c13d-6468-4c10-b0bd-6070307f82d9
5d9f4888-7e06-440a-84c5-d1b9dc4fa3a3
fc3f2141-c21a-4cbf-9ba2-31b81cdb2ff2
5b75dfd9-61b3-42ae-9707-655275d13f6a
465de1d9-c012-461a-82a4-389fdaa3834b
a6a24a6f-fb94-45dc-9031-3ac57f9a015c
6b15d555-f8db-4889-bae1-f336b0a968ca
4e2156e7-cabe-4c1b-b413-0779586129a3
80b1e3b4-58c1-4f4d-96f6-1108c8749593
703d7606-21ed-466a-9628-cb1abcf3441f
55a591c6-bd7e-44cb-952b-b7820961b34c
416c8ab9-a05e-4667-9009-cc8286160d01
e024177f-26fe-483e-a97c-02d854524b5e
85529207-c210-4975-adb6-84ddfc30a9d7
867ff657-f459-42f0-ac8b-b9fefaf6c570
1d26df46-56b7-4d97-b18f-adee21a2a449
3d6b8008-ec18-461d-a7f6-b657c36ada01
af9c6aed-62ab-4e68-888f-c0b0e0be3af1
6a491ee7-60ea-4df8-96bc-eb45e9d827f9
418fdef6-75df-4894-a6e4-779e94e22ba2
60396609-6f37-4f0a-880c-48515465cdbd
2c9f75ed-08e4-45e7-8023-753a4083f772
79c0eeee-204f-4a40-ac2a-d8cfb939fd37
1e196b24-e062-4032-bcb2-9de90dafaf75
47eeace2-df08-40a3-acef-b2098f41e846
7b05f3c0-2a67-42ad-a8bb-5d63b39f53e7
bff0065e-b2fb-4797-82d8-615445d455c8
16d3f099-ce55-4444-a077-824a0c2b4f76
1df5634c-5b78-44e4-b5bf-b027765fc457
d4b31f29-dbe7-485c-8bca-60f6ceaffd17
8478b307-b94e-4fb3-b998-a1dbde8c0da4
bf2e4eae-309f-45ee-9992-3c7105ed317f
24c7ab0b-2e52-4b94-9e25-ce68511759dc
4cc43d11-f255-4f97-93a5-91b640eb175f
fab2e694-cd7b-4542-887a-f5fb91cdd774
541acaca-cc95-4278-b82e-a068953c8fe4
a4b23aa2-97cf-414b-a452-8d66fa67da99
0ab50d21-5840-4723-9a8c-baf3d16c5272
4bd09109-874b-497a-8ccc-6f904396c62c
4f84d0a8-1679-45b8-925b-4da2b2bc9ac4
0d957c0b-08eb-457a-ac36-14b19df13779
adfd739e-d54a-49aa-9941-f5146cb79fdc

d5d00a38-7597-4693-938e-137b7f87a296
7cae0911-53ad-4514-af84-bee19d4a8d69
9fb0e437-3c24-4bdb-88c1-ddd825b07bb2
4106ec06-0271-49ca-b64d-f88992d8676f
e14207fa-1b61-41e4-890e-a485f3f60f67
8c552b18-7e74-4207-ba93-38043e297066
1f58d05f-35b5-40e7-af90-6510195d0e3c
88b266f3-98eb-438e-8826-f71254b1eb1b
c89edfc1-091f-4635-8ec4-39f29e8f97d0
fc0031b4-b9ee-4f2f-b6a7-da75b15338d6
c71398f1-20c1-4765-8998-7a58e274312b
c5731759-ae78-4771-ac35-da70c890b45b
5a0051fb-4bf7-47ef-992b-75c3b7091a14
0736514e-90d8-480e-a806-e6152e3b38de
2347d006-5f88-4cb8-9bb0-13b4e830e618
e4eb3a86-1090-497a-ae3f-38627cb8b9aa
17d71900-5ff7-4d31-837b-2388ccec2bce
e93dc24d-4775-4f60-a228-c8d70980d756
c952100c-d43e-40b2-84eb-ea26c8c2e2bc
bb6b2620-3e31-42fc-86dd-fac0351b57e7
29343697-6312-4988-839c-808c2ba73044
6157a6e0-697e-4fe2-8021-9cd40ace6954
50be0a93-1154-4e1d-8aa8-fb0673bfee37
0dabdeb8-829b-42dc-a12b-fb88a7b41b28
dd1c6021-6ae4-4616-b5d9-6791512f5c0c
a5bf1e14-cb08-45fc-934a-494167b88549
d01123f4-d42b-4d5e-94a7-29023dd83d53
f9945801-d066-41ba-b511-d62478dd4019
e35ef23a-4bb7-4c98-83dc-ccee463306f8
0911369c-8d3a-42d0-a69f-3389351856d8
78c99f6d-1685-4812-9faf-d21de1716e42
6211b3a8-b8c2-48a7-b8f4-440b7f1f2f64
aebd4ca3-a729-4396-b219-ae50d9918886
8126eae6-7cda-4455-b2b0-23171ceb89f9
ba2fd5f6-da99-40bc-ad7a-2b16a2b373dc
d0515ff1-d43c-4054-82e4-f27a0a0467ff
48dd58a1-bf47-4ccc-8261-26cc0d510f80
354ed2e5-a396-48e7-801c-7eeffb243fed
571a318b-4afe-4348-a7bb-0ee8b7f2f279
7a4c0bad-345d-4c1d-b172-5c9132c53b69
5babc01b-1f8a-459b-ae27-4eeba48dbb71
b3137383-6399-4b7e-89d0-06a9555139e5
458dd5a9-784b-4a26-bfeb-17861e53aa84
a0547ab5-083b-4ce1-9358-4b57a98b7c9c
1b17378b-0903-4817-b751-53c9522eb061
4a4ec34a-54da-499e-9bb2-f73452c0a662
ba1b03ee-d99a-4cd2-8cce-9a1741244ae6
d7520210-b540-4063-9081-17247c9b2441
2f312ad8-1e95-4150-a168-78879867ab0a
48bd9500-1bca-4f3f-b4c6-941375cce149
2215d2cc-5471-4bb0-9542-36d0dc32f466
bf02d471-daac-4205-aa74-185df47d4c9b
1bc9df5e-4232-4ada-997d-c4050902e26e
f3fdd864-d5cb-471e-9985-a7942d367053
a9b29bad-e97f-4aab-b5fa-a45476079a23
3d4c153a-b98a-4eca-8d25-b2acf10d4486
4cc6fc92-fa75-416b-9938-e9ef014d1963
574c0f64-3894-4ec3-b767-e388b8fab252
595a3fec-fa97-48aa-931f-b053c959dbe1
68131dd0-5eeb-465f-9d3a-c9d99ac283f6
37b1c9d4-d312-43b3-b2f5-550feae431e7
bf684895-8bad-4067-862b-97b59fa8339d
8d1b7e28-e1cb-40ee-846c-2a842680fff6
e96917f8-c9e3-42cd-bab3-6dc712b4b428
d9ec2339-dcce-4bba-b313-a150cb9f5c8c
99ecb7e4-5a99-4678-b21f-cae8d7437652
45bc75be-4d25-4b1c-ab7e-495cf94fe749
43974ea5-c80c-4fc9-acd1-0ef8900f3b68
984850cb-0a55-4020-bfd2-5dcc0a81c18d
fd08c59e-246c-4bd2-a69a-6f4cd4744f37
aad4117b-980e-4001-aa10-22ba600097df
7f75dab7-ce0a-4242-b6d0-4cb78f783356
62a59018-9a9f-4864-b464-980037513a11
0f9f8d6f-ba3a-4700-aa3f-74c55cd7cb58
c8eec960-a0c9-4edd-b536-5fcf9c50b91b
42991111-3393-4b6b-8fd9-c5833fc7dbab
c148b2bc-ed90-4fc3-8309-d2c2ae991757
f1a9f170-ff28-419a-abe7-5b648a99e0b7
e87cbabe-3459-4f86-b3dc-5de1b0dced13
03f40859-eea6-4dc6-8ac3-2e925ed0f552
4b37215f-4977-4037-bec3-11618ad458fa
14324b2e-2d18-44dd-a3d8-e3aeee115b8b
3b26175e-2d9a-47af-a248-54055b558bb1
87a9cdb8-6eb2-441a-b37e-0b9e63c8bc0c
2abb2557-93cd-4536-88ee-2d522c2560aa
49770ce3-e778-4115-8473-05731c7ab8f1
b3ded535-0bc0-4d5e-801b-9a072b5626e4
a8c64008-079c-42c1-82c3-0aee2ca07da9
b5297b95-be55-4aa9-9e3d-5e010a65d687
13c2c2c9-3dfb-4dec-8a73-e6b07246814e
22cc9c4f-fd66-4165-a5c4-977532d8cad3
a3186806-1011-4f3d-bd98-cb7791395eb1
fceddd77-fdbd-4b0b-ab30-7fff15d25c7c
4b9d2a22-f060-426f-89de-f08cb448878e
7d983c25-790f-4433-b83e-c22d440aed34
fe2c31ae-04c1-4ece-9266-edfff7626329
e84c7dd4-59b9-4406-8ae3-c56546ca462c
d34776d5-8b82-4e7d-afa6-4e7669ad7a82
9fdf7190-a6ce-4299-8fb6-338a9f85d6b1
a1ee9458-d8f9-4967-9257-a4f419ab164a
ee4e77c7-dc83-4863-a75c-d3aec50fd021
d7c564ec-580d-4782-b25c-8b627c2e4b3c
d9b81541-380a-443f-b100-abfbea84aa98
d4684b3b-e150-42d3-b87e-feb3ee543acc
7ff20de5-afa4-4e4b-938d-85e580afda6e
86f91e3a-3361-439b-ae2f-9b173052eb70
48092c7e-5cdf-4ab7-9333-21915800918c
d5600c60-c372-42d5-946c-3996f73bceb3
658b2605-f813-4872-974a-96617531ed06
d329a612-980d-4928-afb8-fbeaed0915ae
7a45a75c-7a1f-4356-a728-b77b38250083
c645b2ee-05ae-4ece-9a38-0d029080af65
a4fdebc8-826e-4464-8b91-67e5dd04856f
88041a73-8e52-4bfa-96db-d54ed451f9f6
a7cd291e-058f-4a6a-bc4b-afb16032e1dc
6eb6cd35-4420-485a-8357-a25ffc356c2a
f8e485a5-d590-4462-918b-e50fc7b32493
42f92ea2-1290-47f7-91d4-0fdf58a67933
a9f8801d-e474-4ed5-88c9-785294f3dfd9
70d116ba-2530-40bb-9565-1b8f4f5f94b7
d3b0be76-e881-4a87-a135-79f75b098c6b
77a6de3d-ee93-445e-9583-a6bbc7f22c0d
7dfa7c58-25ee-47e6-99d8-1b435dc06349
475cb22e-048a-4820-b70d-3a69551957a0
cd33565f-be8d-42ef-97f5-7719d53b407b
78777e93-dcd0-435f-9ddc-6d5b35c9f272
5fa8d8c3-79d1-4f31-8587-79c81f1eaaf3
7fc88da3-9057-4b8c-b870-a7ac926838af
92b614ea-0eb5-4497-9c47-5612e3b923e3
4c86a840-44c3-4cf1-9317-88165aadb000
f236606a-a8f5-443c-8315-7db30f19ed62
a3ea1572-8f8f-47e2-929c-8d0b1281bd5c
d37c652f-a322-4604-8a59-18d2d7a7e54d
874fd7a7-27f9-48b0-92af-a334979e0248
8800c646-31b8-4267-8758-b10334aa156f
408a61e4-7946-4a81-a3eb-a8706feb6fc8
637340b3-2084-443c-8b50-8704feff3bbe
f0d174a4-8dbc-4675-b41e-9532452e9bea
e00c0808-1ab8-44a4-a27d-d7203f66fd19
e4805d3e-9af6-4d72-b0df-9ca3c2586042
b9c3ce90-a434-426c-b21e-48454bf1123c
236a0f3e-232e-453b-bd09-3dedacedffdc
0e08c8ee-013a-4f07-a22a-f59d5089a20c
27e2abc0-4698-498d-adcd-a5a2c32e95a0
19c73dda-401e-4f49-a629-47be21c19f2d
2ac72849-787c-41d2-bd63-9fba6fada1c5
1183daec-5aa8-40ac-add7-0c84024636a6
939a5518-88ae-4fa6-b348-53139ddfdfc0
283a4722-1329-448e-a1df-f09dd574a112
e7d0543f-90de-456b-a76f-2a8eaf857502
798b07d0-f281-4032-8b91-bc50a164ea4c
9c417bbf-7c47-41a0-bc30-294aa93ad177
f2cfec13-cc02-4beb-8230-051d3d4251e2
96283730-598e-427e-98f1-bf6b3a06a1f6
3fa3629c-48b0-47f3-9a28-692a2d21e83c
3627aee0-fc42-42ee-887a-470a56babf27
ed459184-08d2-4a9a-9cf8-6e9d305c0609
c081ff45-1156-421b-9ee0-4830f1e808b1
1cc9fe76-b599-4eec-8975-4c26d12cf1b9
1597d16c-cb6c-430e-ae66-fa6c3c92e288
ec184e1a-ff31-4dcd-b775-2b8aa34c9553
f0cf3f75-1d23-4fec-bf37-8bca6c7ef45a
d6b4173b-c450-4dad-ac8e-8ec681b0d94e
54227748-413f-47e1-af60-a8c6ce98da03
b887791e-6ac0-4ff6-ae4e-2707b128ecb8
cca7ea48-c305-40f5-b044-d194821c5bad
470c7ab5-4041-4957-bc8f-f519aa8481e8
703d4ccc-e605-4ceb-aff7-ae996ef79a9c
c9db310c-d7b6-4a83-a03c-c225e4cc4d9c
29b67948-6f5e-46e5-99a1-100d84d49ad4
88f873d7-75be-4cc1-9019-af1155e55c37
e969a02f-d102-4678-b37d-d842bd8bf7fb
30929b28-cdc6-49cf-8a1e-15c4ff44d8b0
cc5b619c-0607-4c37-8ded-709d8af906e3
90b80df0-38eb-41dc-8876-cc2f3b2479e5
c17b7df1-12c2-44d7-94f2-0edaf7fd01ff
6a022349-4d8c-4819-8cc6-5f41a40ab801
f868255c-79f7-4792-8721-ae9e6bd35ea1
85953afc-8163-425b-889e-70ff8a83c414
9cf36d53-7316-4781-a4f5-bf073a817dff
73b0b63f-7ad0-4236-918c-6fe7032b8dcd
a18e182a-134d-46d5-821f-8bef123aac18
be701740-b224-43a5-ade4-c502a1a1843e
819ba603-c520-4a03-b28f-ecfbb5d85b90
d63a608e-814f-4430-9d17-1c16f1bd1707
c520c3d2-134a-4c59-a794-0fd6810efd62
9a5dc958-5d5b-4b18-a01f-967f502a86cc
6e8081d2-da7d-4ed9-9a9b-70c2cfc94cba
4624ab6d-10af-4397-81c6-57bc4c123cc5
20ef8a00-a8c9-439f-9032-996780f36e73
611d2298-fbb5-4677-9d41-d709efc31122
f2492675-9137-4367-b645-db0cd01ada11
f8bfd2e0-aab4-4cdf-b923-54e68027273e
ca6cfed5-2ca4-4e1d-9003-d2043d817719
e3e4586b-2018-4510-b111-a9e05acda586
6655fd7a-9e32-429d-9b52-6f9651eb889e
4e597b11-123a-4ef6-910e-56e07ca600f1
4129f919-d7dc-4ac0-aa6a-7b5b05aede20
6239080b-568d-47e1-996b-7df661f8e7ae
c9f192ab-65e1-484a-a107-80ce236d5201
59db595f-d85b-49e0-b9dc-0e23b4a5ff2f
7de94930-2c25-40b3-8a0f-0017b5daff32
bdd3902c-a71b-4099-8a8c-10e709fa345b
e193f05f-d8a3-4733-835a-9aec90c330b5
34f81529-a57b-4533-9282-89af04fe27bb
0937548d-84ff-43cf-b55a-a3d760e6c4ce
fa7c344a-4da4-482c-ac34-09eeb09853cc
b770963d-4044-4f0d-8ed5-b5704ecf951e
b6777cba-50b1-46a3-94a3-823a93ffe48c
2e69748e-ba0c-4c08-8b08-7c1956c3f63f
d20eb86f-0748-497b-8038-d11372b26afc
6a0ea426-f530-4a7e-a0f5-c19b6ea7a0d8
6520d38f-4ff7-4c97-acce-3cbd1658b0ed
a036bf5f-a2f4-4ac2-93e5-689761baba6d
759bf9b7-cf00-40b3-aa0d-19edd1179d8a
5299e45a-af62-4fc0-af79-93f892454f52
e4d57033-0406-42ae-8192-c394a4c2b6b7
338a0986-16bb-495c-a8af-225bfef2de59
5a1cd83b-d842-4b8c-ad2e-14223919981e
89638246-60f1-4b4f-951e-d84a7ebaa7cf
2661bf26-58a2-410d-9714-acad034a7492
0742d0f7-f8d7-4195-b3cc-b0ba4465c459
01b8d979-4efa-472f-8709-2963cb89d274
d26e9891-6e95-42a3-9eea-2a7aca758990
82ce20e7-5783-475b-a0ea-8b29616a110f
19a4b87e-9614-4fc9-9721-dc17a18f1a15
619229ed-f670-4ee1-9efd-fd69efdd4ba5
b21c3db1-2232-421a-a84e-7ac8bc8a82e7
1fe9c065-8cf9-4e15-b171-324ec33f9e7e
4e77dec7-56f0-47ae-9abb-fc5ce511acf1
1e2e1214-08b1-4284-8ac7-7f74dc436acc
724a2ecd-600f-42d1-9dab-5f1ff911db7a
286f47f7-c94d-439b-b60f-e3ae9b095f53
43152522-959b-4854-9e2c-c42823ef4bfe
e68dad71-d622-4044-8944-e4f52d852880
ff076956-6bd0-4330-bf10-6c360f8fa798
c0981bc0-5156-42b5-a787-d5395316832b
910cf897-cf83-4c99-aa60-0094f2e0bced
aeaa381e-b685-472a-a4f9-6a8a011dd282
4299b72d-d127-46de-ba88-dbb713db8e28
45146c77-e0e9-43ce-9206-0d5a1f604847
360f0dcd-0a33-4f49-911c-1a36d38394ca
9ed8b860-8fbc-4652-b659-a34a6db39080
62db72a2-1a6a-4f5c-8995-693f46f9c5fb
d83f228f-77d5-4d45-9239-a413a72a9c2f
e9cbb51f-7f2b-4a06-beb4-cfc6dbf70fb5
5477834b-fc78-4d26-aa46-01b14366a4e1
d3caef46-4994-4035-adc3-d51ea8da2e76
ebcd131e-5e8f-48d4-af77-65c65493767c
4255db20-d981-4a62-8afc-daf6c9b2a283
d0d990cd-a381-40a6-ac9a-bf3f0c26ff86
b3e9221e-b3c8-4547-9686-8471bee90683
b135b6f1-d911-402b-ac22-c2bca42fb480
c4e3171e-0a46-4f1c-a18b-8f4429619776
ddc25bc5-d6ff-446a-b2b0-998da1cdf246
933a29bf-4e5a-4c01-b99c-6d1c8b7f4883
a96e0d11-0317-4596-9244-2e9ae2c5e1f4
4233c45c-3ed5-47aa-a449-d3c1dfc10216
e0bb1c82-d9e2-4a92-9ead-942ac4809f91
e6b6f341-92d9-4ca2-b9a4-db2accdfa461
ef54566a-a0e4-447d-b809-c465fbe64ae6
a03afb28-2285-4181-9a93-894580d6187d
d656b2bc-bffd-48ff-9fa5-226e795b9863
49d8a0b5-ec4a-48c9-93d1-99176724ac98
dbac0dd8-9f7d-44f0-82db-d5d70c78469f
96df9617-a021-484b-b621-0d8bf0564ee6
40b939fa-1dd5-44dc-bae0-68f3f4c5626c
e2de99a9-c7bc-4822-92eb-20d2c7a79ce8
8269b476-29ca-439c-a5e2-9a559b595c2b
358d8b1a-4d6c-4761-a585-c7fa5ab0c304
3f5ad557-c05a-46aa-8137-87d9ae7e129f
afb4d85d-6b97-4d96-ad8e-8f3c494e65e2
e5359218-e0a9-41c1-b1e9-a72aafd7a048
253021a7-4eda-405c-bb17-fffeeffdd2d2
dcd8b8de-37b8-42d9-a5f4-351505b1d927
b0f246c7-d4ac-4dad-9afa-354c3855b27c
061e71f4-af5b-446b-b6c0-7f5465ba17ba
a67df4ac-5d0d-4974-b0ca-174defc995ed
ca7c7714-84ee-47ae-a611-e675293cc32e
3e268775-c156-42cf-bd5f-16ba6d41c1b7
9f2d727a-ec3e-4528-bf31-de37fbaa80bd
54f68186-7fe9-4160-a186-2c5437d899de
bfad9b96-998e-4826-9253-bf1fecfbaa85
d046e234-2629-4dfa-ae48-664f789045a6
47019994-fc07-4306-baa0-13d3d130c4b9
574b1497-b537-4224-a8b7-3ae412a65ce9
b3d471d5-0a4d-44d6-8750-cac6cbb65872
16494554-8d13-4e11-8fe7-7743b6df9e02
0b56c961-db57-47a7-806e-4093a861c772
d713a1e7-c2ec-4493-8586-ff72dc0fb879
c9a6ac9f-7ba1-40f3-a6bd-43a0a2ae8514
b320d256-ee10-40ca-aac8-d5957db335b0
24b479a1-0e60-45cb-bba9-b77d92d47dda
0bc08487-19fb-4cfd-96b2-6bab0c9ed7c2
5af480a1-84b0-47af-bd0b-b945418269e5
a3b8eb80-0301-4e06-9df3-0caa507fc757
3a437774-06c6-4460-9c04-74e6246601a5
cd90580e-9300-46ac-a062-197d90fe0c22
2946fe24-4ca8-4acb-9494-8b798de782a0
7193898c-c7bb-451b-a85b-9ee7bf835b44
562ed0ac-1d60-4317-9986-5ab9fae84bf0
f05fa391-837e-4b9d-ae91-ca9683e6f8e5
65402759-6249-4d0b-bc6f-452e62e8cbae
f2d5b2fa-19ae-4aba-8676-e98c4dadea53
11b608ae-fdd5-42d7-b381-9cd2cacb4554
973956f4-308a-4f88-9a11-dd5bd0cc06b5
442c6a67-ad17-4369-bbdc-212a23089d72
84c96ddf-ef0e-42ee-994f-6f2498854854
d56d92ab-1dab-4c2b-b57d-6017aa64df4d
856fe9eb-f2cb-4aeb-9664-26ec5a0707f9
21a4379d-a258-431d-b212-07f26f0c9354
4686aa86-eb58-4ac6-b3f3-882a7c87d243
6593d907-8e7d-4127-b4ae-69a39cd7bddf
7171083e-464c-4adb-aadd-381af0c35bb7
820d70bf-42fe-4fd3-8f1e-c3d93de2ae0e
23ebc718-7b63-46f0-a393-fe4b0871451a
9df5b4cf-2182-46be-ba93-a97c6fdc00b7
73ed418c-0796-4984-ac82-b99193a39766
1a813bf0-fda5-4a00-9001-8944df75db5e
10e134c3-1c67-4d7d-a116-343cced93d2b
d340b195-40b9-40b9-be02-c1df5a1fe775
2b31e1db-6988-4372-92ff-1620289aff9c
3d8a4cab-f463-4c6d-9cd9-1cd8cae21131
ddcab515-eb2d-4d93-9e8f-d5a514eb951f
e3141710-1b35-48df-84f8-eea646519959
97e157a1-d7fa-434d-93cc-f1de8653e7b3
528aee14-30cf-45ab-97eb-241e4e1f2468
30062c19-fd27-42b8-9de9-3ee9456fca1a
25fb5a6d-c06b-4394-8425-75a4d9f44d59
ed3d9f6d-faeb-4d01-8287-ffc90a0a6e05
e2bce663-de47-44d1-acff-7e2276879628
48ba92a1-ea54-4ec1-b09b-756eb6ffe23c
62da10c1-af0e-4c40-b99a-b8fdbc1f494d
4af63251-3a3c-4a88-98e7-5ce1e19993e5
b12f17c0-54b1-4bfb-8f9f-a374f09dcd44
7a139024-ef44-4a7f-a220-44e52e028440
42bbdedd-5fb6-4687-9da0-dbf51dbad88f
4dfaf494-f65b-4123-af01-af309519eb04
0174324b-5c71-40d2-9068-a497f9eddc48
d3bf46f1-9fc4-4d2f-824f-aef52e98ca0b
e0f81dde-d7d9-4e74-8789-7b6899c609b4
e750d1b6-71ca-4265-88c6-6c9cd6beef9f
1beb3fa7-f4ba-435c-94cd-d1a8967e56d4
a54ee108-5b25-4b99-8c43-fe63d3b77abf
732650c0-abed-43b5-8aa3-daa18b1fa4a4
19fcee64-6ae9-4456-9cca-21ce5536acd0
91f695f7-bc58-4b0c-8df1-470b39c610d6
ea605eea-9153-483f-b534-ea547d0616d0
10fe7cc1-4c86-4192-83e8-59239fa8ee31
0b1de43f-7c24-46f1-87c3-cfc12486a0ec
f809a136-0fa8-4239-ac86-80ae891e583d
89d78ca1-fdc9-4970-adb7-98557861e64c
50a659a0-6c61-48e2-a387-47997b04d340
9ae50cad-f43b-4dca-bc8e-24e7888efede
fc19fe83-f634-4ac8-85cf-66267b8b062d
28b0aa1d-a29e-44dc-b597-99819eea64a6
6ca210ae-e765-4ca6-9858-b13ec1443b2a
e15ca175-31ee-4a81-95de-f0d5f1cf8430
e8d38c80-00b9-4fa7-b45f-01fc3b5b64ca
0dc9f4b4-2682-4199-a8ec-6ebc43bca524
f39ccc5b-db1d-4585-bab6-22d1d604c8e9
45dcb5b0-0ad1-41fc-b628-1d0fada7855f
fe2be15a-61ad-4187-9233-2b2f90b3d818
0aceae4a-02f9-4403-9b01-e385ebf2ad9a
77cf80a1-a450-48e8-b818-0fc3535c983f
86ef9610-4aed-4954-822b-f627b31d427d
979abf45-9594-455c-a553-df8f01222338
92a4215d-add4-42a9-990b-6db0887a655d
54cbad67-ae7a-418b-92a9-70303c068ee0
14cdd884-2e0a-41f0-b56e-57d4aeee430c
ffa759c0-0e5e-45b7-91e8-710cb1d7c515
d01599d3-4026-4d4c-bcdb-d7b63a18c73e
ac234b02-41ef-492f-894b-eb726701d05b
3fa8cc51-9354-46d4-88dd-839508ff9cf7
f9a78712-5854-4113-9460-965cd542ed78
b1e2b127-3710-41dc-b033-d4cfd1083af3
3f02702a-aa29-4659-8a1d-2136dc59f0e1
9c77e73d-b37a-4a6b-b2df-6b2b596d8e3b
dd981b2f-ee20-4c5a-aeda-4b7dc2069049
5b93ad74-14c3-458d-9cb0-2e4eaec337c5
60122551-72ef-4c10-bf80-15608b208666
a4ca4dd1-782c-45f1-8349-2df966ac026b
25d9b03b-97c6-4341-9437-1781c5b80e21
12327f88-b358-4819-8241-654803a4565d
fd204953-c8b6-403c-9b88-08668aa61880
e3065033-752b-4094-88a8-04e22797b47b
8b9aca62-3196-419a-b460-cf47e2a7570e
378b3fac-e25d-4ddc-949b-7ba9687476f5
624475bc-a0be-4041-8180-2459b0f1ed4f
1ea79a1f-64f7-4054-a1ad-c5c772eb0fa9
0e2e1540-2b6e-4361-a519-62d0f21c9997
037351ed-212b-40da-b761-b6e18f958483
72eee288-d163-411b-b8f1-47b3a24764f5
cf4d25b8-48f8-48cb-a322-42d4dd3fd468
dc489d4c-5116-4e00-9628-5cd8b5f754a3
589d4d2d-3bcf-4d81-8777-34e564fae835
09a2b768-af28-4775-8e1b-f44e59a09091
d35f3f95-7a6d-4240-9ecc-9b83c70a238e
27f6b563-24cd-4e75-8fa9-41c69899200e
5df3c0d4-fa97-4ab6-ae6b-75b4a595a020
5f291ca1-69ac-4d1b-bfc8-ebf4e91895d2
c5631a98-d4c8-4a6d-add1-b108f0a5bdc4
52ebd597-13f9-4b0d-b3b2-a8a7dad89a4b
5459eb40-653f-4fe8-9804-5bbc8321a78d
668ca87f-cf4f-4692-a768-ca3dadb10d9d
2eb73323-268c-4876-bd40-e026f285f946
61f7079f-e790-404f-8d0d-09c96ff5c792
67499fd7-59ca-4ed2-ac80-db66e4257be6
4aa7ba87-3d4e-4a72-b8bb-e2b4340d4a30
01e5f694-f7f6-4fee-80a5-971760ba6835
49f01fb9-3f75-4002-a512-0a8670bbc5f5
e3d605e8-f6fa-40cd-81e9-52b91ea08d0d
a77052a6-5d71-4a6e-ab0e-1a823a0ae91f
5efd1ef1-9ef7-4315-86b6-604c57537324
e704db93-cdba-4617-b38e-b3ba5a447ddb
00f67342-d4ee-48b1-b17a-a69ca34b1270
75043085-be24-4c95-acfd-d80708c9e966
76211eec-7672-4ed4-b9be-7daa7aa8fd5a
90c08de1-139c-45ef-a756-53bca8614aec
804c8daf-c926-40c1-b91e-2e797d9712e8
478503b4-fe0b-4957-b567-1c8b14c99ce3
8302cd33-49d7-4a56-adb2-d053a1593005
307fc563-23d9-4ed5-a9b6-1b34eac3a174
fa6cf3fa-016e-4dff-b576-6de4b68e8a6a
746d85e3-a7e0-4595-b7f0-b66ce74508af
b819bf92-960a-4058-b4a0-3627f1de1fd3
02dc2649-9ca7-4dc5-9c46-351bf58c7f68
0b2137c1-aa8a-4b2f-bf70-f11cb6899b5e
5064e8a6-1bc3-4360-b64e-a6cf217a7933
aab5592e-98a5-4772-8081-2f17ef3ed0c7
aa698fe0-39fc-4c45-9e05-3570c9e4a60b
4a00aed4-1135-4c98-90ee-8441e22cb0ad
60d5b489-e67e-4873-a8ae-e115258504a2
3ca69f6c-dd3c-4575-9b4f-66eab7779512
316e8768-b9e4-4369-8ef3-e8bed9163239
13984ab3-d2d4-4e7f-b1d9-3659ff68f7e0
2a903100-71e3-4e2b-b5f9-b7f3ec42928f
3956675e-8ab9-4a8d-9ec6-06e1c4230084
be4d076c-58a9-4f3b-8888-c7fca1e4c278
e683c492-e02e-426a-87a9-98296bc71b5c
706332f2-b0c8-446d-bf73-196e9e21a212
e779fb96-85ef-4723-9a63-1f8f029b2ff6
a4b283be-d03d-467e-8ff3-53a5a4429ec3
87aae479-9dc5-4e12-9b96-ea86442f97b6
d51c6c2c-a5e2-411b-8bf7-cc677a55077a
f6c25b2f-c9f8-4c63-ab8c-2ff308483eaf
47138c76-e200-4943-9380-4d45c51c4a24
51b339d6-2ba0-4a54-abb0-b984aaf442a4
34560dde-da3d-4d16-88a6-927a139d5dd4
08c6b7ec-a051-43c3-8da0-187f9f386e31
b79475d3-790f-42d7-a76b-fcb937abe2a8
621c8531-0c60-4941-9492-e62ee5f24e27
c5f0a782-1865-4ff7-b28b-f55db7f839df
23b9887e-0175-4691-aaa0-386e268276de
add253bb-463a-4bd3-a6c3-773c484c95e0
0a29c029-5b62-4f95-9803-b8a40cc974f2
0a798d92-0191-4e26-9a39-2f3244069dde
317e635e-e5d6-4c3d-95e7-2a68928c1a1e
fb072684-57af-40e0-aaf5-2b04f8e9461e
e4a33987-0020-49d8-8628-6e1bdf5f3b71
ec8131a8-981d-47b9-a20e-25baf671396f
6e4d5243-9166-4610-aad3-5cefb78ecd62
9ed40702-614e-4f3b-93e0-87664dd62002
6da65122-6d20-4eba-8587-4a336c4436b3
aef300b9-7b8d-4ceb-a6e7-89952108091d
1c22daf8-fe1e-4675-9cec-d639895f65ca
75966e7f-af35-4b2e-83d4-88f8e4eee6da
6078e65b-a5ef-4dd9-bb5b-bf177affe39c
043f0d92-27f9-458c-8eee-47625103449b
fd19427a-4164-4522-b25e-030a4fc6e768
7b45ccab-fa98-47f2-87f2-9b76de3778ee
e3ebf2bb-3356-4dd6-8005-b4bf7165cf85
b4d178c3-07fd-44c0-8b44-adfa0caae496
c05ea608-59c7-4503-aabe-ffc8b95674e7
53d78124-ffb8-45f9-8ec2-8401e62dfe87
c2a43163-bfe7-4445-bb67-9f987b105132
c7742eb5-f0d9-44a1-ad2a-0f4d25ed2702
c793ca40-a365-44b6-be0a-7dda94672a74
b5b34aa7-9167-40f4-9d41-c34b01adeddb
ac623b04-7101-4a16-83d3-f6e148a6fad6
443fe5da-26fd-4c54-ac14-b5b7936d4106
60cd5ee2-91be-412b-baff-2d230c893ad1
cb995171-5034-48da-bf49-3f42028753b1
dd4f57b6-b51f-4a7c-bc05-8faf0cac444c
4e021fc9-7679-4a2d-9e25-e72977c7efb7
4b8df4e7-b8f3-403a-8cd7-07e83fff48fe
9e92ef58-7ad7-47ae-9c10-010f303bd553
0c7504dd-bfb3-4999-9ad1-9e453d43d2d4
cafe37cf-b3c5-446d-9c8a-09bf583f4881
1d91a246-2a67-4907-9e92-f82e32252def
623eae2b-393e-4ae0-bb41-c1339d7810ec
a3df52d7-5d85-4022-a04c-5e09962dc53d
0d7dfa62-615c-4cfe-ba19-6e4c67e8b05c
69d94cbf-3cb1-48b8-b840-c4c9aed7a238
07a94903-7c0f-4741-94fc-c7cf953b3a5f
9cb17ea8-e371-409a-b420-10176b01f656
55e09466-5b8a-40b2-8635-b3ddea36a01a
71245e0c-2e3d-4cac-a557-a5753247466b
f3cd73a3-31fc-4af6-a51c-b7efd2f34f03
02c921fa-3b27-45da-b317-6de51bec3ed0
1b3c0673-67a0-4579-8f28-aa237d660881
fca5b689-0466-4fb3-ad6e-3524fcc85abe
f89a2bb4-7b4e-4b2d-a2f3-5ac4e06fb3ea
74e6a50c-fe39-42ef-8dfb-e68513e165d2
cc936892-68c2-417a-83a8-2b31bcd9669f
e1dfb3a8-e3c2-49f4-992f-1f6a472724c0
82c2362a-0cd9-48a5-b8be-7b39031a259f
a6198a02-0bf6-4673-bbd5-1e27c054ad91
deeebc29-39a7-4bb2-af8f-ab6dfb2e1515
0cf0db38-f560-4611-a768-9001340f5d02
c538b174-e76c-459d-b676-4d69e37bdb13
281ad171-cc57-4e55-bf30-24d7c87a5670
5708f3b0-bc2d-4de2-94b1-8b5bcc9b3049
891c1590-e205-403d-97cf-d36845a4aac1
e78fcb57-0723-4e25-a30d-c9f31efe7a04
fb64fd88-c1cf-4467-b5ea-990c3266ed85
17c8bc74-c115-4fbe-90ba-cf20925eec4e
c47a62ed-63eb-4cd3-9bbf-f9692f70d36a
76c330e1-c2b8-408b-be53-67e016bd835e
e59202d8-5836-407d-9997-ece3d0b76634
87a3bcb5-93e5-4bf1-b1bc-de628bebb5e2
b4c22db7-24f0-4387-bfdd-c69d38c42114
191aa1de-0ce9-4201-8ce5-4e15cf9ad3f9
44505896-b955-4a60-a073-785752b80deb
9eddcab7-e546-47fa-be54-9d29a29d3e6c
260a26c2-a088-4ea9-ba42-39cb64658c46
db0d7104-1b63-411a-abee-e229b7fe17a7
086c1882-458d-4c6b-9951-25cf6b6f0caa
b7ee769b-be6b-45c4-a4c7-e477731ac014
acb6ee1f-4264-4bef-aca3-fdb56c7007d0
b538564e-c3d4-44c9-8a17-57b5f4f4296f
5dc158c8-5f1a-43d3-948c-e4de1d2d4faf
40a65d7a-cd27-44f6-a122-65edc68b00e8
c0aa6a9b-858f-4158-988b-09061e4340c7
54c5508d-522e-4c69-93ea-7e720c07c9d3
ce6ef822-d8d1-40a7-8fe7-20d3008850b2
a1173836-48fe-4f10-9e6e-63c5fb176848
ec4397bc-1e5f-48bd-80a8-e7c5b8d8218d
c50b5cd7-9b4e-4823-b721-2cd01a58d29f
d981b846-db48-4b26-88d2-0a1b804beec1
18070190-bf50-4f3b-953b-7c2a714bdd7e
0c1e3f23-f9c1-4da1-a026-79dd37a466bb
75b73450-9b6d-4403-a637-562493b55e8b
231d340e-9163-4c30-8870-321b162e5de0
c34d3bb0-53fe-4b76-b2ef-6cc06c378226
098dd088-eebd-4596-bef9-5ce8bf44cb86
27af1c30-e1b4-4bca-a6dc-7f8394886467
91df0b19-0c07-45a3-883e-33a7a64ae500
8d9473cd-fe5c-4400-9620-1bcac63bb085
281bca43-0564-404f-b498-922c07525c2b
85b79ae9-6797-405b-b5fa-4ed247cce844
28ce03cd-ef38-41cb-848b-fc9fc7109bf8
93ecc33f-19c9-464c-805d-e36935840bb2
d8d15a22-5715-4365-87de-89b0605aa808
33ad4b72-997f-41cd-8ac8-0556f85a2030
ae9314e5-0954-485a-9460-9c422d0b8d8f
128db107-0ffc-404b-917e-cd234847a7ab
6839f58c-46b8-4662-9341-7c2952582f04
0e3e9905-bc3a-43a4-b4a4-81bb9e9ca2ff
2cc638e4-819e-4935-970c-8734fa6a4e19
5e2f322d-c372-4c14-81a9-593916dcba48
878f24d8-d7d9-4820-83a6-5c5046654f24
de4493e6-fd95-4a34-9c52-ff93c8d1e936
61d89354-18d6-4a63-948b-8d444c9dcbe4
42aac262-12bf-4215-a970-4fc6899ff985
cc329157-9bb8-4f6b-802c-ecd28568d39f
db3ce4bd-7318-4b60-b628-dfeb8c6c9d71
3c625798-c232-40fc-93af-e99509450844
6a22a32e-8caa-4bc4-baf5-9f28983aa7f0
daf11e6c-6597-495d-bddb-fa54a51ab5eb
b087caaf-e282-4447-a8a6-dcf364db694f
73e68394-3da6-4985-89ff-1f71dc3e908e
fe5d84fc-542e-4657-b439-39f0de4f41b4
f6f1dadf-2ea5-4340-a671-c3447ac1e676
280abbc3-6de1-4f44-8c29-6868341c5e86
c0a92c2d-db8d-444f-8023-e4b60ca4bc58
ee0af0ab-f1e6-441f-a08d-f4042032c78f
fa363077-fe3e-4ef1-962d-da6d07cef213
a27ec3e1-f051-45eb-baae-49966edf18e0
573c5b5e-8cae-4be0-8534-b43297232b88
8158b7ba-c189-418a-8877-34ceca937b6c
29b5d3d4-ed6b-4b13-a9e6-f98550e82247
7e0dc9d6-3258-469f-8a74-629784b599c2
b2725a9b-199e-4d27-8ee3-41186659c912
0c41cc01-ff8e-44f8-9da1-319b71087510
b1fd87a0-e7c7-473a-a49a-1662449cd59e
16a5d04f-d0f8-45b3-a43d-6ebe45360cb8
8f4bd680-7a34-4655-a761-9ec91fb7c2f1
ca182869-2134-475a-a4d6-e2ec9b6bc7ef
3bbf8fb5-8744-4bc6-8b90-ae10a542e02f
fa4fc68d-4802-4d48-a168-4349243f1c0e
b0f4c7de-7f4e-411d-b81e-219a2daae557
c36a9040-98bf-46c7-8661-365983fa86cc
727b9e1c-2715-465f-bdda-852ab2a65200
5efa171b-a367-4d8a-967b-9c551bb9bdd7
392e9f7a-7a33-4b59-a011-34243275906b
c8166355-8980-4b95-a304-bcfddf12ecc5
a9120e32-644f-4d8c-bc6c-1290abf64da8
b19703e4-2665-4bf8-9e52-706b2952f7c5
c2143c90-a01f-43c5-a233-da4bc2b41808
58cb6207-07e1-4216-a0e5-5926609ed6f7
eb3f8de1-2b77-4e33-8413-7abd4acba0c3
796c19cb-3dc5-4084-b75d-ebf194232f82
7f62ab29-100e-4b77-b2d7-0398c91ceb3a
1c8fb53b-83ad-4af9-9956-efa2074f68be
3000cdf3-424e-4354-8ccb-ed7d0902c40c
eee07273-a06f-4d39-ac09-49181bf98b06
255a7327-4269-41d2-9e64-f746345443c3
ebb0bc36-b7a2-4f53-81bb-74e4d99c9017
5f31fb79-bb51-444a-ba37-6e7fb8cfc593
331ee99a-a0ae-48cd-8eb1-7d58b91a3012
4efae354-4e1d-45c0-9ebb-a804bc5767ff
f4b5dbf4-e3bf-43e6-8928-3d8346efcf24
e3d00353-18e2-429b-8d99-fc975e35ec7c
4165de57-685f-476f-8141-4f47e3f93a51
ee2644ae-c02f-4620-bb7f-956906b95708
ec140112-8193-4eac-8770-c4ce880cd7df
016ad74d-84c4-4f70-a9b5-34edab51bc82
1d7f1200-3dcd-4fe0-856e-fd2737e8a081
e0a8b390-154c-4a3c-a8c9-c5eaefbb0d26
a9bcd48f-28be-4230-b817-a397ddba27c3
2b32bed1-cefd-421f-ab0a-b9d8a466177b
270e02c5-afee-45fe-8bb2-94ea5c536777
fe7bd7ba-a059-487b-bcac-95e60148ded6
6b110efe-6cc4-427f-ad5c-757c8500d1fb
004298bf-4778-45a9-92f2-c378e6749fdb
6c6558ec-d9b1-491c-9b1c-71fc457c1aab
112e80f1-c13c-4a3c-8f55-dfb5a3731611
1a712c4b-4668-4c2a-972b-eb52bc572548
d53713d5-d282-4486-93ab-00d7f0bd221a
d0c9f7ba-e40a-483e-b0a0-0614578b9a7e
afefb19d-063d-4865-b682-c128b0ba9971
30591303-3b6f-46a6-b034-c956860d9dcc
e41d8ed9-2de2-4885-b309-b3576157b38e
3e3c0cd2-6523-424a-9c91-0d743f6ad347
e124d644-55d3-46b1-ae84-515a184e8158
6b9f3287-8dc4-477c-b857-def517d655ab
e430d95c-23a8-4ab3-9b9e-427f0b23b573
a1e803e1-7ebf-49a3-b665-2bf0a5d3b084
c6065c1d-ecd8-4cb6-be82-c3c5ddd12de8
d5322517-64ba-459f-b6ed-5180dcc43c38
6564a3db-adf4-4427-9977-28a80c3be804
66c2b5ee-6ac0-448c-ae67-41d203f2e0a6
b5d142cb-b69b-4155-96a5-26d3dd842e7d
674bfce9-dce4-4bcb-b3d2-e25ee35261f3
f4449581-4dfb-4387-912b-7ed5604653bc
1e227dea-6f5a-4d07-b090-10403c3837f8
6d5d6aa0-180f-49cd-8f59-83b8b4146b71
e31e1d09-0c55-4b59-a3f5-e7136d46dc67
aac42204-ae57-4bcb-8dfe-44b41dd92009
4409f08d-2c54-43fe-bc0e-510b98bad3b0
ae6110c7-ab02-4856-a8d4-7b141f65e54d
ff832fe1-3520-48f7-88f5-1873a2fe7fb0
9abb4499-7d6d-41c9-bf15-bebdce9f674f
0cb1ab79-8340-459c-9917-9ac7a6f60839
72fb0bd2-29d1-483f-a338-92ed5f366032
ccbb5121-3395-460b-9888-74727b2a561b
9cddb60d-b467-472c-aa1b-8c7819e93b03
230f1c66-d162-4cf0-bc80-a507e40e03f0
14a520be-3a32-42e3-bd25-46b3ed332951
b8a2491f-32e4-4aa3-bfc9-a14ec542fffe
aedd1d44-dde3-42c9-a6dc-0dc8f9b60a3d
3feddc84-762c-4fb9-8d83-6a4bc7a19bc1
5727e081-9f95-4032-95c4-c5f8c6f23779
84155336-61ef-4a1f-8d61-26cd936db174
0779eabb-7f2d-44e7-9f3b-17539629ee59
1e8d9f94-d8ec-400c-82cb-8871f13cd579
d5238887-5b9c-420a-a4ec-98d18e793055
3c1aaee2-3ef7-45a9-8d64-7f1ed4b3cb92
a6e81a44-7def-40fc-96f3-2cb27856675e
37ce6876-38c7-473a-9308-a2bf57d0fdf9
4e8cb2b1-53db-4fa7-9de0-ccee850862be
9950cba6-c7b9-4ab6-8580-8357967c35bc
5d2b5d4b-0f4b-41f8-a877-929352790c63
3c03ebd1-322f-428a-a788-5796a52bdc87
1c469cc9-3a3b-4e6b-8c44-19ab659dc2f9
0cd7edaf-7a99-479b-865b-ae0db3506785
bf984dd7-ef86-4d9c-a40f-38f7d6a121cd
380b2f09-8a1d-474d-9a6c-6f2474077ad1
1673d719-48fe-439e-9145-8223b8c99c6c
22185e1f-9c80-47ad-b7c8-a06045540f42
6e240159-ff6f-4e91-870b-de2591a25a1b
8ed83d68-9b95-417c-b21a-060f96ec5e17
c05da806-5269-4d31-a413-d51a450501f3
88e70bca-86d0-407c-a0f5-4737e3ebb334
0b661fed-3539-4514-87ca-10aa3fd4a2fd
0c147837-2851-4d5f-883d-e80d53ff96a3
8f2eda86-ee6f-4cd0-a6c7-3caae8e9af36
5fcbe6fb-6347-444e-bf5e-41f4ecbe225f
0d9ddbb3-804a-4a9c-8280-e0e8a35d7268
9606869f-f0fb-40ab-8999-e103278bf1cb
92bd9147-78da-40f7-8e5c-e96dd7fa31c3
9d76c448-c324-47c3-a8e6-b6f43ed29b89
576e3b8a-45c8-4279-aa91-bf6d578b5de0
b1cae075-2a18-4792-b348-1df9bc3a1cc3
38bb0906-0137-4832-b041-7f94384fecbf
bf855294-84e3-4384-bce8-694ed87f577f
82d11b28-3ca2-44ea-9e42-ca26fb49df89
b8493f0c-70ef-4452-8ecf-97cd85ccfc7a
052a71c9-b233-467d-ba9c-64e2208857d5
5949bddd-b5a7-4666-b09e-8719d4dc5de6
16676fde-9436-4b93-ac39-cb9716608c20
4e14a7fd-681e-4865-8075-5a0bab655e05
d917630a-112b-47bf-86fc-7d0a8f906af5
fe2a232e-d61e-49c7-8a8d-88c786a49699
38652b31-b573-4716-91e0-9a26903384d2
22e6adc6-9203-4e80-968c-4b3d1262a927
69062ead-f13f-49a4-b541-24e8fd76220a
f39aacb5-d78e-4962-92ec-6d1150e68f7b
e39dc4fb-9cbf-4353-9088-b7b3ce58d734
a30ce62d-6bfa-4d98-a4ad-31f298334e96
7dcfc7ca-12aa-4473-835b-501a66221c8a
77fa653b-059f-4a90-b9bc-f81630260296
46bc726d-8e14-493a-9fa2-b4a1a452f86a
41cec485-5905-4b65-ac16-737f8345d7a7
d024fe10-910f-420a-be61-e35498920c0b
ec9942ef-4e5d-4d2d-aa1a-880a0f0e0953
9ea62798-a965-4111-b6dd-82c6b199adda
4604fd73-094e-41cd-aeb4-6ced8f1a8f13
e1630c09-1406-48fa-8181-e7d1a94d29d2
f4aaeb65-1a9b-44eb-ae17-75c63f76809f
aa973de0-56e0-4799-aa58-19a04bdba1ae
6b6bb350-48d3-49e9-b292-6c7698108630
82d5c810-1301-4b2e-8fb1-ed471f083350
227ff52e-ee19-4771-868d-3ed1a48bf1c5
6402f098-d570-4676-ae92-1906bc75e739
6a3d9684-df45-4b0a-b403-7957e45a862b
95ef9d13-45c4-490c-b964-9c015ae653f0
f3914c1a-e11a-41ec-a6b5-55d805e7c9f8
4f2fc3c4-8872-4567-9baa-2069ad8836ec
3c5214b0-4e0f-4e40-b17e-8b6aead349d5
b33d2a12-9519-4981-b9cb-d3fe582fcbde
7835b394-f859-42c0-beef-5da69aa4bff6
350059e7-b31c-4cd0-9b6b-7e69270133aa
1b9962f4-0f17-4de9-bd56-038b13124e9e
a9702445-8d98-4d79-acf0-5f3273b46e72
a0556a7f-5c6d-458a-b561-c52b8376c073
f70ed8fb-7163-4cef-a894-ff9a827711c8
fe30f722-cf9e-4fd5-a688-85f31a0c4e57
7105ed1f-d5d9-4aa7-a835-533bbd5cf547
c09fa349-1820-4522-99be-5210c7f67313
1cd7ab46-6f6a-4c6b-8317-26b0cd55f21e
70c51336-15bb-4dba-9326-91365c143cc4
683f8b3f-227f-44ad-bf6e-e7fe29c0a4e9
94cdbd1b-9096-429a-8c67-32320c5161cd
4a244f00-1cdf-40e6-bfb9-6ba9a80a1394
e9cda270-94f6-49ee-8e66-dc3e59cd0f02
325e563e-d462-4c61-8fad-782aa524e4f3
4c5ac1ea-9acc-46d6-b02d-b763007bd4c1
37fb3b44-9978-4c23-9f94-4a8ca8d45770
7ba63f05-05fc-401c-8a5a-bc879fce6770
9d61b3d5-1020-4da3-8ad0-fa46caf20871
fdacefc0-4ca0-4ff4-b830-df8af4555e94
54614314-d8ee-4cef-aa51-d139b80d5134
23bf8516-77d6-4f46-a7f0-657e9e16f974
5a8c66d2-0c59-4269-a8dc-d79a6ae781ee
93ecd704-f3f7-4e0c-a1eb-81e99d42870b
5fe87f88-a1c2-45a7-9358-e428e4df366c
8fcec8aa-d0c7-4eed-9415-d4716afa33f4
f41826f9-198b-48f6-b44f-da2249426a58
f0ace8f9-0cd7-4ea9-a360-2c1cf6f2300d
8495bc5b-832b-4c3f-a921-e74d7ed97608
639d5dbe-7584-4578-934f-0f8cb4ba1575
3399f16c-0f14-4eda-a197-4c0c80324a0b
cd0fbcad-3f60-4263-98cf-51b8fff039c6
248c29e9-cddd-4424-9725-03a2131d3223
b66e8f37-456e-4e1d-acfe-881b7e71ebbe
e74dc4b4-b818-4883-8a5a-c063805b4464
0a2ffa77-4f7c-4f34-84b6-6aa89b28bdb5
278fb53f-608b-41e2-9bdd-58fc80e3be44
b0b23727-ed9f-4653-bc6b-303f77fe65fc
021b8049-e1ae-497e-9227-22cf5867467c
5a3763c6-399b-4823-bae0-e43099c20748
53ee39d1-42aa-4cf6-8755-e47384f44947
c6fe9222-88a9-4dd8-a8b6-9ea4c3e194e7
2215ff2f-8d52-433d-a939-df295949468d
943998d0-0159-40fd-ab50-6be7dce85b92
063da5eb-6441-44ac-97dd-2b363604bc14
99570aa7-8275-407f-af79-dc26bb64519a
a51b956e-13e7-46e6-9cdb-fe48f93ce9ae
9e0c133b-aaf5-4b06-85cd-254b844e90c2
d626e850-ffd9-4169-94ca-1f4a7eaca399
631f3258-a984-4925-9849-e4c6012eba68
cae1ff29-c883-44f9-ab94-ca90b8b32924
8d9ef9d2-b8f5-4528-ac88-614d23a4f899
a43e41b8-e508-49f4-b664-4080201dd1d7
89bbcc31-b2ec-4c1a-aacd-13513b8e5329
d14b81f9-d3ce-4364-9120-1d51ac30ae75
b9352f79-6cc7-4db7-b9e8-952be9ffb349
55387e09-c703-4739-8149-a67456efe653
922f9336-65c2-4cda-9db5-b38e96429a65
77871af8-838d-43a7-b503-b908e634fa1b
27a47d3f-1a43-4902-9854-9d7a29383abf
eaeed0ca-b861-407d-9d77-38e62fac263e
7c1bbacd-acf0-4503-a52e-3a1780b5f71a
c1f8ed40-d985-44a6-a6fc-c76fe2e4e1e5
912945d0-7228-48b8-9471-600a2195e74d
53c6be3d-674c-4842-b58a-9763cf72cc51
fe08df45-4a02-42a9-abae-65e3a5098ce3
412e3b33-1325-4429-92ef-95e0c758e161
fe80f041-f386-4176-8a7e-29555dc09154
d51f8055-74a9-4802-a7ba-878fead313b1
7df8a2d8-9c0d-4bdc-a466-fbe21be663a0
c17bdc7d-cb8d-4c35-8b14-f645a52477a6
4a1f41e6-088a-4678-be9a-dff100c68d52
4f514971-7e26-4715-b2dc-fafb94871513
a3f29436-f98f-4e2d-94f8-62149ea9e378
63a896e9-454a-4831-9020-0d2575d26ee4
46331af7-151b-4358-8b0a-c03ee824571d
3a76c677-cd4d-49c2-b8b3-6b8165d579af
15b07863-778f-47c7-80b5-f1fa45e90b66
5e671769-2a02-4d8e-b6f1-4bb28bf487f9
545ad2ae-db83-431e-9225-eaa3dca9e5cd
f6f7206c-692a-4dc2-80ca-54d9d56c5840
61bdbd9d-c488-4667-baa9-2757f0160200
9b28b94b-64f2-4404-845c-8bcd1908b9fa
eba320d1-ae00-45d3-b7d8-d376278472c8
8eecbd2f-4133-4a76-8d01-45d2f2c32fe4
a6b82fd1-985b-44fe-9313-24678d885462
2474d892-9d54-4d1b-a540-cf75c4a4c140
c408238f-7939-4cb2-a29d-0cadb74ff455
0262cc19-280d-4603-87f4-104ca7643e19
9860aed9-2c28-4905-bab3-eb9982b25159
7d2d0cb1-c116-4eab-abc7-a218bcaa0656
9a22c9a4-7f68-454c-a95d-959dd32c0172
5aec6ede-1938-424c-a309-6d1dc1b007bb
af10b89a-c42c-44c4-9def-a8116fee62b0
e9fc7ae2-9cae-4373-b0c1-f4c3683d1cae
48b9fa94-b91d-429d-92be-1d8cf5f094a5
611cf482-010a-4d9a-935e-ea219cd15742
ea60f9c8-1408-4ce2-ae91-de02e2dedbe4
3558d5cb-2be0-4058-b335-9e78cf2d9dc1
727141f5-3555-428f-a7ce-2c5c8b80239f
f8ed7569-32c3-459d-a36f-3a20825bae8f
9e9ce0b6-9584-4bd7-a659-36bba6f86be7
821b335f-cc8f-4ff4-8386-c39e836f12ec
7f2a0402-3fa5-4d98-b06c-b2ab322ec58d
4268b814-392f-433b-94a3-4339f8c26d1c
c070c5b6-17a0-4343-b4dc-0f872338f108
a6afc69c-fe51-4ff1-8569-50a94c6ecba1
57e353a1-9e5e-4211-828c-b10be7a36737
c2d9233c-97fc-41b3-98b3-79ff7876f540
c11d1a2a-777e-40f7-af57-66390f127875
fea38f4f-8bd4-4902-87f4-b7e30f76a1fc
535edc7a-5628-4b01-8838-00053015386f
c0e6da8a-4164-41b9-8025-e595c07b991d
72506650-4ecc-4672-8c20-41bb2d45b643
69dc6108-f708-4bd5-98a8-021aa944bb76
fbb30987-baaf-4f9c-89e9-f395777137c2
4ea6ab30-7090-485b-879d-f763f963d86a
0074241a-3eba-45e5-b322-abfb2ec85f37
4f0e562e-d378-4405-8ea3-f4123d5f5c32
8d4b5054-f4b3-4924-b01f-cf07d162b2d8
52c60a64-d6ba-4a21-abde-f8f676d02d6d
dbcf582e-0037-47df-a02e-ac0d837b124c
11308864-e6b8-43b5-83e5-653f5f87e01b
cc4716d1-1c78-47eb-821b-4fc96d013d12
e3f13964-09c0-4e80-b028-44920627b6c9
32b7166f-9c75-46fc-8439-9d94de875d8e
7dfb7881-2388-4869-99ba-8635497cbbe9
49fcf0a3-bb72-4fea-96a5-59efd3a4f747
ec6cc611-728e-4957-b406-51135e4002b3
b0c07ba3-b4c2-4d66-a620-72cb9dc9b051
58a67eb6-8555-4672-8934-14f3bd899eae
03959d0c-b3fa-47a9-8b86-6b71f1c45e33
6c5c54b1-2bdc-468e-9409-04b5c3b04456
a8644319-a85f-41f6-afe9-70d2be5a0f17
ea33aa1f-e525-43b9-ba74-b4ac2ac0b216
9faa1c43-5e79-492a-b0b6-c208e20b6c5e
8b12665c-5938-48f0-8d97-afed53749af6
4df9ed7b-7bf9-478c-a957-ff24b061252d
810eec1b-7e9c-4456-86a2-f16bb825056f
fa99344a-f27a-4f94-99d1-a6635772ffd5
e881e293-97f7-4264-bc9f-5b91f5aa899c
b9bee65c-b8e5-4cb1-b9df-436c182513fb
8ac63d24-a5c0-4764-a4e7-c7779ea82f5f
4edcb4e8-722d-4fed-9ee6-3bd9b6bf6694
a431abd2-6904-4225-b89f-3a963a951772
2a2f9a0b-ef9a-43a4-937d-21841613e48d
256dff62-4d4b-44e7-ab53-b54bf235ac7b
62740eb4-a36d-49de-a41e-1e9e03da5d86
c1d2b54d-5462-4054-ad50-76f6d07685f7
74dc37ca-2075-4fb1-80b8-3ec739cfce3d
711f38e1-5cf0-42e0-a8de-b20d0b2915ad
877392a8-6bfe-4290-bbd2-e05c2718aecf
815880d0-ccc4-471a-829e-d9a916184027
b7fef112-8766-4e3e-8ee6-042084f6cfcb
e867759b-fbf3-4a85-8c7f-be03e0373750
09fb7bbd-a00d-4bcd-8a33-ef83a8d54c03
f5e1af8d-44bf-4c1f-b80f-9c4b68676e49
5c2ed705-6a5b-4670-8cf6-ed950896109e
882b6017-44ed-4505-ade7-eecbebef27f8
f8325310-f0c2-4047-a20e-f4ce49be9042
1e9e27bf-d133-4fda-abe4-4728607e9ca0
7af269f8-c25b-4137-9e6f-cf2885a53458
c8ca5bab-a0f2-47b0-bb18-c249a62be185
99153e16-dea1-4058-a10f-c88bd0285735
38939a4e-21c8-4f54-8235-3fc0f501191f
385bb460-d6db-4655-ae92-87d217153875
046da48a-8c60-44f1-a7aa-63ed6fc8607b
b9679059-d627-4f08-b197-992bfc5da751
43ad5aff-1c02-4b56-93a3-becf6e3a903f
8f6462e0-9cf7-40d7-88f6-cc100735a55e
036797ce-0802-402d-aae6-ddb7b372e906
7c6dbdea-e867-4345-9d03-bdf5b3f45936
5726bd4b-9cc0-4087-a1f4-39a50925ef98
f55d2107-06eb-4992-b24d-23e35efdc0bd
063d577f-f57d-469a-8ecf-f47f5b839d10
cb396012-4148-4e92-bad9-04c7b3426cda
7ef889e3-377b-457b-a543-638a49716137
093b0c1b-9966-44de-9da8-ffc724788dba
e9d45080-18ca-4c31-abcb-65873a345df8
fc494497-9623-404f-9549-15da0a0d2a9e
cffc9bb7-c755-4c7b-9803-1edd990d1e97
524d6626-b2a6-4b28-8495-90f7f4d8102c
f0d42881-819a-4a40-a5be-4032fb549022
6ed7c6d9-fe9a-442a-8363-c9deaa8067b4
250e640b-0715-4c2b-9408-668182d5e264
e6b49dc3-15da-4ab6-bb6f-42158d7756af
c03bf87d-f957-4a4f-b231-30251ea7093d
43304528-a344-4009-b4c8-65b1642bec81
5754dc5b-1bf9-4191-9497-7207bb8de5d2
6f266fe2-7b01-4e79-8995-e7765acea6ef
536f4d6e-89c8-49ad-b1b4-8592dc4d0d19
c5f3a78f-4dc8-4525-940c-ca2c785d8ac3
0ac7fd87-2080-40f2-8366-b429c81f51d4
42e3b341-efa4-485c-b302-5f0a690f6938
ba47bbd0-cd7c-4a58-aae0-9dfbc1081296
a97d9449-1a66-46a1-9dc4-d320bdfa81db
27b67759-aec1-4ae6-abe1-10c4080c334e
d681f5fc-5692-44b2-995a-d51ef01b1693
b17ff137-7fd6-4c61-a187-34acb230ce19
a5559b27-d71a-4efb-b2b0-21de7d08b2e1
8e287fff-8f61-4d8d-9ea7-7cdf235676db
117362fb-c829-4b2d-b0e6-f98675702ff5
9b87ab88-a4c4-4f9d-97be-81dec47fc391
e6273be9-95ee-47c2-b699-51acd38c7663
54ab0476-7f33-4d59-95b8-a2a62a3aa88b
29be710a-0fc3-4f10-b299-f6fcd4408108
9a6444d3-3967-4906-9ba2-8e0a0b6dfc18
cea73127-bc45-45d3-af1f-0fe6707792fa
b72924c8-3ca2-4ee1-8075-019fbc11a5c3
cf492312-85cc-48a2-9839-13c61591b484
1348f9a5-f35b-44a3-a473-df17576adb18
93de51dc-0654-4ee1-81cc-38ca4016e763
b2e1acf1-50c2-4310-8b35-a5217a6cbcae
d50cf11d-b5bb-426d-8885-419519b8935c
4c727eb1-9e09-4fbb-8ad3-934f76a493af
c91f616f-ef58-41e6-abb4-4c9c2c9a30f0
6c47e8dc-c835-4f45-ada5-a8a1f18c032d
efc5c07a-b66e-4233-8de2-af876d431db3
fbce7bba-38bc-4652-a65b-49660ca0062e
44916fe7-474f-4aa8-ad5a-cf710d6efc36
3fcee6a5-54d4-41d2-90f3-8c229f9812cc
b21a77df-b7e4-47d9-aed7-950205086df6
221f0579-f67f-45d2-8ca0-c5482bd6d579
4a020b8b-853b-42f2-9b42-5989958a77fe
78d7f8d3-8e5e-464e-bd9e-0fdbadcefab1
ce4ca766-b0ac-4f9c-a08a-c1442023bdd9
1d0878c3-6bab-4943-b914-2490c4cc801d
9bea3c35-adbb-4270-93ea-9e4ddbd97d5b
2e9898fd-27d5-4f9c-a887-28aab8c7df3d
e29d2922-1126-4ad7-9c87-4065444e8a9f
f42a3af5-9a35-4c9c-ae27-e6600764f4d6
a8445c76-1bcb-4ebd-ae0f-8c4b7d322fbf
7ee57451-de2f-4e62-887f-3df945b0080b
bf8635ca-2760-4c62-abd5-ed5599835c77
872c814a-9363-4f54-9484-8b9f19407fba
9563327d-ba02-4b73-8699-748fa1e3a0b7
887e8346-a76a-4756-b093-0261c10e14fe
d5114100-c60f-4671-b562-7cf04acdd3e0
b596c22a-0904-4a12-88cd-64334af8ae0a
02fdd8da-5d80-44c4-b139-1d1d04217380
d2983d72-4861-4efc-8739-03762a2aed77
3ebef8b4-54e1-4468-9e9f-1ec702c4532b
03c32302-4dc7-4596-8e3d-d6f1b170e4db
000765d9-4ce5-45d3-bd3c-5355e15f34cb
663e3460-97a3-4b14-8fc2-8259bf57b6ec
7694dd85-23a7-454e-8771-5153daee7e6b
e66725f2-e0c0-4150-a0e3-78dbbdbd9a9f
72e6e47a-f5b5-4587-ad77-50d6e1552d12
ea3c6b7b-df54-4f9d-a6fe-2db835deaa5e
d93f2b81-c6a6-4a7a-ab4c-5f210b192751
7608e870-37ab-4617-accd-f92fe90b7151
675afe10-382b-4b43-85aa-0506ec9f555d
22dfbbcc-d2d8-45de-a721-7dd5ddf09415
651394ed-4242-4df3-9a69-b76ca3d36214
01544a58-58aa-4594-b8d3-4d7f58ddefd4
6fd3cb33-9e87-49e3-baad-557964d2f64e
66b4f79e-7b6a-4e4d-9822-22b608be0930
fcb22a69-2e6b-424c-9af4-88f00e9ae999
392e18e6-6d49-4817-891b-788ba812b5e2
0e2f44af-39be-4e6b-8878-e1bbd44b59d3
97de39ef-5f96-4ca6-b06f-456669c1ec5f
9b848339-9866-4adf-9bfb-0cf90db9f8dd
09f8fda2-e43b-468e-b184-1d0ae2759002
b5288c5d-1cf5-4f0b-8d1e-d0b13a8fe319
9723621e-0012-4904-9c51-f2f8190fadd0
6b769ed0-1b1e-427f-b199-dc9dad29c6b0
be74e13e-5bec-42a3-ad4e-93279d92fb87
423feb79-7eb8-432b-bdbe-69db0f231c25
c59476b1-4927-4566-a516-71ee4afe002e
cd99e285-852a-4f0e-9e57-196f2bd4f6f4
4f53e035-8417-4e6e-ac69-417baad593c4
db403ef8-498b-413e-90ed-cc4365aabe4a
8e252abe-0fb7-4061-8c5a-6ecd20ff3606
9c701e14-325c-40a4-9d85-7fffbfe5d62b
d73868d1-9492-4beb-8816-bcf672d96071
7e69e71c-cb13-4b01-bdd6-1e7ac0575a46
6cac5bdf-ebc8-4b0c-a4ca-2bd422a7ca2a
f2150316-6f42-4fd5-adca-b2290dea74c6
9a9c52fd-6123-440d-a6e8-2387f62177c9
de98655a-769c-405c-af59-62a5a9363dc4
0ebf9d05-0765-4361-b5f4-25b2957b71ce
718a4e3e-799d-4a49-90aa-2b256c971ef1
f0f7cf9f-8c48-4266-bf41-24d271ac39b4
621a7933-1901-4479-aeb9-cc0d00ff8430
e49ee095-ead2-4293-b05b-f85c8782e137
26727e6a-848d-44c8-b812-a07c2ef548db
b50e4677-df1c-46c8-a1f4-21ae69bb2e83
a9b3411e-f772-40c5-8b0c-4b698eca4bfb
79db10d3-bd67-4d1f-aa76-1b22edfa7b6a
cb34fae5-248d-4217-9b35-39c4831725ae
2215ffdc-090a-435d-b3e5-d273f6ab6375
927ffb63-20fc-421a-a82b-79d072b8493f
dd01823f-c39f-4f7c-b308-035d17a669f8
39061a19-2bf4-487b-9b9d-62fed4eb9ad4
1bc4b405-0bec-44d4-8f54-6d302557f579
8c356d09-4c48-41ef-a4d8-1ba73a6c5b23
57009282-d376-4160-9a8f-00f293365e7e
2db06b82-034c-483a-b58c-61f97d3446dd
847130d3-61ab-4a74-954c-d3644cd5aa3c
1ea2bd3e-02fc-4c01-9e50-6d77aef79c79
12e700af-7d36-4ccd-bffa-38b7330cab89
c9de147c-6784-4b33-b037-9968905916c8
702fe2d6-e4f6-453d-944a-e3b668da29cb
b45b57c5-9328-4fa9-a9e8-bfc755aaa04a
9da829bf-e121-4778-b7e7-0ad502072fe7
ffa32e59-840f-4719-a4b4-9b6617c5e094
27469f5a-9cf8-4f50-a347-cc967c1146df
6aeab91b-aab4-4fe3-9007-56452ea5a547
22e540c3-5f90-47a6-89a2-d092e6183169
05b10446-8795-4ae5-a4d4-0b5327193560
6a895aa1-7741-46a5-91e5-8ad1783b3633
6a9694a1-7321-445b-acbb-3234274e4149
9d067754-277e-4a9b-98af-8ffadd4d5c61
89629991-7b66-4b61-8e79-0ce397fa497d
e5688aa2-e344-43d4-94cf-29cbe0cf2408
bd5f507f-e138-42ee-9e1e-1542251137b4
d2390a9c-20f2-4934-8547-b9113251379b
a3ca18aa-5a4d-4e4b-9045-0b7530da24a5
67b25cd2-390f-4a6b-9f99-bfd59176ddd0
efa4813c-7a04-4748-a532-617cf22b1221
1f668d98-f463-4687-bbce-2390ee4ef113
9d9d4843-9f29-4dee-b0a9-9a127da64f09
822af2fb-6ae3-49ef-ad97-31807bfc4305
600cac76-e5fd-4740-b903-a9f369e07353
776295a9-8218-45b0-8dd8-d653ca3a6521
35f2d833-fdd1-47b7-b10f-1f9a030e6430
b0747543-32e4-4cff-a857-f31f8cb681df
d5c65099-983a-4519-bcea-91924e0542ca
4fac3f2f-796e-45bb-b97b-72b99ffd0caf
76da4c30-4ea6-4dfe-b3f3-7c7bfe72dadf
1fec9929-057b-4e08-9cf9-94e81f0bfa61
5b6b2224-b312-4afe-9dbc-bb659b4d8e6d
63042302-e182-44c2-8078-e5006eb21645
b5ad1ef3-4b5b-463f-ab55-481d5d1798c0
fdd1f6dd-bd04-4090-95c1-e7a7b7b7daa7
21ecf720-2c38-48dd-811a-42bd1ee0ae22
f1f6e362-7bab-48a6-b2e3-8dbc225a5b99
29eb762b-ed07-4f80-9db5-b0980bb035b9
5d57884b-b3ef-4bc7-b149-e6a96ed8cd36
71e6f452-0687-46e3-bc7a-95bf73b49715
f679b49d-8543-496a-9691-137a26fe569f
980c32c2-e074-4aec-a579-cf13056153d6
324bfd4e-caea-4059-bf11-fe6f284e6cb6
872782eb-d7c3-46f5-ac2c-4dd4c2827d39
2bff710f-a12c-492b-a01a-c5a961ce9bce
1fda4157-81e3-4eb0-85ee-d5b479d7a408
23c0089d-d362-417c-8bf4-d9e6e4c7ff74
75b531d2-b9be-4b69-b26e-585f6a26b3ff
1ad30e66-c7a9-4aa9-b7c5-747249eb1f68
ccdcb679-ff4c-4ebf-a8c4-4e67e72cc887
15713477-9102-4c40-b0ec-8d6cd2c4c54d
913d8526-fa18-437a-97c6-49afc4de8776
f6be09df-c4d3-4c7b-949c-ffeaa5b1c9d4
6f4327e8-d208-4607-9e76-9e83ebe0583f
f46aea9d-e9fd-4e55-baf1-3a9148e518c4
3b1f1186-12b7-4674-92b9-692f067c65a7
9e287ef5-cb5f-4c84-b94a-74e37d5b93ea
fbb274dd-bc32-4385-91da-96447b82e776
dff692fd-0be5-4fd1-8e45-d103147e8279
1b8ff4e5-006a-42d1-9c80-565293af523b
0a2a852b-e99e-47e9-9e18-242d24145ca7
9520a0f6-fc02-4a8a-a9c2-4176691b5485
c8db6de9-78d6-45bf-80cc-8d4be73e6ce2
3c0e0d31-9b89-4ce6-b25f-d1d6fd916102
c4a75c90-df2b-4199-a897-3818b1a876bc
ae993569-2b20-479b-a767-114a6cf9310b
b4ed3a93-25eb-4dfb-b348-e2fdf6293bd1
f6ff199d-9b08-4503-81a4-47b41bd2df79
aac2e089-b872-4444-bdaa-d88f9063434c
59be1ebd-832e-4781-8ac7-ba7f7cc91ba2
d2ea93f7-72cd-45b8-b133-1078bc553f3c
8e65a937-d251-450b-a14e-ac8e44f15fde
d7023a15-0b77-4be0-a08c-441679610877
8987166e-9699-4fd4-aaee-72be13e71d9d
7701969e-8468-4b92-90ed-968d9d083ab5
dcdc8703-64d6-42b5-8ef3-06e0cdfeb0e8
82c9f406-8206-4356-aeaf-4fd2f7543a4f
d8734dcd-b693-41a0-89e5-3d9a7ab3e347
879e33ca-1d9f-4404-9452-fa8810c25d23
c95d8d98-990d-42a3-8be7-86f62c939e68
f60f6ae9-ae1e-4b68-ae7e-34465d6d32e9
14c93e7c-b8a3-4a06-b7b7-21039001d159
7537206b-79f3-4b1f-b17d-df44297c4ed2
046c86a9-39fc-4f7d-baa2-75c5c8e3657f
258615a8-9379-48ac-87ff-1f9ae9fc37af
b8856bbf-6b4f-45ef-abc5-b67ba2ea209e
6121cd6c-e6fe-4dcf-ad4f-14c3f05928a7
46c2a220-1f90-4244-9084-c9c33cae417c
e5ea9104-c7be-4fbe-b071-0bea2e78b974
919c631e-7e55-46ec-8636-23d9c010ad06
14527cbe-9908-455d-b35d-dd8e42fb61ad
a65168cf-12e8-4f55-a2e3-c0d7284f4b60
f1431dbb-908a-426a-bfba-24319ab44b29
2986dd7f-cb7d-47cc-829e-92d307d3db17
7bb62c8a-640e-4f16-8a4d-6d843c10ac55
022f08d1-f5a8-405e-ae2a-4af850ab990c
362b40e5-655e-464f-9619-281c02de217a
382b55bd-0b33-43cd-b1a0-e287ba791d4e
a84fe643-d2e5-442e-8016-c2a644724958
d9e85bc8-289e-443e-91a1-1f01662e612a
5ed5687a-9ace-436a-9f74-56913923253d
3f4d12b7-fd67-49bc-9154-357218ef4856
69002367-ff2c-484d-82e3-b20dcf554a36
35122008-79c3-478d-a570-3ca948fa8293
b581a464-5534-41af-97a6-2af33b117cd1
a6d99f1a-a5f9-4c23-b628-8afd913cb70e
c121acfb-144b-4b2a-ac8a-258705c67437
d42f07bb-2d3f-42d2-8d64-fddf79db2f82
b185474e-fe8f-4218-a266-6f1faa7086f7
70d65539-5135-4b7c-995f-957ff97b92c1
f732280a-1d8d-4177-8432-92a73144b047
b93458e9-8c85-4b69-b5b1-36b7834de447
a1d39e3d-bf1e-46ed-8bb9-c1165f67127e
44b06049-00a4-4559-90b3-7805ac6d775a
20247251-c1fa-4404-9bf2-6b927949173d
f9ef156f-78b7-43d0-aa34-cfedc43db6f5
0efdd0e8-8641-4953-a0e6-1bc623a22293
8728c724-6066-4f62-9609-7d70a3a4d4a0
8af50ebf-a549-40a0-84a2-3a6820e5d666
c64d23a3-267d-49d2-8234-fa4426c6e1b1
8f50ab60-095f-454e-8077-d3d316dfb81f
6f27f30e-c21c-43eb-8d71-1e9cf4d17d16
53ce95b0-9e47-4c75-b647-8982b80ddc8c
3d9ef7d7-bca0-4eeb-8550-eeb473724815
295e8241-848d-4062-8c47-4a81b8611d86
aa21195b-2153-443c-a3de-76c7e130a7b3
b54ea94e-1d4f-4a5b-aa3a-658bae00b059
7f90a117-0473-44b3-90d6-2e6daa50a6b9
a607200a-ed76-45a2-ba63-9995187f1524
21fca985-ddc1-49e0-ad30-4aa82ad73266
758cb013-50e6-4710-8e3b-ac39715d2ec8
567224f0-7590-4dc5-a04b-e862fc7f7ea8
e19ca319-2e67-40c6-9d05-89fd87f9d620
caeaead0-43f7-4c6c-906b-5abfe612ffcb
fc0ab996-b6a4-4f5a-9ef6-a1b90d2758d9
1c10a317-1905-4178-9374-99662a87363b
9f4749a8-3aa7-4a54-92eb-8411e2e20c9d
5fc40f9c-6028-4499-8aa6-5993393cf19d
b08a3846-6563-4b78-b5c2-8648cee33795
81e32b37-3cb4-443a-b0f0-20729bc462cd
40fc2529-c4fd-485f-81de-45ee666ff4bf
e850a141-f760-4fc3-8616-d054c8bccba9
eea95a54-d056-4a2f-a4f1-7628881f4a08
e9124db5-fc0a-4087-abcc-277a7d16b67f
596d5863-a350-4acc-bb2c-f3b06d4f8ba2
c1b8284e-c59f-4fd2-9201-6d067022a916
d47614be-873b-484f-83e6-d0025ffda75c
37e21a37-6ab2-4995-b3f0-1c385eb3a678
6cfae7f2-f67a-4307-9e51-290450073f67
9e0ea57d-6906-479e-87bd-1ebc521d8698
7c119f03-a737-40cd-a384-41721cde15c5
4e6fd40f-9c49-4fef-9826-eadbc34be337
a19e8a46-a8c5-4653-bcd7-de58ce34a5a7
5c65fb2c-12d8-4bf5-a06a-19604e634675
ee87d0a7-5d1e-4216-bdf3-de2ab91959e6
b70c093b-046f-4bbb-abe1-4f8a8d0113ef
e73c1587-1453-4257-97ea-d88e23de672b
45eb5ace-e839-4c83-b5fc-06e28b3cfd99
10a793ca-d348-4b0e-96d6-3cc4ef46bc47
c6c0854b-5374-406a-bf39-e4f097041857
b0c703c3-c120-4840-88e2-8ee544973a0e
e2ae667f-9730-45dc-9bb3-5d7491a9b6bf
720b4322-2ad8-42f5-b04d-f3722cfcbe04
4b1a8eb6-3ff3-40d8-8170-cda909e0523f
1112e2f2-93d8-465b-8a37-e1fb9c2c4acd
3d20c854-ff56-4b05-a56a-4138d278388e
91a58008-cba0-4d0f-bf4e-2625b7aedc5a
47135032-3a79-45b1-871e-e5e99e2a1505
f0d61255-9bc6-4cb9-9920-4b288c0e063c
536eb209-af8f-47bc-b555-82c29c037958
cfce97f0-a1bf-458c-8c10-eac314f135b5
3afda381-7b2e-443a-9ee8-aea9e2e63f37
fdb4afca-9670-4336-b7de-51aec827a229
6c243b0e-e156-4931-9d27-d0738f1dc0db
f410fe34-e5b4-45bf-b3b7-6a5e8fee50d1
f5dc56b8-b124-49c5-8bc9-e2e4222d991c
2e6c07af-9b4d-4dfa-a416-61705bc7f0a0
b02a0e8e-7136-4243-bcb0-76156fa411ed
3e41fb83-e05e-458d-a96f-6b5fa346b0c8
866d7189-a13a-4256-a2c8-ded01cad71b7
8600338d-6093-4833-9b10-2b06bb6d63e7
1a112200-d28b-4a26-9407-380e1979b395
798acacc-af8f-4656-969b-648bef717841
8ba6ca5d-4d53-431b-a998-2c4b571c615c
75ea2ac4-6a18-4bee-b06e-f8d5fb91225f
e9d39670-d737-4e96-90f3-b29d158298b7
df9a9cc8-3c0d-42f9-a5a0-ac8d50c15460
469e111f-9034-48d1-a242-7d9ab347119e
126e596c-70d2-4b8c-be7c-de724a2d242b
5a1ddf4e-b336-47b5-a0ec-2b8fed25f775
a5682d72-3bc3-446d-b7bd-6fce2b139b2d
8b72d475-6dc9-4efc-b7b3-457aab59b782
f2efd917-657b-4b1c-9372-e114c6dd3a87
5ad3b5bd-c906-4784-bae0-fe046c81f7f9
10b82a4f-5ec9-45a9-90f2-22c2f5c9e94b
5b507274-ef0e-4d6f-9d2a-61caf9138ea3
7883be03-ede1-4860-a6b5-0aa89c9aa6e6
18dac950-1431-42a7-9110-ed47fb2cc6ed
49c1de4c-14aa-464b-aa4d-c02aca167e02
c9e8f2b8-f6a1-4c7f-a2f2-ad8a5b80cf29
3d6b295c-3ca7-4b8a-901e-bbc945f799f1
bd9e0d32-6642-4eac-80f4-9415749a2771
5686578e-d097-4b30-af29-91066b45d036
bb354908-3afd-443c-990e-1ffe185543af
d4461e82-5a87-4491-9ebd-2668f7c307fb
ac39893e-43dc-4882-b962-4d01b4a4a0f0
f6bf498e-b35b-4d00-82cd-86b06e37ba6d
ef36964c-ecb3-471a-aa44-58b7c73be3c3
5d8d4de2-abf0-46b7-9220-376c9815c643
1f180e03-4f02-4f27-86ea-a35344fe9565
9a8dc902-b509-4ff5-bcab-270f998c3845
5dbd5e06-f1cb-4c75-9d6a-8fa1cd3327d1
f10e7eb8-842a-4101-b6d3-4297c9c67248
a44d6692-d956-409e-bb14-01695d29b3c1
2844f18f-265b-473e-869e-b868a6c3d852
7e2cb1ea-2258-4ee3-bb51-d0394513fda3
cfc561a0-6b5b-411a-a630-89a187dbbb95
4564b261-0b9f-48ce-8353-26f57f866dcc
a60a8020-bd8b-4247-80d7-44ab55ac248a
79ab362e-873c-4295-96ec-db201f9732b1
6c9c24b5-5757-4295-9bd0-1d47e66f35c7
e3b5fe8c-e23d-4bc5-9b13-233aa12e52d9
14824e1b-7d4b-4bcf-91ea-52b1d5222dff
a00098c9-4ff9-437b-9e1b-0695703517a2
ffb863d3-e8b8-4016-b969-181ccde1f772
c0f4fcc9-2244-4e93-b2a1-984ac9b89312
9b429f0c-1a8e-436e-87ae-6831aae43d16
b5563aad-2cb5-4667-82b7-89b570daf16a
4eddb41c-18e1-4e3c-928e-3002806affb6
18cf4863-fde0-4373-81ba-321d00669b9b
6c492021-b4cb-4183-a458-99e473392ea3
c7bb809a-7974-4c0c-86f2-515d89b00278
64463291-e766-4458-a59c-ae369af0d6d8
132d5338-acc6-4995-814e-321c7c6fb97c
e69b5e94-52c8-49e5-a223-58c82caad85c
33cce9e5-c661-496f-ae85-38bb32d4505d
95d3a9c1-fc7c-4fe2-a8a1-e67670b8bca5
e4146325-e58e-41d2-853e-42cd81a651fe
95ca965c-8828-4dfb-91fb-06684a17fe80
39120012-8e7f-4a72-81a8-4cfaf77e080b
f975a69e-ce32-44bb-8ab5-4bb774fd6211
f3601d06-733b-47cf-b076-48db7b5bd4b0
7e7b3821-d6dd-44cb-8dc2-437969deb755
171f139d-4759-48c2-9adb-24b43d465b1b
2fb9d4da-b473-4e93-8fc1-e01d375a61ad
692c432d-19c7-4d4b-b9ee-3119449df8c6
26a9571f-6670-4a72-bf6d-e687e130c534
853d04a5-a6cb-46b3-b21f-b4d1708f4794
4bc6ab28-f44b-4d89-bfb4-81c4aec01c3c
90b76f1c-c47e-4699-b466-1e5f869d0d42
96f278f7-cdd0-43a0-95c6-68d5abeebeab
bd4368d4-d8d0-4309-8fbe-f58bde5f365e
5126c319-a569-4dc0-b2d6-63fda61180de
62d1be06-d9d7-4979-b92e-6ce4591050b1
a27e2e7f-a27e-4351-b5ce-dc0ef8ec53ea
4b35f36a-6bf1-45f6-87a7-fce2e3f5dc23
cce267b6-a5f7-41f2-8cce-cacc003ba0bf
e41c620b-cf08-4dfe-98d3-d89f97ce38c5
ab856835-5ce1-4c7c-8f3b-9229eb55919a
1eb58bc1-5242-449b-82d2-d98ca3612b43
3a16c6c6-be1d-4025-bb6d-1749b1418661
cfbd0a03-ee7f-4d94-87a1-a9fdca69db51
c3dfba75-60b9-4631-ba62-b9729ccd9b97
c6c55288-fd1b-45b6-b013-01c4e4a867fc
8def03aa-e077-4ad3-9e55-b3b22546ea2a
ca977f00-d439-485e-b067-86f570ef2032
8cca9739-6052-4e21-84a3-a6882f95ffa4
e747875f-be88-462e-9dbb-49c2448c969a
9b7454fe-3dba-4aee-a7b1-64aa7333e338
18b06993-1b07-4c4b-b2b6-c3003f07e2bc
97ae9f7b-4cc7-4361-924e-08fb33945706
5edc75e6-ccc0-419b-bb91-3f0f95f1f50e
db9e785a-1904-4489-8c37-8a9488c7293a
1b93d32f-43ad-4a13-a459-b9f88290e9cb
e5a78264-c6ea-4d49-bc5b-28c18eb6629a
b27db2d1-be8e-4c06-8c99-c1f61d0b41b8
2c38f630-9b24-4a32-88db-dd9785a29780
89beac0b-6a1a-4855-92df-1b3e348613d7
dd27eef7-0f71-408a-add3-4f6a612264ee
b93955f4-dfb4-4f2e-9175-212e63146478
b707f4c2-0c33-4840-bae8-491634102df3
0059019c-15d7-4458-8a43-1c91c5d86823
17d89936-51c4-4759-8ed4-db26c435b854
42bd73b0-76e0-4725-b8d1-a05673499ef9
82643c31-62a5-4a43-8860-491c95025403
c9da2641-15c5-41fb-950c-acd5966f995e
4deaabb8-3435-4360-8392-4cd9b778d4c3
9eee4f4e-f6a3-4dc4-8913-376741858b01
2f0f035f-a0f0-4191-9737-91f35f7489bb
0b06ab9a-db6e-42ea-993d-72a2ead21040
04038d0e-78dc-45db-b3bd-463983baa003
2af6c78c-1a89-48ce-ae6c-dc9e7c5b6757
da1a9269-fc60-406e-b54f-80a0d87d3350
4e6ff67f-28cc-47f3-bc8d-205ce0fa7be5
cc02580b-0159-492f-a825-fad5891c943e
02805605-4d65-4a02-a933-915b9828c37b
20f159d4-2c73-4ce0-a358-7572de8794be
540343eb-063c-4380-8577-8f5ec65c8a0f
159371d3-51a4-49f0-b68b-5e89d559d418
a143c448-cfed-4e66-901e-3574b9e06eef
d88ba1b7-7e5d-4498-a486-207f0626f3c6
c96ddc80-b5c5-4ac8-8949-746b9c3e573d
b5b182f4-518c-4351-ad87-84baf404a51f
af4b1271-cca7-4f2e-b9aa-1ee2b249b0ce
71370d97-e601-473e-97e1-451705a1197e
8d402420-cc17-45ad-8d65-21c1ce8b1297
0887f892-0810-47a7-aa48-4af7b40232c0
2a76cf75-6aa9-4b02-b536-d34469b22c03
2f2e3b3f-6e0a-4f4b-8011-2ec2f6a7943d
8bb9473f-9c07-47e2-9401-dbadf5a76b2a
7bc07808-6aaa-49d4-b3f9-645bc868cfd5
df274a38-fb03-4169-9dcd-74c6306d8dbd
8ba455c6-2946-45be-8410-7bc840ae0bb9
c8d837c3-309f-459d-8c7a-69062903590c
04ecc339-37cd-403d-9d91-f31080cb5cec
935ad674-6d42-4fe5-88d4-443ae7a4626a
47945a96-f130-4f1c-a6b3-bf4ee38d45bc
b894cfdb-9bbb-44e3-b80a-4e347b14564f
48a36e32-8bbf-4a3b-b9c3-6aac14c9f6c9
8e08311e-d626-4cc8-812b-e4003699285e
c0fe5c45-26fa-40e9-bcc6-60554a0228ae
da746754-f2bb-4253-bce4-e4051d803d32
17871e92-6551-42b9-87f2-bb73c4b0415e
c429ca13-ef8d-4c26-bb18-f472d6d3c4af
4c8801ba-f17e-4e07-aeb9-6713e296700a
f0e7673a-c79c-4a7a-aa42-098541f66b9e
30fd07d3-a4d2-4b53-91f6-af02b27e710b
2fa09961-8913-46df-8128-2b4a7a0f51bf
fea83ee7-98ee-4785-862e-9c9fc9cdee7f
e713e54e-5d8a-42c8-ad09-1d20aa8feaa2
87c0fd12-c0cd-43ec-8dba-16dad0588dda
5be47b52-5d0d-444f-9cfe-967b3cb6de9a
d1d78fb2-f679-4b92-9e1a-f0e6bdd1cae0
1b06c5e4-4be2-4fe8-8d7c-e34dd85e0ccd
9afeabb8-348f-4f08-832c-60619ffd9359
73e1cbba-2271-44a0-92bc-ce142cfc17ba
70a2de76-bdd9-4624-8d97-a02e84dd47c1
56e46bfc-f0f0-42e7-9d9e-7c0556b6d913
046aa6a3-27eb-4303-9c1c-3cb4892ced47
bf139bb6-de65-43bb-8026-d332a8481db5
2bad9d8b-d603-4d37-b04d-a95cc95656a7
4bb9744b-3027-4f2b-9f8b-a0c9f52c49ac
05922aec-9a10-42d2-ba30-78d405da7cf2
e55e9150-0072-45cc-8c81-bd0ce2adb269
f0ac31b3-8fbf-48e8-bd5d-2feef0e56c48
2953a881-fb63-4757-a5ea-e856a07c29ae
06443333-f983-43bd-a43d-4a533e34661a
0fa8599a-fa44-485f-bde8-4ce6b502fa8b
0ccd8784-b3d5-4cd0-b6a6-8d3c6fc4a95b
243f9a74-8733-4419-8642-cbb541435377
3f2bdfc9-8be7-47b8-b63d-d83e8392901f
03941621-bb9e-4d01-888a-c6a4dcb2be34
e9e8a109-d444-4a97-ad8a-2b2c22063464
616d74b9-e5c5-4995-9d9e-55d8c71bb58a
ae25f2de-d4da-475e-b741-0be9cc663f17
967f048d-3929-421f-bd56-52d8dc721c95
3ff6f3a1-14e5-4368-8b27-0b0af54afd1e
5f54c9d7-bafe-4d18-9d9c-9002262d3f1d
decc8fb8-d71e-4856-b439-3e6dcc04ccd6
4f37adad-6523-4e42-983e-b74a9f8b1f1f
86e737ec-ecaa-4dab-96aa-a377405ba0b8
1e54d7d7-2e00-4f46-bf9d-ee94b124dab1
c0dcd6b2-9a6e-4f28-b688-0196b5d74d2c
913ee801-516c-43d7-b6eb-e0748c283c3c
2517e0cc-1790-4f0f-911f-27a1940f22ef
55e0e718-73e8-474c-9490-94b0bca0b7a4
b9356a25-3081-4d22-b972-c4e81441ed0b
f0a04375-a747-413e-a1c1-6cea434eca94
fea08e6c-f3ba-48a3-9696-d814b0306f76
d263198b-eced-4c0a-8f89-f84fd2157c18
bf550418-2612-4af7-8989-5462a94448c4
5692fdca-50f9-4bf7-8e86-fe021112cf9c
de910156-10e6-45fb-935a-0011923caeba
ddc19ac4-db7a-4ba2-b662-a61fc8e30bc8
81275c68-b33f-46c3-8a09-513c78c18e93
b40d5db6-1e60-4ddd-8c56-9aeec175e0ec
2b0658ca-cb02-44e3-9db5-82866b24b5a8
54f12ef3-9761-4671-aac5-609a62b59df1
6bce4cc2-9e4c-4510-bca0-db853c3763fb
a3183b28-affd-470e-a4be-48555d51094b
03c2f4b7-2ece-4366-a5b4-737214d90f09
e12f68af-c5f5-4295-8273-d1c20d54d181
e30a1353-884e-44f3-a9b7-dbb88417cf42
3a37c16c-21cd-46a7-845b-8f631c317f1e
4876a466-02d3-43bc-9f86-74e407129621
510463eb-eb0c-4721-9cdb-21e9d6be98d2
20999826-6b1f-466e-98b4-84a95fe97af4
f361c17c-5250-4a6e-a1fd-160d86d932b0
5b7e5bd8-946d-4a49-b8d0-28586624d2d1
ff96a954-67a4-4eb7-8846-d2e4713fd3f4
3e33bc6a-3295-4d15-af8e-d631a603754d
fc0fc459-12f8-4fa4-b01b-a56b55c7fd2a
be1de3b8-abdb-4ffd-9289-2a26e3013b01
3ef787b4-0961-4654-ae65-11bde0c2c3a7
0dcd3f78-cfce-4a8f-95ee-85d88e6193cd
93dfd520-d6f4-4c9f-9f3b-10341b816acb
69852520-a2ff-4d38-a08f-6c0bd3f8d536
90ebf578-792b-46b2-ae62-3fc762728fb9
2bde9408-3bd6-4020-9818-34c4b1d8d5d6
1f0e04db-2229-455b-838f-9261a1185330
4ab29b2d-6aba-4f8e-b366-b78eff66448f
c2a85da4-f934-4805-b9e4-100cfac11ba5
d13fea5c-aa0d-4355-8550-b6409ec6d933
47f7330a-a7bf-4b18-a625-efff09ebec2c
7eb97162-3b1e-49fe-9037-822988c72d59
f52a1229-5d5e-4eaa-87e6-fe9e801e86ad
b3acca05-5f45-48a1-9e02-bb1bda79136e
f3075319-6b22-4651-a602-e4af55977cb6
5607331b-b86a-4317-9b0c-82570a9c931c
5f463c46-7ffa-46da-94bc-2d54f4749a86
f03d4793-ab9d-43bb-a13a-8b72bdf967c3
3c812531-6f9c-44d2-a1a8-431e7780f703
c5d520eb-1c14-4206-b907-e3a4d9fca3ea
150c7325-ee2b-409b-9c1c-b758e861714a
4fa7b9d6-9138-4f97-bad4-adb216112928
8db8688a-8f18-4a75-8fb7-a68c9a7a3bab
fe025171-97ce-4072-92fc-fdbe63b50723
de68e5e1-489b-4efe-a53c-be87797394d1
b437be5e-9891-441b-9230-6b2b2af95eeb
0b547751-9dbc-4583-8455-55e181c310c0
27ffc3a7-0999-41e0-984f-4e95ccb6bcee
43d2ff5e-aae9-4eb1-81ef-0ac7fede0013
ee34c12c-c96a-4502-89fc-c82ecc50db6e
115756ba-9760-4042-855a-e32509555c01
d6f1658b-f652-407e-9e5a-8b6ca0d499d7
b9e0b92f-53d1-48bf-a817-4b754b5e1c8d
b7b9cab5-c056-4311-8431-dc6e7eafba95
53ec4e6e-d548-45a0-b001-d93f8dcd80ca
795e1a82-3da4-4480-9d38-e2790eb8bc23
d3c4be9e-443b-40e8-b03e-ce83e741e5d3
53cbc173-8314-48d1-9043-7fb9faebd338
5cc4392b-f7c0-47d0-8362-2177ba830b31
1e4f32d7-9587-473b-a870-c078df519b38
30f3c795-4727-489c-a2c6-322ffd82362e
6e22596f-b83b-4758-a207-9c914f0fd544
0efaaa53-a8e3-46b2-a185-79883e994902
7da41132-ce1e-457f-bf42-a42e095efb72
d3160c3b-8699-490b-9152-378892932f1b
95a874a5-fca2-4d1f-9255-2d9eed7a1f60
5f311b56-0739-43d0-a5ab-ea3b65131b5c
f9133f2e-52dd-49eb-a625-bc504c87e308
0d623e53-2e02-4c7e-bf65-cbd1efee01f1
5d713c81-854a-4baf-a556-5212b8f8c60c
35f429e6-8d88-4ca1-8a68-a75d84700729
ec7f8139-6dde-4252-b759-04ac38056182
3ccf8ff0-5db7-4aa3-aa05-5e0f04406deb
f08cba70-dfbc-493e-a9fb-18a8b3cdd56f
85b1c399-902f-42b3-bd6d-ab188168e99c
a86cf4dd-9819-46f8-8e2a-d94b476ef606
68e84339-5078-4508-a879-1521674960c1
23e40953-4f5e-4ebc-b7ff-bab563d889ef
4f89b313-3ebc-479a-8b24-d9805e24418c
e7974101-4ca5-4198-bd04-76e1282fe581
6977509f-bd32-4b4b-a85e-db429b921983
e62510ff-4a3c-4fdc-97b8-db480c62687a
1b204589-98e4-4696-aff3-437e303b8961
f4f31b61-52a7-4ba4-883d-ab9645c97024
49719bed-b760-418d-b40a-583e9cf20e77
80dc5590-92b8-48a2-ab85-ad26ea97d92c
f1bde3dd-b15e-4668-83b1-a0025ced2577
6c7f69ef-de1a-4976-ab13-c164c42b2c31
b01cf258-e9df-4b53-a0da-49c4c91d5431
6538101e-2000-41cc-8f0d-139f720d20e0
1e7d8db2-90fc-43b2-9388-bf821ec3cb6f
20e0164d-62bb-41f9-b141-ff8ef7aa1ffa
ec77c0a1-0812-44bb-8a79-82b3eb6011af
7680e27d-03b2-4feb-af31-29791b36fed9
0a7b1d60-f2de-4b6d-b79e-13bbabb0f3fd
ccf765ee-55fb-497f-aadd-5a01bb7c8d38
4d769168-9463-4eb1-9e47-019457d7091e
3ba118b8-6db7-4943-a9f5-5efd223f64f9
f412ea14-2a98-4d0a-b8dd-99c0be352423
2c243f91-f25f-4972-9201-de3db52f33a9
8d893df5-3a1e-475d-9a7f-174856dbcf3a
f3b55dba-d677-4757-8b3a-24c71c03de16
bfe64db2-85e1-4381-83ed-289669382698
52f0a058-86fe-4ff1-b5a1-af1aadfb28d5
c52473d0-31ff-49e1-b830-5e33c5053932
12a0aa17-71a5-49e7-a545-ccefc529fd62
503f0c40-3c24-4b81-ba2e-de745beeba5b
fb105e3c-20eb-4510-873a-b6b23f8cb480
74338f4b-bc29-4f8a-b8d9-551d656d45a8
9957bcb2-d27d-4c41-a1fa-95ebba96025e
3c2ae0cf-2b74-4c7e-8193-e877186f380f
2d283513-9a0f-4a30-b5c3-8c78e54ae741
dc57a59d-41fa-49d0-9724-15986d8cab55
e0b6674d-a5c3-4c41-a9a1-ca3c221c08f0
28ed0324-2a35-4bcb-83f9-fa75431a2062
c6b300cd-68e4-49bd-88b5-8a1fb728fbc3
a2be26c4-9068-46a0-8be9-803259316ed6
009f2e84-a479-4fb0-8556-dc16b5983c4d
3bdeabb2-33f6-4e40-91c7-1b6981ace9a9
cbe51fe9-3f44-4320-b0e0-16be56d1cff6
6e6cdbfc-f46b-42da-8d5b-0498c4f5fcfd
7866538f-ab52-424d-a35a-6880f642f85e
4fb1ba95-b59b-44e9-8e88-e5e0ade2b676
11804385-0a16-4d21-8cc9-3770cbe809ae
d8d06573-3d44-445e-97c1-9c8eb6b49044
bdc76735-68a9-49cf-8661-574b3be7a076
2f561521-4bd4-4bba-be32-731204017a33
594d7154-225e-4eb1-8e5d-eb57d17bf101
62a01de6-9e01-4960-9f01-854124255ad8
26c4af95-06b4-420b-a7e8-f9979c653e67
58b735e0-2437-4a06-a029-c95fe6cda5e3
4c9f7b07-6f2f-4ad5-b71a-1f54184fe098
d7137897-dbbf-4043-9c32-f8682834945b
941c5dd3-975c-4123-804e-d925c21ad21c
4724a15f-3650-40d0-a86d-5a05c37afb38
e82beea9-a8cb-408d-87ea-35c6a978694c
78ba1320-8791-4d3e-8481-0b5e04352910
4d7a4e97-b19d-4a78-b92c-1d865db42399
adb51a46-f552-4051-8365-790cbe8122a7
a6455702-c4f4-4d71-b2c2-c6c0d7fd43fe
5ee19a90-9789-4ce9-a2c6-d4772903ab40
1955c137-3c76-4261-b3cc-c32238b40f14
2bce8c0a-29b7-49b3-b494-673a19d7a620
b48f2204-451a-4722-bda2-41832f5f051f
184d29ca-6cc5-4a3b-91c5-88c785b803f1
266b1f0a-98d9-4718-a905-3c80919e633f
7990b983-fc76-405a-9a76-02abf80dd7d8
c2304192-6a12-4884-a226-27d32c768f01
8669141d-1a50-4716-b26b-7ae9075ba289
d8d3fb41-57c9-4c40-b269-75b292ac01b4
4223c625-dcf9-4803-9941-8956cd3afde0
09e326e5-cbe6-416b-bcd0-7e66f2f7b91d
54472878-219b-4b52-81c9-fc49aa6d4ca6
7ebd8188-fed0-4395-a539-4959c058ec6f
87b6cc92-c56f-47f0-9c70-0b7c30ff31ce
c187594c-8dae-47eb-917d-11dd11c38199
5e2770ad-d4ea-41e6-85f1-f6c12d9333cc
e2b3a234-eec2-4990-bc85-61e3e9787d0a
bb0c6ed9-4f9f-4a04-a9dc-900f1063c78b
cd79f21b-ffbd-4b38-99b3-80730a07943a
a9b52bce-bfd0-43e4-9db4-1d4952ad9880
64b310d3-4479-444e-8c52-f5797b90792c
434665dc-3658-4770-917b-fbed54e473d9
8c11b151-c72f-401d-86bf-6274cbce85dd
f8893431-baf4-43ac-abf0-50174d2158bc
7985e6c9-43bc-4457-9098-9f7bd940d715
afa0513f-5b65-4f10-ba52-1a35f94fdf0c
972aab21-c181-4ede-865c-f1593b34a92e
729fd450-c374-4025-807f-f8771d440ff4
bb30b6d0-f77b-4e26-a5a8-dfbec93cb1fb
c7fc9ac7-d3f2-4f71-9caa-be06678844f4
1a0ff5b4-cffd-4d14-b3b5-0af957173658
dbb6b35c-eea9-41c4-ac60-625a4aaa3361
793b6a7f-b5bc-43af-96e2-6f22536a9830
ad69dd7a-e6f6-4502-96cc-47ef2b72a4bd
b83bdcdc-4fd7-47a6-a590-a001e7e52d29
9ab7216b-88c6-46a3-aa49-9be9cfe55957
c6afa040-a505-42ad-a061-edd965a86417
2acabfe7-1aaf-44c1-a0d3-33a2a904872a
6db2a1ce-5d94-4288-ac58-51eebf4efff0
be22bc63-33b2-4abb-a448-3f70dff8fca9
3bf5dfab-e1ad-4a52-8665-542e9dfd098e
3f6c5eb6-f9ac-456f-90f2-103b21eee7d6
f78856eb-1856-4e92-9357-2d493e7ce56e
36490f05-de83-4b96-ae78-cb08700180d0
382bba81-6e94-47b8-9dd0-83754089b464
425f87fa-5e4f-4aa9-a4cb-e9fc59541edf
7acf03de-5e07-4453-93ad-ee770b862092
4dbd893e-9b58-45b1-9032-d2cd8859063c
db639b9e-6a35-48cd-bc27-c693e00fae39
5303644d-b2ba-4d06-bd34-9c6dadfd13f5
ddcfc198-6229-49b1-b8b7-b617b0c5a57d
0c1826d2-3b7f-42e3-9c55-c8243a97df36
8e81abb4-f10a-4512-b458-b812cf8a7f8a
94a5217a-1b13-4f82-b39f-36fd660559d8
43b27b4d-ea56-4e02-ad2d-3d6255232908
5443c554-278c-4110-8530-e8bd3332b16d
bcde7d24-5fa4-424d-9611-4a10397fe69f
3bc1c039-cedd-4581-a1c9-b0aa8e6559ac
aa69636a-89ec-4c6d-9d5a-35db0a2eb365
18d442e7-df2c-4059-9ff5-98dfd8f8603a
6a0ef444-59e3-48f5-a16f-19f59c3dda59
0d397a81-85fa-4025-bdfb-df415bdc027f
b112c2c2-3fb1-4a85-9a84-60b3c9fe88b2
2ce9f39b-ea9f-46a8-b76d-0d4e84582daf
cfa202a7-0aec-414f-b21c-ff5e224ec895
934518ba-16dc-4da4-9d12-b81c4d8e5f4e
767d2d06-c94a-4efc-9eab-3a75d1837dd2
09418001-0475-4ee8-9955-c029b120d962
cd43b143-7d02-45a2-ac84-975fd0118e29
e263e026-af8c-4a9d-b480-53a7850d82a5
ecbe6f8b-a71a-4984-b9f2-18cc64568f56
7f18148d-e389-4b84-9705-baa304c20b90
585cfe52-f86f-4f98-a2dc-68b2d921d487
f8754822-ab2e-474d-a152-5696281b520f
c3991da0-de82-488d-a4cd-787ea22f33a2
983604e1-4db8-4cf7-ba53-e1271975399b
6358beb1-5755-487e-be60-cbf501f52fd9
2cb09f90-7bab-48b4-acab-c8e444fc6de9
4ee1861a-827c-4658-9e65-9a90eff050b2
9496e6fb-fdba-4eac-9494-5c1c81699308
a2e40bb0-4cfe-4726-9874-a3f845b04269
aa269033-e563-47a9-aee7-61b79c49d716
31190076-e82e-4913-8a28-e45142d2d66c
87ba3a61-5dad-43d8-8ba3-7efded2a7fcd
0dcdc7ab-0fca-4422-a4bf-e99874b69425
49e3d1b0-d7e7-4757-93a6-e6a45faeff46
cba3c175-f856-49e9-a289-90a43c14877c
bd0fcfec-cd1d-4ff6-a975-1c42787ee736
c958b7b1-07b5-4d5d-a5f8-182798175484
70bd2aee-f3c0-4362-9070-015f632cb6d9
57f1bf3e-6f59-4dc7-b721-85f2fefca20f
bc4a47c7-ba69-4456-983f-4f67a0e76ecf
bcfad574-7bad-484e-b920-1c910530d3f7
6e455e47-6242-4656-9470-9d8ae32ea28b
8a1b4a25-72a1-4da5-b87c-1cfd2f0728bc
f230ecfd-3469-4724-9cbe-80473006ebcf
a9498a29-f58c-449e-8f0d-b4dd9693b1cf
9e5f668d-46c9-4196-998b-e27031db35de
0d62ebf9-c234-4071-bc0a-786badf2b053
0e3f5e04-3a62-40ad-8b0b-d8beb2d5035c
b028fd39-53c0-4d32-b89c-3aad49254d33
ad99ece2-e80b-45eb-b71e-0689799b6a70
8ab455f6-4f42-4a56-8f87-ade650fc885f
fa055aa8-d87a-4fd0-b010-b6883d35de27
022a7a6b-34fa-4acd-a23c-58dfb9c4f602
dbef6017-2c47-4fc0-ab24-88da447a65ee
38bbd7b9-404e-4151-95eb-a2ec7290fdde
acca15b1-c734-4bc4-b72d-27b6d9e761fa
f77b338e-d7a5-4f7a-b69d-b6f5a622caba
2a93b1ef-2ba1-42f8-9542-b94f7aeba088
da42968e-d49d-41b2-8697-671289870d10
059b22b3-d70c-456b-ae41-75d1fe8854fb
f6147b4e-d47a-4fa2-beaa-d0bc3432066b
5574fb55-0aae-44fa-91e8-22dae2c2fd39
b0a07fc3-4c80-4c81-a455-de9a0392659e
84a802cc-f45c-4908-b4c8-214f16899334
14d4fd89-e687-46d6-8045-c0575362d37e
71f97c1f-66cd-44c6-b633-c7fc7081fd61
7902a316-0eaf-4096-885d-b45d85269669
415159d5-990b-4de9-abfc-afcbcd29023e
e91c18f0-2ddf-4848-a4e5-da6428ecd4ac
c24ca568-1618-47ef-9a99-2bba778b9adf
87cc9661-a0e2-445a-a5a2-ec680e1bc700
5fb16568-07ac-4a0e-93f5-095b4708a596
3fb5f141-7d50-4324-8d70-7023a010ff1e
351da37a-da97-4053-93fe-7871262dbe93
7d7cafa1-0a23-47e1-a688-73f51a045544
b71bc59c-7b8c-4169-a4f5-83441cb646bc
527e4dcf-001f-430e-95da-dc4a5253bafd
2b3173e9-f29a-4f82-9f84-6f09774de83b
36137b2d-e9c1-4f02-8ba9-cc4c2c246be7
91b1fe5b-d34b-4dcc-84f9-a8bdb3e3bc21
4ec4e235-ce66-4656-91c5-3af379627f4f
c16e3218-356d-4fa6-a5a5-39e1728a5ade
30b5f76a-c268-4479-b0fa-01c6beaa7f53
571228a9-c189-4ed8-8030-394008ca3e3a
bb1f55cc-fa95-4552-8ff7-fdffd461619f
a5d11b32-6210-484b-9c81-827e70fc84e3
6193b331-806f-4e1a-ab16-c869eab8967e
2b61c89e-027a-4cf8-b275-7a11e2336d4a
1333979a-4dd3-4eb7-b33d-b7cbdc8154d0
5c6fbc15-b47d-4997-9318-5ec4380e6438
76a60dbb-8d27-4cf6-ba98-02770fed4ee8
8ffc215c-e9bb-43e9-b433-660df53c0259
b85332a7-203f-4960-9a1d-2a6b086b0704
445ea99c-2466-4d97-9fb5-e29d3c7bceee
4396bcce-6b70-4237-ba92-04363538aec2
63bfb63b-fe23-443e-842f-00d26c5e34f0
608d891f-ea2d-4ea3-b5d5-d9322e532bb9
cae3fa87-44ec-426f-9bb0-993d4b7305fd
f1964466-31a9-4b57-837e-8c340b5822e3
94b6a9b0-ccd1-4ecd-9248-fb3d2433753e
de74e30e-be70-4f24-9536-65853db84673
5d6d05ec-e1e0-4b90-95f4-62340221b5bc
8aacb8e4-fe71-4809-a45b-f6e9d36af13b
6074d471-c0cc-4e5e-a319-f37c63d1e1f2
904f0a41-d7a8-43ce-8b8e-dcd49eaef582
9e608abe-7586-4966-ab63-23d91c1573a3
67ed32db-9712-4b8f-9f2a-897ed84fba0a
914cdab7-d9a5-43c7-bfaa-3e9a11c3f54e
8908bded-407f-41be-b382-eabedfa4c6ce
2dd52de1-df08-4da3-bc9b-3a88bd2842be
c2d49d9b-bf5a-448d-9f87-c1129f4d8ac8
04635af7-8140-480d-afe2-7812005af171
b4767647-fa2f-4756-812b-c74ab48ff58a
22d5c6e8-e768-4553-b1ff-5eedad036ac1
d9f584c8-705b-47b8-84a5-baecf6ff4ace
06d6eb2b-76b7-4317-9671-0b99e56fe236
2a2cbeb6-5b66-4a83-926d-df205ee339fe
f513a8ca-402f-49ed-a853-ada2b125efd6
6f6f653a-baf9-42df-aa7e-7593356797ff
9d24fa73-eb37-4dae-be9a-811dc6698fb7
c7b798fa-12dc-4937-b771-eefd09bd471c
eff960f3-4d1c-4fca-8ef4-c08bda7be5ab
5ec514cb-b28d-4480-87a8-b9ac41ed01ff
4e4976ae-b3f9-4ea2-af1e-853d911f95f8
b1e1826c-3b5d-486b-9e5e-0ed1d8517222
60296e2f-2ab4-4f7d-b7ee-23465883540f
3090b7c9-0e36-418d-855e-01ca7b59c5c3
32e7b27d-8fa3-4cb9-940f-16d2949b0a17
18b24ada-0d44-4806-ab76-7fa9e8cb7442
c8864c65-f047-4b1b-a99e-d68d5c584419
02b691bd-4b38-454e-af01-f74eb573aa84
dd857310-9616-48ce-9a95-ee4c1374de00
eb342714-632e-4f6d-ae13-5ba72dee7c76
5a57f453-62a6-42a7-a2fd-703b0ef78e52
3eadcaa9-2e0f-4d8b-bbbe-ea7b8e913d61
053597d0-b40b-4e39-a168-344189ba34e2
cc9e5389-b970-4b4b-9263-e6f4bc895732
6f80bc8d-9437-4639-89a5-404cca1d08cf
51cf35ac-5645-4a62-b428-85bb5d424337
28028b0e-9643-486a-96ea-94147d0835ff
b99c828e-00ee-41f3-95c0-766f397ded55
7528feeb-cd45-41fb-b039-3c0f4f4fe11f
885b6dc2-5ea0-4b75-91c0-0afc5b403fe8
359c2398-8425-4053-81d6-35f9ea053198
8a666505-4e6d-4554-8a4b-d40980639138
1e3dd9cc-2300-4a45-9bbc-d28488c9260a
c8ae6f7e-5472-47a8-a424-6ca3ee0e8112
36a8458a-4871-4bbc-b5ba-f0898654e72e
ce2e8d2d-5f93-47ef-90bd-cb087ef6be7d
2bf38417-87e3-4199-a23b-626c11f75795
a4bf3461-fcd6-4c1c-a517-ad577d668583
953f5977-a447-4ff7-97d8-cc46de2a67d7
f346593e-b251-469a-ba42-649f22d3ebec
c7346eb1-c47f-45fc-8d52-7b41fae262aa
339a18dc-dfe0-418e-ba6d-f0a2dd631a8b
b4b0fc01-44b0-4a97-ad6c-d9094e10fcc6
08719539-79d2-4e75-ba52-8d6439055b7d
d602ec46-ab9c-4a1d-8ecf-c761ed14e787
6494a12d-33e0-4066-91b4-be02e0c9748f
83784bf5-67dc-445c-bd07-d5a54ed54a57
faefbe7a-cbb9-49ca-ac61-a6d3e0fce940
0d79306f-84b3-45cd-aee6-06c897a825fd
b30444fe-ffb7-4993-ac73-e93a802d5026
f42b5d3d-a540-4c79-b33a-feb107e91e2e
7252526a-4d3f-4472-8f87-5cd0d3c05b1f
2dbda36d-a3f3-41ff-9878-4e28292b3f07
19f958dc-5c70-4d1a-93e1-6291cf5a09e8
9f54648f-9221-4600-8641-a94dadc1d228
6b5ffdd1-c19e-4e44-9d84-ee816971c3ba
1d30e3b0-45f3-4533-a7be-1982e0f5ad05
a167f1e1-e78d-4492-9450-308a76155a3e
079c80ad-2cfc-4ccc-a02b-4e8f1bf814e5
11bee255-c17c-414a-8d7e-b4ca6afc2214
ccc78d7a-017e-4bc6-9278-d46a921a9bd6
ee7339da-d912-4c5b-a216-41f1a595a818
efe6f367-377f-4cd6-a52d-75c1a4ed1fbb
1c191efb-30eb-4735-8d01-55c31b518d20
36f10acd-8dfb-4d93-b0cc-d87a556a75e2
e65b1a6c-ebbe-4f66-8937-e4306de7d767
47b5bf7d-b296-4b38-93e0-25e156a0d801
1c2e856b-753d-4601-89c1-e471641b6965
e002d174-8032-441b-9647-fe72dee818ba
c8c3d48e-502a-4030-9eb3-b78deff858cf
36531c2e-0e0d-4633-82fa-d1528bd79982
7362ef38-1928-4bd6-899b-e1f74bacd4d3
34493d51-d88d-49e3-8071-7be302ba30a0
3007175b-ad5f-4a86-89cf-3703d0e73bce
94e42741-ca18-464d-85eb-8156bdad1e63
b0809297-fff5-417f-a5ed-dbc01fb68c7e
fd28f5d2-d59a-4a6c-803a-9a410608a8b9
e713c3f7-d329-4a7f-bbb9-605d9759f4c4
ff36602c-2808-4d53-9858-0ee6790a0ba8
5c7adb5b-8856-45ed-b281-b4d3cc7b3a96
cbaa5d5c-70f1-44f3-b6cf-418b29f1ca3f
148eac3b-8f05-445d-916c-c453159b82b0
4634fcbf-133e-4b72-9de1-f497e1fa820f
57cb7cef-e7b9-4238-ac75-deb821af1c7b
2c236f2b-7d49-4c33-a1fc-983e88707f6d
8c6bd4af-0f15-426e-b172-c583d7df14fb
34b88655-eb64-4211-b3dd-0152b31eb0f5
0976db4e-eaea-419f-9fc0-b67ec2a17142
266a5919-be1b-4648-83e7-8d0a5c163469
9d90294a-06ad-471c-a94d-b621d5d355f1
4dd260e9-9562-4065-9573-51c0d51da8be
3356de07-5db8-4053-8d50-199567a5a9fe
8abe4be0-31b9-4732-bc88-a98d00cd6c07
6ec1e918-b05b-47bb-8dff-7530edabeff3
206edefa-04e0-450c-9705-5d18b75d84f4
b8e68973-f710-4d16-8342-db8364131a07
a7babaaa-6dbb-4c66-8e9c-b75f1a96e9fd
d900a005-4d47-40e6-bdef-8109dc07cadd
51620e79-a7d5-4e09-9b57-0b96afe94f11
3d13235e-0f11-4ee1-8598-63a8daeb5080
5123c743-9a30-4f1b-80b8-6eeb70bcb1db
ae8c4587-ac1b-48eb-bd5c-8cc63c58e01a
51f071a8-e030-4069-bf96-f437b85314b8
21a90ea9-f95e-4229-8e0d-a1e2c47405fb
ab6d148a-3f43-4a62-9188-7b420612272d
17132fc7-ee4b-477c-a0cd-ea5efcd9c1ec
9ae3a6fb-04b9-4949-94d1-8e6e606e432c
4defc375-e78b-484c-b610-f4150cc057b9
e5e46025-c3c3-48d4-ad83-4f572f94ca7e
77012b5f-60e2-4abd-8a87-9d220bd8c70a
cb5d0a83-f582-44b9-8ab7-80e88475f0df
6903029d-e2bf-4bf7-931c-b56390663a05
1de31e56-d32e-4a3c-b3d1-914b7b8e4833
7ca6dc4c-906c-42b0-80c5-e856813c65d1
b81d51e4-58ad-4f42-a272-255909b3ab56
36711cd6-ad66-444f-b913-7c8c933a7cdb
d34b101e-500d-4b1b-b853-f91043098439
b7d25808-b383-43a9-9fb1-5b01baf36ae8
9456e245-b02e-4cf5-9cdc-03520e610754
eacd3023-b960-4e9c-aa1c-badb09ba1501
dcb62fa7-46f8-4dde-89c0-adebfdbd017f
9508796c-79db-4149-a121-d16670f97358
df0b87ec-3daa-4ce7-b13f-b09ad39e2a98
4a427133-e361-49a3-af38-3a3b090dbefe
4a1563ad-56ae-4b1f-a237-6be89e5882bf
efc20e2d-a8b7-4aab-9611-a18ac73453ff
50a3f370-4b85-442f-8b06-d52b3a3f80f0
7f93c560-9706-4eb5-b8db-43c329c69124
6e11d1b6-33e8-49d5-bbbf-b6bda3daa836
bbc43b36-9c8d-4bb2-99ba-72d82a13c968
bd7fdcdc-bf1f-46c7-9e8c-cc880f26432d
9e65f4f6-824f-4ee8-b237-35cdf330ed10
8feba185-2848-4a84-a5b4-08ddbe140c6d
0a82456c-3fa8-4791-909e-22b7132384e4
7ab5eb02-6697-4d3f-ba17-f7bc88b7128c
b918eba1-ae91-444d-9885-ef811825fa1c
83b74dee-4973-42ac-9e1d-894d884bfcbc
b11755b4-6601-4400-8ded-4b7085446154
aa1d2dfe-82b2-4017-88bf-a74d38041b34
85dbca8c-35d3-4a52-95ed-890a4f0c88ed
ddb4bb00-fe53-4943-a954-4f92ce48514d
4ddb6aee-5192-4049-bac8-109d2e1732be
7b9b073e-76eb-4bc4-b0c4-587f3985564b
c425093c-f6f3-47c9-91c6-d540c807228b
7c319871-b83b-4f5e-9f73-2ac2c5f42a3f
b2df018b-b305-45cf-bf02-04ac3b691877
5faa9eb3-3268-41be-9f86-9ec3727d8a6e
7cf3bedb-4357-42ed-84b3-3641720f4809
7d6da959-0bf1-48b5-a327-63b28626fda6
d73e8881-c512-4d03-a09f-74cb34b1f79b
242b88a0-6eff-43b1-89cb-8a3531fc9e52
c041fc30-512d-49dc-bbf3-8170cc8729cb
3f202fd6-da2b-4409-bcdb-15935c92a424
fd01d3e8-c204-409b-8817-f0cb5b678202
de39f393-c050-4481-baaf-334c58eb2e57
07dbafe3-8e35-4c16-aa42-99f6d9c46a7a
c316bbe5-23aa-4ee4-bdd2-e2f729f2f3a4
0e7652f0-2906-4ee8-b871-781f50910e20
206f5634-ea5b-4f69-8dc0-fe4a986f9708
b37ad04a-7fdd-416d-8661-9f0d8132900e
59f428a6-ebdf-4c43-bbd6-d8bef6f65c7c
b1c315c8-8538-48b7-b0cf-3b262e31e989
3284996a-de42-4609-9861-3250a0927c11
ce05a064-11d9-4327-8ab9-4caff3c2a8e6
2ad8caf9-eb3c-4eea-8520-84824473a466
62a92f38-01e8-4a85-a2af-13b4e6720ad0
6f26dc41-b8aa-4673-a478-2363bb6d1a04
67a3d775-67de-4f0a-b816-6c90c58c1f0b
e1dec79e-fef1-4e99-8b8a-3e6b71d52335
64b12730-5f53-4541-aba0-87839093ff44
5e50b981-a410-4c9b-8440-04c7a111a681
77ae3728-eab7-4942-990e-6c1bb87ddd9c
47b9d01c-bfe7-4823-8da7-cb79701b0366
8c7fdf35-bb34-4bdd-9a17-0307be6d4e34
1988b2a4-6d9c-4a16-b52b-5a30acec98cf
8b3a6afa-9223-42d2-98a6-0cbbca7c006d
7eb9e464-dfb3-4072-8f8d-18dd8f0a1f5b
131a2547-0098-4681-a1ba-3f36bb533a85
a394e815-78a0-4ff4-80da-669a8ca02d39
de16db37-57b9-4cae-85d1-4221d5cb442f
2b1262b0-e2db-479b-b63d-3c50b97f34ee
8d9cba28-268a-4cb2-aa49-bb91c55d81c5
961d6209-ad52-4a0b-9dfb-f0db697d7eee
ea60d7bc-3e83-4236-81a4-b379c5f05139
f2c99fab-8715-475b-949f-7ba665a4054a
52032e5f-624a-4e3b-b1a5-4ea6234fc6de
f01ff2bc-fbed-482a-8c05-5a757303dbb7
cbb34736-225b-4959-a16d-03eca47f274f
e28e20db-a070-4352-964c-55b7fbc2fa20
d6e2bb31-e373-4795-b649-20beb8e4f085
d211db56-6030-4038-bad0-d582da847a93
9e54f7e2-1f7f-42ea-b82d-7e4714b1d6ad
383eb057-d759-42d2-9687-fb0471e0ed15
6f1938d4-38a2-4ffd-b9d1-5c3ac41d6819
0f74d259-e30e-4255-b591-2efc5912ab87
1293e8ca-391b-46aa-a8f4-6f0fa5006510
2f90c122-efa2-4102-b7ad-74307c7a7b83
5f1e2298-567d-467e-992f-52a040fc8ca3
ab7762c5-3167-4c6d-9f8a-9b5b06890069
19691f29-88db-41f2-b316-751fca4cc05a
e823548e-69c3-480b-b335-fc8fc437e6bd
1d865ae9-557b-43a8-8a0e-e361afed137b
1eedda23-0e1e-40ea-88cf-e9299e89a37b
cbee620c-43b5-4909-be3a-e0367bf63760
dda6c2e2-b19e-4d38-bb13-06a3cedeff27
21087b24-184b-44e3-b482-2bfb07675209
01787b4f-3f42-4363-ae06-151a26e97b09
2cf2e629-b50d-44f2-9ae0-e5ff16ac79be
cc86bfbf-c2c8-4db4-bf03-304e12ea9b13
93a3864a-abd5-4ae2-bec4-8d1a00e0ebfe
7e8fb0c8-8a7d-41f7-a5aa-059e0165401c
0aaf9442-ee2a-46f3-9c11-8a585a1fa143
1748ec44-5435-4240-b1d2-beba6606cefd
d7200326-2e8c-4b3b-b0a4-8ff273faabe1
4fbcc155-8d61-4765-9789-45998ee04f2a
7edaf651-f3ce-4554-a815-579d8a4c345e
0dc8b544-63c6-48e2-ba46-e43a5762851b
9952b790-3ecb-4f15-b078-4ab2dfc98343
8561178b-5227-404d-ba39-98a727d62112
97356ef1-c1ff-4abf-bb0d-792fe7d5f508
e32635ff-2d26-4d6c-b66f-a6152b674b50
b2c0d90e-a404-4e7e-91c9-6d078ae4b4db
0b335246-8ec6-41b3-8885-c88c5ea2e23f
7c59e753-8724-4fc6-b987-a731bfb59bb5
725b2e6a-8223-497d-8510-f79029458acc
f36f4de4-cb5b-4783-9326-958d7176b26a
1d406d6a-34ab-4f02-a213-e35c7950d60c
d84298ef-3930-4f78-8e3a-6ff869f36fc8
d4646589-e1cb-4958-a9c2-6b2a7d2d3261
c15b6ce7-fd5e-4891-a911-12db849db73c
e07d9174-0edf-4376-a9c0-10dee7f1aa33
d6dc50b5-2742-43ee-bdb5-c3b093635198
679e663c-c509-46b2-8f1a-5db22a219de5
6a815121-55c8-46e7-b93b-d2aac82f3166
787d04a8-9e51-41ee-b345-01f748c555bc
8b4f48be-b888-4d4d-ab3f-857a8d026d56
ec4fc913-164f-4f81-9b99-68ec5e535df0
a626cb6d-c4ad-4550-989a-65336af7335e
5428d880-9bda-4eec-9362-737d8271514c
b57e6d2c-5a15-446f-bb36-ac0c9bca3735
3e3f16a2-8365-4e92-b0f8-7f950cd5e736
f8a34125-5de0-43fd-b3c1-77feac12bf38
728c9f92-6b65-446b-a995-86d26817d1a3
3a6860ad-9c53-4710-8406-07548f2141bd
008bba91-fb30-496a-b675-38087b6cdcc5
a129ce43-415a-4dd8-b706-b14413d98a20
776059dd-7ce9-413b-a911-4e14c8309112
24c987d1-4a68-4b61-95b6-ae94a3c8d20d
5c10169a-1287-411a-b1b2-3303c3f4936e
d4632cd8-72e0-4f55-9070-fb93ee9f7089
e5d90c4b-5942-48da-a362-b9ac918755c1
a7e6929e-ec64-4f4e-9dda-69383b952866
d51aff23-ec67-4972-a95a-1f1bbbcd5397
61300287-ac46-403e-be12-f5e436e85d74
d481f3c1-6e15-4501-b3a0-3b6dd1f675f9
18702247-45cc-4bc4-9bac-c79f2fb80445
599c9f58-6c34-4c0e-b6bc-467a726c38d9
8a2b6073-1737-4920-bc1f-963f697bb584
3d31e7ba-0816-4fde-8b6f-8719be95527b
1fbda870-8935-4054-8844-4b3cb3198ed3
65a9df35-45e1-4283-a269-39afe56afdbe
3066d8f3-462c-4223-8c7a-2ac031ac4d8e
aa21ac89-3d35-4e86-ae18-1b57df40124c
629d0851-0084-49c6-864b-f443f8d0d127
1a20e76a-7252-4ed7-a75f-aff723ec1ce5
9610c57c-7776-4b05-a506-c7b11c10cfc9
2f0c167e-a13e-4d58-930d-83ebadb9feaf
e7ba152d-8c59-4dbc-8197-2e5decf375df
18a7aae5-5ada-462c-b871-b84ce6aa1890
32abec7c-733d-4c95-821d-1effddbb27b6
bb2b7845-6682-4979-a92e-dea5df93622b
453fecfd-2898-4f89-af96-4a69ac286d21
75b8442a-4832-4c69-9ff1-a7d743fac135
ce594815-ac95-4bdc-9728-43485f29814b
8695a0ce-aef1-4463-91b3-f53b6fbb8533
f8308146-0770-481f-9ac7-a5fd64802728
f51155e1-13bb-482d-bea6-372d82699dc5
41927cee-1641-4155-bae7-0099a3b54b48
c7be6fd3-3dd4-47d2-a1a7-f09618d95b46
8e71e198-cc67-4397-9672-f0ab2e5cac1b
d3f93c71-f4b8-4e35-ab87-191bfa5ac99a
959701d6-424c-4ec1-b90a-42555cb666f1
7935c7cc-8486-41f4-b68e-8752165e87a2
c241ea38-8146-4f3a-8e33-000676a19d46
479a93fd-cfef-498d-b224-9176be3e16d8
8da84d87-81d0-4bf8-891b-cd9f5a0f138e
37f905c1-f5aa-42ed-a07d-43add88f7934
ccfee221-7fe3-404e-a5fe-c45c84ae88d0
e791c394-e3d8-4d70-aeab-3734f2ad3a81
0ae31935-06b6-4c6e-85b8-e92b01279178
21081cf2-84f7-4c4a-b66e-ce2a0da4f249
26ff16f9-8470-4448-bc83-93d96e01024c
c16be32e-b30e-4463-b6e3-417defa5193d
9f20162e-115a-4f19-a2f0-f96d477aa55b
18a489c4-5a0c-4b9d-b0e3-2a63975d51c9
7fec62e4-2b6d-4953-ba5d-895dd413193f
0e606559-95ec-47e9-a7fa-85aeba4e1354
0f14320c-a96c-4b8c-82b5-3927d911b8e4
18b97b0a-ea03-4a86-8c68-b0438d9f84b8
430eedf9-06e0-4443-9b19-8bad2b9896b2
b6f37158-4bd3-4502-b5a5-765b2657f0fa
dd0dcd75-5e65-4d58-80af-eba09ef478a3
d7ce537b-3feb-4936-95ee-18ac20f8bfb7
fc95717b-80d6-4b7f-9c45-3eac8add4fa5
07e52d1b-203f-4db1-a2a6-d7976b92c007
bc3638e5-c588-416c-979b-e5ed5987a714
6d14b3c2-43c5-4fc6-9944-40843c9913c7
5f029a39-f78e-4dcc-b21c-a2ee755bcf39
bc6367fb-417f-463d-b560-0499101174b7
71d8d619-af95-4c7a-ba92-c6a29df3f297
226d661d-31e7-4644-ad00-cee1f9a8b815
9ba1d3b5-14fe-4930-a08f-25d6991eb062
5a895f33-2f08-4c14-9d18-98e2e7ad3c31
782a57b9-2521-43c0-96e0-3926f30bebd9
408d4f7c-d443-4f7b-806c-f17640b91d81
a44b65e4-946d-4e60-b890-d06310f40db5
99151aff-69a4-4364-8ab1-4425abc34afe
b3da656f-d352-4322-85b6-402f91b02e9b
3a09e1a0-65e6-4645-b217-4b9525a38e77
c469d667-5de5-44aa-a35a-3a229963784e
ca0f1d6c-35de-48d2-8da7-085445f8c40c
3f35c2da-7d14-4239-aac6-d2e6650a4f8d
fbbf3d9f-a3e2-4e2b-a380-2c7c6baeae94
b037faa5-4a81-44f6-b904-04eef5e7adb7
d40384e7-0264-4221-9829-18359a867411
b971984a-4249-4ea5-9240-e5ca97836b0b
9e330735-bb14-415f-9380-b0a420430eb2
4e81ebae-69f1-4e46-b1b3-0ff47184b93c
698b5133-0074-4c68-a811-a0e857fc5747
72225299-8046-4800-9835-0eef93e75211
ee35ce39-42de-467c-bc28-03afdbe0b3c7
35513b61-10fa-4078-8ff3-f466a3a59910
bddf1b31-35cf-4b10-bc0c-04cf188b9421
490a6ef9-a4ce-40e4-b57b-b852ab83a439
e1779264-9489-4275-b226-544fe2c53dd0
d69f7f78-de60-49ca-a045-e1a9affb8a3f
445760e8-4136-4922-9e64-8ba4f0191b79
7f59da44-9193-49f0-8887-f02132b798c4
f5190eef-d74d-4137-afd7-a5615a70126c
697afa71-e07c-46d8-b687-5e53731738a8
3f3b0df1-d523-47fc-a7de-1e77a0581b11
01171e29-1316-41e1-8bdc-401becc89366
bb236fa8-327d-480c-8840-cc2932b8c468
684b4040-5ed5-4b52-a5be-51748d35b727
af98a627-d508-498e-b759-d023fbce5efd
28935c46-c011-4053-9097-03fba04e7aca
c8f1159b-e4cb-41cb-8f44-4eabf8831171
93e3aca9-e942-4928-8aeb-713d2ddb9a48
93ff18e8-e912-4aa8-88f7-908a552085c0
1bd875f5-5f38-436c-849d-85ba2af6a33d
0deb1f18-2bc1-484b-9ed5-dca299c4eccc
21b5ad31-a2ed-4428-bffa-de800c076e69
ac7665e0-d618-4b23-8b7b-743ca2f70342
1ebe55e8-8ac1-44bf-aa20-48d97cf7ea27
ea171ea3-9ab2-4cc4-9c36-406ac39bff6f
1db1f59d-7685-4da6-946e-180d8380f8c0
bcb4f239-3f65-4f5c-8d80-f3a2d9d3d0a0
36efec76-3c51-41a1-b632-9cd2daab5602
7a3aa4f5-c001-4563-a357-c2a57ddb66d2
94639825-de88-473b-89b0-9efa965e0400
effad060-7511-4091-96b2-43e59bc7e272
95b62665-b8e1-4979-a562-536e3538f5cd
08d2e475-43dc-4171-99d7-3ab615677e49
c35f5ad8-49eb-4072-b83b-cfee8c7bcec1
0f7769a2-9034-4001-868d-418654811d89
cab20836-beeb-4b1a-913c-164997dd16a2
42530ea5-d478-4292-91ac-3c5f4f4fe27b
268d8c89-4254-4869-baba-f58189be9d80
36117c3d-2a62-4981-a302-87b1b380ebf7
2f0943e1-1872-4a32-b4c3-6c7d221a981a
9e0346f7-7ffa-4ade-a552-22e1f0da99cd
5639b876-405d-4310-9940-7d1ffdc16984
b141a9e4-e655-426f-8b4e-a9f6b2984ff1
85205548-9847-47d2-b5ab-68611c0c4e20
6dad694c-119f-4182-bee6-98775836d030
ea2a23f8-a2a8-4385-8ec7-acaf14edcfd7
980e14db-92f2-458e-a874-c6580651e0f2
a59631bc-0124-469f-aea5-e8e57ed7cf91
77b967a3-7a88-4541-91a8-a78ea7939e93
87a978db-bc51-4d6f-8d27-e423592fa503
b6432470-2cc5-4f23-adb8-ad1b5f3e96da
619a0d49-23db-4d4a-a187-cb39242bcc41
f5d97a2d-cd6c-4c39-8ab7-3400c39ce484
8adc6cf2-c813-440c-8fff-f45dd00d20cd
a0a3be4c-b05f-4801-b1fb-50356f335d7b
e4ed07a5-7fd3-48d5-9e7d-0b15481c370e
267c712d-3fc7-4dfe-82d9-22d80a6c7811
b2f9e2c2-bc8a-44f3-bb1e-4784c8cabb3a
e6ad2890-e2bb-491a-a671-618e6742d9dc
e74a3d4e-9000-48de-b330-b47987f6babd
85e27a92-e5f9-4891-b645-659d3106f9a8
08ffe72e-0ab7-4b0b-abc5-a3bce58267b3
f1771c0b-f8b5-46ff-9b3d-f54d115fabc6
e03f0a48-4d50-46fa-a0b7-bbdb8d511fa5
cd0eb219-53ee-44e2-ac99-25c8dd225fb8
14efb562-cf34-490d-9be2-92b92f25c770
a9e58919-ab31-43c2-9255-2ae71d75c889
854c274d-98e0-46dc-8445-48c939c0f53b
1857bb6b-b041-4959-802b-48573a2b9367
8827def7-1b4d-4057-9e1c-b6d6f61877f8
1283456b-69e2-43a8-a027-906b8aa5ff04
36201aeb-515b-412e-ae33-7ff21e46fb06
0a712682-b3cd-46da-afa3-907f72816819
7138374f-f72b-44a9-baa4-0936a4104be9
d463d0b6-ddce-4fde-93a3-309f92057431
f1eb1fc5-337f-47a9-941c-00ebae6829d6
61c5eff7-b30a-44b6-b6ed-9d4ae8370682
c231846d-8cf1-4f70-b43f-ba0ce8fd1edf
ad61b93c-99ca-4f27-b1bb-d6cbfec732ba
1655235e-fe2e-475c-8ac2-0f09778adcf5
3bb1ea0e-f048-4572-b3e6-e7ad9925cc0e
19bb5c00-a89a-483a-8a10-ed30c0d81381
762e4012-e287-4983-8a5c-c4dafc8c3ac7
0e87cf2f-c44d-4dc2-b7c6-4ead9728cd3b
741f9ae0-2907-4465-b714-0f9e440049d2
97adf156-a6cd-4778-afa7-6308c09ebe68
4bfc4dd2-4070-4fe9-821b-015fec5110ea
cbc45596-9e82-4783-b61f-cf50ccc43038
4914c263-7725-4edf-9f75-2db04f7dba24
1cab36be-f6aa-4b5d-9afc-0d05d2fb73be
b78cd879-50d5-4161-95da-807407ad8916
c9877481-8e85-4c36-a0e0-f25441923464
a192111e-9f73-494c-aa11-a6f7ed4a3621
ef79d4a6-ca0f-45b4-8530-4653ae40a72b
0f2da5d4-5123-425b-8de9-75f87ff1ef6d
9bcd5b76-c18c-4665-b433-3da9aa3c6391
b64faad4-80bc-4797-8907-674834edd821
1bb69e81-3aff-4a69-b821-51de76f4b28f
e315c70a-d880-424f-80f5-6d297ceadfb8
dfdd69d3-69c9-4852-bccf-46fa9dc19181
cb97afbb-d43f-4dd2-9574-48b10469ae28
d3d78458-0cdc-4937-aadc-31dd84d1a350
3a10567c-113d-47a5-91f8-202c1d1ca22f
d9b7f707-24a0-49b1-9f6b-48d700b8d453
ea54a26d-0539-436e-a725-6a25ef71d69f
26d6ea3c-68b9-4bcd-b713-0fee0b878b10
e2ffe0f1-c72a-4c04-a7dc-f0b6fc879af8
b3423aa9-423d-4021-9daf-0a51ed81057f
ee34df6d-1641-4ea5-957d-bee17f46104b
49234d16-0659-4c73-9e75-45946c76adcd
8bc62cf1-6ff4-4950-90d0-e06295856d59
64cf279a-1fc4-4e8b-b095-7d7651c8b993
f2b66316-7229-4c57-a736-9fa453b17120
613d233c-5b5b-40bc-bdb5-7602b93fa833
6bb3b248-6e67-48eb-b51b-355e85e138ad
2fc4a65a-579b-41e2-9cc4-e3dfbe100ad8
36188c8d-13b2-42b3-beec-6f82453b63ae
65a71eeb-58f6-43ab-8547-d55322bcfaa0
a1177329-b168-438e-ad24-8d706c3a082f
8dda1490-6a49-4774-abcd-67253427682b
4fd85874-0824-4f87-a8b6-132eac818e1c
b5bc9c2c-4b4c-4edd-8a75-198033e0dc9e
1a90325e-d1c0-43a7-95ce-506fc42c6048
bae0ee99-dd44-49d1-aa62-f02991e966da
2192e252-dd40-4c79-bb72-805ad5494ec5
6a60b8f0-f630-44a4-af2a-f0dd6b71f9eb
1b20efb4-a591-45b4-ae80-de910f9337a1
3e6e29aa-89d3-468e-91a9-7c7b46becaa5
dbfda996-dbcf-49ec-9f36-9316bb5b417b
7b90b0d2-5927-4daf-a32e-c946119937b2
66a8886f-b64f-4afd-bf9f-4c7cb4f77096
881fe70f-4d9a-4e54-a8a4-e2dddff5158c
f65870a2-9e5e-49fe-849a-31c0c076ffa1
2ea587d2-054f-4f73-a142-4e124e5b3125
33f7260a-7d9d-47f4-b6bc-322f89633872
9e66a571-0718-4b33-92cf-aeea88e5fccd
8f3f46c2-6f47-46e5-8deb-66aefe60d72f
3b2af254-4b4d-40d4-9b16-cb74603c944c
6256b3c7-1152-402d-8407-0eee5fbcf313
1987fb2f-6774-42f0-9acc-68bf892638a6
ce751869-4596-434a-909e-69fa37af1e83
9be8e700-7523-4747-81bc-567afbfa348e
6b9fe80b-6296-4a29-bb8e-1b54dc33acea
62662a91-c11c-412d-be4d-6befaf08b08d
26db67a4-2370-4e99-b69c-373084cf670c
d812a2c6-98cb-47cc-ae3e-b693f7a7bcfd
ba8229a6-e13e-4ece-83b7-3ad76d24426e
f5c89d8e-d2b6-4e54-8fa5-ffa7dc0fdabc
dc584b4f-c37b-4d09-bbcd-b61fe475db3a
93dc8e1d-b5e2-458d-b0fb-f5f78f2b0605
b93deffc-b628-44f8-9114-7d8d9ac9d876
123a39ff-c948-4279-84de-cd6c6ee697a4
ba47508e-ca10-421f-ae08-599ff96eade5
2a33f14d-f597-4c4b-b129-bac8e128ab84
466f6235-8180-411a-9de7-a416123b659e
84572c70-f497-4c9e-9ab3-68e71500e53f
02cc33e3-9aab-4fbb-bc37-2e767101d0e4
084ec0f0-6c93-430b-9335-9b1671462c29
138b52fa-7f54-457c-84bf-d34178f069ca
c995f0c6-042c-4148-b04f-a8c7e9963b96
634ceb72-8028-4f4e-b25b-e1729169fdf8
140b9260-c0a4-42aa-871b-63e1f04a74b1
9f763962-7b26-40c6-b495-845ad67b0d74
811d6b99-d664-4630-aea9-195f5092ae4e
c9aa4524-2ccd-42c4-b736-0074c2d9cb80
67b36314-d88d-4d28-9f44-a78b2a60b537
4d1a2c58-6b0a-4e2b-a13d-33d8aa89c9e1
2af7c301-8b3e-463b-979d-9fc5a4d58d33
fbf4e4f6-2f78-4339-9c72-a17984a3a004
719347d5-c5cb-4a0f-b922-982d5a2d0855
e20e2761-cc1c-4b44-ae8a-7963c6cfed0e
5b2482dc-931d-4219-8b6c-8921e91b0595
303ab659-8402-4a2b-8ce5-7c23d52d2d2d
6b567e07-8011-42c0-a256-90cd5a32e3d8
cc8164e3-d9b0-4b61-9cf5-cb816633e78d
748e3792-ef44-46d8-b5cf-e1d0c4b07db6
efac8516-b3c4-4b06-b554-bbbe89b51e04
d066257a-7db2-4ecf-ae9e-83a84d98ed2b
93ec760e-4b7f-4a86-93db-8fc557ff5291
f3cf37c8-0999-4689-8607-86ca54e19821
005ff250-b7c3-4df6-9c15-51e4fa187c62
a4441f49-ad37-44ca-8f9e-3ca7459d06e8
e83dbd60-7b23-4342-9750-e6eda0a67c32
41c1e81d-991d-4356-a411-d7d1fbaa1784
848002ae-87c1-4bb2-951e-910a53bceae2
55a67ec0-c741-43ea-9e18-f38153631726
41a14f96-edb3-4065-9a7c-88aa605a68a5
76556df4-de8c-425c-a6a3-c3f37ec05faf
6ea7a814-48f0-4049-b36b-4c38861a03c7
9e0d6178-c967-406f-8411-d234af9b4656
4c36fa7f-3a64-434e-910f-ac2a9748adb5
255b6a17-194b-443f-97d2-7fcaeb9a14a7
9e2832ec-3691-4426-9eec-7e0a2560c4c7
8ab47aa8-8be6-4669-bf1e-e73946d6e1c8
ebb7cf4e-29e6-4af7-b989-518d6e522545
b486e2df-2c9f-4aa1-b487-28fc96a5a4f0
572c6d97-e927-4891-a9dc-ece56757a15b
c88fb4fd-9f6a-4269-8a0b-d8ed1901d4b8
b89ed4ba-bee7-4d0e-978d-df5c1720a121
f00e3134-e531-4bc3-9916-0acf236ce553
c9d48b05-6e95-44e4-a043-ee2045473082
2301f7d0-515e-41d3-ae45-1a6618fcbdbc
c23c1f65-aadf-408d-8700-03dc4d9945ca
7283643d-8dd9-44b3-b30d-87dfec36714c
c3d74813-dc3e-427d-a0fd-9fdd47f3cf39
0796b73e-3149-42f5-ab02-f41a4c5810c8
7d4eadbe-5fb7-4b55-ad6f-9d3d29b8e8e8
8f610ad0-ccfc-4c1c-8113-04dd0c9ae641
437e2b5d-ae68-40d2-826b-02782e28fb45
d68dc960-42ba-436a-b905-564aabe5bce2
b3c979af-5da9-4098-ab6a-d00171304bf9
52e9d52e-2a7e-443c-8e3e-e0bd4904a001
1746e098-b982-4db3-99ea-8db0ccafd317
2a686509-bf9e-4cdd-8ba4-b6354904933f
cf977a67-88d3-42ad-9d49-9767fa5734ac
d87158ae-4a3a-4b1e-a460-109cf5c07f26
dadfe70f-fb43-4748-8fda-125e0bf859c4
3cf7a234-6b3a-45bd-a2aa-47355573a93c
58c992a7-04c6-4489-a1b1-b6368292d260
02179b70-bae1-44b3-970f-6957478852f5
a815a362-f56f-4198-a609-d697adf9e1ed
11020406-8adb-4d52-8787-c7250750c493
d1960f50-8de0-4625-9efd-0c0325397853
28552ebe-7150-483b-95ff-8674ca2d4d6c
854c3c9d-67a7-4cc4-af9a-5b0cf56879d6
628d92f9-e77e-429e-8fc4-de5319da5b9f
c4bf2a3d-3d41-4a2b-b8e9-c5125e482c70
448deaa0-e0e0-4bad-9fb4-787179fc00df
64fc0c23-5044-4f8d-a464-cd2a45603102
2e6ef437-39b5-4c58-9a48-eb3152b54967
ca4e37dc-bca2-4721-ba79-1f917b14c54d
1d349ffb-5708-4734-a468-dc1ccb93b217
6961d215-728e-4c2c-8eb6-0a0d478c8248
a18a77e4-eedc-48f4-98ad-68d272b952ea
a3d74417-1f3a-43e0-a5ba-90d953264b58
212aa2d3-02d4-40ba-bac0-ffa9d121e69a
69344dde-f2fd-4bb5-945c-7eef5a1f2658
4cbf749e-5076-4663-baeb-20bbb8abc55a
aeea17ff-8312-436d-825a-f84a1ffb0c10
9e6a561e-011f-43f8-962e-3fb782808593
a8c8e536-91b5-4842-8142-c29bc7ee8288
d29134f7-1080-46ac-b0a9-23b13be94eb4
fc70fe97-f441-4f79-a6f4-4993e25515b7
7d926607-ea85-4ce5-aeea-5d40888be7c7
675f343e-ab08-405d-8d47-2b8e8c6020bd
91d43f82-413d-4c9a-990b-ff5b2d217200
fa9a67a7-f336-41a3-95e9-b34417fd2629
0624206d-90a1-4622-96ca-19f7d4e99daf
8096195e-1b54-48a8-9406-c14e092a2636
eabd7582-f0f4-4779-953e-28be281fd6af
aac8180a-7c5f-473d-9b35-b95cb11be7d9
62c9509b-f6b5-4036-83cb-c7b71a8f369b
89489279-ea52-4631-9c87-4a9c94672cb0
4aad1929-bd7a-4f92-abaf-9eb12931d492
a93c5014-e5a1-4476-a9b4-cdc5f33fb041
29a91e05-a2ed-44e4-b399-6f9124e6d197
891ae793-a155-45d9-b930-9fb999ef37b0
96042d5d-b3a4-4580-b02e-b63a8a1248b1
27a91d2b-5220-4e63-aa4c-0937b896a107
8ee9368b-cd79-4f0e-b3c7-a4787941b4ce
f97989d0-c712-4efd-ae2b-e3dfcafca8b9
db837650-daee-449a-a0a2-050a93e1ba58
0a8bd5ec-2f00-4616-b025-b73d971ebc33
53b33674-d371-4583-a3fd-db81b52bd352
ff5812f3-b062-4865-926e-44bade1e1e0c
fa78f8eb-71d7-40fd-a66e-7e9f512a35e1
b5ad42bd-c800-4acc-a2d5-4e118c17dae6
ea6c5c74-2439-49cd-aa75-fcab00005e14
6794626e-3e5c-4ab9-b660-5d31338cfd3f
e9cf3da9-a2a6-4182-bd70-6fe555bae722
e98e8d3a-d258-443f-bb63-571c6a4001ec
fac685cd-a946-458d-b6fe-5fd00ef7eca1
a248a47e-ae88-42fc-9c4a-601672b58f84
6997cd27-bbb3-4883-b562-93d5f43122c6
b57cad88-dee1-4fc4-834e-2609cf6969d5
5e13b367-88ad-476e-b1df-1a18c7886e48
65b82c55-5c6d-4e7e-b260-8512a560cb90
1324c84f-fe83-4578-9ff0-6cde3e810d7f
b6ee72e5-e08a-432d-998a-81c3d44dbe69
39bcd11e-5ed6-4a05-9cec-790633d41a9c
a56a0966-8e3a-432d-8867-b702020a476e
0886fe28-16fe-43b8-9525-820fb8c26ae3
86660a5f-f09a-4650-9586-1573f6f71dbf
79e8cc20-416e-4c94-9c9a-6dc80480d4d1
ba33a3a9-7545-419e-b706-79d773da35c0
d31af797-8f6d-4309-8c98-d96377c064c0
7fc4acce-e65f-45ec-ab77-f84912259660
43e65816-0be4-4e65-9152-6860f676adf7
60320e19-1ec6-4386-a8e7-7af3305ccaa1
7bb11fab-ee0b-47e6-818f-7612ddf551e9
2f6c6b16-ef5b-453d-83ad-3e19cf3e67cf
834ee9d0-0b81-48bd-9e01-ba9d935e29d0
3af4e051-b526-4ce0-ba4a-eba79d913a4b
69de3aa1-80d5-4d88-b7f6-e34edb143929
053b87ce-85ea-4270-889f-8ed6b94e6b20
b6ce7723-eb93-433a-8a51-7cc27ef6b46f
9fc89826-b62b-4ead-8212-c8b08d3de495
72bf7140-e482-4849-bcbb-81ac2745e4b7
af813440-39c2-4b9f-be10-8b626a9d5970
fb26b12c-f33a-4571-bdc7-d43292483e26
bc3843e5-d9d0-48b8-891c-f4e36062ef61
5b9fc2f9-795a-46ba-bf8d-bd2f8b3eb198
cd2602ab-8ec2-4abc-804e-56d1e9f72e85
03ab834b-d4a1-4338-9291-3a6b689ac768
ee794f34-12c7-4f57-8c4a-7385a286208e
80bbcdf1-7754-4f7a-a3fb-90d21b825932
0d53040f-8d29-43b4-8bd3-eea253fc2c54
7ddb9237-8ff9-4273-acf5-a6c5495729ae
045c82ba-8740-4377-8e05-ce885bcf5edb
73ad122e-047d-482a-972a-02d829b484a6
2fb3f7be-ab0a-4983-8e38-cef7439a8d28
1274fe27-f6f8-4499-9b9d-8e9ec05769ed
d4adc2b3-7b91-474f-8454-a5095ef4f298
c6761e05-85f6-4940-9243-dee5eb0b8416
6ce8a52c-cbcb-4bc1-b2de-7796f2a8841f
e4ee7237-f21a-4518-b847-b99940001f89
84aa1244-5e40-42be-baff-ab93a9a65380
94c60ee8-e51b-4b9a-b7c2-3d59eb4f4c99
6aa4ab94-972b-42ef-bb87-b18ac692694a
3d9bfadc-d550-45c7-a7e3-f9ea1eb2cea5
20cefbc8-5277-4dbb-99b5-ab07229f5bc0
c5d4ff0b-8a29-460f-86e5-7d22d48925ec
496af3f4-5f0b-43f7-b06d-e2dbf521bf85
d741dea5-d8aa-4e3b-a9b0-f9f669c5adfb
cd32f805-6cb6-4e6f-90e1-f712e5ba0ede
dd58b122-b44b-4ac2-82f3-431bf57c993b
93a8ab2d-6cf5-4c9b-98c5-074431382b6f
62d03ab2-c416-4060-b6fc-e198b651d2bc
5b386a77-5e4c-4beb-bb5c-e2307a410825
37805132-93e4-4725-806c-2dff6dd3b70a
f3947305-6e8d-430b-833c-bb6bfc3ad27f
396e8793-f84e-4362-85ae-1009e4fb197d
4b5f0cfd-43a4-4fbc-920b-1cdb1b7c9c91
d30a27fb-1a5f-4850-af5c-3989e1d44746
7bc9d149-7de6-45a3-89cb-2a84901aee20
3025f1b5-38f1-4d07-953f-05282fe31cdd
6c63c8b5-0aea-4348-be96-2f83c581566f
971a3208-8fcc-4bb6-9bb9-79d0de4ed10c
ccd11159-a021-44d5-b7e7-37f77ea9cc88
67012f3e-9f5d-484a-87ec-1b2aa1d08601
6eea771a-0ce5-4fc4-9b49-1bc8ff581013
a0d06b2d-82cf-438a-adc1-00e87b42e06b
d3ec6560-3d40-4569-8ab0-90f8b72fa2fd
72be7a40-1b1e-40fd-a7e0-39bc80f247a5
bd97575b-a62e-49df-98c0-4fe5a396f0f2
1bb9ba4a-4e73-4fbe-b7ab-6af2a460700e
3e1ce8ab-c315-468d-8d96-6596b79fa226
703b8061-6632-4e1c-bce8-5c160baa7b73
b0f896ab-f69c-4179-aac7-152c99ed0217
50c60106-d223-45b2-8e44-be3d06603786
fe092c13-814f-4664-8520-682591cb0b04
fe2e1de0-33a2-41d1-89e1-dd5b8a4c443f
2f9b0584-636a-4e55-a324-33ad84602ef9
649aae1d-808e-4faa-a2ef-95628f3afc2b
dfc3a9a4-e608-40c1-8d3d-7ca6c6e3ae06
c8f2178b-72a9-4cfc-b4f5-753e2890b668
66f304a9-30dc-4613-bd83-0b72a1836f9c
1a234ed2-c32b-40e8-8c69-7f200b9c5cc5
cf0ba12c-0324-4cfd-8048-3204c054fce1
2e76f67e-7f6c-4215-b267-aa1d153a543b
f171667b-75ae-49cc-a49f-1334f2edbb4b
c5121f94-11ea-4162-a5fb-af1bcd1a2551
43efdaf0-c7b0-4652-98c2-e1ce86635685
ebcae0da-2932-4109-8e11-ad66f3a0e801
d5e8c665-307e-4d1b-98b9-dd7f208294fa
7b80c454-be63-4170-841e-802a63c7377f
2ae77837-f46d-4aba-8b62-5b2977d07cc8
69439123-767c-439e-bf1f-c5cecdd932ba
2ab0448a-686e-4fc3-a99d-ce38a61512eb
4249b616-727d-4c42-9aa6-128adb1909c6
21ddc5c8-2936-4c38-98ca-8db0b43144d0
e541c7fe-7699-4a11-8ac2-b8cc7c0a0883
b8c0f588-c67f-44c2-92dc-8c89e6278b55
eaf58771-3d52-4222-96ac-179663df723f
252868f1-c3c9-4eab-9fcf-99e4f06b4741
4617e551-e8d6-45b2-9d9b-095743c9586c
6296e667-381f-48f6-8dde-9a04e70376a5
884a3723-ba0d-4e23-91df-444b4ed8c51a
88bdcdf3-d802-4944-a8b1-0551af0fb3b8
102797f7-2f06-4b00-88e2-e448a78d3927
3bdf096a-6ade-4bcd-8a9c-be40e08c3365
8082be11-0602-42b5-9d73-01c06b22419a
8a553ae0-b343-4906-92be-79ba4bc68552
4295ece0-53b4-4904-a8fd-6799f286aa39
f3b58260-3eb3-433f-88c8-850d17d46749
7b256e21-8595-41c8-a180-43676daeccf8
dbf34b16-63a7-46f1-ac81-ee0e7c1de6c8
edf65ac1-9059-43ed-8f63-281f2cd5ec80
020275fd-101c-4f5f-9d1c-989b5b31293d
71e5d80b-0b66-414b-bfea-fc2bda423aca
1120d6f7-62ce-46c2-8b4e-0eeed27fa353
03aed689-7821-45aa-8986-4d0a21372f88
bf7dde26-c2aa-4311-bfe2-6ccc4ff74f4f
24745505-1fec-4136-9c3e-89d2899917c6
95145651-1054-430f-93c6-a2eae3ebfb71
811c8bf4-bbb3-4ff2-b2ce-2ca27da38d76
3c82d591-386e-445a-b152-65841fde428e
9d0f81f8-4b58-4b54-9487-087e8d184377
e4a3803a-f8c3-4baa-b5a0-8ba145493bae
c3284d4c-c944-4dd1-ae82-1934f56c98b7
d6dab7cd-1468-4a06-8cf7-be1e2f5cb875
e21982e8-8403-4744-ab77-3b909387aaa1
9d85d08c-0ede-4683-b2ee-feba1708f01d
73da4482-1ccb-497e-bf9f-962cf798d8c7
9ee2f934-dcdf-4877-91f6-f61a1bae0dc7
b7a04616-0e57-4aa2-9f19-3d4f58d61647
54dbe61d-3989-430a-aee6-f300d3c4f6a2
86c04f82-b9a3-46bd-9e78-c987f7b086ff
c296254f-26b1-4369-bcfb-2397c8a7a7bd
c2b00d18-210e-4231-80f6-be18584f0cac
716709d7-1d8d-4aba-adfa-e1b847272935
2d42d0e0-2edb-4e41-9d5e-d272868d2d18
8c643d13-3df5-4533-b60c-11f592b3bfb2
2ca6ee3e-f752-40c8-9ace-f362babd87fd
a65708e6-0075-4d59-a79c-8aa71c8d6fb2
cd26a75d-da51-439c-8b70-3ec91bfc73e1
d278985c-a51d-4807-a020-7dec26b549c6
6ab99683-9304-41ad-b889-17cac1230b0a
bf69f36b-9e50-472a-a8c8-af94d11aff71
31300d8c-108b-4471-8541-7c9f7b7bd76b
554c2c7c-d877-4954-9caa-91710586575b
3676f194-4baa-4270-9108-bba3b78eb549
3654951d-4669-4587-93f6-6637241bdb7c
bca1999e-a67a-4a2d-9cc1-eea0de6c4196
52325176-7e9c-4ff9-aee3-54f7afdf6ed8
b4e110cd-0ed6-409d-9b64-d1be255b2782
5b68b391-b255-4ba1-aa33-bcb8e7b96603
6d0c51f5-158b-4335-a908-ecb7f74f074d
90c9b4c0-338a-4233-989d-81e385bdaa3f
c1507194-181b-4561-80ac-746399bd0c28
4e0ec98f-134a-4422-9d61-8eeb922be83f
a9893efb-45a4-438d-9b92-a611f9acbf04
5a46e1d0-1c12-454a-8ccf-940bc13d314d
48b6c101-7075-48ce-9455-033da0af5e79
1a72b1af-9e7f-40ed-b1fc-f35e2e24c81c
32c45fc8-d7c4-4282-8752-c70eb760fe62
5d9bd62f-1771-4269-8ba4-bef2c3572f98
57bd493d-8776-49ad-942e-5301e9058b3f
8c1d0c83-e965-494a-abe6-cd23d4b195a2
e652dd7a-01a2-4406-9f72-44aeeda57410
f1a97fc9-36b5-479e-999d-f6a7a3c1bf7c
373d6fe8-5268-4d1d-9600-ff2aff641c9e
2ee233f8-6ef9-4d65-99c5-681b43a6bc12
9fad4877-cc60-4703-bc7f-c6994932ba56
f3c73aaf-07be-4239-9bc8-3cb92234a898
821167b0-aad1-4758-95b4-e25bf265cff3
bd7d6f85-28a0-4bde-80a3-01852fa2cf63
9980a830-4541-4930-9c09-e74f8c2d39c2
df08de19-3aac-448b-a9db-0f9185e6dc89
2ef09886-2704-4393-9f71-7e31a8b5f273
2772ebc6-d174-4d8e-acff-21dc37c2b399
fb217cd0-ae17-480a-a4b3-6d6a8079f53b
c0f2b8a8-0446-4cda-b46a-5aca6b676415
1d5dc82d-9650-4800-b076-fdc29f65b173
bd60fa49-ec16-492c-bc0e-e2fee059c0ef
2500ad3f-3704-43c9-93b7-c086a488cb2c
2bb53aa3-3f54-41ee-9c26-368dc8f7c537
6852be23-5aa4-4560-9f92-b5ecc24f5f0a
c2dae3d0-cf9e-493e-87f9-a953f6fe98b4
ade3b7ee-30ec-42c0-8d74-c8b80f4f0617
c0c4cf7c-9175-4eb4-822f-989edebc0030
840a873e-2f9e-4478-8d08-334ca9c7ab98
3eca8247-2af0-44d3-bb04-c59d5710377e
f152aea2-cafb-4e65-a6d3-f03e811d516b
7c46e565-e497-4393-8a04-c292adc0997f
87498aab-a524-4ef2-8629-7cdaca971173
192f64cd-3673-4e1d-a75c-d81f93802a57
b81bd678-8ea2-4523-bae9-0cee9cda5350
946b9a87-8748-436b-85dd-eeec4acef5ef
b651ac06-114e-4946-b5a8-95942abf4c26
55cb4f11-900d-4157-bc47-d8642f4d0dcc
85dc8d24-98c6-4959-b6f7-6bf4ff146dc8
11d0479d-c3ea-42df-8dec-4ab33682bbfe
af3c0f9c-9135-401f-9394-16e7d0b38ab0
1e3f7c30-ecf4-45ee-8ec4-c8c0b773055f
ca3fe1d3-f7cd-43fb-8c45-4bbc63a341de
83e73481-1e36-4625-9240-f63355efd570
ae17860c-6edd-444f-9134-25d44284d668
59cb0a69-74e6-4899-a043-6e6d02c155d6
6c32c49a-57c0-40f8-a087-b7344ccc3f70
7ff78b3e-967c-4609-b1c4-2421175ce525
e0ffe973-f741-4c09-8fd7-2075e69a4db6
022d47fe-130e-4ee6-967a-33cafab0ac15
6b7797fe-816f-4b88-810a-ee51ccbb8fe9
82849651-d222-4137-8e91-7b780d9c33d7
1e57ed71-64de-4475-a292-29d7f2ebb1cc
22c9b470-0d3b-4f3a-bfe1-85931c9be34c
83e013b6-ebcf-4e0b-b986-34c6c8252f47
b7a5e518-ac95-4d27-a7ff-f26d0edc6ee9
c812d1ba-6844-4d24-9403-1be4ee5a5e62
f729b196-082d-4450-9777-24cc9753f97b
9fee73ef-082a-4c3d-b73c-25e3089ac109
320d6876-3a76-415a-a9d0-7d2e77794a8f
03dbc201-2498-4625-a7f6-82abeedf268d
69bed307-ba30-462f-982f-380702fe412f
1a105f90-1130-43da-9664-9e8cd9a4e3b5
cf9aa6cf-b4f2-4fa0-92e4-bac83de60947
beda5090-2752-4587-8290-cd8d83c18130
84ab34f5-ecf3-4c4f-9083-fb9d5582309a
b3522478-02c6-4623-844c-bd882d213e5b
2fe73c82-0710-4213-aca4-9bc38031290a
14c01466-dbcd-45cf-9221-af91e6169544
75d1b6af-6cd7-409b-be90-352a11c8efae
932653d4-a140-45a3-b279-b76baa0ab347
b7d344aa-be36-481a-a590-8148d46ea5bb
015ee9ba-983c-4c68-a6e3-76098a547436
bfcdebe3-9089-4122-96ea-2866d816fbf1
9d41bb6e-edf2-4484-b944-ad25fbc6d7e3
061bc740-fa32-44a1-a765-c6ad0a1bc7e2
98c83fda-b1b8-4c23-9f11-7e70bf6fff1b
f52f0458-ee25-4dea-93be-dbba008cd279
984ad652-666a-46ee-b712-41a23a7547d2
92147b39-5c98-4573-ad14-bb3f82e0dd40
f33c01d7-15ab-4a55-b125-72ec5c33ae22
4a68be78-9540-49ff-b17c-27b445458e29
01856d6d-53b8-4e6d-8a83-8ca34cf974a9
d4d7a497-4983-41e7-8a96-62a081d3db25
93ef2feb-717b-4a7d-a2e9-d1df6057b5e4
0aa43836-2d84-4dde-845b-003ac418f375
a4e6e765-8015-439f-a38d-9d45810748fe
376809be-019e-4ef6-b1f3-951964800b45
d0d57a83-12fd-49ae-ab87-f9f9149668fa
81996677-62bb-4638-b218-a86de15ffc69
6ef024df-fec5-4fda-8d30-1787d046c420
79f290ab-9af1-42c3-813a-7dfa8cbce916
c5d1dc14-2d26-481b-964d-e8e01bb4a894
de342df1-002e-4c84-b018-6463f8b8a56d
02fca1d6-71e2-4fb2-87d8-e48d6f4655e5
3b7073aa-5ff1-4c31-abf9-56839126cf7f
5913b67d-9562-49f4-a181-92374ba899b4
2649348d-e069-4a7b-b499-ccb6b64cc3b1
7d6facef-4576-4e8d-85f2-1852a670a291
372e9215-4902-4748-8ee7-e623fd82d54a
c7b27981-4ace-4188-bcf3-b50681757c3a
4c2feb1a-fb01-4fe1-9750-25049901e063
3ccad0d7-f5aa-4f1f-8a89-9009d66e1695
4c33a923-56e7-4c27-b677-02d7aa7f6a82
6982bed6-e4d0-4e4b-b7f5-f3a5f7a277e6
2cd3883b-30b6-41b5-bc3d-10b750be19ea
9ca507db-5d86-4000-9e94-f7b196504e69
bbbc5d3b-8cce-430e-8917-80a5b1ee993d
132b31bf-5532-46dd-9dea-937a3bdf6861
da60d020-62e3-4720-8211-68a7a6e64045
94036522-c4c7-4707-9c4c-cd58b029f66a
cc809668-9137-4ad7-bb4d-558e5a687b6d
ee8d0ee6-404c-434c-be81-7edc2b057eb7
4bd3212f-07b1-43dc-bb61-5749da7a10b0
52f77d80-9f3e-40ad-b9ee-cb7fb6daa971
4535c8f1-536b-41b4-8300-2c1c2b8366f7
d975c24f-eb92-483f-b3b9-d520760dd282
37e09ba2-22c5-481d-8740-52b62b1f21f7
950d5f16-e717-472a-9595-904b9ab68bec
71e74952-3a70-45ef-8338-34652d304d5b
cb45ec56-3c03-4081-a576-67ff3d87ff23
2454a180-407c-4262-ac12-02a4dd8ddcc0
34f04fe0-b26d-4e20-9c52-c391ffa0ad84
dde84873-27f0-4f16-b95a-9f1a913ffed4
0ce61432-de63-48fa-b4e5-90bd1efa8132
583f2772-19d7-43cb-9ffa-7cd66f4c9ce9
af4408d1-b0c6-4298-95b1-33ede8d26764
b2d1540f-2e26-46d1-ad26-1f4e5a75602c
b31df6a3-c91a-4511-950b-25b6026bbd17
14201f34-ee6b-42e6-affa-4033a623ecbe
5fb73b6c-18de-43ca-8056-2d75db6ef776
15afa4eb-4739-4851-adc7-bdd66d862261
b624e332-dc1b-4c07-853c-ba0d681fdd7d
4b35e9ac-3f3a-4cf0-a406-b67ca011d24f
57c01ab6-324c-4100-bce1-8c6ba099f4be
0281adc6-791d-4582-bbe4-645dd704650d
28d11bc2-4e7e-4b2c-aabb-d75fe2d89b5b
cdce9ef8-e1fd-4277-97c2-f568504b70ef
83ff4e05-74ef-45c0-bd86-137c725de5ec
7964e83e-e811-494c-aab4-a756e610caf7
ceb5fbca-05ec-4720-bca9-c30dcdbacc2c
976ad93a-beb3-44b2-bd99-f131ec426bd2
8ade1165-c1a4-4112-8887-3a52bc054045
2cbcc445-56e5-436f-a4a6-6f4be8dd72d5
ff716800-1e3b-40f8-bf1c-055bc824c983
5d8fc2bc-5770-4de2-9a9d-dcd070638406
2d6677d8-6575-4d86-a0aa-ea2c0c533009
e04ee068-408e-435a-8949-62b5d26baa60
fb1d7d01-d0ac-4809-945a-623f274d6547
e6a8257e-f6a7-483a-86ba-92b570bd3292
d669cd49-3b7f-4a0a-97ce-2804ae017d54
ec2118b8-4314-42ce-91c7-b40a42d88731
8906ff3d-0458-4131-ba2d-4b050f5396fc
0f4da203-3c68-47c1-b271-363b09b5de16
ea32eff0-cae1-4f25-8e70-ecc572f3b5e4
e2389f26-aaef-4ac6-a738-c3e962a1c9c8
5d83ac71-2ed1-4ede-a037-fc7f48a6ca8c
338808f4-a72c-4401-9961-45f2abc999d2
1845adfb-04df-4b01-83f0-7863452cc6f0
5eae4eb4-c93c-47e5-a61b-0f37fbe05d8b
d621d1ff-22aa-40de-82cf-5aa97c7513e4
9a87ba4f-7054-4431-85aa-4a866d247c11
8a809090-0ba6-4e14-a495-370ab7f8eed5
278cac23-85bf-4619-9efd-516f70bce182
5695160e-6be0-4c04-beb5-fd7c5c908ab5
eacd9541-7932-4001-ad6b-9821c0ad0104
9cf063c8-b7d5-43f5-a809-a4392a4ad7ab
e34e416c-2fbb-4120-8521-fc21fcb7a6c8
447988b8-b14c-47c2-9664-0ab8d80e86e6
ced45741-3931-4acf-8fe3-1d5356649485
e032734a-c3d1-4a53-89c8-56285ffbb49c
d8def336-71a6-4bf2-9c04-15386e5ac42f
f0cd5077-a06d-42d5-ba30-31e67dda38f7
63a6b76d-ac50-4aca-9e25-c5d62c2da4f9
08744645-f525-4bdf-9f94-744b35922e8f
e0495bcf-b240-46c9-8ce3-97701ec3f0c3
50839983-4164-42b1-afb5-274e2941c2ee
64bedad9-ebeb-44db-9947-93160866a8b3
16d96e43-89a0-434f-8c4b-aaaed9e9da01
4be1693f-96cb-4e8c-9b0b-cdc5a64cd3f5
662e544a-3e24-4c3f-925f-8d6b8fb6a901
24283972-469e-4512-8ee8-3b029ef6bf0e
a0210358-ac23-4b14-b1b1-1bdbe67f898e
06e3882b-5132-4e15-b17f-4fe57fc2df2b
2b177abf-d266-4395-8365-dbfcdd7dfd84
906248cc-9f7d-4ae5-87e6-b583af1e9990
bbdeb224-5363-4a1a-bb00-6c5a6eb49578
c3768a35-87e6-48a0-b873-21e93f3030d8
f4347b2c-8108-4856-ada1-915e1688a599
e0971a54-efb6-4bd0-82a4-e4b185b44832
18fb1700-9bdb-4df4-9cd2-dbd31101db29
7c38127d-8827-4664-9c3c-3e0cd953b1ec
f953f9f1-2923-4ad8-9cdb-8442fd49495f
dca37a9e-fca6-47e5-8b88-fc87c099d296
7239c54f-1e3c-4788-8c0d-b05cd2257f24
242e4360-b2cf-40de-9473-79e51dd318af
72d2e208-bc41-4615-b9df-4b8ad2c375a4
d9906d5d-0546-402e-a3ff-7171cd7088ff
186cd9e6-6948-47dc-af31-da0ad412a2ac
adf6dda1-76a4-4dee-8020-4c4674e422f2
470e2e50-39a2-4082-b10d-92e0174fb866
b7025f0a-47b7-4a5e-b6fb-39aa67a6831d
cee43f2e-606c-4e9f-9d1b-b9038d604b5e
75611165-f02f-4d67-8bfb-f12d1aa9f577
ba043aef-e6f0-420b-b152-e6f38c496a4a
ee99990b-5b64-4ee2-ae29-3721496bd1ce
0da8cf5d-04b4-4814-9744-bae717244658
256271a2-f3e4-4428-8d34-5407f8a58487
8600e118-ffc2-40c4-9346-b80f5750f781
78f226ac-bee2-470d-805c-8ceabfff55d1
710a5f2e-18aa-4c62-ad2f-a0c9ed59524a
15a0cea7-f793-4546-9838-be726d31c38a
dfe36cd9-7596-4e34-a332-9c97d7fd1905
5528f0ec-8ef4-45c2-ad65-169e32acd637
7dcd13fb-1489-4bb0-b27b-a8b3e96574ae
2b090e76-8bb8-4de0-be2d-43d05fbc6116
5e1a69df-c6f7-465f-b8e3-6024de21362c
fd3f3aa9-a13e-4670-bdbf-199866748824
d2226a06-5bf4-4018-942b-6849c7b958ab
6e0c9228-2da3-4779-9bb8-f3c2eb509552
c904e5cf-c572-4dc8-9e22-d78332aa2d25
d20646c6-fbc3-47d2-bf7c-acab85bd73ba
0d16a466-8836-4621-8498-93ea84a0e221
4e501b11-b641-4a1d-a17a-4e8c60f3feda
f8f4ea33-8875-415c-aeef-4a7c933021e4
11a54359-8531-4627-82ef-325da8b13130
4ae1579e-a6d6-4530-abaa-7c54ae7f68af
c4134954-ab4a-4c1d-b793-d67b6d2dc83a
2e6771af-db8c-4400-a7a7-f9ff9792dad1
ced1a5b3-0132-45e3-8a07-2afdd4dfe97f
3fdb8d44-30e2-4120-b995-d8f2c101d044
24669d30-7162-4250-961a-b9e42657b4e4
58e85174-ffcb-4d9f-a093-16306cbe60be
5dda01d6-0bf9-4e48-9ac9-4f453a3449b2
6b21d82e-e516-4c2e-9c04-62f332fbdb0f
4867688f-1084-4a16-8b62-28a025f33ed3
1162cc35-f9ca-49dc-a5ce-d92beaa5a19b
ce8a8502-2048-49e6-ab0c-df2997973a13
d9d2ff2b-5942-45c3-9449-bd10a7d3bc00
b1e79e58-c556-42d4-b3c7-872525cd9317
dbf86820-f19e-41ca-9c78-c16542f5efd4
6ed90c94-fbf5-4739-a616-79f01db8834b
01f7e0ab-b39c-4676-b9f6-2252de3eae26
f59b0299-32c3-42ae-bbbc-67fb71be9822
7f7bb340-e2c0-45d6-971a-604923d818d3
3433f34a-3efd-427e-971b-a9a0dadf0c84
b0b08797-3bf8-4896-aa9f-95f57e846010
8da08bab-46f9-4382-a33c-0a857e113e11
c3d45943-f246-46cd-a4d9-ea9b9e8bae36
8d62e869-2161-49b1-b6a8-c92d3f3ef9b6
5d15dfd5-e797-4347-9b82-a2d8a564cc54
8a448049-fd72-4b5a-8375-8b93db4c7b2c
b5609644-cc4a-42bd-837f-532cbba028e7
a457d1f3-5206-4b1f-aadd-049aab1a7953
21e8a224-032d-4c93-ad67-a7bd17a4687b
ca4db4f2-003b-4e8a-8c2c-edf819d00f6b
bb0ea636-296b-417a-b18e-e2899894de69
8d7d6b84-3d2d-4b18-8caa-b5754f5734bb
67a0760e-e746-46b4-8167-134aa8736bcf
168e40a6-3338-422b-b430-76d271c1b0e4
9e3460b1-8f0e-4514-8e02-dc57e6d454a9
13a78647-0989-4d81-b445-420458fb0c13
846d175a-1561-44e4-b339-adaa4b076ba8
2ec1ecc1-36f8-4879-9aed-24b7019f6847
b0f4e072-c45e-4079-bb83-fa5af584139f
adf69cc8-85fb-4eef-a09a-54b210f8c6e7
3793a340-c231-4832-83fe-921b501e76a1
1dc0417d-6a06-49a5-9785-24e4c1fa7f74
3ba58300-c446-4887-bf6e-e742c1e57c9b
91d77a00-af46-433f-874e-ebfb0702f769
1b2cda0f-7620-464e-8cb4-894847f77ead
254c4998-732d-4c21-bb45-08e5445d795e
3daeff72-a904-4858-8581-bf1b56917cf6
ffffbe76-1683-4669-8d20-b96981a8735f
00c620aa-6696-4384-81ea-37a0af0cadf5
b4b69440-5f00-4455-aab5-44d8a5f43de0
3751dbf2-14f9-45be-a785-75e996f2c4bc
92f6e54b-c228-4b8c-a764-05c0b2b8da03
6e3ceebf-a05f-4f3d-8e24-a37a11a08d8d
3cc896af-018c-407e-8cbd-77ef2c6d9196
898d0bef-1a76-44e8-b5d4-72573d300a60
0efce8b7-3a21-42c9-b387-29c94ed19fe8
03beb33a-221c-43e5-a447-2fc165b0e7c1
72e97beb-5a47-48e5-8a85-ea660a84af90
27c5b7cb-10b2-464f-a95e-44d02375ac7f
53324b9a-16e7-4592-b16b-4d63b84d62cc
2aa5ff8f-032b-4fa7-8691-21865cbf4357
34201d44-2436-443b-a639-ae23bcd6e33f
3efc77a9-6264-4145-9007-32d485e23c76
1a4e5be6-522f-4582-811d-a23d4dc82079
2cdb5a1d-504a-488e-a22a-70abc6e4423c
964f56bf-6f19-42d1-9d84-51ac0febb560
4986b189-0d48-438c-bbb1-8d2b52ac2212
e17dbd01-f214-4f72-b017-b945dfc5b53a
480b7fa8-07fe-4365-9fd5-813d2a88d0f6
058395c6-0898-44e1-b19b-ac562147408f
4a54676f-0b51-427c-ad61-94e61d2ca725
73f1a4d3-934f-4197-9ea2-c5fefb3ac1e5
b3d1c41b-a1d3-4d70-901c-f4ed479f7888
a37c569c-8cd6-459c-8d95-a5ff6eff6ed8
c44f7728-0bc5-4019-adbc-696a6a7bd04f
db9d983f-4ae1-451b-95ff-6a8774a0ca41
1fc83b7f-e8cd-45a5-950d-a032adfafb7e
3e2e834e-569e-4cd9-b859-1c7448293bff
55775093-fb62-42ce-8166-05c4cb3a1c46
b8bfb6f0-0fdc-4614-a275-8934c0238727
14350586-9eb3-480f-bc32-7ba798a4af68
f6ff5b5b-e4c4-4153-8665-bc7c00c3501c
664d7f95-d1a6-4bc6-9f3b-57193869dad0
60f0e27d-7b48-49c4-8e57-9d71b3d9fbc4
509aab18-7237-45f9-bfd1-db7cf090af79
6998bbfe-ec59-421a-9248-c4497e515681
71483d12-4965-408b-b229-254edb92fb2b
25024021-b3f0-41c7-97bb-0114e8bf4497
e0a86f61-afe0-41fd-ab4e-820d63171b3a
f63daab1-a970-4ff3-96a1-1464862ba7cc
c128ee88-9fc7-4262-bf61-6c5051fe2aac
ddc626ff-eb1b-4666-b2fe-570ae19241b6
b79c6951-6c9e-4ce3-ae23-3f00ab7ba7b3
fdef6c4b-edde-422d-a8e3-ad6ffd2399ff
caab41c0-51bc-4ed4-bad0-06b5645fff2e
10e2dda2-3f27-494d-b35f-b20bea0388ed
cd293b0a-9d27-4fe8-8423-789afe018e4d
63a64f2d-7f46-437d-9786-dfdf813a8924
042393cc-ca1a-4323-ab79-2fd02b908e09
f6b44d2d-2d88-455c-beb4-b23cf5b0d10f
b3b6bcc8-b441-4a7b-a969-38d5b88834eb
4eedc07b-c421-4b98-8591-b06541181301
59e0f787-1748-43fd-adb2-b49cb0d65cd6
ecee82b4-1571-498c-8bc9-c1cfc615dde0
57c88679-0e5b-4e32-9279-a0b9ebb4c42b
b74c3940-3ba5-4187-ae21-c39c233617a0
d37f871d-2608-453f-937e-585e517f2c1e
08ffe6a1-af83-448d-840c-e421216372a8
9bd99606-1b75-42e9-9b73-fe499f1f8d2c
63e9b7b1-f371-44dd-a976-3132daacd4e3
69ec50cf-0e1c-45e7-9841-1deba62f19a0
6364d7ea-97fb-41cb-a628-f7e8b10d4db4
074a0404-4fba-407b-a167-9fbd266b8974
209bbb9f-d3f2-4878-9bc0-108e1611f4fe
5c3a8119-26aa-45a5-97f2-187672364dd6
2125f982-5f7b-45d3-8bd0-1b9c5ed50f9e
766fe412-a463-4ec7-9c27-cdd326f9b4bd
46d52f87-2e40-4b49-a0f0-af6836728c0f
7f4446ca-ac8b-42b3-89bb-a091fec69cbb
5fcb24cc-d24b-47c5-80f0-a8630a39c587
c41ff7f4-c8b5-4b80-b091-4c3d23f0cd9e
706baa53-2d82-45fb-878e-0fe29107aecc
cd395729-198d-40bd-99b2-78f8253d5b64
382df327-3a21-4b2c-9a7b-da99936bc7b0
9864b70d-5e5b-4886-adca-7a28faae0b0b
49624ab5-6666-4a00-8b8c-d986ffedce4f
864508ba-beb9-4d83-885e-f92129bcd499
c9bb8f7b-225e-441d-bcea-2a7f4118b816
560e4d05-ec25-4407-b29a-016ce0da1291
affcf0d7-140c-4d34-aac0-3e9273806086
89f82675-4785-48ad-9fe4-70e78a2a318d
700f0339-2d6a-4aa6-966e-03a451fd3fd4
2f9ec3ec-d599-490e-a83a-6215db61fd08
4698c0ef-ae8a-4b95-b57f-e8af4f4d7fff
b6d95194-d02b-4c90-8bd7-b44e736cca96
07a5a414-63b6-4f38-9356-3a379996d2c1
eeeb246b-68bc-4839-9e5e-ba617eb2edf4
223ef5d5-427f-4e35-85b6-27151e9078a7
1688bbcf-05ba-4fe4-b2d5-03a352afecc2
a4421d89-94b0-4509-9cc3-2e7b0e46a709
c0cfdac1-65ea-4975-b83e-8497fd6b0eba
4ea737f3-1af1-4f34-b4b9-dbb20f3f4684
72c7c8c1-a8be-4e80-b93b-d9fdca7ea68b
4da2db12-3625-4f9c-bd6c-b5d207f2583d
7e00812d-87cd-48a2-8248-6209bba6ca84
4154ac2d-8529-4d66-aac8-6b2b67f69f03
a247313c-f40a-4450-bf30-f0b30d47c9b0
30077b9d-32d5-4be0-b3ff-67c99a5b56d1
86c9c2b2-133a-4d41-b3ba-0b173b91a292
36fcad3d-627e-40d8-9174-6ff3f39d5ca3
943134a0-7620-4215-b5ab-7f3a4ecc3a1f
1fd5598c-69b9-440f-82db-a2de53d97604
98d5dc95-11c1-4caa-ad29-aae6cb5d4403
8749a61b-bcc6-4be0-96c7-70037a4888b9
5f602dc5-6b7f-4389-a0bd-4cc68bac5b9f
1c709891-ba62-4271-93d8-74b8e55ba968
0d18361a-5f03-4b67-ab53-cff0cf6fff8b
b40e73b5-001c-463a-8476-a2f3fb480eb4
e8a05924-2c0a-493a-9c44-fe6451004fa4
57260cb3-f218-4492-b05d-375a6202fb7d
be0262ab-176d-4f1b-a19b-cecc11d8c1c5
091f7b26-d9e0-453a-ad77-b20aaa295a89
950dc356-4db0-444b-8be3-249bcc2f4f00
b65fa76c-c2d5-4c00-aa49-983daa62ec1a
325581f8-e2ba-4d64-b7d7-f8c330b9e2e8
3124b7e9-7013-45e8-af7f-f50653608149
83c3aa1a-57ce-45ea-8be3-3dcac827a11a
63c1045b-2720-494b-b2b2-1f971f380892
b33478d0-3e43-48b2-86b2-8fe2e629d234
9abc1305-5b38-40ff-95c3-e6ef52393f94
10915f03-811b-499b-981a-64ae76a6f402
0125c0b0-d07f-4606-8898-0dd14a6f6018
9c714cfb-db16-4920-ab9e-d6fa2ff4bdac
4f969d79-f5bd-48be-9a93-15516ef784a6
54697ce2-5150-4708-b625-0a3f5e7da446
6b30837f-99f9-4650-8652-81482f66b526
b677302c-1b1a-4b20-a70b-9268b00107ff
b0b9b0c9-46ff-4cc1-a83f-0c9113e596b6
62644a3c-bed2-4ba5-b095-fc411aa85f02
804ff758-7661-4e08-9932-84fccdd52084
4b82eee0-866e-4b36-997e-93e6d955738d
0a4a76a0-62d7-4764-b1ac-447763919981
d26b246c-909a-4011-a44c-7884ca6003f8
2fe2d5e6-2297-4ed0-ab3d-9c27cb7a6876
b5101dbe-5b38-43cc-8ab8-5cca05f0e493
3340b195-bcf1-4bd7-8147-e05111da2a62
29ca3fe6-c497-4f8a-8540-503b208a3ca6
9753a20c-9ee7-4689-91c8-08ac07736736
b2868fdc-0f75-489a-bdba-6d9d7fc70868
82789c0e-789a-4575-81f0-017d61d86f94
635e23be-c727-4608-9ffc-2cbd8eb1c2b8
a914800a-7b87-486a-85e9-6c5265756a18
a8701af7-80d2-41f7-b67c-45663802dbfa
744adaa5-6d90-4512-aeb9-b37d37c13010
a76c40ae-90a5-48ac-bd07-f1f34f8e8109
3acf4883-0ae5-49b7-91f1-bf19848e8a68
0ee20d69-95e7-4303-a432-4143b7581095
2de8f859-26f7-469f-aacd-1a28d2422d7e
c24a71d3-5acd-473e-a019-805d5b7f3cc5
d2c76a0c-389c-440c-941e-4a284c3dddb0
fa2b7593-e03a-45d1-b8a9-3870f6806e5d
8e1460e5-dd61-4411-b992-7840a3a788ad
4f7d0270-8b57-48d8-ba99-36218e6720b7
aa7804e5-5177-4f64-8e90-6363d9fe7b9a
d77094e1-4fc4-425b-9289-b977f81aa83e
c64b6864-1c94-4496-9f2b-17ac9614dafb
84d67bc1-14e0-471d-ad23-11017f49b29a
29d8db4a-5dc5-4040-b5b9-83c546fe92d7
a8a9a43f-93f0-4e4f-a3b6-1f1219a16d07
c04254db-2309-471c-a0a2-df700eb8e715
f5852f11-e6da-413c-a05f-351cdcad5180
eafbdc7e-a960-4db4-9c5c-c480c08fa803
1c34f9d9-cec7-462f-9c47-e2303661a6b5
6b94edc3-f9ac-4b87-801a-0a70570ce732
0116daf9-e749-462c-92b0-78280d68da40
dc2a9ec9-be77-4042-8ca0-7c9b566eb6bb
2495aefe-78ba-49ba-868c-2bc306b1f2c8
591869eb-3ceb-44de-9708-3ead104dc579
3f102748-9987-4fec-84e4-21360684a06f
b0e770f1-e66e-4298-b5a6-69f121a3f06e
b87c4550-7988-4ea5-a419-e2fdb7d3d9ab
2795ffe0-824a-4467-af41-7000dd5c0f26
5585854d-e741-4096-b4d2-26233f762d5f
c4adb011-ac95-4183-b817-85474db1b5e0
bb74213f-0060-43b6-963a-f758ada9b504
cf526db7-9e08-4512-ad37-a857480f54ba
66901ea7-5c60-4f79-b1b7-a15c6d2b9947
ec8d4f51-c334-4d50-9b4d-aaa1979219f8
20316f51-ecaa-4048-9416-1f9994544454
0dc88c21-c7b6-40cb-9a46-6a3c0e7900eb
f6bec5ca-87b8-44be-a135-48a326572152
6c2a1c00-fde2-4693-833b-88aec4937447
f4048efd-8579-4401-9326-a293fa39d8b8
90c1297e-d74d-4c79-86f7-19554ae72ac0
9e5bb596-ffec-42e7-a42b-7f10b1dcad3b
ec3c3d43-eecf-4ed5-86a3-ffccc82f5450
57aadcae-9558-4d48-b323-540b7b39a6c3
cdef1640-3ede-4b14-8c45-740108506bb6
5a9bdc84-4b6a-415f-8892-4703f57829d3
d7dd7edb-56e3-4c11-8bab-328098b3755f
4f73b3c7-834b-4e83-9357-7d6da5f9af59
4ed2bede-ecd9-4b04-89bc-8898f8328d4b
fa36fc31-42c7-4d50-b9e7-45a02400b0ed
e7ace5cd-102c-479f-b183-439813a086de
f3157aec-c67d-4d3b-a87d-75c72af22aff
aaa6e8a0-cde8-4906-b162-297cc2038beb
bc0a0887-31fb-4157-b334-be296d58dfc3
54a3c22a-224c-45bc-b834-94f84409b7b3
4eefc14f-876d-4338-bd3f-35b234c196b5
3a574d97-6421-4218-93ba-b9b9c2eb9300
6eb1ccc1-b52b-40a2-a1f2-00dabfe4481f
8cfa46a3-092e-4348-9074-4831d225d3ca
dec9c911-4e96-4f48-9de3-79485ccf076b
d49e3e65-f4c5-4a3a-b03c-c507e1efc3e4
1c55a287-30c9-470b-a6fd-b73b12bda745
69eaa735-983b-4d94-944e-5b0ff0bc868b
3015cc74-471c-4bda-b32d-3a4e806874d8
fea1efd9-269e-43ee-af5a-8a213bebc997
fab2a2b5-e6cf-408c-a00e-17fe0cc04549
ca9c3186-da60-4b1e-94b7-ab4355f3e33b
7f16613a-6de7-4175-aad4-6d4990cdc832
ad7cf020-352c-4712-96dd-67a2843fa9c3
001c05dc-8326-4fb5-b4fa-083fcbf0c46d
483678bc-b05b-4a61-8123-9cf8dee347c8
46edc4bf-83ad-497f-9967-985668f14987
b30a48ed-7ab8-4709-b44c-c9557193f172
e6a8a6bc-ed45-4f97-bc8e-d50fa5f5b58b
45baee4a-86bb-4ecd-82f7-d17cf6064a89
b095bc30-a393-4284-864d-be2415551da2
cbce4d39-0e16-4c27-9e60-9432737ab789
a66c3070-d52e-4a5b-ba74-60c6bf4cbd33
70bfca86-66d5-44e5-9959-17498a595ef9
68a5f4b7-75ee-4766-9931-449f367a98b7
34745d4a-95d8-4b65-ade6-b2edd8a06eeb
7226792a-a6c1-43aa-bf47-2d61739947ce
64149c67-ace0-4c10-ab94-d79cbc1a630e
9d1ba985-9bbb-4d1b-bcd5-dc5338146590
f39f6424-3bf5-463a-ae45-385171f2c413
cff00be6-b309-4938-91ae-bd03e9d6dada
3ef264a1-ebeb-461b-a6f8-3f364b592657
5c1621ba-6c20-4951-ab52-1edec87b59ef
6344e2e6-50ae-4526-afcb-6a28a54de676
a27cb10b-6ce6-4606-8937-296345144896
59753ebb-6e60-4ef7-a25d-2bb4ae447fbc
2e676afd-1667-4da3-b311-e83dd137b1f4
c0a3e492-8f83-40bd-9c36-e2159e8f3fc1
5eb1b375-0752-4b95-a3af-ff4ab11b9ee7
e2328685-5f19-4f3a-ba11-f66a03bb5a0e
989c0486-56a0-46c8-ba18-30cb1a6a0df9
f3d67738-5485-4f2a-a63a-ff8a1ce8344d
b778103e-90a6-4f4f-8151-78d3feb1fa79
618889dd-9f82-4c5b-b310-d596a45eddc8
f67adc99-6585-4ce3-b292-25bb59f095f1
1bc46468-9bfb-4d50-bd4f-00e91f0e7143
a51dba58-7e50-4fc0-98c3-1a78d50a8d23
da9e63ee-9b25-43d7-bc58-0b206944dc28
ae1bb5d4-fc22-49a7-9470-867fff2b1d20
4b108751-b62c-4124-bca2-fe1a148c3c18
31faee52-e084-41a5-b3a3-09c097dfb10b
ab956d87-28ff-4010-ab3a-e9c9c9a24f78
466f09b2-8ccc-4253-bc96-2e1d8117414f
9a298d90-7dcf-4343-8a3b-185bf313d611
b77d08e1-6e8c-446d-8a5d-041983261d6c
5ced51f8-90c1-48b7-8580-b11d3392e3fa
aa3ef183-8f64-4177-877a-3062a5a2b8d7
e69ec06f-a702-498a-b084-f71b2842c18c
6749db21-29d4-473d-b137-b8ed1d2ee84e
5492f9ec-1922-4e66-9f2b-03bfcb5c5e47
2e5fd7bf-467b-472d-abab-a868983544c1
aced97aa-7f39-4e53-8528-162fdbdbfa97
322f300c-d589-429a-8170-4f791f134dde
e07c9cfa-6fa0-416e-a70e-682a5a88786e
6068c505-216c-4dc1-8817-70ca6a1a8ba3
73abd061-9295-4a72-b5b3-88a4a459ede9
45c0e544-b5b6-42b6-89a2-c0891b6fd447
db3b3cbc-39b3-4860-88fb-cf0d6f226a8e
8af91794-4746-4a23-a4c7-d7272e5e368d
3f7cbd03-45b2-4540-944c-a08a035a1351
c9452936-3333-4e9d-8a33-f6d3f9d3b4bb
ae0c5034-a009-48f8-abc3-055b07da3f2c
3684021c-b5a0-43dd-b9f6-1fb8be66f2c8
86d73e49-c2b7-4e2b-9073-ed03f657df99
095e0d74-94dc-486c-9cbe-4b00e4c44e33
93c2adac-ca97-4d1b-8d68-53ce5ef27e91
9edb6314-0931-471b-b4e5-073546a80487
24b71f4d-0622-4a2f-a044-0a3db58049d3
cc38253f-7606-432e-8a3e-c3f9ab1e080b
ad4eb638-e15e-4801-89af-3d78df89cdfd
94258ca9-f9c4-4946-b123-7c729034b339
d09ac74d-e8f3-48ac-8b2d-3e69afeb143a
89446367-eb2e-4ace-b1a3-f6b58cdd0b0c
5c043d1a-6986-4ef1-baae-93a88b74ee28
9e546fd9-96e1-4931-a526-0289f50abc40
5b3315e6-e96d-4fce-94d9-2a6cc8944f66
3c90bf65-3411-410f-8adc-56a0d5b61765
551eafb7-03ed-4957-9ed8-3f2b48a357a2
aa0fbe6c-e1c0-4a10-a359-3f18aa4f342c
eab74e03-ff47-4a38-9f4b-52f3bcfc3ec1
fdc00424-0e8a-4c60-b796-37bc4524cb4b
36cb48c2-de35-4300-9a77-75a4c3b21aa1
0a76bd9f-07ac-4d19-b98c-916c86a08653
9becbc69-ebb0-42c6-8573-e450be470af6
268597af-0478-47dd-8ab9-dcffb2b7d350
01c16cd1-7ad7-4e36-9b6b-75c3bf5195ca
352b4946-372e-42ff-b743-152607270f56
b37b83b5-a84b-48f7-8bce-478687fc0578
af18cda8-464d-4f55-8e58-0cbeea14ebfb
eef46ce9-db4b-4694-97c6-eef71f6cbd70
f0b5047d-6be5-4077-a4ec-1b3f13bedc80
f6dee139-0037-487d-aed2-2f5c104a614c
49cce6b6-f869-409e-8b8a-6bd10eae00d4
9e1a427a-1fd3-46de-9876-fce990d66c1c
7e2b05b2-88e5-471d-913d-b8c2723405b1
c8927cb9-1bd5-46f2-8fc7-fbd2a6fab0e6
f8735237-a814-4c5b-8919-969dca974882
69bd5075-7552-41eb-861a-3776b050630c
e184cdf4-d2e2-47d9-b69d-cafcede0afcc
8050a645-63b2-4eb8-a3c8-24bd2f594cbc
8d4bfda8-ea0a-418d-afd8-9f79edd93d8c
482296ca-71e9-4b49-b61c-d214aa546bd8
c1e34346-7682-4b18-aa39-d0fbc05697d7
6c9a9036-05df-4bfa-a23b-ebdb51cf1ef1
ef88d2e9-240d-4583-ab00-df23d15c78a3
913eeaf5-e8b3-459e-9d3e-ddfb9395461a
cc43e72f-0f4f-46e1-82cf-5c942de2a02f
9941d674-9078-4a40-905a-4b8e30fff402
93cf446f-32c1-4867-a8e5-3ef4460418da
4b869685-3518-4a44-8fe2-143e3160c740
7a182532-52ad-4edb-bc18-60452f5f5c7d
d5134681-b39a-4828-9a4b-e169ffab1b5a
21e54fab-eac7-47a6-affc-ed7100556cf4
c4b53d6f-10b4-466f-920a-ca6b0647b07e
033eae3f-25fb-4c85-af94-a9ea78d6ee88
7faa587d-9bd0-4122-81f9-e34441478c6b
323b148f-a8ef-479f-a7d0-92284555d654
0192df2b-953a-4591-8f3a-9c9425d2cbbc
d2ce7e86-85d1-4fe4-8eac-9e68e3cc6fd2
90b6fb27-60be-48bf-9286-d717480eb4ce
b07c7ef1-a00d-4152-831d-4422204c3949
09f7ba4e-5370-425f-8e6f-ae71932f627b
d87c5645-b0c6-4da9-989e-cd1dd315a8e5
a738c072-9e04-4b2a-85cb-38fc972ab80c
1b432bf6-05ee-421d-af56-feab8232c854
a6318783-947c-45c4-b4c5-d14c9e51411a
10d4cebc-c2f6-43f4-8434-7721e1959a4f
5ad4ca9f-efbd-4578-812b-fd276d6c7273
a54629bc-0d85-47ff-a9b8-791ab46e7d0d
c76bde0e-e08a-4cb4-9222-9d295db18e68
e0008b7c-64eb-4dbe-a957-8fc95421dbbc
09c7d138-931d-48bd-8af5-4c1bb93cd266
3805c9b6-75a5-42a8-ab16-09114a5373d3
24211099-5db5-4928-9c2a-25a125548b82
a2db348e-0b76-45d6-9f8b-a8872b2a744c
0f34990f-5c4b-462e-a6af-c9c104ce999e
1342ed7a-71ce-4b99-b0c5-598d1b3a8709
a40254d6-6ede-4aa1-866e-b7a979dcaf26
c41ccc93-ca68-4239-893f-f0568de935f6
a69421dc-d643-47af-b814-428a9521f7e2
6227418a-17a9-4df0-b7b8-e0f42629cc3d
2e46ed21-11c5-41e6-bb96-36d05fd18aba
667b4403-a6ad-428d-bd67-e7d4976c0e25
9868c75b-8765-4b93-8654-b3aa9bb4005d
6ebf2b2c-3b08-4896-b61f-a44c80ecd068
00f678f5-7e38-4d20-9cd9-7a93b1a5cc35
675c0db1-8afd-4071-b9de-752fce0fbb3b
c456bde9-c660-485b-b794-2dab2a43a50f
dd9a0249-8ddf-499e-90b3-464e29cf67e7
6585300e-6f5d-4124-88a4-0a599d16586d
9e648256-62da-4665-b750-81f7d0fb38f7
98569c3d-373e-4421-a6da-4a3250ed2423
7b0b997f-a89a-4d95-b0fe-5792775d9a58
da999572-84dc-4fd5-b9de-a278a3526979
554a4ff3-89cf-4cec-b2d1-486356305c3d
6557c4d3-b7fb-4cf0-8c64-90af39771f3a
ff4decb2-f1a3-458e-b63e-f9fdae8258be
d250bbd1-2c1f-47e2-8f6f-e0f623c572bc
dd7e4688-d15c-4583-8e05-c937287e68f6
a332da91-3480-45de-b44a-4b4dcb743053
f683d4d6-a15e-467f-bfad-9fd7c4c27d6d
78f89635-8d5f-421a-8754-eafd9560ebe0
dec23716-b7d0-4ca8-809f-64e01fdb91bc
71b27ca8-8a96-45d4-8422-52d8b24a03d5
5ba0a892-c85a-4e60-88b7-867d7dd765cf
88cce1ff-52b7-4db2-aed9-cedcfeba83a1
34522995-f616-4d2c-a04d-6218eac21138
8d1bb32c-4b53-41ba-875f-19af026fb26c
832b2c92-372f-4840-ba76-ad812a026aa9
23889a79-cf44-416e-90b7-be56fee35193
7e5edb8c-d62d-45af-bc64-cd3538353330
27ebd534-dfa4-4d45-8e03-126e1274a1d7
a2bc587e-a8d1-414d-a30c-eb26d537dff4
21afbe32-53b0-4596-af72-58b10e30994f
580aafcb-3b52-4943-bb9b-e3b4c3aab063
6746f6e4-799d-496b-9929-4c46f8e46325
f42c6707-042f-456b-91dc-8beb0c28372b
a0e2c56a-961c-4139-b11a-96a0145fe147
a4f42538-013c-4edf-913c-3c91186721a7
5a0ac348-2876-493a-827d-9bece3b6e5e6
2dbe62d0-7e7b-4c0d-9592-035814b2801d
ef4a151b-5d18-47ef-90af-29a90bd77f51
847dbfec-6742-4528-a92d-676dbe128ad1
7deab527-2496-4fb1-bef7-19208091111e
4eff8158-6edd-4622-9826-ca72db933962
109497df-2605-46ed-a920-adaf3044b9aa
f56f5608-e79a-4179-b44b-72b69eaf5b87
e6ea9e94-f676-4179-9740-b4fc3ba1933d
2ac23014-5af5-4918-8275-4fef82147ab7
a8190f73-1913-4a58-8f30-ed7d6e26d8ae
6765d591-b060-48e2-aee9-39078077e651
8a8c43c3-61fd-4913-bc69-7809e204eebd
bfedf802-abfc-4742-8633-317fda8fc4cd
3fb95d62-7bb9-45ea-80af-388178e01407
4eda7d8a-ddf4-4a83-8643-16ec688a0f1a
38bf781a-cc2c-44f8-8da2-ee2e99d7a660
bfd0fa6b-f9d7-4053-83af-8c4c2a0cd138
f358b3de-e17a-444a-b6e4-988ad6f09c59
d49e97a1-42d3-4856-8f41-e8fd1f5647f9
b968fda4-2f2d-4886-8328-316f46b19152
647167c5-61d6-4a88-b1dd-a94f0ef09c58
c1fd239f-30b7-477c-a5f5-a84bbc81aad9
a37f1d86-79a9-4ce2-b626-3f941209f9dc
dddbb261-fb3b-4474-b9b8-960141e9fbdd
995b2160-2085-4f62-9bdb-d22ad8ad3847
c6f06913-1abf-4e01-9a09-1645ddabf758
d243ca4b-99a3-46c8-8803-a18ff3083fd2
4dae262e-30fd-43af-95d2-f2d7666e599b
a3ccf6c3-7a3e-4b0f-b576-f09a37c870d2
78e4283f-03f2-4407-a5a8-db81047d93ae
c1fe8424-afd0-4b00-bf86-ef8d0923e88d
7ed09c80-87f6-45f3-b2dd-e71549b6537d
4088c245-b9ee-42e0-929e-455968f19361
5c334d30-7fb6-4b48-a3a7-79a336874078
1befe15a-3f2f-4e24-a2b8-e34fb64a4564
b33523db-faaf-4565-a8e0-626957b91aee
d3892109-89f8-4633-819c-27443498f699
956cf083-79a8-4472-85fc-9e1051cf849d
5b1ad403-9e8b-48d4-aac6-c9d31e041a5f
3ae7fb45-7738-43e3-8a2c-a2d2d1b931bf
51dfbdc8-224d-4702-bd4c-a8e93d488854
fb8af990-8995-432e-9726-09d23a0af650
7553619e-001b-40a0-8926-e82dc7804087
1ce1f1a3-58e0-49a5-817d-a35da1615c01
01eec53d-a916-4dab-97fa-8facf114546c
554c2a7f-1aa3-44d5-88cb-5023803eb8b8
f4508fd2-b881-476d-8db0-e861f1faa725
dcde698e-21a2-42fa-a309-d355983aaac7
d1f431c3-2ff2-4829-a81c-c950196a5c8a
a640e72d-8bb5-40ef-8ffb-4de981a1b8af
e2253f2f-62ed-4191-ac28-5ef7702e82cf
e16197f8-9f26-4c51-b6b9-5432f7e2071f
325c69a5-6fd2-4866-a5f4-d72744dbade7
d1b86bbe-9fc7-4ca6-8e03-ba23d4bbfa39
6e77c68b-ca11-43df-8ff5-aa681276be8d
6f34f366-5a89-4d58-94dc-ed6688cf4de1
acb98cc2-8a34-4925-b41f-c6d98fef15a4
4706757e-05e4-48aa-bde4-93cfbb9ec6a3
468e924d-893b-4938-8191-c63ffe07876d
82dc87ad-cb71-4d5b-a180-628af3df9a4c
41f06e3a-0291-4869-aeba-0311a241bd8f
c991b1ae-a0d4-45b7-957b-ec3bb8d1d88a
6f9678ea-c318-4112-a668-0608c05f0d5a
6489fbd2-f10a-4f0b-a3dc-e6408055a9f8
e9d5686f-59fc-4e56-bfe5-00c6f34796e0
32dcb0b3-fe2a-4ef9-9199-4455c4feaf6f
ebb83a6b-8f9e-4dcf-94f7-90e69e104a99
195894ba-f80e-4789-8093-8f2af46457ad
3c65d11c-bc04-40f2-9b5a-97dad9089ac6
3e02088b-c1cb-4df1-86a5-6f6a83fbbd46
2ff806cc-0a6c-43df-8dbe-0c3be6510939
0a4246be-5dc4-4f20-848a-1095c9b27a7d
1d85d0ea-c70a-445b-864b-9ab3d6013dbe
87274917-afff-479e-99a7-0a5807d8ef88
56b5fe95-06eb-4736-a2f9-fffa79533c6c
6af28440-5e1b-4413-9b00-f0c1b916156a
d4c77b1d-2c12-4a5c-a5ef-e005ae81b716
45d12700-706d-4194-9044-57ca171fc1a9
b8dd3d78-3220-4efb-9fb0-6d173604696b
6ac9d2f4-f756-4ab8-b2b8-1138b7686331
bb214e4e-8d5f-400a-8107-755e9f29eb9c
de4194e7-b3f0-44b3-bc3d-deeec5aeb3a2
373451d3-5a7a-4e2e-8f85-388bec2c6d07
6efcfaf0-4584-4f48-8fc7-79a378227ca7
3cb7c2a3-bc86-462f-85af-9f6bf46bbdab
4d6546cd-41fd-43c0-9f99-e12cf3b1ea57
aa2c2127-12f7-47e9-9100-03d92c338de0
0faccb1f-baca-4e12-b112-0cca42a012da
de906a13-38b5-46fc-b6f2-e593ed74835a
f74c4449-9ffe-4ce5-99a6-e805f545c253
a3fd4f34-a7c8-41cf-946d-67d5297e6d7b
4cd0d39f-c443-494e-ab4f-b6bf4aee7587
41be375e-a40f-46a6-a863-10b63d285a22
7b24c0ff-05b6-4c4e-844d-2e6c8b26cc83
894ffbce-8f75-44dc-bde6-1b235cbb4c7b
a1562fa9-f45c-4b63-a0b7-c506021b9481
3685dff0-3aba-486f-b535-76a7c9d608c0
ac962598-c223-490d-b015-6f41cea20f14
96699836-2efb-4bca-93cd-69bf63662e2e
22d8ae01-e5a2-4600-b0c7-8e53ff55cd2e
6ff64f10-1ae9-4d18-95bc-4f2172d96d58
43392841-7f69-4949-ad0d-d95c505beb47
bb6adaf2-02b7-4499-8f3d-c101b46ad5c2
7903134c-9b78-4a3c-8d3d-064bf4ffee99
5acfbbf2-e8f9-43c7-9a79-fd6a4886d59c
47933c39-888a-491a-97be-354c71c64a59
3e2e027b-8247-4cc1-8e74-01da9589d8dc
1aa30830-71b9-4d2a-b7cd-798f4f03ac07
35b97161-8d2c-4572-bd06-b1d7e32bd10b
f4553756-56b8-47db-ba02-5760abb06b1b
7a4f6182-6397-49c2-991f-18bdf69f8b07
08370006-dc96-40ca-bbff-fec01156aa7a
dfee94e8-f993-4be6-9259-360119f1fcf5
c7d2cc99-2e11-4995-98aa-7c837595252c
0554a0e4-3269-42dc-8bc2-768d4976dadb
07d823c3-c98f-4b01-a765-2f29be0ccfe9
f4d46938-1943-4370-96a0-1b2a285394be
b88a964a-afc9-44d7-aafe-3147f5b42ebe
7c747e31-ec60-49df-9e45-ec09a887b2f6
289d263f-ba40-47c6-998c-0b3c37c5ab7b
935ab5cd-4053-4ba9-bf23-6ffb6c7c7667
7be6f663-260a-4e37-902a-9fdb047cff09
b72eaae1-4908-49ae-ae42-9247690cefc2
89b1609f-14d7-4e99-9dc2-398032a2b264
9634b327-63bf-47f2-91bf-6764712539cd
feea2f90-73d6-4b41-b730-9d442dcbb494
23f70ecd-c7aa-42c8-96db-6167194b8721
fe080ef3-0feb-4bbd-ab85-4aed2a4c8df6
4ddc3b59-744c-4cde-a11b-71a0342b8795
f06b3f01-98e1-462d-9b35-904b504c4096
1636bfc0-669c-4b5c-92d2-e07fdf961952
7169be58-097d-45e7-b2df-8f58e85aa221
62d2e7b2-d92c-43dd-ad96-90261f7bdc74
41570753-5820-4295-9ab3-7520568202ef
8a8ed3cc-604a-4a7f-89b6-09e77ed1254c
7f724dcb-ffce-4e89-bd6e-6a8a663d967f
b43d59ec-e64d-4c67-8c35-93d1b1ceccad
e0e734e7-292d-433c-9434-c3dc38adaa31
2f9795d2-3ffd-4e85-88be-2eb8e990d726
7ad468dd-b92a-44f0-a860-95863f4dce67
00525eaa-b2bd-4d93-8e5a-21d3d027b0f1
24ef5019-5ac7-43c2-b5ba-5093a360cf34
703c2208-d569-45c2-8c7d-e38c9869684a
602f8005-6830-4b77-abfa-6fa568883a59
4b06247a-7487-42de-9eb3-c94801ec88c5
d987b61b-118a-48e7-94ff-3dfbd228790b
fc7bf5c1-7150-43ea-a596-edf110e907cc
c83eecf4-bfb5-47bb-aa34-712008eba566
432441f2-84a3-4cc3-bbfe-c22ef2d5e89b
a02c054e-18a9-4445-aa06-2cc3aa1ee833
2588e573-c711-4fb5-9fa1-87b56c027e4a
a48b1e20-f823-439b-a839-0e23be4d768d
dd3da72e-1886-4577-9b5b-fd86eb24a27b
61484e55-9f6f-4ac1-8c05-e1724c623138
524dda40-4712-4951-a84f-44dc93533b2a
0e9c5b51-d73a-424a-9f1f-542402e6a95a
99637d6d-982c-44e6-8a46-108109269939
baa0344d-e4e2-4785-a5c9-26b5cccee3eb
c38be00a-5631-46af-8f12-e426c5625160
e4fcffaa-249c-4065-b721-021f57f017ec
deb1c596-aa23-49d1-ac24-d636fb8cf487
f26d6bd8-ea96-4744-b12b-9c6195b28b63
a915fe3c-8160-4c32-9865-1604804a7901
99d57eb0-ca3f-47c9-b7fb-feb2e20c6828
5f3a3b3e-be86-4f0f-89b2-d26a9470b954
461388e8-a3b5-4b11-b18f-19f396cb5b34
2db79822-8d6b-4655-9a0f-576c9e4f2c54
d2397c9a-9ad1-4e86-adb0-a807b44d8e34
18b5521f-2f9f-471d-b9e4-f4c6a7df29f7
a04765a3-8c1c-4d3f-87d8-ecaacd4cc3fd
80a00a56-d301-44d9-b99e-c5aae6c5dc9f
d09cb01a-f329-4acf-b960-a6294382a4bb
dff322d1-283a-4773-a662-46f20169e582
59b2b0b2-cde6-41a4-8188-c80e22083f2d
1d4511d8-e5b9-4755-98d7-96022de90f2b
cae56218-89fe-403e-a3b6-39dafd268040
42941f9a-71cd-4003-8017-38b3d7577933
b2a1ff69-604a-4121-ae5a-38aeca99b6be
a78eaa9e-5876-4618-b46e-77e628f3a281
bd058292-9efc-41d8-af40-152dbdd72deb
94f638bc-4a4c-4404-978e-27819b20b785
a3382d82-e485-4f6d-b5e7-d87883a549b6
cf9b992c-8174-452d-b3ab-95d415acd83e
05c80ac5-fa6f-4bce-90a4-4a29cc4b5569
dbf18c49-241e-480b-9fd0-bd0b9681945d
94c9fd1e-5251-484d-8581-e04730e91860
b367d1ed-4b17-42b0-b750-5cf0fd60a431
587ab22f-fe1d-4a57-a07a-de82eab9576c
2a305274-6044-42c4-8795-9e2efa564015
1d82e55c-770f-4fd4-8a99-4280ca05d1f5
cfc73742-1b22-4b5e-b276-ac83db6f2506
7bef34e3-3454-4567-ba8b-f25cdde3ef25
12aa3d3f-ecf3-4676-9a8b-c86ff3a6ce09
9ac38fdf-6037-42e2-a958-d2f470b49215
d0fc173d-8f6e-4d81-b6e8-f567d6a2ec3b
7ad9dce3-6dbd-483a-9fd4-710974ba1317
cce6d3d8-9909-43ba-b302-827519b8f4a2
742fb46c-acdd-4b27-a4ec-f025e1eec538
914d4421-decf-4b21-8e63-6737e19abf16
f79afdf6-7001-4226-a1f2-59a3d55473bb
7253f88e-518e-47c9-90ac-2809588e948e
97b22c37-8747-4167-84ba-0cf9ffe5a249
566e4fff-c942-4914-8531-fa1bc2bed367
ad04e8d5-e2c8-4874-a0f0-00e83eb8dbf0
bdc4b95f-e2ae-405b-8ffb-6a8bece85ce6
4056c815-a2d5-4add-b23a-b4e177a14207
70abf7ec-b94c-421d-86ee-bce29f08e98a
8e944490-d63f-4dfa-9325-40e7f6fa0d5c
ffc1f0a2-b161-47c9-a797-849739f0eaba
d0cd572b-3caa-4e56-8cab-a94fc9792f8b
f6d3a6c5-6af9-4edf-9e64-8c826e4b05fd
e1b8b03b-5da2-44cf-87a6-946fcb62ab27
12207c33-b45d-4b20-85ee-7d384511b0c5
92cc3c6f-5e7b-446e-adde-cc7081747f30
411626df-ca4e-4117-a1f5-ed8fe2de3d33
47035ed7-909c-4f6b-8350-92c9c533e0df
3b23ce87-9ed1-41a8-81d9-725c6b9c8fee
3206245e-2a6b-4512-aa07-73a514cfe4cc
5ecb98df-b15a-469a-841a-f82afb8663a5
121f6cb5-ff4e-413d-a2e4-d4b693552d6f
27965359-780a-44c3-99de-468a533b7b06
4cc37e92-e147-44f6-9e79-66988934252e
b8482825-63b0-46fe-8426-4332a608d691
281a8cc0-dd4f-4372-9bd3-27d0ec900db2
6d56c636-a5d8-4dad-b2e5-a82719d43e40
1193d5ad-8ced-4952-94b2-64cf86c19e0a
11bcb344-cf59-46c3-aed3-6eb9720ded83
83f47484-17bb-4f0a-9a6a-35f0c9387ee7
d09f92f7-dc8e-4049-a0c5-d62e694f7e8e
a6429783-b782-430c-8089-342456df4091
527defcd-caca-4cd4-af52-80d7be04618b
96a04d3d-7eaa-46e3-96f2-7c538a7ba6fd
e87e5843-5d5c-43d8-9b3e-9987aa16b6ea
84b94990-7685-4bd5-a50b-54e34bedeee1
404ccad6-366f-4c73-bed9-682b30ed54be
b08adfb7-03ea-4cd8-91e8-0d0dad4f26d9
a23daa39-9546-4ee8-ba00-7be4fb570df2
e850dc9e-47aa-49d3-b69c-809c6702f8eb
578159bf-cae9-488b-847a-66d89acc1e7d
b4b1504b-d780-457b-bbc8-2c28216e5f8e
9f199ade-0684-4071-a7c4-c1d37ca78329
b9aa9917-c663-4ba8-8805-809a9ea518ae
8cdaceb3-953e-4ec7-a1e1-5197abc81a9c
c9b63682-b8dd-4bdf-841d-a6f8716881ed
8330fb27-a16c-42b8-817b-d6e01ccf1fdc
9228dea7-94a8-429c-b7b1-7646fddb257a
b6d1e3b9-79d6-4481-879b-b4923019b2b6
a4e342db-ba37-4fa4-9919-cdcc6e9fc03c
9887e0ee-fd02-4700-9904-cb80cc65042d
1706e5cb-cef3-482d-b404-7c41a46a37bc
f8062f9d-61aa-43fa-af14-3a5135c841dc
1d5a520a-5f7a-42ca-8999-a4e7ac488544
8eb5cf8f-c2c1-409d-b262-c2ada1123012
1959841f-8651-4547-8d2a-39a54c10bfac
bffccdbd-7558-41d3-acb2-a2c386218981
38b46297-82c1-4e47-a8dd-3f717037323e
daa53822-ff72-480b-b337-aced8d8e29b6
301a6196-abb7-4b1c-9cd8-256cc76967a8
e7ff401e-61dc-4b76-bdb5-6d4d07e0ea0b
421090e9-e1e1-4b92-ac0f-186c57f728cc
7df1c198-4a2d-4b98-a3d5-48ace2ea2e7f
26385360-2588-4013-a7dc-a754e852e25b
d85f6dcb-60f0-4d21-80c0-a9ed752df6f9
eeff6da5-d985-4f30-be2b-d9d292716526
182b6669-dd38-43f3-b680-414e306e6d8b
0995d1f7-82a9-483e-8151-e9ad7b89aa00
95d218a1-4333-4682-a096-ae7e6ff1a364
3e92e66e-dc60-46bf-8369-d369bbb63491
e2451a38-03f4-49c6-ba11-6d52f88b67ba
97d4ac67-92be-4f04-b3c0-fe4c7a3b7cad
c4cc7949-77a6-4555-aeae-8b02bff800da
be08c9a8-cbba-443c-8c04-1acdc2a84804
4d17d683-2dd8-4ec2-83b5-4db30423d79f
78f081b5-6606-4420-bdc8-a5232f35444b
9a171eb2-c5de-475e-beaf-77d03f1699ca
d97aaf3e-aa18-4a2d-8273-c68ccce3a769
459c4192-0ef8-43e0-bef4-05fab5ab7d01
af654dbf-0260-479f-b020-2c8e9252aa39
90303df7-0d96-40ea-80ae-fc7154a683de
87af460b-68b0-404e-b23a-9702b0eaf0fd
3f4958a3-b346-4198-a59e-4385d8d5e58c
8d41d0af-c946-47ab-88ab-002a73935ee3
c754bc7c-c8e9-40fd-b719-d721a8a4cc3e
b5f1137d-7e8b-4f51-9309-c351f2a5efe6
ba645a0f-6847-44b6-b929-52954c9c7798
2dbc0cda-f0e8-4f76-955d-0f9c457e7b76
92ece8c9-99f5-4020-90a2-b05008fb925b
f6fd2cfd-8720-4a72-b127-0cdfbd67ea0d
16b5bf16-60a3-49f6-8eb4-6eea28b98bc6
89d46ccf-d1d9-4ed6-abfa-5b57992428fe
713ae9c0-adf7-4f8b-9c63-05e2d857486b
b734cc2d-fcde-4197-92e6-e3f45e9c6ee3
33337f6a-7100-4836-bdd9-fb0b7009e2d0
6a32ce2b-e6ec-401b-b902-d3bc0701ff4a
d9f35711-1f51-419e-95b8-fa43ce40310b
ae0d4426-3c3b-4edf-b876-7d1c6c8e2f6d
84ed10b5-821b-4342-bd39-4acb9d93c712
0c928676-4fa6-4a9e-bdf0-1b1e37bf0100
7d6d76a0-1d66-422f-8d13-51bedd0ac15b
b9d09a5b-5e86-4721-96fe-52b1f6fb78bb
11dfbf1b-21a7-4f02-a9c7-80a74aeed977
856ffd5c-0e38-4a65-9424-e0e0e08a333e
f93fe2bb-3216-49ef-9776-44fa1f466a39
746eb726-6dd2-405f-8000-303e4623bc62
dc7945aa-2510-44e9-ad2c-8acb43c1013a
b9ceea27-77f1-48bf-813d-155670379123
e6543396-0781-4c7a-b6db-9f5fef166863
e06663d6-84c0-459f-b39b-291327d50233
a7bd311c-af56-49fa-aa9e-cf08d8ca756b
712fb553-9d04-42d9-be4a-0e6b6e750a60
0f845caa-8498-42e2-b2d5-8acc4510bf58
c74fb601-19a4-45cc-a2fd-3bfca683c5ec
5f70ea52-74a8-4d3d-b10b-80319ca15eb4
1f1a0737-ac6c-4eb8-822e-055b5ec6b988
b0ae32e6-7983-4f34-b48e-ae3fa81178e9
42a5b08e-5bec-4430-a8da-c22c0751fc7a
74c504d3-c6cb-41c1-9358-db76089cb9aa
e7a4da5d-d063-4048-85b1-b3af50bf6a84
cec08ade-ce1d-458b-b6a0-496e0c99f27b
539201c3-2d6e-4d4e-8190-4adc7c59944e
b85669d0-1a68-45dd-ba84-a36c75029505
84281e1e-330c-4461-8e92-800851760854
b44745dd-7ccd-42e1-a1d6-c89884fcc36c
6aab95fd-727e-4f11-bcea-aca1b88d85ed
3fdbaa05-7e6a-493e-aad8-1e37658c233f
391e71ab-7725-462b-97de-bff19be1f9bc
8959c374-e75c-479d-a019-f97305e85751
a13dae6c-f50b-4b5f-8e9e-c228fadc8f7b
0bf3de48-5df7-426c-a27f-a0230105d7de
dc7f2724-5f7c-4da9-9b31-4e025daf0f7f
0312f950-3429-43ae-8497-175decfbf9b0
ba16cb08-6f91-49de-9cd3-c8097fb3c235
1a4dd719-8ae7-47c7-b126-dba95a8634be
2ed383b2-7a08-48d3-ac06-d371743d1a87
d5c2622c-d362-4947-ae61-bb1dd3f4ee57
d12a92cf-3a74-4329-80d0-b57935779992
3cca3fd1-4642-481d-be85-24cddd95a958
909d5101-f20a-46bf-954e-c9249df5873a
5fe1c810-cb67-4879-b4b4-5e2e6226d9e5
dae92d7f-7c84-4fc4-9b57-508f302462fa
2161febe-e459-4508-adab-9ed5b16e5d9b
28e452e6-2d9f-4169-80f0-cda504508478
974b9064-625b-46ca-ba52-619ea1e79833
de1b7aee-a5af-460b-93cb-740cb1aecc04
918b79e7-2781-4d57-bc8a-f6a194b17994
54c9ffb8-6954-4237-9218-939feb33836c
1334b734-21ff-4fa2-b0a4-374a166921ad
3841648f-def7-4c23-bd58-890890121643
4a7f1606-99ca-4d2b-989a-35f76782d868
5e9f6ee2-e69a-413b-90bf-98410e2bf322
e2f3663a-4e72-44ef-a7f5-dd37f7362a69
11bb9946-0c76-4548-972c-d939ffea67f4
1d9ac400-7c8b-4255-8dcc-c72343e14fd9
e1160879-f4df-4690-be6c-115b0c48d2e6
066fceec-9806-43c8-bbb6-8189fa07b56d
e4b5596b-fd9b-4a6c-bb29-b6aa45b0089c
fcd029aa-f68b-42ae-ad24-5658a4aa3868
70c584ab-2d2e-4dc3-9948-cc9ad78e32ef
4daec5db-3bbb-41e7-9cfb-6e2b500573c7
681381ef-49e6-47de-a87d-277c9d17441b
368e4064-d97e-4ab4-8a07-5f742b0d24f5
1513e583-4afe-4473-80c4-2b34a2a1b494
dfa12ac6-3dbc-44c6-a282-4beacca9a30c
8b49d76d-cf6f-4e45-a08e-18df6306d85a
e4e1b2a9-935a-4523-8151-66e7a0ee7ceb
1dd3cb0d-8b14-4cf6-99ad-9e1961389106
ed6a802c-33e2-4741-8bee-c00771718524
957318a2-8dd7-4df8-ac0a-362fc4fb5bbc
2c0819b4-b696-464e-8dcf-167e7e1f5377
bd725302-8cbb-4dd0-aab2-3f13eadd986d
c4b49aef-6a0a-40aa-8df3-ad57db3f342e
151a2b3e-952e-45ce-ba24-c4af5bffa824
c71b47d0-1d80-460f-9e5d-d68c8439e7aa
a9da6b1c-7e33-4c8a-83cb-bcb17a688e21
f4ecd373-29e4-4281-9784-ca361148893e
c25a1d36-7b5e-44d8-a8c7-27a3ac185f29
0b7b2062-611a-4d54-b503-87f291ef132a
170dbe95-5836-4232-9064-751ac8d5bbe6
cf0b6cf6-7d94-4011-a0e9-7eb1119411c0
117c7f9d-39ef-4821-9633-0cbcf4e1477a
6cbb1df0-7373-40ae-8ecd-473dd957b413
08e3932d-de59-48d5-bdbc-d5c5dad9f5b2
c3f34943-e600-4c62-b0ac-9cb8d0b6da0a
d2937630-16f2-4876-adea-c2141bb43dd0
7159cbe9-e1cc-4d8a-9345-93991cbffc9c
6b79e2d5-43b2-4830-9a63-a3046e2bb44c
f459f906-e6ec-4d1c-b7ba-f534a2ccc85c
ed35ef67-a272-44bd-9eca-9519c8da2f6d
4b05393c-0bbb-4ac3-9de7-73b98e937921
63c9d107-fe3e-4177-b984-794ddcb508b2
cfcea824-a5f2-4d11-8da2-0ca3fb0879ab
29e3b3ce-a608-4e3f-9367-d5de3b9ae39a
8e4bfd6b-c70f-40b5-a56c-6e848bafbb93
65fcfc4d-fbfd-4119-970b-4c0cdde87f7c
942ec422-f19b-4ee0-9eb9-898ff5949279
0d5a99d3-14ac-4f7d-9b1f-36eb90dcb71a
7f46d92f-aa32-4f1b-8269-20245ebcbc8f
24b259b0-5073-45b9-9c59-cd499329ec3f
ed700b3a-cc27-49c5-93bc-138f6b69bc37
ba8a66af-10ab-48c0-8c99-5c7517de4b40
6ecc7e48-662f-4956-aef6-3b97ad1293ae
4a5d0187-59a9-4391-9e59-a4713708112e
b1eb97fe-61c0-4b30-adfa-ce97a68dad8c
002b5604-7143-4484-a232-d3262f3a787f
c1151970-7b9b-442f-9dc1-d34c9c0e2ef6
f3651773-35a6-4cfa-b5ca-3b0b97356ca6
36c1bbd3-2e90-4527-96ba-7998e8ca47be
7c2a954e-1977-49fd-833e-bb4772f5605c
c83a072b-b0db-40ef-ae4c-a8a940d4209d
d51fec58-2b91-46a2-9017-316b6c7d0aa6
6497db32-8a7f-4841-80c2-7394d7949184
0e6664db-0380-46e8-9056-607db4351e80
1b0ada06-ea35-41d6-a1ec-ef289aad6067
9e0def2f-e09c-4a95-8b49-3ea6c3768b06
1cdd92e4-2487-455f-83e4-663d9be5119c
6e1e0663-def9-47d3-9065-82cad05d035e
f4746063-882c-4ac1-97d0-a5434e06d251
d12ae4f9-c9cc-4012-a7d1-ee9fc554f62a
7876f1b7-b80d-4733-8a31-96d8a28861fa
1a51d777-63de-4e89-9b78-034917aa85c6
e112f0a9-ca6c-488b-8f04-e3dbbe5d820d
4b2ff564-05bc-4cdf-a324-1d4bd649696d
7bedac1d-9bb7-40e0-86f2-618eca836459
606a4427-91bf-4c84-8034-309a6a4c6544
e378011a-bad1-4c32-bcb3-4ca6503c36e3
bbde37ed-3a10-4211-9bb1-66ea327d8274
26159e6a-41b8-4db2-a20d-b74f7d6a19cf
e35d94a0-b399-485f-8252-62195253048e
f65e888f-c863-4370-9997-3bceac8aa525
8726947b-a4b7-4c04-a0ec-12b498aca265
f85dbc17-60fc-44a4-a247-3683dd8f0692
c97557d7-318b-49ce-91b0-e2ad45896fa6
98d64f9e-b3de-431c-acfe-f5b13c64c15d
311c5847-23f7-45c8-8896-b726a3b3488a
a88ab097-1b90-4fe6-a227-8ed0dc8a2801
5d99f9ed-0437-4d20-9c3d-c268810d5287
b55b9baa-5fd8-4ddd-ae0e-c96fe8f08aba
f8147676-ab10-41a7-b7a4-d776a9c60b6d
32fcc19f-551c-40bc-bc59-0b5268842301
13228119-c9b5-45f1-822d-d395457eeff7
1b92cd65-d8a4-4bff-88d3-35f6ebf48987
199fef75-5f6b-4e95-9dc9-089619ebe001
44023dfc-3b3b-4f02-8aba-02c80c97db08
0f3c5f60-3003-4141-aaf6-cba9769e1ab1
369cb978-0652-4a25-905b-de0c7ca21100
255635e9-1239-49f0-8d6e-dcfe52635df4
75cff09e-e1b8-4fb7-8e32-8dacf2ec04ba
c054ca9f-4c14-4904-8d3f-0af347fdeeb4
8d855ade-1621-4098-8276-e5e52a7df276
6ab11083-efbd-4627-8361-5c16151bc223
1ab2c117-7eab-4451-9b94-b3453168dd99
59665a6a-7f50-4e09-a550-ab280fe9f550
aabb7cf8-843f-4077-90ef-84c8c341676a
f4ac2343-2305-478f-96f6-2dd6b2b1eda4
b3dd02fc-8fb2-4c6d-a8a6-6882e84e3f75
009f2c8c-33bc-4c71-9ce4-6908452896fe
a3461a95-147b-435b-b73c-32490a1f53ca
158ced8b-0bd1-4e35-bcd9-c6d647f53575
bf22af7d-01c3-4226-b586-3a30acc81c6a
724a1ae3-10af-49b0-b51b-b9a3e237dfe9
0ad83477-e5be-4473-a062-df0713a14058
84606526-742a-427e-ac08-29c63767f683
340d6390-3795-4870-b2ba-cf4704335190
917c7da8-fd36-487e-9321-f8956c6cd954
92272498-8ccf-416d-81a0-f3725e8baa6f
c2be4303-2a61-421c-808e-d8bf13ef9bd5
9bd73781-6912-45b1-aa94-95fb4dab50ef
3831bc20-e1a7-485b-a333-514798afa2cb
1474aa37-a3ba-4fc1-b4a5-37931f85cb15
ac93a37a-9fc7-4399-92ba-856c1471a0bf
f24285c1-3638-491c-b717-103554a3c9e3
61d8c7e8-2868-41a6-97dd-4045fc1ecd75
81ee8474-07ac-4215-bb07-2143a4ff3bc3
8d9fdbd3-a242-41c9-99f4-8193d22c4d24
77ef61c3-fde1-4dac-aa82-9673f5d31f7f
8e88e326-2d05-472f-8daf-dc395f352791
0517d2d1-a395-4631-a8b2-aac564820a73
a453bbd3-0209-4677-afcf-19c148c5e45e
c362845c-7072-4888-b72a-d56a66334714
67876fae-76fd-4bfa-b443-0a6d0ca78480
a90dc545-3a52-4399-8c36-74a56c3bf79a
ba6e526d-968b-4385-aa93-39b0947776e8
543c5644-ed33-4dfb-b333-b9126578f584
c77e8f70-f876-418f-8419-6dc40f38ceac
08850ff1-6165-406d-80ce-37c6682b5d06
1919552d-41f4-4b54-ab7d-74ab6381abe2
6b81aeed-b4d0-420d-99d8-14b156d6e6a0
495b878d-64e4-4617-9248-a6d5ab03a5fd
933121da-1fd6-49c9-8ff7-8eda2d8aaa8d
2045caa3-7190-4552-aac0-ca1c34e082a4
330a17f0-d639-4dd1-8496-6127299ba0b5
5dc1cd63-61ec-44d4-a185-0b2b0dacd83e
b6f696e3-ddf5-4d89-86bc-32e4dad6f017
c39b4fee-144a-44cd-97ad-07772e9ca89e
d24e47cd-9821-41dd-a61a-06b57a36bd38
05aa59db-f53c-4d95-8d12-10326752de87
358d8606-75ae-4319-873b-58cef459d6f0
875345a8-3aa7-4ac3-a806-0518eaa09000
d0c0a130-7ff2-4247-9165-6bc31b99f7ff
c0934ac9-b605-4b5e-93c2-89cb2dd32413
e6f49991-0dcb-4067-a38c-e948338d1207
8234373a-1064-47b5-9167-5bd96f3ac1ec
d4104759-6474-4d4f-a78b-243fcc780199
519e890f-d4b7-467d-b77a-747ed55826eb
06b249a1-3243-4be3-9197-d6918f6622d1
005442a9-230d-4915-b658-af27319f62b3
8514e660-2e03-44d8-83d1-7caa01f16f57
bf9f74f7-b74c-4361-ad94-7c9328bf904c
3985e053-6176-435f-b82d-beffcb4395a0
80ab39be-d28e-4caa-b1ff-72995f7da356
d7e7d89c-0142-404d-93d6-6de342b86356
3cf8bef4-2336-4fba-9172-c48b7f60e2ea
25360359-2ed2-47f4-9f8d-e80f6c1498a3
939bd321-7ea7-4d21-949c-24e43a40113a
a7870609-59db-4f7f-aab3-014e80cb2e9d
3ae03557-cb80-4112-9e5e-1a65afccb493
2be03c60-5195-4dfc-8b02-19d207ff6989
2563896b-63f5-49d1-b111-9497933b08f3
ad5b9588-01a0-440b-a5d8-b26198a1fe4a
5c9e0dcb-f802-45d0-8530-63f39f90e112
032996f2-9665-460a-9acb-e36187ce1f7e
2d777d22-adce-4ac7-af74-ae2365faa7e5
bb0e4991-b693-4bdf-b1a2-6b54eceaf5d7
e253267c-e05c-4412-a204-38e5e797266b
9ee449e1-ea12-47a8-820c-471275c8fb13
66fd1991-8c38-414c-8919-d34adcdd30bb
103a0725-7a1e-4a63-980b-c0589d4da0df
aaff6c17-2bfa-4072-b825-2d9c07170b5f
b3c29956-e261-4e87-91c7-cad1dceff073
8c929c06-f771-474d-9059-0e7362a04633
037d1b50-63b9-4314-8e30-89c9cbe87f25
a0732d4b-75ce-4c1b-bdce-2b441ede588f
4b77e6e9-78c3-4f44-a878-8b8dfeff2d43
82ca7dc1-f2cd-4ea9-b4f4-e2ef083db3b5
b3922423-6770-4815-8544-820b3fefcccf
0edb4085-0e9b-46e3-85b8-2aea7f50f71c
2f64d3c4-fc41-4ba4-a672-d21b3a24ea11
939dba63-9021-4edb-bafd-6972d0a0e00f
eaa1ff2a-6d43-42d5-b538-02810b30d9a6
bce23668-3f06-4049-8446-976bc0a07180
5a2fb41b-9452-4fc5-9a74-55bf7da46e65
8f1e0f78-3bb9-4025-97cd-73dfe5d75814
3b299392-d5c7-44d0-80b8-404afac6f6d9
e08e3c60-b9e4-474f-a930-808b1c381ef8
9d730ec0-7d39-4a7e-8d7f-fe94ea59fdf5
186d7771-6097-4cc7-b970-514f2ad5c1d7
c83ffd6a-df04-4c6f-a0d8-84fb60d2754c
15677904-ea3f-4321-8389-5ec7b82c4dc1
55eccd28-268c-410d-a531-d567824d90fb
51f28b0b-1a44-4e10-b194-bb5f0d309a69
65f97348-1b57-4f76-ba0f-2eac64f72c8a
afe49d6b-310a-4c26-9c52-052a7ca9dbf6
97ca2797-400d-40d7-9b51-baf85b5a9ea0
32119b31-0622-4386-8bec-f1a5f7648d1d
20cb2cdf-aeaf-4784-a0ec-e72d30775136
7379af26-1220-4728-9067-786b82755a40
38f8f01d-1f2a-4577-a09c-760274ffaf69
05194708-861d-445f-8cc7-aa6ba5b6fcb0
aa717e2f-fa01-406e-9028-3cbf36742c2c
29fe6438-165a-4676-be51-7d76f1b3cce4
4bb4d350-7330-4556-82fc-57d953a842aa
1e397300-f931-4b98-aa1d-12493549d8f1
2a8ccddb-854d-47da-bb5f-06eb9058ebc5
9cb89b17-53d6-4f3e-aa61-104626e066d9
7f968dbb-fef3-4072-9d87-59f15a4978c4
1ed4e250-23af-4593-a0e7-4c13f0957eb7
b71df9f1-4db8-4f5e-b1ae-02f3b8877fcc
788876ad-e57f-4bd8-a404-ff4a397bbb80
d8097713-36b8-4569-8145-0b77741781b8
935cd059-fd75-4f19-8e87-cd4a3c1733cb
dcd6501f-bd04-458f-b249-d9d3aad5a5e1
9c291fa4-f716-4229-8918-8cc9334aba95
85c2bc0e-eabf-42c4-9514-5b68506223f0
b142bf25-11fa-4e38-928a-f3e2d628b2eb
26fd15aa-dac6-492b-ac73-dea712e50de7
3b574012-442f-4894-abb1-56e90b4c58b9
93c15084-d029-4d85-a224-700b0dab1dd3
700d03f5-c4fc-4723-b6a5-bba654869c13
fb8157ff-8278-4504-ae2d-6435652865ab
75951206-1469-492d-a012-185700598412
9a8de978-3b47-4cba-a009-9269a2d97050
443be389-d918-46c1-8f00-35bbd521751d
42186333-05e2-4ec6-9ce0-b44722fcf4bd
83eddbc4-a42f-4c8a-bdda-ff483ca08b64
ce2a4386-b3aa-4c26-bc32-8e47ad52b976
d083630f-d9c2-4742-b674-4840f730a54b
671449d0-9afc-4323-a132-3cf5ae423567
ae724605-966d-483f-8b58-75a83e634e5b
22a52a2f-beec-4e04-a311-a0fee933b854
627fa6a6-cae0-41b3-9b4e-960db9048ce5
81e8012d-b934-47f0-a4ec-7c92bb80a7f1
c9d02e52-96f1-4e99-b16b-c40dcca9c5c8
a996cc5e-ceee-4b99-acf4-19e621d619f9
bdbaf6e0-e4fa-4285-9475-615c49e80c0f
f6b9bdb1-98d4-492f-ae02-cde208f30fa5
052dd4bb-5bf6-4062-aaa9-2920e0106642
419c88dd-9c6e-41c1-bbfb-6b4a56517d19
0b48e45a-b295-4db5-a5e9-4af78ce3e49f
7c7c34c5-bd0f-49da-a8f8-145236ad6472
558cc27f-0465-4592-b2ab-3478a1c6e4e4
c5518125-fe3d-41e1-86a9-cac24fe850f7
a35cabba-6cd3-46e8-b8eb-f0fccea25cc8
8a2ef820-5080-4afc-8133-e8213c2ab03a
af6e5fe4-d4e0-4ba5-9502-997099d22bba
86007451-606c-49ae-8521-df734e4461b3
0ae15065-539e-4e57-9251-f08b15980ff0
3f756259-f07f-41ea-ba0c-171a4bfe3012
dd120ed2-9e40-48ec-bac1-8dde228d0a9c
da4abd70-9cb9-41ca-b655-a0347c4bd675
72cc46b2-42c3-413f-96c0-191427bc48f7
1e44611b-7cb1-4680-8e03-6dd2dca7b525
9c5b3357-c77d-4af8-8c02-9d22843d87e7
382df2e1-2a8c-44f3-b697-7b432e3748cf
de76f0ab-bcd0-405b-9873-e4b17f7b43aa
0211c51f-34d7-48b2-995f-d8e1d438d6e3
b618240f-cafd-4ede-8855-3b6a915c90d5
802d634f-81a6-40f9-bfbb-280be96c4ea8
4a891e30-3eee-4186-9be4-3b12bec1903b
d9a8f9e4-e764-4ae8-94a5-b610e32867b4
f7745cec-76bb-4603-acff-8dcc4002a883
236b8b1b-8afa-4bb6-ba6d-a45a3c5c162a
65ee8460-eaad-4060-925e-0fdb9e66b39c
15efa840-16c4-43e8-9ab5-62e6517babe8
5de691d8-7513-4c80-acad-3b5d0461dfa5
0c4adb87-55a3-4072-9232-6d7d69e04926
777f1667-3b16-4485-babb-492650e69409
5f6bd688-d476-4d39-8893-2604b9572185
cdcb9f49-dd11-4bcf-9e1f-f5eb80f19aac
9791e0fe-4838-4cab-aa78-74243e6b552d
a215c3ec-827e-4e15-93c9-00492a8a9ef2
d1665c23-d8e1-4a35-8cd3-982c0a506918
e14a8a79-4876-440b-adf9-3fdf18e00385
329f731f-e502-423f-89e9-5b75c79a3783
f8fb8037-a209-4e24-996c-04a9122f1634
850f0d48-6292-4f1d-9481-cf762ead060c
a1004a5c-115b-4d7a-b97f-a4500cc2e319
903e8acc-19ef-42eb-a6c7-8be360236a0d
dd4c0dd8-f3fa-4fc5-a793-bcf04c2c1a8f
2ab9c368-f249-4af7-80ea-3f5eda0dfd85
60281978-7ec0-4103-84c2-56e06365cfe4
4b6fc25f-3d40-49cb-bced-0cec9383f138
8be72c8b-a09e-49c7-a891-44e1d73e9b44
695058c7-0d9a-459c-bbe5-12f1213cdcfd
1c7e2bfd-f0bb-47ef-946b-769c98eb52c5
6ae93a1c-99e9-4510-9c63-ba7daf5e9e00
3e54a340-96f3-4b9b-9b4d-65cf5d1fe063
a6ae4527-99b6-474a-966e-4a65cf071527
f3563aa8-c4a5-4fc4-8e7f-aea616fd7726
ecf6a255-839a-44e7-ace8-2c2164901ad7
e586f7e6-c6bb-4f54-ae0d-be926152d8f1
4f23f7ab-5bfd-47ce-8132-c014f0711e46
b1cd13b4-8885-4f68-ba0f-d3d177521f08
ec0bb8fd-b26c-4c45-9804-ad4a264e6441
61aeea18-f040-4314-af32-5c58a04401dd
56d8c7df-5900-4362-9e4a-5d11d4a8b884
3b3b6906-ea39-45c0-b6ad-1b1fcce87201
2afc2e3b-44e9-4ea6-bb62-fdd73954d4b1
e2549530-b706-4ff9-adee-048ec775d8eb
dad0ee17-bff3-4cad-b074-4b06468a46f2
71c7c9b2-5af0-4271-a0d4-fe6f9374186a
41979d09-98e2-4d7e-bb93-c2930f0de774
1e5eaf61-3b26-41e6-ab4c-ace068eb36b1
f219da38-d085-4f3b-abc9-3c5e85c2a882
c32d92fc-f1a3-40a8-86be-1075c5292125
d3c53f48-7b7e-4080-969c-f0a139172aaf
716e9bf8-27e0-4a1a-9660-b52b6f475aea
da7826d1-b44a-4800-9f7c-edc2febe83a0
3f412d1f-6a18-40f7-b235-72c88b41dd9d
99b81acf-d52d-444c-9e0f-d3838d617f45
2f3c0822-892c-4be7-9dd7-17d64ddbca00
7154f7bd-abe9-4176-ab57-32468bb63453
e99d64b5-983b-4ddb-b137-e538e6576d44
3031c665-9c85-4209-9552-1fcb98e78d26
35d32b8f-578e-4250-a275-2995444f0047
ec608442-027a-48cf-b9e6-a216d3886bda
b92815e9-162c-4bc5-a908-59ea74826315
e89e8681-b5ef-40fd-aed4-eec87167b0c4
ab69c0f8-a311-4705-8a74-68af7f4efcdf
a53b1f8f-badf-4932-a197-287d816bdfb8
25c1ccc6-7326-4d2c-80ae-885dcbd110c9
0a6298dd-ca94-45bb-a766-6fefd248a545
e4f01428-626b-4fe8-b81f-4eb8c495def0
d8663b2c-4c3b-434f-b749-95eee71413a8
f4bc13f0-24f0-4067-8fc0-08d9cbbf8a75
45b73a55-f1cb-483a-9ef9-5844a2552e93
52b1557e-0b2a-480f-a3e2-a6dd0c87598c
22cb7e6e-3ddb-4d41-9c78-fcdaf056c26b
3fcf9879-60ca-4888-b8e2-dd23806a1c22
733f4454-307e-4898-9169-42e2a68c3b88
58e1664d-dee7-474e-9a87-9b09b07ce0d4
e0c98b63-5988-469a-8a1d-dd80201b6937
6fe99069-eaf4-4791-afbd-e2fa2b7eb8ec
22cc1ed0-d078-4830-85a7-b5ee5111ea82
da0030f8-4373-40ab-ac72-1a2700e88e41
c06678b4-ffcc-4e7e-bc11-de21c5ff7cf8
0e33f0c2-2d15-452b-b816-bc55dd0c383d
f8edf561-3926-43d6-a8af-3eb2e7b3fc81
542bec64-187c-47b7-af3d-b26f27c42f11
e7c4a55f-d915-4b4b-889a-a679e142ca38
40715cd0-8e9f-434e-8227-946e29b64e47
2965f8d8-f53e-48a5-bbc3-679b73f952d7
5c0579a5-aa45-4a7e-9cd8-4544fbfc4102
6ebfd416-a596-4be4-b378-c396312e01ed
6c4b57f8-b76a-4584-81ec-c83b17718b51
5f5357dd-55fd-43cb-84d3-5c740fc5f418
69c7055b-87a9-4acb-bb28-e61bbfe67b8f
ee8e0afd-7334-4004-89ce-a7d924ec4913
9944de09-dcad-4c03-abca-10760d324e64
e2638c04-7453-4bce-aa53-e190e2a3aba0
8fbab890-c8f9-42ef-9747-f6064dc047a9
066b4618-8f22-452a-bbfd-6a1c70c722a8
7d0c41ec-7155-4da3-b5c2-5d59cfccb7a0
e191e39e-b898-4445-8cae-48501e151cd1
73f06690-1637-4feb-bbeb-f418954118b4
13a97177-5245-4c60-b2fd-37ed01fef3e5
d67522e2-6041-4e95-aa73-643a0ef25eca
b0cafc0b-f1f9-4847-b908-365ad318c87f
e8b95e5d-274c-4a70-8b4e-b0e1c63ce860
ccd4a578-0c2c-48ca-b6aa-b5e5a28cab41
095ab483-3578-4df7-adb5-ee476de45215
295c2cc6-abed-4577-84a6-a5674e812128
6f7c208c-2c36-498a-aca6-093bb3f6af08
b9d76114-2a89-469d-ac86-df1eef201369
1c800bf9-6b60-4031-b492-907a3760b29e
0b064f3b-ded3-46d5-87c4-074ed6d4b26b
ce9bbf16-5d92-47b7-8460-71aa63cc5c0e
278971e2-18e8-4fdb-ad53-011bec41bfd7
f293251f-906c-44a4-aabd-dce93757f542
315ab0cb-4730-4e93-83e9-256e26280c8a
05d3db52-eb72-4ca0-977e-c11f395a812f
98577af7-6ce2-466a-8976-1edda3053ed0
892ef1af-5e99-4ed5-9be0-7c8b457cb1c0
f55c49e3-eb7d-46b7-a7e3-0f1139217e19
e2b22a00-2dc7-409a-978c-667c9b71238e
d6ac7994-81df-4d0d-87eb-3ec1c6a4f84c
76ff3806-19eb-43f8-8e9f-57a1961ba01c
365789d6-e1b7-4b78-ac89-083eb1079c65
8bf71fe3-320a-46d1-a96f-4fb13362c397
c7689fe3-9b5e-4780-ae28-6eec8d62ce35
e5c2fae2-9d56-481d-a143-32f39b6fed85
7d383617-abe1-44f3-bf61-6e44be341f1a
b2e3c44f-f78d-4174-95fd-a6509481e0a5
603b9fac-68b1-4f34-a670-706be4a57d00
5c0723d7-dd40-4f0b-ad6d-4a6d2d8dd4c2
75c23a26-60fe-493f-b1a8-f6d2fa60ebeb
3f2f2fa4-0f86-4a0b-b1f5-09325e39484d
7f13e7f4-26c6-41ae-bd21-2c6f9c3140ca
d4f5c266-2ccd-4f2b-bdfc-4fddc1c2632a
aba21757-6d8c-4dc8-ab34-2e4496ef7c41
eecacbfd-9838-4297-a284-dd00deaa48ef
7bd6649a-de1f-48a2-9675-30436a211d5c
4a3860c2-807b-4838-8e2f-2ae28138e654
f79d5abe-4d39-4400-8f9e-c4121f478428
81957629-c61a-4ea2-83a3-461bff1b179c
88c12d79-92af-4abb-b17c-8b2a0dab3977
861eeee3-28a2-427d-9e3b-9a582d4344d9
e2e1a2cf-d3cb-431b-aa09-d634f0fca528
83e3e58b-b234-4205-bdf6-6ae52564e707
5642b5fa-fbb5-4cb8-93c1-143f9c65875c
f9b8fc42-976a-4d89-aa50-aee0801a74b3
8f700ead-b896-4c64-99ab-ffd3b087c6d1
c2b9024d-91a2-4b50-bac5-0141e5f43703
924a0432-3691-4cc8-8dd8-1c88579d9cd8
3a27c058-6a15-492f-9df1-b485e0487a38
19c5cd27-2d1b-491a-a1de-0b0d5f37cfe9
9ebc8791-dc32-4ed1-8819-761e14bc6bc0
a457c1af-9547-470d-9428-c6088c12ba70
7650b85d-7765-4615-8e26-0da7c809cc90
7854a35b-12a4-4f5e-b1ef-c5e94b63aadf
417bdee4-a764-4e1f-afa0-694c497ffe7c
65118a16-7a9d-4443-b913-0ebfb611e635
f6ecf2df-af4c-4257-a943-e422c1588800
4ec09af3-28db-4c36-980b-0432c0d870a8
ad0bed77-7655-4636-a61f-892e3daf4175
f5a6a7ab-3596-49ec-be93-2ba09397cc7f
0c9410e5-10da-45b4-96a4-d7ae373adba2
b72d8640-49e7-492b-adb4-c5af68c5a624
59aa6c45-0dc0-47ed-8469-c45bb0a6a308
b34c332d-4daa-4b75-9509-698d6e4f0e57
24d92477-cd84-4046-b532-9b4496428112
5c97d9e1-61bd-4d9a-ab04-8382e0d324d0
121eb483-5a10-40f3-968f-8e1da5549868
dc2e905d-402f-4120-9feb-7a2f6edf3ef2
ab1fa7e5-c071-4904-9c81-a1a0dd979adc
884c95c7-83c7-4556-9d58-528bdf3a43d3
a74127af-45e3-45f5-9387-2c90d127af5b
5186965f-b7d0-4be5-86b9-6230f915024f
a7469f75-af0c-4212-b7c7-0fe86e2ace04
63f929aa-6b1f-49f9-afd1-fa94578187c5
c8694e0c-e731-46a2-bac5-de836691d695
2c02771c-0eee-445a-8d6b-cd093efffe33
745c192d-a3f4-461d-9a68-344fe8bcfb05
d2690e98-89a4-4497-b0ee-1fa723797d96
34422a7a-bf8f-4a5d-9510-c732ec052f55
3aa5fd42-285c-482b-98c5-856349b96978
e55f0e09-943a-49b1-8014-c07aba3cec6f
3bdf711e-8ab9-4647-aa41-17b76168bebc
84cb45d8-e983-4ec7-b9ad-3bd39c19252e
79500ef2-802d-48b4-bfa1-b733c2520f58
411fe9a8-e93b-40c1-a8cd-f26edd6743bf
551d7c7a-91a4-4f39-a92f-c16f889e064d
7bdf16f1-9a93-4a6b-b41a-cbc9e3ba8d02
ba25fca1-ac6c-4bd7-949e-5dbf5a4df9f0
db7b5e7e-a526-4d43-bfc3-74cc3c3402ab
94d86585-40ca-423f-b0eb-467c5e70a12d
fd97370a-7d17-4266-a2a3-885b5e4990a2
93f6a533-b971-4705-bf54-f9890689b466
089cc452-c1b1-4018-b08b-05a56faaa44a
8263e969-5a30-4ed9-b5c5-95d21c643b6b
da9a64f6-e7b7-4864-8f9e-84b0d129abe3
f37c3be4-ddfd-4ec0-a8dd-9593e6c3f7a2
e2113f94-3800-4f32-a6f1-26d0b511a11a
341fde5e-452c-4b86-8f50-ae331b749868
368a08fb-ea5b-4b56-8c2c-9cd32ee3c5f5
a670a3fa-deb3-43c8-8a7a-a36b976397c7
24f19f43-9e4a-4457-95cd-e96fdff06ed2
a463ded4-2bea-4ffe-b23a-a7ad4cd8e692
927a16ed-b334-4028-9bb0-9db2a739c8ed
c4071cbb-132b-4e89-9ab4-4a3bea3af438
9fca55b5-d6ed-4095-8e59-3bc0d69ea039
1c172277-38ba-413e-9822-47f69295e3b8
181d4370-a071-4375-83be-144f1eca8813
fd90848d-e260-4cc7-8bd6-db38bf588500
b57ef0b2-9d85-422e-b7e6-cf63cd4b2c71
39f5993f-30ad-4dd7-809e-fd12644a4cf4
144539e5-af81-41bc-8778-526137c56585
699e2fb1-17d1-45f9-aebe-49a3bfaae093
4525f283-360d-4feb-9231-afac6f67e379
40071915-3ca4-4574-b25f-15001842c8c1
aeac197d-34b3-4bbf-b8a2-48f9d569d6f0
66821d43-fd06-4799-aa41-8a4642a563dc
5e58d5aa-e8c2-496e-aab1-6db17d5f4fd9
1ebdfde9-b908-4109-9cbf-a0d1322a1adb
2c9a2e6d-a317-4b46-93d7-4339c5e871ed
a90935ed-1687-4cc8-8397-68622bf95e3f
0e87db96-4b34-45ec-b0e2-c0bc99c47d95
dbc9beb5-58eb-4469-ba2f-5448914a68f5
2fc7236f-943d-4d8e-aa81-73f10e1c5e0d
79551bef-4c73-430d-a5d8-710f64f13b1c
a69e8ab9-7897-4767-80aa-0e2f8cdcf1d2
235104f5-46c1-42b6-bb60-064aa090bcc4
4cd7232b-5729-418c-bd2d-f22439f69dc0
18da7544-0396-4b2a-9769-383e6fdf82f3
e0ceb921-f2f9-41d8-8217-bcd5699fc80b
8d4f3951-cce7-497b-a4d8-ae047340664c
aa9cba4d-635a-4344-8180-401a45b65a2c
89819fc4-4104-4a6d-be13-d3ec2e1b3c77
949c95bd-cbcb-4306-8dcd-04a2eeb44beb
fcf0c35a-3ccb-4530-ba29-887d0545c665
269dd058-1eea-44ab-83eb-3a9ef10d9e34
6d25dea1-7688-4950-b6cc-553933cf7ffc
d178c647-d0e9-4478-a5a4-be5efde6c8d6
4fa294e5-0b19-4293-acfd-a69dac8ab0fa
681d7fb5-1a5d-4bab-b621-3810c32e6573
ff30ca88-10f1-44d1-bbdc-f50a73e13873
08f718bb-05a8-4fe6-9418-7bd012c416aa
54f8db0e-b322-4e0c-81fd-9a4eeccc227d
93d42339-2863-4fab-9f0c-dd555f97fe28
0ea02168-463c-4de4-b71c-803445e3d12e
38fdee7d-af41-46c8-b532-a8b2778de200
5f090b7d-e18d-438d-8e7e-48bfede3b64a
4030f474-5300-48d5-96df-f11fba0b7c89
459c1940-3039-4e0d-b9a1-ccd1a61dea62
3fc55ba9-3708-46b5-a37f-f25a9695e700
99abdc13-7126-4407-b8eb-f87180518ccb
7b66c2c0-ffce-414a-825b-446a02515d7e
c156cf91-7aa1-4ed2-a8e9-2d9539b6fae2
49860e91-9314-4072-a977-80ad88a67be7
c16230a8-5a96-4856-80f1-9c39ff230f20
ac12c98d-9e87-460e-a371-694782b20a6c
99241e62-801e-4464-a6cb-ae5d2f8a4f46
bb07c304-fac1-433e-9c75-e2b1b81b3010
262d3ab4-61de-4add-80bc-6c11334f8020
1176a7a6-6c74-449a-8302-d62df841e9b3
52f92bdf-43a8-4ba3-99ea-bb3d980157d2
468f274c-4c61-4fb2-8a83-3fd53d65d2e5
50b83364-bbdc-415b-82b4-06d918526e45
8dff6c05-5576-468a-8e1b-619cb9842647
5e1d8acc-a842-49fe-a4ab-0ec8927a77e8
62e37d25-0f1b-4df2-902b-612edcdfd664
68be9ab0-503b-43c6-bc9c-e5eb262c3f44
1c13c754-e761-41fd-92ec-2ca0f44ca29b
cbaffdd7-0fec-4f5b-8327-ab7e6089c63b
362307ed-6f5e-416d-a002-c71447147d8a
1255bf8f-061d-4dce-bc13-401a0fe0e43e
f5d37a40-1290-4618-a9e6-c70296465fa8
3fbe571d-9fc8-4fa5-ba1d-ca7371113bd8
e340ebca-6751-4bb4-b911-f55a0df7062b
85694ce2-cb7c-4618-bb28-982ba75f52fb
71ce350c-b3e2-4366-8ce2-447612da3e02
af87026e-b590-458d-af48-c100e3431fef
0a74fdf3-e9b7-4329-8f4d-76f2ea932de3
a9f4dc8c-46b7-4738-b187-0ec92d98d648
9edad0a9-d2b0-4814-94aa-58924b51cde7
0b4c0dff-fe31-4dce-b0ae-a6dbd7142cd8
542891fa-9d5f-4429-9ea9-fff8834379a2
ba9a51f0-2f2a-4be4-a6e8-c54e8d437ff1
6ee3f8ef-aa5b-4f17-83b6-7e3deb84760f
7b4bdd85-20d8-4eb4-8418-3184a78f8771
ef528598-15fe-4590-984c-b1c1827d5f7e
4df05fa7-8d4b-4763-b6bf-073df0d07eb4
cb83dced-cc40-491f-853c-5d3bd9b235df
6fd09cb7-c7ea-43f2-93a0-2650c79a695d
1fc81713-ebb5-4c94-9181-a7f9e4e9bc32
741cca05-7856-48f6-9cd7-6ffde6f6df9b
c8a0d816-aea3-43be-a6e8-e2cb738e71b3
56d51e80-085b-4f04-96ba-ec1a824edc6c
ca735f95-1d43-43a7-b8f8-c98dad0c366a
4e4fbe7f-8e87-4b38-afe0-19b8a20c773c
22fc5b47-9ea5-4d50-8c8c-ced0a678d7a0
13ecaf44-6cce-4982-bf8c-1fd8db7cf905
6a75ae85-7a8d-4294-9e7f-60231db4ab45
eba3efac-36b7-4035-8c16-c6a16bc3faf3
ae587c26-9db6-4ed0-b967-493961f46ec0
2a7a9c32-13e8-41b8-a611-0639716eb7d5
fb4bb2aa-38bd-4788-b561-4f949021c0d5
97e8c824-e852-4d21-8df7-649bcba68267
ae54f5c1-60bf-495e-b02b-78536712521b
c827a37d-397c-4b19-8f4c-37dd2b3a69ba
739b485b-aeeb-45f5-bcf7-9c9455b3af0f
bec875ce-f3a1-4dc3-98c6-ac3f8ffa1751
dda24902-d18a-4ef7-83f4-72f11c482e2c
c58ba9d7-2dfa-47d9-b76a-5901144e707c
fc8a29eb-2a8a-462e-aed3-ccd9f713f9a3
1f88e56c-6e1b-41d0-ae4a-8e15befa9ff4
d1eefb36-2344-4ac9-bf9a-54daacf44735
14591017-57ce-423c-9f22-ce0ff4ab7ac0
afbe7edc-62e7-42d7-9f65-7cad34a3c313
9b6ede92-37a2-40d3-8d51-a54dcbb22431
16b7dfbf-256e-41f5-b536-5e0d27a74e8c
8c828a50-a540-44b6-ba6d-b2bd24bbd778
412dc83c-0443-4a0f-9d70-d41eae578e0e
94d5cded-6bc1-4be1-b4c4-9abfb6f87e8f
5cd53dd2-b275-4828-b4ab-6bb58bde94c0
fb8d055d-9572-48ab-baf3-697048eb7e15
650d4e3d-10e0-40e1-894d-d3598907477a
8391f938-81aa-43d3-a1e1-6a0a03996589
9815b25f-0c5d-4edf-a470-9ccff6491292
4a079ddd-26fd-48b2-954b-dbc772172783
a135c4f4-ba18-4f48-96c2-ba119ec01564
bd48a87e-f765-4555-b547-84043e31d449
3ee194bc-1768-4e10-b9f7-9940f2b0bb7a
2b88eb62-53dd-4b46-ad29-1a9601d9c35f
8b5f8101-cd82-4c4d-a54c-1357cc264437
e67b3221-a8cc-4089-81cb-7669b1c056d4
63638eb8-7e49-4e5b-a425-579e0d81769b
3d4b55da-d830-4725-8ed5-ba5930e38c4c
0077eda7-7382-465e-9829-0090f30f0eb9
57572912-93ec-4d2a-832c-c2cec634626f
c0b0d17a-f4d5-4a9d-b550-cfb5b5ed0b5c
95902c34-7805-456d-8f03-1ab9686a61ae
2b8b94ca-2d01-4eb5-a4ff-1d2ead8cff9b
a10d2163-abda-4893-9589-10e4d4f71c2e
d2c99002-9ddd-4301-bc07-adfc8297912a
ada81542-7ee3-493c-bcc0-5c839a40caa3
b187950d-4a4d-4479-8203-16147286cbb3
eb1c30f7-24da-4ad7-b040-0789f9dc213b
eaa3315d-27a2-476c-8d33-a779d864386d
a7ec1ca6-13a8-49a6-9ed8-e5b7e32c3c9c
01a1528f-d23b-4022-86c8-4242de1e0fd9
c82794a2-a8d5-4978-9a88-0009ecdf3190
819edb2f-ae81-4152-acd3-97f593d0c985
99e58e3e-9368-48f2-ab49-d30b5eb500a0
1e1440c1-7c6f-40d1-86c7-a8750f95c161
d6aa3850-e0d8-4a5f-800a-49f57f574f06
2bc66103-dc3e-465d-b869-cfd289cdefdd
7d68b523-292a-491b-b2e6-e7fc6869f3a3
56f6401d-cf0d-47eb-b261-dc979aa2d34d
9ce71965-2642-4efa-89db-84a99c448702
1b8f1c09-4a43-439f-b36d-75bc31b0a48b
7b36c79f-2d99-4fef-bd45-421d53a873c0
b91acfa3-6a97-48c2-9281-ace29e06078e
59a74481-33cf-415d-bcc1-11e846181d8b
9b71c78d-f1b4-4b77-95bc-70c49f826632
7da99418-c483-4d46-81ec-1309c40abd8d
2d91d88f-12d3-4db7-afe6-6fd4058ea185
d4415607-8fcb-48b6-b2d5-7453f29610f6
77aec155-47ad-4a31-9a27-c0075f401c85
d29bc0e9-185f-44a2-9d61-71e1c005b0c4
12d0fb52-48d3-4a26-8fc1-107b44953343
afbb3d99-457e-40f2-b392-e3a979f20f25
03962959-c813-4434-aabe-746b25a8e773
c55c571c-2a68-4839-b22f-5d67e1d35dee
551a675a-4f66-4ebf-90ad-300b55c3f0a4
ad802e62-326f-40ee-8ccf-b495e9d54d97
3c5bd9bd-b5e3-427a-9fd2-b3330a653d6a
6b4ee44a-aa44-4e4d-bdea-123c393441c1
cccdf614-2a0f-4947-af36-828572cc7dbf
03cad29a-6878-49f5-8d75-6981a0058780
d2a91184-819b-4e86-a18e-b8b178089c07
36ba68e6-f091-44c1-8cd8-9b3dfa32de41
d1621056-d9aa-4c66-83ff-da11404d7262
e65743e9-8923-4a41-b823-c5d858d4da8a
23510930-8f1c-43c1-9178-db47d39bc74e
0b73b6d6-6119-464d-bbea-7241930c0b98
9c196009-7086-48a8-9ded-dd6a0d3764b4
45459855-ae76-4dcd-ad43-1050d6c3dda9
d4a2a63c-d010-4345-8233-a0078786972f
31ebcb44-d238-4af0-a478-165cc9fa546e
66525472-839f-47fb-ba2c-9726d6de8160
dadab95c-4e86-4e76-ba65-1e5b5d5157a1
11c0640a-cde6-4f74-b2ac-811bd3715f89
2a279300-84ed-461c-9bc9-234242fe02a5
1df03c1e-26e4-4f7f-8659-d3c88d3e32de
e048aff3-fc21-42ca-b104-643c19c9f1a4
9325727f-fb4f-409b-9b05-ca83a72d480b
69b73863-764f-44fd-b8a0-22631168a7e4
9f22017f-eeee-494d-bb4c-3e740d4b9f95
a3912f3b-7327-4701-b2c5-5031f4b4387b
b79e1fb7-cefb-4cb2-a23e-a66bf88edcd7
5f5a432c-4caa-4a25-9b19-5195f72de950
026b8ad7-206d-4c7c-ab3e-5bdca8def9f9
57753808-decd-4da1-b91c-860633555fa0
f1c0b051-233d-4f6f-9033-87b2e40e6a13
eea65c90-e0c2-4574-8972-708b52eab4a7
c0f4008b-f904-43dd-8db9-9610a355a165
eb8579fc-6942-4951-884d-22a0638d3c83
47b0fce4-5f58-4a3a-8f70-0cd44103038d
803e3c7e-b16f-444f-bc28-4659cd1a56cc
7f7508f7-891a-4d0e-8b86-0e0b4b1cf747
523f4b46-7abb-411e-bfea-9329442554fe
feac9961-f4f0-4ba5-9353-786415d42f84
d7f70501-2544-4a90-9572-88d02810640d
9886a9df-e0b1-4382-976a-ebb86da93471
42e3c13d-6468-4c10-b0bd-6070307f82d9
5d9f4888-7e06-440a-84c5-d1b9dc4fa3a3
fc3f2141-c21a-4cbf-9ba2-31b81cdb2ff2
5b75dfd9-61b3-42ae-9707-655275d13f6a
465de1d9-c012-461a-82a4-389fdaa3834b
a6a24a6f-fb94-45dc-9031-3ac57f9a015c
6b15d555-f8db-4889-bae1-f336b0a968ca
4e2156e7-cabe-4c1b-b413-0779586129a3
80b1e3b4-58c1-4f4d-96f6-1108c8749593
703d7606-21ed-466a-9628-cb1abcf3441f
55a591c6-bd7e-44cb-952b-b7820961b34c
416c8ab9-a05e-4667-9009-cc8286160d01
e024177f-26fe-483e-a97c-02d854524b5e
85529207-c210-4975-adb6-84ddfc30a9d7
867ff657-f459-42f0-ac8b-b9fefaf6c570
1d26df46-56b7-4d97-b18f-adee21a2a449
3d6b8008-ec18-461d-a7f6-b657c36ada01
af9c6aed-62ab-4e68-888f-c0b0e0be3af1
6a491ee7-60ea-4df8-96bc-eb45e9d827f9
418fdef6-75df-4894-a6e4-779e94e22ba2
60396609-6f37-4f0a-880c-48515465cdbd
2c9f75ed-08e4-45e7-8023-753a4083f772
79c0eeee-204f-4a40-ac2a-d8cfb939fd37
1e196b24-e062-4032-bcb2-9de90dafaf75
47eeace2-df08-40a3-acef-b2098f41e846
7b05f3c0-2a67-42ad-a8bb-5d63b39f53e7
bff0065e-b2fb-4797-82d8-615445d455c8
16d3f099-ce55-4444-a077-824a0c2b4f76
1df5634c-5b78-44e4-b5bf-b027765fc457
d4b31f29-dbe7-485c-8bca-60f6ceaffd17
8478b307-b94e-4fb3-b998-a1dbde8c0da4
bf2e4eae-309f-45ee-9992-3c7105ed317f
24c7ab0b-2e52-4b94-9e25-ce68511759dc
4cc43d11-f255-4f97-93a5-91b640eb175f
fab2e694-cd7b-4542-887a-f5fb91cdd774
541acaca-cc95-4278-b82e-a068953c8fe4
a4b23aa2-97cf-414b-a452-8d66fa67da99
0ab50d21-5840-4723-9a8c-baf3d16c5272
4bd09109-874b-497a-8ccc-6f904396c62c
4f84d0a8-1679-45b8-925b-4da2b2bc9ac4
0d957c0b-08eb-457a-ac36-14b19df13779
adfd739e-d54a-49aa-9941-f5146cb79fdc
a62287f6-10c8-40a7-b156-b2a80be9fda6
e0285647-6674-403d-af48-091dd2ffcce7
d8c58624-1ca6-4f61-8630-529b988f8aec
7b315b07-7012-4577-8877-7512df4d8e3a
cad7422b-9c8c-46e0-acb0-2694b13a6894
4cb36112-8cf8-452e-bd0c-720a561d5226
a0cabc2a-4211-4470-b2d2-304780ee6eb7
87b88a52-3b9c-4099-b0b1-29c06dc08736
fb2ccf54-dc43-4871-b3c6-15f2c4294478
5baef2c7-5ed3-4cb0-9c03-7edf43cf6dff
139aaba4-4f27-40e9-a102-6b2fd4be4775
f7c44e74-ad8a-4a92-856b-939bd8960f8f
f2137fc5-128d-4948-b9f4-13927f11a91c
ea306e01-f116-4257-a2b1-d390b4a2a122
9fef5f01-eaa9-405e-9e62-42f4711f9a72
6e2f74ee-3d9f-41df-8a45-79dfc3cf44d1
7699b908-f153-4d88-ac44-b9cac160e0c4
bf1c8be2-ba22-42bd-af55-521eb47ce306
a2841916-16d5-4767-854a-e4cd8cbf16fa
bf97fe29-5cbc-41fa-abbf-12fe2a2e6f1a
3bfa7c18-e56f-4d19-9be8-f834c98cad04
c3f33e10-496d-4abc-8b01-11d0862e496c
1e9a904a-4f09-4527-a0bf-9b5866b6b348
dafd1169-c233-4b01-8761-47d36a0c2a4c
58c58b07-4e04-4064-b608-a21b991ad56d
41fad3c0-cf40-4118-a862-c02752b78181
6eb8a9d9-371e-4a45-a717-c8a811f58822
3f77215a-c620-4501-984d-e4823133e13d
045761f4-002a-4540-bb78-326517d40349
27fbdd5a-d755-4976-9552-7cbe17f3a1c4
de83c7ef-ef56-40d8-9b2f-e160eb0adf68
6a797263-00e1-4c3d-838e-1bdf604087df
87da43f6-6209-491b-b5a2-5bb11c0a5db4
0e785b6b-57ce-4180-bc4a-5b74181e6317
e9adeca5-6644-43d4-a71a-dc803ecf2df9
70679047-2a30-4950-8136-fb679202dff5
ad230cc8-29f9-41f9-b82d-092d6c5737e6
e26c5513-e4a0-4e0d-b97d-addb0bc820e8
b56791e8-a53b-4e2c-9f24-55f3674d4577
fabe1cb3-3454-4d81-8d83-831ae073818a


package org.apache.spark.examples.mllib

import scala.reflect.runtime.universe._

/**
 * Abstract class for parameter case classes.
 * This overrides the [[toString]] method to print all case class fields by name and value.
 * @tparam T  Concrete parameter class.
 */
abstract class AbstractParams[T: TypeTag] {

  private def tag: TypeTag[T] = typeTag[T]

  /**
   * Finds all case class fields in concrete class instance, and outputs them in JSON-style format:
   * {
   *   [field name]:\t[field value]\n
   *   [field name]:\t[field value]\n
   *   ...
   * }
   */
  override def toString: String = {
    val tpe = tag.tpe
    val allAccessors = tpe.declarations.collect {
      case m: MethodSymbol if m.isCaseAccessor => m
    }
    val mirror = runtimeMirror(getClass.getClassLoader)
    val instanceMirror = mirror.reflect(this)
    allAccessors.map { f =>
      val paramName = f.name.toString
      val fieldMirror = instanceMirror.reflectField(f)
      val paramValue = fieldMirror.get
      s"  $paramName:\t$paramValue"
    }.mkString("{\n", ",\n", "\n}")
  }
}
package com.bitnei.report.tempjob

/*
* created by wangbaosheng on 2017/12/7
*/

import akka.actor.Actor

class MyActor extends Actor {
  override def receive: Receive = {
    case "test" => println("received test")
    case _ => println("received unknow message")
  }
}


class ActorDemo {

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.regression.AFTSurvivalRegression
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * An example demonstrating AFTSurvivalRegression.
 * Run with
 * {{{
 * bin/run-example ml.AFTSurvivalRegressionExample
 * }}}
 */
object AFTSurvivalRegressionExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("AFTSurvivalRegressionExample")
      .getOrCreate()

    // $example on$
    val training = spark.createDataFrame(Seq(
      (1.218, 1.0, Vectors.dense(1.560, -0.605)),
      (2.949, 0.0, Vectors.dense(0.346, 2.158)),
      (3.627, 0.0, Vectors.dense(1.380, 0.231)),
      (0.273, 1.0, Vectors.dense(0.520, 1.151)),
      (4.199, 0.0, Vectors.dense(0.795, -0.226))
    )).toDF("label", "censor", "features")
    val quantileProbabilities = Array(0.3, 0.6)
    val aft = new AFTSurvivalRegression()
      .setQuantileProbabilities(quantileProbabilities)
      .setQuantilesCol("quantiles")

    val model = aft.fit(training)

    // Print the coefficients, intercept and scale parameter for AFT survival regression
    println(s"Coefficients: ${model.coefficients}")
    println(s"Intercept: ${model.intercept}")
    println(s"Scale: ${model.scale}")
    model.transform(training).show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

// $example on$
import org.apache.spark.graphx.{Graph, VertexRDD}
import org.apache.spark.graphx.util.GraphGenerators
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * An example use the [`aggregateMessages`][Graph.aggregateMessages] operator to
 * compute the average age of the more senior followers of each user
 * Run with
 * {{{
 * bin/run-example graphx.AggregateMessagesExample
 * }}}
 */
object AggregateMessagesExample {

  def main(args: Array[String]): Unit = {
    // Creates a SparkSession.
    val spark = SparkSession
      .builder
      .appName(s"${this.getClass.getSimpleName}")
      .getOrCreate()
    val sc = spark.sparkContext

    // $example on$
    // Create a graph with "age" as the vertex property.
    // Here we use a random graph for simplicity.
    val graph: Graph[Double, Int] =
      GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _) => id.toDouble )
    // Compute the number of older followers and their total age
    val olderFollowers: VertexRDD[(Int, Double)] = graph.aggregateMessages[(Int, Double)](
      triplet => { // Map Function
        if (triplet.srcAttr > triplet.dstAttr) {
          // Send message to destination vertex containing counter and age
          triplet.sendToDst(1, triplet.srcAttr)
        }
      },
      // Add counter and age
      (a, b) => (a._1 + b._1, a._2 + b._2) // Reduce Function
    )
    // Divide total age by number of older followers to get average age of older followers
    val avgAgeOfOlderFollowers: VertexRDD[Double] =
      olderFollowers.mapValues( (id, value) =>
        value match { case (count, totalAge) => totalAge / count } )
    // Display the results
    avgAgeOfOlderFollowers.collect.foreach(println(_))
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.samples.sca.akka

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-04 15:13
  *
  */

object AkkaClient extends App {

  sealed trait AkkaTestMsg

  case object Start

  case object Stop

//  class Client extends Actor {
//
//    val log = Logging(context.system, this)
//
//    def receive = {
//      case Start => println("start")
//        log.info("good start!")
//        self ! Stop
//      case Stop => println("stop")
//        log.info("good stop!")
//        context.system.shutdown
//    }
//  }
//
//  val system = ActorSystem("system")
//  val client = system.actorOf(Props[Client], "client")
//  val log = Logging(system, client)
//
//  log.info("ok")
//  client ! Start
}package com.bitnei.report.dayreport.alarm

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{StringParser, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.report.stateGenerate.Window

import scala.reflect.ClassTag

class AlarmComputeBase[T:ClassTag](val vid:String,
                               val reportDate:String,
                               stateConf:StateConf,
                               window:Option[Window[T]],
                               getAlarmId:(T)=>Option[String],
                               getAlarmTime:(T)=>Option[String],
                               getAlarmStatus:(T)=>Option[String],
                               getAlarmLevel:(T)=>Option[String],
                               getAlarmType:(T)=>Option[String]
                              ) extends Serializable with Logging {

  var faultTime = 0L

  val alarmLevels = Array(0, 0, 0, 0, 0, 0)
  var (alarmTempDiffTimes: Int, alarmTempDiffTime: Long) = (0, 0L)
  var (alarmBatteryPostTempTimes: Int, alarmBatteryPostTempTime: Long) = (0, 0L)
  var (alarmPabOverVoltageTimes: Int, alarmPabOverVoltageTime: Long) = (0, 0L)
  var (alarmPABUnderVoltageTimes: Int, alarmPABUnderVoltageTime: Long) = (0, 0L)
  var (alarmLowSocTimes: Int, alarmLowSocTime: Long) = (0, 0L)
  var (alarmSecondaryOverVoltageTimes: Int, alarmSecondaryOverVoltageTime: Long) = (0, 0L)
  var (alarmSecondaryUnderVoltageTimes: Int, alarmSecondaryUnderVoltageTime: Long) = (0, 0L)
  var (alarmLowLowSocTimes: Int, alarmLowLowSocTime: Long) = (0, 0L)
  var (alarmUpperSocTimes: Int, alarmUpperSocTime: Long) = (0, 0L)
  var (alarmPABPackageNotMatchTimes: Int, alarmPABPackageNotMatchTime: Long) = (0, 0L)
  var (alarmPABConsistencyTimes: Int, alarmPABConsistencyTime: Long) = (0, 0L)
  var (alarmSecondaryInsulationFaultTimes: Int, alarmSecondaryInsulationFaultTime: Long) = (0, 0L)

  val alarmStartTime = new scala.collection.mutable.HashMap[String, String]()


  //日故障总次数
  def alarmCount:Int = alarmLevels(1) + alarmLevels(2) + alarmLevels(3)

  def alarmTempDiffPercent: Float = if (alarmCount != 0) alarmTempDiffTimes.toFloat / alarmCount.toFloat else 0

  def alarmBatteryPostTempPercent: Float = if (alarmCount != 0) alarmBatteryPostTempTimes.toFloat / alarmCount.toFloat else 0

  def alarmPabOverVoltagePercent: Float = if (alarmCount != 0) alarmPabOverVoltageTimes.toFloat / alarmCount.toFloat else 0

  def alarmPabUnderVoltagePercent: Float = if (alarmCount != 0) alarmPABUnderVoltageTimes.toFloat / alarmCount.toFloat else 0

  def alarmLowSocPercent: Float = if (alarmCount != 0) alarmLowSocTimes.toFloat / alarmCount.toFloat else 0

  def alarmSecondaryOverVoltagePercent: Float = if (alarmCount != 0) alarmSecondaryOverVoltageTimes.toFloat / alarmCount.toFloat else 0

  def alarmSecondaryUnderVoltagePercent: Float = if (alarmCount != 0) alarmSecondaryUnderVoltageTimes.toFloat / alarmCount.toFloat else 0

  def alarmLowLowSocPercent: Float = if (alarmCount != 0) alarmLowLowSocTimes.toFloat / alarmCount.toFloat else 0

  def alarmUpperSocPercent: Float = if (alarmCount != 0) alarmUpperSocTimes.toFloat / alarmCount.toFloat else 0

  def alarmPABPackageNotMatchPercent: Float = if (alarmCount != 0) alarmPABPackageNotMatchTimes.toFloat / alarmCount.toFloat else 0

  def alarmPABConsistencyPercent: Float = if (alarmCount != 0) alarmPABConsistencyTimes.toFloat / alarmCount.toFloat else 0

  def alarmSecondaryInsulationFaultPercent: Float = if (alarmCount != 0) alarmSecondaryInsulationFaultTimes.toFloat / alarmCount.toFloat else 0

  try {
    window.foreach(_.foreach(alarm=>{
      val status=getAlarmStatus(alarm).get

      if(status=="1") {
        //统计1，2，3，4，5级报警
        val alarmLevel = getAlarmLevel(alarm).map(_.toInt).getOrElse(Int.MaxValue)

        if (alarmLevel >= 1 && alarmLevel <= 5) alarmLevels(alarmLevel) += 1

        //记录报警开始时间
        alarmStartTime.put(getAlarmId(alarm).get,getAlarmTime(alarm).get)

        getAlarmType(alarm) match {
          case Some(Constant.AlarmTempDiffName) =>
            alarmTempDiffTimes += 1
          case Some(Constant.AlarmBatteryPostTempName) =>
            alarmBatteryPostTempTimes += 1
          case Some(Constant.AlarmPABOverVoltageName) =>
            alarmPabOverVoltageTimes += 1
          case Some(Constant.AlarmPABUnderVoltageName) =>
            alarmPABUnderVoltageTimes += 1
          case Some(Constant.AlarmLowSocName) =>
            alarmLowSocTimes += 1
          case Some(Constant.AlarmSecndaryOverVoltageName) =>
            alarmSecondaryOverVoltageTimes += 1
          case Some(Constant.AlarmSecondaryUnderVoltageName) =>
            alarmSecondaryUnderVoltageTimes += 1
          case Some(Constant.ALarmLowLowSocName) =>
            alarmLowLowSocTimes += 1
          case Some(Constant.ALarmUpperSocName) =>
            alarmUpperSocTimes += 1
          case Some(Constant.AlarmPABPackageNotMatchName) =>
            alarmPABPackageNotMatchTimes += 1
          case Some(Constant.AlarmPABConsistencyName) =>
            alarmPABConsistencyTimes += 1
          case Some(Constant.AlarmSecondaryInsulationFaultName) =>
            alarmSecondaryInsulationFaultTimes += 1
          case _ =>
        }
      }else if(status=="3"){
        (getAlarmType(alarm), alarmStartTime.get(getAlarmId(alarm).get)) match {
          case (Some(Constant.AlarmTempDiffName), Some(startAlarmTime)) =>
            alarmTempDiffTime += Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmBatteryPostTempName), Some(startAlarmTime)) =>
            alarmBatteryPostTempTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmPABOverVoltageName), Some(startAlarmTime)) =>
            alarmPabOverVoltageTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmPABUnderVoltageName), Some(startAlarmTime)) =>
            alarmPABUnderVoltageTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmLowSocName), Some(startAlarmTime)) =>
            alarmLowSocTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmSecndaryOverVoltageName), Some(startAlarmTime)) =>
            alarmSecondaryOverVoltageTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmSecondaryUnderVoltageName), Some(startAlarmTime)) =>
            alarmSecondaryUnderVoltageTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.ALarmLowLowSocName), Some(startAlarmTime)) =>
            alarmLowLowSocTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.ALarmUpperSocName), Some(startAlarmTime)) =>
            alarmUpperSocTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmPABPackageNotMatchName), Some(startAlarmTime)) =>
            alarmPABPackageNotMatchTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmPABConsistencyName), Some(startAlarmTime)) =>
            alarmPABConsistencyTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case (Some(Constant.AlarmSecondaryInsulationFaultName), Some(startAlarmTime)) =>
            alarmSecondaryInsulationFaultTime+= Utils.timeDiff(getAlarmTime(alarm).get, startAlarmTime)
          case _ =>
        }
      }

      faultTime+=alarmTempDiffTime+alarmBatteryPostTempTime+alarmPabOverVoltageTime+alarmPABUnderVoltageTime+alarmLowSocTime+alarmSecondaryOverVoltageTime+alarmSecondaryUnderVoltageTime+alarmLowLowSocTime+alarmUpperSocTime+alarmPABPackageNotMatchTime+alarmPABConsistencyTime+alarmSecondaryInsulationFaultTime
    }))

    faultTime+=alarmTempDiffTime+alarmBatteryPostTempTime+alarmPabOverVoltageTime+alarmPABUnderVoltageTime+alarmLowSocTime+alarmSecondaryOverVoltageTime+alarmSecondaryUnderVoltageTime+alarmLowLowSocTime+alarmUpperSocTime+alarmPABPackageNotMatchTime+alarmPABConsistencyTime+alarmSecondaryInsulationFaultTime
  } catch {
    case e: Exception =>logError(e.toString)
  }

  override def toString: String =
    s",$reportDate,$vid,$faultTime,$alarmCount,$alarmLevels(1),$alarmLevels(2),$alarmLevels(3),$alarmLevels(4),$alarmLevels(5)" +
      s",$alarmTempDiffTimes,$alarmBatteryPostTempTimes,$alarmPabOverVoltageTimes,$alarmPABUnderVoltageTimes,$alarmSecondaryOverVoltageTimes,$alarmSecondaryUnderVoltageTimes" +
      s",$alarmPABPackageNotMatchTimes,$alarmPABConsistencyTimes,$alarmLowSocTimes,$alarmLowLowSocTimes,$alarmUpperSocTimes,$alarmSecondaryInsulationFaultTimes" +
      s",$alarmTempDiffTime,$alarmBatteryPostTempTime,$alarmPabOverVoltageTime,$alarmPABUnderVoltageTime,$alarmSecondaryOverVoltageTime,$alarmSecondaryUnderVoltageTime" +
      s",$alarmLowSocTime,$alarmLowLowSocTime,$alarmUpperSocTime,$alarmPABPackageNotMatchTimes,$alarmPABConsistencyTime,$alarmSecondaryInsulationFaultTime"


  def result():AlarmStateMode={
    val h=1000*3600

    AlarmStateMode(vid,
      getAlarmTime(window.head.head).map( Utils.parsetDate(_,"yyyyMMddHHmmss").map(_.getTime).getOrElse(0L)).get,
      faultTime.toInt,
      alarmCount,
      alarmLevels(1),alarmLevels(2),alarmLevels(3),alarmLevels(4),alarmLevels(5),
      alarmTempDiffTimes,alarmBatteryPostTempTimes,alarmPabOverVoltageTimes,alarmPABUnderVoltageTimes,alarmSecondaryOverVoltageTimes,alarmSecondaryUnderVoltageTimes,
      alarmPABPackageNotMatchTimes,alarmPABConsistencyTimes,alarmLowSocTimes,alarmLowLowSocTimes,alarmUpperSocTimes,alarmSecondaryInsulationFaultTimes,
      alarmTempDiffPercent,alarmBatteryPostTempPercent,alarmPabOverVoltagePercent,alarmPabUnderVoltagePercent,alarmSecondaryOverVoltagePercent,alarmSecondaryUnderVoltagePercent,
      alarmPABPackageNotMatchPercent,alarmPABConsistencyPercent,alarmLowSocPercent,alarmLowLowSocPercent,alarmUpperSocPercent,
      alarmSecondaryInsulationFaultPercent,
      alarmTempDiffTime.toFloat/h,alarmBatteryPostTempTime.toFloat/h,alarmPabOverVoltageTime.toFloat/h,alarmPABUnderVoltageTime.toFloat/h,
      alarmSecondaryOverVoltageTime.toFloat/h,alarmSecondaryUnderVoltageTime.toFloat/h,
      alarmLowSocTime,alarmLowLowSocTime.toFloat/h,alarmUpperSocTime.toFloat/h,alarmPABPackageNotMatchTime.toFloat/h,alarmPABConsistencyTime.toFloat/h,
      alarmSecondaryInsulationFaultTime.toFloat/h)
  }
}


class AlarmComputeBaseOnString(vid:String,
                                           reportDate:String,
                                           stateConf:StateConf,
                                           window:Option[Window[String]])
extends AlarmComputeBase[String](vid,
  reportDate,
  stateConf,
  window,
  row=>StringParser.get(row,Constant.AlarmId),
  row=>StringParser.get(row,Constant.AlarmTime),
  row=>StringParser.get(row,Constant.AlarmStatusName),
  row=>StringParser.get(row,Constant.AlarmLevel),
  row=>StringParser.get(row,Constant.AlarmTypeName)){}

class AlarmComputeBaseOnAlarmInfo(vid:String,
                               reportDate:String,
                               stateConf:StateConf,
                               window:Option[Window[AlarmInfoModel]])
  extends AlarmComputeBase[AlarmInfoModel](vid,
    reportDate,
    stateConf,
    window,
    row=>Some(row.alarmId),
    row=>Some(row.time),
    row=>Some(row.status),
    row=>Some(row.level),
    row=>Some(row.alarmType)){}
package com.bitnei.report.dayreport.alarm

/**
  * Created by francis on 2017/2/13.
  */
case class AlarmInfoModel(vid:String,time:String,alarmId:String,level: String,status:String,alarmType:String){}
package com.bitnei.report.dayreport.alarm

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import com.bitnei.report.stateGenerate.Window
import com.bitnei.report.{AutoPartition, Job, JobRunner}
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.sql.{Dataset, SparkSession}

/**
  * Created by wangbaosheng on 2017/4/6.
  * 报警作业
  */
class AlarmJob(@transient sparkSession:SparkSession,stateConf:StateConf) extends
  Serializable
  with Logging
  with Job with AutoPartition {
  @transient private   val sqlContext = sparkSession.sqlContext
  @transient private   val reportDate = stateConf.getString(Constant.ReportDate)
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)
  import sparkSession.implicits._

  override type R = AlarmStateMode

  override def registerIfNeed() = {
    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,"alarm")
  }

  override def unRegister() = {
    sparkSession.catalog.dropTempView("alarm")
  }

  override def doCompute[Product <:AlarmStateMode]() = {
    sparkSession.sql(s"""
      SELECT
        VID,
        TIME,
        ALARM_ID AS alarmId,
        STATUS,
        ALARM_LEVEL AS level,
        LEFT1 AS alarmType
      FROM alarm
      WHERE ${SqlHelper.buildWhere(stateConf)}
    """.stripMargin).as[AlarmInfoModel]
      .groupByKey(_.vid)
      .mapGroups((vid, alarms) => {
        if (alarms.isEmpty) {
          new AlarmComputeBaseOnAlarmInfo(vid, reportDate, stateConf, None).result()
        } else {
          val alarmWindow=new Window[AlarmInfoModel]()
          alarms.foreach(alarm=>alarmWindow.append(alarm))
          new AlarmComputeBaseOnAlarmInfo(vid, reportDate, stateConf, Some(alarmWindow)).result()
        }
      })
  }

  override def write[Product <:AlarmStateMode](result: Dataset[AlarmStateMode]) = {
    val ouputModels = stateConf.getString("report.output").split(',')
    if(ouputModels.length>1) result.cache()

    ouputModels.foreach(model=>{
      if(model=="hdfs"){
        SparkHelper.saveToPartition(sparkSession,stateConf,result.toDF(),"alarmReport")

      }else if(model=="oracle" || model=="mysql"){
        logInfo("开启数据库输出")
        stateConf.set("database",model)
        //        logWarning(s"开始删除${reportDate}报警日报表的数据")
        //        new AlarmStateModelManager(stateConf).delete(reportDate)
        result.foreachPartition(par => {
          new AlarmStateModelManager(stateConf).insert(par.toIterable)
        })
      }else{
        logWarning("没有输出功能")
      }
    })
  }

  override def getThreshold:Long=Utils.getParquetThreshold(stateConf,hadoopConfiguration)
}

object  AlarmJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)
    new AlarmJob(sparkSession, stateConf).compute()
  }
}package com.bitnei.alarm

import java.sql.DriverManager
import java.text.SimpleDateFormat

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.{SaveMode, SparkSession}


/**
  *
  * @author zhangyongtian
  * @define 通用报警标识按天统计报警数
  *
  * create 2018-01-31 11:43
  *
  */
object AlarmMarkIntervalCnt2Oracle extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    val date = stateConf.getOption("input.date").getOrElse("20171102")

    val inputPath = stateConf.getOption("input.data.path").getOrElse("/spark/vehicle/result/alarmMarkInterval/day")

    //参数校验
    var dataTimeRange = ""

    date.length match {
      case 4 => dataTimeRange = s"year=${date.substring(0, 4)}"
      case 6 => dataTimeRange = s"year=${date.substring(0, 4)}/month=${date.substring(4, 6)}"
      case 8 => dataTimeRange = s"year=${date.substring(0, 4)}/month=${date.substring(4, 6)}/day=${date.substring(6)}"
    }


    //输出参数
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/$dataTimeRange"
    var outputTargets = stateConf.getOption("output").getOrElse("oracle")
    var outFormat = stateConf.getOption("output.format").getOrElse("json")



    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._

    ////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.parquet("data/realinfo/20171102/*.parquet").toJSON.repartition(1).write.json("data/realinfo/20171102/tmp")
        //        sparkSession.read.parquet("data/realinfo/20171102/*.parquet").createOrReplaceTempView("realinfo")
        sparkSession.read.json("data/alarmMarkInterval/*.json").createOrReplaceTempView("alarmMark")
      }

      case "prd" => {
        sparkSession
          .read
          .format("json")
          .load(s"$inputPath/$dataTimeRange").createOrReplaceTempView("alarmMark")
      }

    }

    /////////////////////////////业务逻辑处理//////////////////////////////////////

    val sql =
      """
        select vid,vin,alarmType,substring(startTime,0,8) as date,cast(count(1) as Int) as num
        from alarmMark
        group by vid,vin,alarmType,substring(startTime,0,8)
      """.stripMargin

    val initDS = sparkSession.sql(sql)
      .as[Input]


    //行转列
    val types = Array[String](
      "2901",
      "2902",
      "2903",
      "2904",
      "2905",
      "2906",
      "2907",
      "2908",
      "2909",
      "2910",
      "2911",
      "2912",
      "2913",
      "2914",
      "2915",
      "2916",
      "2917",
      "2918",
      "2919",
      "2930"

    ).toList

    initDS
      .groupBy($"vid", $"vin", $"date")
      .pivot("alarmType", types)
      .sum("num")
      .na
      .fill(0)
      .createOrReplaceTempView("temp")

    //    |vid  |vin |date |2901|2902|2903|2904|2905|2906|2907|2908|2909|2910|2911|2912|2913|2914|2915|2916|2917|2918|2919|2930

    val sql2 =
      """
         SELECT
         cast(`2901` as int) As `GENERAL_ALARM1_NUM`,
         cast(`2902` as int) As `GENERAL_ALARM2_NUM`,
         cast(`2903` as int) As `GENERAL_ALARM3_NUM`,
         cast(`2904` as int) As `GENERAL_ALARM4_NUM`,
         cast(`2905` as int) As `GENERAL_ALARM5_NUM`,
         cast(`2906` as int) As `GENERAL_ALARM6_NUM`,
         cast(`2907` as int) As `GENERAL_ALARM7_NUM`,
         cast(`2908` as int) As `GENERAL_ALARM8_NUM`,
         cast(`2909` as int) As `GENERAL_ALARM9_NUM`,
         cast(`2910` as int) As `GENERAL_ALARM10_NUM`,
         cast(`2911` as int) As `GENERAL_ALARM11_NUM`,
         cast(`2912` as int) As `GENERAL_ALARM12_NUM`,
         cast(`2913` as int) As `GENERAL_ALARM13_NUM`,
         cast(`2914` as int) As `GENERAL_ALARM14_NUM`,
         cast(`2915` as int) As `GENERAL_ALARM15_NUM`,
         cast(`2916` as int) As `GENERAL_ALARM16_NUM`,
         cast(`2917` as int) As `GENERAL_ALARM17_NUM`,
         cast(`2918` as int) As `GENERAL_ALARM18_NUM`,
         cast(`2919` as int) As `GENERAL_ALARM19_NUM`,
         cast(`2930` as int) As `GENERAL_ALARM30_NUM`,
          VID,
          date as REPORT_DATE
         FROM temp
      """.stripMargin

    //    sparkSession.sql(sql2).printSchema()

    val result = sparkSession.sql(sql2).as[Output]




    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."VID" IS 'UUID';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."REPORT_DATE" IS '报表日期';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM1_NUM" IS '温度差异报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM2_NUM" IS '电池极柱高温报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM3_NUM" IS '车载储能装置类型过压报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM4_NUM" IS '车载储能装置类型欠压报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM5_NUM" IS 'SOC低报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM6_NUM" IS '单体电池过压报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM7_NUM" IS '单体电池欠压报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM8_NUM" IS 'SOC太低报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM9_NUM" IS 'SOC过高报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM10_NUM" IS '可充电储能系统不匹配报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM11_NUM" IS '动力蓄电池一致性差报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM12_NUM" IS '绝缘报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM13_NUM" IS 'DC-DC温度报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM14_NUM" IS '制动系统报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM15_NUM" IS 'DC-DC状态报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM16_NUM" IS '驱动电机控制器温度报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM17_NUM" IS '高压互锁状态报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM18_NUM" IS '驱动电机温度报警';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM19_NUM" IS '车载储能装置类型过充';
    //    COMMENT ON COLUMN "EV"."VEH_DAYREPORT_ENERGY_FAULT"."GENERAL_ALARM30_NUM" IS 'SOC跳变报警';


    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("alarmMark")
    sparkSession.catalog.dropTempView("temp")





    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      //      result.count()

      result.show(false)
    }

    if (env.equals("prd")) {

      if (outputTargets.contains("oracle")) {

        // TODO: 输出到oracle
        logInfo("输出到oracle　start....")
        var user = stateConf.getOption("jdbc.user").getOrElse("ev")
        var password = stateConf.getOption("jdbc.password").getOrElse("ev")
        var ip = stateConf.getOption("jdbc.ip").getOrElse("192.168.6.146")
        var port = stateConf.getOption("jdbc.port").getOrElse("1521")
        var server = stateConf.getOption("jdbc.server").getOrElse("evmsc1")

        var tableName = stateConf.getOption("jdbc.tableName").getOrElse("VEH_DAYREPORT_ENERGY_FAULT")

        //        val url = s"jdbc:oracle:thin:@$ip:$port:" + server

        val url = s"jdbc:oracle:thin:@//$ip:$port/" + server
        println("url=" + url)

        //        val prop = new java.util.Properties
        //        prop.setProperty("user", user)
        //        prop.setProperty("password", password)
        //
        //        result.write.option(JDBCOptions.JDBC_TRUNCATE, "true").mode(SaveMode.Append).jdbc(url, tableName, prop);


        //插入
        result.foreachPartition(values => {
          val sql =
            s"""INSERT INTO $tableName VALUES (?, to_date(?, 'yyyymmdd'),?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) """.stripMargin

          val connection = DriverManager.getConnection(url, user, password)

          val stmt = connection.prepareStatement(sql)

          try {
            stmt.clearBatch()
            values.foreach(v => {
              var i = 0
              i += 1
              stmt.setString(i, v.VID)
              i += 1
              stmt.setString(i, v.REPORT_DATE)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM1_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM2_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM3_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM4_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM5_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM6_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM7_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM8_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM9_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM10_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM11_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM12_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM13_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM14_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM15_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM16_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM17_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM18_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM19_NUM)
              i += 1
              stmt.setInt(i, v.GENERAL_ALARM30_NUM)

              stmt.addBatch()
            })
            stmt.executeBatch()
          } catch {
            case e: Exception =>
              logError(s"数据在写入到${tableName}中出现异常")
              throw new Exception(s"throw en exception when writting $tableName", e)
          } finally {
            if (!stmt.isClosed) stmt.close()
            if (!connection.isClosed) connection.close()
          }
        }
        )

      }


      if (outputTargets.contains("hdfs")) {

        //TODO: 输出到HDFS
        logInfo("输出到HDFS　start....")


        result.repartition(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }


    }

    sparkSession.stop()
  }

  case class Input(vid: String, vin: String, alarmType: String, date: String, num: Int)

  case class Output(
                     GENERAL_ALARM1_NUM: Int,
                     GENERAL_ALARM2_NUM: Int,
                     GENERAL_ALARM3_NUM: Int,
                     GENERAL_ALARM4_NUM: Int,
                     GENERAL_ALARM5_NUM: Int,
                     GENERAL_ALARM6_NUM: Int,
                     GENERAL_ALARM7_NUM: Int,
                     GENERAL_ALARM8_NUM: Int,
                     GENERAL_ALARM9_NUM: Int,
                     GENERAL_ALARM10_NUM: Int,
                     GENERAL_ALARM11_NUM: Int,
                     GENERAL_ALARM12_NUM: Int,
                     GENERAL_ALARM13_NUM: Int,
                     GENERAL_ALARM14_NUM: Int,
                     GENERAL_ALARM15_NUM: Int,
                     GENERAL_ALARM16_NUM: Int,
                     GENERAL_ALARM17_NUM: Int,
                     GENERAL_ALARM18_NUM: Int,
                     GENERAL_ALARM19_NUM: Int,
                     GENERAL_ALARM30_NUM: Int,
                     VID: String,
                     REPORT_DATE: String
                   )

}
package com.bitnei.alarm

import java.text.SimpleDateFormat

import com.bitnei.alarm.generator.AlarmMarkIntervalGenerator
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 通用报警标识区间按天统计明细
  *
  * create 2018-01-31 11:43
  *
  */
object AlarmMarkIntervalDaily extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    val date = stateConf.getOption("input.date").getOrElse("20171102")


    //帧数间隔时间超过该值，就算下一个报警区间
    val TIMEOUT_DURATION = stateConf.getOption("input.timeout.duartion").getOrElse("180").toLong


    //参数校验
    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)


    //输出参数
    var outputTargets = stateConf.getOption("output").getOrElse("console")
    var outFormat = stateConf.getOption("output.format").getOrElse("json")
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"
    var stat_hdfsPath = s"${stateConf.getString("output.hdfs.stat.path")}/year=${year}/month=${month}/day=${day}"


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._

    ////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.parquet("data/realinfo/20171102/*.parquet").toJSON.repartition(1).write.json("data/realinfo/20171102/tmp")
        //        sparkSession.read.parquet("data/realinfo/20171102/*.parquet").createOrReplaceTempView("realinfo")
        sparkSession.read.json("data/realinfo/mock.txt").createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }

    /////////////////////////////业务逻辑处理//////////////////////////////////////

    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    val has2908 = sparkSession.sql("select * from realinfo").schema.toString().contains("2908")

    if (!has2908) {
      return
    }

    val sql =
      s"""
        SELECT
        nvl(`2901`,0) As `2901`,
        nvl(`2902`,0) As `2902`,
        nvl(`2903`,0) As `2903`,
        nvl(`2904`,0) As `2904`,
        nvl(`2905`,0) As `2905`,
        nvl(`2906`,0) As `2906`,
        nvl(`2907`,0) As `2907`,
        nvl(`2908`,0) As `2908`,
        nvl(`2909`,0) As `2909`,
        nvl(`2910`,0) As `2910`,
        nvl(`2911`,0) As `2911`,
        nvl(`2912`,0) As `2912`,
        nvl(`2913`,0) As `2913`,
        nvl(`2914`,0) As `2914`,
        nvl(`2915`,0) As `2915`,
        nvl(`2916`,0) As `2916`,
        nvl(`2917`,0) As `2917`,
        nvl(`2918`,0) As `2918`,
        nvl(`2919`,0) As `2919`,
        nvl(`2930`,0) As `2930`,
        nvl(`2920`,0) AS maxLevel,
         VID,
         VIN,
         `2000` AS TIME
        FROM realinfo
        where VID is not null and VIN is not null  and `2000` like '${date}%'
      """.stripMargin

    var initDS = sparkSession.sql(sql)
      .as[Intput]

    val result = initDS
      .groupByKey(_.VID)
      .flatMapGroups {
        case (vid, inputs) => {

          val arr = inputs.toArray.sortBy(_.TIME)

          val res = new ArrayBuffer[Output]();

          val alarmCols = Array[String](
            "2901",
            "2902",
            "2903",
            "2904",
            "2905",
            "2906",
            "2907",
            "2908",
            "2909",
            "2910",
            "2911",
            "2912",
            "2913",
            "2914",
            "2915",
            "2916",
            "2917",
            "2918",
            "2919",
            "2930"
          )

          alarmCols.foreach(x => {
            val tmp: Array[Output] = AlarmMarkIntervalGenerator.handle(arr, TIMEOUT_DURATION, x)
            res ++= (tmp)
          })

          res.toList
        }
      }
    //      {"vid":"dd8bb2ab-e243-4da1-a8df-b5daef50a9cf","vin":"LZYTBGBW8H1009850","date":"20171101","total":"10000","alarm1Cnt":6378,"alarm2Cnt":6378,"alarm13Cnt":6378}

    //TODO 统计一天一辆车按报警等级统计报文条数
    val result2 = initDS
      .groupByKey(x => (x.VIN))
      .mapGroups {
        case (vin, inputs) => {
          val arr = inputs.toArray
          val vid = arr.head.VID

          //总报文条数
          val total = arr.length

          var alarm1Cnt = 0
          var alarm2Cnt = 0
          var alarm3Cnt = 0

          arr.foreach(x => {
            val maxLevel = x.maxLevel.trim.toInt
            if (1 == maxLevel) {
              alarm1Cnt = alarm1Cnt + 1
            }
            if (2 == maxLevel) {
              alarm2Cnt = alarm2Cnt + 1
            }
            if (3 == maxLevel) {
              alarm3Cnt = alarm3Cnt + 1
            }
          })

          Output2(vid, vin, date, total, alarm1Cnt, alarm2Cnt, alarm3Cnt)
        }
      }


    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()

      result2.show(false)
    }

    if (env.equals("prd")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      if (outputTargets.contains("hdfs")) {

        result.repartition(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        result2.repartition(1).write.format(outFormat).mode(SaveMode.Overwrite).save(stat_hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }


    }

    sparkSession.stop()
  }

  case class Intput(
                     `2901`: String,
                     `2902`: String,
                     `2903`: String,
                     `2904`: String,
                     `2905`: String,
                     `2906`: String,
                     `2907`: String,
                     `2908`: String,
                     `2909`: String,
                     `2910`: String,
                     `2911`: String,
                     `2912`: String,
                     `2913`: String,
                     `2914`: String,
                     `2915`: String,
                     `2916`: String,
                     `2917`: String,
                     `2918`: String,
                     `2919`: String,
                     `2930`: String,
                     maxLevel: String,
                     VID: String,
                     VIN: String,
                     TIME: String
                   )

  case class Output(
                     VID: String,
                     VIN: String,
                     startTime: String,
                     stopTime: String,
                     alarmType: String,
                     maxLevel: String
                   )


  case class Output2(vid: String, vin: String, date: String, total: Int, alarm1Cnt: Int, alarm2Cnt: Int, alarm3Cnt: Int)

}
package com.bitnei.alarm.generator

import java.text.SimpleDateFormat

import com.bitnei.alarm.AlarmMarkIntervalDaily
import com.bitnei.alarm.AlarmMarkIntervalDaily.Output
import com.bitnei.report.util.GeoUtils

import scala.annotation.tailrec
import scala.collection.mutable.ArrayBuffer

/**
  *
  *
  * @author zhangyongtian
  * @define 报警标识区间生成
  *
  * create 2018-03-19 18:32
  *
  */

object AlarmMarkIntervalGenerator {


  def handle(arr: Array[AlarmMarkIntervalDaily.Intput], TIMEOUT_DURATION: Long, alarmCol: String): Array[Output] = {

    val sortedArr = arr.sortBy(_.TIME)

    //    for(f<-sortedArr.head.getClass.getDeclaredFields){
    //      println(f.getName)
    //    }

    val vid = sortedArr.head.VID
    val vin = sortedArr.head.VIN

    val res = new ArrayBuffer[Output]()

    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")


    var startIndex = 0
    var stopIndex = 0

    var flag = false //标识是否处于报警状态

    @tailrec
    def handleTailRec(curIndex: Int): Unit = {

      if (curIndex < sortedArr.length) {

        val curInput = sortedArr(curIndex)
        val field = curInput.getClass.getDeclaredField(alarmCol)
        field.setAccessible(true)
        val curValue = field.get(curInput).toString

        if (!flag && curValue.equals("1")) {
          startIndex = curIndex
          flag = true
        }




        //结束条件一： 报警值为0
        if (curValue.equals("0") && curIndex > 0 && flag) {
          stopIndex = curIndex - 1
          val output = Output(vid, vin, sortedArr(startIndex).TIME, sortedArr(stopIndex).TIME, alarmCol, sortedArr(startIndex).maxLevel)
          res.append(output)
          flag = false
        }

        //结束条件二：帧数间隔时间超过阈值
        if (curValue.equals("1") && curIndex > 0 && flag) {
          val lastTimesatmp = sdf.parse(sortedArr(curIndex - 1).TIME).getTime
          val curTimesatmp = sdf.parse(curInput.TIME).getTime

          val diffTimes = (curTimesatmp - lastTimesatmp) / 1000

          if (diffTimes >= TIMEOUT_DURATION && curIndex - 1 >= startIndex) {
            val output = Output(vid, vin, sortedArr(startIndex).TIME, sortedArr(curIndex - 1).TIME, alarmCol, sortedArr(startIndex).maxLevel)
            res.append(output)
            startIndex = curIndex
            flag = true
          }

        }

        //最后一帧
        if (curIndex == sortedArr.length - 1 && curValue.equals("1") && flag) {
          stopIndex = curIndex
          val output = Output(vid, vin, sortedArr(startIndex).TIME, sortedArr(stopIndex).TIME, alarmCol, sortedArr(startIndex).maxLevel)
          res.append(output)
          flag = false
        }


        handleTailRec(curIndex + 1)

      }

    }

    handleTailRec(0)

    res.toArray
  }


}
package com.bitnei.report.dayreport.alarm

import java.sql.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant

/**
  * Created by franciswang on 2016/11/4.
  */
case class AlarmStateMode(
    vid:String,
    time:Long,
  /**
    * 日故障总时间
    */
    fault_time_sum:Int,
  /**
    * 日故障总次数
    */
    fault_times:Int,
  /**
    * 一级故障总数
    */
    fault_level1_times :Int ,
  /**
    * 二级故障总数
    */
    fault_level2_times :Int ,
  /**
    * 三级故障总数
    */
    fault_level3_times :Int ,
  /**
    * 四级故障总数
    */
    fault_level4_times :Int ,
  /**
    * 五级故障总数
    */
    fault_level5_times :Int ,
  /**
    * 温度差异报警次数
    */
    fault_tempdiff_times :Int ,
  /**
    * 电池极柱高温报警次数
    */
    fault_batt_hightemptimes :Int ,
  /**
    * 动力蓄电池包过压报警次数
    */
    fault_battgroup_highvoltimes :Int ,
  /**
    * 动力蓄电池包欠压报警次数
    */
    fault_battgroup_lowvoltimes :Int ,
  /**
    * 单体蓄电池过压报警次数
    */
    fault_batt_highvoltimes :Int ,
  /**
    * 单体蓄电池欠压报警次数
    */
    fault_batt_lowvoltimes :Int ,
  /**
    * 动力蓄电池包不匹配报警次数
    */
    fault_battgroup_nomatch:Int,
  /**
    * 动力蓄电池一致性差报警次数
    */
    fault_battgroup_difftimes:Int ,
  /**
    * SOC低报警次数
    */
    fault_soc_lowtimes:Int ,
  /**
    * SOC太低报警次数
    */
    fault_soc_toolowtimes:Int ,
  /**
    * SOC过高报警次数
    */
    fault_soc_hightimes :Int ,
  /**
    * 绝缘故障报警次数
    */
    fault_insulation_times :Int ,
  /**
    * 温度差异报警比例
    */
    fault_tempdiff_percent:Float,
  /**
    * 电池极柱高温报警比例
    */
    fault_batt_htemppercent :Float,
  /**
    * 动力蓄电池包过压报警比例
    */
    fault_battgroup_hvolpercent :Float,
  /**
    * 动力蓄电池包欠压报警比例
    */
    fault_battgroup_lvolpercent :Float,
  /**
    * 单体蓄电池过压报警比例
    */
    fault_batt_hvolpercent :Float,
  /**
    * 单体蓄电池欠压报警比例
    */
    fault_batt_lvolpercent :Float,
  /**
    * 动力蓄电池包不匹配报警比例
    */
    fault_battgroup_nomatchpercent :Float,
  /**
    * 动力蓄电池一致性差报警比例
    */
    fault_battgroup_diffpercent :Float,
  /**
    * SOC低报警比例
    */
    fault_soc_lowpercent :Float,
  /**
    * SOC太低报警比例
    */
    fault_soc_tlowpercent :Float,
  /**
    * SOC过高报警比例
    */
    fault_soc_highpercent :Float,
  /**
    * 绝缘故障报警比例
    */
    fault_insulation_percent :Float,
  /**
    * 温度差异报警总时间
    */
    fault_tempdiff_timesum:Float,
  /**
    * 电池极柱高温报警时间
    */
    fault_batt_hightemptime_sum :Float,
  /**
    * 动力蓄电池包过压报警时间
    */
    fault_battgroup_hvol_timesum :Float,
  /**
    * 动力蓄电池包欠压报警时间
    */
    fault_battgroup_lvol_timesum :Float,
  /**
    * 单体蓄电池过压报警时间
    */
    fault_batt_hvol_timesum :Float,
  /**
    * 单体蓄电池欠压报警时间
    */
    fault_batt_lvol_timesum :Float,
  /**
    * SOC低报警时间
    */
    fault_soc_lowtimesum :Float,
  /**
    * SOC太低报警时间
    */
    fault_soc_tlowtimesum :Float,
  /**
    * SOC太高报警时间
    */
    fault_soc_hightimesum :Float,
  /**
    * 动力蓄电池包不匹配报警时间
    */
    fault_battgroup_nomatchtimesum :Float,
  /**
    * 动力蓄电池一致性差报警时间
    */
    fault_battgroup_difftimesum :Float,
  /**
    * 绝缘故障报警时间
    */
    fault_insulation_timesum :Float){
}

class AlarmStateModelManager(stateConf:StateConf) extends Serializable with Logging {
  private val tableName=stateConf.getOption(Constant.StateAlarmTable).getOrElse("veh_dayreport_alarm")
  private val sql =stateConf.getString("database") match {
    case "oracle" => s"insert into $tableName t " +
      "(t.id,t.report_time,t.vid,"+
      " t.fault_time_sum,t.fault_times,t.fault_level1_times, " +
      " t.fault_level2_times,t.fault_level3_times,t.fault_level4_times,"+
      " t.fault_level5_times, " +
      " t.fault_tempdiff_times,t.fault_batt_hightemptimes,t.fault_battgroup_highvoltimes, " +
      " t.fault_battgroup_lowvoltimes,t.fault_batt_highvoltimes,t.fault_batt_lowvoltimes, " +
      " t.fault_battgroup_nomatch,t.fault_battgroup_difftimes,t.fault_soc_lowtimes, " +
      " t.fault_soc_toolowtimes,t.fault_soc_hightimes,t.fault_insulation_times, " +
      " t.fault_tempdiff_percent,t.fault_batt_htemppercent,t.fault_battgroup_hvolpercent, " +
      " t.fault_battgroup_lvolpercent,t.fault_battgroup_nomatchpercent,t.fault_battgroup_diffpercent,"+
      " t.fault_batt_hvolpercent,t.fault_batt_lvolpercent,t.fault_soc_lowpercent,"+
      " t.fault_soc_tlowpercent,t.fault_soc_highpercent,t.fault_insulation_percent,"+
      " t.fault_tempdiff_timesum,t.ault_batt_hightemptime_sum,t.fault_battgroup_hvol_timesum,"+
      " t.fault_battgroup_lvol_timesum,t.fault_batt_hvol_timesum,t.fault_batt_lvol_timesum,"+
      " t.fault_soc_lowtimesum,t.fault_soc_tlowtimesum,t.fault_soc_hightimesum,"+
      "t.fault_battgroup_nomatchtimesum,t.fault_battgroup_difftimesum,t.fault_insulation_timesum) " +
      " values (seq_veh_report.Nextval,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?,"+
      "?,?,?)"
    case "mysql" =>
      s"insert into $tableName" +
        "(report_time,vid,fault_time_sum,fault_times,fault_level1_times, " +
        " fault_level2_times,fault_level3_times,fault_level4_times,fault_level5_times, " +
        " fault_tempdiff_times,fault_batt_hightemptimes,fault_battgroup_highvoltimes, " +
        " fault_battgroup_lowvoltimes,fault_batt_highvoltimes,fault_batt_lowvoltimes, " +
        " fault_battgroup_nomatch,fault_battgroup_difftimes,fault_soc_lowtimes, " +
        " fault_soc_toolowtimes,fault_soc_hightimes,fault_insulation_times, " +
        " fault_tempdiff_percent,fault_batt_htemppercent,fault_battgroup_hvolpercent, " +
        " fault_battgroup_lvolpercent,fault_battgroup_nomatchpercent, " +
        " fault_battgroup_diffpercent,fault_batt_hvolpercent,fault_batt_lvolpercent, " +
        " fault_soc_lowpercent,fault_soc_tlowpercent, " +
        " fault_soc_highpercent,fault_insulation_percent,fault_tempdiff_timesum, " +
        " ault_batt_hightemptime_sum,fault_battgroup_hvol_timesum,fault_battgroup_lvol_timesum, " +
        " fault_batt_hvol_timesum,fault_batt_lvol_timesum,fault_soc_lowtimesum, " +
        " fault_soc_tlowtimesum,fault_soc_hightimesum,fault_battgroup_nomatchtimesum, " +
        " fault_battgroup_difftimesum,fault_insulation_timesum) " +
        " values (?,?,?,?,?,?,?,?,?,?, " +
        "         ?,?,?,?,?,?,?,?,?,?," +
        "         ?,?,?,?,?,?,?,?,?,?," +
        "         ?,?,?,?,?,?,?,?,?,?," +
        "         ?,?,?,?,?)"
  }

  def insert(vs: Iterable[AlarmStateMode]):Array[Int]={
    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setDate(1,new Date(v.time))
          stmt.setString(2,v.vid)
          stmt.setDouble(3,v.fault_time_sum)
          stmt.setDouble(4,v.fault_times)
          stmt.setDouble(5,v.fault_level1_times)
          stmt.setDouble(6,v.fault_level2_times)
          stmt.setDouble(7,v.fault_level3_times)
          stmt.setDouble(8,v.fault_level4_times)
          stmt.setDouble(9,v.fault_level5_times)
          stmt.setDouble(10,v.fault_tempdiff_times)
          stmt.setDouble(11,v.fault_batt_hightemptimes)
          stmt.setDouble(12,v.fault_battgroup_highvoltimes)
          stmt.setDouble(13,v.fault_battgroup_lowvoltimes)
          stmt.setDouble(14,v.fault_batt_highvoltimes)
          stmt.setDouble(15,v.fault_batt_lowvoltimes)

          stmt.setDouble(16,v.fault_battgroup_nomatch)
          stmt.setDouble(17,v.fault_battgroup_difftimes)

          stmt.setDouble(18,v.fault_soc_lowtimes)
          stmt.setDouble(19,v.fault_soc_toolowtimes)
          stmt.setInt(20,v.fault_soc_hightimes)
          stmt.setInt(21,v.fault_insulation_times)

          stmt.setDouble(22,v.fault_tempdiff_percent)
          stmt.setDouble(23,v.fault_batt_htemppercent)
          stmt.setDouble(24,v.fault_battgroup_hvolpercent)
          stmt.setDouble(25,v.fault_battgroup_lvolpercent)
          stmt.setDouble(26,v.fault_battgroup_nomatchpercent)
          stmt.setDouble(27,v.fault_battgroup_diffpercent)
          stmt.setDouble(28,v.fault_batt_hvolpercent)
          stmt.setDouble(29,v.fault_batt_lvolpercent)
          stmt.setDouble(30,v.fault_soc_lowpercent)
          stmt.setDouble(31,v.fault_soc_tlowpercent)
          stmt.setDouble(32,v.fault_soc_highpercent)
          stmt.setDouble(33,v.fault_insulation_percent)
          stmt.setDouble(34,v.fault_tempdiff_timesum)
          stmt.setDouble(35,v.fault_batt_hightemptime_sum)
          stmt.setDouble(36,v.fault_battgroup_hvol_timesum)
          stmt.setDouble(37,v.fault_battgroup_lvol_timesum)
          stmt.setDouble(38,v.fault_batt_hvol_timesum)
          stmt.setDouble(39,v.fault_batt_lvol_timesum)
          stmt.setDouble(40,v.fault_soc_lowtimesum)
          stmt.setDouble(41,v.fault_soc_tlowtimesum)
          stmt.setDouble(42,v.fault_soc_hightimesum)
          stmt.setDouble(43,v.fault_battgroup_nomatchtimesum)
          stmt.setDouble(44,v.fault_battgroup_difftimesum)
          stmt.setDouble(45,v.fault_insulation_timesum)
          stmt.addBatch()
        })
      })
    } catch {
      case e: Exception =>
        logError(s"数据在写入到${tableName}中出现异常，${e.toString}")
        e.printStackTrace()
        Array()
    }
  }

  def delete(reportDate:String): Int ={
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"DELETE FROM $tableName WHERE report_time=to_date(?,'yyyy-mm-dd')", stmt => {
        stmt.setString(1, reportDate)
        stmt.addBatch()
      })(0)
  }
}



package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.alarm.{AlarmStateMode, AlarmStateModelManager}
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/11/8.
  */
class AlarmModelTest extends FunSuite{
  test("insert alarm to database"){
    val stateConf=new StateConf

    def configOracle = {
      stateConf.set(Constant.JdbcUserName, "ev")
      stateConf.set(Constant.JdbcPasswd, "ev")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.6.146:1521:evmsc1")
      stateConf.set("database", "oracle")
    }

    def configmysql = {
      stateConf.set(Constant.JdbcUserName, "root")
      stateConf.set(Constant.JdbcPasswd, "")
      stateConf.set(Constant.JdbcDriver, "com.mysql.jdbc.Driver")
      stateConf.set(Constant.JdbcUrl, "jdbc:mysql://192.168.6.146:3306/evsmc")
      stateConf.set("database", "mysql")
    }

    configmysql
    new AlarmStateModelManager(stateConf).insert(Array(new AlarmStateMode("ssssssssss",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)))
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.recommendation.ALS
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * An example demonstrating ALS.
 * Run with
 * {{{
 * bin/run-example ml.ALSExample
 * }}}
 */
object ALSExample {

  // $example on$
  case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)
  def parseRating(str: String): Rating = {
    val fields = str.split("::")
    assert(fields.size == 4)
    Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)
  }
  // $example off$

  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("ALSExample")
      .getOrCreate()
    import spark.implicits._

    // $example on$
    val ratings = spark.read.textFile("data/mllib/als/sample_movielens_ratings.txt")
      .map(parseRating)
      .toDF()
    val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))

    // Build the recommendation model using ALS on the training data
    val als = new ALS()
      .setMaxIter(5)
      .setRegParam(0.01)
      .setUserCol("userId")
      .setItemCol("movieId")
      .setRatingCol("rating")
    val model = als.fit(training)

    // Evaluate the model by computing the RMSE on the test data
    val predictions = model.transform(test)

    val evaluator = new RegressionEvaluator()
      .setMetricName("rmse")
      .setLabelCol("rating")
      .setPredictionCol("prediction")
    val rmse = evaluator.evaluate(predictions)
    println(s"Root-mean-square error = $rmse")
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

import scala.collection.mutable

import org.apache.spark._
import org.apache.spark.graphx._
import org.apache.spark.graphx.PartitionStrategy._
import org.apache.spark.graphx.lib._
import org.apache.spark.internal.Logging
import org.apache.spark.storage.StorageLevel

/**
 * Driver program for running graph algorithms.
 */
object Analytics extends Logging {

  def main(args: Array[String]): Unit = {
    if (args.length < 2) {
      System.err.println(
        "Usage: Analytics <taskType> <file> --numEPart=<num_edge_partitions> [other options]")
      System.err.println("Supported 'taskType' as follows:")
      System.err.println("  pagerank    Compute PageRank")
      System.err.println("  cc          Compute the connected components of vertices")
      System.err.println("  triangles   Count the number of triangles")
      System.exit(1)
    }

    val taskType = args(0)
    val fname = args(1)
    val optionsList = args.drop(2).map { arg =>
      arg.dropWhile(_ == '-').split('=') match {
        case Array(opt, v) => (opt -> v)
        case _ => throw new IllegalArgumentException("Invalid argument: " + arg)
      }
    }
    val options = mutable.Map(optionsList: _*)

    val conf = new SparkConf()
    GraphXUtils.registerKryoClasses(conf)

    val numEPart = options.remove("numEPart").map(_.toInt).getOrElse {
      println("Set the number of edge partitions using --numEPart.")
      sys.exit(1)
    }
    val partitionStrategy: Option[PartitionStrategy] = options.remove("partStrategy")
      .map(PartitionStrategy.fromString(_))
    val edgeStorageLevel = options.remove("edgeStorageLevel")
      .map(StorageLevel.fromString(_)).getOrElse(StorageLevel.MEMORY_ONLY)
    val vertexStorageLevel = options.remove("vertexStorageLevel")
      .map(StorageLevel.fromString(_)).getOrElse(StorageLevel.MEMORY_ONLY)

    taskType match {
      case "pagerank" =>
        val tol = options.remove("tol").map(_.toFloat).getOrElse(0.001F)
        val outFname = options.remove("output").getOrElse("")
        val numIterOpt = options.remove("numIter").map(_.toInt)

        options.foreach {
          case (opt, _) => throw new IllegalArgumentException("Invalid option: " + opt)
        }

        println("======================================")
        println("|             PageRank               |")
        println("======================================")

        val sc = new SparkContext(conf.setAppName("PageRank(" + fname + ")"))

        val unpartitionedGraph = GraphLoader.edgeListFile(sc, fname,
          numEdgePartitions = numEPart,
          edgeStorageLevel = edgeStorageLevel,
          vertexStorageLevel = vertexStorageLevel).cache()
        val graph = partitionStrategy.foldLeft(unpartitionedGraph)(_.partitionBy(_))

        println("GRAPHX: Number of vertices " + graph.vertices.count)
        println("GRAPHX: Number of edges " + graph.edges.count)

        val pr = (numIterOpt match {
          case Some(numIter) => PageRank.run(graph, numIter)
          case None => PageRank.runUntilConvergence(graph, tol)
        }).vertices.cache()

        println("GRAPHX: Total rank: " + pr.map(_._2).reduce(_ + _))

        if (!outFname.isEmpty) {
          logWarning("Saving pageranks of pages to " + outFname)
          pr.map { case (id, r) => id + "\t" + r }.saveAsTextFile(outFname)
        }

        sc.stop()

      case "cc" =>
        options.foreach {
          case (opt, _) => throw new IllegalArgumentException("Invalid option: " + opt)
        }

        println("======================================")
        println("|      Connected Components          |")
        println("======================================")

        val sc = new SparkContext(conf.setAppName("ConnectedComponents(" + fname + ")"))
        val unpartitionedGraph = GraphLoader.edgeListFile(sc, fname,
          numEdgePartitions = numEPart,
          edgeStorageLevel = edgeStorageLevel,
          vertexStorageLevel = vertexStorageLevel).cache()
        val graph = partitionStrategy.foldLeft(unpartitionedGraph)(_.partitionBy(_))

        val cc = ConnectedComponents.run(graph)
        println("Components: " + cc.vertices.map { case (vid, data) => data }.distinct())
        sc.stop()

      case "triangles" =>
        options.foreach {
          case (opt, _) => throw new IllegalArgumentException("Invalid option: " + opt)
        }

        println("======================================")
        println("|      Triangle Count                |")
        println("======================================")

        val sc = new SparkContext(conf.setAppName("TriangleCount(" + fname + ")"))
        val graph = GraphLoader.edgeListFile(sc, fname,
          canonicalOrientation = true,
          numEdgePartitions = numEPart,
          edgeStorageLevel = edgeStorageLevel,
          vertexStorageLevel = vertexStorageLevel)
          // TriangleCount requires the graph to be partitioned
          .partitionBy(partitionStrategy.getOrElse(RandomVertexCut)).cache()
        val triangles = TriangleCount.run(graph)
        println("Triangles: " + triangles.vertices.map {
          case (vid, data) => data.toLong
        }.reduce(_ + _) / 3)
        sc.stop()

      case _ =>
        println("Invalid task type.")
    }
  }
}
// scalastyle:on println
package com.bitnei.alarm

import org.apache.commons.math3.stat.descriptive.DescriptiveStatistics

/**
  *
  * @author zhangyongtian
  * @define 数值计算
  *
  * create 2018-03-21 14:15
  *
  */
object apacheMath3Test {

  def main(args: Array[String]): Unit = {
    val ds = new DescriptiveStatistics()

    ds.addValue(1)
    ds.addValue(2)
    ds.addValue(3)


    val median = ds.getPercentile(50)


    println("数据集中的全部元素：")
    println(java.util.Arrays.toString(ds.getValues))
    println("数据集的算数平均数是：" + ds.getMean)
    println("数据集的几何平均数是：" + ds.getGeometricMean)
    println("数据集的方差是：" + ds.getVariance)
    println("数据集的标准方差是：" + ds.getStandardDeviation)
    println("数据集的和是：" + ds.getSum)
    println("数据集的平方和是：" + ds.getSumsq)
    println("数据集的最大值是：" + ds.getMax)
    println("数据集的最小值是：" + ds.getMin)
    println("数据集的中位数是：" + ds.getPercentile(50))
    println("数据集的偏度是：" + ds.getSkewness)
    println("数据集的峰度是：" + ds.getKurtosis)



  }
}
package com.bitnei.tools

import scala.collection.mutable.ArrayBuffer

/**
  * @author ${user.name}
  */
object App {

  def foo(x: Array[String]) = x.foldLeft("")((a, b) => a + b)

  def getSubArr(arr: Array[Int], range: Range): _root_.scala.Array[Int] = {

    if (range.length > arr.length) {
      throw new RuntimeException("range is out of arr's length !")
    }
    val res = new ArrayBuffer[Int]()

    for (i <- range) {
      res.append(arr(i))
    }
    res.toArray
  }

  def getSubArr(arr: Array[Int], step: Int): _root_.scala.Array[Int] = {

    val res = new ArrayBuffer[Int]()

    arr.splitAt(3)

    //    for (i <- range) {
    //      res.append(arr(i))
    //    }
    res.toArray
  }


  def main(args: Array[String]) {
    //    println("Hello World!")
    //    println("concat arguments = " + foo(args))

    import scala.Array._
    val arr = Array(1, 2, 3, 4, 5)

    //    val range = Range(0, 4)

    //    val subArr: Array[Int] = getSubArr(arr, range)

    //    subArr.foreach(println)

    val res = new ArrayBuffer[Array[Int]]

    val step = 20

    var index = 0
    while (index <= arr.length) {
      val start = index
      val next = index + step
      res.append(arr.slice(start, next))
      index = next
    }

    res.foreach(x => {
      x.foreach(println)
      println("-----------------")
    })
  }

}
package com.bitnei.samples.apply

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-04 11:00
  *
  */
object ApplyDemo {

  // The injection method (optional)
  def apply(user: String, domain: String) = {
    user + "@" + domain
  }

  // The extraction method (mandatory)
  def unapply(str: String): Option[(String, String)] = {
    val parts = str split "@"

    if (parts.length == 2) {
      Some(parts(0), parts(1))
    } else {
      None
    }
  }

  def apply(x: Int) = {

    println("调用apply....")
    x * 2
  }

  def unapply(z: Int): Option[Int] = {
    println("调用unapply...")
    if (z % 2 == 0) Some(z / 2) else None
  }


  def main(args: Array[String]): Unit = {
    ApplyDemo(9)

    val x = 9 match {
      case ApplyDemo(9) => "haha"
      case _ => "..."
    }
  }
}
package com.bitnei.alarm

import org.junit._
import Assert._

@Test
class AppTest {

    @Test
    def testOK() = assertTrue(true)

//    @Test
//    def testKO() = assertTrue(false)



}


package com.bitnei.report.taxis

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.SparkHelper
import org.apache.spark.sql.{Dataset, SparkSession}


/**
  * Created by wangbaosheng on 2017/9/27.
  */
case class Area(id:String, pathId:String, areaName:String,pathName:String=""){
  override def toString: String = s"$pathName"
}

//<id,full path>
class AreaUtil (stateConf: StateConf,sparkSession: SparkSession) {

  import sparkSession.implicits._

  private lazy val areaMap: Map[String, Area] = this.areasFromDb

  lazy val getAreas: Seq[Area] = areaMap.map({ case (id, area) =>
    Area(id, area.pathId, area.areaName, getFullAreaPathByAreaId(areaMap, area))
  }).toSeq


  def areaTable: Dataset[Area] = {
    sparkSession.createDataset(getAreas)
  }


  private def areasFromDb: Map[String, Area] = {
    val sql =
      """
       |SELECT id,path,name
       |FROM sys_area
       |WHERE path IS NOT NULL
     """.stripMargin

    val areas= scala.collection.mutable.Map[String,Area]()
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).executeQuery(sql,stmt => {
      while (stmt.next()) {
        val area = Area(id = stmt.getString(1), pathId = stmt.getString(2), areaName = stmt.getString(3))
        areas.put(area.id, area)
      }
    })

    areas.toMap
   }


  private def getFullAreaPathByAreaId(areaList: Map[String, Area], area: Area): String = {
    val childAreaIdList = area.pathId.split('/')
    val childNameList= childAreaIdList.map(childAreaId => {
      if (areaList.contains(childAreaId)) {
        if(childAreaId== "172") "上海市,上海市"
        else if(childAreaId == "110") "北京市,北京市"
        else if (childAreaId == "180") "天津市,天津市"
        else if (childAreaId == "176") "重庆市,重庆市"
        else areaList(childAreaId).areaName
      }
      else ""
    })

    val fullName= childNameList.foldLeft("")((a, b) => if (a.isEmpty) b else s"$a,$b")

    if(fullName.contains("重庆市")&&fullName.count(_==',')==4){
      var first=true
      val indexOfChongqingShi=fullName.indexOf("重庆市")

      val head=fullName.substring(0,indexOfChongqingShi+3)
      val tail=fullName.substring(indexOfChongqingShi+8)

     // println(fullName)
      val result=s"$head,$tail"
     // println(result)
      result
    }else {
      val paddingLength=
        if(fullName.contains("上海市")||fullName.contains("北京市")||fullName.contains("天津市")||fullName.contains("重庆市")){
          4-childNameList.length-1
        }else 4-childNameList.length

      (0 to paddingLength).map(x => ",").reduceOption((a, b) => s" $a$b") match {
        case Some(padding) => fullName + padding
        case None => fullName
      }
    }
  }
}

object AreaUtil {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf

    def configOracle(): Unit = {
      stateConf.set(Constant.JdbcUserName, "ev")
      stateConf.set(Constant.JdbcPasswd, "ev")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.2.51:1521:evmsc1")
      stateConf.set("database", "oracle")
    }

    configOracle()
    val sparkSession=SparkSession.builder().master("local").config("spark.testing.memory","471859200").getOrCreate()

    val areaUtils = new AreaUtil(stateConf, sparkSession)
    areaUtils.areaTable.collect().foreach(println)
  }
}package com.bitnei.report.distribute

import scala.collection.mutable


/**
  * @param start 等差分布范围的最小值
  * @param end 等差分布范围的最大值
  * @param interval 等差分布的间距
  * @param startInitValue 等差分布的起始值
  * @param endInitValue 等差分布的结束值
  *
  * */
case class ArithmeticDistributed( start:Int,
                             end:Int,
                            interval:Int,
                            startInitValue:Int,
                            endInitValue:Int,
                            initValues:mutable.Traversable[Int])extends Serializable{
  require(interval!=0)

  def this(start:Int,end:Int,interval:Int, startInitValue:Int,endInitValue:Int)=this(start,end,interval,startInitValue,endInitValue,null)
  def this(start:Int,end:Int,inter:Int)=this(start,end,inter,0,-1)
  def this(start:Int,end:Int,inter:Int,initValues:mutable.Traversable[Int])=this(start,end,inter,0,-1,initValues)

  val len:Int =(end-start)/interval

  private val distributed=mutable.Buffer[Int]()




  if(initValues==null){
    //如果没有指定默认值，就初始化为0
    for(i<-0 until len) distributed.append(0)
  }else{
    initValues.foreach(distributed.append(_))
    for(i<-initValues.size until len) distributed.append(0)
  }

  if(startInitValue<=endInitValue){
    try {
      val startI = toIndex(startInitValue)
      val endI = toIndex(endInitValue)
      for (i <- startI to endI) distributed(i) += 1
    }catch {
      case e:IndexOutOfBoundsException=> throw new IndexOutOfBoundsException(
        "ArimeticDistributed(start=%d,end=%d,startv=%d,endv=%d,startI=%d,endI=%d,length=%d)=%s".format(start,end,startInitValue,endInitValue,toIndex(startInitValue),toIndex(endInitValue),distributed.length,distributed.toString()))
    }
  }






  def +(that:ArithmeticDistributed):ArithmeticDistributed={
    val dis= new ArithmeticDistributed(start,end,interval)
    for(i<- this.distributed.indices)
      dis.distributed(i)=this.distributed(i)+that.distributed(i)
    dis
  }



  def toIndex(v:Int):Int={
    if(v%interval==0){
      if(v==start) 0
      else (v-start-interval)/interval
    }else{
      (v-start)/interval
    }
  }

  def apply(start:Int,end:Int):Int={
    var count:Int=0
    val startI=toIndex(start)
    val endI=toIndex(end)

    for(i<-startI to endI) count+=distributed(i)

    count
  }


  def apply(index:Int):Int={
    distributed(index)
  }

  override def toString: String ={
    val strBuilder=new mutable.StringBuilder()
    distributed.foreach(v=>{
      strBuilder.append(v)
      strBuilder.append(",")
    })

    strBuilder.lastOption match {
      case Some(',')=>strBuilder.deleteCharAt(strBuilder.length-1)
      case _ =>
    }

    strBuilder.toString()
  }

}
package com.bitnei.report.common.mileage

import com.bitnei.report.{ArrayUndoMonidFoldable, FoldMonod, Foldable, FoldableBaseHead}

/*
* created by wangbaosheng on 2017/12/15
*/
class ArrayFoldableBaseHead extends FoldableBaseHead[Array] {
  override def foldLeft[A, B](f: Array[A], acc: FoldMonod[A, B]): B = {
    //对每日的运营指标查询结果累加。
    f.tail.foldLeft(acc.zero())(acc.fold)
  }
}


package com.bitnei.alarm

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-19 18:07
  *
  */
object ArrayTest {

  def main(args: Array[String]): Unit = {
//    val arr = (ArrayBuffer(1, 2) ++ ArrayBuffer(3, 4))
//    arr.foreach(println)

//    val a = 151.1D
//    val b = 151.2D
//
//    println(b-a)


    val SPEED_INTERVALS_MAP = Map[String, Array[Double]](
      "0-30" -> Array(0.001D, 30D),
      "31-50" -> Array(31D, 50D),
      "51-80" -> Array(51D, 80D),
      "81-" -> Array(81D, Integer.MAX_VALUE)
    )

    val SPEED_INTERVALS = SPEED_INTERVALS_MAP.keySet.toArray

    println(SPEED_INTERVALS(0))
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.fpm.AssociationRules
import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset
// $example off$

object AssociationRulesExample {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("AssociationRulesExample")
    val sc = new SparkContext(conf)

    // $example on$
    val freqItemsets = sc.parallelize(Seq(
      new FreqItemset(Array("a"), 15L),
      new FreqItemset(Array("b"), 35L),
      new FreqItemset(Array("a", "b"), 12L)
    ))

    val ar = new AssociationRules()
      .setMinConfidence(0.8)
    val results = ar.run(freqItemsets)

    results.collect().foreach { rule =>
      println("[" + rule.antecedent.mkString(",")
        + "=>"
        + rule.consequent.mkString(",") + "]," + rule.confidence)
    }
    // $example off$
  }

}
// scalastyle:on println
package com.bitnei.report

import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import org.apache.hadoop.fs.FileSystem

import scala.collection.mutable.ArrayBuffer


case class BlockSize(startIndex:Int,endIndex: Int,size:Long)
case class BlockFile(files:Array[String],size:Long)

/**
  * Created by wangbaosheng on 2017/4/20.
  * 当一个目录下面的文件大小差距非常大时，比如一个目录下面的60%的文件都是小文件，那么有必要将这些小文件进行合并成一个大文件。
  * 该算法的输入是一个文件目录，假设该文件目录下的文件集合file={file1,file2,file3,...,filen}
  * 该算法的输出是一个block file集合：{block file 1,block file 2,......,block file k},其中block file i包含m个文件，这个m个文件的总大小将大于threshold。
  * */
trait AutoMergeFile extends  Logging{
  def getBlockFiles(fs:FileSystem,inputPath:String,threshold:Long)= {
    val childFiles: Array[(String, Long)] = Utils.getChildFiles(fs, inputPath)
    val childFilesSortedByFileSize = childFiles.sortBy(_._2)
    //按照文件大小升序排序
    val blockFiles = mergeFile(childFilesSortedByFileSize, threshold)

    logInfo(s"the parquet threshold is $threshold file merged num is ${blockFiles.size}")
    blockFiles.foreach(bf => logInfo(s"block file size is ${bf.size / (1024 * 1024)},block file num=${bf.files.size}"))
    //blockFiles.foreach(bf => bf.files.foreach(f => logInfo("blockFile:" + f)))
    blockFiles
  }

  /**
    * 将sortedSizeSet聚合为BlockSize数组，其中每一个BlockSize的总大小大于等于输入的阈值threshold。
    *
    * @param sortedSizeSet 升序排序数组
    * @param threshold     BlockSize阈值，超过threshold后将生产一个新的BlockSize。
    **/
  def mergeSize(sortedSizeSet: Array[Long], threshold: Long): Array[BlockSize] = {
    val blockSizeArray = new ArrayBuffer[BlockSize]()

    def doMergeSize(start:Int,curIndex:Int, size: Long): Unit = {
      if (size >= threshold) {
        val newBlockSize=BlockSize(start,endIndex = curIndex, size)
        blockSizeArray.append(newBlockSize)
        if ((curIndex + 1) < sortedSizeSet.length) doMergeSize(curIndex + 1,curIndex+1, sortedSizeSet(curIndex + 1))
      } else {
        if ((curIndex + 1) < sortedSizeSet.length) {
          doMergeSize(start, curIndex+1,size + sortedSizeSet(curIndex + 1))
        } else {
          blockSizeArray.append(BlockSize(startIndex=start,endIndex = curIndex, size))
        }
      }
    }

    doMergeSize(0,0, sortedSizeSet(0))


    assert(sortedSizeSet.size==blockSizeArray.foldLeft(0)((a,b)=>a+b.endIndex-b.startIndex+1))

    blockSizeArray.toArray
  }

  /**
    *
    * @param filesSortedByFileSize:Array[String,Long],其中String是文件路径，Long是文件大小，filesSortedByFileSize其中的元素按照文件大小升序排序。
    *
    * */
  def mergeFile(filesSortedByFileSize: Array[(String, Long)], threshold: Long): Array[BlockFile] = {
    val childFileSizes = filesSortedByFileSize.map(_._2)
    val blockSizes = mergeSize(childFileSizes, threshold)

    val blockFiles = blockSizes.map(blockSize => {
      val filePaths = (blockSize.startIndex to blockSize.endIndex).map(filesSortedByFileSize(_)._1).toArray
      BlockFile(filePaths,blockSize.size)
    })
    blockFiles
  }
}

package com.bitnei.report

import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import org.apache.hadoop.fs.FileSystem

/**
  * Created by wangbaosheng on 2017/4/17
  *
  * 根据最近n天的文件大小预测当前文件大小和当前分区数。.
  */
trait AutoPartition extends Logging{
  /**
    *   @param path 最近n天的文件路径，其中path[i]的时间大于path[i+1]
    *   @return 返回当前分区数的预测值
    * */
  def getPartitionNum(fs:FileSystem,path:Array[String]): Int = {
    /**
      * 1.使用加权移动平均值算法来预测当前的文件大小
      * 2.当前分区数应该是文件大小的一个函数，且随着文件大小的增加，分区数也应该增加
      * 分区数=threshold*文件大小,其中0<threshold<1,threshold=用户预设的每一个文件大小的倒数
      */
    val currentFileSize = estimateCurrentFileSize(fs, path)
    val parNum = Math.ceil(currentFileSize / getThreshold).toInt
    if (parNum < 2) {
      logWarning(s"the partition num is $parNum,the file size estimated  is ${path.length},the threshold size is $getThreshold , the current file size estimated is $currentFileSize")
      8
    } else parNum
  }

  /**
    * 使用指数移动平均值来预测当前文件大小，这里的权重设置为0.7
    * */
  def estimateCurrentFileSize(fs:FileSystem,path:Array[String]):Long= {
    val w = 0.7D

    if (path.size < 2) 0L
    else {
      val lastN = Utils.getChildFiles(fs, path(0)).foldLeft(0L)((a, b) => a + b._2)
      val lastN_1 = Utils.getChildFiles(fs, path(1)).foldLeft(0L)((a, b) => a + b._2)

      Math.ceil(lastN * w + lastN_1 * (1 - w)).toLong
    }
  }

  def getThreshold: Long
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.pythonconverters

import java.util.{Collection => JCollection, Map => JMap}

import scala.collection.JavaConverters._

import org.apache.avro.generic.{GenericFixed, IndexedRecord}
import org.apache.avro.mapred.AvroWrapper
import org.apache.avro.Schema
import org.apache.avro.Schema.Type._

import org.apache.spark.api.python.Converter
import org.apache.spark.SparkException


object AvroConversionUtil extends Serializable {
  def fromAvro(obj: Any, schema: Schema): Any = {
    if (obj == null) {
      return null
    }
    schema.getType match {
      case UNION => unpackUnion(obj, schema)
      case ARRAY => unpackArray(obj, schema)
      case FIXED => unpackFixed(obj, schema)
      case MAP => unpackMap(obj, schema)
      case BYTES => unpackBytes(obj)
      case RECORD => unpackRecord(obj)
      case STRING => obj.toString
      case ENUM => obj.toString
      case NULL => obj
      case BOOLEAN => obj
      case DOUBLE => obj
      case FLOAT => obj
      case INT => obj
      case LONG => obj
      case other => throw new SparkException(s"Unknown Avro schema type ${other.getName}")
    }
  }

  def unpackRecord(obj: Any): JMap[String, Any] = {
    val map = new java.util.HashMap[String, Any]
    obj match {
      case record: IndexedRecord =>
        record.getSchema.getFields.asScala.zipWithIndex.foreach { case (f, i) =>
          map.put(f.name, fromAvro(record.get(i), f.schema))
        }
      case other => throw new SparkException(
        s"Unsupported RECORD type ${other.getClass.getName}")
    }
    map
  }

  def unpackMap(obj: Any, schema: Schema): JMap[String, Any] = {
    obj.asInstanceOf[JMap[_, _]].asScala.map { case (key, value) =>
      (key.toString, fromAvro(value, schema.getValueType))
    }.asJava
  }

  def unpackFixed(obj: Any, schema: Schema): Array[Byte] = {
    unpackBytes(obj.asInstanceOf[GenericFixed].bytes())
  }

  def unpackBytes(obj: Any): Array[Byte] = {
    val bytes: Array[Byte] = obj match {
      case buf: java.nio.ByteBuffer =>
        val arr = new Array[Byte](buf.remaining())
        buf.get(arr)
        arr
      case arr: Array[Byte] => arr
      case other => throw new SparkException(
        s"Unknown BYTES type ${other.getClass.getName}")
    }
    val bytearray = new Array[Byte](bytes.length)
    System.arraycopy(bytes, 0, bytearray, 0, bytes.length)
    bytearray
  }

  def unpackArray(obj: Any, schema: Schema): JCollection[Any] = obj match {
    case c: JCollection[_] =>
      c.asScala.map(fromAvro(_, schema.getElementType)).toSeq.asJava
    case arr: Array[_] if arr.getClass.getComponentType.isPrimitive =>
      arr.toSeq.asJava.asInstanceOf[JCollection[Any]]
    case arr: Array[_] =>
      arr.map(fromAvro(_, schema.getElementType)).toSeq.asJava
    case other => throw new SparkException(
      s"Unknown ARRAY type ${other.getClass.getName}")
  }

  def unpackUnion(obj: Any, schema: Schema): Any = {
    schema.getTypes.asScala.toList match {
      case List(s) => fromAvro(obj, s)
      case List(n, s) if n.getType == NULL => fromAvro(obj, s)
      case List(s, n) if n.getType == NULL => fromAvro(obj, s)
      case _ => throw new SparkException(
        "Unions may only consist of a concrete type and null")
    }
  }
}

/**
 * Implementation of [[org.apache.spark.api.python.Converter]] that converts
 * an Avro IndexedRecord (e.g., derived from AvroParquetInputFormat) to a Java Map.
 */
class IndexedRecordToJavaConverter extends Converter[IndexedRecord, JMap[String, Any]]{
  override def convert(record: IndexedRecord): JMap[String, Any] = {
    if (record == null) {
      return null
    }
    val map = new java.util.HashMap[String, Any]
    AvroConversionUtil.unpackRecord(record)
  }
}

/**
 * Implementation of [[org.apache.spark.api.python.Converter]] that converts
 * an Avro Record wrapped in an AvroKey (or AvroValue) to a Java Map. It tries
 * to work with all 3 Avro data mappings (Generic, Specific and Reflect).
 */
class AvroWrapperToJavaConverter extends Converter[Any, Any] {
  override def convert(obj: Any): Any = {
    if (obj == null) {
      return null
    }
    obj.asInstanceOf[AvroWrapper[_]].datum() match {
      case null => null
      case record: IndexedRecord => AvroConversionUtil.unpackRecord(record)
      case other => throw new SparkException(
        s"Unsupported top-level Avro data type ${other.getClass.getName}")
    }
  }
}
package com.bitnei.tools.util

/**
  *
  * @author zhangyongtian
  * @define base64 工具类
  *
  *                create 2017-12-05 12:55
  *
  */

import java.io.UnsupportedEncodingException
import java.math.BigInteger

object Base64Utils {

  private val base64Table = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"
  private val add = "="

  /**
    * base64编码
    */
  def encode(str: String, charsetName: String): String = {
    val base64Str = new StringBuilder

    val bytesStr = str.getBytes(charsetName)
    // 编码为二进制字符串
    var bytesBinary = binary(bytesStr, 2)
    // 24位为一组，不够后面补0,计算出需要补充的个数
    var addCount = 0
    while ( {
      bytesBinary.length % 24 != 0
    }) {
      bytesBinary += "0"
      addCount += 1
    }
    var i = 0
    while ( {
      i <= bytesBinary.length - 6
    }) { // 二进制转十进制
      val index = Integer.parseInt(bytesBinary.substring(i, i + 6), 2)
      // 如果是有补位的6个0组成，则转换为'='
      if (index == 0 && i >= bytesBinary.length - addCount) base64Str.append(add)
      else base64Str.append(base64Table.charAt(index))

      i += 6
    }
    base64Str.toString
  }

  /**
    * base64解码
    */
  def decode(base64str: String, charsetName: String): String = {
    var base64Binarys = ""
    var i = 0
    while ( {
      i < base64str.length
    }) {
      val s = base64str.charAt(i)
      if (s != '=') { // 十进制转二进制
        var binary = Integer.toBinaryString(base64Table.indexOf(s))
        // 不够六位进行补位
        while ( {
          binary.length != 6
        }) binary = "0" + binary
        base64Binarys += binary
      }

      {
        i += 1;
        i - 1
      }
    }
    // 长度应该是8的倍数，去除后面多余的0
    base64Binarys = base64Binarys.substring(0, base64Binarys.length - base64Binarys.length % 8)
    val bytesStr = new Array[Byte](base64Binarys.length / 8)
    var bytesIndex = 0
    while ( {
      bytesIndex < base64Binarys.length / 8
    }) { // 八位截取一次，转化为一个字节
      bytesStr(bytesIndex) = Integer.parseInt(base64Binarys.substring(bytesIndex * 8, bytesIndex * 8 + 8), 2).toByte

      {
        bytesIndex += 1;
        bytesIndex - 1
      }
    }
    try
      new String(bytesStr, charsetName)
    catch {
      case e: UnsupportedEncodingException =>
        throw new RuntimeException(e)
    }
  }

  /**
    * 字节数组转自定义进制字符串
    */
  def binary(bytes: Array[Byte], radix: Int): String = { // 转化为二进制字符串   1代表正数,如果第一个字节以0开头，转化后会省略，所以要重新补位
    var strBytes = new BigInteger(1, bytes).toString(radix)
    while ( {
      strBytes.length % 8 != 0
    }) strBytes = "0" + strBytes
    strBytes
  }
}package com.bitnei.report.util.jdbc

import java.sql.{Connection, PreparedStatement, ResultSet, Statement}

///**
//  *
//  * @author zhangyongtian
//  * @define
//  *
//  * create 2018-03-28 15:40
//  *
//  */
//private[org] abstract class BaseDao[T](conn: Connection) {
//
//
//  /**
//    * 插入数据
//    *
//    * @param sql     SQL语句
//    * @param params  参数列表
//    * @param convert 主键转换方法
//    * @return 转换结果
//    */
//  protected def insert[T](sql: String, params: Array[Any])(convert: ResultSet => T) = {
//    val pstmt = conn prepareStatement(sql, Statement.RETURN_GENERATED_KEYS)
//    setParameters(pstmt, params)
//    pstmt.executeUpdate
//    val rs = pstmt.getGeneratedKeys
//    rs.next
//    convert(rs)
//  }
//
//  /**
//    * 更新数据
//    *
//    * @param sql    SQL语句
//    * @param params 参数列表
//    * @return 影响行数
//    */
//  protected def update(sql: String, params: Array[Any]) = createStatement(sql, params).executeUpdate
//
//  /**
//    * 查询对象
//    *
//    * @param sql     SQL语句
//    * @param params  参数列表
//    * @param convert 结果集转换方法
//    * @return 泛型对象
//    */
//  protected def queryForObject[T](sql: String, params: Array[Any])(convert: ResultSet => T) = {
//    val rs = query(sql, params)
//    if (rs.next) {
//      val result = convert(rs)
//      if (rs.next) {
//        val ex = new ResultsTooManyException
//        throw ex
//      } else Some(result)
//    } else None
//  }
//
//  /**
//    * 查询对象列表
//    *
//    * @param sql     SQL语句
//    * @param params  参数列表
//    * @param convert 结果集转换方法
//    * @return 泛型对象列表
//    */
//  protected def queryForList[T](sql: String, params: Array[Any])(convert: ResultSet => T) = {
//    val rs = query(sql, params)
//    var results = List[T]()
//    while (rs.next) {
//      results = results :+ convert(rs)
//    }
//    results
//  }
//
//  /**
//    * 查询对象映射
//    *
//    * @param sql     SQL语句
//    * @param params  参数列表
//    * @param convert 结果集转换方法
//    * @return 泛型对象映射
//    */
//  protected def queryForMap[K, V](sql: String, params: Array[Any])(convert: ResultSet => (K, V)) = {
//    val rs = query(sql, params)
//    var results = Map[K, V]()
//    while (rs.next) {
//      results += convert(rs)
//    }
//    results
//  }
//
//  /**
//    * 查询
//    *
//    * @param sql    SQL语句
//    * @param params 参数列表
//    */
//  private def query(sql: String, params: Array[Any]) = createStatement(sql, params).executeQuery
//
//  /**
//    * 创建声明
//    *
//    * @param sql    SQL语句
//    * @param params 参数列表
//    */
//  private def createStatement(sql: String, params: Array[Any]) = {
//    val pstmt = conn prepareStatement sql
//    setParameters(pstmt, params)
//    pstmt
//  }
//
//  /**
//    * 插入参数
//    *
//    * @param pstmt  预编译声明
//    * @param params 参数列表
//    */
//  private def setParameters(pstmt: PreparedStatement, params: Array[Any]) {
//    for (i <- 1 to params.length) {
//      pstmt setObject(i, params(i - 1))
//    }
//  }
//
//}
//
///**
//  * 结果值读取器
//  */
//object ResultValueGetter {
//
//  /**
//    * 查询结果值
//    *
//    * @param rs        结果集
//    * @param getResult 获得单个值结果的方法
//    * @return 值
//    */
//  def getResultValue[T](rs: ResultSet)(getResult: ResultSet => T) = {
//    val result = getResult(rs)
//    if (rs.wasNull) None else Some(result)
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs     结果集
//    * @param colNum 列号
//    */
//  def getStringValue(rs: ResultSet, colNum: Int) = getResultValue(rs) {
//    _ getString colNum
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs     结果集
//    * @param colNum 列号
//    */
//  def getIntValue(rs: ResultSet, colNum: Int) = getResultValue(rs) {
//    _ getInt colNum
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs     结果集
//    * @param colNum 列号
//    */
//  def getLongValue(rs: ResultSet, colNum: Int) = getResultValue(rs) {
//    _ getLong colNum
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs     结果集
//    * @param colNum 列号
//    */
//  def getDoubleValue(rs: ResultSet, colNum: Int) = getResultValue(rs) {
//    _ getDouble colNum
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs     结果集
//    * @param colNum 列号
//    */
//  def getBooleanValue(rs: ResultSet, colNum: Int) = getResultValue(rs) {
//    _ getBoolean colNum
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs     结果集
//    * @param colNum 列号
//    */
//  def getTimestampValue(rs: ResultSet, colNum: Int) = getResultValue(rs) {
//    _ getTimestamp colNum
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs      结果集
//    * @param colName 列名
//    */
//  def getStringValue(rs: ResultSet, colName: String) = getResultValue(rs) {
//    _ getString colName
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs      结果集
//    * @param colName 列名
//    */
//  def getIntValue(rs: ResultSet, colName: String) = getResultValue(rs) {
//    _ getInt colName
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs      结果集
//    * @param colName 列名
//    */
//  def getLongValue(rs: ResultSet, colName: String) = getResultValue(rs) {
//    _ getLong colName
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs      结果集
//    * @param colName 列名
//    */
//  def getDoubleValue(rs: ResultSet, colName: String) = getResultValue(rs) {
//    _ getDouble colName
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs      结果集
//    * @param colName 列名
//    */
//  def getBooleanValue(rs: ResultSet, colName: String) = getResultValue(rs) {
//    _ getBoolean colName
//  }
//
//  /**
//    * 获得字符串结果的值
//    *
//    * @param rs      结果集
//    * @param colName 列名
//    */
//  def getTimestampValue(rs: ResultSet, colName: String) = getResultValue(rs) {
//    _ getTimestamp colName
//  }
//
//}
//
///**
//  * 结果太多异常
//  */
//class ResultsTooManyException extends Exception("Returned too many results.") {}
package com.bitnei.alarm

import java.sql.Timestamp
import java.text.SimpleDateFormat
import java.util.{Date, UUID}

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.ValidateUtils
import com.bitnei.util.NumberUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SaveMode, SparkSession}

/**
  *
  * @author zhangyongtian
  * @define 将能量消耗率结果插入表格
  *
  * create 2018-01-31 11:43
  *
  */
object BatteryRatioIntoOracle extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")
    val user = "ev"
    val password = "ev"
    val ip = "192.168.6.146"
    val port = "1521"
    val server = "evmsc1"
    val url = s"jdbc:oracle:thin:@$ip:$port:" + server


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()

    val sqlContext = sparkSession.sqlContext

    val sc = sparkSession.sparkContext

    import sparkSession.implicits._


    ////////////////////////////////////////////////

    //    sparkSession.read.parquet("data/dynamicmileageCheck/*.parquet").toJSON.show(100,false)
    sparkSession.read.parquet("data/dynamicmileageCheck/*.parquet")
      .createOrReplaceTempView("dyncheck")


    //TODO CHECK_INSTANT_BATTERY  电池容量
//
//    val sql =
//      """
//            select vid,startTime,endTime,closedType,socDiff,mileageDiff,powerConsumption,maxTotalCurrent,minTotalCurrent,quickChargeFactor,m from dyncheck
//          """.stripMargin
//
//    val df = sparkSession.sql(sql).as[(String, String, String, String, String, String, String, String, String, String, String)]
//
//    val initRdd = df.rdd
//
//    val filterRdd = initRdd.filter(x => {
//      val closedType = x._4
//      val m = x._11
//      val mileageDiff = x._5
//      closedType.equals("rcr") && ValidateUtils.isNumber(m) && x._5.toDouble > 0D
//    })
//
//    println(filterRdd.collect.length)
//
//    val rdd = filterRdd.map(row => {
//
//      // vid,startTime,endTime,closedType,socDiff,mileageDiff,powerConsumption,maxTotalCurrent,minTotalCurrent,quickChargeFactor,m
//
//      val ID = UUID.randomUUID().toString
//
//      val VID = row._1
//
//      val VIN = "xx"
//
//      val sdf2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
//
//      val startDate = row._2.toLong
//
//      val stopDate = row._3.toLong
//
//      val START_TIME = Timestamp.valueOf(sdf2.format(startDate))
//
//      val END_TIME = Timestamp.valueOf(sdf2.format(stopDate))
//
//
//      val DATA_VALUE = row._11.toDouble
//
//      //仪表里程
//      val KILOMETRES = row._5.toDouble
//
//
//      val STATUS = 0
//      val IS_DELETE = "N"
//      val MODIFY_SYS = 1
//      val MODIFY_TIME = Timestamp.valueOf(sdf2.format(new Date()))
//
//
//      Row(ID, VID, VIN, START_TIME, END_TIME, DATA_VALUE, STATUS, IS_DELETE, MODIFY_SYS, MODIFY_TIME, KILOMETRES)
//    })
//
//    val dfschema = StructType(
//      Array(
//        StructField("ID", StringType),
//        StructField("VID", StringType),
//        StructField("VIN", StringType),
//        StructField("START_TIME", TimestampType),
//        StructField("END_TIME", TimestampType),
//        StructField("DATA_VALUE", DoubleType),
//        StructField("STATUS", IntegerType),
//        StructField("IS_DELETE", StringType),
//        StructField("MODIFY_SYS", IntegerType),
//        StructField("MODIFY_TIME", TimestampType),
//        StructField("KILOMETRES", DoubleType)
//      ))
//
//    val result = sparkSession.createDataFrame(rdd, dfschema)
//
//    val tableName = "CHECK_INSTANT_BATTERY"
//
//    val prop = new java.util.Properties
//    prop.setProperty("user", user)
//    prop.setProperty("password", password)
//    prop.put("oracle.jdbc.mapDateToTimestamp", "false")
//
//    result.write.mode(SaveMode.Append).jdbc(url, tableName, prop);


    //TODO:CHECK_INSTANT_CHARGE  快充倍率


    val sql =
      """
            select vid,startTime,endTime,closedType,socDiff,mileageDiff,powerConsumption,maxTotalCurrent,minTotalCurrent,quickChargeFactor,m from dyncheck
          """.stripMargin

    val df = sparkSession.sql(sql).as[(String, String, String, String, String, String, String, String, String, String, String)]

    val initRdd = df.rdd

    val filterRdd = initRdd.filter(x => {
      val closedType = x._4
      val quickChargeFactor = x._10
      val mileageDiff = x._5
      closedType.equals("rcr") && ValidateUtils.isNumber(quickChargeFactor) && x._5.toDouble > 0D
    })


    val rdd = filterRdd.map(row => {

      // vid,startTime,endTime,closedType,socDiff,mileageDiff,powerConsumption,maxTotalCurrent,minTotalCurrent,quickChargeFactor,m

      val ID = UUID.randomUUID().toString

      val VID = row._1

      val VIN = "xx"

      val sdf2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")

      val startDate = row._2.toLong

      val stopDate = row._3.toLong

      val START_TIME = Timestamp.valueOf(sdf2.format(startDate))

      val END_TIME = Timestamp.valueOf(sdf2.format(stopDate))


      val DATA_VALUE = row._10.toDouble

      //仪表里程
      val KILOMETRES = row._5.toDouble


      val STATUS = 0
      val IS_DELETE = "N"
      val MODIFY_SYS = 1
      val MODIFY_TIME = Timestamp.valueOf(sdf2.format(new Date()))


      Row(ID, VID, VIN, START_TIME, END_TIME, DATA_VALUE, STATUS, IS_DELETE, MODIFY_SYS, MODIFY_TIME, KILOMETRES)
    })

    val dfschema = StructType(
      Array(
        StructField("ID", StringType),
        StructField("VID", StringType),
        StructField("VIN", StringType),
        StructField("START_TIME", TimestampType),
        StructField("END_TIME", TimestampType),
        StructField("DATA_VALUE", DoubleType),
        StructField("STATUS", IntegerType),
        StructField("IS_DELETE", StringType),
        StructField("MODIFY_SYS", IntegerType),
        StructField("MODIFY_TIME", TimestampType),
        StructField("KILOMETRES", DoubleType)
      ))

    val result = sparkSession.createDataFrame(rdd, dfschema)

    val tableName = "CHECK_INSTANT_CHARGE"

    val prop = new java.util.Properties
    prop.setProperty("user", user)
    prop.setProperty("password", password)
    prop.put("oracle.jdbc.mapDateToTimestamp", "false")

    result.write.mode(SaveMode.Append).jdbc(url, tableName, prop);





    //TODO:CHECK_INSTANT_ENERGY  能量消耗率

//    val sql =
//      """
//            select vid,startTime,endTime,closedType,socDiff,mileageDiff,powerConsumption,maxTotalCurrent,minTotalCurrent,quickChargeFactor,m from dyncheck
//          """.stripMargin
//
//    val df = sparkSession.sql(sql).as[(String, String, String, String, String, String, String, String, String, String, String)]
//
//    val initRdd = df.rdd
//
//    val filterRdd = initRdd.filter(x => {
//      val closedType = x._4
//      val powerConsumption = x._7
//      val mileageDiff = x._5
//      closedType.equals("crc") && ValidateUtils.isNumber(powerConsumption) && x._5.toDouble > 0D
//    })
//
//
//    val rdd = filterRdd.map(row => {
//
//      // vid,startTime,endTime,closedType,socDiff,mileageDiff,powerConsumption,maxTotalCurrent,minTotalCurrent,quickChargeFactor,m
//
//      val ID = UUID.randomUUID().toString
//
//      val VID = row._1
//
//      val VIN = "xx"
//
//      val sdf2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")
//
//      val startDate = row._2.toLong
//
//      val stopDate = row._3.toLong
//
//      val START_TIME = Timestamp.valueOf(sdf2.format(startDate))
//
//      val END_TIME = Timestamp.valueOf(sdf2.format(stopDate))
//
//
//      val DATA_VALUE = row._7.toDouble
//
//      //仪表里程
//      val KILOMETRES = row._5.toDouble
//
//
//      val STATUS = 0
//      val IS_DELETE = "N"
//      val MODIFY_SYS = 1
//      val MODIFY_TIME = Timestamp.valueOf(sdf2.format(new Date()))
//
//
//      Row(ID, VID, VIN, START_TIME, END_TIME, DATA_VALUE, STATUS, IS_DELETE, MODIFY_SYS, MODIFY_TIME, KILOMETRES)
//    })
//
//    val dfschema = StructType(
//      Array(
//        StructField("ID", StringType),
//        StructField("VID", StringType),
//        StructField("VIN", StringType),
//        StructField("START_TIME", TimestampType),
//        StructField("END_TIME", TimestampType),
//        StructField("DATA_VALUE", DoubleType),
//        StructField("STATUS", IntegerType),
//        StructField("IS_DELETE", StringType),
//        StructField("MODIFY_SYS", IntegerType),
//        StructField("MODIFY_TIME", TimestampType),
//        StructField("KILOMETRES", DoubleType)
//      ))
//
//    val result = sparkSession.createDataFrame(rdd, dfschema)
//
//    val tableName = "CHECK_INSTANT_ENERGY"
//
//    val prop = new java.util.Properties
//    prop.setProperty("user", user)
//    prop.setProperty("password", password)
//    prop.put("oracle.jdbc.mapDateToTimestamp", "false")
//
//    result.write.mode(SaveMode.Append).jdbc(url, tableName, prop);



    ///////////////////////////////////////////////////

    sparkSession.stop()

  }


  case class Iput(groupId: String, vin: String, startTime: String, stopTime: String, duration: Long, mileage: Double)

  case class Otput(ID: String, VID: String, VIN: String, START_TIME: Long, END_TIME: Long, KILOMETRES: Int, KILOMETRES_DRIVING: Double, TIMES: Long, STATUS: Int, IS_DELETE: Char, MODIFY_SYS: Int, MODIFY_TIME: Long, REMARK: String, MORE_TERMINAL_ID: String)


}



//package com.bitnei.alarm.generator
//
//import com.bitnei.report.Job
//import com.bitnei.report.common.configuration.StateConf
//import com.bitnei.report.common.log.Logging
//import com.bitnei.report.common.utils.Utils
//import com.bitnei.report.constants.Constant
//import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
//import org.apache.spark.sql.{Dataset, SparkSession}
//
//import scala.collection.mutable.ArrayBuffer
//
//abstract class Detail {
//  val state: String
//}
//
//
//case class DetailModel(vid:String,
//                      //开始时间
//                       startTime:Long=0,
//                      //结束时间
//                       endTime:Long=0,
//                       //开始里程
//                       startMileage:Int=0,
//                       //结束里程
//                       stopMileage:Int=Int.MaxValue,
//
//                       //开始，结束soc
//                       startSoc:Int=0,
//                       endSoc:Int=0,
//
//                       //总电流
//                       totalCharge:Double=0,
//
//                      //标称能量
//                       standendPower:Double=0,
//                      //充电行驶状态
//                       val state: String) extends  Detail
//
//
///**
//  * @param vid 车辆vid
//  * @param startTime 开始时间
//  * @param endTime 结束时间
//  * @param closedType 闭包类型
//  * @param socDiff 闭包内开始-结束 soc差值
//  * @param mileageDiff 闭包内里程差值
//  * @param powerConsumption 能量消耗率
//  * @param maxTotalCurrent 闭包内最大总电流
//  * @param minTotalCurrent 闭包内最小总电流
//  * @param quickChargeFactor 快充倍率
//  * @param m 电池衰减
//  * */
//case class ClosedResult(
//                               vid:String,
//                               startTime:Long,
//                               endTime:Long,
//                               closedType:String,
//                               socDiff:Double,
//                               mileageDiff:Double,
//                               powerConsumption:Double,
//                               maxTotalCurrent:Double,
//                               minTotalCurrent:Double,
//                               quickChargeFactor:Double,
//                               m:Double)
//
//class BatteryRelatedGenerator(  @transient sparkSession:SparkSession, stateConf:StateConf)  extends Serializable
//  with Logging
//  with Job {
//
//  import sparkSession.implicits._
//
//  //输入表名
//  private val detailTableName = stateConf.getOption("detail.table.name").getOrElse("detail")
//
//  //输出表名
//  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("mileageCheck")
//
//  override type R = ClosedResult
//
//
//  //注册输入表
//  override def registerIfNeed(): Unit = {
//    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, detailTableName)
//    //注册config表
//
//    sparkSession.read.parquet("/tmp/spark/vehicle/result/config").createOrReplaceTempView("config")
//  }
//
//
//  override def doCompute[Product <: DetailModel](): Dataset[R] = {
//    val whereCondition = SqlHelper.buildWhereConditionBasePartitionColumn(SparkHelper.getTableInfo(stateConf, detailTableName)).get
//
//    val sql =
//      s"""
//      SELECT
//         CAST (d.VID AS String),
//         d.startTime,
//         d.startMileage,
//         d.stopMileage,
//         d.startSoc,
//         d.endSoc,
//         d.totalCharge,
//         d.category AS state,
//         config.ENERGY_STORAGE_TOTAL_CAP AS standendPower
//       FROM $detailTableName d INNER JOIN config ON d.VID=config.VID
//       WHERE vid IS NOT NULL AND $whereCondition
//    """.stripMargin
//
//    //读取输入数据
//    val realinfoDs = sparkSession.sql(sql).as[DetailModel]
//
//
//    realinfoDs.groupByKey(_.vid)
//      .flatMapGroups({ case (vid: String, details: Iterator[DetailModel]) => {
//        //首先按照时间排序
//        val sortedDetails = Utils.sortByDate2[DetailModel](
//          details.toArray,
//          detail => Some(detail.startTime)
//        )
//
//        //计算crc闭包值
//        val powerConsumptionResult = computeCrc(sortedDetails)
//
//        //计算rcr闭包值
//        val rcrResult = computeRcr(sortedDetails)
//
//        //将crc闭包值和rcr闭包值合并到一起
//        powerConsumptionResult ++ rcrResult
//      }
//      })
//  }
//
//  //计算(充电，行驶，充电)闭包值
//  def computeCrc(sortedDetails:Array[DetailModel]):Array[ClosedResult]= {
//    val powerConsumptionResult = new ArrayBuffer[ClosedResult]()
//
//    //遍历每个(充电-行驶-充电)闭包
//    BatteryRelatedGenerator.foreachCRC(sortedDetails, (start, end) => {
//      //闭包内第一个行驶
//      val firstRun = sortedDetails(start + 1)
//      //闭包内最后一个行驶
//      val lastRun = sortedDetails(end - 1)
//      //闭包soc差值
//      val socDiff = lastRun.endSoc - firstRun.startSoc
//      //闭包里程差值
//      val mileageDiff = lastRun.stopMileage - firstRun.startMileage
//      //最大峰值电流
//      val (maxTotalCurrent, minTotalCurrent) = getMaxAndMinTotalCurrent(sortedDetails, start, end)
//
//      //车辆能量消耗率
//      val powerConsumption = socDiff * firstRun.standendPower / mileageDiff
//      powerConsumptionResult.append(
//        ClosedResult(
//          vid = firstRun.vid,
//          startTime = sortedDetails(start).startTime,
//          endTime = sortedDetails(end).endTime,
//          closedType = "crc",
//          socDiff = socDiff,
//          mileageDiff = mileageDiff,
//          powerConsumption = powerConsumption,
//          maxTotalCurrent = maxTotalCurrent,
//          minTotalCurrent = minTotalCurrent,
//          quickChargeFactor = 0,
//          m = 0))
//    })
//    powerConsumptionResult.toArray
//  }
//
//  //计算(行驶,充电,行驶) 闭包值
//  def computeRcr(sortedDetails:Array[DetailModel]):Array[ClosedResult]= {
//    val result = new ArrayBuffer[ClosedResult]()
//
//    //遍历每个(充电-行驶-充电)闭包
//    BatteryRelatedGenerator.foreachCRC(sortedDetails, (start, end) => {
//      //闭包内第一个行驶
//      val firstRun = sortedDetails(start + 1)
//      //闭包内最后一个行驶
//      val lastRun = sortedDetails(end - 1)
//      //闭包soc差值
//      val socDiff = lastRun.endSoc - firstRun.startSoc
//      //闭包里程差值
//      val mileageDiff = lastRun.stopMileage - firstRun.startMileage
//      //最大峰值电流
//      val (maxTotalCurrent, minTotalCurrent) = getMaxAndMinTotalCurrent(sortedDetails, start, end)
//
//      //车辆能量消耗率
//      val powerConsumption = socDiff * firstRun.standendPower / mileageDiff
//
//      //计算快充倍率
//      val quickChargeFactor = maxTotalCurrent / 1
//      //充电容量
//      val power = 1
//      //计算电池衰减=soc差值/充电容量
//      val m = socDiff / power
//      result.append(
//        ClosedResult(
//          vid = firstRun.vid,
//          startTime = sortedDetails(start).startTime,
//          endTime = sortedDetails(end).endTime,
//          closedType = "rcr",
//          socDiff = socDiff,
//          mileageDiff = mileageDiff,
//          powerConsumption = 0,
//          maxTotalCurrent = maxTotalCurrent,
//          minTotalCurrent = minTotalCurrent,
//          quickChargeFactor = quickChargeFactor,
//          m = m))
//    })
//    result.toArray
//  }
//
//
//  //获取最大/最小总电流
//  def getMaxAndMinTotalCurrent(details: Array[DetailModel], start: Int, end: Int): (Double, Double) = {
//    //最大峰值电流
//    var maxTotalCurrent: Double = 0
//
//    //最小峰值电流
//    var minTotalCurrent: Double = 10000
//
//    //计算峰值电流
//    (start + 1 until end).foreach(i => {
//      val detail = details(i)
//      if (detail.totalCharge <= minTotalCurrent) minTotalCurrent = detail.totalCharge
//      else if (detail.totalCharge >= maxTotalCurrent) maxTotalCurrent = detail.totalCharge
//    })
//    (maxTotalCurrent, minTotalCurrent)
//  }
//
//  override def write[Product <: DetailModel](result: Dataset[R]) = {
//
//    val outptuModels = stateConf.getString("report.output").split(',')
//    outptuModels.foreach(outputModel => {
//      //将计算结果写入到hdfs
//      if (outputModel == "hdfs") SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), outputTableName)
//      else if (outputModel == "oracle" || outputModel == "mysql") {
//      }
//    })
//  }
//}
//
//object MileageCheck extends  Logging {
//  def main(args: Array[String]): Unit = {
//    val stateConf = new StateConf
//    stateConf.add(args)
//    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)
//
//    if (stateConf.getOption("readJDBCConfigTable").contains("true")) {
//      logInfo("readJDBCConfigTable=ture")
//      saveJDBCConifgTableToHDFS(sparkSession, stateConf)
//
//
//      new MileageCheck(sparkSession, stateConf).compute()
//    }
//  }
//
//  def saveJDBCConifgTableToHDFS(sparkSession: SparkSession, stateConf: StateConf): Unit = {
//    val url = stateConf.getString(Constant.JdbcUrl)
//    val driver = stateConf.getString(Constant.JdbcDriver)
//    val user = stateConf.getString(Constant.JdbcUserName)
//    val password = stateConf.getString(Constant.JdbcPasswd)
//
//    val sql =
//      """
//      SELECT UUID AS VID,VEH_MODEL_ID,ENERGY_STORAGE_TOTAL_CAP
//      FROM SYS_VEHICLE INNER JOIN SYS_VEH_CONFIG_NUM config ON SYS_VEHICLE.VEH_MODEL_ID=config.VEH_MODEL_ID
//    """.stripMargin
//
//
//    logInfo(sql)
//
//    SparkHelper.registerJdbcAsTempView(sparkSession.sqlContext, url, driver, user, password, "config", sql, 5)
//
//    sparkSession.sql("SELECT * FROM config")
//      .write.save("/tmp/spark/vehicle/result/config")
//
//    logInfo("save JDBC config table to /tmp/spark/vehicle/result/config")
//
//  }
//}
//
//
//object BatteryRelatedGenerator {
//  //遍历每一个RCR闭包。
//  //每一个RCR闭包长度至少为3.
//  def foreachRCR[T <: Detail](details: Seq[T], f: (Int, Int) => Unit): Unit = {
//    foreachClosed(Constant.TravelState, Constant.ChargeState)(details, f)
//  }
//
//  //遍历每一个CRC闭包。
//  //每一个CRC闭包长度至少为3.
//  def foreachCRC[T <: Detail](details: Seq[T], f: (Int, Int) => Unit): Unit = {
//    foreachClosed(Constant.ChargeState, Constant.TravelState)(details, f)
//  }
//
//
//  /**
//    * @param closed 闭包开始和结束元素的状态。
//    * @param mid 闭包中间元素的状态
//    * @param details 输入集合。
//    * @param f 一个用于处理闭包的函数。 第一个参数是闭包开始下标，第二个参数是闭包结束下标
//    *
//    * 在details序列中查找闭包(closed)，闭包是detail的一个子序列，这个子序列满足如下条件：
//    * 1.闭包的第一个元素和最后一个的状态相同。
//    * 2.闭包的中间元素状态相同
//    * 例如(充电，行驶，行驶，行驶，充电)就是一个闭包，这个闭包我们叫做crc闭包。c是充电的意思，r是行驶的意思。
//    * 再比如，(行驶，充电，行驶)也是一个闭包,z这个闭包叫做rcr.
//    * 事实上，（行驶，行驶，行驶）也是一个闭包，只不过这个闭包中的所有detail的状态都是行驶。
//    * */
//  def foreachClosed[T <: Detail](closed: String, mid: String)(details: Seq[T], f: (Int, Int) => Unit): Unit = {
//    if (details.size < 3) {
//      //do nothing
//    } else {
//      //闭包开始索引
//      var start = -1
//      //闭包结束索引
//      var end = -1
//
//      var i = 0
//
//      while (i < details.size - 1) {
//        if (details(i).state == closed && details(i + 1).state == mid) {
//          start = i
//        } else if (details(i).state == mid && details(i + 1).state == closed) {
//          end = i
//        }
//
//        //如果有闭包开始索引和结束索引，那么[start,end]内的所有数据形成了一个闭包。
//        //然后调用f来处理这个闭包。
//        if (start != -1 && end != -1) f(start, end)
//
//        i += 1
//      }
//    }
//  }
//}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.Binarizer
// $example off$
import org.apache.spark.sql.{SparkSession}

object BinarizerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("BinarizerExample")
      .getOrCreate()

    // $example on$
    val data = Array((0, 0.1), (1, 0.8), (2, 0.2))
    val dataFrame = spark.createDataFrame(data).toDF("id", "feature")

    val binarizer: Binarizer = new Binarizer()
      .setInputCol("feature")
      .setOutputCol("binarized_feature")
      .setThreshold(0.5)

    val binarizedDataFrame = binarizer.transform(dataFrame)

    println(s"Binarizer output with Threshold = ${binarizer.getThreshold}")
    binarizedDataFrame.show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.log4j.{Level, Logger}
import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, SVMWithSGD}
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.optimization.{L1Updater, SquaredL2Updater}
import org.apache.spark.mllib.util.MLUtils

/**
 * An example app for binary classification. Run with
 * {{{
 * bin/run-example org.apache.spark.examples.mllib.BinaryClassification
 * }}}
 * A synthetic dataset is located at `data/mllib/sample_binary_classification_data.txt`.
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object BinaryClassification {

  object Algorithm extends Enumeration {
    type Algorithm = Value
    val SVM, LR = Value
  }

  object RegType extends Enumeration {
    type RegType = Value
    val L1, L2 = Value
  }

  import Algorithm._
  import RegType._

  case class Params(
      input: String = null,
      numIterations: Int = 100,
      stepSize: Double = 1.0,
      algorithm: Algorithm = LR,
      regType: RegType = L2,
      regParam: Double = 0.01) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("BinaryClassification") {
      head("BinaryClassification: an example app for binary classification.")
      opt[Int]("numIterations")
        .text("number of iterations")
        .action((x, c) => c.copy(numIterations = x))
      opt[Double]("stepSize")
        .text("initial step size (ignored by logistic regression), " +
          s"default: ${defaultParams.stepSize}")
        .action((x, c) => c.copy(stepSize = x))
      opt[String]("algorithm")
        .text(s"algorithm (${Algorithm.values.mkString(",")}), " +
        s"default: ${defaultParams.algorithm}")
        .action((x, c) => c.copy(algorithm = Algorithm.withName(x)))
      opt[String]("regType")
        .text(s"regularization type (${RegType.values.mkString(",")}), " +
        s"default: ${defaultParams.regType}")
        .action((x, c) => c.copy(regType = RegType.withName(x)))
      opt[Double]("regParam")
        .text(s"regularization parameter, default: ${defaultParams.regParam}")
      arg[String]("<input>")
        .required()
        .text("input paths to labeled examples in LIBSVM format")
        .action((x, c) => c.copy(input = x))
      note(
        """
          |For example, the following command runs this app on a synthetic dataset:
          |
          | bin/spark-submit --class org.apache.spark.examples.mllib.BinaryClassification \
          |  examples/target/scala-*/spark-examples-*.jar \
          |  --algorithm LR --regType L2 --regParam 1.0 \
          |  data/mllib/sample_binary_classification_data.txt
        """.stripMargin)
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"BinaryClassification with $params")
    val sc = new SparkContext(conf)

    Logger.getRootLogger.setLevel(Level.WARN)

    val examples = MLUtils.loadLibSVMFile(sc, params.input).cache()

    val splits = examples.randomSplit(Array(0.8, 0.2))
    val training = splits(0).cache()
    val test = splits(1).cache()

    val numTraining = training.count()
    val numTest = test.count()
    println(s"Training: $numTraining, test: $numTest.")

    examples.unpersist(blocking = false)

    val updater = params.regType match {
      case L1 => new L1Updater()
      case L2 => new SquaredL2Updater()
    }

    val model = params.algorithm match {
      case LR =>
        val algorithm = new LogisticRegressionWithLBFGS()
        algorithm.optimizer
          .setNumIterations(params.numIterations)
          .setUpdater(updater)
          .setRegParam(params.regParam)
        algorithm.run(training).clearThreshold()
      case SVM =>
        val algorithm = new SVMWithSGD()
        algorithm.optimizer
          .setNumIterations(params.numIterations)
          .setStepSize(params.stepSize)
          .setUpdater(updater)
          .setRegParam(params.regParam)
        algorithm.run(training).clearThreshold()
    }

    val prediction = model.predict(test.map(_.features))
    val predictionAndLabel = prediction.zip(test.map(_.label))

    val metrics = new BinaryClassificationMetrics(predictionAndLabel)

    println(s"Test areaUnderPR = ${metrics.areaUnderPR()}.")
    println(s"Test areaUnderROC = ${metrics.areaUnderROC()}.")

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
// $example off$

object BinaryClassificationMetricsExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("BinaryClassificationMetricsExample")
    val sc = new SparkContext(conf)
    // $example on$
    // Load training data in LIBSVM format
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_binary_classification_data.txt")

    // Split data into training (60%) and test (40%)
    val Array(training, test) = data.randomSplit(Array(0.6, 0.4), seed = 11L)
    training.cache()

    // Run training algorithm to build the model
    val model = new LogisticRegressionWithLBFGS()
      .setNumClasses(2)
      .run(training)

    // Clear the prediction threshold so the model will return probabilities
    model.clearThreshold

    // Compute raw scores on the test set
    val predictionAndLabels = test.map { case LabeledPoint(label, features) =>
      val prediction = model.predict(features)
      (prediction, label)
    }

    // Instantiate metrics object
    val metrics = new BinaryClassificationMetrics(predictionAndLabels)

    // Precision by threshold
    val precision = metrics.precisionByThreshold
    precision.foreach { case (t, p) =>
      println(s"Threshold: $t, Precision: $p")
    }

    // Recall by threshold
    val recall = metrics.recallByThreshold
    recall.foreach { case (t, r) =>
      println(s"Threshold: $t, Recall: $r")
    }

    // Precision-Recall Curve
    val PRC = metrics.pr

    // F-measure
    val f1Score = metrics.fMeasureByThreshold
    f1Score.foreach { case (t, f) =>
      println(s"Threshold: $t, F-score: $f, Beta = 1")
    }

    val beta = 0.5
    val fScore = metrics.fMeasureByThreshold(beta)
    f1Score.foreach { case (t, f) =>
      println(s"Threshold: $t, F-score: $f, Beta = 0.5")
    }

    // AUPRC
    val auPRC = metrics.areaUnderPR
    println("Area under precision-recall curve = " + auPRC)

    // Compute thresholds used in ROC and PR curves
    val thresholds = precision.map(_._1)

    // ROC Curve
    val roc = metrics.roc

    // AUROC
    val auROC = metrics.areaUnderROC
    println("Area under ROC = " + auROC)
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.mllib

// scalastyle:off println
import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.clustering.BisectingKMeans
import org.apache.spark.mllib.linalg.{Vector, Vectors}
// $example off$

/**
 * An example demonstrating a bisecting k-means clustering in spark.mllib.
 *
 * Run with
 * {{{
 * bin/run-example mllib.BisectingKMeansExample
 * }}}
 */
object BisectingKMeansExample {

  def main(args: Array[String]) {
    val sparkConf = new SparkConf().setAppName("mllib.BisectingKMeansExample")
    val sc = new SparkContext(sparkConf)

    // $example on$
    // Loads and parses data
    def parse(line: String): Vector = Vectors.dense(line.split(" ").map(_.toDouble))
    val data = sc.textFile("data/mllib/kmeans_data.txt").map(parse).cache()

    // Clustering the data into 6 clusters by BisectingKMeans.
    val bkm = new BisectingKMeans().setK(6)
    val model = bkm.run(data)

    // Show the compute cost and the cluster centers
    println(s"Compute Cost: ${model.computeCost(data)}")
    model.clusterCenters.zipWithIndex.foreach { case (center, idx) =>
      println(s"Cluster Center ${idx}: ${center}")
    }
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.operationIndex.Parser
import com.bitnei.report.operationIndex

import scala.util.parsing.combinator.RegexParsers



trait BoolFunctionCompilationError
case class BoolFunctionLexerError(msg: String) extends BoolFunctionCompilationError

/**
  * Created by wangbaosheng on 2017/9/19.
  */
object BoolFunctionLexer extends RegexParsers {
  override def skipWhitespace: Boolean = true

  override val whiteSpace = "[ \t\r\f]+".r

  def literal: Parser[Literal] = {
    "[0-9]+".r ^^ { str => Literal(str) }
  }

  def identifer:Parser[Identitier]="[a-cA-C]+".r ^^{str=>Identitier(str)}


  def logicalOp="[&|]".r ^^ (x=>
    if(x=="&") And
    else Or
  )


  def compareOp=">|>=|<=|<|=".r^^ (x=> {
    if (x == ">=") GreateThanEq
    else if (x == ">") GreateThan
    else if (x == "<=") LessThanEq
    else if (x == "<") LessThan
    else Eq
  })

  def tokens: Parser[List[BooLFunctionToken]] = {
    phrase(rep1(identifer | logicalOp |compareOp|literal)) ^^ (x => x)
  }

  def apply(code: String): Either[BoolFunctionCompilationError, List[BooLFunctionToken]] = {
    parse(tokens, code) match {
      case NoSuccess(msg, next) => Left(BoolFunctionLexerError(msg))
      case Success(result, next) => Right(result)
    }
  }
}package com.bitnei.report.operationIndex.Parser

import scala.util.parsing.combinator.Parsers
import scala.util.parsing.input.{NoPosition, Position, Reader}

sealed trait BoolFunctionAST

sealed  trait ConditionAST extends BoolFunctionAST

sealed trait CompareAST extends  ConditionAST
case class GreaterThanAST(iden:Identitier,value:Literal) extends  CompareAST
case class GreaterThanEqAst(iden:Identitier,value:Literal) extends  CompareAST
case class LessThanAst(iden:Identitier,value:Literal) extends  CompareAST
case class LessThanEqAst(iden:Identitier,value:Literal) extends  CompareAST
case class EqAst(iden:Identitier,value:Literal) extends  CompareAST


sealed trait LogicalAST extends  ConditionAST
case class AndAST(left:ConditionAST, right:ConditionAST) extends  LogicalAST
case class OrAST(left:ConditionAST, right:ConditionAST) extends  LogicalAST


case class WorkflowParserError(msg: String) extends BoolFunctionCompilationError

object BoolFunctionParser extends Parsers {
  //1.mapping token
  //2.primitive parser:accept,acceptIf,acceptMatch
  //3.combinator:mapping parser to anather parser.

  override type Elem = BooLFunctionToken

  def program: Parser[BoolFunctionAST] = {
    phrase(stmt)
  }

  //primitive  parser,
  private def literal: Parser[Literal] = {
    accept("string literal", { case lit@Literal(name) => lit })
  }

  private def identifier: Parser[Identitier] = {
    accept("identifier", { case id@Identitier(name) => id })
  }

  def logical: Parser[LogicalOperator] = {
    accept("logical operator", {
      case id@And => id
      case id@Or => id
    })
  }

  def compare: Parser[CompareOperator] = {
    accept("compare operator", {
      case id@GreateThan => id
      case id@GreateThanEq => id
      case id@LessThan => id
      case id@LessThanEq => id
      case id@Eq => id
    })
  }

  def stmt: Parser[BoolFunctionAST] = {
    condition
  }

  //condition (op condition)+

  def condition: Parser[ConditionAST] = {
    def simpleCondition: Parser[ConditionAST] = {
      (identifier ~ compare ~ literal) ^^ {
        case ider ~ GreateThan ~ lter => GreaterThanAST(ider, lter)
        case ider ~ GreateThanEq ~ lter => GreaterThanEqAst(ider, lter)
        case ider ~ LessThan ~ lter => LessThanAst(ider, lter)
        case ider ~ LessThanEq ~ lter => LessThanEqAst(ider, lter)
        case ider ~ Eq ~ lter => EqAst(ider, lter)
      }
    }

    def doC(comp:CompareOperator,left:Identitier,right:Literal):ConditionAST={
      comp match {
        case GreateThan=>GreaterThanAST(left,right)
//        case ider ~ GreateThanEq ~ lter => GreaterThanEqAst(ider, lter)
//        case ider ~ LessThan ~ lter => LessThanAst(ider, lter)
//        case ider ~ LessThanEq ~ lter => LessThanEqAst(ider, lter)
//        case ider ~ Eq ~ lter => EqAst(ider, lter)
      }
    }
    null

  //  ((simpleCondition ~ (logical~simpleCondition).*))
  }


  def apply(tokens: Seq[BooLFunctionToken]): Either[WorkflowParserError, BoolFunctionAST] = {
    val reader = new BoolFunctionTokenReader(tokens)
    program(reader) match {
      case NoSuccess(msg, next) => Left(WorkflowParserError(msg))
      case Success(result, next) => Right(result)
    }
  }
}


class BoolFunctionTokenReader(tokens:Seq[BooLFunctionToken]) extends  Reader[BooLFunctionToken] {
  override def first: BooLFunctionToken = tokens.head

  override def atEnd: Boolean = tokens.isEmpty

  override def pos: Position = NoPosition

  override def rest: Reader[BooLFunctionToken] = new BoolFunctionTokenReader(tokens.tail)
}


object BoolFunctionCompiler {
  def apply(code: String): Either[BoolFunctionCompilationError, BoolFunctionAST] = {
    for {
      tokens <- BoolFunctionLexer(code).right
      ast <- BoolFunctionParser(tokens).right
    } yield ast
  }
}

object Test{
  def main(args: Array[String]): Unit = {
    //C >= 12 | B   <=  14 & A > 11


    """
           op
     A,B,C     literal
  <statement>::= [ABC]<op>[0-9]+
  <op>::=<=|=|>=|>|=
  """
     println(BoolFunctionLexer("C > 12 & A > 16  | b<12"))
     println(BoolFunctionCompiler("C > 12 | A > 16 & | B<12"))


  }
}



import scala.util.parsing.combinator.JavaTokenParsers

object Arith extends JavaTokenParsers {

  type D = Double

  def expr:   Parser[D]    = term ~ rep(plus | minus)     ^^ {case a~b => (a /: b)((acc,f) => f(acc))}
  def plus:   Parser[D=>D] = "+" ~ term                   ^^ {case "+"~b => _ + b}
  def minus:  Parser[D=>D] = "-" ~ term                   ^^ {case "-"~b => _ - b}
  def term:   Parser[D]    = factor ~ rep(times | divide) ^^ {case a~b => (a /: b)((acc,f) => f(acc))}
  def times:  Parser[D=>D] = "*" ~ factor                 ^^ {case "*"~b => _ * b }
  def divide: Parser[D=>D] = "/" ~ factor                 ^^ {case "/"~b => _ / b}
  def factor: Parser[D]    = fpn | "(" ~> expr <~ ")"
  def fpn:    Parser[D]    = floatingPointNumber          ^^ (_.toDouble)

  def main(args: Array[String]): Unit = {
    println(parseAll(expr,"1+2*3").get)

  }
}











package com.bitnei.report.operationIndex.Parser

sealed trait BooLFunctionToken

case class Identitier(str:String) extends BooLFunctionToken
case class Literal(str:String) extends BooLFunctionToken
case object TotalMileage extends  BooLFunctionToken
case object AccDayNum  extends  BooLFunctionToken
case object MileagePerDay  extends  BooLFunctionToken

 class LogicalOperator extends  BooLFunctionToken

case object And extends  LogicalOperator
case object Or  extends  LogicalOperator

trait CompareOperator extends BooLFunctionToken

case object GreateThan   extends  CompareOperator
case object GreateThanEq   extends  CompareOperator
case object LessThan   extends  CompareOperator
case object LessThanEq   extends  CompareOperator
case object Eq   extends  CompareOperator
package com.bitnei.alarm

import breeze.linalg._
import breeze.numerics._
import org.apache.commons.math3.linear.ArrayRealVector


/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-20 11:03
  *
  */
object BreezeTest {

  def main(args: Array[String]): Unit = {


    val a = DenseVector(1, 2, 3)

    val b = DenseVector(4, 5, 6)

//    println(a ^:^ b)

        val c = new ArrayRealVector(Array[Double](1, 2, 3))
        val d = new ArrayRealVector(Array[Double](4, 5, 6))

    println(c.getNorm)
    println(c.getL1Norm)
    println(c.getLInfNorm)

    //    val vectorDot = DenseVector(1, 2, 3, 4) dot DenseVector(1, 1, 1, 1)
    //    println(vectorDot) //10
    //
    //
    //    //全0矩阵
    //    //    DenseMatrix.zeros[Double](3,2)
    //
    //    val m1 = DenseMatrix((1.0, 2.0, 3.0), (4.0, 5.0, 6.0))
    //    val m2 = DenseMatrix((1.0, 2.0, 3.0), (4.0, 5.0, 6.0))
    //
    //    println(m1 + m2)
    //    println(m1 - m2)
    //    println(m1 :* m2)
    //    println(m1 :/ m2)
    //    //    println(m1 * m2)
    //    println(m1 / m2)

  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import org.apache.spark.sql.SparkSession

/**
 * Usage: BroadcastTest [slices] [numElem] [blockSize]
 */
object BroadcastTest {
  def main(args: Array[String]) {

    val blockSize = if (args.length > 2) args(2) else "4096"

    val spark = SparkSession
      .builder()
      .appName("Broadcast Test")
      .config("spark.broadcast.blockSize", blockSize)
      .getOrCreate()

    val sc = spark.sparkContext

    val slices = if (args.length > 0) args(0).toInt else 2
    val num = if (args.length > 1) args(1).toInt else 1000000

    val arr1 = (0 until num).toArray

    for (i <- 0 until 3) {
      println("Iteration " + i)
      println("===========")
      val startTime = System.nanoTime
      val barr1 = sc.broadcast(arr1)
      val observedSizes = sc.parallelize(1 to 10, slices).map(_ => barr1.value.length)
      // Collect the small RDD so we can print the observed sizes locally.
      observedSizes.collect().foreach(i => println(i))
      println("Iteration %d took %.0f milliseconds".format(i, (System.nanoTime - startTime) / 1E6))
    }

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.BucketedRandomProjectionLSH
import org.apache.spark.ml.linalg.Vectors
// $example off$
import org.apache.spark.sql.SparkSession

object BucketedRandomProjectionLSHExample {
  def main(args: Array[String]): Unit = {
    // Creates a SparkSession
    val spark = SparkSession
      .builder
      .appName("BucketedRandomProjectionLSHExample")
      .getOrCreate()

    // $example on$
    val dfA = spark.createDataFrame(Seq(
      (0, Vectors.dense(1.0, 1.0)),
      (1, Vectors.dense(1.0, -1.0)),
      (2, Vectors.dense(-1.0, -1.0)),
      (3, Vectors.dense(-1.0, 1.0))
    )).toDF("id", "keys")

    val dfB = spark.createDataFrame(Seq(
      (4, Vectors.dense(1.0, 0.0)),
      (5, Vectors.dense(-1.0, 0.0)),
      (6, Vectors.dense(0.0, 1.0)),
      (7, Vectors.dense(0.0, -1.0))
    )).toDF("id", "keys")

    val key = Vectors.dense(1.0, 0.0)

    val brp = new BucketedRandomProjectionLSH()
      .setBucketLength(2.0)
      .setNumHashTables(3)
      .setInputCol("keys")
      .setOutputCol("values")

    val model = brp.fit(dfA)

    // Feature Transformation
    model.transform(dfA).show()
    // Cache the transformed columns
    val transformedA = model.transform(dfA).cache()
    val transformedB = model.transform(dfB).cache()

    // Approximate similarity join
    model.approxSimilarityJoin(dfA, dfB, 1.5).show()
    model.approxSimilarityJoin(transformedA, transformedB, 1.5).show()
    // Self Join
    model.approxSimilarityJoin(dfA, dfA, 2.5).filter("datasetA.id < datasetB.id").show()

    // Approximate nearest neighbor search
    model.approxNearestNeighbors(dfA, key, 2).show()
    model.approxNearestNeighbors(transformedA, key, 2).show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.Bucketizer
// $example off$
import org.apache.spark.sql.SparkSession

object BucketizerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("BucketizerExample")
      .getOrCreate()

    // $example on$
    val splits = Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity)

    val data = Array(-999.9, -0.5, -0.3, 0.0, 0.2, 999.9)
    val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF("features")

    val bucketizer = new Bucketizer()
      .setInputCol("features")
      .setOutputCol("bucketedFeatures")
      .setSplits(splits)

    // Transform original data into its bucket index.
    val bucketedData = bucketizer.transform(dataFrame)

    println(s"Bucketizer output with ${bucketizer.getSplits.length-1} buckets")
    bucketedData.show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println

package com.bitnei.report.dayreport.realinfo

import com.bitnei.report.{AutoPartition, Job}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.Model.{CategoryDayReportManager, CategoryDayReportModel}
import com.bitnei.report.dayreport.distribution.{ContinueDistributionCounter, DiscreteDistribution, MileageDistribution, SocDistribution}
import com.bitnei.report.distribute._
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql.{Dataset, SparkSession}

//分类报表作业，对应于数据库中的veh_dayreport_category表
class CatagoryJob (@transient sparkSession:SparkSession, stateConf:StateConf) extends Serializable
  with Logging
  with Job {
  @transient private val sqlContext = sparkSession.sqlContext
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConfiguration)

  import sparkSession.implicits._

  override type R = CategoryDayReportModel

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("dayreport")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("categoryDayReport")

  override def registerIfNeed(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)
  }

  override def unRegister(): Unit = {}

  override def doCompute[Product <: CategoryDayReportModel](): Dataset[CategoryDayReportModel] = {
    //构建where条件
    val whereCondition = SqlHelper.buildWhereConditionBasePartitionColumn(
      SparkHelper.getTableInfo(stateConf, inputTableName),
      (partitionColumnName, partitionColumnValue) => s"$partitionColumnName=${partitionColumnValue}"
    ).get

    sqlContext.sql(s"SELECT * FROM $inputTableName WHERE ${whereCondition}")
      .as[DayReportResult]
      .groupByKey(_.vid)
      .mapGroups({ case (vid: String, values: Iterator[DayReportResult]) =>

        var runDayReportResult: DayReportResult = null
        var chargeDayReportResult: DayReportResult = null

        //判断当前日报表状态时充电开始行驶
        values.foreach(value => {
          if (value.category == Constant.ChargeState) chargeDayReportResult = value
          else if (value.category == Constant.TravelState) runDayReportResult = value
        })

        //报表日期
        val reportDate = if (runDayReportResult != null) runDayReportResult.reportDate else if (chargeDayReportResult != null) chargeDayReportResult.reportDate else 0

        CategoryDayReportModel(
          vid = vid,
          reportDate = reportDate,
          chargeSoc = if (chargeDayReportResult != null) chargeDayReportResult.getChargeSocDistribute else SocDistribution.default,
          charge_time = if (chargeDayReportResult != null) chargeDayReportResult.chargeTimeLengthDistribution else DiscreteDistribution.default,
          chargeStartTime = if (chargeDayReportResult != null) chargeDayReportResult.chargeTimeRangeDistribution else ContinueDistributionCounter.default,
          runTimeRange = if (runDayReportResult != null) runDayReportResult.runTimeRangeDistribution else ContinueDistributionCounter.default,
          runKm = if (runDayReportResult != null) {
            val runKm = new MileageDistribution()
            runKm.add(runDayReportResult.stopMileage - runDayReportResult.startMileage)
            runKm.getDistribution
          } else MileageDistribution.default,
          kwh100Km = if (chargeDayReportResult != null) ChargePer100Km(chargeDayReportResult.chargePer100Km) else new ChargePer100Km(),
          runOnceTime = if (runDayReportResult != null) runDayReportResult.runTimeLengthDistribution else DiscreteDistribution.default,
          runOnceMileage = if (runDayReportResult != null) runDayReportResult.runMileageDistribution else MileageDistribution.default
        )
      }).repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(5))
  }

  override def write[Product <: CategoryDayReportModel](result: Dataset[CategoryDayReportModel]): Unit = {
    val outptuModels = stateConf.getString("report.output").split(',')
    if (outptuModels.length > 1) result.cache()

    outptuModels.foreach(outputModel => {
      if (outputModel == "hdfs") {
        SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), outputTableName)
      } else if (outputModel == "mysql" || outputModel == "oracle") {
        result.foreachPartition(values => {
          stateConf.set("database", outputModel)
          new CategoryDayReportManager(stateConf).output(values.toIterable)
        })
      }
    })
  }
}


object CatagoryJob {
  def main(args: Array[String]): Unit = {
    val sparkSession = SparkHelper.getSparkSession(None)
    val stateConf = new StateConf
    stateConf.add(args)

    new CatagoryJob(sparkSession, stateConf).compute()
  }
}package com.bitnei.report.dayreport.Model

import java.sql.Date

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import com.bitnei.report.distribute._

/**
  * Created by franciswang on 2016/10/20.
  */
case  class CategoryDayReportModel(
                               val id:Int,
                               val report_time:Long,
                               val vid:String,
                               val soc0:Int,val soc1:Int,val soc2:Int,val soc3:Int,val soc4:Int,val soc5:Int,val soc6:Int,val soc7:Int,val soc8:Int,val soc9:Int,

                               val chargeTime0:Int,val chargeTime1:Int,val chargeTime2:Int,val chargeTime3:Int,val chargeTime4:Int,val chargeTime5:Int,val chargeTime6:Int,
                               val chargeTime7:Int,val chargeTime8:Int,val chargeTime9:Int,val chargeTime10:Int,val chargeTime11:Int,val chargeTime12:Int,

                               val chargeRange0:Int,val chargeRange1:Int,val chargeRange2:Int,val chargeRange3:Int,val chargeRange4:Int,
                               val chargeRange5:Int,val chargeRange6:Int,val chargeRange7:Int,val chargeRange8:Int,val chargeRange9:Int,val chargeRange10:Int,val chargeRange11:Int,

                               val runRange0:Int,val runRange1:Int,val runRange2:Int,val runRange3:Int,val runRange4:Int,val runRange5:Int,val runRange6:Int,val runRange7:Int,
                               val runRange8:Int,val runRange9:Int,val runRange10:Int,val runRange11:Int,

                               val runKm0:Int,val runKm1:Int,val runKm2:Int,val runKm3:Int,val runKm4:Int,val runKm5:Int,val runKm6:Int,val runKm7:Int,val runKm8:Int,
                               val runKm9:Int,val runKm10:Int,val runKm11:Int,val runKm12:Int,val runKm13:Int,val runKm14:Int,

                               val kwh0:Int,val kwh1:Int,val kwh2:Int,val kwh3:Int,val kwh4:Int,val kwh5:Int,val kwh6:Int,

                               val lease_run_km_dist1:Int,
                               val lease_run_km_dist2:Int,
                               val lease_run_km_dist3:Int,
                               val lease_run_km_dist4:Int,
                               val lease_run_km_dist5:Int,
                               val lease_run_km_dist6:Int,
                               val lease_time_dist1:Int,
                               val lease_time_dist2:Int,
                               val lease_time_dist3:Int,
                               val lease_time_dist4:Int,
                               val lease_time_dist5:Int,
                               val lease_run_car1:Int,
                               val lease_run_car2:Int,
                               val lease_run_car3:Int,
                               val lease_run_cartimes1:Int,
                               val lease_run_cartimes2:Int,
                               val lease_run_cartimes3:Int,

                               val runOneTime0:Int,val runOneTime1:Int,val runOneTime2:Int,val runOneTime3:Int,val runOneTime4:Int,val runOneTime5:Int,val runOneTime6:Int,
                               val runOneTime7:Int,val runOneTime8:Int,val runOneTime9:Int,val runOneTime10:Int,val runOneTime11:Int,val runOneTime12:Int,

                               val runOnceKm0:Int,val runOnceKm1:Int,val runOnceKm2:Int,val runOnceKm3:Int,val runOnceKm4:Int,val runOnceKm5:Int,val runOnceKm6:Int,val runOnceKm7:Int,val runOnceKm8:Int,
                               val runOnceKm9:Int,val runOnceKm10:Int,val runOnceKm11:Int,val runOnceKm12:Int,val runOnceKm13:Int,val runOnceKm14:Int
                            ) extends Serializable {

  def this(vid: String,
           reportDate: Long,
           chargeSoc: Array[Int],
           charge_time: Array[Int],
           chargeTimeRange: Array[Int],
           runTimeRange: Array[Int], //行驶时段分布
           runKm: Array[Int], //日总行驶里程分布
           kwh100Km: ChargePer100Km, //百公里耗电量分布
           runOneTime: Array[Int], /*行驶时长分布*/
           runOnceMileage: Array[Int] /*行驶里程分布*/) = this(
    0, reportDate, vid,
    chargeSoc(0), chargeSoc(1),chargeSoc(2),chargeSoc(3),chargeSoc(4),chargeSoc(5),chargeSoc(6),chargeSoc(7),chargeSoc(8),chargeSoc(9),
    charge_time(0), charge_time(1), charge_time(2), charge_time(3), charge_time(4), charge_time(5),
    charge_time(6), charge_time(7), charge_time(8), charge_time(9), charge_time(10), charge_time(11),charge_time(12),
    chargeTimeRange(0), chargeTimeRange(1), chargeTimeRange(2), chargeTimeRange(3), chargeTimeRange(4),
    chargeTimeRange(5), chargeTimeRange(6), chargeTimeRange(7), chargeTimeRange(8), chargeTimeRange(9), chargeTimeRange(10), chargeTimeRange(11),
    runTimeRange(0), runTimeRange(1), runTimeRange(2), runTimeRange(3), runTimeRange(4),
    runTimeRange(5), runTimeRange(6), runTimeRange(7), runTimeRange(8), runTimeRange(9), runTimeRange(10), runTimeRange(11),
    runKm(0),runKm(1),runKm(2),runKm(3),runKm(4),runKm(5),runKm(6),runKm(7),runKm(8),runKm(9),runKm(10),runKm(11),runKm(12),runKm(13),runKm(14),
    kwh100Km(0),kwh100Km(1),kwh100Km(2),kwh100Km(3),kwh100Km(4),kwh100Km(5),kwh100Km(6),
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

    runOneTime(0), runOneTime(1), runOneTime(2), runOneTime(3), runOneTime(4), runOneTime(5),
    runOneTime(6), runOneTime(7), runOneTime(8), runOneTime(9), runOneTime(10), runOneTime(11),runOneTime(12),
    runOnceMileage(0),runOnceMileage(1),runOnceMileage(2),runOnceMileage(3),runOnceMileage(4),runOnceMileage(5),runOnceMileage(6),runOnceMileage(7),
    runOnceMileage(8),runOnceMileage(9),runOnceMileage(10),runOnceMileage(11),runOnceMileage(12),runOnceMileage(13),runOnceMileage(14)
  )

  def this(vid: String,
           reportDate: Long,
           chargeSoc: Array[Int],
           charge_time: Array[Int],
           chargeTimeRange: Array[Int],
           runTimeRange: Array[Int], //行驶时段分布
           runKm: Array[Int], //日总行驶里程分布
           kwh100Km: Array[Int],//百公里耗电量分布
           runOneTime: Array[Int], /*行驶时长分布*/
           runOnceMileage:Array[Int]/*行驶里程分布*/) = this(
    0, reportDate, vid,
    chargeSoc(0), chargeSoc(1),chargeSoc(2),chargeSoc(3),chargeSoc(4),chargeSoc(5),chargeSoc(6),chargeSoc(7),chargeSoc(8),chargeSoc(9),
    charge_time(0), charge_time(1), charge_time(2), charge_time(3), charge_time(4), charge_time(5),
    charge_time(6), charge_time(7), charge_time(8), charge_time(9), charge_time(10), charge_time(11),charge_time(12),
    chargeTimeRange(0), chargeTimeRange(1), chargeTimeRange(2), chargeTimeRange(3), chargeTimeRange(4),
    chargeTimeRange(5), chargeTimeRange(6), chargeTimeRange(7), chargeTimeRange(8), chargeTimeRange(9), chargeTimeRange(10), chargeTimeRange(11),
    runTimeRange(0), runTimeRange(1), runTimeRange(2), runTimeRange(3), runTimeRange(4),
    runTimeRange(5), runTimeRange(6), runTimeRange(7), runTimeRange(8), runTimeRange(9), runTimeRange(10), runTimeRange(11),
    runKm(0),runKm(1),runKm(2),runKm(3),runKm(4),runKm(5),runKm(6),runKm(7),runKm(8),runKm(9),runKm(10),runKm(11),runKm(12),runKm(13),runKm(14),
    kwh100Km(0),kwh100Km(1),kwh100Km(2),kwh100Km(3),kwh100Km(4),kwh100Km(5),kwh100Km(6),
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,

    runOneTime(0), runOneTime(1), runOneTime(2), runOneTime(3), runOneTime(4), runOneTime(5),
    runOneTime(6), runOneTime(7), runOneTime(8), runOneTime(9), runOneTime(10), runOneTime(11),runOneTime(12),
    runOnceMileage(0),runOnceMileage(1),runOnceMileage(2),runOnceMileage(3),runOnceMileage(4),runOnceMileage(5),runOnceMileage(6),runOnceMileage(7),
    runOnceMileage(8),runOnceMileage(9),runOnceMileage(10),runOnceMileage(11),runOnceMileage(12),runOnceMileage(13),runOnceMileage(14)
  )


  def chargeSoc:String=s"${soc0},${soc1},${soc2},${soc3},${soc4},${soc5},${soc6},${soc7},${soc8},${soc9}"
  def chargeTime:String=s"${chargeTime0},${chargeTime1},${chargeTime2},${chargeTime3},${chargeTime4},${chargeTime5},${chargeTime6},${chargeTime7},"+
  s"${chargeTime8},${chargeTime9},${chargeTime10},${chargeTime11},${chargeTime12}"
  def chargeTimeRange:String=s"${chargeRange0},${chargeRange1},${chargeRange2},${chargeRange3},${chargeRange4},${chargeRange5},${chargeRange6},"+
  s"${chargeRange7},${chargeRange8},${chargeRange9},${chargeRange10},${chargeRange11}"

  def runKm:String=s"${runKm0},${runKm1},${runKm2},${runKm3},${runKm4},${runKm5},"+
    s"${runKm6},${runKm7},${runKm8},${runKm9},${runKm10},${runKm11},${runKm12},${runKm13},${runKm14}"

  def runTimeRange:String=s"${runRange0},${runRange1},${runRange2},${runRange3},${runRange4},${runRange5},${runRange6},"+
  s"${runRange7},${runRange8},${runRange9},${runRange10},${runRange11}"

  def runOnceKm:String=s"${runOnceKm0},${runOnceKm1},${runOnceKm2},${runOnceKm3},${runOnceKm4},${runOnceKm5},"+
    s"${runOnceKm6},${runOnceKm7},${runOnceKm8},${runOnceKm9},${runOnceKm10},${runOnceKm11},${runOnceKm12},${runOnceKm13},${runOnceKm14}"

  def runOnceTime:String=s"${runOneTime0},${runOneTime1},${runOneTime2},${runOneTime3},${runOneTime4},${runOneTime5},${runOneTime6},${runOneTime7},"+
    s"${runOneTime8},${runOneTime9},${runOneTime10},${runOneTime11},${runOneTime12}"

  def kwh:String=s"${kwh0},${kwh1},${kwh2},${kwh3},${kwh4},${kwh5},${kwh6}"

}


object CategoryDayReportModel{
  def apply(vid:String,
            reportDate:Long,
            chargeSoc:Array[Int],
            charge_time:Array[Int],
            chargeStartTime:Array[Int],
            runTimeRange:Array[Int], //行驶时段分布
            runKm:Array[Int], //日总行驶里程分布
            kwh100Km:ChargePer100Km, //百公里耗电量分布
            runOnceTime:Array[Int], /*行驶时长分布*/
            runOnceMileage:Array[Int] /*行驶里程分布*/
    ):CategoryDayReportModel= {
    new CategoryDayReportModel(vid,reportDate,chargeSoc,charge_time,chargeStartTime,runTimeRange,runKm,kwh100Km,runOnceTime,runOnceMileage)
  }
}





class CategoryDayReportManager(stateConf:StateConf) extends Serializable with OutputManager with Logging {
  private val tableName =stateConf.getOption(Constant.CategoryDayReportTable).getOrElse("veh_dayreport_category")
  private val sql =stateConf.getString("database") match {
    case "oracle" => s"insert into ${tableName} t (" +
      " t.id, report_time, vid, charge_soc_0, charge_soc_1, charge_soc_2," +
      " charge_soc_3, charge_soc_4, charge_soc_5, charge_soc_6, charge_soc_7, charge_soc_8," +
      " charge_soc_9, charge_time_dist0, charge_time_dist1, charge_time_dist2," +
      " charge_time_dist3, charge_time_dist4, charge_time_dist5, charge_time_dist6," +
      " charge_time_dist7, charge_time_dist8, charge_time_dist9, charge_time_dist10," +
      " charge_time_dist11,charge_time_dist12," +
      " charge_starttime_dist0, charge_starttime_dist1, charge_starttime_dist2," +
      " charge_starttime_dist3, charge_starttime_dist4, charge_starttime_dist5," +
      " charge_starttime_dist6, charge_starttime_dist7, charge_starttime_dist8," +
      " charge_starttime_dist9, charge_starttime_dist10, charge_starttime_dist11," +
      " run_time_dist1,run_time_dist2,run_time_dist3,run_time_dist4,run_time_dist5," +
      " run_time_dist6,run_time_dist7,run_time_dist8,run_time_dist9,run_time_dist10," +
      " run_time_dist11,run_time_dist12," +
      " run_km_dist0, run_km_dist1, run_km_dist2, run_km_dist3, run_km_dist4," +
      " run_km_dist5, run_km_dist6, run_km_dist7, run_km_dist8, run_km_dist9," +
      " run_km_dist10, run_km_dist11, run_km_dist12, run_km_dist13, run_km_dist14," +
      " kwh_100km_0, kwh_100km_1, kwh_100km_2, kwh_100km_3, kwh_100km_4,kwh_100km_5,kwh_100km_6," +
      " lease_run_km_dist1,lease_run_km_dist2,lease_run_km_dist3," +
      " lease_run_km_dist4,lease_run_km_dist5,lease_run_km_dist6," +
      " lease_time_dist1,lease_time_dist2,lease_time_dist3," +
      " lease_time_dist4,lease_time_dist5,LEASE_RUN_CAR1,LEASE_RUN_CAR2,LEASE_RUN_CAR3,LEASE_RUN_CARTIMES1," +
      " LEASE_RUN_CARTIMES2,LEASE_RUN_CARTIMES3," +
      " RUN_ONCE_DURATION_DIST0,RUN_ONCE_DURATION_DIST1,RUN_ONCE_DURATION_DIST2,RUN_ONCE_DURATION_DIST3," +
      " RUN_ONCE_DURATION_DIST4,RUN_ONCE_DURATION_DIST5,RUN_ONCE_DURATION_DIST6,RUN_ONCE_DURATION_DIST7," +
      " RUN_ONCE_DURATION_DIST8,RUN_ONCE_DURATION_DIST9,RUN_ONCE_DURATION_DIST10,RUN_ONCE_DURATION_DIST11," +
      " RUN_ONCE_DURATION_DIST12," +
      " RUN_ONCE_MILEAGE_DIST0,RUN_ONCE_MILEAGE_DIST1,RUN_ONCE_MILEAGE_DIST2,RUN_ONCE_MILEAGE_DIST3," +
      " RUN_ONCE_MILEAGE_DIST4,RUN_ONCE_MILEAGE_DIST5,RUN_ONCE_MILEAGE_DIST6,RUN_ONCE_MILEAGE_DIST7," +
      " RUN_ONCE_MILEAGE_DIST8,RUN_ONCE_MILEAGE_DIST9,RUN_ONCE_MILEAGE_DIST10,RUN_ONCE_MILEAGE_DIST11," +
      " RUN_ONCE_MILEAGE_DIST12,RUN_ONCE_MILEAGE_DIST13,RUN_ONCE_MILEAGE_DIST14" +
      ")" +
      " values (seq_veh_report.nextval,?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
    case "mysql" =>s"insert into ${tableName}(" +
      "report_time, vid, charge_soc_0, charge_soc_1, charge_soc_2," +
      " charge_soc_3,charge_soc_4,charge_soc_5, charge_soc_6, charge_soc_7, charge_soc_8," +
      " charge_soc_9, charge_time_dist0, charge_time_dist1, charge_time_dist2," +
      " charge_time_dist3, charge_time_dist4, charge_time_dist5, charge_time_dist6," +
      " charge_time_dist7, charge_time_dist8, charge_time_dist9, charge_time_dist10," +
      " charge_time_dist11,charge_time_dist12," +
      " charge_starttime_dist0, charge_starttime_dist1, charge_starttime_dist2," +
      " charge_starttime_dist3, charge_starttime_dist4, charge_starttime_dist5," +
      " charge_starttime_dist6, charge_starttime_dist7, charge_starttime_dist8," +
      " charge_starttime_dist9, charge_starttime_dist10, charge_starttime_dist11," +
      " run_time_dist1,run_time_dist2,run_time_dist3,run_time_dist4,run_time_dist5," +
      " run_time_dist6,run_time_dist7,run_time_dist8,run_time_dist9,run_time_dist10," +
      " run_time_dist11,run_time_dist12," +
      " run_km_dist0, run_km_dist1, run_km_dist2, run_km_dist3, run_km_dist4," +
      " run_km_dist5, run_km_dist6, run_km_dist7, run_km_dist8, run_km_dist9," +
      " run_km_dist10, run_km_dist11, run_km_dist12, run_km_dist13, run_km_dist14," +
      " kwh_100km_0, kwh_100km_1, kwh_100km_2, kwh_100km_3, kwh_100km_4,kwh_100km_5,kwh_100km_6," +
      " lease_run_km_dist1,lease_run_km_dist2,lease_run_km_dist3," +
      " lease_run_km_dist4,lease_run_km_dist5,lease_run_km_dist6," +
      " lease_time_dist1,lease_time_dist2,lease_time_dist3," +
      " lease_time_dist4,lease_time_dist5,LEASE_RUN_CAR1,LEASE_RUN_CAR2,LEASE_RUN_CAR3,LEASE_RUN_CARTIMES1," +
      " LEASE_RUN_CARTIMES2,LEASE_RUN_CARTIMES3," +
      " RUN_ONCE_DURATION_DIST0,RUN_ONCE_DURATION_DIST1,RUN_ONCE_DURATION_DIST2,RUN_ONCE_DURATION_DIST3," +
      " RUN_ONCE_DURATION_DIST4,RUN_ONCE_DURATION_DIST5,RUN_ONCE_DURATION_DIST6,RUN_ONCE_DURATION_DIST7," +
      " RUN_ONCE_DURATION_DIST8,RUN_ONCE_DURATION_DIST9,RUN_ONCE_DURATION_DIST10,RUN_ONCE_DURATION_DIST11," +
      " RUN_ONCE_DURATION_DIST12," +
      " RUN_ONCE_MILEAGE_DIST0,RUN_ONCE_MILEAGE_DIST1,RUN_ONCE_MILEAGE_DIST2,RUN_ONCE_MILEAGE_DIST3," +
      " RUN_ONCE_MILEAGE_DIST4,RUN_ONCE_MILEAGE_DIST5,RUN_ONCE_MILEAGE_DIST6,RUN_ONCE_MILEAGE_DIST7," +
      " RUN_ONCE_MILEAGE_DIST8,RUN_ONCE_MILEAGE_DIST9,RUN_ONCE_MILEAGE_DIST10,RUN_ONCE_MILEAGE_DIST11," +
      " RUN_ONCE_MILEAGE_DIST12,RUN_ONCE_MILEAGE_DIST13,RUN_ONCE_MILEAGE_DIST14" +
      ")" +
      " values (?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?,?,?,?," +
      "?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
  }

  override type T = CategoryDayReportModel

  override def output(vs: Iterable[CategoryDayReportModel]):Unit = {
    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setDate(1, new Date(v.report_time))
          stmt.setString(2, v.vid)
          stmt.setInt(3, v.soc0)
          stmt.setInt(4, v.soc1)
          stmt.setInt(5, v.soc2)
          stmt.setInt(6, v.soc3)
          stmt.setInt(7, v.soc4)
          stmt.setInt(8, v.soc5)
          stmt.setInt(9, v.soc6)
          stmt.setInt(10, v.soc7)
          stmt.setInt(11, v.soc8)
          stmt.setInt(12, v.soc9)


          stmt.setInt(13, v.chargeTime0)
          stmt.setInt(14, v.chargeTime1)
          stmt.setInt(15, v.chargeTime2)
          stmt.setInt(16, v.chargeTime3)
          stmt.setInt(17, v.chargeTime4)
          stmt.setInt(18, v.chargeTime5)
          stmt.setInt(19, v.chargeTime6)
          stmt.setInt(20, v.chargeTime7)
          stmt.setInt(21, v.chargeTime8)
          stmt.setInt(22, v.chargeTime9)
          stmt.setInt(23, v.chargeTime10)
          stmt.setInt(24, v.chargeTime11)
          stmt.setInt(25, v.chargeTime12)

          stmt.setInt(26, v.chargeRange0)
          stmt.setInt(27, v.chargeRange1)
          stmt.setInt(28, v.chargeRange2)
          stmt.setInt(29, v.chargeRange3)
          stmt.setInt(30, v.chargeRange4)
          stmt.setInt(31, v.chargeRange5)
          stmt.setInt(32, v.chargeRange6)
          stmt.setInt(33, v.chargeRange7)
          stmt.setInt(34, v.chargeRange8)
          stmt.setInt(35, v.chargeRange9)
          stmt.setInt(36, v.chargeRange10)
          stmt.setInt(37, v.chargeRange11)

          stmt.setInt(38, v.runRange0)
          stmt.setInt(39, v.runRange1)
          stmt.setInt(40, v.runRange2)
          stmt.setInt(41, v.runRange3)
          stmt.setInt(42, v.runRange4)
          stmt.setInt(43, v.runRange5)
          stmt.setInt(44, v.runRange6)
          stmt.setInt(45, v.runRange7)
          stmt.setInt(46, v.runRange8)
          stmt.setInt(47, v.runRange9)
          stmt.setInt(48, v.runRange10)
          stmt.setInt(49, v.runRange11)

          stmt.setInt(50, v.runKm0)
          stmt.setInt(51, v.runKm1)
          stmt.setInt(52, v.runKm2)
          stmt.setInt(53, v.runKm3)
          stmt.setInt(54, v.runKm4)
          stmt.setInt(55, v.runKm5)
          stmt.setInt(56, v.runKm6)
          stmt.setInt(57, v.runKm7)
          stmt.setInt(58, v.runKm8)
          stmt.setInt(59, v.runKm9)
          stmt.setInt(60, v.runKm10)
          stmt.setInt(61, v.runKm11)
          stmt.setInt(62, v.runKm12)
          stmt.setInt(63, v.runKm13)
          stmt.setInt(64, v.runKm14)

          stmt.setInt(65, v.kwh0)
          stmt.setInt(66, v.kwh1)
          stmt.setInt(67, v.kwh2)
          stmt.setInt(68, v.kwh3)
          stmt.setInt(69, v.kwh4)
          stmt.setInt(70, v.kwh5)
          stmt.setInt(71, v.kwh6)

          stmt.setInt(72, v.lease_run_km_dist1)
          stmt.setInt(73, v.lease_run_km_dist2)
          stmt.setInt(74, v.lease_run_km_dist3)
          stmt.setInt(75, v.lease_run_km_dist4)
          stmt.setInt(76, v.lease_run_km_dist5)
          stmt.setInt(77, v.lease_run_km_dist6)

          stmt.setInt(78, v.lease_time_dist1)
          stmt.setInt(79, v.lease_time_dist2)
          stmt.setInt(80, v.lease_time_dist3)
          stmt.setInt(81, v.lease_time_dist4)
          stmt.setInt(82, v.lease_time_dist5)

          stmt.setInt(83, v.lease_run_car1)
          stmt.setInt(84, v.lease_run_car2)
          stmt.setInt(85, v.lease_run_car3)
          stmt.setInt(86, v.lease_run_cartimes1)
          stmt.setInt(87, v.lease_run_cartimes2)
          stmt.setInt(88, v.lease_run_cartimes3)

          stmt.setInt(89, v.runOneTime0)
          stmt.setInt(90, v.runOneTime1)
          stmt.setInt(91, v.runOneTime2)
          stmt.setInt(92, v.runOneTime3)
          stmt.setInt(93, v.runOneTime4)
          stmt.setInt(94, v.runOneTime5)
          stmt.setInt(95, v.runOneTime6)
          stmt.setInt(96, v.runOneTime7)
          stmt.setInt(97, v.runOneTime8)
          stmt.setInt(98, v.runOneTime9)
          stmt.setInt(99, v.runOneTime10)
          stmt.setInt(100, v.runOneTime11)
          stmt.setInt(101, v.runOneTime12)

          stmt.setInt(102, v.runOnceKm0)
          stmt.setInt(103, v.runOnceKm1)
          stmt.setInt(104, v.runOnceKm2)
          stmt.setInt(105, v.runOnceKm3)
          stmt.setInt(106, v.runOnceKm4)
          stmt.setInt(107, v.runOnceKm5)
          stmt.setInt(108, v.runOnceKm6)
          stmt.setInt(109, v.runOnceKm7)
          stmt.setInt(110, v.runOnceKm8)
          stmt.setInt(111, v.runOnceKm9)
          stmt.setInt(112, v.runOnceKm10)
          stmt.setInt(113, v.runOnceKm11)
          stmt.setInt(114, v.runOnceKm12)
          stmt.setInt(115, v.runOnceKm13)
          stmt.setInt(116, v.runOnceKm14)

          stmt.addBatch()
        })
      })
    } catch {
      case e: Exception =>
        logError(s"数据在写入到${tableName}中出现异常，${e.toString}")
        vs.foreach(v=>logInfo(v.toString))
        throw new Exception(s"throw en exception when writting $tableName",e)
    }

  }



//  def insert(vs:Iterator[String]):Array[Int]={
//    var i:Int=0
//   try{
//     JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
//       vs.foreach(v => {
//         v.split(',').zipWithIndex.foreach(field => {
//           if(field._2>0) {
//             println(field._2 + ":" + field._1)
//             i = field._2
//             if(field._2==1) stmt.setDate(1,java.sql.Date.valueOf(field._1))else
//             stmt.setObject(field._2, field._1)
//             stmt.addBatch()
//           }
//         })
//       })
//     })
//   } catch {
//     case e:Exception=>
//       logError(s"数据在写入到${tableName}中出现异常,${e.toString}")
//       println(i)
//       e.printStackTrace()
//       Array()
//   }
//  }

  def delete(reportDate:String):Int={
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      "DELETE FROM %s WHERE report_time=to_date(?,'yyyy-mm-dd')".format(stateConf.getString(Constant.CategoryDayReportTable)),stmt=>{
        stmt.setString(1,reportDate)
        stmt.addBatch()
      })(0)
  }
}package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.dayreport.Model.{CategoryDayReportManager, CategoryDayReportModel}
import com.bitnei.report.distribute._
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/10/22.
  */
class CategoryDayReportTest extends FunSuite{
   test("insert to category day report table"){

//     val values=Array(
//        CategoryDayReportModel("728a83e0-d13f-4267-811b-2b4d7e6ba747",1,SocDistributed(0),ChargeTimeLengthDistributed(0),
//         TimeRangeDistributed(0,0),TimeRangeDistributed(0,0),MileageDistributed(0),
//         ChargePer100Km(0),RunTimeLengthDistributed(0),MileageDistributed(0))
//     )
//     new CategoryDayReportManager(new StateConf).insert(values)
   //  new CategoryDayReportManager(new StateConf).delete(values(0).vid)
   }
}
package com.bitnei.report.dayreport.realinfo

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.Model._
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql.{Dataset, SparkSession}

import scala.collection.mutable.ArrayBuffer

/*
* created by wangbaosheng on 2017/11/2
* 分类报表输出作业
*/
class CategoryOutputJob(stateConf: StateConf, sparkSession: SparkSession) extends Serializable with Logging with Job {
  override type R = CategoryDayReportModel
  private val sqlContext = sparkSession.sqlContext

  import sqlContext.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("dayreport")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("detail")

  override def registerIfNeed() = SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)

  override def unRegister() = sparkSession.catalog.dropTempView(inputTableName)


  override def doCompute[Product <: CategoryDayReportModel]() = sqlContext.sql(s"SELECT * FROM $inputTableName ${SqlHelper.buildWhere(stateConf)}").as[CategoryDayReportModel]


  override def write[Product <: CategoryDayReportModel](result: Dataset[CategoryDayReportModel]) = {
    val outputPaths = stateConf.getString("report.output").split(',')
    outputPaths.foreach({
      case e if e == "oracle" || e == "mysql" =>
        stateConf.set("database",e)
        result.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(10)).foreachPartition(values=>{
          new CategoryDayReportManager(stateConf).output(values.toIterable)
        })
      case e =>
    })
  }
}

object CategoryOutputJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    new CategoryOutputJob(stateConf, SparkHelper.getSparkSession(sparkMaster = None)).compute()
  }
}package com.bitnei.report.cellVoltageConsist

import java.util.Base64
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{ SystemClock, Utils}
import com.bitnei.report.{AutoPartition, Job, JobRunner, OutputManager}
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.{Dataset, SaveMode, SparkSession}

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @param sparkSession
  * @param stateConf
  */
class CellVoltageConsistenceJob(@transient sparkSession: SparkSession, stateConf: StateConf) extends Serializable
  with Logging
  with Job {

  private val sqlContext = sparkSession.sqlContext
  private val clock = new SystemClock
  @transient private val hadoopConfig = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfig)

  override type R = ConsistResultModel

  import sqlContext.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("detail")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("dayreport")


  /**
    * 获取明细数据
    */
  override def registerIfNeed() = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)
  }

  override def doCompute[Product <: ConsistResultModel](): Dataset[ConsistResultModel] = {
    //${SqlHelper.buildWhere(stateConf)} 里面就用到了report.date
    val realinfoDataSet = sqlContext.sql(
      s"""
      SELECT
        VID,
        TIME,
        `2615` AS soc,
        `2003` AS cellVoltage,
        `2614` AS elecFlow
      FROM $inputTableName
      WHERE ${SqlHelper.buildWhere(stateConf)} AND VID is not null
    """.stripMargin)
      .as[Realinfo]
    //realinfoDataSet.show(false)
    //统计单车数据
    realinfoDataSet.groupByKey(_.vid).flatMapGroups({
      case (vid: String, groupedRealinfos: Iterator[Realinfo]) =>
        //按照时间排序并去除无效的数据
        val sortedValues = sortAndRemoveEmpty(groupedRealinfos.toIterable)

        findConsistCellVoltage(sortedValues) match {
          //case Some((fullCharge, socIn50_60, socEquals20)) =>
            //Array(fullCharge, socIn50_60, socEquals20)
          case Some(result) =>
            Array(result)
          case None =>
            //throw new Exception(vid)
            Array.empty[ConsistResultModel]
        }

    }).repartition(10)
  }


  def sortAndRemoveEmpty(realinfos: Iterable[Realinfo]): Iterable[Realinfo] = {
    val sorted = Utils.strict[Realinfo, String](
      Utils.sortByDate(realinfos.toArray, relainfoRow => {
        if (relainfoRow.time.trim.isEmpty) {
          logInfo(relainfoRow.toString)
          None
        } else Some(relainfoRow.time)
      }),
      relainfoRow => Some(relainfoRow.time)
    )
    val sortedAndRemoved = realinfos.filter(v => {
      v.soc.nonEmpty &&
        v.cellVoltage.nonEmpty &&
        v.elecFlow.nonEmpty
    })
    sortedAndRemoved
  }


  type Discharge = ArrayBuffer[Realinfo]

  /****
    *
    * 1.寻找这么一个集合，也叫做放电集合。findAllDischarge
    *   放电判断逻辑：soc递减，电流>0
    * 2.寻找{满电状态，放电到850-60%，放电到20}
    *    满电：soc=100
    *    放电到80%：soc=80
    *    放电到20%:soc=20
    * 3.在步骤2中的放电集合，也就是满电，放电到50-60%，放电到20% 中随便取一个单体电压，这个电压就叫做一致性电压。
    */
  def findConsistCellVoltage(rows: Iterable[Realinfo]): Option[ConsistResultModel] = {

    //如果soc有递减=>判断有没有充电，如果有充电，跳过，如果没有充电，找第一条数据。
    //如果soc没有递减，不统计

    /**
      *
      * i=1=>dischargeSet=empty,currentDischarge=empty
      * i>1
      * if(i.soc<(i-1).soc && i.charge>=0) {
      *       currentDischarge.add(i)
      * }
      * else{
      * if(currentDischarge!=emtpy) {
      *        dischargeSet.add(currentDischarge)
      * currentDischarge=changeToNextDischarge
      * }
      * }
      **/
    def findConsistResult(dischargeSet: Array[Discharge]): Option[ConsistResultModel] = {

      /**
        * dischargeSet,
        *
        * findConsist()
        *
        *
        *
        * if(i.soc==100) fullCharge=true,fullChargeValue=i
        *
        * if(i.soc>=50&&i.soc<=60) socIn50_60=true,socIn50_60_value=i
        *
        * if(i.soc==20){
        * if(fullCharge && socIn50_60){
        * (fullChargeValue,socIn50_60_value,i)
        * }
        * }
        *
        **/
      def findConsistDischarge(discharges: Array[Discharge]): (Realinfo, Realinfo, Realinfo) = {
        var (fullChargeResult, socIn50_60Result, socEqual20Result): (Realinfo, Realinfo, Realinfo) = (null, null, null)

        var consist=false

        discharges.foreach(disCharge => {
          if(!consist){
            val (currentConsist, fullChargeValue, socIn50_60_value, socEqual20) = findInDisCharge(disCharge)

            consist=currentConsist

            if (currentConsist) {
              fullChargeResult = fullChargeValue
              socIn50_60Result = socIn50_60_value
              socEqual20Result = socEqual20
            }
          }
        })

        (fullChargeResult, socIn50_60Result, socEqual20Result)
      }

      val (fullCharge, socIn50_60, socEquals20) = findConsistDischarge(dischargeSet)

      def decodeBase64AsString(base64String: String): String = {
        new String(Base64.getDecoder.decode(base64String), "UTF-8")
      }

      if (socEquals20 != null){

        Some((
          ConsistResultModel(
            vid = fullCharge.vid,
            month = fullCharge.time.substring(0,6),
            mVoltage_100 = fullCharge.cellVoltage.get,
            mVoltage_50 = socIn50_60.cellVoltage.get,
            mVoltage_20 = socEquals20.cellVoltage.get
          )
        ))
      }else None

      /*if (socEquals20 != null) {

        Some((
          ConsistResultModel(vid = fullCharge.vid,
            //partitionTime = Utils.parsetDate(fullCharge.time).get.getTime,
            partitionTime = fullCharge.time,
            socLeftPower = fullCharge.soc.get,
            //mVoltage = decodeBase64AsString(fullCharge.cellVoltage.get)),
          mVoltage = fullCharge.cellVoltage.get),
          ConsistResultModel(vid = socIn50_60.vid,
            //partitionTime = Utils.parsetDate(socIn50_60.time).get.getTime,
            partitionTime = fullCharge.time,
            socLeftPower = socIn50_60.soc.get,
            //mVoltage = decodeBase64AsString(socIn50_60.cellVoltage.get)),
            mVoltage = socIn50_60.cellVoltage.get),
          ConsistResultModel(vid = socEquals20.vid,
            //partitionTime = Utils.parsetDate(socEquals20.time).get.getTime,
            partitionTime = fullCharge.time,
            socLeftPower = socEquals20.soc.get,
            //mVoltage = decodeBase64AsString(socEquals20.cellVoltage.get))
            mVoltage = socEquals20.cellVoltage.get)
        ))
      } else None*/
    }


    val disChargeSet = findAllDischarge(rows.toSeq)

    logInfo(s"disCharge number:${disChargeSet.length}")
    disChargeSet.foreach(disCharge=>{
      disCharge.foreach(v=>logInfo(v.toString))
    })

    val result = findConsistResult(disChargeSet)

    logInfo(s"discharge result:${result.getOrElse("no result")}")
    result
  }

  /**
    * 第一步：
    * 判断递减，并且为连续放电过程的soc
    *
    * @param models
    * @return
    */
  def findAllDischarge(models: Seq[Realinfo]): Array[Discharge] = {
    var currentDischarge:Discharge = null
    val disChargeSet = new ArrayBuffer[Discharge]()
    var flag = new Array[Int](models.size)


    for (i <- (0 until models.size)) {
      if (i > 0) {
        //SOC递减，电流>=0
        if (models(i).soc.get < models(i - 1).soc.get && models(i).elecFlow.get >= 0) {
          if (currentDischarge == null){
            currentDischarge = new Discharge
            if(models(i-1).elecFlow.get>= 0) currentDischarge.append(models(i-1))
          }

          currentDischarge.append(models(i))
        } else {
          if (currentDischarge!=null&&currentDischarge.nonEmpty) {
            disChargeSet.append(currentDischarge)
            currentDischarge =null
          }
        }
      }
    }


    if (currentDischarge!=null&&currentDischarge.nonEmpty) disChargeSet.append(currentDischarge)

    /*println(disChargeSet)
    println(disChargeSet.size)*/

    disChargeSet.toArray

  }

  /**
    * 第二步：
    * 在所有放电集合中找到连续放电过程的三个指标值,即100,50-60,20
    *
    * @param discharge
    * @return
    */
  def findInDisCharge(discharge: Discharge): (Boolean, Realinfo, Realinfo, Realinfo) = {
    var (fullCharge, fullChargeValue): (Boolean, Realinfo) = (false, null)
    var (socIn50_60, socIn50_60_value): (Boolean, Realinfo) = (false, null)
    var socEqual20: Realinfo = null

    discharge.foreach(v => {

      //println(v)

      if (v.soc.contains(100)) {
        fullCharge = true
        fullChargeValue = v
      } //(fullCharge, fullChargeValue) = (true, v)
      if (fullCharge && v.soc.get >= 50 && v.soc.get <= 60) {
        socIn50_60 = true
        socIn50_60_value = v
      }

      if (socIn50_60 && v.soc.contains(20)) socEqual20 = v
      //if(socIn50_60 && stateConf.getOption("soc.charge.min").map(_.toInt) == 20) socEqual20 = v
    })

    (socEqual20 != null,
      fullChargeValue,
      socIn50_60_value,
      socEqual20
    )
  }


  override def write[Product <: ConsistResultModel](result: Dataset[ConsistResultModel]) = {
    //写入到HDFS文件中

    val outPutModelList = stateConf.getString("report.output").split(",")

    result.cache()

    //outPutModelList.foreach(println)
    outPutModelList.foreach(outputModel => {
      if (outputModel == "hdfs") {

        SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), outputTableName)
        logInfo(s"数据写入到HDFS文件$outputTableName 结束")
      } else if (outputModel == "oracle" || outputModel == "mysql") {
        //写入到数据库中
        result.foreachPartition(values => new CellVoltageConsistenceOutputManager(stateConf).output(values.toIterable))
        logInfo(s"数据写入到数据库结束")
      }
    })
  }


  override def unRegister(): Unit = sparkSession.catalog.dropTempView(inputTableName)

}

object CellVoltageConsistenceJob extends Logging {
  def main(args: Array[String]): Unit = {
     val stateConf=new StateConf

    stateConf.add(args)

    new CellVoltageConsistenceJob(SparkHelper.getSparkSession(None),stateConf).compute()
  }
}
package com.bitnei.report.cellVoltageConsist

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.sparkhelper.SparkHelper
import org.scalatest.{BeforeAndAfter, FunSuite}

import scala.collection.mutable.ArrayBuffer

class CellVoltageConsistenceJobTest extends FunSuite with BeforeAndAfter {
  val c = new CellVoltageConsistenceJob(SparkHelper.getSparkSession(sparkMaster = Some("local")), new StateConf)

  /*test("test findInDischarge") {
            val result = c.findInDisCharge(
              ArrayBuffer(
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(81), elecFlow = Some(1), cellVoltage = Some("79")),

                Realinfo(vid = "vid1", time="20171011000000", soc = Some(77), elecFlow = Some(10), cellVoltage = Some("60")),
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(88), elecFlow = Some(9), cellVoltage = Some("88")),
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(81), elecFlow = Some(2), cellVoltage = Some("81")),

                Realinfo(vid = "vid1", time="20171011000000", soc = Some(55), elecFlow = Some(1), cellVoltage = Some("33")),
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(76), elecFlow = Some(10), cellVoltage = Some("76")),

                Realinfo(vid = "vid1", time="20171011000000", soc = Some(20), elecFlow = Some(9), cellVoltage = Some("10"))
              )
            )

                assert(result._1==false)
                assert(result._2==null)
                assert(result._3==null)
                assert(result._4==null)

            val result2 = c.findInDisCharge(
              ArrayBuffer(
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(81), elecFlow = Some(2), cellVoltage = Some("81")),
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(79), elecFlow = Some(1), cellVoltage = Some("79")),

                Realinfo(vid = "vid1", time="20171011000000", soc = Some(100), elecFlow = Some(10), cellVoltage = Some("60")),
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(88), elecFlow = Some(9), cellVoltage = Some("88")),
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(81), elecFlow = Some(2), cellVoltage = Some("81")),

                Realinfo(vid = "vid1", time="20171011000000", soc = Some(55), elecFlow = Some(1), cellVoltage = Some("33")),
                Realinfo(vid = "vid1", time="20171011000000", soc = Some(50), elecFlow = Some(-10), cellVoltage = Some("76")),

                Realinfo(vid = "vid1", time="20171011000000", soc = Some(20), elecFlow = Some(9), cellVoltage = Some("10"))
              )
            )

                assert(result2._1==true)
                assert(result2._2.cellVoltage.get=="60")
                assert(result2._3.cellVoltage.get=="76")
                assert(result2._4.cellVoltage.get=="10")

        val result3 = c.findInDisCharge(
          ArrayBuffer(
            Realinfo(vid = "vid1", time="20171011000000", soc = Some(81), elecFlow = Some(2), cellVoltage = Some("81")),
            Realinfo(vid = "vid1", time="20171011000000", soc = Some(79), elecFlow = Some(3), cellVoltage = Some("79")),

            Realinfo(vid = "vid1", time="20171011000000", soc = Some(100), elecFlow = Some(10), cellVoltage = Some("60")),
            Realinfo(vid = "vid1", time="20171011000000", soc = Some(88), elecFlow = Some(9), cellVoltage = Some("88")),
            Realinfo(vid = "vid1", time="20171011000000", soc = Some(81), elecFlow = Some(10), cellVoltage = Some("81")),

            Realinfo(vid = "vid1", time="20171011000000", soc = Some(55), elecFlow = Some(1), cellVoltage = Some("33")),
            Realinfo(vid = "vid1", time="20171011000000", soc = Some(76), elecFlow = Some(-10), cellVoltage = Some("76")),

            Realinfo(vid = "vid1", time="20171011000000", soc = Some(15), elecFlow = Some(9), cellVoltage = Some("10"))
          )
        )

        assert(result3._1 == false)
        assert(result3._2.cellVoltage.get == "60")
        assert(result3._3.cellVoltage.get == "33")
        assert(result3._4 == null)

  }*/

  test("test findConsistCellVoltage") {
//    val result4 = c.findAllDischarge(
//      ArrayBuffer(
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(80), cellVoltage = Some("10"), elecFlow = Some(1)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(88), cellVoltage = Some("10"), elecFlow = Some(-1)),
//        //discharge 1
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(81), cellVoltage = Some("10"), elecFlow = Some(2)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(100), cellVoltage = Some("100"), elecFlow = Some(2)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(79), cellVoltage = Some("10"), elecFlow = Some(1)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(76), cellVoltage = Some("10"), elecFlow = Some(10)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(50), cellVoltage = Some("50"), elecFlow = Some(9)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(40), cellVoltage = Some("10"), elecFlow = Some(10)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(20), cellVoltage = Some("20"), elecFlow = Some(9)),
//        //discharge 2
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(40), cellVoltage = Some("10"), elecFlow = Some(1)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(39), cellVoltage = Some("10"), elecFlow = Some(2)),
//
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(45), cellVoltage = Some("10"), elecFlow = Some(10)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(60), cellVoltage = Some("10"), elecFlow = Some(9)),
//
//        //discharge 3
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(76), cellVoltage = Some("10"), elecFlow = Some(10)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(60), cellVoltage = Some("10"), elecFlow = Some(9)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(59), cellVoltage = Some("10"), elecFlow = Some(9)),
//
//        //discharge 4
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(76), cellVoltage = Some("10"), elecFlow = Some(10)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(60), cellVoltage = Some("10"), elecFlow = Some(9)),
//        Realinfo(vid = "vid1", time = "20171011000000", soc = Some(59), cellVoltage = Some("10"), elecFlow = Some(9)
//        )
//      ))
    //result4.foreach(println)
    //assert(result4.nonEmpty)


    c.findAllDischarge(Array(
      Realinfo("vid", "20171011000000", Some(80), Some("10"), Some(1)),
      Realinfo("vid", "20171011000000", Some(88), Some("10"), Some(-1)),
      Realinfo("vid", "20171011000000", Some(81), Some("10"), Some(2)),
      Realinfo("vid", "20171011000000", Some(100), Some("100"), Some(2)),
      Realinfo("vid", "20171011000000", Some(79), Some("10"), Some(1)),
      Realinfo("vid", "20171011000000", Some(76), Some("10"), Some(10)),
      Realinfo("vid", "20171011000000", Some(50), Some("50"), Some(9)),
      Realinfo("vid", "20171011000000", Some(40), Some("10"), Some(10)),
      Realinfo("vid", "20171011000000", Some(20), Some("20"), Some(9)),
      Realinfo("vid", "20171011000000", Some(40), Some("10"), Some(1)),
      Realinfo("vid", "20171011000000", Some(39), Some("10"), Some(2)),
      Realinfo("vid", "20171011000000", Some(45), Some("10"), Some(10)),
      Realinfo("vid", "20171011000000", Some(60), Some("10"), Some(9)),
      Realinfo("vid", "20171011000000", Some(76), Some("10"), Some(10)),
      Realinfo("vid", "20171011000000", Some(60), Some("10"), Some(9)),
      Realinfo("vid", "20171011000000", Some(59), Some("10"), Some(9)),
      Realinfo("vid", "20171011000000", Some(76), Some("10"), Some(10)),
      Realinfo("vid", "20171011000000", Some(60), Some("10"), Some(9)),
      Realinfo("vid", "20171011000000", Some(59), Some("10"), Some(9))
    )).foreach(dis => {
      println(dis)
    })
  }


  /*test("test findAllDischarge") {

    val disChargeSet = c.findAllDischarge(Array(
      Realinfo(vid = "vid1", time = "20170926125615", soc = Some(80), cellVoltage = Some("10"), elecFlow = Some(1)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(88), cellVoltage = Some("10"), elecFlow = Some(-1)),
      //dicharge 1
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(81), cellVoltage = Some("10"), elecFlow = Some(2)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(79), cellVoltage = Some("10"), elecFlow = Some(1)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(76), cellVoltage = Some("10"), elecFlow = Some(10)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(60), cellVoltage = Some("10"), elecFlow = Some(9)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(50), cellVoltage = Some("10"), elecFlow = Some(-10)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(49), cellVoltage = Some("10"), elecFlow = Some(-9)),
      //discharge 2
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(40), cellVoltage = Some("10"), elecFlow = Some(1)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(39), cellVoltage = Some("10"), elecFlow = Some(2)),

      Realinfo(vid = "vid1", time="20171011000000", soc = Some(45), cellVoltage = Some("10"), elecFlow = Some(10)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(60), cellVoltage = Some("10"), elecFlow = Some(9)),

      //discharge 3
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(76), cellVoltage = Some("10"), elecFlow = Some(10)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(60), cellVoltage = Some("10"), elecFlow = Some(9)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(59), cellVoltage = Some("10"), elecFlow = Some(9)),

      //discharge 4
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(76), cellVoltage = Some("10"), elecFlow = Some(10)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(80), cellVoltage = Some("10"), elecFlow = Some(9)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(90), cellVoltage = Some("10"), elecFlow = Some(9))
    ))


    assert(disChargeSet.length==3)
  }*/
  /*test("test findAllDischarge2"){
    val disChargeSet = c.findAllDischarge(Array(
      Realinfo(vid = "vid1", time="20171011000000", soc = None, cellVoltage = None, elecFlow = None),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(79), cellVoltage = Some("10"), elecFlow = Some(1)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(76), cellVoltage = Some("10"), elecFlow = Some(10)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(60), cellVoltage = Some("10"), elecFlow = Some(9)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(50), cellVoltage = Some("10"), elecFlow = Some(-10)),
      Realinfo(vid = "vid1", time="20171011000000", soc = Some(49), cellVoltage = Some("10"), elecFlow = Some(-9))
    ))
    assert(disChargeSet.length == 0)
  }*/
}package com.bitnei.report.cellVoltageConsist

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import org.apache.commons.codec.binary.Base64


/*
* created by wangbaosheng on 2017/11/2
*/

class CellVoltageConsistenceOutputManager(stateConf: StateConf) extends OutputManager {
  override type T = ConsistResultModel
  def decodeBase64AsString(base64String: String): String = {
    val stList: Array[String] = base64String.split("\\|")
    var st :String = ""
    var i:Int = 0
    while(i < stList.length){
      st += new String(Base64.decodeBase64(stList(i)))
      if (i != stList.length-1){
        st += ","
      }
      i += 1
    }
    st
    /*stList.map(v => new String(Base64.decodeBase64(v))).foldLeft("")((left, acc) => {
      if (left.nonEmpty) {
        s"$left,$acc"
      } else acc
    })*/
  }


  override def output(values: Iterable[ConsistResultModel]): Unit = {
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      """
        insert into VEH_CONSISREPORT_MONTH (
          VID,
          MONTH,
          SINGLE_VAL_CURRENT1,
          SINGLE_VAL_CURRENT2,
          SINGLE_VAL_CURRENT3)
        VALUES(?,?,?,?,?)
      """.stripMargin, resultData => {
      values.foreach(valueResult => {
        resultData.setString(1, valueResult.vid)
        resultData.setString(2, valueResult.month)
        resultData.setString(3,decodeBase64AsString(valueResult.mVoltage_100))
        resultData.setString(4,decodeBase64AsString(valueResult.mVoltage_50))
        resultData.setString(5,decodeBase64AsString(valueResult.mVoltage_20))
        //resultData.setInt(3, valueResult.socLeftPower)
        //resultData.setString(4, valueResult.mVoltage)
        //resultData.setString(4,new String(Base64.decodeBase64(valueResult.mVoltage)))

        resultData.addBatch()
      })
    })
  }
}package com.bitnei.report.handler

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-14 15:35
  *
  */
class ChargeChangeCheckBaseSimple {

}
package com.bitnei.report.handler

import com.bitnei.report.common.configuration.StateConf

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-14 15:35
  *
  */
class ChargeChangeCheckBaseTimeSerial[T](stateConf:StateConf) extends Serializable with ChrgeChangeStrategy {

  //上升趋势的跳跃值
  val socBaseStep = stateConf.getOption("SocBaseStep").map(_.toInt).getOrElse(2)

  def status(source: Seq[T], curIndex: Int): (Boolean, Double) = {
    val curSoc = getSoc(source(curIndex))
    val exc = getSoc(source(curIndex - 2)) * 0.3 + getSoc(source(curIndex - 1)) * 0.7
    val correct = (curSoc - exc) / 100D
    (curSoc > (exc + socBaseStep), correct)
  }

  //换电检测
  def check(source: Seq[T], curIndex: Int): (Boolean, Double) = {

    val simple = for (i <- curIndex to curIndex + 7 if i < source.length) yield getSoc(source(i))
    //  Utils.z_score(simple)

    if (curIndex < 2 || curIndex > (source.length - 10)) (false, 0)
    else {
      val (change, correct) = status(source, curIndex)
      if (change) {
        ///  println(source(curIndex))
        val d = ascent(source, curIndex + 1, curIndex + 10, getSoc(source(curIndex)))
        (change && d, correct)
      }
      else (false, correct)
    }
  }


  def descent(source: Seq[T], start: Int, end: Int, baseValue: Int): Boolean = {
    var descentCount = 0
    for (i <- start until end if i - 1 >= 0) {

      descentCount += (if (getSoc(source(i)) <= getSoc(source(i - 1))) 1 else 0)
    }
    descentCount.toDouble / (end - start) >= 0.9
  }

  def ascent(source: Seq[T], start: Int, end: Int, baseValue: Int): Boolean = {
    var descentCount = 0
    for (i <- start until end) {
      //   println(getSoc(source(i)),baseValue)
      descentCount += (if (getSoc(source(i)) >= baseValue) 1 else 0)
    }
    //  println(descentCount)
    descentCount.toDouble / (end - start) >= 0.9
  }
}package com.bitnei.report.stateGenerate

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant

import scala.annotation.tailrec
import scala.collection.mutable.ListBuffer

/**
  * 将给定的，按照时间排序数据，划分为充电，行驶，满点状态集合。
  * */
abstract class ChargeChangeGenerator(stateConf:StateConf) extends Logging with StateGenerator{
  private val chargeBeginLength = stateConf.getInt(Constant.StateWindowChargeBeginLength, 10)
  private val chargeEndLength = stateConf.getInt(Constant.StateWindowChargeEndLength, 25)
  private val fullChargeEndLength = stateConf.getInt(Constant.StateWindowFullChargeEndLength, 10)
  private val maxSoc=stateConf.getInt(Constant.MaxSoc,100)
  var startMielage=0
  var endMileage=0

  protected def  getVid:(T)=>String
  protected def getTime:(T)=>String
  protected def getSpeed:(T)=>Int
  protected def getCharge:(T)=>Int
  protected def getSoc:(T)=>Int
  protected def getMileage:(T)=>Int
  // override type Source =  Seq[T]
  //override type States = List[Window[T]]
  //在线时间

  var onlineTime: Long = 0

  private  val onlineBound=stateConf.getLong(Constant.StateWindowLength, 180)

  val chargeChangeChecker:ChrgeChangeStrategy=stateConf.getString("ChargeChange") match {
    case "Simple" =>new  ChargeChangeCheckBaseSimple()
    case _ => new ChargeChangeCheckBaseTimeSerial
  }

  /**
    * 为了减少bug和便于后续维护，比如添加新的判断条件，这里全部采用函数式实现(尾递归)，最大化的减少变量的修改，便于程序推断。
    * */
  def handle(source: Seq[T]):List[Window[T]] = {
    val windows = new ListBuffer[Window[T]]()

    @tailrec
    def splitWindow(curIndex: Int, curState: String): List[Window[T]] = {
      def beginChargeStrict(i: Int): Boolean = {
        val speed = getSpeed(source(i))
        val charge = getCharge(source(i))
        charge < 0 && speed <= 50
      }

      if (curIndex < source.length) {
        //计算当日开始里程和结束里程
        startMielage = if (startMielage == 0) getMileage(source(curIndex)) else startMielage
        if (getMileage(source(curIndex)) != 0) endMileage = getMileage(source(curIndex))

        //如果当前状态是满电，那么上一个状态一定是充电。
        val (endIndex, state) = if (curState == Constant.ChargeState && beginFullCharge(source, curIndex)) {
          def append(v: T): FullChargeWindow[T] = {
            val fullChargeWindow = FullChargeWindow[T]()
            fullChargeWindow.append(v)
            windows.append(fullChargeWindow)
            fullChargeWindow
          }

          val fullChargeWindow = append(source(curIndex))

          (inFullCharge(source)(curIndex + 1, v => fullChargeWindow.append(v)), Constant.FullChargeState)
        } else if (beginChargeStrict(curIndex) && beginCharge(source, curIndex)) {
          def append(v: T): ChargeWindow[T] = {
            val chargeWindow = ChargeWindow[T]()
            windows.append(chargeWindow)
            chargeWindow.append(v)
            chargeWindow
          }

          val chargeWindow = append(source(curIndex))

          (inCharge(source)(curIndex + 1, v => chargeWindow.append(v)), Constant.ChargeState)
        } else {

          if(validate(source(curIndex))) lastValidateData=Some(source(curIndex))

          chargeChangeChecker.check(source, curIndex) match {
            //如果检测到了换电行为
            case (true, corrent) =>
              windows.lastOption match {
                case Some(chargeChangeState: ChargeChangeWindow[T]) =>
                  chargeChangeState.append(source(curIndex))
                //  println("charge change"+source(curIndex),corrent)
                case _ =>
                  val chargeChangeState = new ChargeChangeWindow[T](corrent)

                  //追加开始有效数据
                  if(!validate(source(curIndex))&&lastValidateData.nonEmpty) chargeChangeState.append(lastValidateData.get)

                  chargeChangeState.append(source(curIndex))
                  windows.append(chargeChangeState)
                 // println("charge change"+source(curIndex),corrent)
              }
              (curIndex + 1, Constant.ChargeChangeState)
            case _ =>
              windows.lastOption match {
                case Some(chargeChange: ChargeChangeWindow[T]) =>
                  if (!validate(chargeChange.last)&&validate(source(curIndex))) chargeChange.append(source(curIndex))
                case _ =>
              }


              val curMap = source(curIndex)
              val prevMap = if (curIndex >= 1) source(curIndex - 1) else curMap
              if (!online(curMap, prevMap)) {
                windows.append(TravelWindow().append(source(curIndex)))
              } else {
                onlineTime += getOnline(source, curIndex)
                windows.lastOption match {
                  case Some(travelWindow: TravelWindow[T]) =>
                    travelWindow.append(source(curIndex))
                  case _ =>
                    windows.append(TravelWindow().append(source(curIndex)))
                }
              }
              (curIndex + 1, Constant.TravelState)
          }
        }
        setWindow(windows.last, state)
        splitWindow(endIndex, state)
      } else windows.toList
    }

    splitWindow(0, Constant.NoneState).filter(window => {
      window match {
        case travelWindow: TravelWindow[T] =>
          setWindow(travelWindow, Constant.TravelState)
          travelWindow.length >= 10
        case _ => true
      }
    })
  }

  def validate(v:T):Boolean=getSoc(v)!=0&&getMileage(v)!=0

  def online(cur: T, prev: T): Boolean = {
    if (prev == null) true
    else {
      val curTime = getTime(cur)
      val prevTime = getTime(prev)
      Utils.lessEq(curTime, prevTime,onlineBound)
    }
  }



  @tailrec
  private def inFullCharge(source:Seq[T])(cur: Int,f:(T)=>Unit): Int = {
    @tailrec
    def inFullChargeTail(i: Int, curMap: T, prevMap: T, continuedInEndFullCharge: Int): Boolean = {
      if (i < source.length) {
        if (!online(curMap, prevMap)) {
          /**
            * 如果是第一条记录离线，肯定进行状态切换了，否则满10条才进行状态切换,考虑如下序列
            * time=20160913101010,soc=100,charge=0,speed=0
            * time=20160913101020,soc=99,charge=0,spped=0  该条数据属于满电状态
            * time=20160913101520,soc=99,charge=0,spped=0  离线
            */
          if (continuedInEndFullCharge == 0) false else continuedInEndFullCharge < fullChargeEndLength
        } else if (continuedInEndFullCharge < fullChargeEndLength) {
          val soc = getSoc(curMap)
          val charge = getCharge(curMap)
          val speed = getSpeed(curMap)
          if (full(soc,charge,speed)) true
          else {
            val nextMap = if ((i + 1) < source.length) source(i + 1) else curMap
            inFullChargeTail(i + 1, nextMap, curMap, continuedInEndFullCharge + 1)
          }
        } else false

      } else {
        continuedInEndFullCharge<chargeEndLength
      }
    }

    if (cur < source.length && inFullChargeTail(cur, source(cur), source(cur - 1), 0)) {
      f(source(cur))
      onlineTime += getOnline(source, cur)
      inFullCharge(source)(cur + 1,f)
    } else {
      cur
    }
  }


  @tailrec
  private def inCharge(source:Seq[T])(cur: Int,f:(T)=>Unit): Int = {
    @tailrec
    def inChargeTail(i: Int, curMap: T, prevMap: T, continuedInEndCharge: Int): Boolean = {
      if (i < source.length) {
        if (!online(curMap, prevMap)) {
          if (continuedInEndCharge == 0) false else continuedInEndCharge < chargeEndLength
        } else if (beginFullCharge(source, i)) false
        else if (continuedInEndCharge < chargeEndLength) {
          val speed = getSpeed(curMap)
          val charge = getCharge(curMap)
          if (charge < 0 && speed <= 50) true
          else {
            val nextMap = if ((i + 1) < source.length) source(i + 1) else curMap
            inChargeTail(i + 1, nextMap, curMap, continuedInEndCharge + 1)
          }
        } else false
      } else {
        continuedInEndCharge < chargeEndLength
      }
    }

    if (cur < source.length && inChargeTail(cur, source(cur), source(cur - 1), 0)) {
      f(source(cur))
      onlineTime += getOnline(source, cur)
      inCharge(source)(cur + 1, f)
    } else {
      cur
    }
  }



  var lastValidateData:Option[T]=None

  private def beginCharge(source: Seq[T], curIndex: Int): Boolean = {
    @tailrec
    def beginChargeTail(i: Int, validLength: Int): Boolean = {
      if (validLength == 0) {
        if (i >= source.length) false
        else if (beginFullCharge(source, i)) false
        else {
          val map = source(i)
          val speed = getSpeed(map)
          val charge = getCharge(map)
          if (charge < 0 && speed <= 50) beginChargeTail(i + 1, validLength + 1)
          else false
        }
      } else {
        if (i < source.length) {
          val curMap = source(i)
          val prevMap = if ((i - 1) >= 0)source(i - 1) else curMap
          if (!online(curMap, prevMap) && validLength < chargeBeginLength) false
          else if (beginFullCharge(source, i)) false
          else if (validLength < chargeBeginLength) {
            val map = source(i)
            val speed = getSpeed(map)
            val charge = getCharge(map)
            if (charge < 0 && speed <= 50) beginChargeTail(i + 1, validLength + 1)
            else false
          } else {
            true
          }
        } else {
          false
        }
      }
    }

    require(curIndex < source.length)
    beginChargeTail(curIndex, 0)
  }

  def beginFullCharge(source: Seq[T], curIndex: Int): Boolean = {
    val map = source(curIndex)
    val soc = getSoc(map)
    val speed = getSpeed(map)
    val charge = getCharge(map)
    full(soc,charge,speed)
  }

  val full=(soc:Int,charge:Int,speed:Int)=> soc == maxSoc && charge == 0 && speed <= 50

  def getOnline(source: Seq[T], curIndex: Int): Long = {
    if (curIndex == 0 || curIndex >= source.length) 0
    else {
      (Utils.getTime(getTime(source(curIndex))),
        Utils.getTime(getTime(source(curIndex-1)))) match {
        case (Some(curTime), Some(prevTime)) =>
          val timedif = curTime - prevTime
          if (timedif <= 3 * 60 * 1000) timedif
          else 0
        case _ => 0
      }
    }
  }

  var chargeTime: Long = 0
  var fullChargeTime: Long = 0

  def setWindow(window: Window[T], state: String): Unit = {

    def generateWindowId(head: T, last: T, state: String): String = s"${getVid(head)}|${getTime(head)}|${getTime(last)}|$state"


    val (startTime: Long, startH: Int) = Utils.parsetDate(getTime(window.head)) match {
      case Some(startDate) =>
        if (startDate.getMinutes == 0) (startDate.getTime, startDate.getHours)
        else (startDate.getTime, startDate.getHours + 1)
      case None =>
        (0, -1)
    }
    val (endTime: Long, endH: Int) = Utils.parsetDate(getTime(window.last)) match {
      case Some(endDate) =>
        if (endDate.getMinutes == 0) (endDate.getTime, endDate.getHours)
        else (endDate.getTime, endDate.getHours + 1)
      case None =>
        (0, -1)
    }

    window.onLineTime = onlineTime
    window.startMileage=startMielage
    window.endMileage=endMileage

    if (state == Constant.ChargeState) {
      chargeTime += (endTime - startTime)
    } else if (state == Constant.FullChargeState) {
      fullChargeTime += (endTime - startTime)
    } else if (state == Constant.TravelState) {
      window.chargeTime = chargeTime
      window.fullChargeTime = fullChargeTime
    }else if(state==Constant.ChargeChangeState){
    }
  }


  trait ChrgeChangeStrategy extends Serializable{
    def check(source:Seq[T],curIndex:Int): (Boolean,Double)
  }

  class ChargeChangeCheckBaseSimple extends Serializable with ChrgeChangeStrategy{
    val simple:Array[Int]=stateConf.getOption("chargeChangeSimple") match {
      case Some(smp) => smp.split(',').map(_.trim.toInt)
        //样本中的数值越集中，异常数据越敏感。比如//Array(1, 2, 3, 4, 5, 6, 7, 13, 34, 89)对于不能有效处理soc缺失的情况
      case _ =>Array(1,2,3,5,8,13,21,34,55,89)//Array(0,1,3,5,11,13,17,37,64,101,128)
    }

    //换电上升斜率，该值越大，表示上升的趋势越明显
    val ascentGradient=stateConf.getOption("ascentGradient").map(_.toDouble).getOrElse(0.8D)

    //换电下降斜率，该值越大，表示下降越明显
    val descentGradient=stateConf.getOption("descentGradient").map(_.toDouble).getOrElse(0.3D)

    //上升趋势的跳跃值
    val socBaseStep=stateConf.getOption("SocBaseStep").map(_.toInt).getOrElse(10)

    def status(source:Seq[T],offset:Int): (Double,Double) = {
      @tailrec
      def doStatus(i: Int, curRand: Int, descent: Int, ascent: Int,baseValue:Int): (Int, Int) = {
        if (i < simple.length) {
          val nextRand = offset + simple(i)
          if (nextRand < source.length) {
         //   println(nextRand+":"+offset+":"++":"+ascent+":"+source(curRand))
            doStatus(i + 1,
              nextRand,
              descent + (if (getSoc(source(nextRand)) <= getSoc(source(curRand))) 1 else 0),
              //1.和换电前相比：换电后的soc整体趋势要大于换电前，
              //2.换电后的soc呈现递增趋势
              ascent + (if (getSoc(source(nextRand)) > (baseValue+ socBaseStep)) 1 else 0),
              baseValue
            )
          } else (descent, ascent)
        } else {
          (descent, ascent)
        }
      }



      var  sum=0
      var count=0
      simple.foreach(a=>{
        if((offset-a)>=0) {
          sum=sum+getSoc(source(offset-a))
          count+=1
        }
      })
      val baseValue=if(count>0)sum/count else getSoc(source(offset))

     // println(baseValue)
      val (descent, ascent) = doStatus(0, offset, 0, 0, getSoc(source(offset)))
    //  println(descent,ascent)

      (descent.toDouble / simple.length, ascent.toDouble / simple.length)

   ///   val ace=(getSoc(source(Math.min(offset+60,source.length-1)))-getSoc(source(Math.min(offset+10,source.length-1))))/50
     // val dst=(getSoc(source(Math.max(0,offset-10)))-getSoc(source(Math.max(0,offset-60))))/50

      //(dst,ace)
    }



    //换电检测
    def check(source:Seq[T],curIndex:Int): (Boolean,Double) = {
      val (descent, ascent) = status(source, curIndex)

      (
      ///  ascent<=0&&Math.abs(ascent)>descent
        ascent >= ascentGradient && descent >= descentGradient,
        ascent
      )
    }
  }


  class ChargeChangeCheckBaseTimeSerial extends Serializable with ChrgeChangeStrategy{
    //上升趋势的跳跃值
    val socBaseStep=stateConf.getOption("SocBaseStep").map(_.toInt).getOrElse(2)

    def status(source:Seq[T],curIndex:Int): (Boolean,Double) ={
      val curSoc=getSoc(source(curIndex))
      val exc=getSoc(source(curIndex-2))*0.3+getSoc(source(curIndex-1))*0.7
      val correct=(curSoc-exc)/100D
      (curSoc>(exc+socBaseStep),correct)
    }

    //换电检测
    def check(source:Seq[T],curIndex:Int): (Boolean,Double) = {

      val simple=for (i <-curIndex to curIndex+7 if i<source.length) yield getSoc(source(i))
    //  Utils.z_score(simple)

      if(curIndex<2||curIndex>(source.length-10)) (false,0)
      else {
        val (change,correct)=status(source,curIndex)
        if(change){
        ///  println(source(curIndex))
          val d=ascent(source,curIndex+1,curIndex+10,getSoc(source(curIndex)))
          (change&&d,correct)
        }
        else (false,correct)
      }
    }


    def descent(source:Seq[T],start:Int,end:Int,baseValue:Int):Boolean={
      var descentCount=0
      for(i <- start until end if i-1 >=0 )  {

       descentCount+= (if(getSoc(source(i)) <=getSoc(source(i-1))) 1 else 0)
      }
      descentCount.toDouble/(end-start) >=0.9
    }

    def ascent(source:Seq[T],start:Int,end:Int,baseValue:Int):Boolean={
      var descentCount=0
      for(i <- start until end )  {
     //   println(getSoc(source(i)),baseValue)
        descentCount+= (if(getSoc(source(i))>= baseValue) 1 else 0)
      }
    //  println(descentCount)
      descentCount.toDouble/(end-start) >=0.9
    }
  }
}
package com.bitnei.report.detail.model

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.constants.Constant
import com.bitnei.report.detail._
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/10/22.
  */
class chargeDayReportTest extends FunSuite {
  test("insert into charge day report table") {
    val stateConf = new StateConf

    def conifgMysql(): Unit = {
      stateConf.set(Constant.JdbcUserName, "root")
      stateConf.set(Constant.JdbcPasswd, "123")
      stateConf.set(Constant.JdbcDriver, "com.mysql.jdbc.Driver")
      stateConf.set(Constant.JdbcUrl, "jdbc:mysql://192.168.2.140:3306/evmsc")
      stateConf.set("database", "mysql")
    }

    def configOracle(): Unit = {
      stateConf.set(Constant.JdbcUserName, "ev")
      stateConf.set(Constant.JdbcPasswd, "ev")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.2.51:1521:evmsc1")
      stateConf.set("database", "oracle")
    }

    //conifgMysql();
    configOracle()


    val detailValues = Array(
      DetailModel("764a17fd-62c6-4f17-a4f5-940504e3d782", "vin", Constant.ChargeState,  190000, new Date().getTime, new Date().getTime, 190000, 0,0,1000, 1100, 10, 100, 100, 100, 100, 100, 100, 100, 100, 100, Some(100), 100, Some(100), 100, 100, 100, 100, 100,
        10, 100, 19000, 100, new Date().getTime.toInt, 100, 0, 0, 0,false,null),
      DetailModel("764a17fd-62c6-4f17-a4f5-940504e3d782", "vin", Constant.TravelState,  190000, new Date().getTime, new Date().getTime, 190000, 0,0,1000, 1100, 10, 100, 100, 100, 100, 100, 100, 100, 100, 100, Some(100), 100, Some(100), 100, 100, 100, 100, 100,
        10, 100, 19000, 100, new Date().getTime.toInt, 100, 0, 0, 0,false,null)
    )

    val detailOutput = new DetailModelManager(stateConf)
    stateConf.set("detail.table", "veh_day_single_charge")
    detailOutput.insertToChargeTable(detailValues.toIterable)


    detailOutput.insertToRun(detailValues.toIterable)
    detailOutput.delete(detailValues.last.vid, "2017-07-18")

    val output = new DetailRunChargeOutput(stateConf)
//    val chargeRun = ChargeRun(new Date().getTime, "ssssssssssss", 1100, 100,
//      new RunTimeOneHourLengthDistributed().distributed,
//      TimeRangeOneHourDistributed().distributed, TimeRangeOneHourDistributed().distributed., ChargeDistributed().distributed, 100)
//    output.insert(Array(chargeRun))
  }
}
package com.bitnei.report.distribute

import com.bitnei.report.common.utils.Utils


/**
  * 百公里耗电量分布
  * 百公里耗电量=总耗电量/总行驶里程*100
  *
  * */
case class ChargePer100Km(val chargeKmH:Double) extends Serializable{
  private var charge0_30Kw_h = if(0<=chargeKmH && chargeKmH<=30) new ArithmeticDistributed(0,30,5,Utils.ceil(chargeKmH),Utils.ceil(chargeKmH))
  else  new ArithmeticDistributed(0,30,5)

  private var charge30Kw_h=if(30<chargeKmH) 1 else 0

  def this()=this(-1)

  def apply(index:Int):Int={
    if(0<=index&&index<charge0_30Kw_h.len) charge0_30Kw_h(index)
    else if(index==charge0_30Kw_h.len) charge30Kw_h
    else 0
  }
  def +(that:ChargePer100Km):ChargePer100Km={
    val chargedis=new ChargePer100Km()
    chargedis.charge0_30Kw_h=this.charge0_30Kw_h+that.charge0_30Kw_h
    chargedis.charge30Kw_h=this.charge30Kw_h+that.charge30Kw_h
    chargedis
  }

  override def toString: String = s"${charge0_30Kw_h.toString},${charge30Kw_h.toString}"
}

//object ChargePer100Km{
//  def apply(chargeKmH: Double): ChargePer100Km = new ChargePer100Km(chargeKmH)
//  def apply():ChargePer100Km=new ChargePer100Km()
//}
package com.bitnei.report.detail

import com.bitnei.report.detail.distribution.{PowerDistribution, RunTimeLengthDistribution, TimeRangeDistribution}

//充电行驶计算结果
case class ChargeRun(startTime:Long=0,
                     vid:String,
                     //当天开始里程
                     startMileage:Long=0L,
                     //当天结束里程
                     endMileage:Long=0L,
                     //行驶时长分布，0-1小时，1-2小时，....，9-10小时，10小时以上
                     runTimeLenthDistribution:Array[Int]=RunTimeLengthDistribution.default,
                     //行驶时段分布
                     runTimeRangeDistribution:Array[Int]=TimeRangeDistribution.default,
                     //充电时段分布
                     chargeTimeRangeDistribution:Array[Int]=TimeRangeDistribution.default,
                     //充电量分布
                     chargePowerDistribution:Array[Double]=PowerDistribution.default,
                     //实际行驶里程
                     mileage:Long=0L)
package com.bitnei.report.detail

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.Job
import com.bitnei.report.common.utils.{SystemClock, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.report.detail.distribution.{PowerDistribution, PowerDistributionBaseMean, RunTimeLengthDistribution, TimeRangeDistribution}
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.sql.{Dataset, SparkSession}


case class ChargeRunInput( vid:String,
                           category:String,
                           startMileageOfCurrentDay:Int,
                           endMileageOfCurrentDay:Int,
                           startTime:Long,
                           endTime:Long,
                           startMileage:Int,
                           stopMileage:Int,
                           totalCharge:Double,
                           powerDistribution:Array[Double])


//充电行驶计算作业，对于应数据库中的veh_single_charge_run表
class ChargeRunJob(  @transient sparkSession:SparkSession, stateConf:StateConf)  extends Serializable
  with Logging
  with Job {
  override type R = ChargeRun

  private val sqlContext = sparkSession.sqlContext
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)

  import sparkSession.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("detail")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("chargeRunDetail")


  //注册表
  override def registerIfNeed(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)
  }

  override def doCompute[Product <: ChargeRun](): Dataset[ChargeRun] = {
    val whereCondition = SqlHelper.buildWhereConditionBasePartitionColumn(SparkHelper.getTableInfo(stateConf, inputTableName)).get
    val realinfoDs = sparkSession.sql(s"""
    SELECT
    vid,
    category,
    startMileageOfCurrentDay,
    endMileageOfCurrentDay,
    startTime,
    endTime,
    startMileage,
    stopMileage,
    totalCharge,
    powerDistribution
    FROM $inputTableName
    where ${whereCondition}""".stripMargin).as[ChargeRunInput]

    val result = realinfoDs.groupByKey(_.vid)
      .flatMapGroups({ case (vid: String, rows: Iterator[ChargeRunInput]) =>
        //充电行驶计算
        if(rows.nonEmpty) Array(getChargeRun(rows.toList))
        else Array.empty[ChargeRun]
      })

    result.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(10))
  }

  //充电行驶计算
  def getChargeRun(detailValues: List[ChargeRunInput]): ChargeRun = {
    //充电时段计算器
    val chargeTimeRangeCounter=new TimeRangeDistribution
   //行驶时段计算器
    val runTimeRangeCounter=new TimeRangeDistribution
    //行驶时长分布计算器
    val runDurationCounter=new RunTimeLengthDistribution
    //充电量分布计算器
    val totalowerDistributionCounter=new PowerDistributionBaseMean()

    detailValues.foreach(detail=>{
      detail.category match {
        case Constant.ChargeState =>
          //如果时充电状态，那么计算充电时段分布
          chargeTimeRangeCounter.add(detail.startTime,detail.endTime)
          totalowerDistributionCounter.add(detail.totalCharge,detail.startTime,detail.endTime)
        case Constant.TravelState=>
          //计算行驶时段分布
          runTimeRangeCounter.add(detail.startTime,detail.endTime)
          //计算行驶时长分布
          runDurationCounter.add((detail.endTime-detail.startTime).toInt)
        case _=>
      }
    })

    //充电行驶计算结果
    ChargeRun(vid=detailValues.head.vid,
      startTime = detailValues.head.startTime,
      startMileage = detailValues.head.startMileageOfCurrentDay,
      endMileage = detailValues.head.endMileageOfCurrentDay,
      runTimeLenthDistribution = runDurationCounter.getDistribution,
      runTimeRangeDistribution = runTimeRangeCounter.getDistribution,
      chargeTimeRangeDistribution = chargeTimeRangeCounter.getDistribution,
      chargePowerDistribution = totalowerDistributionCounter.getDistribution,
      mileage = detailValues.head.endMileageOfCurrentDay-detailValues.head.startMileageOfCurrentDay)
  }

  override def write[Product <: ChargeRun](result: Dataset[ChargeRun]) = {
    val outptuModels = stateConf.getString("report.output").split(',')

    if(outptuModels.length>1) result.cache()
    //将计算结果输出到hdfs和数据库
    outptuModels.foreach(outputModel => {
      if (outputModel == "hdfs") SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), outputTableName)
      else if (outputModel == "oracle" || outputModel == "mysql") {
        stateConf.set("database", outputModel)
        outputChargeRunDetail(result)
      }
    })
  }


  //将明细数据输出到充电行驶明细表中。
  def outputChargeRunDetail(result: Dataset[ChargeRun]): Unit = {
    result.foreachPartition(par => {
      val output = new DetailRunChargeOutput(stateConf)
      output.output(par.toIterable)
    })
  }
}

object ChargeRunJob extends Logging {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    batchProcess(stateConf, (sparkSession, stateConf, job) => job.write(job.doCompute()))
  }


  def batchProcess(stateConf: StateConf,f:(SparkSession,StateConf,ChargeRunJob)=>Unit): Unit ={
    val spark = SparkHelper.getSparkSession(sparkMaster = None)
    val com = new ChargeRunJob(spark, stateConf)
    com.registerIfNeed()
    if (stateConf.getOption("batchProcess.enable").contains("true")) {
      val startDate = Utils.parsetDate(stateConf.getString("startDate"), "yyyyMMdd").get
      val endDate = Utils.parsetDate(stateConf.getString("endDate"), "yyyyMMdd").get


      while (startDate.getTime <= endDate.getTime) {
        val year = Utils.formatDate(startDate, "yyyy")
        val month = Utils.formatDate(startDate, "MM")
        val day = Utils.formatDate(startDate, "dd")

        stateConf.set("report.date",s"$year$month$day-$year$month$day")

        SparkHelper.setPartitionValue(stateConf, "detail", Array(year, month, day))
        SparkHelper.setPartitionValue(stateConf, "chargeRunDetail", Array(year, month, day))

        val s=s"${year}${month}${day}"
        stateConf.set("report.date",s"$s-$s")

        logInfo(s"begin execute $year-$month-$day")

        f(spark,stateConf,com)
        startDate.setDate(startDate.getDate + 1)
      }
    } else {
      f(spark,stateConf,com)
    }
  }

}


package com.bitnei.report.dayreport.Model
import java.sql.Date

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.realinfo.DayReportResult
/**
  * Created by franciswang on 2016/10/19.
  */
 class ChargeStateDayReportModel (val id: Int,
                                val report_time: String,
                                val vid: String,
                                val charge_time_sum: Double,
                                val charge_times: Int,
                                val charge_consume: Double,
                                val charge_con_100km: Double,
                                val charge_time_max: Double,
                                val charge_time_avg: Double,
                                val charge_vol_max: Double,
                                val charge_vol_min: Double,
                                val charge_cur_max: Double,
                                val charge_cur_min: Double,
                                val charge_soc_max: Double,
                                val charge_soc_min: Double,
                                val charge_svol_max: Double,
                                val charge_svol_min: Double,
                                val charge_cptemp_max: Int,
                                val charge_cptemp_min: Int,
                                val charge_engtemp_max: Int,
                                val charge_engtemp_min: Int,
                                val charge_sconsume_max: Double,
                                val charge_sconsume_avg: Double) extends  Serializable {
  def this(vi:String,reportDate:String)=this(0,reportDate,vi,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

  override def toString():String=s"CHARGE,$report_time,$vid,$charge_time_sum,$charge_times,$charge_consume,$charge_con_100km,"+
    s"$charge_time_max,$charge_time_avg,$charge_vol_max,$charge_vol_min,$charge_cur_max,$charge_cur_max,"+
    s"$charge_cur_min,$charge_soc_max,$charge_soc_min,$charge_svol_max,$charge_svol_min,$charge_cptemp_max,$charge_cptemp_min,$charge_engtemp_max,$charge_engtemp_min,$charge_sconsume_max,$charge_sconsume_avg"
}


class ChargeStateDayReportManager(stateConf:StateConf) extends  Serializable with OutputManager with Logging {
  private  val tableName = stateConf.getOption(Constant.ChargeStateDayReportTable).getOrElse("veh_dayreport_chargestate")

  override type T = DayReportResult

  override def output(vs: Iterable[DayReportResult]): Unit = {

    val sql: String = stateConf.getString("database") match {
      case "oracle" => s"INSERT INTO $tableName " +
        "(id,report_time,vid, charge_time_sum ,charge_times,charge_consume,charge_con_100km,charge_time_max, charge_time_avg" +
        ",charge_vol_max,charge_vol_min,charge_cur_max,charge_cur_min,charge_soc_max,charge_soc_min,charge_svol_max" +
        ",charge_svol_min,charge_cptemp_max,charge_cptemp_min,charge_engtemp_max,charge_engtemp_min, charge_sconsume_max,charge_sconsume_avg)" +
        "  VALUES(SEQ_VEH_REPORT.Nextval,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
      case "mysql" =>
        s"INSERT INTO $tableName " +
          "(report_time,vid, charge_time_sum ,charge_times,charge_consume,charge_con_100km,charge_time_max, charge_time_avg" +
          ",charge_vol_max,charge_vol_min,charge_cur_max,charge_cur_min,charge_soc_max,charge_soc_min,charge_svol_max" +
          ",charge_svol_min,charge_cptemp_max,charge_cptemp_min,charge_engtemp_max,charge_engtemp_min, charge_sconsume_max,charge_sconsume_avg)" +
          "  VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
    }


    try {
      val efficient = JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setDate(1, new Date(v.reportDate))
          stmt.setString(2, v.vid)
          stmt.setDouble(3, DataPrecision.toHour(v.timeLeng))
          stmt.setInt(4, v.times)
          stmt.setDouble(5, DataPrecision.charge(v.totalCharge))
          stmt.setDouble(6, DataPrecision.charge(v.chargePer100Km))
          stmt.setDouble(7, DataPrecision.toHour(v.maxTime))
          stmt.setDouble(8, DataPrecision.toHour(v.avgTime))

          stmt.setDouble(9, DataPrecision.totalVoltage(v.maxTotalVoltage))
          stmt.setDouble(10, DataPrecision.totalVoltage(v.minTotalVoltage))
          stmt.setDouble(11, DataPrecision.totalCurrent(v.maxTotalEctriccurrent))
          stmt.setDouble(12, DataPrecision.totalCurrent(v.minTotalEctriccurrent))
          stmt.setDouble(13, DataPrecision.soc(v.maxSoc))
          stmt.setDouble(14, DataPrecision.soc(v.minSoc))
          stmt.setDouble(15, DataPrecision.secondaryVoltage(v.maxSecondaryVolatage))
          stmt.setDouble(16, DataPrecision.secondaryVoltage(v.minSecondaryVolatage))
          stmt.setInt(17, DataPrecision.temp(v.maxAcquisitionPointTemp))
          stmt.setInt(18, DataPrecision.temp(v.minAcquisitionPointTemp))
          stmt.setInt(19, DataPrecision.temp(v.maxEngineTemp))
          stmt.setInt(20, DataPrecision.temp(v.minEngineTemp))
          stmt.setDouble(21, DataPrecision.charge(v.maxCharge))
          stmt.setDouble(22, DataPrecision.charge(v.avgCharge))
          stmt.addBatch()
        })
      })

      logInfo(s"数据成功写入到$tableName")
      efficient
    } catch {
      case e: Exception =>
        logError(s"数据在写入到$tableName 中出现异常，${e.toString}")
        vs.foreach(v=>logInfo(v.toString))
        throw new Exception(s"throw en exception when writting $tableName",e)
    }
  }

  def delete(reportDate: String): Int = {
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"DELETE FROM ${getTable()} WHERE report_time=to_date(?,'yyyy-mm-dd')", stmt => {
        stmt.setString(1, reportDate)
        stmt.addBatch()
      })(0)
  }

  def getTable(): String = stateConf.getString(Constant.ChargeStateDayReportTable)
}
package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/10/22.
  */
class ChargeStateModelTest extends  FunSuite {
  val stateConf=new StateConf

  test("insert to charge state model table"){
//    val values=Array(
//      new ChargeStateMode(
//        id = 0,
//        vid = "728a83e0-d13f-4264-811b-2b4d7e6ba744",
//        start ="2016-10-22 11:11:11",
//        end ="2016-10-22 11:11:11",
//        charge_time_sum = 1,
//        charge_consume = 1,
//        charge_time_max =1,
//        charge_vol_max = 1,
//        charge_vol_min = 1,
//        charge_cur_max = 1,
//        charge_cur_min = 1,
//        charge_soc_max = 1,
//        charge_soc_min = 1,
//        charge_svol_max =1,
//        charge_svol_min =1,
//        charge_cptemp_max = 1,
//        charge_cptemp_min = 1,
//        charge_engtemp_max =1,
//        charge_engtemp_min =1,
//        charge_sconsume_max = 0
//      )
//    )
//
//    val iter=values.toIterator
//    val efficients=new ChargeStateManager(stateConf).insert(values)
//    //assert(efficients.length==1&&efficients(0)==1)
//     new ChargeStateManager(stateConf).delete(values(0).vid)

   // new FullChargeStateManager(new StateConf).delete("2016-11-18")
   // new RunStateModelManager(new StateConf).delete("2016-09-13")
   // new ChargeStateDayReportManager(new StateConf).delete("2016-09-13")
  }
}
package com.bitnei.report.dayreport.Model

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant


case class ChargeStateMode( id: Int,
                            start:String,
                            end:String,
                            vid: String,
                            charge_time_sum: Double,//日总充电时长
                            charge_consume: Double,//日总耗电量
                            charge_time_max: Double,//日最大耗电量
                            charge_vol_max: Double,//充电状态最高充电电压
                            charge_vol_min: Double,
                            charge_cur_max: Double,//充电状态最高充电电流
                            charge_cur_min: Double,
                            charge_soc_max: Double,
                            charge_soc_min: Double,
                            charge_svol_max: Double,
                            charge_svol_min: Double,
                            charge_cptemp_max: Int,
                            charge_cptemp_min: Int,
                            charge_engtemp_max: Int,
                            charge_engtemp_min: Int,
                            charge_sconsume_max: Double//单次充电最大耗电量
                          ) extends  Serializable{}

class ChargeStateManager(stateConf:StateConf) extends  Serializable with OutputManager with Logging {
  private val tableName=stateConf.getString(Constant.ChargeStateTable)

  override type T = ChargeStateMode

  def output(vs:Iterable[ChargeStateMode]):Unit= {
     val sql: String = s"INSERT INTO $tableName " +
      "(id,start_time,end_time,vid, charge_time_sum ,charge_consume,charge_time_max" +
      ",charge_vol_max,charge_vol_min,charge_cur_max,charge_cur_min,charge_soc_max,charge_soc_min,charge_svol_max" +
      ",charge_svol_min,charge_cptemp_max,charge_cptemp_min,charge_engtemp_max,charge_engtemp_min, charge_sconsume_max)" +
      "  UES(SEQ_VEH_REPORT.Next,to_date(:start_time,'yyyy-mm-dd hh24:mi:ss'),to_date(:end_time,'yyyy-mm-dd hh24:mi:ss'),?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"

    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setString(1, v.start)
          stmt.setString(2, v.end)
          stmt.setString(3, v.vid)
          stmt.setDouble(4, v.charge_time_sum)
          stmt.setDouble(5, v.charge_consume)
          stmt.setDouble(6, v.charge_time_max)
          stmt.setDouble(7, v.charge_vol_max)
          stmt.setDouble(8, v.charge_vol_min)
          stmt.setDouble(9, v.charge_cur_max)
          stmt.setDouble(10, v.charge_cur_min)

          stmt.setDouble(11, v.charge_soc_max)
          stmt.setDouble(12, v.charge_soc_min)
          stmt.setDouble(13, v.charge_svol_max)
          stmt.setDouble(14, v.charge_svol_min)
          stmt.setDouble(15, v.charge_cptemp_max)
          stmt.setDouble(16, v.charge_cptemp_min)
          stmt.setDouble(17, v.charge_engtemp_max)
          stmt.setDouble(18, v.charge_engtemp_min)
          stmt.setDouble(19, v.charge_sconsume_max)

          stmt.addBatch()
        })

      })
    } catch {
      case e: Exception =>
        logError(s"数据在写入到${tableName}中出现异常，${e.toString}")
        Array()
    }
  }

  def delete(reportDate:String):Int={
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"DELETE FROM ${stateConf.getString(Constant.ChargeStateTable)} WHERE  start_time>=to_date(?,'yyyy-mm-dd') and end_time<to_date(?,'yyyy-mm-dd')+1",stmt=>{
       stmt.setString(1,reportDate)
        stmt.setString(2,reportDate)
        stmt.addBatch()
      })(0)
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.ChiSqSelector
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
// $example off$

object ChiSqSelectorExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("ChiSqSelectorExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load some data in libsvm format
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
    // Discretize data in 16 equal bins since ChiSqSelector requires categorical features
    // Even though features are doubles, the ChiSqSelector treats each unique value as a category
    val discretizedData = data.map { lp =>
      LabeledPoint(lp.label, Vectors.dense(lp.features.toArray.map { x => (x / 16).floor }))
    }
    // Create ChiSqSelector that will select top 50 of 692 features
    val selector = new ChiSqSelector(50)
    // Create ChiSqSelector model (selecting features)
    val transformer = selector.fit(discretizedData)
    // Filter the top 50 features from each feature vector
    val filteredData = discretizedData.map { lp =>
      LabeledPoint(lp.label, transformer.transform(lp.features))
    }
    // $example off$

    println("filtered data: ")
    filteredData.foreach(x => println(x))

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.handler

import com.bitnei.report.constants.Constant

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-14 15:29
  *
  */
trait ChrgeChangeStrategy[T] extends Serializable {

  private val chargeBeginLength = stateConf.getInt(Constant.StateWindowChargeBeginLength, 10)
  private val chargeEndLength = stateConf.getInt(Constant.StateWindowChargeEndLength, 25)
  private val fullChargeEndLength = stateConf.getInt(Constant.StateWindowFullChargeEndLength, 10)
  private val maxSoc=stateConf.getInt(Constant.MaxSoc,100)
  var startMielage=0
  var endMileage=0

  protected def  getVid:(T)=>String
  protected def getTime:(T)=>String
  protected def getSpeed:(T)=>Int
  protected def getCharge:(T)=>Int
  protected def getSoc:(T)=>Int
  protected def getMileage:(T)=>Int
  // override type Source =  Seq[T]
  //override type States = List[Window[T]]
  //在线时间



  def check(source: Seq[T], curIndex: Int): (Boolean, Double)



}






/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

// $example on$
import org.apache.spark.graphx.GraphLoader
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * Suppose I want to build a graph from some text files, restrict the graph
 * to important relationships and users, run page-rank on the sub-graph, and
 * then finally return attributes associated with the top users.
 * This example do all of this in just a few lines with GraphX.
 *
 * Run with
 * {{{
 * bin/run-example graphx.ComprehensiveExample
 * }}}
 */
object ComprehensiveExample {

  def main(args: Array[String]): Unit = {
    // Creates a SparkSession.
    val spark = SparkSession
      .builder
      .appName(s"${this.getClass.getSimpleName}")
      .getOrCreate()
    val sc = spark.sparkContext

    // $example on$
    // Load my user data and parse into tuples of user id and attribute list
    val users = (sc.textFile("data/graphx/users.txt")
      .map(line => line.split(",")).map( parts => (parts.head.toLong, parts.tail) ))

    // Parse the edge data which is already in userId -> userId format
    val followerGraph = GraphLoader.edgeListFile(sc, "data/graphx/followers.txt")

    // Attach the user attributes
    val graph = followerGraph.outerJoinVertices(users) {
      case (uid, deg, Some(attrList)) => attrList
      // Some users may not have attributes so we set them as empty
      case (uid, deg, None) => Array.empty[String]
    }

    // Restrict the graph to users with usernames and names
    val subgraph = graph.subgraph(vpred = (vid, attr) => attr.size == 2)

    // Compute the PageRank
    val pagerankGraph = subgraph.pageRank(0.001)

    // Get the attributes of the top pagerank users
    val userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices) {
      case (uid, attrList, Some(pr)) => (pr, attrList.toList)
      case (uid, attrList, None) => (0.0, attrList.toList)
    }

    println(userInfoWithPageRank.vertices.top(5)(Ordering.by(_._2._1)).mkString("\n"))
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.taxis

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant

import scala.collection.mutable.ArrayBuffer
/**
  * Created by wangbaosheng on 2017/9/18.
  * 输入：该车的按照时间排序的明细数据
  * 输出：该车的出租车报表结果
  */
class Compute(stateConf:StateConf) {
  def compute(sortedDetails: Iterable[DetailModel]): Iterable[TaxisResult] = {
    val mergedDetails = merge(sortedDetails)
    doCompute(mergedDetails)
  }

  def merge(sortedDetails: Iterable[DetailModel]): Iterable[DetailModel] = {
    //    var prevDetail: DetailModel = null
    val result = new ArrayBuffer[DetailModel]()

    sortedDetails.foreach(curDetail => {
      if (result.lastOption.nonEmpty) {
        val last = result.last
        if (needMerge(last, curDetail)) {
          result.remove(result.length - 1)
          result.append(doMerge(last, curDetail))
        } else {
          result.append(curDetail)
        }
      } else {
        result.append(curDetail)
      }
    })
    result
  }

  def doCompute(details: Iterable[DetailModel]): Iterable[TaxisResult] = {
    //行驶状态的上一个充电
    var prevChargeOfCurrentRunState: Option[DetailModel] = None

    //充电状态的上一次充电
    var prevChargeOfCurrentChargeState:Option[DetailModel]=None

    details.map(curDetail => {
      val curState = curDetail.category
      val reportDate=new Date(curDetail.startTime)

      if (curState == Constant.ChargeState) {
        val result=TaxisResult(vid = curDetail.vid,
          category = curState,
          startTime = curDetail.startTime,
          endTime = curDetail.endTime,
          timeLength = curDetail.timeLeng.toInt,
          accRunTime=curDetail.accRunTime,
          startMileage = curDetail.startMileage,
          endMileage = curDetail.stopMileage,
          mileage = curDetail.stopMileage - curDetail.startMileage,
          startSoc = curDetail.startSoc,
          endSoc = curDetail.endSoc,
          startLongitude = curDetail.startLongitude,
          startLatitude = curDetail.startLatitude,
          stopLongitude = curDetail.endLongitude,
          stopLatitude = curDetail.endLatitude,

          timeBetweenCharge = curDetail.startTime - prevChargeOfCurrentChargeState.map(_.endTime).getOrElse(curDetail.startTime),
          charge = curDetail.totalCharge,
          prevChargeEndMileage = prevChargeOfCurrentChargeState.map(_.stopMileage).getOrElse(0),
          maxCurent = curDetail.minTotalCurrent,

          avgSpeed = 0,
          prevChargeEndTime =None,
          prevChargeMaxCurrent = None,

          isQuickCharge = curDetail.isQuickCharge)

        prevChargeOfCurrentChargeState=Some(curDetail)
        prevChargeOfCurrentRunState = Some(curDetail)

        result
      } else if (curState == Constant.TravelState) {
        val result=TaxisResult(vid = curDetail.vid,
          category = curState,
          startTime = curDetail.startTime,
          endTime = curDetail.endTime,
          timeLength = curDetail.timeLeng.toInt,
          accRunTime=curDetail.accRunTime,
          startMileage = curDetail.startMileage,
          endMileage = curDetail.stopMileage,
          mileage = curDetail.stopMileage - curDetail.startMileage,
          startSoc = curDetail.startSoc,
          endSoc = curDetail.endSoc,
          startLongitude = curDetail.startLongitude,
          startLatitude = curDetail.startLatitude,
          stopLongitude = curDetail.endLongitude,
          stopLatitude = curDetail.endLatitude,

          timeBetweenCharge = 0,
          charge = 0,
          prevChargeEndMileage = 0,
          maxCurent = 0,

          avgSpeed = DataPrecision.mileage(curDetail.stopMileage-curDetail.startMileage)/DataPrecision.toHour(curDetail.accRunTime),
          prevChargeEndTime =  prevChargeOfCurrentRunState.map(_.endTime),
          prevChargeMaxCurrent = prevChargeOfCurrentRunState.map(_.minTotalCurrent),

          isQuickCharge = curDetail.isQuickCharge
        )

        prevChargeOfCurrentRunState = None
        result
      } else {
        null
      }
    }).filter(_ != null)
  }

  /**
    * 判断是否需要合并明细数据
    * 合并条件为：两条数据同为充电/行驶/满点，并且跨越零点,并且相邻状态的时间间隔不能太远。
    **/
  def needMerge(prevDetail: DetailModel, curDetail: DetailModel): Boolean = {
    //2016-02-01 12:12:12
    if (prevDetail == null || prevDetail.category != curDetail.category) false
    else {
      val zeroDate = new Date(prevDetail.endTime)
      zeroDate.setDate(zeroDate.getDate + 1)
      zeroDate.setHours(0)
      zeroDate.setMinutes(0)
      zeroDate.setSeconds(0)

      val zero = zeroDate.getTime
      prevDetail.endTime < zero && curDetail.startTime >= zero &&
        (curDetail.startTime - prevDetail.endTime) <= 10 * 60 * 1000
    }
  }

  def doMerge(prevDetail: DetailModel, curDetail: DetailModel): DetailModel = {
    DetailModel(
      vid = curDetail.vid,
      category = curDetail.category,

      startTime = prevDetail.startTime,
      endTime = curDetail.endTime,
      timeLeng = prevDetail.timeLeng + curDetail.timeLeng,

      accRunTime = prevDetail.accRunTime + curDetail.accRunTime,
      startMileage = prevDetail.startMileage,
      stopMileage = curDetail.stopMileage,
      maxSpeed = Math.max(prevDetail.maxSpeed, curDetail.maxSpeed),
      minTotalCurrent = Math.min(prevDetail.minTotalCurrent, curDetail.minTotalCurrent),
      startSoc = prevDetail.startSoc,
      endSoc = curDetail.endSoc,
      startLongitude = prevDetail.startLatitude,
      startLatitude = prevDetail.startLatitude,
      endLongitude = curDetail.endLongitude,
      endLatitude = curDetail.endLatitude,
      totalCharge = prevDetail.totalCharge + curDetail.totalCharge,
      isQuickCharge = curDetail.isQuickCharge)
  }
}
package com.bitnei.report.dayreport.validate

import com.bitnei.report.common.log.Logging
import org.apache.spark.sql.Row

case class RealinfoValidateResult(vid:String,
                                  realCount: Int,
                                  realValidityCount: Int,
                                  realInvalidityCount: Int, secondaryVoltageCount: Int, secondaryVoltageValidateCount: Int, secondaryVoltageInvalidateCount: Int,
                                  pabTempCount: Int, pabTempValidCount: Int, pabTempInvalidCount: Int,
                                  vehicleCount: Int, vehicleValidCount: Int, vehicleInvalidCount: Int,
                                  gpsCount: Int, gpsValidCount: Int, gpsInvalidCount: Int,
                                  limitationCount: Int, limitationValidCount: Int, limitationInvalidCount: Int,
                                  alarmCount: Int, alarmValidCount: Int, alarmInvalidCount: Int, default_1: Int, default_2: Int, default_3: Int //平台转发暂时不实现
                                 )
class ComputeRealinfoValidate extends Serializable with Logging{
  //REALTIME 数据接收有效条数
  var realValidityCount: Int = 0
  //数据接受无效条数
  var realInvalidityCount: Int = 0

  var secondaryVoltageCount: Int = 0
  var secondaryVoltageValidateCount: Int = 0
  var secondaryVoltageInvalidateCount: Int = 0

  var pabTempCount: Int = 0
  var pabTempValidCount = 0
  var pabTempInvalidCount = 0

  var vehicleCount = 0
  var vehicleValidCount = 0
  var vehicleInvalidCount = 0

  var gpsCount = 0
  var gpsValidCount = 0
  var gpsInvalidCount = 0

  var limitationCount = 0
  var limitationValidCount = 0
  var limitationInvalidCount = 0

  var alarmCount = 0
  var alarmValidCount = 0
  var alarmInvalidCount = 0


  def compute(realinfos: Iterable[Row]): RealinfoValidateResult = {
    var vid=""
    realinfos.foreach(row => {
      vid=row.getAs("VID")
      computeRealinfo(row)
    })



    RealinfoValidateResult(vid,realinfos.size, realValidityCount, realInvalidityCount,
      secondaryVoltageCount, secondaryVoltageValidateCount, secondaryVoltageInvalidateCount,
      pabTempCount, pabTempValidCount, pabTempInvalidCount,
      vehicleCount, vehicleValidCount, vehicleInvalidCount,
      gpsCount, gpsValidCount, gpsInvalidCount,
      limitationCount, limitationValidCount, limitationInvalidCount,
      alarmCount, alarmValidCount, alarmInvalidCount, 0, 0, 0
    )
  }

  def computeRealinfo(row: Row) {
    def toMap: scala.collection.immutable.Map[String, String] = {
      val m = scala.collection.mutable.HashMap[String, String]()
      for (field <- row.schema.fields) {
        m.put(field.name, row.getAs[String](field.name))
      }
      m.toMap
    }

    def validate(map: Map[String, String]): Boolean = {
      map.contains("2001") && map.contains("2002") && map.contains("2003") &&
        map.contains("2101") && map.contains("2102") && map.contains("2103") &&
        map.contains("2201") && map.contains("2202") && map.contains("2203") &&
        map.contains("2208") && map.contains("2209") && map.contains("2210") &&
        map.contains("2301") && map.contains("2302") && map.contains("2303") &&
        map.contains("2304") && map.contains("2305") && map.contains("2306") &&
        map.contains("2501") && map.contains("2502") && map.contains("2503") &&
        map.contains("2504") && map.contains("2505") && map.contains("2601") &&
        map.contains("2602") && map.contains("2603") && map.contains("2604") &&
        map.contains("2605") && map.contains("2606") && map.contains("2607") &&
        map.contains("2608") && map.contains("2609") && map.contains("2610") &&
        map.contains("2611") && map.contains("2612") && map.contains("2613") &&
        map.contains("2614") && map.contains("2615") && map.contains("2616") &&
        map.contains("2617") && map.contains("2901") && map.contains("2902") &&
        map.contains("2903") && map.contains("2904") && map.contains("2905") &&
        map.contains("2909") && map.contains("2910") && map.contains("2911") &&
        map.contains("2912")

    }


    def isEmpty(f: Option[String]): Boolean = f.isEmpty || f.get == null || f.get.trim == ""

    val map = toMap

    try {
      if (validate(map)) {
        realValidityCount += 1
      } else {
        realInvalidityCount += 1
      }

      if (map.contains("2001") && map.contains("2002") && map.contains("2003")) {
        secondaryVoltageCount += 1
      }


      if (!isEmpty(map.get("2001")) && !isEmpty(map.get("2002")) && !isEmpty(map.get("2003"))) {
        secondaryVoltageValidateCount += 1
      } else {
        secondaryVoltageInvalidateCount += 1
      }


      if (map.contains("2101") && map.contains("2102") && map.contains("2103")) {
        pabTempCount += 1
      }

      if (!isEmpty(map.get("2101")) && !isEmpty(map.get("2102")) && !isEmpty(map.get("2103"))) {
        pabTempValidCount += 1
      } else {
        pabTempInvalidCount += 1
      }

      if (map.contains("2201") && map.contains("2202") && map.contains("2203") &&
        map.contains("2208") && map.contains("2209") && map.contains("2210") &&
        map.contains("2301") && map.contains("2302") && map.contains("2303") &&
        map.contains("2304") && map.contains("2305") && map.contains("2306")) {
        vehicleCount += 1
      }


      if (!isEmpty(map.get("2201")) && !isEmpty(map.get("2202")) && !isEmpty(map.get("2203")) &&
        !isEmpty(map.get("2208")) && !isEmpty(map.get("2209")) && !isEmpty(map.get("2210")) &&
        !isEmpty(map.get("2301")) && !isEmpty(map.get("2302")) && !isEmpty(map.get("2303")) &&
        !isEmpty(map.get("2304")) && !isEmpty(map.get("2305")) && !isEmpty(map.get("2306"))) {
        vehicleValidCount += 1
      } else {
        vehicleInvalidCount += 1
      }

      //卫星定位数据总条数
      if (map.contains("2501") && map.contains("2502") && map.contains("2503") &&
        map.contains("2504") && map.contains("2505")) {
        gpsCount += 1
      }


      if (!isEmpty(map.get("2501")) && !isEmpty(map.get("2502")) && !isEmpty(map.get("2503")) &&
        !isEmpty(map.get("2504")) && !isEmpty(map.get("2505"))) {
        gpsValidCount += 1
      } else {
        gpsInvalidCount += 1
      }


      if (map.contains("2601") && map.contains("2602") && map.contains("2603") &&
        map.contains("2604") && map.contains("2605") && map.contains("2606") &&
        map.contains("2607") && map.contains("2608") && map.contains("2609") &&
        map.contains("2610") && map.contains("2611") && map.contains("2612") &&
        map.contains("2613") && map.contains("2614") && map.contains("2615") &&
        map.contains("2616") && map.contains("2617")) {
        limitationCount += 1
      }


      if (!isEmpty(map.get("2601")) && !isEmpty(map.get("2602")) && !isEmpty(map.get("2603")) &&
        !isEmpty(map.get("2604")) && !isEmpty(map.get("2605")) && !isEmpty(map.get("2606")) &&
        !isEmpty(map.get("2607")) && !isEmpty(map.get("2608")) && !isEmpty(map.get("2609")) &&
        !isEmpty(map.get("2610")) && !isEmpty(map.get("2611")) && !isEmpty(map.get("2612")) &&
        !isEmpty(map.get("2613")) && !isEmpty(map.get("2614")) && !isEmpty(map.get("2615")) &&
        !isEmpty(map.get("2616")) && !isEmpty(map.get("2617"))) {
        limitationValidCount += 1
      } else {
        limitationInvalidCount += 1
      }

      //报警数据总条数

      if (map.contains("2901") && map.contains("2902") && map.contains("2903") &&
        map.contains("2904") && map.contains("2905") && map.contains("2906") &&
        map.contains("2907") && map.contains("2908") && map.contains("2909") &&
        map.contains("2610") && map.contains("2611") && map.contains("2612") &&
        map.contains("2910") && map.contains("2911") && map.contains("2912")) {
        alarmCount += 1
      }

      if (!isEmpty(map.get("2901")) && !isEmpty(map.get("2902")) && !isEmpty(map.get("2903")) &&
        !isEmpty(map.get("2904")) && !isEmpty(map.get("2905")) && !isEmpty(map.get("2906")) &&
        !isEmpty(map.get("2907")) && !isEmpty(map.get("2908")) && !isEmpty(map.get("2909")) &&
        !isEmpty(map.get("2610")) && !isEmpty(map.get("2611")) && !isEmpty(map.get("2612")) &&
        !isEmpty(map.get("2910")) && !isEmpty(map.get("2911")) && !isEmpty(map.get("2912"))) {
        alarmValidCount += 1
      } else {
        alarmInvalidCount += 1
      }
    } catch {
      case e: Exception =>
        logError(row.toString()+e.getMessage)
    }
  }
}

package com.bitnei.report.common.configuration

import com.bitnei.report.common.log.Logging

/**
  * Created by franciswang on 2016/9/28.
  */


object Configuration extends Logging{
  val conf=new org.apache.commons.configuration.PropertiesConfiguration()
  val confPath=System.getenv("SPARK_REPORT_CONF")

  try {

    logInfo("加载默认配置文件"+confPath)
    conf.load(confPath)
    logInfo("默认配置文件加载完成"+confPath)
  }catch {
    case e: Exception =>
      logError("默认配置文件加载失败" + confPath)

  }

  def getAsString(name: String,defaultValue:String): String = {
    conf.getString(name,defaultValue)
  }

  def getKeys():java.util.Iterator[String]=conf.getKeys() match {
    case iter:java.util.Iterator[String]=> iter
    case _=>null
  }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

// $example on$
import org.apache.spark.graphx.GraphLoader
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * A connected components algorithm example.
 * The connected components algorithm labels each connected component of the graph
 * with the ID of its lowest-numbered vertex.
 * For example, in a social network, connected components can approximate clusters.
 * GraphX contains an implementation of the algorithm in the
 * [`ConnectedComponents` object][ConnectedComponents],
 * and we compute the connected components of the example social network dataset.
 *
 * Run with
 * {{{
 * bin/run-example graphx.ConnectedComponentsExample
 * }}}
 */
object ConnectedComponentsExample {
  def main(args: Array[String]): Unit = {
    // Creates a SparkSession.
    val spark = SparkSession
      .builder
      .appName(s"${this.getClass.getSimpleName}")
      .getOrCreate()
    val sc = spark.sparkContext

    // $example on$
    // Load the graph as in the PageRank example
    val graph = GraphLoader.edgeListFile(sc, "data/graphx/followers.txt")
    // Find the connected components
    val cc = graph.connectedComponents().vertices
    // Join the connected components with the usernames
    val users = sc.textFile("data/graphx/users.txt").map { line =>
      val fields = line.split(",")
      (fields(0).toLong, fields(1))
    }
    val ccByUsername = users.join(cc).map {
      case (id, (username, cc)) => (username, cc)
    }
    // Print the result
    println(ccByUsername.collect().mkString("\n"))
    // $example off$
    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.cellVoltageConsist

import java.util.Date

import com.bitnei.report.common.utils.Utils

/**
  *
  * @param vid           单车车号
  * @param partitionTime 时间
  /** @param socLeftPower  用来标记剩余电量*/
  /** @param mVoltage      根据前三个参数算出的单体电压*/
  */
/*case class ConsistResultModel(
                               vid: String,
                               partitionTime: Long,
                               socLeftPower: Int,
                               mVoltage: String
                             ) {
  override def toString = f"${vid},${Utils.formatDate(new Date(partitionTime), "yyyy-MM-dd HH:mm:ss")},${socLeftPower},${mVoltage}"
}*/

case class ConsistResultModel(
                               vid: String,
                               month: String,
                               mVoltage_100:String,
                               mVoltage_50:String,
                               mVoltage_20:String
                             )
{
  override def toString = f"${vid},${month},${mVoltage_100},${mVoltage_50},${mVoltage_20}"
}
package com.bitnei.common.constants

/**
  * Created by franciswang on 2016/9/29.
  */
object Constant {
  val VehicleIdName:String = "VID"
  val VehicleVINName:String="VIN"
  val VehicleTimeName:String="TIME"
  val MessageType:String="MESSAGETYPE"

  val VehicleChargeName:String="2614"
  val VehicleMileageName:String="2202"
  val VehicleRealSocName:String="2615"
  val VehicleRealSpeedName:String="2201"
  val VehicleRealTotalVoltageName:String="2613"
  val VehicleRealTotalCurrentName:String="2614"
  val VehicleSecondaryCellMaxVoltageName:String="2603"
  val VehicleSecondaryCellMinVoltageName:String="2606"
  val VehicleAccuisitionPointMaxTempName:String="2609"
  val VehicleAccuisitionPointMinTempNameE:String="2612"
  val VehicleEngineTempName:String="2304"

  val AlarmTempDiffName:String="2901"
  val AlarmBatteryPostTempName:String="2902"
  val AlarmPABOverVoltageName:String="2903"
  val AlarmPABUnderVoltageName:String="2904"
  val AlarmLowSocName:String="2905"
  val AlarmSecndaryOverVoltageName:String="2906"
  val AlarmSecondaryUnderVoltageName:String="2907"
  val ALarmLowLowSocName:String="2908"
  val ALarmUpperSocName:String="2909"
  val AlarmPABPackageNotMatchName:String="2910"
  val AlarmPABConsistencyName:String="2911"
  val AlarmSecondaryInsulationFaultName:String="2912"

  val VEHICLE_ALARM_LEVEL_NAME:String="ALARM_LEVEL"
  val INVALID_VEHICE_VID:String ="-1"

  val Longitude="2502"//经度
  val Latitude="2503"//纬度

  val MessageTypeRealInfo:String="REALTIME"
  val MessageTypeLogin:String="LOGIN"
  val MessageTypeHistory:String="HISTORY"
  val MessageTypeForward:String="FORWARD"
  val MessageTypeAlarm:String="ALARM"
  val MessageTypeTerm:String="TERMSTATUS"
  val MessageTypeRentcar:String="RENTCAR"
  val MessageTypeCar:String="CARSTATUS"


  val ForwardResultName:String="RESULT"

  val AlarmStatusName:String="STATUS"
  val AlarmTypeName:String="LEFT1"
  val AlarmTime:String="TIME"
  val AlarmId:String="ALARM_ID"
  val AlarmLevel:String="ALARM_LEVEL"

  val Alarm_Level_1:String="1"
  val Alarm_Level_2:String="2"
  val Alarm_Level_3:String="3"
  val Alarm_Level_4:String="4"
  val Alarm_Level_5:String="5"

  val NoneState:String="none"
  val ChargeState: String = "charge"
  val TravelState: String = "run"
  val FullChargeState: String = "fullcharge"
  val ValidateState:String="validity"
  val ChargeChangeState="chargechange"

  val LogginInvaid:String=""
  val LoginSucess:String="0"
  val LogginFailed:String="1"

  val SPARK_DEPLOY_LOCAL:String="spark.deploy.local"

  val StateWindowLength:String="state.window.internal"
  val STATE_WINDOW_TRAVEL_LENGTH:String="state.window.travel.length"
  val STATE_WINDOW_FULL_CHARGE_LENGTH:String="state.window.fullcharge.length"
  val STATE_WINDOW_ENABLE_PARALLE:String="state.bitnei.state.window.enableparalle"
  val STATE_WINDOW_NUM_PARTITIONS:String="state.bitnei.state.window.numpartitions"
  val StateWindowChargeSpeedUpper="state.window.charge.begin.speed.upper"
  val StateWindowChargeLength:String="state.window.charge.length"
  val StateWindowChargeBeginLength:String="state.window.charge.begin.length"
  val StateWindowChargeEndLength:String="state.window.charge.end.length"
  val StateWindowTravelBeginLength:String="state.window.travel.begin.length"
  val StateWindowFullChargeBeginLength:String="state.window.fullcharge.begin.length"
  val StateWindowFullChargeEndLength:String="state.window.fullcharge.end.length"
  val MaxSoc:String="soc.max"

  val OutputDatabase="state.output.database.enable"

  val AlluxioMaster:String="alluxio.master"
  val AlluxioCachePath="alluxio.cache.path"

  val JdbcDriver:String="jdbc.driver"
  val JdbcUrl:String="jdbc.url"
  val JdbcUserName:String="jdbc.username"
  val JdbcPasswd:String="jdbc.passwd"
  val JdbcPoolInitSize:String="jdbc.pool.initsize"
  val JdbcPoolMaxActive="jdbc.pool.active.max"
  val JdbcPoolMaxIdel="jdbc.pool.idel.max"
  val JdbcPoolMaxWait="jdbc.pool.wait.max"

  val ChargeStateDayReportTable:String="state.table.charge.dayreport"
  val FullChargeStateDayReportTable:String="state.table.fullcharge.dayreport"
  val TravelStateDayReportTable:String="state.table.travel.dayreport"
  val CategoryDayReportTable:String="state.table.category.dayreport"
  val StateAlarmTable="state.table.alarm.report"
  val StateValidateTable="state.table.validate.report"
  val FullChargeStateTable:String="state.table.fullcharge"
  val ChargeStateTable:String="state.table.charge"
  val TravelStateTable:String="state.table.travel"

  val RealinfoSchema="realinfo.schema"
  val ReportDate="state.reportdate"
  val RealinfoPath="realinfo.input.path"
  val AlarmPath="alarm.input.path"

  val CheckPointPath="spark.checkpoint.path"

  val RealInfoSchema= Array(
    "name:VID,type:StringType,nullable:false,alias:VID,default: ",
    "name:VIN,type:StringType,nullable:true,alias:VIN,default: ",
    "name:TIME,type:StringType,nullable:false,alias:TIME,default: ",
    "name:ruleId,type:StringType,nullable:true,alias:ruleId,default:1",//默认为地标
    "name:2201,type:IntegerType,nullable:true,alias:speed ",
    "name:2614,type:IntegerType,nullable:true,alias:charge ",
    "name:2615,type:IntegerType,nullable:true,alias:soc ",
    "name:2613,type:IntegerType,nullable:true,alias:totalVoltage ",
    "name:2603,type:IntegerType,nullable:true,alias:secondaryCellMaxVoltage ",
    "name:2606,type:IntegerType,nullable:true,alias:secondaryCellMinVoltage ",
    "name:2304,type:IntegerType,nullable:true,alias:engineTemp ",
    "name:2609,type:IntegerType,nullable:true,alias:accuisitionPointMaxTemp ",
    "name:2612,type:IntegerType,nullable:true,alias:accuisitionPointMinTemp ",
    "name:2202,type:IntegerType,nullable:true,alias:mileage ",
    "name:2502,type:LongType,nullable:true,alias:longitude ",
    "name:2503,type:LongType,nullable:true,alias:latitude ")

  val ForwardInputPath="forward.input.path"
  val LoginInputPath="login.input.path"
  val TermInputPath="term.input.path"
}
package com.bitnei.tools.common

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-11-22 13:56
  *
  */
object Constants {


  //////////////////////////////////////realinfo////////////////////////////////////

  //  0x01：停车充电；0x02：行驶充电；0x03：未充电状态；0x04：充电完成；“0xFE”表示异常，“0xFF”表示无效

  val REALINFO_COL_NAME_CHARGE_STATE = "2301"

  val REALINFO_CHARGE_STATE = Map(
    "stop" -> 1,
    "run" -> 2,
    "non" -> 3,
    "finish" -> 4,
    "exception" -> 254,
    "invalid" -> 255
  )


  /////////////////////////////////////////////////////////////////////////
  val VehicleIdName: String = "VID"
  val VehicleVINName: String = "VIN"
  val VehicleTimeName: String = "TIME"
  val MessageType: String = "MESSAGETYPE"

  val VehicleChargeName: String = "2614"
  val VehicleMileageName: String = "2202"
  val VehicleRealSocName: String = "2615"
  val VehicleRealSpeedName: String = "2201"
  val VehicleRealTotalVoltageName: String = "2613"
  val VehicleRealTotalCurrentName: String = "2614"
  val VehicleSecondaryCellMaxVoltageName: String = "2603"
  val VehicleSecondaryCellMinVoltageName: String = "2606"
  val VehicleAccuisitionPointMaxTempName: String = "2609"
  val VehicleAccuisitionPointMinTempNameE: String = "2612"
  val VehicleEngineTempName: String = "2304"

  val AlarmTempDiffName: String = "2901"
  val AlarmBatteryPostTempName: String = "2902"
  val AlarmPABOverVoltageName: String = "2903"
  val AlarmPABUnderVoltageName: String = "2904"
  val AlarmLowSocName: String = "2905"
  val AlarmSecndaryOverVoltageName: String = "2906"
  val AlarmSecondaryUnderVoltageName: String = "2907"
  val ALarmLowLowSocName: String = "2908"
  val ALarmUpperSocName: String = "2909"
  val AlarmPABPackageNotMatchName: String = "2910"
  val AlarmPABConsistencyName: String = "2911"
  val AlarmSecondaryInsulationFaultName: String = "2912"

  val VEHICLE_ALARM_LEVEL_NAME: String = "ALARM_LEVEL"
  val INVALID_VEHICE_VID: String = "-1"

  val Longitude = "2502"
  //经度
  val Latitude = "2503" //纬度

  val MessageTypeRealInfo: String = "REALTIME"
  val MessageTypeLogin: String = "LOGIN"
  val MessageTypeHistory: String = "HISTORY"
  val MessageTypeForward: String = "FORWARD"
  val MessageTypeAlarm: String = "ALARM"
  val MessageTypeTerm: String = "TERMSTATUS"
  val MessageTypeRentcar: String = "RENTCAR"
  val MessageTypeCar: String = "CARSTATUS"


  val ForwardResultName: String = "RESULT"

  val AlarmStatusName: String = "STATUS"
  val AlarmTypeName: String = "LEFT1"
  val AlarmTime: String = "TIME"
  val AlarmId: String = "ALARM_ID"
  val AlarmLevel: String = "ALARM_LEVEL"

  val Alarm_Level_1: String = "1"
  val Alarm_Level_2: String = "2"
  val Alarm_Level_3: String = "3"
  val Alarm_Level_4: String = "4"
  val Alarm_Level_5: String = "5"

  val NoneState: String = "none"
  val ChargeState: String = "charge"
  val TravelState: String = "run"
  val FullChargeState: String = "fullcharge"
  val ValidateState: String = "validity"
  val ChargeChangeState = "chargechange"

  val LogginInvaid: String = ""
  val LoginSucess: String = "0"
  val LogginFailed: String = "1"

  val SPARK_DEPLOY_LOCAL: String = "spark.deploy.local"

  val StateWindowLength: String = "state.window.internal"
  val STATE_WINDOW_TRAVEL_LENGTH: String = "state.window.travel.length"
  val STATE_WINDOW_FULL_CHARGE_LENGTH: String = "state.window.fullcharge.length"
  val STATE_WINDOW_ENABLE_PARALLE: String = "state.bitnei.state.window.enableparalle"
  val STATE_WINDOW_NUM_PARTITIONS: String = "state.bitnei.state.window.numpartitions"
  val StateWindowChargeSpeedUpper = "state.window.charge.begin.speed.upper"
  val StateWindowChargeLength: String = "state.window.charge.length"
  val StateWindowChargeBeginLength: String = "state.window.charge.begin.length"
  val StateWindowChargeEndLength: String = "state.window.charge.end.length"
  val StateWindowTravelBeginLength: String = "state.window.travel.begin.length"
  val StateWindowFullChargeBeginLength: String = "state.window.fullcharge.begin.length"
  val StateWindowFullChargeEndLength: String = "state.window.fullcharge.end.length"
  val MaxSoc: String = "soc.max"

  val OutputDatabase = "state.output.database.enable"

  val AlluxioMaster: String = "alluxio.master"
  val AlluxioCachePath = "alluxio.cache.path"

  val JdbcDriver: String = "jdbc.driver"
  val JdbcUrl: String = "jdbc.url"
  val JdbcUserName: String = "jdbc.username"
  val JdbcPasswd: String = "jdbc.passwd"
  val JdbcPoolInitSize: String = "jdbc.pool.initsize"
  val JdbcPoolMaxActive = "jdbc.pool.active.max"
  val JdbcPoolMaxIdel = "jdbc.pool.idel.max"
  val JdbcPoolMaxWait = "jdbc.pool.wait.max"

  val ChargeStateDayReportTable: String = "state.table.charge.dayreport"
  val FullChargeStateDayReportTable: String = "state.table.fullcharge.dayreport"
  val TravelStateDayReportTable: String = "state.table.travel.dayreport"
  val CategoryDayReportTable: String = "state.table.category.dayreport"
  val StateAlarmTable = "state.table.alarm.report"
  val StateValidateTable = "state.table.validate.report"
  val FullChargeStateTable: String = "state.table.fullcharge"
  val ChargeStateTable: String = "state.table.charge"
  val TravelStateTable: String = "state.table.travel"

  val RealinfoSchema = "realinfo.schema"
  val ReportDate = "state.reportdate"
  val RealinfoPath = "realinfo.input.path"
  val AlarmPath = "alarm.input.path"

  val CheckPointPath = "spark.checkpoint.path"

  val RealInfoSchema = Array(
    "name:VID,type:StringType,nullable:false,alias:VID,default: ",
    "name:VIN,type:StringType,nullable:true,alias:VIN,default: ",
    "name:TIME,type:StringType,nullable:false,alias:TIME,default: ",
    "name:ruleId,type:StringType,nullable:true,alias:ruleId,default:1", //默认为地标
    "name:2201,type:IntegerType,nullable:true,alias:speed ",
    "name:2614,type:IntegerType,nullable:true,alias:charge ",
    "name:2615,type:IntegerType,nullable:true,alias:soc ",
    "name:2613,type:IntegerType,nullable:true,alias:totalVoltage ",
    "name:2603,type:IntegerType,nullable:true,alias:secondaryCellMaxVoltage ",
    "name:2606,type:IntegerType,nullable:true,alias:secondaryCellMinVoltage ",
    "name:2304,type:IntegerType,nullable:true,alias:engineTemp ",
    "name:2609,type:IntegerType,nullable:true,alias:accuisitionPointMaxTemp ",
    "name:2612,type:IntegerType,nullable:true,alias:accuisitionPointMinTemp ",
    "name:2202,type:IntegerType,nullable:true,alias:mileage ",
    "name:2502,type:LongType,nullable:true,alias:longitude ",
    "name:2503,type:LongType,nullable:true,alias:latitude ")

  val ForwardInputPath = "forward.input.path"
  val LoginInputPath = "login.input.path"
  val TermInputPath = "term.input.path"

}
package com.bitnei.report.dayreport.distribution

import java.util.Date

import com.bitnei.report.distribute.Distribution



class ContinueDistributionCounter extends Distribution {
  private val distributed = Array.fill(12)(0)
  override def interval = 2

  def add(startValue: Long, endValue: Long): Unit = {
    val startIndex = getIndex(startValue)
    val endIndex = getIndex(endValue)

    (startIndex to endIndex).foreach(i => {
      if (0 <= i && i < distributed.length) distributed(i) += 1
    })
  }


  def getIndex(ms: Long): Int = {
    val startDate = new Date(ms)
    val startH = if (startDate.getMinutes == 0 && startDate.getSeconds == 0) startDate.getHours else startDate.getHours + 1
    val v = startH

    super.index(v)
  }

  override def getDistribution: Array[Int] = distributed
}


object ContinueDistributionCounter{
  def default: Array[Int] = Array.fill(12)(0)
}package com.bitnei.report.dayreport.realinfo

import scala.collection.mutable.ArrayBuffer

/*
* created by wangbaosheng on 2017/11/14
*/

object CoordFilter {

  case class Coord(time: Long, longitude: Long, latitude: Long)

  type ContinueCoordWindow = ArrayBuffer[Coord]

//  def compute(x: Array[Coord]): Array[ContinueCoordWindow] = {
//    def doCompute(i: Int, continuCoordWindowList: ArrayBuffer[ContinueCoordWindow]): Array[ContinueCoordWindow] = {
//      if (i == x.length) {
//        continuCoordWindowList.toArray
//      } else {
//        if(distence(x(i),x(i+1))<=L){
//
//        }else{
//
//        }
//      }
//    }
//
//    doCompute(0, new ArrayBuffer[ContinueCoordWindow]())
//  }
}
package com.bitnei.report

import com.bitnei.report.common.utils.DataPrecision

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-11-20 13:22
  *
  */




object CoordsHandler {

  case class Coord(longitude: Long, lantitude: Long)

  case class PointWindow[T]() extends ArrayBuffer[T] {}

  /**
    * 计算两个经纬度坐标之间的距离
    *
    * @param start 起点的经纬度
    * @param end   终点的经纬度
    * @return
    */
  def getDistance(start: Array[Long], end: Array[Long]): Double = {

    val radLat1 = Math.toRadians(start(1))
    val radLat2 = Math.toRadians(end(1))

    val a = radLat1 - radLat2

    val b = Math.toRadians(start(0)) - Math.toRadians(end(0))

    var s = 2 * Math.asin(Math.sqrt(Math.pow(Math.sin(a / 2), 2) +
      Math.cos(radLat1) * Math.cos(radLat2) * Math.pow(Math.sin(b / 2), 2)));

    s = s * 6378.137 //地球赤道半径

    //        s = s * 6371.004

    //    s = Math.round(s * 10000) / 10000;

    return s * 1000;
  }


  /**
    *
    * 窗口切分算法
    *
    * @param coords          单车坐标集合
    * @param minL            相邻轨迹点间最小距离
    * @param maxL            相邻轨迹点间最大距离
    * @param minWindowLength 窗口最小轨迹点个数，如果某个低于该值，则是异常窗口
    * @return 窗口集合
    */
  def splitByWindow(coords: ArrayBuffer[Array[Long]], minL: Int, maxL: Int, minWindowLength: Int): Array[Array[Array[Double]]] = {


    val coordSplit = new PointSplit[Coord](minL, maxL, minWindowLength)

    val mapCoords = coords.map(x => Coord(x(0), x(1))).toArray

    def L(a: Coord, b: Coord): Double = getDistance(Array(a.longitude, a.lantitude), Array(b.longitude, b.lantitude))

    val windows = coordSplit.split(mapCoords)(L)

    windows.map(x => {

      x .map(e => Array(DataPrecision.latitude(e.longitude), DataPrecision.latitude(e.lantitude))).toArray
    })

  }


  /**
    * 区间过滤
    *
    * @param realinfoInputs
    * @param coordDiffHigh
    * @param coordDiffLow
    * @return
    */
  def rangeHandle(realinfoInputs: Array[RealinfoInput2], coordDiffHigh: Int, coordDiffLow: Int): ArrayBuffer[Array[Long]] = {

    val filtedCoords = new scala.collection.mutable.ArrayBuffer[Array[Long]]()

    //          // TODO: 验证经纬度的正确性
    for (i <- 0 until realinfoInputs.length) {
      if (i + 1 < realinfoInputs.length) {

        // TODO: 经纬度差判断
        val longDiff = Math.abs(realinfoInputs(i + 1).longitude.get.toLong - realinfoInputs(i).longitude.get.toLong)

        val latiDiff = Math.abs(realinfoInputs(i + 1).latitude.get.toLong - realinfoInputs(i).latitude.get.toLong)

        val filerRuler01 = longDiff <= coordDiffHigh && longDiff >= coordDiffLow && latiDiff <= coordDiffHigh && latiDiff >= coordDiffLow

        if (filerRuler01) {
          filtedCoords.+=(Array[Long](realinfoInputs(i).longitude.getOrElse(0L).toString.toLong, realinfoInputs(i).latitude.getOrElse(0L).toString.toLong))
          if (i.equals(realinfoInputs.length - 2)) {
            filtedCoords.+=(Array[Long](realinfoInputs(i + 1).longitude.getOrElse(0L).toString.toLong, realinfoInputs(i + 1).latitude.getOrElse(0L).toString.toLong))
          }
        }


      }
    }
    filtedCoords
  }


}
package com.bitnei.report

import com.bitnei.sparkhelper.HbaseHelper
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.util.Bytes


/*
* created by wangbaosheng on 2017/11/15
*/

case class Coord(longitude:Double, lantitude:Double)

//case class MonthCoord(vid: String, vin:String,date: String, mielage: Double, hours: Double, days: Int, coords: Array[Array[Double]])

object CoordSplitTest {
//
//  def test(path:String,vid:String,minL:Double,maxL:Double,minWindowLength:Int,quorum:String, zkport:String,L:(Coord,Coord)=>Double): Unit = {
//    val coords = scala.io.Source.fromFile(path).getLines().map(line => {
//      val fields = line.split(',')
//      val longitude = fields(0).toDouble
//      val lantitude = fields(1).toDouble
//      Coord(longitude, lantitude)
//    }).toArray
//
//    val coordSplit = new PointSplit[Coord](minL, maxL, minWindowLength)
//    val windows = coordSplit.split(coords)(L)
//
//    val jsonCoords=windows.map(window=>{
//      window.map(coord=>Array(coord.longitude,coord.lantitude))
//    })
//
//    val monthCoord = MonthCoord(vid, "vin", "201710", 10, 10, 10, null)
//
//    HbaseHelper.bulkPut(quorum, zkport, "mileage_check_coords", (table) => {
//      val mapper = new ObjectMapper()
//      mapper.registerModule(DefaultScalaModule)
//
//      val rowKey = s"${monthCoord.vid}_${monthCoord.date}"
//      val jsonValue = toJson(mapper, monthCoord)
//      table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "coord", jsonValue))
//    })
//
//    def toJson(mapper: ObjectMapper, locus: MonthCoord): String = {
//      val jsonValue = mapper.writeValueAsString(locus)
//      jsonValue
//    }
//  }
//

  def main(args: Array[String]): Unit = {
    //定义距离函数
    def L(a: Coord, b: Coord): Double = Math.abs(a.lantitude - b.lantitude)

    val minL = 2D
    val maxL = 7D

    val coordSplit = new PointSplit[Coord](minL, maxL, 2)
    //测试值
    val values = Array(
      Coord(10, 10),

      //有效轨迹窗口
      Coord(1, 1),
      Coord(2, 2),
      Coord(3, 3),

      //无效轨迹窗口，术语异常值

      Coord(100, 100),
      Coord(101,101),
      //有效轨迹窗口
      //Coord1(9,9),
      Coord(11, 11),
      Coord(15, 15),
      Coord(14, 14),
      Coord(11, 11),
      Coord(12, 12),
      Coord(14, 14),


      //有效轨迹窗口
      Coord(34, 34),
      Coord(30, 30),
      Coord(34, 34),
      Coord(29, 29),

      Coord(22, 22)
    )


    //轨迹窗口切分
    val windows = coordSplit.split(values)(L)

    assert(windows.length == 3)

    coordSplit.test(windows)(L)

    print(windows)
  }

  def print(windows: Array[PointWindow[Coord]]): Unit = {
    windows.foreach(window => {
      println("******")
      window.foreach(println)
    })

  }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.mllib.util.MLUtils

/**
 * An example app for summarizing multivariate data from a file. Run with
 * {{{
 * bin/run-example org.apache.spark.examples.mllib.Correlations
 * }}}
 * By default, this loads a synthetic dataset from `data/mllib/sample_linear_regression_data.txt`.
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object Correlations {

  case class Params(input: String = "data/mllib/sample_linear_regression_data.txt")
    extends AbstractParams[Params]

  def main(args: Array[String]) {

    val defaultParams = Params()

    val parser = new OptionParser[Params]("Correlations") {
      head("Correlations: an example app for computing correlations")
      opt[String]("input")
        .text(s"Input path to labeled examples in LIBSVM format, default: ${defaultParams.input}")
        .action((x, c) => c.copy(input = x))
      note(
        """
        |For example, the following command runs this app on a synthetic dataset:
        |
        | bin/spark-submit --class org.apache.spark.examples.mllib.Correlations \
        |  examples/target/scala-*/spark-examples-*.jar \
        |  --input data/mllib/sample_linear_regression_data.txt
        """.stripMargin)
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"Correlations with $params")
    val sc = new SparkContext(conf)

    val examples = MLUtils.loadLibSVMFile(sc, params.input).cache()

    println(s"Summary of data file: ${params.input}")
    println(s"${examples.count()} data points")

    // Calculate label -- feature correlations
    val labelRDD = examples.map(_.label)
    val numFeatures = examples.take(1)(0).features.size
    val corrType = "pearson"
    println()
    println(s"Correlation ($corrType) between label and each feature")
    println(s"Feature\tCorrelation")
    var feature = 0
    while (feature < numFeatures) {
      val featureRDD = examples.map(_.features(feature))
      val corr = Statistics.corr(labelRDD, featureRDD)
      println(s"$feature\t$corr")
      feature += 1
    }
    println()

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.rdd.RDD
// $example off$

object CorrelationsExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("CorrelationsExample")
    val sc = new SparkContext(conf)

    // $example on$
    val seriesX: RDD[Double] = sc.parallelize(Array(1, 2, 3, 3, 5))  // a series
    // must have the same number of partitions and cardinality as seriesX
    val seriesY: RDD[Double] = sc.parallelize(Array(11, 22, 33, 33, 555))

    // compute the correlation using Pearson's method. Enter "spearman" for Spearman's method. If a
    // method is not specified, Pearson's method will be used by default.
    val correlation: Double = Statistics.corr(seriesX, seriesY, "pearson")
    println(s"Correlation is: $correlation")

    val data: RDD[Vector] = sc.parallelize(
      Seq(
        Vectors.dense(1.0, 10.0, 100.0),
        Vectors.dense(2.0, 20.0, 200.0),
        Vectors.dense(5.0, 33.0, 366.0))
    )  // note that each Vector is a row and not a column

    // calculate the correlation matrix using Pearson's method. Use "spearman" for Spearman's method
    // If a method is not specified, Pearson's method will be used by default.
    val correlMatrix: Matrix = Statistics.corr(data, "pearson")
    println(correlMatrix.toString)
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}

/**
 * Compute the similar columns of a matrix, using cosine similarity.
 *
 * The input matrix must be stored in row-oriented dense format, one line per row with its entries
 * separated by space. For example,
 * {{{
 * 0.5 1.0
 * 2.0 3.0
 * 4.0 5.0
 * }}}
 * represents a 3-by-2 matrix, whose first row is (0.5, 1.0).
 *
 * Example invocation:
 *
 * bin/run-example mllib.CosineSimilarity \
 * --threshold 0.1 data/mllib/sample_svm_data.txt
 */
object CosineSimilarity {
  case class Params(inputFile: String = null, threshold: Double = 0.1)
    extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("CosineSimilarity") {
      head("CosineSimilarity: an example app.")
      opt[Double]("threshold")
        .required()
        .text(s"threshold similarity: to tradeoff computation vs quality estimate")
        .action((x, c) => c.copy(threshold = x))
      arg[String]("<inputFile>")
        .required()
        .text(s"input file, one row per line, space-separated")
        .action((x, c) => c.copy(inputFile = x))
      note(
        """
          |For example, the following command runs this app on a dataset:
          |
          | ./bin/spark-submit  --class org.apache.spark.examples.mllib.CosineSimilarity \
          | examplesjar.jar \
          | --threshold 0.1 data/mllib/sample_svm_data.txt
        """.stripMargin)
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName("CosineSimilarity")
    val sc = new SparkContext(conf)

    // Load and parse the data file.
    val rows = sc.textFile(params.inputFile).map { line =>
      val values = line.split(' ').map(_.toDouble)
      Vectors.dense(values)
    }.cache()
    val mat = new RowMatrix(rows)

    // Compute similar columns perfectly, with brute force.
    val exact = mat.columnSimilarities()

    // Compute similar columns with estimation using DIMSUM
    val approx = mat.columnSimilarities(params.threshold)

    val exactEntries = exact.entries.map { case MatrixEntry(i, j, u) => ((i, j), u) }
    val approxEntries = approx.entries.map { case MatrixEntry(i, j, v) => ((i, j), v) }
    val MAE = exactEntries.leftOuterJoin(approxEntries).values.map {
      case (u, Some(v)) =>
        math.abs(u - v)
      case (u, None) =>
        math.abs(u)
    }.mean()

    println(s"Average absolute error in estimate is: $MAE")

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.common.mileage

import com.bitnei.report.FoldMonod

/*
* created by wangbaosheng on 2017/12/15
*/

class CounterMonid extends FoldMonod[Counter,Counter]{
  override def zero(): Counter = Counter(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
  override def fold(a: Counter, b: Counter): Counter = {
    val validateCount=a.validateCount+b.validateCount
    val totalCount=a.totalCount+b.totalCount

    Counter(
      totalCount=a.totalCount+b.totalCount,
      speedEmptyCount=a.speedEmptyCount+b.speedEmptyCount,
      mileageEmptyCount=a.mileageEmptyCount+b.mileageEmptyCount,
      voltageEmptyCount=a.voltageEmptyCount+b.voltageEmptyCount,
      currentEmptyCount=a.currentEmptyCount+b.currentEmptyCount,
      socEmptyCount=a.socEmptyCount+b.socEmptyCount,
      longitudeEmptyCount=a.longitudeEmptyCount+b.longitudeEmptyCount,
      latitudeEmptyCount=a.latitudeEmptyCount+b.latitudeEmptyCount,
      flameoutCount=a.flameoutCount+b.flameoutCount,
      emptyCount=a.emptyCount+b.emptyCount,
      nonEmptyCount=a.nonEmptyCount+b.nonEmptyCount,
      speedExceptionCount=a.speedExceptionCount+b.speedExceptionCount,
      mileageExceptionCount=a.mileageExceptionCount+b.mileageExceptionCount,
      voltageExceptionCount=a.voltageExceptionCount+b.voltageExceptionCount,
      currentExceptionCount=a.currentExceptionCount+b.currentExceptionCount,
      socExceptionCount=a.socExceptionCount+b.socExceptionCount,
      longitudeExceptionCount=a.longitudeEmptyCount+b.longitudeEmptyCount,
      latitudeExceptionCount=a.latitudeEmptyCount+b.latitudeEmptyCount,
      exceptionCount=a.exceptionCount+b.exceptionCount,
      nonExceptionCount=a.nonExceptionCount+b.nonExceptionCount,
      okPercent=(validateCount / totalCount.toDouble *100).toInt
    )
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}
// $example off$
import org.apache.spark.sql.SparkSession

object CountVectorizerExample {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("CountVectorizerExample")
      .getOrCreate()

    // $example on$
    val df = spark.createDataFrame(Seq(
      (0, Array("a", "b", "c")),
      (1, Array("a", "b", "b", "c", "a"))
    )).toDF("id", "words")

    // fit a CountVectorizerModel from the corpus
    val cvModel: CountVectorizerModel = new CountVectorizer()
      .setInputCol("words")
      .setOutputCol("features")
      .setVocabSize(3)
      .setMinDF(2)
      .fit(df)

    // alternatively, define CountVectorizerModel with a-priori vocabulary
    val cvm = new CountVectorizerModel(Array("a", "b", "c"))
      .setInputCol("words")
      .setOutputCol("features")

    cvModel.transform(df).show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println


/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import java.io.{BufferedReader, InputStreamReader}
import java.net.Socket
import java.nio.charset.StandardCharsets

import org.apache.spark.SparkConf
import org.apache.spark.internal.Logging
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.receiver.Receiver

/**
 * Custom Receiver that receives data over a socket. Received bytes are interpreted as
 * text and \n delimited lines are considered as records. They are then counted and printed.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.CustomReceiver localhost 9999`
 */
object CustomReceiver {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: CustomReceiver <hostname> <port>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    // Create the context with a 1 second batch size
    val sparkConf = new SparkConf().setAppName("CustomReceiver")
    val ssc = new StreamingContext(sparkConf, Seconds(1))

    // Create an input stream with the custom receiver on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    val lines = ssc.receiverStream(new CustomReceiver(args(0), args(1).toInt))
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}


class CustomReceiver(host: String, port: Int)
  extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) with Logging {

  def onStart() {
    // Start the thread that receives data over a connection
    new Thread("Socket Receiver") {
      override def run() { receive() }
    }.start()
  }

  def onStop() {
   // There is nothing much to do as the thread calling receive()
   // is designed to stop by itself isStopped() returns false
  }

  /** Create a socket connection and receive data until receiver is stopped */
  private def receive() {
   var socket: Socket = null
   var userInput: String = null
   try {
     logInfo("Connecting to " + host + ":" + port)
     socket = new Socket(host, port)
     logInfo("Connected to " + host + ":" + port)
     val reader = new BufferedReader(
       new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8))
     userInput = reader.readLine()
     while(!isStopped && userInput != null) {
       store(userInput)
       userInput = reader.readLine()
     }
     reader.close()
     socket.close()
     logInfo("Stopped receiving")
     restart("Trying to connect again")
   } catch {
     case e: java.net.ConnectException =>
       restart("Error connecting to " + host + ":" + port, e)
     case t: Throwable =>
       restart("Error receiving data", t)
   }
  }
}
// scalastyle:on println
package com.bitnei.report.local

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession


object DataExportTools {

  def main(args: Array[String]): Unit = {


    // TODO: 日志级别设置
    Logger.getLogger("org").setLevel(Level.ERROR)


    // TODO: 参数集合
    //   val stateConf=new StateConf
    //   stateConf.add(args)

    // TODO: 上下文
    val sparkSession = SparkSession
      .builder()
      .appName(this.getClass.getSimpleName)
      .master("local[*]")
      .config("spark.some.config.option", "some-value")
      .config("hadoop.dfs.permissions", "false")
      .getOrCreate()


    val sparkContext = sparkSession.sparkContext
    val sqlContext = sparkSession.sqlContext
    import sqlContext.implicits._

    //    sparkContext.setLogLevel("ERROR")

    //设置日志级别
    //    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
    //    Logger.getLogger("org.apache.spark.sql").setLevel(Level.WARN)

    ////////////////////////////////////////////////////////////////////////////////////////////

    //    sparkSession.read.json("data/dayreport/*.json").groupByKey(_.getAs[String]("dateStr")).mapGroups{
    //      case (dateStr,rps)=>{
    //        (dateStr,rps.length)
    //      }
    //    }.show(10000)


    //    println("LR83VPF6XFB320706".length)
    //    // TODO: 获取需要导出数据的VIN
    //    val vnumDS = sparkSession.read.textFile("data/export-vnum-uid/vins.txt").as[String]
    //      .map(x => {
    //        val cols = x.split("\t")
    //        val v_num = cols(0)
    //        v_num.toString
    //      })
    //      .filter(_.length == 17)
    //
    //    val arr = vnumDS.collect()
    //      .reduce((x, y) => {
    //        "\'" + x.toString + "\'" + "," + "\'" + y.toString + "\'"
    //      })
    //
    //      .foreach(print)


    val startDay = "20160101"

    val endDay = "20171122"


    sparkSession.read.parquet("data/operationIndex/*.parquet").toJSON.show(50,false)


//    //    // TODO: 拼接VID和StartTime StopTime
//    sparkSession.read.textFile("data/export-vnum-uid/uuid_to_number.txt").as[String]
//      .map(x => {
//        x.split(",")(0) + s",starttime:${startDay}000000,endtime:${endDay}235959"
//      }).collect()
//      .foreach(println)


    //    sparkSession.read.textFile("data/mileagecheck/*").as[String]
    //      .map(x => {
    //        val i = x.indexOf("days")
    //
    //        x.substring(i, i + 10)
    //
    //      }).show(false)


    //    sparkSession.sparkContext.makeRDD(arr)
    //      .repartition(1).saveAsTextFile("data/export-vnum-uid/vnums")

    //    MockDataProvider.realInfo(sparkSession)
    //
    //    MockDataProvider.dayReport(sparkSession)
    //
    //    val res01 = sparkSession.sql("select * from realinfo")
    //    val res02 = sparkSession.sql("select * from dayreport")
    //
    //    println(res01.show(false))
    //    println(res02.show(false))


    //   ====================================================

    // TODO: 实时数据
    ////////////////////////////////////text 2 table/////////////////////////////
    //    val realInfoRDD = sparkContext.textFile("data/realinfo/mock.txt")
    //      .map(line => {
    //        val cols = line.split(",")
    //        val map = new java.util.HashMap[String, String]()
    //        for (col <- cols) {
    //          val arr = col.split(":")
    //          var key = arr(0)
    //
    ////          if(key.matches("^\\d*$")){
    ////            key = "N"+key
    ////          }
    //
    //          var value = "#"
    //          if (arr.length > 1) {
    //            value = arr(1)
    //          }
    //          map.put(key, value)
    //        }
    //        val res =  JSONObject.toJSONString(map)
    //        println(res)
    //
    //        res
    //      })
    //
    ////    val realInfoDS = sparkSession.read.json(realInfoRDD)
    //
    ////    realInfoDS.show(false)
    //
    //    realInfoRDD.count()

    //    realInfoDS.createOrReplaceTempView("realinfo")


    ////////////////////////////////////JSon 2 table/////////////////////////////
    //    val realInfoDS = sparkSession.read.json("data/realinfo/mock.txt")
    //
    //    realInfoDS.show(false)


    //   // TODO: 报警数据
    //   val alarmDS =  sparkSession.read.textFile("data/alarm/mock*")

    // TODO: 转发数据
    //   val forwardDS =  sparkSession.read.textFile("data/forward/mock*")
    //
    //   // TODO: 终端数据
    //   val termDS =  sparkSession.read.textFile("data/termstatus/mock*")
    //
    //   // TODO: 登录数据
    //   val loginDS =  sparkSession.read.textFile("data/login/mock*")
    //
    //   // TODO: 单车明细数据
    //   val detailDS =  sparkSession.read.textFile("data/detail/mock*")
    //


    //   // TODO: 单车日报数据
    //    val dayReportDS = sparkSession.read.json("data/dayreport/mock*")

    //    dayReportDS.createOrReplaceTempView("dayreport")


    //    dayReportDS.repartition(1).write.mode(SaveMode.Append).save("data/dayreport/mock")

    //    dayReportDS.collect().map(line => {
    //////      val arr = new scala.collection.mutable.ArrayBuffer[String]
    //      println(line)
    ////
    //    })


    //   dayReportDS.show(false)


    sparkSession.stop()


  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

import java.io.File

import scopt.OptionParser

import org.apache.spark.examples.mllib.AbstractParams
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.util.Utils

/**
 * An example of how to use [[org.apache.spark.sql.DataFrame]] for ML. Run with
 * {{{
 * ./bin/run-example ml.DataFrameExample [options]
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object DataFrameExample {

  case class Params(input: String = "data/mllib/sample_libsvm_data.txt")
    extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("DataFrameExample") {
      head("DataFrameExample: an example app using DataFrame for ML.")
      opt[String]("input")
        .text(s"input path to dataframe")
        .action((x, c) => c.copy(input = x))
      checkConfig { params =>
        success
      }
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val spark = SparkSession
      .builder
      .appName(s"DataFrameExample with $params")
      .getOrCreate()

    // Load input data
    println(s"Loading LIBSVM file with UDT from ${params.input}.")
    val df: DataFrame = spark.read.format("libsvm").load(params.input).cache()
    println("Schema from LIBSVM:")
    df.printSchema()
    println(s"Loaded training data as a DataFrame with ${df.count()} records.")

    // Show statistical summary of labels.
    val labelSummary = df.describe("label")
    labelSummary.show()

    // Convert features column to an RDD of vectors.
    val features = df.select("features").rdd.map { case Row(v: Vector) => v }
    val featureSummary = features.aggregate(new MultivariateOnlineSummarizer())(
      (summary, feat) => summary.add(Vectors.fromML(feat)),
      (sum1, sum2) => sum1.merge(sum2))
    println(s"Selected features column with average values:\n ${featureSummary.mean.toString}")

    // Save the records in a parquet file.
    val tmpDir = Utils.createTempDir()
    val outputDir = new File(tmpDir, "dataframe").toString
    println(s"Saving to $outputDir as Parquet file.")
    df.write.parquet(outputDir)

    // Load the records back.
    println(s"Loading Parquet file with UDT from $outputDir.")
    val newDF = spark.read.parquet(outputDir)
    println(s"Schema from Parquet:")
    newDF.printSchema()

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.common.utils

import com.bitnei.report.common.log.Logging

import scala.math.BigDecimal.RoundingMode

/**
  * Created by wangbaosheng on 2017/3/28.
  */

object DataPrecision extends  Logging {

  def toDayNum(ms:Long):Int= {
    val d = if (ms % (1000 * 3600 * 24) == 0)
      ms / (1000 * 3600 * 24)
    else ms / (1000 * 3600 * 24) +1
    d.toInt
  }

  def toHour(ms: Long): Double = BigDecimal(ms.toDouble / (1000 * 3600)).setScale(2, RoundingMode.HALF_DOWN).toDouble

  def toHour(ms: Double): Double = BigDecimal(ms / (1000 * 3600)).setScale(2, RoundingMode.HALF_DOWN).toDouble
  def toM(ms:Long):Double=BigDecimal(ms.toDouble/(1000*60)).setScale(2,RoundingMode.HALF_DOWN).toDouble

  def secondaryVoltage(v: Int): Double = BigDecimal(v * 0.001).setScale(2, RoundingMode.HALF_DOWN).toDouble

  def totalVoltage(v: Int): Double = BigDecimal(v * 0.1).setScale(1, RoundingMode.HALF_DOWN).toDouble

  def totalCurrent(v: Int): Double = BigDecimal(v * 0.1).setScale(1, RoundingMode.HALF_DOWN).toDouble

  def soc(v: Int): Double = BigDecimal(v).setScale(1, RoundingMode.HALF_DOWN).toDouble

  def charge(v: Double): Double = BigDecimal(v).setScale(1, RoundingMode.HALF_DOWN).toDouble

  def mileage(v: Int): Double = {
    try {
      BigDecimal(v * 0.1).setScale(1, RoundingMode.HALF_DOWN).toDouble
    } catch {
      case e: Exception =>
        logError(s"$v", e)
        throw new Exception(s"$v", e)
    }
  }
  def mileage(v: Double): Double = {
    try {
      BigDecimal(v*0.1).setScale(1, RoundingMode.HALF_DOWN).toDouble
    } catch {
      case e: Exception =>
        logError(s"$v", e)
        throw new Exception(s"$v", e)
    }
  }


  //  def mileage(v:Double):Double=BigDecimal(v).setScale(1,RoundingMode.HALF_DOWN).toDouble
  def latitude(v: Long): Double = BigDecimal(v * 0.000001).setScale(6, RoundingMode.HALF_DOWN).toDouble

  def speed(v:Double):Double=BigDecimal(v).setScale(1, RoundingMode.HALF_DOWN).toDouble
  def speed(v:Int):Double=BigDecimal(v*0.1).setScale(1, RoundingMode.HALF_DOWN).toDouble
  //充电量，单位为千瓦时，也就是度
  def totalCharge(current: Int, voltage: Int, timeDf: Int): Double = -1 * voltage * 0.1 * current * 0.1 * timeDf / 1000 / 3600

  def resistance(v:Int):Double=BigDecimal(v).setScale(1, RoundingMode.HALF_DOWN).toDouble

  def electricMotorVoltage(v:Int)=BigDecimal(v*0.1).setScale(1, RoundingMode.HALF_DOWN).toDouble

  def electricMotorCurrent(v:Int)=BigDecimal(v*0.1).setScale(1, RoundingMode.HALF_DOWN).toDouble

  def temp(v: Int): Int = if(v==Int.MaxValue||v==Int.MinValue) 0 else v

}package com.bitnei.samples

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession


object DataReader {

  def main(args: Array[String]): Unit = {


    // TODO: 日志级别设置
    Logger.getLogger("org").setLevel(Level.ERROR)


    // TODO: 参数集合
    //   val stateConf=new StateConf
    //   stateConf.add(args)

    // TODO: 上下文
    val sparkSession = SparkSession
      .builder()
      .appName(this.getClass.getSimpleName)
      .master("local[*]")
      .config("spark.some.config.option", "some-value")
      .config("hadoop.dfs.permissions", "false")
      .getOrCreate()


    val sparkContext = sparkSession.sparkContext
    val sqlContext = sparkSession.sqlContext

    //    sparkContext.setLogLevel("ERROR")

    //设置日志级别
    //    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
    //    Logger.getLogger("org.apache.spark.sql").setLevel(Level.WARN)

    ////////////////////////////////////////////////////////////////////////////////////////////

//    sparkSession.read.parquet("data/realinfo/*.parquet").toJSON.show(false)
    //    sparkSession.read.json("data/dayreport/*.json").groupByKey(_.getAs[String]("dateStr")).mapGroups{
    //      case (dateStr,rps)=>{
    //        (dateStr,rps.length)
    //      }
    //    }.show(10000)



//    val realInfoDS = sparkSession.read.json("data/dayreport/mock.txt")

//    realInfoDS.createOrReplaceTempView("dayreport")

//    sparkSession.read.parquet("data/dayreport/*.parquet").show(false)

//    sparkSession.read.parquet("data/monthreport/*/*.parquet").toJSON.as[String].rdd.foreach(println)

//    sparkSession.read.parquet("data/deadlineReport/*/*.parquet").show(false)

//    sparkSession.read.parquet("data/realinfo/*.parquet").show(false)

    //    sparkSession.sparkContext.makeRDD(arr)
    //      .repartition(1).saveAsTextFile("data/uid-vnum/vnums")

    //    MockDataProvider.realInfo(sparkSession)
    //
    //    MockDataProvider.dayReport(sparkSession)
    //
    //    val res01 = sparkSession.sql("select * from realinfo")
    //    val res02 = sparkSession.sql("select * from dayreport")
    //
    //    println(res01.show(false))
    //    println(res02.show(false))


    sparkSession.read.json("data/deadlineReport/*.txt")

        .printSchema()


    // TODO: 运营天数
//    val dayReport = sparkSession.read.parquet("data/dayreport/tmp/*.parquet")

//    dayReport.printSchema()

//        .show(false)

//    println(dayReport.count())

//    +------------------------------------+-------+----------+------+
//    |VID                                 |mileage|runtime   |dayNum|
//    +------------------------------------+-------+----------+------+
//    |01873efa-264f-4c5e-8f81-ae38e368e3f3|35853  |1148038000|88    |
//      |1d49f1e0-57a1-4f17-84d6-6b90f6e75edc|3056   |357678000 |88    |
//      |44a848d0-ab68-4a54-ab6b-efbaf0918193|28803  |1492434000|88    |
//      |4caaafe5-1028-43f5-bb82-8afbbab8ce80|36710  |672254000 |82    |
//      |54aabf19-7284-49f5-8669-4dadc401d363|40513  |1837928000|88    |
//      |6e8ea106-9057-4947-9037-20f9679c6fe4|16487  |453588000 |64    |
//      |7bc5bd41-cd33-4f20-862c-20e97a96367e|12530  |270865000 |69    |
//      |84d115b5-3767-4b08-9b01-a7f9d1b72b65|61095  |1349281000|88    |
//      |8badd887-b833-4769-9cc2-92e74e3fdf78|97519  |1547026000|88    |
//      |b37391fa-18e1-4206-a626-1ef9e5a8514a|62750  |1403977000|87    |
//      |bc82eb01-de5b-40ec-9901-aa7d6a0fa79c|34858  |961297000 |88    |
//      |cfef82ec-d44c-401a-82e8-735015623211|44626  |1345997000|82    |
//      |f2d30b11-3e5b-471a-8475-4767b11d31e0|65143  |1833665000|88    |
//      |fe4998cb-b4a3-4e96-9081-6a6a3cfe2178|25051  |763911000 |85    |
//      |0406d960-2253-4181-b848-fb6363c76aad|20820  |472004000 |72    |
//      |261d03fe-4665-40a3-a596-b88abfc5dd57|5820   |136070000 |42    |
//      |27E2ADD6911D7EB1E0533C02A8C0B094    |53270  |1200123000|78    |
//      |27E2ADD697B07EB1E0533C02A8C0B094    |28870  |807971000 |51    |
//      |2fc9a039-aef2-45ba-8c8a-f69451d60031|34114  |689434000 |88    |
//      |3268ede9-5117-487a-8653-673742a4a4f4|73340  |1539292000|88    |
//      +------------------------------------+-------+----------+------+

    //   ====================================================

//
//    def whereBuilder(dateList:Array[String]):String={
//      val filter=dateList.map(date=>{
//        val year=date.substring(0,4)
//        val month=date.substring(4,6)
//        val day=date.substring(6,8)
//        s"(year=$year AND month=$month AND day=$day)"
//      }).reduce((a,b)=>s"$a OR $b")
//
//      val whereFilter=if(filter.trim.nonEmpty) s"WHERE $filter" else ""
//      whereFilter
//    }
//
//
//    val arr = "20170901-20170902-20170903-20170904-20170905-20170906-20170907-20170908-20170909-20170910-20170911-20170912-20170913-20170914-20170915-20170916-20170917-20170918-20170919-20170920-20170921-20170922-20170923-20170924-20170925-20170926-20170927-20170928-20170929-20170930".split("-")

//    WHERE (year=2017 AND month=09 AND day=01) OR (year=2017 AND month=09 AND day=02) OR (year=2017 AND month=09 AND day=03) OR (year=2017 AND month=09 AND day=04) OR (year=2017 AND month=09 AND day=05) OR (year=2017 AND month=09 AND day=06) OR (year=2017 AND month=09 AND day=07) OR (year=2017 AND month=09 AND day=08) OR (year=2017 AND month=09 AND day=09) OR (year=2017 AND month=09 AND day=10) OR (year=2017 AND month=09 AND day=11) OR (year=2017 AND month=09 AND day=12) OR (year=2017 AND month=09 AND day=13) OR (year=2017 AND month=09 AND day=14) OR (year=2017 AND month=09 AND day=15) OR (year=2017 AND month=09 AND day=16) OR (year=2017 AND month=09 AND day=17) OR (year=2017 AND month=09 AND day=18) OR (year=2017 AND month=09 AND day=19) OR (year=2017 AND month=09 AND day=20) OR (year=2017 AND month=09 AND day=21) OR (year=2017 AND month=09 AND day=22) OR (year=2017 AND month=09 AND day=23) OR (year=2017 AND month=09 AND day=24) OR (year=2017 AND month=09 AND day=25) OR (year=2017 AND month=09 AND day=26) OR (year=2017 AND month=09 AND day=27) OR (year=2017 AND month=09 AND day=28) OR (year=2017 AND month=09 AND day=29) OR (year=2017 AND month=09 AND day=30)


//    println(whereBuilder(arr))

    //////////////////////////////////////////////////

    // TODO: 实时数据
    ////////////////////////////////////text 2 table/////////////////////////////
    //    val realInfoRDD = sparkContext.textFile("data/realinfo/mock.txt")
    //      .map(line => {
    //        val cols = line.split(",")
    //        val map = new java.util.HashMap[String, String]()
    //        for (col <- cols) {
    //          val arr = col.split(":")
    //          var key = arr(0)
    //
    ////          if(key.matches("^\\d*$")){
    ////            key = "N"+key
    ////          }
    //
    //          var value = "#"
    //          if (arr.length > 1) {
    //            value = arr(1)
    //          }
    //          map.put(key, value)
    //        }
    //        val res =  JSONObject.toJSONString(map)
    //        println(res)
    //
    //        res
    //      })
    //
    ////    val realInfoDS = sparkSession.read.json(realInfoRDD)
    //
    ////    realInfoDS.show(false)
    //
    //    realInfoRDD.count()

    //    realInfoDS.createOrReplaceTempView("realinfo")


    ////////////////////////////////////JSon 2 table/////////////////////////////
    //    val realInfoDS = sparkSession.read.json("data/realinfo/mock.txt")
    //
    //    realInfoDS.show(false)


    //   // TODO: 报警数据
    //   val alarmDS =  sparkSession.read.textFile("data/alarm/mock*")

    // TODO: 转发数据
    //   val forwardDS =  sparkSession.read.textFile("data/forward/mock*")
    //
    //   // TODO: 终端数据
    //   val termDS =  sparkSession.read.textFile("data/termstatus/mock*")
    //
    //   // TODO: 登录数据
    //   val loginDS =  sparkSession.read.textFile("data/login/mock*")
    //
    //   // TODO: 单车明细数据
    //   val detailDS =  sparkSession.read.textFile("data/detail/mock*")
    //


    //   // TODO: 单车日报数据
    //    val dayReportDS = sparkSession.read.json("data/dayreport/mock*")

    //    dayReportDS.createOrReplaceTempView("dayreport")


    //    dayReportDS.repartition(1).write.mode(SaveMode.Append).save("data/dayreport/mock")

    //    dayReportDS.collect().map(line => {
    //////      val arr = new scala.collection.mutable.ArrayBuffer[String]
    //      println(line)
    ////
    //    })


    //   dayReportDS.show(false)


    sparkSession.stop()


  }
}
package com.bitnei.report.local

import com.bitnei.report.common.configuration.StateConf
import org.apache.spark.sql.{SaveMode, SparkSession}
import org.json4s.native.Json
import org.json4s.DefaultFormats
import org.apache.log4j.{Level, Logger}

import scala.collection.mutable


object DataReaderTest {

  def main(args: Array[String]): Unit = {


    // TODO: 日志级别设置
    Logger.getLogger("org").setLevel(Level.ERROR)


    // TODO: 参数集合
    //   val stateConf=new StateConf
    //   stateConf.add(args)

    // TODO: 上下文
    val sparkSession = SparkSession
      .builder()
      .appName(this.getClass.getSimpleName)
      .master("local[*]")
      .config("spark.some.config.option", "some-value")
      .config("hadoop.dfs.permissions", "false")
      .getOrCreate()


    val sparkContext = sparkSession.sparkContext
    val sqlContext = sparkSession.sqlContext
    import sqlContext.implicits._

    //    sparkContext.setLogLevel("ERROR")

    //设置日志级别
    //    Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
    //    Logger.getLogger("org.apache.spark.sql").setLevel(Level.WARN)

    ////////////////////////////////////////////////////////////////////////////////////////////

    sparkSession.read.parquet("data/realinfo/*.parquet").toJSON.show(false)
    //    sparkSession.read.json("data/dayreport/*.json").groupByKey(_.getAs[String]("dateStr")).mapGroups{
    //      case (dateStr,rps)=>{
    //        (dateStr,rps.length)
    //      }
    //    }.show(10000)



//    val realInfoDS = sparkSession.read.json("data/dayreport/mock.txt")

//    realInfoDS.createOrReplaceTempView("dayreport")

//    sparkSession.read.parquet("data/dayreport/*.parquet").show(false)

//    sparkSession.read.parquet("data/monthreport/*/*.parquet").toJSON.as[String].rdd.foreach(println)

//    sparkSession.read.parquet("data/deadlineReport/*/*.parquet").show(false)

//    sparkSession.read.parquet("data/realinfo/*.parquet").show(false)

    //    sparkSession.sparkContext.makeRDD(arr)
    //      .repartition(1).saveAsTextFile("data/uid-vnum/vnums")

    //    MockDataProvider.realInfo(sparkSession)
    //
    //    MockDataProvider.dayReport(sparkSession)
    //
    //    val res01 = sparkSession.sql("select * from realinfo")
    //    val res02 = sparkSession.sql("select * from dayreport")
    //
    //    println(res01.show(false))
    //    println(res02.show(false))

    // TODO: 运营天数
    val dayReport = sparkSession.read.parquet("data/dayreport/tmp/*.parquet")

//    dayReport.printSchema()

//        .show(false)

//    println(dayReport.count())

//    +------------------------------------+-------+----------+------+
//    |VID                                 |mileage|runtime   |dayNum|
//    +------------------------------------+-------+----------+------+
//    |01873efa-264f-4c5e-8f81-ae38e368e3f3|35853  |1148038000|88    |
//      |1d49f1e0-57a1-4f17-84d6-6b90f6e75edc|3056   |357678000 |88    |
//      |44a848d0-ab68-4a54-ab6b-efbaf0918193|28803  |1492434000|88    |
//      |4caaafe5-1028-43f5-bb82-8afbbab8ce80|36710  |672254000 |82    |
//      |54aabf19-7284-49f5-8669-4dadc401d363|40513  |1837928000|88    |
//      |6e8ea106-9057-4947-9037-20f9679c6fe4|16487  |453588000 |64    |
//      |7bc5bd41-cd33-4f20-862c-20e97a96367e|12530  |270865000 |69    |
//      |84d115b5-3767-4b08-9b01-a7f9d1b72b65|61095  |1349281000|88    |
//      |8badd887-b833-4769-9cc2-92e74e3fdf78|97519  |1547026000|88    |
//      |b37391fa-18e1-4206-a626-1ef9e5a8514a|62750  |1403977000|87    |
//      |bc82eb01-de5b-40ec-9901-aa7d6a0fa79c|34858  |961297000 |88    |
//      |cfef82ec-d44c-401a-82e8-735015623211|44626  |1345997000|82    |
//      |f2d30b11-3e5b-471a-8475-4767b11d31e0|65143  |1833665000|88    |
//      |fe4998cb-b4a3-4e96-9081-6a6a3cfe2178|25051  |763911000 |85    |
//      |0406d960-2253-4181-b848-fb6363c76aad|20820  |472004000 |72    |
//      |261d03fe-4665-40a3-a596-b88abfc5dd57|5820   |136070000 |42    |
//      |27E2ADD6911D7EB1E0533C02A8C0B094    |53270  |1200123000|78    |
//      |27E2ADD697B07EB1E0533C02A8C0B094    |28870  |807971000 |51    |
//      |2fc9a039-aef2-45ba-8c8a-f69451d60031|34114  |689434000 |88    |
//      |3268ede9-5117-487a-8653-673742a4a4f4|73340  |1539292000|88    |
//      +------------------------------------+-------+----------+------+

    //   ====================================================

//
//    def whereBuilder(dateList:Array[String]):String={
//      val filter=dateList.map(date=>{
//        val year=date.substring(0,4)
//        val month=date.substring(4,6)
//        val day=date.substring(6,8)
//        s"(year=$year AND month=$month AND day=$day)"
//      }).reduce((a,b)=>s"$a OR $b")
//
//      val whereFilter=if(filter.trim.nonEmpty) s"WHERE $filter" else ""
//      whereFilter
//    }
//
//
//    val arr = "20170901-20170902-20170903-20170904-20170905-20170906-20170907-20170908-20170909-20170910-20170911-20170912-20170913-20170914-20170915-20170916-20170917-20170918-20170919-20170920-20170921-20170922-20170923-20170924-20170925-20170926-20170927-20170928-20170929-20170930".split("-")

//    WHERE (year=2017 AND month=09 AND day=01) OR (year=2017 AND month=09 AND day=02) OR (year=2017 AND month=09 AND day=03) OR (year=2017 AND month=09 AND day=04) OR (year=2017 AND month=09 AND day=05) OR (year=2017 AND month=09 AND day=06) OR (year=2017 AND month=09 AND day=07) OR (year=2017 AND month=09 AND day=08) OR (year=2017 AND month=09 AND day=09) OR (year=2017 AND month=09 AND day=10) OR (year=2017 AND month=09 AND day=11) OR (year=2017 AND month=09 AND day=12) OR (year=2017 AND month=09 AND day=13) OR (year=2017 AND month=09 AND day=14) OR (year=2017 AND month=09 AND day=15) OR (year=2017 AND month=09 AND day=16) OR (year=2017 AND month=09 AND day=17) OR (year=2017 AND month=09 AND day=18) OR (year=2017 AND month=09 AND day=19) OR (year=2017 AND month=09 AND day=20) OR (year=2017 AND month=09 AND day=21) OR (year=2017 AND month=09 AND day=22) OR (year=2017 AND month=09 AND day=23) OR (year=2017 AND month=09 AND day=24) OR (year=2017 AND month=09 AND day=25) OR (year=2017 AND month=09 AND day=26) OR (year=2017 AND month=09 AND day=27) OR (year=2017 AND month=09 AND day=28) OR (year=2017 AND month=09 AND day=29) OR (year=2017 AND month=09 AND day=30)


//    println(whereBuilder(arr))

    //////////////////////////////////////////////////

    // TODO: 实时数据
    ////////////////////////////////////text 2 table/////////////////////////////
    //    val realInfoRDD = sparkContext.textFile("data/realinfo/mock.txt")
    //      .map(line => {
    //        val cols = line.split(",")
    //        val map = new java.util.HashMap[String, String]()
    //        for (col <- cols) {
    //          val arr = col.split(":")
    //          var key = arr(0)
    //
    ////          if(key.matches("^\\d*$")){
    ////            key = "N"+key
    ////          }
    //
    //          var value = "#"
    //          if (arr.length > 1) {
    //            value = arr(1)
    //          }
    //          map.put(key, value)
    //        }
    //        val res =  JSONObject.toJSONString(map)
    //        println(res)
    //
    //        res
    //      })
    //
    ////    val realInfoDS = sparkSession.read.json(realInfoRDD)
    //
    ////    realInfoDS.show(false)
    //
    //    realInfoRDD.count()

    //    realInfoDS.createOrReplaceTempView("realinfo")


    ////////////////////////////////////JSon 2 table/////////////////////////////
    //    val realInfoDS = sparkSession.read.json("data/realinfo/mock.txt")
    //
    //    realInfoDS.show(false)


    //   // TODO: 报警数据
    //   val alarmDS =  sparkSession.read.textFile("data/alarm/mock*")

    // TODO: 转发数据
    //   val forwardDS =  sparkSession.read.textFile("data/forward/mock*")
    //
    //   // TODO: 终端数据
    //   val termDS =  sparkSession.read.textFile("data/termstatus/mock*")
    //
    //   // TODO: 登录数据
    //   val loginDS =  sparkSession.read.textFile("data/login/mock*")
    //
    //   // TODO: 单车明细数据
    //   val detailDS =  sparkSession.read.textFile("data/detail/mock*")
    //


    //   // TODO: 单车日报数据
    //    val dayReportDS = sparkSession.read.json("data/dayreport/mock*")

    //    dayReportDS.createOrReplaceTempView("dayreport")


    //    dayReportDS.repartition(1).write.mode(SaveMode.Append).save("data/dayreport/mock")

    //    dayReportDS.collect().map(line => {
    //////      val arr = new scala.collection.mutable.ArrayBuffer[String]
    //      println(line)
    ////
    //    })


    //   dayReportDS.show(false)


    sparkSession.stop()


  }
}
package com.bitnei.report.local

/**
  *
  * @author zhangyongtian
  * @define
  *
  * date: 2017-11-09
  *
  */


object DataSchema {

  case class RealInfo(

                       VID: String,
                       VIN: String,
                       TIME: String,
                       MESSAGETYPE: String,
                       N2914: String,
                       ISFILTER: String,
                       VTYPE: String,
                       N2915: String,
                       N2912: String,
                       N2608: String,
                       N2913: String,
                       N2609: String,
                       N2918: String,
                       N2606: String,
                       N2919: String,
                       N2607: String,
                       N2916: String,
                       N2002: String,
                       N2604: String,
                       N2917: String,
                       N2003: String,
                       N2605: String,
                       N2602: String,
                       N2603: String,
                       N2601: String,
                       N2910: String,
                       N2911: String,
                       N2103: String,
                       N2205: String,
                       N2101: String,
                       N2204: String,
                       N2102: String,
                       N2203: String,
                       N2503: String,
                       N2202: String,
                       N2502: String,
                       N2201: String,
                       N2501: String,
                       N3201: String,
                       N2901: String,
                       N2902: String,
                       N2903: String,
                       N2904: String,
                       N2905: String,
                       N2615: String,
                       N2906: String,
                       N2907: String,
                       N2617: String,
                       N2611: String,
                       N2612: String,
                       N2613: String,
                       N2808: String,
                       N2614: String,
                       N2809: String,
                       N2804: String,
                       N2610: String,
                       N2805: String,
                       N10002: String,
                       N10005: String,
                       N10003: String,
                       N10004: String,
                       N2909: String,
                       N2214: String,
                       N2213: String,
                       N7003: String,
                       N3801: String,
                       N7101: String,
                       N7103: String,
                       N7102: String,
                       N7001: String,
                       N7002: String,
                       N2930: String,
                       N2923: String,
                       N2307: String,
                       N2924: String,
                       N2308: String,
                       N2920: String,
                       N2921: String,
                       N2922: String,
                       N2301: String,
                       N2001: String,
                       N2000: String
                     )

  case class DayReport(
                        vid: String,
                        category: String,
                        reportDate: String,
                        onlineTime: String,
                        timeLeng: Long,
                        accRunTime: Int,
                        maxTime: Long,
                        minTime: Long,
                        startMileage: Int,
                        stopMileage: Int,
                        maxMileage: Int,
                        totalMileage: Int,
                        gpsMileage: Int,
                        maxSpeed: Int,
                        maxTotalVoltage: Int,
                        minTotalVoltage: Int,
                        maxTotalEctriccurrent: Int,
                        minTotalEctriccurrent: Int,
                        maxSecondaryVolatage: Int,
                        minSecondaryVolatage: Int,
                        maxAcquisitionPointTemp: Int,
                        minAcquisitionPointTemp: Int,
                        maxEngineTemp: Int,
                        minEngineTemp: Int,
                        maxSoc: Int,
                        minSoc: Int,
                        startSoc: Int,
                        totalCharge: Double,
                        maxCharge: Double,
                        times: Int,
                        chargeTime: Long,
                        fullChargeTime: Long,
                        chargePer100Km: Double,
                        chargeSocDistribution: Array[Int],
                        chargeTimeRangeDistribution: Array[Int],
                        runTimeRangeDistribution: Array[Int],
                        runTimeLengthDistribution: Array[Int],
                        runMileageDistribution: Array[Int],
                        chargeTimeLengthDistribution: Array[Int]

                      )

}package com.bitnei.report

import java.util.Date

import com.bitnei.report.common.GpsDistance
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.sql.{DataFrame, Dataset, SaveMode, SparkSession}

import scala.collection.mutable.ArrayBuffer


case class RealinfoInput(vid: String, vin:String,time: String,longitude:Double, latitude: Double)
case class MonthCoord(vid: String,
                       vin:String,
                       mielage: Double,
                       hours: Double,
                       coords: Array[Array[Array[Double]]])

case class JsonMonthCoord(mielage: Double,
                          hours: Double,
                          coords: Array[Array[Array[Double]]])
case class RealinfoCoord(vid: String, vin:String,coords: Array[Array[Array[Double]]])
case class DayReport(vid: String, mileage: Double, runtime: Int)


/*
* created by wangbaosheng on 2017/12/25
* 计算有效日轨迹，有效日里程。
*
*/
class DayCoordJob(stateConf:StateConf, @transient sparkSession: SparkSession) extends Serializable with Logging with Job{
  override type R = MonthCoord

  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)

  import  sparkSession.implicits._

  //两点间最小距离，单位米
  private val minL = stateConf.getOption("minLm").map(_.toInt).getOrElse(50)

  //两点间最大距离，单位米
  private val maxL = stateConf.getOption("maxLm").map(_.toInt).getOrElse(500)

  require(minL < maxL)

  //轨迹窗口最小轨迹点个数
  private val minWindowLength = stateConf.getOption("minWindowLength").map(_.toInt).getOrElse(3)

  require(minWindowLength >= 1)

  logInfo(s"minL=$minL M,maxL=$maxL M,minWindowLength=$minWindowLength")


  //日最大里程阈值
  val dayMileageMaxThreshold=stateConf.getInt("dayMileageMaxThreshold",800)
 //日最小里程阈值
  val dayMileageMinThreshold=stateConf.getInt("dayMileageMinThreshold",1)


  //输出表
  private val mileageTableName = stateConf.getOption("mileagecheck.table.name").getOrElse("mileage_check_coords")

  private val dayCoordTable=stateConf.getOption("dayCoord.table.name").getOrElse("mileageCheckDayCoord")


  override def registerIfNeed() = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, "realinfo")
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, "dayreport")
  }


  override def unRegister() = {
    sparkSession.catalog.dropTempView("realinfo")
    sparkSession.catalog.dropTempView("dayreport")
  }

  override def doCompute[Product <: MonthCoord](): Dataset[MonthCoord] = {
    //获取要计算的日期
    val date = Utils.parsetDate(stateConf.getString("date"), "yyyyMMdd") match {
      case Some(d) => d
      case _ => throw new Exception("date parse error")
    }

    val year = Utils.formatDate(date, "yyyy")
    val month = Utils.formatDate(date, "MM")
    val day = Utils.formatDate(date, "dd")

    SparkHelper.setPartitionValue(stateConf,dayCoordTable,Array(year,month,day))
    //计算有效日里程,
    val dayReportDs = computeDayMileage(date)
    //计算有效日轨迹
    val dayCoordDs = computeDayCoord(date)
    //生成有效数据：日里程大于0小于800，并且轨迹非空
    val validateDayCoords = dayReportDs.joinWith(dayCoordDs, dayReportDs("vid").equalTo(dayCoordDs("vid")), "inner")
      .map({ case (dayreport, coord) =>
        MonthCoord(
          vid = coord.vid,
          vin = coord.vin,
          //里程
          mielage = dayreport.mileage,
          //时长
          hours = DataPrecision.toHour(dayreport.runtime),
          //日轨迹
          coords = coord.coords)
      }).filter(v => v.mielage >= dayMileageMinThreshold && v.mielage <= dayMileageMaxThreshold && v.coords.nonEmpty)

    validateDayCoords
  }


  override def write[Product <: MonthCoord](result: Dataset[MonthCoord]): Unit = {
    //输出到hdfs和数据库
    val date=Utils.parsetDate(stateConf.getString("date"),"yyyyMMdd") match {
      case Some(d)=>d
      case _=> throw new Exception("date parse error")
    }

    val outputModels = stateConf.getOption("report.output").getOrElse("hdfs").split(',')
    if (outputModels.length > 1) result.cache()
    outputModels.foreach(outputModel => {
      if (outputModel == "hdfs") {
        SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), dayCoordTable)
      } else if (outputModel == "hbase") {
        (stateConf.getOption("hbase.quorum"), stateConf.getOption("hbase.zkport")) match {
          case (Some(quorm), Some(zkport)) =>
            logInfo(s"writting to hbase,$quorm:$zkport,$mileageTableName")
            writeToHbase(result, date, quorm, zkport)
          case _ =>
            throw new RuntimeException("hbase.quorum or hbase.zkport not exists.")
        }
      }
    })
  }


   //日日轨迹数据
  def computeDayCoord(startDate: Date): Dataset[RealinfoCoord] = {
    val date=Utils.parsetDate(stateConf.getString("date"),"yyyyMMdd") match {
      case Some(d)=>d
      case _=> throw new Exception("date parse error")
    }

    val year=Utils.formatDate(date,"yyyy")
    val month=Utils.formatDate(date,"MM")
    val day=Utils.formatDate(date,"dd")

    def compute(): DataFrame = {
      logInfo("no cache for coord,so compute it....")
      val sql =
        s"""
             SELECT
               VID,
               vin,
               TIME,
               CAST(`2502`*0.000001 AS DOUBLE) AS longitude,
               CAST(`2503`*0.000001 AS DOUBLE) AS latitude
               FROM realinfo
             WHERE year=$year AND month=$month AND day=$day AND `2502` IS NOT NULL AND `2503` IS NOT NULL
           """.stripMargin

      val result = sparkSession.sql(sql)
        .as[RealinfoInput]
        .groupByKey(_.vid)
        .mapGroups({ case (vid, valuesIterator: Iterator[RealinfoInput]) =>
          val values = valuesIterator.toArray
          //按照时间对轨迹排序
          val sortedValues = Utils.sortByDate[RealinfoInput](values, x => Some(x.time))
          //计算某一辆车的有效轨迹
          val validateCoords = computeOneVehicle(sortedValues)

          val v = RealinfoCoord(vid, sortedValues.head.vin, validateCoords)


          values.dropWhile(x => 1 == 1)
          sortedValues.dropWhile((x => 1 == 1))
          v
        }).filter(_.coords.nonEmpty)
      result.toDF()
    }

    compute.as[RealinfoCoord]
  }

  //计算日里程
  def computeDayMileage(startDate: Date): Dataset[DayReport] = {
    val date=Utils.parsetDate(stateConf.getString("date"),"yyyyMMdd") match {
      case Some(d)=>d
      case _=> throw new Exception("date parse error")
    }

    val year=Utils.formatDate(date,"yyyy")
    val month=Utils.formatDate(date,"MM")
    val day=Utils.formatDate(date,"dd")


    logInfo("no cache for dayreport,so compute it....")
    val sql =
      s"""
            SELECT
              VID,
              CAST(totalMileage*0.1 AS DOUBLE) AS  mileage,
              CAST(timeLeng AS INT)  AS runtime
            FROM dayreport
            WHERE year=$year AND month=$month AND day=$day
             AND category = '${Constant.TravelState}'
             AND startMileage!=0 and stopMileage!=2147483647
             AND totalMileage*0.1 between $dayMileageMinThreshold AND $dayMileageMaxThreshold
          """.stripMargin
    sparkSession.sql(sql).as[DayReport]
  }


  /***
    * 计算日轨迹
    * @param sortedValues 按照时间排序的轨迹序列
    * @return 有效轨迹
    ***/
  def computeOneVehicle(sortedValues: Array[RealinfoInput]): Array[Array[Array[Double]]] = {
    val coordWindows = new PointSplit[RealinfoInput](minL, maxL, minWindowLength).split(sortedValues)(L)

    coordWindows.map(coordWindow => {
      val result = new ArrayBuffer[Array[Double]]()
      coordWindow.foreach(v => result.append(Array(v.longitude, v.latitude)))
      result.toArray
    })
  }


  //计算两点之间GPS距离
  def L(a: RealinfoInput, b: RealinfoInput): Double = {
    GpsDistance.getDistance(a.longitude, a.latitude, b.longitude, b.latitude)
  }


  def writeToHbase(result: Dataset[MonthCoord], date: Date, quorum: String, zkport: String) = {
    result.foreachPartition(values => {
      val day = Utils.formatDate(date, "yyyyMMdd")
      HbaseHelper.bulkPut(quorum, zkport, mileageTableName, (table) => {
        val mapper = new ObjectMapper()
        mapper.registerModule(DefaultScalaModule)

        values.foreach(v => {
          val rowKey = s"${v.vid}_d_${day}"
          val jsonMonthCoord=new JsonMonthCoord(mielage = v.mielage,hours = v.hours,coords = v.coords)
          val jsonValue = toJson(mapper, jsonMonthCoord)
          table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "coord", jsonValue))
        })
      })
    })
  }

  //将计算结果转换为json
  def toJson(mapper: ObjectMapper, locus: JsonMonthCoord): String = {
    val jsonValue = mapper.writeValueAsString(locus)
    jsonValue
  }

  def toJson(locuses: Array[JsonMonthCoord]): Array[String] = {
    val mapper = new ObjectMapper()
    mapper.registerModule(DefaultScalaModule)
    locuses.map(mapper.writeValueAsString(_))
  }
}

object DayCoordJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)
    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)
    new DayCoordJob(stateConf, sparkSession).compute()
  }
}
package com.bitnei.report

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.sql.{Dataset, SparkSession}

/*
* created by wangbaosheng on 2017/12/28
* 日轨迹输出作业，用于将日轨迹输出到hbase。方便补写某一天的数据
*/
class DayCoordOutputJob(stateConf:StateConf, @transient sparkSession: SparkSession) extends Serializable with Logging with Job {
  private val monthDate = stateConf.getString("monthDate")
  private val year = Utils.formatDate("yyyyMM", "yyyy", monthDate)
  private val month = Utils.formatDate("yyyyMM", "MM", monthDate)
  logInfo(s"monthDate=$monthDate")


  override type R = MonthCoord

  if (monthDate == Utils.formatDate(new Date, "yyyyMM")) {
    throw new IllegalArgumentException(s"the monthDate must be not equal current month:monthDate=$monthDate")
  }


  private val mileageTableName = stateConf.getOption("mileagecheck.table.name").getOrElse("mileage_check_coords")
  private val dayCoordTable = stateConf.getOption("dayCoord.table.name").getOrElse("mileageCheckDayCoord")
  private val deadlineTable = stateConf.getOption("deadline.table.name").getOrElse("mileageCheckDeadline")

  import sparkSession.sqlContext.implicits._

  override def registerIfNeed() = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, "dayreport")
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, dayCoordTable)
  }


  override def unRegister() = {
    sparkSession.catalog.dropTempView("dayreport")
    sparkSession.catalog.dropTempView(dayCoordTable)
  }

  override def doCompute[Product <: R](): Dataset[R] = {
    val date = Utils.parsetDate(stateConf.getString("date"), "yyyyMMdd") match {
      case Some(d) => d
      case _ => throw new Exception("date parse error")
    }

    val year = Utils.formatDate(date, "yyyy")
    val month = Utils.formatDate(date, "MM")
    val day = Utils.formatDate(date, "dd")

    sparkSession.sql(
      s"""
      SELECT
      vid,
      vin,
      CAST(mielage AS DOUBLE) AS mielage,
      hours,
      coords
      FROM $dayCoordTable AS  daycoord
      WHERE year=$year AND month=$month and day=$day
    """.stripMargin).as[MonthCoord]
  }

  override def write[Product <: MonthCoord](result: Dataset[MonthCoord]): Unit = {
    (stateConf.getOption("hbase.quorum"), stateConf.getOption("hbase.zkport")) match {
      case (Some(quorm), Some(zkport)) =>
        val date = Utils.parsetDate(stateConf.getString("date"), "yyyyMMdd") match {
          case Some(d) => d
          case _ => throw new Exception("date parse error")
        }
        logInfo(s"writting to hbase,$quorm:$zkport,$mileageTableName")
        writeToHbase(result, date, quorm, zkport)
      case _ =>
        throw new RuntimeException("hbase.quorum or hbase.zkport not exists.")
    }

    def writeToHbase(result: Dataset[MonthCoord], date: Date, quorum: String, zkport: String) = {
      result.foreachPartition(values => {
        val day = Utils.formatDate(date, "yyyyMMdd")
        HbaseHelper.bulkPut(quorum, zkport, mileageTableName, (table) => {
          val mapper = new ObjectMapper()
          mapper.registerModule(DefaultScalaModule)

          values.foreach(v => {
            val rowKey = s"${v.vid}_d_${day}"
            val jsonMonthCoord = new JsonMonthCoord(mielage = v.mielage, hours = v.hours, coords = v.coords)
            val jsonValue = toJson(mapper, jsonMonthCoord)
            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "coord", jsonValue))
          })
        })
      })
    }

    def toJson(mapper: ObjectMapper, locus: JsonMonthCoord): String = {
      val jsonValue = mapper.writeValueAsString(locus)
      jsonValue
    }
  }
}

object DayCoordOutputJob{
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)
    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)
    new DayCoordOutputJob(stateConf, sparkSession).compute()
  }
}package com.bitnei.report.year

import java.text.SimpleDateFormat

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 根据realinfo报文按天统计工作日车辆以下信息
  * 工作日：指法定上班日期；
  *
  * 早高峰：早上7:00-9:00；（4）晚高峰：下午5:00-7:00；
  *
  * 车牌  	总行驶里程	总行驶时长	总充电时长	总上线天数	平均车速	总充电量	早高峰平均车速	晚高峰平均车速
  *
  * 充电状态判断：电流 负数 车速 <=0.5 m
  *
  * create 2018-01-03 16:05
  *
  */
object DayReport extends Serializable with Logging {


  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var date = "20171102"

    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
    }


    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    val sc = sparkSession.sparkContext
    import sparkSession.implicits._

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////


    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")
      }

      case "dev" => {
        //研发环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }
      ///tmp/zyt/data/realinfo

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }


    /////////////////////////////////业务逻辑//////////////////////////////////////////////

    val sql =
      s"""
         SELECT
           VID,
         `2000` AS TIME,
         `2202` AS Mileage,
         `2201` As speed,
         `2613` As dy,
         `2614` As dl
         FROM realinfo
         where VID is not null
         and `2000` like '${date}%'
         and `2202` is not null
         and `2201` is not null
         and `2613` is not null
         and `2614` is not null
      """.stripMargin
    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String)]


    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        val vid = x._1
        val time = x._2
        val mileage = x._3.toDouble

        val cond01 = mileage >= 0 && mileage <= 9999999

        val speed = x._4.toDouble
        val cond02 = speed >= 0 && speed <= 2200

        val dy = x._5.toDouble
        val cond03 = dy >= 0 && dy <= 10000

        //parquet数据已经减去了偏移量
        val dl = x._6.toDouble
        //[-10000,10000]
        val cond04 = dl >= -10000 && dl <= 10000

        cond01 && cond02 && cond03 && cond04
      })

    // TODO: 提取
    val mappedDS = filteredDS
      .map(x => {
        //单位换算
        val vid = x._1.toString
        val timeStr = x._2.toString
        val mileage = x._3.toDouble / 10 //km
        val speed = x._4.toDouble / 10 //km/h
        val dy = x._5.toDouble / 10 //V

        //        println(dy)
        //parquet数据已经减去了偏移量
        val dl = (x._6.toDouble / 10) //A

        val sdf = new SimpleDateFormat("yyyyMMDDHHmmss")
        val timeStamp = sdf.parse(timeStr).getTime

//        println(dy)
//         println(dl/1000 >0.8 )
//        println(dy * dl / 1000)

        Input(vid, timeStr, mileage, speed, dy, dl, timeStamp)
      })


    val result = mappedDS.groupByKey(_.vid)
      .mapGroups {
        case (vid, inputs: Iterator[Input]) => {

          val arr = inputs.toArray.sortBy(_.time).distinct
          //TODO 	总行驶里程
          var totalMilage = arr.last.mileage - arr.head.mileage
          if (totalMilage < 0) totalMilage = 0;

          //TODO 早高峰行驶里程
          val morningPeakArr = arr.filter(x => {
            //            20171102114428
            isMorningPeak(x.time)
          }).sortBy(_.time)


          var morningPeakMileage = 0D;

          if (morningPeakArr.length > 0) {
            morningPeakMileage = morningPeakArr.last.mileage - morningPeakArr.head.mileage
            if (morningPeakMileage < 0) morningPeakMileage = 0;
          }

          //TODO 晚高峰行驶里程	早高峰平均车速	晚高峰平均车速  总上线天数
          val nightPeakArr = arr.filter(x => {
            isnightPeak(x.time)
          }).sortBy(_.time)

          var nightPeakMileage = 0D;

          if (nightPeakArr.length > 0) {
            nightPeakMileage = nightPeakArr.last.mileage - nightPeakArr.head.mileage
            if (nightPeakMileage < 0) nightPeakMileage = 0;
          }

          //TODO 总行驶时长
          var total_run_duration = 0D;
          val runlstArr = new ArrayBuffer[ArrayBuffer[Input]]()

          var runArr = new ArrayBuffer[Input]()

          for (i <- 0 until arr.length) {
            if (getRunRuler(arr(i))) {
              runArr.append(arr(i))
              if (i == (arr.length - 1)) {
                runlstArr.append(runArr)
              }
            } else {
              if (runArr.length > 0) {
                runlstArr.append(runArr)
                runArr = new ArrayBuffer[Input]()
              }
            }
          }


          // TODO: 早高峰行驶时长
          var morningPeakRunDuration = 0D


          // TODO: 晚高峰行驶时长
          var nightPeakRunDuration = 0D


          runlstArr.filter(_.length > 1).foreach(e => {
            val sortedArr = e.sortBy(_.time)

            val time_len = sortedArr.last.timeStamp - sortedArr.head.timeStamp + 10 * 1000
            total_run_duration = total_run_duration + time_len.toDouble


            val morningPeakRunArr = sortedArr.filter(x => isMorningPeak(x.time))

            if (morningPeakRunArr.length > 0) {
              val morningPeakRun_time_len = morningPeakRunArr.last.timeStamp - morningPeakRunArr.head.timeStamp + 10 * 1000
              morningPeakRunDuration = morningPeakRunDuration + morningPeakRun_time_len.toDouble
            }

            val nightPeakRunArr = sortedArr.filter(x => isnightPeak(x.time))
            if (nightPeakRunArr.length > 0) {
              val nightPeakRun_time_len = nightPeakRunArr.last.timeStamp - nightPeakRunArr.head.timeStamp + 10 * 1000
              nightPeakRunDuration = nightPeakRunDuration + nightPeakRun_time_len.toDouble
            }


          })

          // TODO: 早高峰平均车速 km/h
          var morningAvgSpeed = 0D
          if (morningPeakRunDuration != 0) {
            morningAvgSpeed = morningPeakMileage / (morningPeakRunDuration / (60 * 60 * 1000))
          }


          // TODO: 晚高峰平均车速
          var nightAvgSpeed = 0D
          if (nightPeakRunDuration != 0) {
            nightAvgSpeed = nightPeakMileage / (nightPeakRunDuration / (60 * 60 * 1000))
          }


          //TODO 平均车速 km/h
          var avg_speed = 0D
          if (total_run_duration == 0) totalMilage = 0;
          if (total_run_duration != 0) {
            avg_speed = totalMilage / (total_run_duration / (60 * 60 * 1000))
          }

          //TODO 	总充电时长
          val chargelstArr = new ArrayBuffer[ArrayBuffer[Input]]()

          var chargeArr = new ArrayBuffer[Input]()

          for (i <- 0 until arr.length) {
            if (getChargeRuler(arr(i))) {
              chargeArr.append(arr(i))
              if (i == (arr.length - 1)) {
                chargelstArr.append(chargeArr)
              }
            } else {
              if (chargeArr.length > 0) {
                chargelstArr.append(chargeArr)
                chargeArr = new ArrayBuffer[Input]()
              }
            }
          }

          //TODO 总充电量
          var total_charge_num = 0D

          var total_charge_duration = 0D

          //连续超过10帧
          chargelstArr.filter(_.length >= 10).foreach(e => {
            val sortedArr = e.sortBy(_.time)

            val time_len = sortedArr.last.timeStamp - sortedArr.head.timeStamp + 10 * 1000
            total_charge_duration = total_charge_duration + time_len

            for (i <- 1 until sortedArr.length) {
              val timeDiff = sortedArr(i).timeStamp - sortedArr(i - 1).timeStamp
              //单帧电量
              val charge_num = sortedArr(i).dy * sortedArr(i).dl / 1000 * (timeDiff.toDouble / (60 * 60 * 1000)) //kw.h

              total_charge_num = total_charge_num + Math.abs(charge_num)
            }

          })

          //车牌  	总行驶里程	总行驶时长	总充电时长	总上线天数	平均车速	总充电量	早高峰平均车速	晚高峰平均车速
          if (totalMilage == 0D) total_run_duration = 0D;
          if (morningPeakMileage == 0D) morningPeakRunDuration = 0D;
          if (nightPeakMileage == 0D) nightPeakRunDuration = 0D;
//          if (total_charge_num > 100D) total_charge_num = 100D;

          Output(vid,
            date,
            totalMilage.formatted("%.4f"),
            (total_run_duration / (60 * 60 * 1000)).formatted("%.4f"),
            avg_speed.formatted("%.4f"),
            (total_charge_duration / (60 * 60 * 1000)).formatted("%.4f"),
            total_charge_num.formatted("%.4f"),
            morningPeakMileage.formatted("%.4f"),
            (morningPeakRunDuration / (60 * 60 * 1000)).formatted("%.4f"),
            morningAvgSpeed.formatted("%.4f"),
            nightPeakMileage.formatted("%.4f"),
            (nightPeakRunDuration / (60 * 60 * 1000)).formatted("%.4f"),
            nightAvgSpeed.formatted("%.4f"))
        }
      }
      .filter(x => {
        val cond01 = x.avg_speed.toDouble > 0 && x.avg_speed.toDouble < 120
        val cond02 = x.totalMilage.toDouble >= 2 && x.totalMilage.toDouble <= 800
        val cond03 = x.run_duration.toDouble > 0 && x.run_duration.toDouble < 20

        cond01 && cond02 && cond03
      })



    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")

    // TODO: 输出
    if (env.equals("local")) {

      result.show(false)
      //      parkingInfoDS.show(false)
      //      result.show(100, false)

      //      result.count()

      //      result.count()
      //    result.printSchema()
      //    result.show(false)
      //


    }

    // TODO: 输出到
    if (env.equals("dev")) {
      result.repartition(1).write.json(s"/spark/vehicle/result/yearReport/day/year=${year}/month=${month}/day=${day}")
    }

    if (env.equals("prd")) {
    }

    logInfo("任务完成...")

    sparkSession.stop()
  }

  def isMorningPeak(time: String): Boolean = {
    val clock = time.substring(8, 10).toInt
    clock >= 7 && clock <= 9
  }

  def isnightPeak(time: String): Boolean = {
    val clock = time.substring(8, 10).toInt
    clock >= 17 && clock <= 19
  }


  def getChargeRuler(input: Input): Boolean = {
    input.dl < 0 && (input.speed * 1000) <= 0.5
    //    && (input.dy > 0)
  }

  def getRunRuler(input: Input): Boolean = {
    input.speed * 1000 > 0.5 && input.dl > 0
    //    !getChargeRuler(input)
  }


  case class Input(vid: String, time: String, mileage: Double, speed: Double, dy: Double, dl: Double, timeStamp: Long)


  case class Output(vid: String, date: String, totalMilage: String, run_duration: String, avg_speed: String, charge_duration: String, total_charge_num: String, morningPeakMileage: String, morningPeakRunDuration: String, morningAvgSpeed: String, nightPeakMileage: String, nightPeakRunDuration: String, nightAvgSpeed: String)

}

//.formatted("%.3f")package com.bitnei.report.year

import java.text.SimpleDateFormat

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.sparkhelper.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 根据realinfo报文按天统计工作日车辆以下信息
  * 工作日：指法定上班日期；
  *
  * 早高峰：早上7:00-9:00；（4）晚高峰：下午5:00-7:00；
  *
  * 车牌  	总行驶里程	总行驶时长	总充电时长	总上线天数	平均车速	总充电量	早高峰平均车速	晚高峰平均车速
  *
  * 充电状态判断：电流 负数 车速 <=0.5 m
  *
  * create 2018-01-03 16:05
  *
  */
object DayReportFromLzo extends Serializable with Logging {


  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var date = "20171102"

    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
    }


    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    val sc = sparkSession.sparkContext
    import sparkSession.implicits._

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////


    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")

      }

      case "dev" => {
        //研发环境

        val initRDD = SparkHelper.readHadoopTextLzoFile(sc, s"/vehicle/data/realinfo/${year}/${month}/${day}/*.lzo")

        initRDD.map(x=>{
          val arr = x.split(",");

          arr.map(m=>{
            val kv = m.split(":");
            val key = kv(0);
            val value = kv(1);

          })

        })

        initRDD.take(10).foreach(println)

        return

        sparkSession.read.json(initRDD).createOrReplaceTempView("realinfo")

      }
      ///tmp/zyt/data/realinfo

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }


    /////////////////////////////////业务逻辑//////////////////////////////////////////////

    val sql =
      s"""
         SELECT
           VID,
         `2000` AS TIME,
         `2202` AS Mileage,
         `2201` As speed,
         `2613` As dy,
         `2614` As dl
         FROM realinfo
         where VID is not null
         and `2000` like '${date}%'
         and `2202` is not null
         and `2201` is not null
         and `2613` is not null
         and `2614` is not null
      """.stripMargin
    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String)]


    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        val vid = x._1
        val time = x._2
        val mileage = x._3.toDouble

        val cond01 = mileage >= 0 && mileage <= 9999999

        val speed = x._4.toDouble
        val cond02 = speed >= 0 && speed <= 2200

        val dy = x._5.toDouble
        val cond03 = dy >= 0 && dy <= 10000

        //parquet数据已经减去了偏移量
        val dl = x._6.toDouble
        //[-10000,10000]
        val cond04 = dl >= -10000 && dl <= 10000

        cond01 && cond02 && cond03 && cond04
      })

    // TODO: 提取
    val mappedDS = filteredDS
      .map(x => {
        //单位换算
        val vid = x._1.toString
        val timeStr = x._2.toString
        val mileage = x._3.toDouble / 10 //km
        val speed = x._4.toDouble / 10 //km/h
        val dy = x._5.toDouble / 10 //V

        val dl = (x._6.toDouble / 10) //A

        val sdf = new SimpleDateFormat("yyyyMMDDHHmmss")
        val timeStamp = sdf.parse(timeStr).getTime

        Input(vid, timeStr, mileage, speed, dy, dl, timeStamp)
      })


    val result = mappedDS.groupByKey(_.vid)
      .mapGroups {
        case (vid, inputs: Iterator[Input]) => {

          val arr = inputs.toArray.sortBy(_.time).distinct
          //TODO 	总行驶里程
          var totalMilage = arr.last.mileage - arr.head.mileage
          if (totalMilage < 0) totalMilage = 0;

          //TODO 早高峰行驶里程
          val morningPeakArr = arr.filter(x => {
            //            20171102114428
            isMorningPeak(x.time)
          }).sortBy(_.time)


          var morningPeakMileage = 0D;

          if (morningPeakArr.length > 0) {
            morningPeakMileage = morningPeakArr.last.mileage - morningPeakArr.head.mileage
            if (morningPeakMileage < 0) morningPeakMileage = 0;
          }

          //TODO 晚高峰行驶里程	早高峰平均车速	晚高峰平均车速  总上线天数
          val nightPeakArr = arr.filter(x => {
            isnightPeak(x.time)
          }).sortBy(_.time)

          var nightPeakMileage = 0D;

          if (nightPeakArr.length > 0) {
            nightPeakMileage = nightPeakArr.last.mileage - nightPeakArr.head.mileage
            if (nightPeakMileage < 0) nightPeakMileage = 0;
          }

          //TODO 总行驶时长
          var total_run_duration = 0D;
          val runlstArr = new ArrayBuffer[ArrayBuffer[Input]]()

          var runArr = new ArrayBuffer[Input]()

          for (i <- 0 until arr.length) {
            if (getRunRuler(arr(i))) {
              runArr.append(arr(i))
              if (i == (arr.length - 1)) {
                runlstArr.append(runArr)
              }
            } else {
              if (runArr.length > 0) {
                runlstArr.append(runArr)
                runArr = new ArrayBuffer[Input]()
              }
            }
          }


          // TODO: 早高峰行驶时长
          var morningPeakRunDuration = 0D


          // TODO: 晚高峰行驶时长
          var nightPeakRunDuration = 0D


          runlstArr.filter(_.length > 1).foreach(e => {
            val sortedArr = e.sortBy(_.time)

            val time_len = sortedArr.last.timeStamp - sortedArr.head.timeStamp + 10 * 1000
            total_run_duration = total_run_duration + time_len.toDouble


            val morningPeakRunArr = sortedArr.filter(x => isMorningPeak(x.time))

            if (morningPeakRunArr.length > 0) {
              val morningPeakRun_time_len = morningPeakRunArr.last.timeStamp - morningPeakRunArr.head.timeStamp + 10 * 1000
              morningPeakRunDuration = morningPeakRunDuration + morningPeakRun_time_len.toDouble
            }

            val nightPeakRunArr = sortedArr.filter(x => isnightPeak(x.time))
            if (nightPeakRunArr.length > 0) {
              val nightPeakRun_time_len = nightPeakRunArr.last.timeStamp - nightPeakRunArr.head.timeStamp + 10 * 1000
              nightPeakRunDuration = nightPeakRunDuration + nightPeakRun_time_len.toDouble
            }


          })

          // TODO: 早高峰平均车速 km/h
          var morningAvgSpeed = 0D
          if (morningPeakRunDuration != 0) {
            morningAvgSpeed = morningPeakMileage / (morningPeakRunDuration / (60 * 60 * 1000))
          }


          // TODO: 晚高峰平均车速
          var nightAvgSpeed = 0D
          if (nightPeakRunDuration != 0) {
            nightAvgSpeed = nightPeakMileage / (nightPeakRunDuration / (60 * 60 * 1000))
          }


          //TODO 平均车速 km/h
          var avg_speed = 0D
          if (total_run_duration == 0) totalMilage = 0;
          if (total_run_duration != 0) {
            avg_speed = totalMilage / (total_run_duration / (60 * 60 * 1000))
          }

          //TODO 	总充电时长
          val chargelstArr = new ArrayBuffer[ArrayBuffer[Input]]()

          var chargeArr = new ArrayBuffer[Input]()

          for (i <- 0 until arr.length) {
            if (getChargeRuler(arr(i))) {
              chargeArr.append(arr(i))
              if (i == (arr.length - 1)) {
                chargelstArr.append(chargeArr)
              }
            } else {
              if (chargeArr.length > 0) {
                chargelstArr.append(chargeArr)
                chargeArr = new ArrayBuffer[Input]()
              }
            }
          }

          //TODO 总充电量
          var total_charge_num = 0D

          var total_charge_duration = 0D

          //连续超过10帧
          chargelstArr.filter(_.length >= 10).foreach(e => {
            val sortedArr = e.sortBy(_.time)

            val time_len = sortedArr.last.timeStamp - sortedArr.head.timeStamp + 10 * 1000
            total_charge_duration = total_charge_duration + time_len

            for (i <- 1 until sortedArr.length) {
              val timeDiff = sortedArr(i).timeStamp - sortedArr(i - 1).timeStamp
              //单帧电量
              val charge_num = sortedArr(i).dy * sortedArr(i).dl / 1000 * (timeDiff.toDouble / (60 * 60 * 1000)) //kw.h

              total_charge_num = total_charge_num + Math.abs(charge_num)
            }

          })

          //车牌  	总行驶里程	总行驶时长	总充电时长	总上线天数	平均车速	总充电量	早高峰平均车速	晚高峰平均车速
          if (totalMilage == 0D) total_run_duration = 0D;
          if (morningPeakMileage == 0D) morningPeakRunDuration = 0D;
          if (nightPeakMileage == 0D) nightPeakRunDuration = 0D;
          if (total_charge_num > 100D) total_charge_num = 100D;

          Output(vid,
            date,
            totalMilage.formatted("%.4f"),
            (total_run_duration / (60 * 60 * 1000)).formatted("%.4f"),
            avg_speed.formatted("%.4f"),
            (total_charge_duration / (60 * 60 * 1000)).formatted("%.4f"),
            total_charge_num.formatted("%.4f"),
            morningPeakMileage.formatted("%.4f"),
            (morningPeakRunDuration / (60 * 60 * 1000)).formatted("%.4f"),
            morningAvgSpeed.formatted("%.4f"),
            nightPeakMileage.formatted("%.4f"),
            (nightPeakRunDuration / (60 * 60 * 1000)).formatted("%.4f"),
            nightAvgSpeed.formatted("%.4f"))
        }
      }
      .filter(x => {
        val cond01 = x.avg_speed.toDouble > 0 && x.avg_speed.toDouble < 120
        val cond02 = x.totalMilage.toDouble >= 2 && x.totalMilage.toDouble <= 800
        val cond03 = x.run_duration.toDouble > 0 && x.run_duration.toDouble < 20

        cond01 && cond02 && cond03
      })



    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")

    // TODO: 输出
    if (env.equals("local")) {

      result.show(false)
      //      parkingInfoDS.show(false)
      //      result.show(100, false)

      //      result.count()

      //      result.count()
      //    result.printSchema()
      //    result.show(false)
      //


    }

    // TODO: 输出到
    if (env.equals("dev")) {
      result.repartition(1).write.json(s"/spark/vehicle/result/yearReport/lzo/year=${year}/month=${month}/day=${day}")
    }

    if (env.equals("prd")) {
    }

    logInfo("任务完成...")

    sparkSession.stop()
  }

  def isMorningPeak(time: String): Boolean = {
    val clock = time.substring(8, 10).toInt
    clock >= 7 && clock <= 9
  }

  def isnightPeak(time: String): Boolean = {
    val clock = time.substring(8, 10).toInt
    clock >= 17 && clock <= 19
  }


  def getChargeRuler(input: Input): Boolean = {
    input.dl < 0 && (input.speed * 1000) <= 0.5
    //    && (input.dy > 0)
  }

  def getRunRuler(input: Input): Boolean = {
    input.speed * 1000 > 0.5 && input.dl > 0
    //    !getChargeRuler(input)
  }


  case class Input(vid: String, time: String, mileage: Double, speed: Double, dy: Double, dl: Double, timeStamp: Long)


  case class Output(vid: String, date: String, totalMilage: String, run_duration: String, avg_speed: String, charge_duration: String, total_charge_num: String, morningPeakMileage: String, morningPeakRunDuration: String, morningAvgSpeed: String, nightPeakMileage: String, nightPeakRunDuration: String, nightAvgSpeed: String)

}

//.formatted("%.3f")package com.bitnei.report.dayreport.realinfo

import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.Model.CategoryDayReportModel
import com.bitnei.report.dayreport.distribution.{ContinueDistributionCounter, DiscreteDistribution, MileageDistribution, SocDistribution}
import com.bitnei.report.distribute._

/**
  * Created by wangbaosheng on 2017/3/28.
  * 日报表计算结果
  **/
case class DayReportResult(vid: String = "",
                           category: String = "",
                           reportDate:Long=0,
                           onlineTime: Int = 0,
                           timeLeng: Long = 0,
                           //实际运营时间=单次实际运营时间的累加
                           accRunTime:Int=0,
                           maxTime: Long = 0,
                           minTime: Long ,
                           startMileage: Int = 0,
                           stopMileage: Int ,
                           maxMileage: Int = 0,
                           totalMileage: Int = 0,
                           gpsMileage:Option[Int]=None,
                           maxSpeed: Int = 0,
                           maxTotalVoltage: Int = 0,
                           minTotalVoltage: Int= 0,
                           maxTotalEctriccurrent: Int = Int.MinValue, //电流可以为负值
                           minTotalEctriccurrent: Int= 0,
                           maxSecondaryVolatage: Int = 0,
                           minSecondaryVolatage: Int= 0,
                           maxAcquisitionPointTemp: Int = Int.MinValue,
                           minAcquisitionPointTemp: Int= 0,
                           maxEngineTemp: Int = Int.MinValue, //温度也可以为负值
                           minEngineTemp: Int= 0,
                           maxSoc: Int = 0,
                           minSoc: Int= 0,
                           startSoc: Int = -1,
                           totalCharge: Double = 0,
                           maxCharge: Double = 0,
                           times: Int = 0,
                           chargeTime:Long=0,  fullChargeTime:Long=0,
                           chargePer100Km: Double = 0,
                           chargeSocDistribution: Array[Int]=SocDistribution.default,
                           chargeTimeRangeDistribution: Array[Int]=ContinueDistributionCounter.default,
                           /*充电时段*/
                           runTimeRangeDistribution:Array[Int]=ContinueDistributionCounter.default,
                           runTimeLengthDistribution:Array[Int]= DiscreteDistribution.default,
                           runMileageDistribution:Array[Int]=MileageDistribution.default,
                           chargeTimeLengthDistribution:Array[Int]= DiscreteDistribution.default
){
  //充电时长
  def getChargeSocDistribute: Array[Int] = chargeSocDistribution


  def setChargePer100Km(v: Double): DayReportResult = {
    val dayResult = DayReportResult(
      vid = this.vid, category = this.category, reportDate = this.reportDate,
      onlineTime = this.onlineTime, timeLeng = this.timeLeng,
      maxTime = this.maxTime,
      minTime = this.minTime,
      startMileage = this.startMileage, stopMileage = this.stopMileage,
      maxMileage = this.maxMileage, //单次最大行驶里程
      totalMileage = this.totalMileage,
      maxSpeed = this.maxSpeed,
      maxTotalEctriccurrent = this.maxTotalEctriccurrent,
      maxTotalVoltage = this.maxTotalVoltage, minTotalVoltage = this.minTotalVoltage,
      minTotalEctriccurrent = this.minTotalEctriccurrent, maxSecondaryVolatage = this.maxSecondaryVolatage,
      minSecondaryVolatage = this.minSecondaryVolatage, maxAcquisitionPointTemp = this.maxAcquisitionPointTemp,
      minAcquisitionPointTemp = this.minAcquisitionPointTemp,
      maxEngineTemp = this.maxEngineTemp, minEngineTemp = this.minEngineTemp,
      maxSoc = this.maxSoc, minSoc = this.minSoc,
      totalCharge = this.totalCharge,
      maxCharge = this.maxCharge,
      times = this.times,
      chargeTime = this.chargeTime,
      fullChargeTime = this.fullChargeTime,
      chargePer100Km = v,
      chargeSocDistribution = this.getChargeSocDistribute,
      chargeTimeRangeDistribution = this.chargeTimeRangeDistribution,
      runTimeRangeDistribution = this.runTimeRangeDistribution,
      runMileageDistribution = this.runMileageDistribution,
      runTimeLengthDistribution = this.runTimeLengthDistribution,
      chargeTimeLengthDistribution = this.chargeTimeLengthDistribution)

    dayResult
  }

  def getChargePer100Km: Double = chargePer100Km






  //行驶里程
  def travelMileage: Int = totalMileage

  //平均单次行驶里程
  def avgtravelMileage: Double = if (times != 0) DataPrecision.mileage(travelMileage) / times.toDouble else 0

  //实际总行驶时间
  def travelTime: Long = {
    val rt = onlineTime - chargeTime - fullChargeTime
    if (rt < 0) 0 else rt
  }


  def avgTravelTime: Double = if (times != 0) travelTime.toDouble / times.toDouble else 0

  def avgSpeed: Double = {
    val r = if (travelTime != 0) DataPrecision.mileage(travelMileage) / DataPrecision.toHour(travelTime) else 0
    if (r.isPosInfinity || r.isNegInfinity || r.isNaN) 0 else r
  }

  def avgTime: Double = if (times != 0) timeLeng.toDouble / times.toDouble else 0

  def avgCharge: Double = if (times != 0) totalCharge / times.toDouble else 0

  override def toString: String = category match {
    case Constant.ChargeState =>
      f"$category,$vid,${DataPrecision.toHour(timeLeng)},$times,${DataPrecision.charge(totalCharge)}," +
        f"$getChargePer100Km%1.2f," +
        f"${DataPrecision.toHour(maxTime)},${DataPrecision.toHour(avgTime)},${DataPrecision.totalVoltage(maxTotalVoltage)},${DataPrecision.totalVoltage(minTotalVoltage)}," +
        f"${DataPrecision.totalCurrent(maxTotalEctriccurrent)},${DataPrecision.totalCurrent(minTotalEctriccurrent)},${DataPrecision.soc(maxSoc)},${DataPrecision.soc(minSoc)}," +
        f"${DataPrecision.secondaryVoltage(maxSecondaryVolatage)},${DataPrecision.secondaryVoltage(minSecondaryVolatage)},$maxAcquisitionPointTemp,$minAcquisitionPointTemp," +
        f"$maxEngineTemp,$minEngineTemp,${DataPrecision.charge(maxCharge)},${DataPrecision.charge(avgCharge)}"

    case Constant.FullChargeState =>
      f"fullchargeDay,$vid,${DataPrecision.toHour(timeLeng)},$times," +
        f"${DataPrecision.toHour(maxTime)},${avgCharge / (1000 * 3600f)}%1.2f,${DataPrecision.totalVoltage(maxTotalVoltage)}, ${DataPrecision.totalVoltage(minTotalVoltage)}," +
        f"${DataPrecision.secondaryVoltage(maxSecondaryVolatage)}, ${DataPrecision.secondaryVoltage(minSecondaryVolatage)},$maxAcquisitionPointTemp, $minAcquisitionPointTemp," +
        f"$maxEngineTemp, $minEngineTemp"
    case Constant.TravelState =>
      f"runDay,$vid,${DataPrecision.toHour(onlineTime)},${DataPrecision.toHour(travelTime)},$times," +
        f"${if ((accRunTime - travelTime.toInt) >= 30 * 60 * 1000) "无效行驶时间超过30分钟" else ""}," +
        f"${DataPrecision.mileage(travelMileage)},${DataPrecision.toHour(avgTravelTime)},$avgtravelMileage%1.2f," +
        f"$avgSpeed%1.2f,${DataPrecision.toHour(maxTime.toDouble)},${DataPrecision.toHour(minTime.toDouble)},${DataPrecision.mileage(maxMileage)},${maxSpeed.toDouble * 0.1f}%1.1f," +
        f"${DataPrecision.totalVoltage(maxTotalVoltage)},${DataPrecision.totalVoltage(minTotalVoltage)},${DataPrecision.totalCurrent(maxTotalEctriccurrent)},${DataPrecision.totalCurrent(minTotalEctriccurrent)}," +
        f"${DataPrecision.soc(maxSoc)},${DataPrecision.soc(minSoc)},${DataPrecision.secondaryVoltage(maxSecondaryVolatage)},${DataPrecision.secondaryVoltage(minSecondaryVolatage)}," +
        f"$maxAcquisitionPointTemp,$minAcquisitionPointTemp,$maxEngineTemp,$minEngineTemp"
    case _ => ""
  }
}

object DayReportResult {
  def zero(vid: String, category: String, reportDate: Long,onlineTime:Int,chargeTime:Int,fullChargeTime:Int): DayReportResult = {
    new DayReportResult(vid, category, reportDate, onlineTime, 0, 0, 0, 0, 0, 0, 0, 0, Some(0),
      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, chargeTime, fullChargeTime, 0)
  }

  def zero(): DayReportResult = zero("", "", 0,0,0,0)
}



object DayReportResultUtil{
  //平均单次行驶里程
  def avgtravelMileage(times:Int,travelMileage:Int): Double = if (times != 0) DataPrecision.mileage(travelMileage) / times.toDouble else 0

  //实际总行驶时间
  def travelTime(onlineTime:Long,chargeTime:Long,fullChargeTime:Long): Long = onlineTime - chargeTime - fullChargeTime

  def avgTravelTime(times:Int, runTimeLength:Int): Double = if (times != 0) runTimeLength.toDouble / times.toDouble else 0

  def avgSpeed(travelMileage:Int,travelTime:Int): Double = {
    val r = if (travelTime != 0) DataPrecision.mileage(travelMileage) / DataPrecision.toHour(travelTime) else 0
    if (r.isPosInfinity || r.isNegInfinity || r.isNaN) 0 else r
  }

  def avgTime(times:Int,timeLeng:Int): Double = if (times != 0) timeLeng.toDouble / times.toDouble else 0

  def avgCharge(times:Int,totalCharge:Double): Double = if (times != 0) totalCharge / times.toDouble else 0
}package com.bitnei.report.dayreport.realinfo

import com.bitnei.report.FoldMonod
import com.bitnei.report.constants.Constant


/*
* created by wangbaosheng on 2017/11/2
* 累计日报表和明细报表
*/
class DayReportResultFoldMonoid(vid:String, category:String, reportDate:Long, ct:Int, ft:Int) extends FoldMonod [DetailModel,DayReportResult]{
  override def fold(a: DayReportResult,that: DetailModel): DayReportResult = {
    DayReportResult(
      vid = that.vid, category = that.category, reportDate = that.startTime,
      onlineTime = that.onlineTime, timeLeng = a.timeLeng + that.timeLeng,
      maxTime = Math.max(a.maxTime, that.timeLeng),
      minTime = Math.min(a.minTime, that.timeLeng),
      accRunTime = a.accRunTime + that.accRunTime,

      startMileage = that.startMileageOfCurrentDay,
      stopMileage = that.endMileageOfCurrentDay,

      maxMileage = Math.max(a.maxMileage, that.stopMileage - that.startMileage), //单次最大行驶里程

      totalMileage = if (that.stopMileage == Int.MaxValue || that.startMileage == 0) a.totalMileage else a.totalMileage + (that.stopMileage - that.startMileage),

      gpsMileage = Some(a.gpsMileage.getOrElse(0) + that.gpsMileage),

      maxSpeed = Math.max(a.maxSpeed, that.maxSpeed),

      maxTotalEctriccurrent = Math.max(a.maxTotalEctriccurrent, that.maxTotalCurrent),
      maxTotalVoltage = Math.max(a.maxTotalVoltage, that.maxTotalVoltage), minTotalVoltage = Math.min(a.minTotalVoltage, that.minTotalVoltage),

      minTotalEctriccurrent = Math.min(a.minTotalEctriccurrent, that.minTotalCurrent), maxSecondaryVolatage = Math.max(a.maxSecondaryVolatage, that.maxSecondaryVolatage),
      minSecondaryVolatage = Math.min(a.minSecondaryVolatage, that.minSecondaryVolatage),

      maxAcquisitionPointTemp = if(that.maxAcquisitionPointTemp.nonEmpty) Math.max(a.maxAcquisitionPointTemp, that.maxAcquisitionPointTemp.get) else a.maxAcquisitionPointTemp,
      minAcquisitionPointTemp = Math.min(a.minAcquisitionPointTemp, that.minAcquisitionPointTemp),

      maxEngineTemp = if(that.maxEngineTemp.nonEmpty) Math.max(a.maxEngineTemp, that.maxEngineTemp.get) else a.maxEngineTemp,
      minEngineTemp = Math.min(a.minEngineTemp, that.minEngineTemp),

      maxSoc = Math.max(a.maxSoc, that.maxSoc),
      minSoc = Math.min(a.minSoc, that.minSoc), startSoc = 0,

      totalCharge = a.totalCharge + that.totalCharge,
      maxCharge = Math.max(a.maxCharge, that.totalCharge),
      times = a.times + 1,
      chargeTime = if (that.category == a.category && a.category == Constant.TravelState) ct else (a.timeLeng + that.timeLeng).toInt,
      fullChargeTime = if (that.category == a.category && a.category == Constant.TravelState) ft else (a.timeLeng + that.timeLeng).toInt
    )

  }


  //detail+dayreport=dayreport
  //dayreport=details.head
  //
  override def zero(): DayReportResult = {
    new DayReportResult(vid =vid,
      category = category,
      reportDate = reportDate,
      onlineTime = 0,
      timeLeng = 0,
      //实际运营时间=单次实际运营时间的累加
      accRunTime = 0,
      maxTime = 0,
      minTime = Long.MaxValue,
      startMileage = 0,
      stopMileage = Int.MaxValue,
      maxMileage = 0,
      totalMileage = 0,
      gpsMileage = None,
      maxSpeed = 0,
      maxTotalVoltage = 0,
      minTotalVoltage = Int.MaxValue,
      maxTotalEctriccurrent = Int.MinValue, //电流可以为负值
      minTotalEctriccurrent = Int.MaxValue,
      maxSecondaryVolatage = 0,
      minSecondaryVolatage = Int.MaxValue,
      maxAcquisitionPointTemp = Int.MinValue,
      minAcquisitionPointTemp = Int.MaxValue,
      maxEngineTemp = Int.MinValue, //温度也可以为负值
      minEngineTemp = Int.MaxValue,
      maxSoc = 0,
      minSoc = Int.MaxValue,
      startSoc = -1,
      totalCharge = 0,
      maxCharge = 0,
      times = 0,
      chargePer100Km = 0)
  }
}





package com.bitnei.report.dayreport.realinfo

import com.bitnei.report.EndoMonoid


/*
* created by wangbaosheng on 2017/11/2
* 对日报表执行累计操作
* */
class DayReportResultMonid(vid:String,category:String,reportDate:Long) extends EndoMonoid[DayReportResult]{
  override def op(a: DayReportResult, b: DayReportResult): DayReportResult = {
    val result = DayReportResult(
      vid = b.vid, category = b.category, reportDate = b.reportDate,
      onlineTime = a.onlineTime + b.onlineTime,
      timeLeng = a.timeLeng + b.timeLeng,
      maxTime = Math.max(a.maxTime, b.timeLeng),
      minTime = Math.min(a.minTime, b.timeLeng),
      accRunTime = a.accRunTime + b.accRunTime,
      startMileage = if (a.startMileage == 0||b.startMileage==0) b.startMileage else Math.min(a.startMileage, b.startMileage), //开始仪表里程
      stopMileage = if (b.stopMileage == Int.MaxValue) a.stopMileage else Math.max(a.stopMileage, b.stopMileage), //结束仪表里程
      maxMileage = Math.max(a.maxMileage, b.maxMileage), //单次最大行驶里程
      totalMileage = a.totalMileage + b.totalMileage, //实际总行驶里程
      gpsMileage = Some(a.gpsMileage.getOrElse(0) + b.gpsMileage.getOrElse(0)),
      maxSpeed = Math.max(a.maxSpeed, b.maxSpeed),
      maxTotalEctriccurrent = Math.max(a.maxTotalEctriccurrent, b.maxTotalEctriccurrent),
      maxTotalVoltage = Math.max(a.maxTotalVoltage, b.maxTotalVoltage), minTotalVoltage = Math.min(a.minTotalVoltage, b.minTotalVoltage),
      minTotalEctriccurrent = Math.min(a.minTotalEctriccurrent, b.minTotalEctriccurrent), maxSecondaryVolatage = Math.max(a.maxSecondaryVolatage, b.maxSecondaryVolatage),
      minSecondaryVolatage = Math.min(a.minSecondaryVolatage, b.minSecondaryVolatage), maxAcquisitionPointTemp = Math.max(a.maxAcquisitionPointTemp, b.maxAcquisitionPointTemp),
      minAcquisitionPointTemp = Math.min(a.minAcquisitionPointTemp, b.minAcquisitionPointTemp),
      maxEngineTemp = Math.max(a.maxEngineTemp, b.maxEngineTemp), minEngineTemp = Math.min(a.minEngineTemp, b.minEngineTemp),
      maxSoc = Math.max(a.maxSoc, b.maxSoc), minSoc = Math.min(a.minSoc, b.minSoc),
      startSoc = a.startSoc,
      totalCharge = a.totalCharge + b.totalCharge,
      maxCharge = Math.max(a.maxCharge, b.maxCharge),
      times = a.times + b.times)
    result
  }

  override def zero(): DayReportResult = {
    new DayReportResult(vid =vid,
      category = category,
      reportDate = reportDate,
      onlineTime = 0,
      timeLeng = 0,
      //实际运营时间=单次实际运营时间的累加
      accRunTime = 0,
      maxTime = 0,
      minTime = Long.MaxValue,
      startMileage = 0,
      stopMileage = Int.MaxValue,
      maxMileage = 0,
      totalMileage = 0,
      gpsMileage = None,
      maxSpeed = 0,
      maxTotalVoltage = 0,
      minTotalVoltage = Int.MaxValue,
      maxTotalEctriccurrent = Int.MinValue, //电流可以为负值
      minTotalEctriccurrent = Int.MaxValue,
      maxSecondaryVolatage = 0,
      minSecondaryVolatage = Int.MaxValue,
      maxAcquisitionPointTemp = Int.MinValue,
      minAcquisitionPointTemp = Int.MaxValue,
      maxEngineTemp = Int.MinValue, //温度也可以为负值
      minEngineTemp = Int.MaxValue,
      maxSoc = 0,
      minSoc = Int.MaxValue,
      startSoc = -1,
      totalCharge = 0,
      maxCharge = 0,
      times = 0,
      chargePer100Km = 0)
  }
}
package com.bitnei.report.dayreport

//ackage test

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.Model._
import com.bitnei.report.dayreport.realinfo.{DayReportResult, RealinfoJob}
import org.apache.spark.sql.SparkSession
import org.scalatest.{BeforeAndAfter, FunSuite}



/**
  * Created by franciswang on 2016/11/7.
  */
class DayreportTest extends FunSuite with BeforeAndAfter{
  val stateConf=new StateConf()



  before{
    stateConf.set("state.database.remove.enable","false")
    stateConf.set("useparquet","false")
    stateConf.set(Constant.OutputDatabase,"false")
    stateConf.set(Constant.CheckPointPath,"")
  }


  val sparkSession=SparkSession.builder().appName("spark单元测试").master("local").getOrCreate()

  val stateCompute=new RealinfoJob(sparkSession,stateConf)

  test("insert into oracle charge day report table") {
    val values = Array(
      new DayReportResult(
        "728a83e0-d13f-4264-811b-2b4d7e6ba747",
        Constant.ChargeState, new Date().getTime,
        1111111,1222, 12, 9, 1, 1, 1, 1000, 1,Some(1),1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
      ),
      new DayReportResult(
        "728a83e0-d13f-4264-811b-2b4d7e6ba747",
        Constant.TravelState, new Date().getTime,
        1111111,11222, 12, 9, 1, 1, 1, 1000, 1,Some(1),1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
      ),
      new DayReportResult(
        "728a83e0-d13f-4264-811b-2b4d7e6ba747",
        Constant.FullChargeState, new Date().getTime,
        1111111,11222, 12, 9, 1, 1, 1,1,1,Some(1) , 1, 1, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1
      )
    )

    val stateConf = new StateConf

    def conifgMysql(): Unit = {
      stateConf.set(Constant.JdbcUserName, "root")
      stateConf.set(Constant.JdbcPasswd, "123")
      stateConf.set(Constant.JdbcDriver, "com.mysql.jdbc.Driver")
      stateConf.set(Constant.JdbcUrl, "jdbc:mysql://192.168.2.140:3306/evmsc")
      stateConf.set("database", "mysql")
    }

    def configOracle(): Unit = {
      stateConf.set(Constant.JdbcUserName, "ev")
      stateConf.set(Constant.JdbcPasswd, "ev")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.2.51:1521:evmsc1")
      stateConf.set("database", "oracle")
    }

    configOracle()

    stateConf.set(Constant.ChargeStateDayReportTable, "veh_dayreport_chargestate")

    new ChargeStateDayReportManager(stateConf).output(values)

    stateConf.set(Constant.TravelStateDayReportTable, "veh_dayreport_runstate")
    new RunStateDayReportManager(stateConf).output(values)

    stateConf.set(Constant.FullChargeStateDayReportTable, "veh_dayreport_fullstate")
    new FullChargeStateDayReportModelManager(stateConf).output(values)


    val result = values(0)
//    val category = CategoryDayReportModel(result.vid, result.reportDate, result.getChargeSocDistribute, result.chargeTimeLengthDistribution, result.chargeTimeRangeDistribution,)
//
//    stateConf.set(Constant.CategoryDayReportTable, "veh_dayreport_category")
//    new CategoryDayReportManager(stateConf).insert(Array(category))
//
//
//    stateConf.set(Constant.StateAlarmTable, "veh_dayreport_alarm")
//    new AlarmStateModelManager(stateConf).insert(Array(new AlarmStateMode("", 0,0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)))
//
//
//    stateConf.set(Constant.StateValidateTable, "veh_dayreport_datastat")
//    new ValidateModelManager(stateConf).insert(
//      Array(
//        new ValidityModel("", "2016-10-22", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
//      )
//    )
  }

  test("test 01c3d73e-a3c5-4b85-aa2c-4d330c8c6015"){


    /**
      * 测试如下车辆的数据
      * 0222510f-0261-4e82-ab33-71e81c1e9c30
      * 1ef74cd7c-1043-49ae-aa94-84e28d9426f2
      * 01c3d73e-a3c5-4b85-aa2c-4d330c8c6015"
      * 02a82879-ffc4-4f54-8470-f4134259c70f
      */


    def assert01c3d73ea3c54b8aa2c4d330c8c6015(): Unit = {
      stateConf.set(Constant.RealinfoPath, "C:\\D\\dayreport\\src\\main\\resources\\dayreport\\01c3d73e-a3c5-4b85-aa2c-4d330c8c6015")
      stateCompute.registerIfNeed()

      val actualResults = stateCompute.doCompute().collect()

      actualResults.foreach(println)

      val exceptedRunResults = Array(
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913000026,20160913185710,22.77,70210,69920,60.00,572.4,419.4,439.8,-180.2,99.6,72.8,3.418,2.997,35,27,52,0",
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913200746,20160913221252,22.77,70220,70210,14.00,569.0,564.0,44.3,-42.8,99.6,99.6,3.393,3.343,35,32,48,43",
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913221553,20160913235941,22.77,70220,70220,0.00,564.2,0.0,0.0,0.0,99.6,0.0,3.363,0.000,35,0,46,0")


      val exceptedChargeResults = Array(
        "charge,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913185740,20160913200616,1.14,14.67,1.14,603.9,563.7,-51.3,-67.3,99.2,72.8,3.651,3.348,35,32,44,0,14.67"
      )

      val exceptedFullChargeResults = Array(
        "fullcharge,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913200646,20160913200716,0.01,0.00,0.01,587.6,587.6,0.0,0.0,100.0,100.0,3.536,3.431,35,32,0,0,0.00")


      val exceptedRunDayResult = Array("runDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,23.92,22.77,3,30.0,7.6,10.00,1.32,18.95,1.73,29.0,60.0,572.4,419.4,439.8,-180.2,99.6, 0.0,3.42,3.00,35,27,52,26")
      val exceptedChargeDayResult = Array("chargeDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,1.14,1,14.67,48.90,1.1,1.1,603.9,563.7,-51.3,-67.3,99.2,72.8,3.65,3.35,35,32,44,0,14.67,14.67")
      val exceptedFullChargeResult = Array("fullchargeDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,0.01,1,0.01,0.01,587.6, 587.6,3.54, 3.43,35, 32,0, 0")

      val exceptedResults = Map(
        "run" -> exceptedRunResults,
        "charge" -> exceptedChargeResults,
        "fullcharge" -> exceptedFullChargeResults,
        "runDay" -> exceptedRunDayResult,
        "chargeDay" -> exceptedChargeDayResult,
        "fullchargeDay" -> exceptedFullChargeResult
      )
      actualResults.foreach(println)



//      assertResult(actualResults, exceptedResults, "charge")
//      assertResult(actualResults, exceptedResults, "fullcharge")
//      assertResult(actualResults, exceptedResults, "run")
//      assertResult(actualResults, exceptedResults, "runDay")
//      assertResult(actualResults, exceptedResults, "chargeDay")
//      assertResult(actualResults, exceptedResults, "fullchargeDay")
    }

    def assert0222510f02614e82ab3371e81c1e9c30(): Unit = {
      stateConf.set(Constant.RealinfoPath, "C:\\D\\dayreport\\src\\main\\resources\\dayreport\\0222510f-0261-4e82-ab33-71e81c1e9c30")

      stateCompute.registerIfNeed()
      val actualResults = stateCompute.doCompute().collect()

      //actualResults.foreach(println)
      val exceptedRunResults = Array(
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913000026,20160913185710,22.77,70210,69920,60.00,572.4,419.4,439.8,-180.2,99.6,72.8,3.418,2.997,35,2997,52,0",
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913200746,20160913221252,22.77,70220,70210,14.00,569.0,564.0,44.3,-42.8,99.6,99.6,3.393,3.343,35,3343,48,43",
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913221553,20160913235941,22.77,70220,70220,0.00,564.2,0.0,0.0,0.0,99.6,0.0,3.363,0.000,35,0,46,0")

      val exceptedChargeResults = Array(
        "charge,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913185740,20160913200616,1.14,14.668854361111116,1.14,603.9,563.7,-51.3,-67.3,99.2,72.8,3.651,3.348,35,3348,44,0,14.668854361111116"
      )

      val exceptedFullChargeResults = Array(
        "fullcharge,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913200646,20160913200716,0.01,0.0,0.01,587.6,587.6,0.0,0.0,100.0,100.0,3.536,3.431,35,3431,0,0,0.0")

      val exceptedChargeDay = Array("chargeDay,0222510f-0261-4e82-ab33-71e81c1e9c30,0.00,0,0.00,0.00,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00,0.00,0,0,0,0,0.00,0.00")
      val exceptedFullChargeDay = Array("fullchargeDay,0222510f-0261-4e82-ab33-71e81c1e9c30,0.00,0,0.00,0.00,0.0, 0.0,0.00, 0.00,0, 0,0, 0")
      val exceptedRunDay = Array("runDay,0222510f-0261-4e82-ab33-71e81c1e9c30,1.71,1.71,13,1.0,0.13,0.08,0.58,0.17,0.08,1.0,28.1,512.7,483.6,84.2,-0.9,68.0, 0.0,3.32,0.00,20,19,26,20")


      val exceptedResults = Map(
        "run" -> exceptedRunResults,
        "charge" -> exceptedChargeResults,
        "fullcharge" -> exceptedFullChargeResults,
        "chargeDay" -> exceptedChargeDay,
        "fullchargeDay" -> exceptedFullChargeDay,
        "runDay" -> exceptedRunDay
      )

      actualResults.foreach(println)


//      assertResult(actualResults, exceptedResults, "charge")
      //assertResult(actualResults, exceptedResults, "fullcharge")
      //assertResult(actualResults, exceptedResults, "run")
//      assertResult(actualResults, exceptedResults, "runDay")
//      assertResult(actualResults, exceptedResults, "chargeDay")
//      assertResult(actualResults, exceptedResults, "fullchargeDay")
    }

    def assert02a82879_ffc4_4f54_8470_f4134259c70f(): Unit ={
      stateConf.set(Constant.RealinfoPath,"C:\\D\\dayreport\\src\\main\\resources\\dayreport\\02a82879-ffc4-4f54-8470-f4134259c70f")

      stateCompute.registerIfNeed()
      val actualResults= stateCompute.doCompute().collect()

      val exceptedRunResults=Array(
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913000026,20160913185710,22.77,70210,0,60.00,572.4,419.4,439.8,-180.2,99.6,72.8,3.418,2.997,35,2997,52,0",
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913200746,20160913221252,22.77,70220,70210,14.00,569.0,564.0,44.3,-42.8,99.6,99.6,3.393,3.343,35,3343,48,43",
        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913221553,20160913235941,22.77,70220,0,0.00,564.2,0.0,0.0,0.0,99.6,0.0,3.363,0.000,35,0,46,0")


      val exceptedChargeResults=Array(
        "charge,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913185740,20160913200616,1.14,14.66,1.14,603.9,563.7,-51.3,-67.3,99.2,72.8,3.651,3.348,35,3348,44,0,14.66"
      )

      val exceptedFullChargeResults=Array(
        "fullcharge,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913200646,20160913200716,0.01,0.0,0.01,587.6,587.6,0.0,0.0,100.0,100.0,3.536,3.431,35,3431,0,0,0.0")

      val exceptedResults=Map(
        "run"->exceptedRunResults,
        "charge"->exceptedChargeResults,
        "fullcharge"->exceptedFullChargeResults
      )

      actualResults.foreach(println)
      //assertResult(actualResults,exceptedResults,"charge")
      //assertResult(actualResults,exceptedResults,"fullcharge")
      //assertResult(actualResults,exceptedResults,"run")
    }


    def assertResult(actualResults:Array[String], exceptedResults:Map[String,Array[String]],state:String): Unit = {
      exceptedResults.get(state) match {
        case Some(excepted) =>
          val actual = actualResults.filter(_.split(',')(0) == state)
          assert(actual.length==excepted.length)

          actual.indices.foreach(index=>{
            val actualFields=actual(index).split(',')
            val exceptedFields=excepted(index).split(',')

            actualFields.indices.foreach(fieldIndex=>{
              //测试run,vid,开始时间,结束时间
              if(0<=fieldIndex&&fieldIndex<=3) {
                assert(actualFields(fieldIndex) == exceptedFields(fieldIndex))
                assert(actualFields(fieldIndex) == exceptedFields(fieldIndex))
              }else {
               // println(actualFields(fieldIndex)+":"+exceptedFields(fieldIndex))
               // assert(Math.abs(actualFields(fieldIndex).toDouble-exceptedFields(fieldIndex).toDouble)<=0.01)
              }
            })
          })
        case None =>
      }
    }

    assert01c3d73ea3c54b8aa2c4d330c8c6015()
   // assert0222510f02614e82ab3371e81c1e9c30()
    //assert02a82879_ffc4_4f54_8470_f4134259c70f()
  }


  test("alarm"){
    //sparkSession.sparkContext.textFile("c:/D/dayreport/src/main/resources/alarm_27E2ADD68F2F7EB1E0533C02A8C0B094").filter(_.contains("27E2ADD68F2F7EB1E0533C02A8C0B094")).saveAsTextFile("./src/main/resources/alarm2")
    //stateCompute.initRdd.collect.foreach(println)


    //val alarmResult=stateCompute.alarmResult.collect()
   // alarmResult.foreach(println)
    //stateCompute.registerAlarm()
   // stateCompute.computeAlarm().foreach(x=>println(x))
  }

}
package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.realinfo.RealinfoJob
import org.apache.spark.sql.SparkSession
import org.scalatest.{BeforeAndAfter, FunSuite}

/**
  * Created by wangbaosheng on 2017/3/27.
  */
class DayreportTest2 extends FunSuite with BeforeAndAfter {
  val stateConf = new StateConf()


  before {
    stateConf.set("state.database.remove.enable", "false")
    stateConf.set("dayreport.realinfo.input.format", "text")
    stateConf.set(Constant.OutputDatabase, "false")

    stateConf.set(Constant.CheckPointPath, "")
  }


  val sparkSession = SparkSession.builder().appName("spark单元测试").master("local").getOrCreate()

  val realinfoJob = new RealinfoJob(sparkSession, stateConf)


  test("test 01c3d73e-a3c5-4b85-aa2c-4d330c8c6015") {
    def assert01c3d73ea3c54b8aa2c4d330c8c6015(): Unit = {
      stateConf.set("dayreport.realinfo.input.path", "C:\\D\\dayreport\\src\\main\\resources\\dayreport\\01c3d73e-a3c5-4b85-aa2c-4d330c8c6015.detail")
      stateConf.set("dayreport.output.hdfs.enable","false")
      realinfoJob.registerIfNeed()

      //realinfoJob.compute()

      val actualResults = realinfoJob.doCompute().collect()


      val exceptedRunDayResult = Array("runDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,23.92,22.77,3,30.0,7.6,10.00,1.32,18.95,1.73,29.0,60.0,572.4,419.4,439.8,-180.2,99.6, 0.0,3.42,3.00,35,27,52,26")
      val exceptedChargeDayResult = Array("chargeDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,1.14,1,14.67,48.90,1.1,1.1,603.9,563.7,-51.3,-67.3,99.2,72.8,3.65,3.35,35,32,44,0,14.67,14.67")
      val exceptedFullChargeResult = Array("fullchargeDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,0.01,1,0.01,0.01,587.6, 587.6,3.54, 3.43,35, 32,0, 0")

      val exceptedResults = Map(
        "runDay" -> exceptedRunDayResult,
        "chargeDay" -> exceptedChargeDayResult,
        "fullchargeDay" -> exceptedFullChargeResult
      )
      actualResults.foreach(println)


      //assertResult(actualResults, exceptedResults, "runDay")
    //  assertResult(actualResults, exceptedResults, "chargeDay")
    //  assertResult(actualResults, exceptedResults, "fullchargeDay")
    }

    assert01c3d73ea3c54b8aa2c4d330c8c6015()
  }
}package com.bitnei.report.util.jdbc

import java.sql.{Connection, DriverManager}
import java.util
import java.util.ResourceBundle

/**
  *
  * @author zhangyongtian
  * @define 数据库连接池工具类
  *
  * create 2018-03-28 15:38
  *
  */
object DBConnectionPool {
  private val reader = ResourceBundle.getBundle("connection")
  private val max_connection = reader.getString("jeecg.max_connection") //连接池总数
  private val connection_num = reader.getString("jeecg.connection_num") //产生连接数
  private var current_num = 0 //当前连接池已产生的连接数
  private val pools = new util.LinkedList[Connection]() //连接池
  private val driver = reader.getString("jeecg.driver")
  private val url = reader.getString("jeecg.url")
  private val username = reader.getString("jeecg.username")
  private val password = reader.getString("jeecg.password")
  /**
    * 加载驱动
    */
  private def before() {
    if (current_num > max_connection.toInt && pools.isEmpty()) {
      print("busyness")
      Thread.sleep(2000)
      before()
    } else {
      Class.forName(driver)
    }
  }
  /**
    * 获得连接
    */
  private def initConn(): Connection = {
    val conn = DriverManager.getConnection(url, username, password)
    conn
  }
  /**
    * 初始化连接池
    */
  private def initConnectionPool(): util.LinkedList[Connection] = {
    AnyRef.synchronized({
      if (pools.isEmpty()) {
        before()
        for (i <- 1 to connection_num.toInt) {
          pools.push(initConn())
          current_num += 1
        }
      }
      pools
    })
  }
  /**
    * 获得连接
    */
  def getConn():Connection={
    initConnectionPool()
    pools.poll()
  }
  /**
    * 释放连接
    */
  def releaseCon(con:Connection){
    pools.push(con)
  }

}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.DCT
import org.apache.spark.ml.linalg.Vectors
// $example off$
import org.apache.spark.sql.SparkSession

object DCTExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("DCTExample")
      .getOrCreate()

    // $example on$
    val data = Seq(
      Vectors.dense(0.0, 1.0, -2.0, 3.0),
      Vectors.dense(-1.0, 2.0, 4.0, -7.0),
      Vectors.dense(14.0, -2.0, -5.0, 1.0))

    val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF("features")

    val dct = new DCT()
      .setInputCol("features")
      .setOutputCol("featuresDCT")
      .setInverse(false)

    val dctDf = dct.transform(df)
    dctDf.select("featuresDCT").show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println

package com.bitnei.report

import java.util.Date

import com.bitnei.report.DeadlineMileageJob.logInfo
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper, SqlHelper}
import org.apache.hadoop.fs.Path
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.sql.{Dataset, SparkSession}
import org.apache.spark.storage.StorageLevel

/**
  * Created by wangbaosheng on 2017/8/18.
  */

/**
  * @param vid 车辆vid
  * @param deadline 结束日期;
  * @param accMileage 实际运营里程
  * @param totalMileage 仪表里程
  * */
case class DeadlineMileage(vid:String, deadline:Long, accMileage:Double, totalMileage:Double,gpsMileage:Int)

case class MonthReportInput(vid: String,
                           category: String,
                           reportDate:Long,
                            startMileage: Int,
                            stopMileage: Int,
                            totalMileage:Int,//实际行驶里程
                           gpsMileage:Option[Int]
                          )


/**
  *  计算里程核查的运营总里程和仪表总里程
  *
  */

class DeadlineMileageJob(stateConf:StateConf,sparkSession: SparkSession) extends Serializable with Logging with Job {
  override type R = DeadlineMileage
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConfiguration)

  import sparkSession.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("monthreport")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("deadlineReport")

  override def registerIfNeed() = SparkHelper.createOrReplaceTempView(sparkSession,stateConf,inputTableName)

  override def unRegister() = sparkSession.catalog.dropTempView("monthreport")




  override def doCompute[Product <: DeadlineMileage]() = {
    sparkSession.sql(s"SELECT vid, category,reportDate,reportDate,startMileage,stopMileage,totalMileage,gpsMileage" +
      s" FROM $inputTableName " +
      s" ${SqlHelper.buildWhere(stateConf)} AND category='${stateConf.getOption("state").getOrElse("run")}'")
      .as[MonthReportInput]
      .groupByKey(_.vid)
      .mapGroups({ case (vid: String, monthValues: Iterator[MonthReportInput]) =>
        val sortedMonth = Utils.sortByDate2[MonthReportInput](monthValues.toArray, month => Some(month.reportDate))

        val (accMileage, gpsMileage) = sortedMonth.foldLeft((0, 0))((left, right) => {
          (left._1 + right.totalMileage, left._2 + right.gpsMileage.getOrElse(0))
        })

        val totalMileage = sortedMonth.last.stopMileage - sortedMonth.head.startMileage
        val deadLine = sortedMonth.last.reportDate

        DeadlineMileage(
          vid,
          deadLine,
          DataPrecision.mileage(accMileage),
          DataPrecision.mileage(totalMileage),
          gpsMileage)
      })
  }

  override def write[Product <: DeadlineMileage](result: Dataset[DeadlineMileage]) = {
    def writeToHbase(zkQuorum: String, zkport: String): Unit = {
      result.foreachPartition(values => {
        HbaseHelper.bulkPut(zkQuorum, zkport, "mileage_check_coords", table => {
          values.foreach(v => {
            //val deadlineMonth = Utils.formatDate(new Date(v.deadline), "yyyyMM")

           logInfo(s"${v.vid}_${v.deadline}")
            val rowKey = Bytes.toBytes(s"${v.vid}_${v.deadline}")

            table.put(HbaseHelper.createRow(rowKey, "df", "accMileage", v.accMileage.toString))
            table.put(HbaseHelper.createRow(rowKey, "df", "totalMileage", v.totalMileage.toString))
          })
        })
      })
    }

    val outputModels = stateConf.getOption("report.output").getOrElse("hdfs").split(',').map(_.trim)
    if (outputModels.length > 1) {
      result.persist(StorageLevel.MEMORY_ONLY_SER)
    }

    outputModels.foreach({
      case "hdfs" =>

        SparkHelper.saveToPartition(sparkSession,stateConf,result.toDF(),outputTableName)
      case "hbase" => writeToHbase(stateConf.getString("hbase.quorum"), stateConf.getString("hbase.zkport"))
      case _ => throw new RuntimeException("output param error")
    })
  }
}

object DeadlineMileageJob extends  Logging{
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)


    val sparkSession = SparkHelper.getSparkSession(None)

    new DeadlineMileageJob(stateConf, sparkSession).compute()
  }
}package com.bitnei.report

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{ Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.sql.{DataFrame, Dataset, SaveMode, SparkSession}

import scala.collection.mutable.ArrayBuffer

case class MonthReportInput(vid: String,
                            category: String,
                            reportDate:Long,
                            startMileage: Double,
                            stopMileage: Double,
                            totalMileage:Double,//实际行驶里程
                            gpsMileage:Option[Double]
                           )


/**
  * @param vid 车辆vid
  * @param deadline 结束日期;
  * @param accMileage 实际运营里程
  * @param totalMileage 仪表里程
  * */
case class DeadlineMileage(vid:String, vin:String,deadline:Long, startDate:Long,startMileage:Double,stopDate:Long,stopMileage:Double,accMileage:Double, totalMileage:Double,gpsMileage:Double)
/*
* created by wangbaosheng on 2017/12/25
* 月总里程计算作业
*/
class DeadlineMileageJob(stateConf:StateConf, @transient sparkSession: SparkSession) extends Serializable with Job with Logging {
  override type R = DeadlineMileage
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)

  //hbase表
  private val mileageTableName = stateConf.getOption("mileagecheck.table.name").getOrElse("mileage_check_coords")
  //日轨迹表
  private val dayCoordTable = stateConf.getOption("dayCoord.table.name").getOrElse("mileageCheckDayCoord")
  //月总里程表
  private  val deadlineTable=stateConf.getOption("deadline.table.name").getOrElse("mileageCheckDeadline")


  //日最大里程阈值
  val dayMileageMaxThreshold=stateConf.getInt("dayMileageMaxThreshold",800)
  //日最小里程阈值
  val dayMileageMinThreshold=stateConf.getInt("dayMileageMinThreshold",1)

  //要计算的月份
  private val monthDate = stateConf.getString("monthDate")
  private val year = Utils.formatDate("yyyyMM", "yyyy", monthDate)
  private val month = Utils.formatDate("yyyyMM", "MM", monthDate)
  logInfo(s"monthDate=$monthDate")

  SparkHelper.setPartitionValue(stateConf,deadlineTable,Array(year,month))

  if (monthDate == Utils.formatDate(new Date, "yyyyMM")) {
    throw new IllegalArgumentException(s"the monthDate must be not equal current month:monthDate=$monthDate")
  }

  import sparkSession.sqlContext.implicits._

  override def registerIfNeed() = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, "dayreport")
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, dayCoordTable)
  }


  override def unRegister() = {
    sparkSession.catalog.dropTempView("dayreport")
    sparkSession.catalog.dropTempView( dayCoordTable)
  }


  override def doCompute[Product <: DeadlineMileage]() = {
    def getAllDeadline(): Dataset[DeadlineMileage] = {
      val sql =
        s"""
           SELECT vid,
             category,
             reportDate,
             CAST(startMileage*0.1 AS DOUBLE) AS startMileage,
             CAST(stopMileage*0.1 AS DOUBLE) AS stopMileage,
             CAST(totalMileage*0.1 AS DOUBLE) AS totalMileage,
             gpsMileage
           FROM dayreport
           where ${whereCondition()}
           AND category = '${Constant.TravelState}'
         """.stripMargin

      val result = sparkSession.sql(sql).as[MonthReportInput]
        .groupByKey(v => (v.vid, Utils.formatDate(new Date(v.reportDate), "yyyyMM")))
        .mapGroups({ case ((vid: String, date: String), monthValues: Iterator[MonthReportInput]) =>
          computeDeadlineMileage(monthValues.toArray)
        }).groupByKey(_.vid)
        .mapGroups({ case (vid: String, monthValues: Iterator[DeadlineMileage]) =>
          val sortedMonth = Utils.sortByDate2[DeadlineMileage](monthValues.toArray, month => Some(month.deadline))
          //核算里程
          var accMileage = 0D
          //轨迹里程
          var gpsMileage = 0D
          //开始时间
          var startTime=0L
          //结束时间
          var stopTime=0L
          //开始里程
          var startMileage = 0D
          //结束里程
          var stopMileage = 0D
          sortedMonth.foreach(deadlineMonth => {
            accMileage+=deadlineMonth.accMileage

            gpsMileage += deadlineMonth.gpsMileage

            if (startMileage < 1) {
              startTime=deadlineMonth.startDate
              startMileage=deadlineMonth.startMileage
            }

            if (deadlineMonth.stopMileage > 1 && deadlineMonth.stopMileage < 100000000) {
              stopTime=deadlineMonth.deadline
              stopMileage = deadlineMonth.stopMileage
            }
          })

          DeadlineMileage(
            vid = sortedMonth.head.vid,
            vin = "",
            deadline = stopTime,
            startDate=startTime,
            startMileage=startMileage,
            stopDate=stopTime,
            stopMileage=stopMileage,
            accMileage = accMileage,
            totalMileage = stopMileage - startMileage,
            gpsMileage = gpsMileage)
        })

      result
    }

    //获取deadline 里程信息
    val deadlineDs = getAllDeadline()

    //过滤掉无效的deadline
    val validateDeadlineDs = getValidateDeadline(deadlineDs)

    validateDeadlineDs
  }


  def computeDeadlineMileage(values:Array[MonthReportInput]):DeadlineMileage={
    val sortedMonth = Utils.sortByDate2[MonthReportInput](values, month => Some(month.reportDate))

    //核算里程
    var accMileage = 0D
    //轨迹里程
    var gpsMileage = 0D
    //开始时间
    var startTime=0L
    //结束时间
    var stopTime=0L
    //开始里程
    var startMileage = 0D
    //结束里程
    var stopMileage = 0D

    //计算累计里程和核算里程
    sortedMonth.foreach(dayreport => {
      accMileage += {
        if (dayreport.totalMileage >= dayMileageMinThreshold && dayreport.totalMileage <= dayMileageMaxThreshold)
          dayreport.totalMileage
        else 0
      }

      gpsMileage += dayreport.gpsMileage.getOrElse(0D)

      if (startMileage < 1) {
        startTime=dayreport.reportDate
        startMileage=dayreport.startMileage
      }

      if (dayreport.stopMileage > 1 && dayreport.stopMileage < 100000000) {
        stopTime=dayreport.reportDate
        stopMileage = dayreport.stopMileage
      }
    })

    val deadLine = sortedMonth.last.reportDate


    DeadlineMileage(
      vid = sortedMonth.head.vid,
      vin = "",
      deadline = deadLine,
      startDate=startTime,
      startMileage=startMileage,
      stopDate=stopTime,
      stopMileage=stopMileage,
      accMileage = accMileage,
      totalMileage = stopMileage - startMileage,
      gpsMileage = gpsMileage)
  }

  //过滤掉无效的总里程，从而确保有总里程，一定有日轨迹。
  def getValidateDeadline(deadlineDs: Dataset[DeadlineMileage]): Dataset[DeadlineMileage] = {
    deadlineDs.createOrReplaceTempView("deadline")
    val sql =
      s"""
        select
         deadline.vid,
         dayCoord.vin,
         deadline.startDate,
         deadline.startMileage,
         deadline.stopDate,
         deadline.stopMileage,
         deadline.deadline,
         deadline.accMileage,
         deadline.totalMileage,
         deadline.gpsMileage
        FROM $dayCoordTable as dayCoord INNER JOIN  deadline ON dayCoord.vid=deadline.vid
        WHERE dayCoord.year=$year and month=$month
      """.stripMargin

    sparkSession.sql(sql).as[DeadlineMileage]
  }

  def whereCondition(): String = {
    val startMonth = Utils.parsetDate(stateConf.getOption("report.startMonth").getOrElse("201601"), "yyyyMM").get
    val endMonth = Utils.parsetDate(monthDate, "yyyyMM").get
    val curMonth = new Date(startMonth.getTime)

    val condition = new ArrayBuffer[String]()
    while (curMonth.getTime <= endMonth.getTime) {
      val year = Utils.formatDate(curMonth, "yyyy")
      val month = Utils.formatDate(curMonth, "MM")
      condition.append(s"(year=$year AND month=$month)")
      curMonth.setMonth(curMonth.getMonth + 1)
    }

    val s=condition.foldLeft("")((left, acc) => {
      if (left.isEmpty) acc else {
        s"$left OR $acc"
      }
    })

    logInfo(s)
    s
  }


  override def write[Product <: DeadlineMileage](result: Dataset[DeadlineMileage]) = {
    def writeToHdfs(path: String): Unit = {
      SparkHelper.saveToPartition(sparkSession,stateConf,result.toDF(),deadlineTable)
    }

    def writeToHbase(quorum: String, zkport: String) = {
      result.foreachPartition(values => {
        HbaseHelper.bulkPut(quorum, zkport, mileageTableName, (table) => {
          val mapper = new ObjectMapper()
          mapper.registerModule(DefaultScalaModule)
          values.foreach(v => {
            val deadline = Utils.formatDate(new Date(v.deadline),"yyyyMM")
            val rowKey = s"${v.vid}_m_$deadline"

            logInfo(s"rowkey:$rowKey")

            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "vin", v.vin.toString))
            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "accMileage", v.accMileage.toString))
            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "totalMileage", v.totalMileage.toString))
          })
        })
      })
    }


    val outputModels = stateConf.getOption("report.output").getOrElse("hdfs").split(',').map(_.trim)
    if (outputModels.length > 1) {
      result.cache()
    }

    outputModels.foreach({
      case "hdfs" =>
        val path = s"${stateConf.getString("output.hdfs.path")}/$monthDate"
        writeToHdfs(path)
      case "hbase" =>
        (stateConf.getOption("hbase.quorum"), stateConf.getOption("hbase.zkport")) match {
          case (Some(quorm), Some(zkport)) =>
            logInfo(s"writting to hbase,$quorm:$zkport")
            writeToHbase(quorm, zkport)
          case _ =>
            throw new RuntimeException("hbase.quorum or hbase.zkport not exists.")
        }
      case _ =>
    })
  }
}

object DeadlineMileageJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)
    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)
    new DeadlineMileageJob(stateConf, sparkSession).compute()
  }
}
package com.bitnei.report

import com.bitnei.report.MileageCheckJob.logInfo
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper, SqlHelper}
import org.apache.hadoop.hbase.client.Get
import org.apache.hadoop.hbase.util.Bytes
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SaveMode
import org.apache.spark.storage.StorageLevel

/**
  * @param vid          车辆vid
  * @param deadline     结束日期;
  * @param accMileage   实际运营里程
  * @param totalMileage 仪表里程
  **/
case class DeadlineMileage2(vid: String, deadline: Long, accMileage: Double, totalMileage: Double, gpsMileage: Long)

case class MonthReportInput2(vid: String,
                             category: String,
                             reportDate: Long,
                             startMileage: Long,
                             stopMileage: Long,
                             totalMileage: Long, //实际行驶里程
                             gpsMileage: Option[Long]
                            )

/**
  *
  * @author zhangyongtian
  * @define 计算里程核查的运营总里程和仪表总里程
  * create 2017-11-17 9:07
  *
  */
object DeadlineMileageJob2 extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))
    println("appName:" + app)

    // TODO: 初始化参数集合
    println("初始化参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    println("日志级别：" + logLevel)

    logLevel match {
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    println("运行模式：" + env)

    //   monthreport.table.partitionColumn=year \
    //   deadlineReport.table.partitionValue=2017-10 \


    // TODO: 验证参数
    logInfo("验证参数....")


   //时间参数
    val yearMonth = stateConf.getOption("deadlineMonth")
      //      .get

      /////////////////////////test/////////////////
      .getOrElse("201711")
    ////////////////////////////////////////////

    //车辆状态参数
    val state = stateConf.getOption("state").getOrElse("run")

    ////////////////////////test///////////////
    //      .getOrElse("201711")
    ///////////////////////////////////////////////

    if (yearMonth.length != 6) {
      throw new Exception("input.month error")
    }

    val year = yearMonth.substring(0, 4)

    val month = yearMonth.substring(4)


    stateConf.set("monthreport.table.partitionColumn", "year")
    stateConf.set("deadlineReport.table.partitionValue", s"${year}-${month}")


    //输出目标
    val outputTargets = stateConf.getOption("output").getOrElse("console")










    // TODO: 加载上下文
    logInfo("加载上下文")

    //    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)

    ////////////////////////test////////////////////////////
    val sparkSession = SparkHelper.getSparkSession(sparkMaster = Option("local[*]"))
    ////////////////////////////////////////////////////////

    val sparkConf = sparkSession.conf
    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
    sparkConf.set("spark.kryo.registrator", classOf[DeadlineMileage2].getName)
    sparkConf.set("spark.kryo.registrator", classOf[MonthReportInput2].getName)

    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //    sparkConf.set("spark.kryo.registrationRequired", "true")

    import sparkSession.implicits._

    //    ////////////////////////test//////////////////////////////////
    //
    // TODO: 注册模拟数据
    MockDataProvider.monthReport(sparkSession)

    //    ////////////////////////////////////////////////


    ////////////////////////////////////////////////
    // TODO: 将数据注册成表
    logInfo("将数据注册成表")

    ////////////////////////////生产环境////////////////////

    sparkSession.sqlContext
      .read
      .format("parquet")
      .load(s"/spark/vehicle/result/monthreport/").createOrReplaceTempView("monthreport")
    //////////////////////////////////////////////////////

    // TODO: 计算
    logInfo("计算")

    // TODO: 截至当前所有月
    val result =
    //0134b9c5-f394-487d-b4b9-82f62714207e run 1509541169000  379520  379590 70 v
      sparkSession.sql(s"SELECT vid, category,reportDate,reportDate,startMileage,stopMileage,totalMileage,gpsMileage" +
        s" FROM monthreport where year <= '${year}' and month <= '${month}' and category='${Constant.TravelState}'")
        .as[MonthReportInput2]
        .groupByKey(_.vid)
        .mapGroups({ case (vid: String, monthValues: Iterator[MonthReportInput2]) =>

          val sortedMonth = Utils.sortByDate2[MonthReportInput2](monthValues.toArray, month => Some(month.reportDate))

          val (accMileage, gpsMileage) = sortedMonth.foldLeft((0L, 0L))((left, right) => {
            (left._1 + right.totalMileage, left._2 + right.gpsMileage.getOrElse(0L))
          })

          val totalMileage = sortedMonth.last.stopMileage - sortedMonth.head.startMileage
          val deadLine = sortedMonth.last.reportDate

          DeadlineMileage2(
            vid,
            deadLine,
            DataPrecision.mileage(accMileage),
            DataPrecision.mileage(totalMileage),
            gpsMileage)
        })

        .filter(x => {
          x.totalMileage < 3000000
        }).coalesce(10)

    //////////////////////////////test////////////////////
    result.show(false)
    ///////////////////////////////////////////////////////
    //    21 4741 9987

    if (outputTargets.split(",").length > 1)
      result.persist(StorageLevel.MEMORY_ONLY_SER)



    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")
    sparkSession.catalog.dropTempView("dayreport")


    if (outputTargets.contains("hdfs")) {
      // TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")
      ///spark/vehicle/result/deadlineReport
      //            SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), "deadlineReport")

      val hdfsPath = s"/spark/vehicle/result/deadlineReport/year=${year}/month=${month}"

      result.toDF().write.mode(SaveMode.Overwrite).parquet(hdfsPath)

      logInfo("输出到HDFS　end....")
    }


    if (outputTargets.contains("hbase")) {
      //TODO: 输出到HBase
      logInfo("输出到HBase start....")

      //生产环境
      var quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89")
      var zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")

      //研发环境 	yf-nn2:2181,yf-dn1:2181,yf-dn2:2181
      if (env.equals("dev")) {
        quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.6.103,192.168.6.104,192.168.6.105")
        zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")
      }


      val htableName = "mileage_check_coords"

      //      hbase.quorum=192.168.2.70,192.168.2.71,192.168.2.89
      //      hbase.zkport=2181


      result.foreachPartition(values => {
        HbaseHelper.bulkPut(quorum, zkport, "mileage_check_coords", table => {
          values.foreach(v => {
            //val deadlineMonth = Utils.formatDate(new Date(v.deadline), "yyyyMM")
            //            val deadlineMonth = "201710"
            val rowKey = Bytes.toBytes(s"${v.vid}_${yearMonth}")
            if (table.exists(new Get(rowKey))) {
              table.put(HbaseHelper.createRow(rowKey, "df", "accMileage", v.accMileage.toString))
              table.put(HbaseHelper.createRow(rowKey, "df", "totalMileage", v.totalMileage.toString))
            }

          })
        })
      })
      logInfo("输出到HBase end ....")
    }

    logInfo("任务完成...")

    sparkSession.stop
  }
}package com.bitnei.report

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.{ Utils}
import com.bitnei.report.constants.Constant
import org.scalatest.FunSuite


/**
  * Created by wangbaosheng on 2017/8/21.
  */
class DeadlineMileageJobTest  extends FunSuite  {
  test("test deadline mileage"){
    //获取开始日期
    val startDate = Utils.parsetDate("201601", "yyyyMM").get
    //获取结束日期
    val endDate = new Date(startDate.getTime)
    endDate.setMonth(endDate.getMonth + 1)

    endDate.setDate(endDate.getDate-1)
    println(Utils.formatDate(startDate),Utils.formatDate(endDate))
//      val md=new MileageCheck(null,null)
//      val values=Utils.readAllLines("C:\\Users\\francis\\Desktop\\aaaa.txt" ,line=>{
//        val fields=line.split(',').map(_.trim).map(x=>(x.toDouble*1000000).toLong)
//        val long=Some(fields(0))
//        val lati=Some(fields(1))
//        RealinfoInput("","",None,long,lati)
//      })
//      val v=md.computeOneVehicle(values)
//
//      println(v.length)

//    val stateConf=new StateConf
//    val sparkSession=SparkHelper.getSparkSession(Some("local"))
//
//    import  sparkSession.implicits._
//
//    val deadLineJob=new DeadlineMileageJob(stateConf,sparkSession)
//
//    sparkSession.createDataset(Array(
//      MonthReportInput("vid1",Constant.TravelState,new Date().getTime,10,100,80),
//      MonthReportInput("vid1",Constant.TravelState,new Date().getTime+1,200,210,10),
//        MonthReportInput("vid2",Constant.TravelState,new Date().getTime,10,100,80)
//    )).createOrReplaceTempView("monthreport")
//
//    deadLineJob.doCompute().collect.foreach(x=> {
//      if(x.vid=="vid1") assert(x.accMileage==90D&&x.totalMileage==200D)
//     // else if(x.vid=="vid2") as
//    })
  }
}
package com.bitnei.report

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.sql.{Dataset, SparkSession}

/*
* created by wangbaosheng on 2017/12/28
* 月里程输出作业，用于将某个月的月里程输出到hbase，主要用来补数据。
*/
class DeadlineMileageOutputJob (stateConf:StateConf, @transient sparkSession: SparkSession) extends Serializable with Job with Logging {
  override type R = DeadlineMileage
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)


  private val monthDate = stateConf.getString("monthDate")
  private val year = Utils.formatDate("yyyyMM", "yyyy", monthDate)
  private val month = Utils.formatDate("yyyyMM", "MM", monthDate)
  logInfo(s"monthDate=$monthDate")


  //日最大里程阈值
  val dayMileageMaxThreshold = stateConf.getInt("dayMileageMaxThreshold", 800)
  //日最小里程阈值
  val dayMileageMinThreshold = stateConf.getInt("dayMileageMinThreshold", 1)

  if (monthDate == Utils.formatDate(new Date, "yyyyMM")) {
    throw new IllegalArgumentException(s"the monthDate must be not equal current month:monthDate=$monthDate")
  }


  private val mileageTableName = stateConf.getOption("mileagecheck.table.name").getOrElse("mileage_check_coords")
  private val dayCoordTable = stateConf.getOption("dayCoord.table.name").getOrElse("mileageCheckDayCoord")
  private val deadlineTable = stateConf.getOption("deadline.table.name").getOrElse("mileageCheckDeadline")

  import sparkSession.sqlContext.implicits._

  override def registerIfNeed() = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf,dayCoordTable)
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, deadlineTable)
  }


  override def unRegister() = {
    sparkSession.catalog.dropTempView("dayreport")
    sparkSession.catalog.dropTempView(dayCoordTable)
  }

  override def doCompute[Product <: DeadlineMileage](): Dataset[DeadlineMileage] = {
    sparkSession.sql(
      s"""
      SELECT *
      FROM $deadlineTable AS deadline
      WHERE year=$year and month=$month
    """.stripMargin).as[DeadlineMileage]
  }

  override def write[Product <: DeadlineMileage](result: Dataset[DeadlineMileage]): Unit = {
    def writeToHbase(quorum: String, zkport: String) = {
      result.foreachPartition(values => {
        HbaseHelper.bulkPut(quorum, zkport, mileageTableName, (table) => {
          val mapper = new ObjectMapper()
          mapper.registerModule(DefaultScalaModule)
          values.foreach(v => {
            val deadline = Utils.formatDate(new Date(v.deadline),"yyyyMM")
            val rowKey = s"${v.vid}_m_$deadline"

            logInfo(rowKey)

            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "vin", v.vin.toString))
            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "accMileage", v.accMileage.toString))
            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "totalMileage", v.totalMileage.toString))
          })
        })
      })
    }

    (stateConf.getOption("hbase.quorum"), stateConf.getOption("hbase.zkport")) match {
      case (Some(quorm), Some(zkport)) =>
        logInfo(s"writting to hbase,$quorm:$zkport")
        writeToHbase(quorm, zkport)
      case _ =>
        throw new RuntimeException("hbase.quorum or hbase.zkport not exists.")
    }
  }
}

object DeadlineMileageOutputJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)
    new DeadlineMileageOutputJob(stateConf, sparkSession).compute()
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils
// $example off$

object DecisionTreeClassificationExample {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("DecisionTreeClassificationExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load and parse the data file.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
    // Split the data into training and test sets (30% held out for testing)
    val splits = data.randomSplit(Array(0.7, 0.3))
    val (trainingData, testData) = (splits(0), splits(1))

    // Train a DecisionTree model.
    //  Empty categoricalFeaturesInfo indicates all features are continuous.
    val numClasses = 2
    val categoricalFeaturesInfo = Map[Int, Int]()
    val impurity = "gini"
    val maxDepth = 5
    val maxBins = 32

    val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
      impurity, maxDepth, maxBins)

    // Evaluate model on test instances and compute test error
    val labelAndPreds = testData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val testErr = labelAndPreds.filter(r => r._1 != r._2).count().toDouble / testData.count()
    println("Test Error = " + testErr)
    println("Learned classification tree model:\n" + model.toDebugString)

    // Save and load model
    model.save(sc, "target/tmp/myDecisionTreeClassificationModel")
    val sameModel = DecisionTreeModel.load(sc, "target/tmp/myDecisionTreeClassificationModel")
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

import scala.collection.mutable
import scala.language.reflectiveCalls

import scopt.OptionParser

import org.apache.spark.examples.mllib.AbstractParams
import org.apache.spark.ml.{Pipeline, PipelineStage, Transformer}
import org.apache.spark.ml.classification.{DecisionTreeClassificationModel, DecisionTreeClassifier}
import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.ml.regression.{DecisionTreeRegressionModel, DecisionTreeRegressor}
import org.apache.spark.ml.util.MetadataUtils
import org.apache.spark.mllib.evaluation.{MulticlassMetrics, RegressionMetrics}
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.sql.{DataFrame, SparkSession}

/**
 * An example runner for decision trees. Run with
 * {{{
 * ./bin/run-example ml.DecisionTreeExample [options]
 * }}}
 * Note that Decision Trees can take a large amount of memory. If the run-example command above
 * fails, try running via spark-submit and specifying the amount of memory as at least 1g.
 * For local mode, run
 * {{{
 * ./bin/spark-submit --class org.apache.spark.examples.ml.DecisionTreeExample --driver-memory 1g
 *   [examples JAR path] [options]
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object DecisionTreeExample {

  case class Params(
      input: String = null,
      testInput: String = "",
      dataFormat: String = "libsvm",
      algo: String = "Classification",
      maxDepth: Int = 5,
      maxBins: Int = 32,
      minInstancesPerNode: Int = 1,
      minInfoGain: Double = 0.0,
      fracTest: Double = 0.2,
      cacheNodeIds: Boolean = false,
      checkpointDir: Option[String] = None,
      checkpointInterval: Int = 10) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("DecisionTreeExample") {
      head("DecisionTreeExample: an example decision tree app.")
      opt[String]("algo")
        .text(s"algorithm (classification, regression), default: ${defaultParams.algo}")
        .action((x, c) => c.copy(algo = x))
      opt[Int]("maxDepth")
        .text(s"max depth of the tree, default: ${defaultParams.maxDepth}")
        .action((x, c) => c.copy(maxDepth = x))
      opt[Int]("maxBins")
        .text(s"max number of bins, default: ${defaultParams.maxBins}")
        .action((x, c) => c.copy(maxBins = x))
      opt[Int]("minInstancesPerNode")
        .text(s"min number of instances required at child nodes to create the parent split," +
          s" default: ${defaultParams.minInstancesPerNode}")
        .action((x, c) => c.copy(minInstancesPerNode = x))
      opt[Double]("minInfoGain")
        .text(s"min info gain required to create a split, default: ${defaultParams.minInfoGain}")
        .action((x, c) => c.copy(minInfoGain = x))
      opt[Double]("fracTest")
        .text(s"fraction of data to hold out for testing. If given option testInput, " +
          s"this option is ignored. default: ${defaultParams.fracTest}")
        .action((x, c) => c.copy(fracTest = x))
      opt[Boolean]("cacheNodeIds")
        .text(s"whether to use node Id cache during training, " +
          s"default: ${defaultParams.cacheNodeIds}")
        .action((x, c) => c.copy(cacheNodeIds = x))
      opt[String]("checkpointDir")
        .text(s"checkpoint directory where intermediate node Id caches will be stored, " +
         s"default: ${defaultParams.checkpointDir match {
           case Some(strVal) => strVal
           case None => "None"
         }}")
        .action((x, c) => c.copy(checkpointDir = Some(x)))
      opt[Int]("checkpointInterval")
        .text(s"how often to checkpoint the node Id cache, " +
         s"default: ${defaultParams.checkpointInterval}")
        .action((x, c) => c.copy(checkpointInterval = x))
      opt[String]("testInput")
        .text(s"input path to test dataset. If given, option fracTest is ignored." +
          s" default: ${defaultParams.testInput}")
        .action((x, c) => c.copy(testInput = x))
      opt[String]("dataFormat")
        .text("data format: libsvm (default), dense (deprecated in Spark v1.1)")
        .action((x, c) => c.copy(dataFormat = x))
      arg[String]("<input>")
        .text("input path to labeled examples")
        .required()
        .action((x, c) => c.copy(input = x))
      checkConfig { params =>
        if (params.fracTest < 0 || params.fracTest >= 1) {
          failure(s"fracTest ${params.fracTest} value incorrect; should be in [0,1).")
        } else {
          success
        }
      }
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  /** Load a dataset from the given path, using the given format */
  private[ml] def loadData(
      spark: SparkSession,
      path: String,
      format: String,
      expectedNumFeatures: Option[Int] = None): DataFrame = {
    import spark.implicits._

    format match {
      case "dense" => MLUtils.loadLabeledPoints(spark.sparkContext, path).toDF()
      case "libsvm" => expectedNumFeatures match {
        case Some(numFeatures) => spark.read.option("numFeatures", numFeatures.toString)
          .format("libsvm").load(path)
        case None => spark.read.format("libsvm").load(path)
      }
      case _ => throw new IllegalArgumentException(s"Bad data format: $format")
    }
  }

  /**
   * Load training and test data from files.
   * @param input  Path to input dataset.
   * @param dataFormat  "libsvm" or "dense"
   * @param testInput  Path to test dataset.
   * @param algo  Classification or Regression
   * @param fracTest  Fraction of input data to hold out for testing. Ignored if testInput given.
   * @return  (training dataset, test dataset)
   */
  private[ml] def loadDatasets(
      input: String,
      dataFormat: String,
      testInput: String,
      algo: String,
      fracTest: Double): (DataFrame, DataFrame) = {
    val spark = SparkSession
      .builder
      .getOrCreate()

    // Load training data
    val origExamples: DataFrame = loadData(spark, input, dataFormat)

    // Load or create test set
    val dataframes: Array[DataFrame] = if (testInput != "") {
      // Load testInput.
      val numFeatures = origExamples.first().getAs[Vector](1).size
      val origTestExamples: DataFrame =
        loadData(spark, testInput, dataFormat, Some(numFeatures))
      Array(origExamples, origTestExamples)
    } else {
      // Split input into training, test.
      origExamples.randomSplit(Array(1.0 - fracTest, fracTest), seed = 12345)
    }

    val training = dataframes(0).cache()
    val test = dataframes(1).cache()

    val numTraining = training.count()
    val numTest = test.count()
    val numFeatures = training.select("features").first().getAs[Vector](0).size
    println("Loaded data:")
    println(s"  numTraining = $numTraining, numTest = $numTest")
    println(s"  numFeatures = $numFeatures")

    (training, test)
  }

  def run(params: Params): Unit = {
    val spark = SparkSession
      .builder
      .appName(s"DecisionTreeExample with $params")
      .getOrCreate()

    params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir)
    val algo = params.algo.toLowerCase

    println(s"DecisionTreeExample with parameters:\n$params")

    // Load training and test data and cache it.
    val (training: DataFrame, test: DataFrame) =
      loadDatasets(params.input, params.dataFormat, params.testInput, algo, params.fracTest)

    // Set up Pipeline.
    val stages = new mutable.ArrayBuffer[PipelineStage]()
    // (1) For classification, re-index classes.
    val labelColName = if (algo == "classification") "indexedLabel" else "label"
    if (algo == "classification") {
      val labelIndexer = new StringIndexer()
        .setInputCol("label")
        .setOutputCol(labelColName)
      stages += labelIndexer
    }
    // (2) Identify categorical features using VectorIndexer.
    //     Features with more than maxCategories values will be treated as continuous.
    val featuresIndexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("indexedFeatures")
      .setMaxCategories(10)
    stages += featuresIndexer
    // (3) Learn Decision Tree.
    val dt = algo match {
      case "classification" =>
        new DecisionTreeClassifier()
          .setFeaturesCol("indexedFeatures")
          .setLabelCol(labelColName)
          .setMaxDepth(params.maxDepth)
          .setMaxBins(params.maxBins)
          .setMinInstancesPerNode(params.minInstancesPerNode)
          .setMinInfoGain(params.minInfoGain)
          .setCacheNodeIds(params.cacheNodeIds)
          .setCheckpointInterval(params.checkpointInterval)
      case "regression" =>
        new DecisionTreeRegressor()
          .setFeaturesCol("indexedFeatures")
          .setLabelCol(labelColName)
          .setMaxDepth(params.maxDepth)
          .setMaxBins(params.maxBins)
          .setMinInstancesPerNode(params.minInstancesPerNode)
          .setMinInfoGain(params.minInfoGain)
          .setCacheNodeIds(params.cacheNodeIds)
          .setCheckpointInterval(params.checkpointInterval)
      case _ => throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }
    stages += dt
    val pipeline = new Pipeline().setStages(stages.toArray)

    // Fit the Pipeline.
    val startTime = System.nanoTime()
    val pipelineModel = pipeline.fit(training)
    val elapsedTime = (System.nanoTime() - startTime) / 1e9
    println(s"Training time: $elapsedTime seconds")

    // Get the trained Decision Tree from the fitted PipelineModel.
    algo match {
      case "classification" =>
        val treeModel = pipelineModel.stages.last.asInstanceOf[DecisionTreeClassificationModel]
        if (treeModel.numNodes < 20) {
          println(treeModel.toDebugString) // Print full model.
        } else {
          println(treeModel) // Print model summary.
        }
      case "regression" =>
        val treeModel = pipelineModel.stages.last.asInstanceOf[DecisionTreeRegressionModel]
        if (treeModel.numNodes < 20) {
          println(treeModel.toDebugString) // Print full model.
        } else {
          println(treeModel) // Print model summary.
        }
      case _ => throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }

    // Evaluate model on training, test data.
    algo match {
      case "classification" =>
        println("Training data results:")
        evaluateClassificationModel(pipelineModel, training, labelColName)
        println("Test data results:")
        evaluateClassificationModel(pipelineModel, test, labelColName)
      case "regression" =>
        println("Training data results:")
        evaluateRegressionModel(pipelineModel, training, labelColName)
        println("Test data results:")
        evaluateRegressionModel(pipelineModel, test, labelColName)
      case _ =>
        throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }

    spark.stop()
  }

  /**
   * Evaluate the given ClassificationModel on data. Print the results.
   * @param model  Must fit ClassificationModel abstraction
   * @param data  DataFrame with "prediction" and labelColName columns
   * @param labelColName  Name of the labelCol parameter for the model
   *
   * TODO: Change model type to ClassificationModel once that API is public. SPARK-5995
   */
  private[ml] def evaluateClassificationModel(
      model: Transformer,
      data: DataFrame,
      labelColName: String): Unit = {
    val fullPredictions = model.transform(data).cache()
    val predictions = fullPredictions.select("prediction").rdd.map(_.getDouble(0))
    val labels = fullPredictions.select(labelColName).rdd.map(_.getDouble(0))
    // Print number of classes for reference.
    val numClasses = MetadataUtils.getNumClasses(fullPredictions.schema(labelColName)) match {
      case Some(n) => n
      case None => throw new RuntimeException(
        "Unknown failure when indexing labels for classification.")
    }
    val accuracy = new MulticlassMetrics(predictions.zip(labels)).accuracy
    println(s"  Accuracy ($numClasses classes): $accuracy")
  }

  /**
   * Evaluate the given RegressionModel on data. Print the results.
   * @param model  Must fit RegressionModel abstraction
   * @param data  DataFrame with "prediction" and labelColName columns
   * @param labelColName  Name of the labelCol parameter for the model
   *
   * TODO: Change model type to RegressionModel once that API is public. SPARK-5995
   */
  private[ml] def evaluateRegressionModel(
      model: Transformer,
      data: DataFrame,
      labelColName: String): Unit = {
    val fullPredictions = model.transform(data).cache()
    val predictions = fullPredictions.select("prediction").rdd.map(_.getDouble(0))
    val labels = fullPredictions.select(labelColName).rdd.map(_.getDouble(0))
    val RMSE = new RegressionMetrics(predictions.zip(labels)).rootMeanSquaredError
    println(s"  Root mean squared error (RMSE): $RMSE")
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel
import org.apache.spark.mllib.util.MLUtils
// $example off$

object DecisionTreeRegressionExample {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("DecisionTreeRegressionExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load and parse the data file.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
    // Split the data into training and test sets (30% held out for testing)
    val splits = data.randomSplit(Array(0.7, 0.3))
    val (trainingData, testData) = (splits(0), splits(1))

    // Train a DecisionTree model.
    //  Empty categoricalFeaturesInfo indicates all features are continuous.
    val categoricalFeaturesInfo = Map[Int, Int]()
    val impurity = "variance"
    val maxDepth = 5
    val maxBins = 32

    val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity,
      maxDepth, maxBins)

    // Evaluate model on test instances and compute test error
    val labelsAndPredictions = testData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val testMSE = labelsAndPredictions.map{ case (v, p) => math.pow(v - p, 2) }.mean()
    println("Test Mean Squared Error = " + testMSE)
    println("Learned regression tree model:\n" + model.toDebugString)

    // Save and load model
    model.save(sc, "target/tmp/myDecisionTreeRegressionModel")
    val sameModel = DecisionTreeModel.load(sc, "target/tmp/myDecisionTreeRegressionModel")
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import scala.language.reflectiveCalls

import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.tree.{impurity, DecisionTree, RandomForest}
import org.apache.spark.mllib.tree.configuration.{Algo, Strategy}
import org.apache.spark.mllib.tree.configuration.Algo._
import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.rdd.RDD
import org.apache.spark.util.Utils

/**
 * An example runner for decision trees and random forests. Run with
 * {{{
 * ./bin/run-example org.apache.spark.examples.mllib.DecisionTreeRunner [options]
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 *
 * Note: This script treats all features as real-valued (not categorical).
 *       To include categorical features, modify categoricalFeaturesInfo.
 */
object DecisionTreeRunner {

  object ImpurityType extends Enumeration {
    type ImpurityType = Value
    val Gini, Entropy, Variance = Value
  }

  import ImpurityType._

  case class Params(
      input: String = null,
      testInput: String = "",
      dataFormat: String = "libsvm",
      algo: Algo = Classification,
      maxDepth: Int = 5,
      impurity: ImpurityType = Gini,
      maxBins: Int = 32,
      minInstancesPerNode: Int = 1,
      minInfoGain: Double = 0.0,
      numTrees: Int = 1,
      featureSubsetStrategy: String = "auto",
      fracTest: Double = 0.2,
      useNodeIdCache: Boolean = false,
      checkpointDir: Option[String] = None,
      checkpointInterval: Int = 10) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("DecisionTreeRunner") {
      head("DecisionTreeRunner: an example decision tree app.")
      opt[String]("algo")
        .text(s"algorithm (${Algo.values.mkString(",")}), default: ${defaultParams.algo}")
        .action((x, c) => c.copy(algo = Algo.withName(x)))
      opt[String]("impurity")
        .text(s"impurity type (${ImpurityType.values.mkString(",")}), " +
          s"default: ${defaultParams.impurity}")
        .action((x, c) => c.copy(impurity = ImpurityType.withName(x)))
      opt[Int]("maxDepth")
        .text(s"max depth of the tree, default: ${defaultParams.maxDepth}")
        .action((x, c) => c.copy(maxDepth = x))
      opt[Int]("maxBins")
        .text(s"max number of bins, default: ${defaultParams.maxBins}")
        .action((x, c) => c.copy(maxBins = x))
      opt[Int]("minInstancesPerNode")
        .text(s"min number of instances required at child nodes to create the parent split," +
          s" default: ${defaultParams.minInstancesPerNode}")
        .action((x, c) => c.copy(minInstancesPerNode = x))
      opt[Double]("minInfoGain")
        .text(s"min info gain required to create a split, default: ${defaultParams.minInfoGain}")
        .action((x, c) => c.copy(minInfoGain = x))
      opt[Int]("numTrees")
        .text(s"number of trees (1 = decision tree, 2+ = random forest)," +
          s" default: ${defaultParams.numTrees}")
        .action((x, c) => c.copy(numTrees = x))
      opt[String]("featureSubsetStrategy")
        .text(s"feature subset sampling strategy" +
          s" (${RandomForest.supportedFeatureSubsetStrategies.mkString(", ")}), " +
          s"default: ${defaultParams.featureSubsetStrategy}")
        .action((x, c) => c.copy(featureSubsetStrategy = x))
      opt[Double]("fracTest")
        .text(s"fraction of data to hold out for testing.  If given option testInput, " +
          s"this option is ignored. default: ${defaultParams.fracTest}")
        .action((x, c) => c.copy(fracTest = x))
      opt[Boolean]("useNodeIdCache")
        .text(s"whether to use node Id cache during training, " +
          s"default: ${defaultParams.useNodeIdCache}")
        .action((x, c) => c.copy(useNodeIdCache = x))
      opt[String]("checkpointDir")
        .text(s"checkpoint directory where intermediate node Id caches will be stored, " +
         s"default: ${defaultParams.checkpointDir match {
           case Some(strVal) => strVal
           case None => "None"
         }}")
        .action((x, c) => c.copy(checkpointDir = Some(x)))
      opt[Int]("checkpointInterval")
        .text(s"how often to checkpoint the node Id cache, " +
         s"default: ${defaultParams.checkpointInterval}")
        .action((x, c) => c.copy(checkpointInterval = x))
      opt[String]("testInput")
        .text(s"input path to test dataset.  If given, option fracTest is ignored." +
          s" default: ${defaultParams.testInput}")
        .action((x, c) => c.copy(testInput = x))
      opt[String]("dataFormat")
        .text("data format: libsvm (default), dense (deprecated in Spark v1.1)")
        .action((x, c) => c.copy(dataFormat = x))
      arg[String]("<input>")
        .text("input path to labeled examples")
        .required()
        .action((x, c) => c.copy(input = x))
      checkConfig { params =>
        if (params.fracTest < 0 || params.fracTest > 1) {
          failure(s"fracTest ${params.fracTest} value incorrect; should be in [0,1].")
        } else {
          if (params.algo == Classification &&
            (params.impurity == Gini || params.impurity == Entropy)) {
            success
          } else if (params.algo == Regression && params.impurity == Variance) {
            success
          } else {
            failure(s"Algo ${params.algo} is not compatible with impurity ${params.impurity}.")
          }
        }
      }
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  /**
   * Load training and test data from files.
   * @param input  Path to input dataset.
   * @param dataFormat  "libsvm" or "dense"
   * @param testInput  Path to test dataset.
   * @param algo  Classification or Regression
   * @param fracTest  Fraction of input data to hold out for testing.  Ignored if testInput given.
   * @return  (training dataset, test dataset, number of classes),
   *          where the number of classes is inferred from data (and set to 0 for Regression)
   */
  private[mllib] def loadDatasets(
      sc: SparkContext,
      input: String,
      dataFormat: String,
      testInput: String,
      algo: Algo,
      fracTest: Double): (RDD[LabeledPoint], RDD[LabeledPoint], Int) = {
    // Load training data and cache it.
    val origExamples = dataFormat match {
      case "dense" => MLUtils.loadLabeledPoints(sc, input).cache()
      case "libsvm" => MLUtils.loadLibSVMFile(sc, input).cache()
    }
    // For classification, re-index classes if needed.
    val (examples, classIndexMap, numClasses) = algo match {
      case Classification =>
        // classCounts: class --> # examples in class
        val classCounts = origExamples.map(_.label).countByValue()
        val sortedClasses = classCounts.keys.toList.sorted
        val numClasses = classCounts.size
        // classIndexMap: class --> index in 0,...,numClasses-1
        val classIndexMap = {
          if (classCounts.keySet != Set(0.0, 1.0)) {
            sortedClasses.zipWithIndex.toMap
          } else {
            Map[Double, Int]()
          }
        }
        val examples = {
          if (classIndexMap.isEmpty) {
            origExamples
          } else {
            origExamples.map(lp => LabeledPoint(classIndexMap(lp.label), lp.features))
          }
        }
        val numExamples = examples.count()
        println(s"numClasses = $numClasses.")
        println(s"Per-class example fractions, counts:")
        println(s"Class\tFrac\tCount")
        sortedClasses.foreach { c =>
          val frac = classCounts(c) / numExamples.toDouble
          println(s"$c\t$frac\t${classCounts(c)}")
        }
        (examples, classIndexMap, numClasses)
      case Regression =>
        (origExamples, null, 0)
      case _ =>
        throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }

    // Create training, test sets.
    val splits = if (testInput != "") {
      // Load testInput.
      val numFeatures = examples.take(1)(0).features.size
      val origTestExamples = dataFormat match {
        case "dense" => MLUtils.loadLabeledPoints(sc, testInput)
        case "libsvm" => MLUtils.loadLibSVMFile(sc, testInput, numFeatures)
      }
      algo match {
        case Classification =>
          // classCounts: class --> # examples in class
          val testExamples = {
            if (classIndexMap.isEmpty) {
              origTestExamples
            } else {
              origTestExamples.map(lp => LabeledPoint(classIndexMap(lp.label), lp.features))
            }
          }
          Array(examples, testExamples)
        case Regression =>
          Array(examples, origTestExamples)
      }
    } else {
      // Split input into training, test.
      examples.randomSplit(Array(1.0 - fracTest, fracTest))
    }
    val training = splits(0).cache()
    val test = splits(1).cache()

    val numTraining = training.count()
    val numTest = test.count()
    println(s"numTraining = $numTraining, numTest = $numTest.")

    examples.unpersist(blocking = false)

    (training, test, numClasses)
  }

  def run(params: Params): Unit = {

    val conf = new SparkConf().setAppName(s"DecisionTreeRunner with $params")
    val sc = new SparkContext(conf)

    println(s"DecisionTreeRunner with parameters:\n$params")

    // Load training and test data and cache it.
    val (training, test, numClasses) = loadDatasets(sc, params.input, params.dataFormat,
      params.testInput, params.algo, params.fracTest)

    val impurityCalculator = params.impurity match {
      case Gini => impurity.Gini
      case Entropy => impurity.Entropy
      case Variance => impurity.Variance
    }

    params.checkpointDir.foreach(sc.setCheckpointDir)

    val strategy
      = new Strategy(
          algo = params.algo,
          impurity = impurityCalculator,
          maxDepth = params.maxDepth,
          maxBins = params.maxBins,
          numClasses = numClasses,
          minInstancesPerNode = params.minInstancesPerNode,
          minInfoGain = params.minInfoGain,
          useNodeIdCache = params.useNodeIdCache,
          checkpointInterval = params.checkpointInterval)
    if (params.numTrees == 1) {
      val startTime = System.nanoTime()
      val model = DecisionTree.train(training, strategy)
      val elapsedTime = (System.nanoTime() - startTime) / 1e9
      println(s"Training time: $elapsedTime seconds")
      if (model.numNodes < 20) {
        println(model.toDebugString) // Print full model.
      } else {
        println(model) // Print model summary.
      }
      if (params.algo == Classification) {
        val trainAccuracy =
          new MulticlassMetrics(training.map(lp => (model.predict(lp.features), lp.label))).accuracy
        println(s"Train accuracy = $trainAccuracy")
        val testAccuracy =
          new MulticlassMetrics(test.map(lp => (model.predict(lp.features), lp.label))).accuracy
        println(s"Test accuracy = $testAccuracy")
      }
      if (params.algo == Regression) {
        val trainMSE = meanSquaredError(model, training)
        println(s"Train mean squared error = $trainMSE")
        val testMSE = meanSquaredError(model, test)
        println(s"Test mean squared error = $testMSE")
      }
    } else {
      val randomSeed = Utils.random.nextInt()
      if (params.algo == Classification) {
        val startTime = System.nanoTime()
        val model = RandomForest.trainClassifier(training, strategy, params.numTrees,
          params.featureSubsetStrategy, randomSeed)
        val elapsedTime = (System.nanoTime() - startTime) / 1e9
        println(s"Training time: $elapsedTime seconds")
        if (model.totalNumNodes < 30) {
          println(model.toDebugString) // Print full model.
        } else {
          println(model) // Print model summary.
        }
        val trainAccuracy =
          new MulticlassMetrics(training.map(lp => (model.predict(lp.features), lp.label))).accuracy
        println(s"Train accuracy = $trainAccuracy")
        val testAccuracy =
          new MulticlassMetrics(test.map(lp => (model.predict(lp.features), lp.label))).accuracy
        println(s"Test accuracy = $testAccuracy")
      }
      if (params.algo == Regression) {
        val startTime = System.nanoTime()
        val model = RandomForest.trainRegressor(training, strategy, params.numTrees,
          params.featureSubsetStrategy, randomSeed)
        val elapsedTime = (System.nanoTime() - startTime) / 1e9
        println(s"Training time: $elapsedTime seconds")
        if (model.totalNumNodes < 30) {
          println(model.toDebugString) // Print full model.
        } else {
          println(model) // Print model summary.
        }
        val trainMSE = meanSquaredError(model, training)
        println(s"Train mean squared error = $trainMSE")
        val testMSE = meanSquaredError(model, test)
        println(s"Test mean squared error = $testMSE")
      }
    }

    sc.stop()
  }

  /**
   * Calculates the mean squared error for regression.
   *
   * This is just for demo purpose. In general, don't copy this code because it is NOT efficient
   * due to the use of structural types, which leads to one reflection call per record.
   */
  // scalastyle:off structural.type
  private[mllib] def meanSquaredError(
      model: { def predict(features: Vector): Double },
      data: RDD[LabeledPoint]): Double = {
    data.map { y =>
      val err = model.predict(y.features) - y.label
      err * err
    }.mean()
  }
  // scalastyle:on structural.type
}
// scalastyle:on println
package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.dayreport.Model._
import com.bitnei.report.dayreport.alarm.AlarmStateModelManager
import com.bitnei.report.dayreport.validate.ValidateModelManager

/**
  * Created by wangbaosheng on 2017/3/31.
  */
class DeleteDayReportData(stateConf:StateConf,reportDate:String)extends  Logging{
  def run(): Unit ={
    logWarning(s"开始删除${reportDate}充电日报表的数据")
    new ChargeStateDayReportManager(stateConf).delete(reportDate)
    logWarning(s"开始删除${reportDate}满电日报表的数据")
    new FullChargeStateDayReportModelManager(stateConf).delete(reportDate)
    logWarning(s"开始删除${reportDate}行驶日报表的数据")
    new RunStateDayReportManager(stateConf).delete(reportDate)
    logWarning(s"开始删除${reportDate}分类日报表的数据")
    new CategoryDayReportManager(stateConf).delete(reportDate)
    logWarning(s"开始删除${reportDate}报警日报表的数据")
    new AlarmStateModelManager(stateConf).delete(reportDate)
    logWarning(s"开始删除${reportDate}数据有效性表的数据")
    new ValidateModelManager(stateConf).delete(reportDate)

  }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.log4j.{Level, Logger}
import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors

/**
 * An example k-means app. Run with
 * {{{
 * ./bin/run-example org.apache.spark.examples.mllib.DenseKMeans [options] <input>
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object DenseKMeans {

  object InitializationMode extends Enumeration {
    type InitializationMode = Value
    val Random, Parallel = Value
  }

  import InitializationMode._

  case class Params(
      input: String = null,
      k: Int = -1,
      numIterations: Int = 10,
      initializationMode: InitializationMode = Parallel) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("DenseKMeans") {
      head("DenseKMeans: an example k-means app for dense data.")
      opt[Int]('k', "k")
        .required()
        .text(s"number of clusters, required")
        .action((x, c) => c.copy(k = x))
      opt[Int]("numIterations")
        .text(s"number of iterations, default: ${defaultParams.numIterations}")
        .action((x, c) => c.copy(numIterations = x))
      opt[String]("initMode")
        .text(s"initialization mode (${InitializationMode.values.mkString(",")}), " +
        s"default: ${defaultParams.initializationMode}")
        .action((x, c) => c.copy(initializationMode = InitializationMode.withName(x)))
      arg[String]("<input>")
        .text("input paths to examples")
        .required()
        .action((x, c) => c.copy(input = x))
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"DenseKMeans with $params")
    val sc = new SparkContext(conf)

    Logger.getRootLogger.setLevel(Level.WARN)

    val examples = sc.textFile(params.input).map { line =>
      Vectors.dense(line.split(' ').map(_.toDouble))
    }.cache()

    val numExamples = examples.count()

    println(s"numExamples = $numExamples.")

    val initMode = params.initializationMode match {
      case Random => KMeans.RANDOM
      case Parallel => KMeans.K_MEANS_PARALLEL
    }

    val model = new KMeans()
      .setInitializationMode(initMode)
      .setK(params.k)
      .setMaxIterations(params.numIterations)
      .run(examples)

    val cost = model.computeCost(examples)

    println(s"Total cost = $cost.")

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.detail

import java.sql.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, TimeParser, Utils}
import com.bitnei.report.{Compute, InvalidateValue}
import com.bitnei.report.constants.Constant
import com.bitnei.report.stateGenerate._
import com.bitnei.report.common.{GpsDistance, GpsMileage}
import com.bitnei.report.detail.distribution.PowerDistribution
import org.apache.spark.sql.catalyst.expressions.aggregate.Count

import scala.annotation.tailrec
import scala.collection.mutable.ListBuffer


/**
  * Created by wangbaosheng on 2017/3/27.
  *
  * 计算充电/满电/形势状态的明细报表，比如计算某次充电的开始/结束soc，最高/最低电压
  *
  * @param window 要计算的状态，比如充电状态
  * @param prevResult 当前状态的的上一个状态
  * @param prevChargeResult 当前状态的上一个充电状态
  */

class DetailCompute(
                     stateConf:StateConf,
                     window: Window[RealinfoModel],
                     prevResult:Option[DetailModel],//上一个状态的计算结果
                     prevChargeResult:Option[DetailModel]//上一个充电状态的计算结果
                   )extends  Serializable with Logging with  Compute {
  override type Result = DetailModel

  class InvalidateWindow{
    var startTime: String=""
    var endTime: String=""
    var totalCount: Int=0
    var emptryCount:Int=0
  }

  /**
    * 1.快慢充判断逻辑：
    *  如果在充电开始的quickWindowMaxTime分钟内(默认值为5分钟)，有70%的数据的电流小于-20A，那么认为这是一个快充。
    * 2.无效行驶判断逻辑：
    *   2.1 统计连续里程没有发生变化的最大窗口，continueMileageWindow
    *   2.2 如果continueMileageWindow的时间范围超过了10分钟，那么统计continueMileageWindow内速度为0，挡位为空的比例，如果超过70%，那么认为这个
    *   窗口是无效行驶。
    * */
  override def compute(): DetailModel = {
    val startTimeOption = Utils.parsetDate(window.head.time)
    val (startTime: Long, startH: Int) = startTimeOption match {
      case Some(startDate) =>
        if (startDate.getMinutes == 0) (startDate.getTime, startDate.getHours)
        else (startDate.getTime, startDate.getHours + 1)
      case None =>
        (0, -1)
    }

    val endTimeOption = Utils.parsetDate(window.last.time)
    val (endTime: Long, endH: Int) = endTimeOption match {
      case Some(endDate) =>
        if (endDate.getMinutes == 0) (endDate.getTime, endDate.getHours)
        else (endDate.getTime, endDate.getHours + 1)
      case None =>
        (0, -1)
    }

    val state = window.getState

    val timeRange = endTime - startTime

    //开始里程
    var startMileage: Int = Int.MaxValue
    //结束里程
    var stopMileage: Int = 0
    var maxSpeed: Int = 0
    //最大总电压
    var maxTotalVoltage: Int = 0
    var minTotalVoltage: Int = Int.MaxValue

    //最大总电流
    var maxTotalEctriccurrent: Int = Int.MinValue
    var minTotalEctriccurrent: Int = Int.MaxValue

    //最大单体电压
    var maxSecondaryVolatage: Int = 0
    var minSecondaryVolatage: Int = Int.MaxValue

    //最大采集温度
    var maxAcquisitionPointTemp: Int = Int.MinValue
    var minAcquisitionPointTemp: Int = Int.MaxValue

    //最大发电机温度
    var maxEngineTemp: Int = Int.MinValue
    var minEngineTemp: Int = Int.MaxValue

    //最大soc
    var maxSoc: Int = 0
    var minSoc: Int = Int.MaxValue

    //总电流
    var totalCharge: Double = 0

    //开始经度
    var startLongitude = 0L
    //开始纬度
    var startLatitude = 0L
    var endLongitude = 0L
    var endLatitude = 0L

    //gps里程
    var gpsMileageM = 0D

    //开始soc
    var startSoc = 0
    var endSoc = 0


    //上一条实时数据
    var prevRealinfo: Option[RealinfoModel] = None
   //充电量分布
    val chargeDistributed = new PowerDistribution()
    //上一条明细
    var prevDetail: Option[RealinfoModel] = None

    //累计无效行驶时长和当前无效行驶时长
    var accInvalidateTime = 0
    //无效行驶滑动窗口长度
    var invalidateRunWindowSlidingNum = 0
    //无效行驶滑动窗口持续时间
    var invalidateRunWindowSlidingTime = 0
    //连续里程窗口总元素个数
    var continueMileageWindowTotalNum = 0


    //快充最大时间窗口，默认为5分钟
    var quickWindowMaxTime = stateConf.getOption("quickWindowMaxTime").map(TimeParser.parserAsMs(_).get).getOrElse((5 * 60 * 1000))
    //快充窗口的滑动时间，最大为quickWindowMaxTime分钟。
    var quickWindowSlidingTime = 0
    var quickChargeSlidingNum = 0
    var quickWindowMaxNum = 0
    var quickThreshold = stateConf.getOption("quickThreshold").map(_.toInt).getOrElse(-200)

    window.foreach(curRealinfo => {
      val timeDfMs: Int = if (prevRealinfo.isEmpty) 0 else {
        Utils.timeDiff(curRealinfo.time, prevRealinfo.get.time).toInt
      }

      val timeDfS = timeDfMs / 1000

      //统计GPS里程
      if (prevDetail.nonEmpty) {
        val diffMileageM = getDistanceM(prevDetail.get.longitude, prevDetail.get.latitude, curRealinfo.latitude, curRealinfo.longitude, timeDfS)
        gpsMileageM += diffMileageM
        //当前有效
        if (diffMileageM != 0) {
          prevDetail = Some(curRealinfo)
        }
      }

      //统计充电分布
      chargeDistributed.add(prevRealinfo, Some(curRealinfo))

      /** * 计算无效行驶时间
        * 首先设置一个时间窗口，假设为10分钟，如果这十分钟内里程没有变化，并且速度为0和挡位为空的比例超过70%，那么认为这是无效行驶。
        */
      prevRealinfo match {
        case Some(prev) =>
          if (curRealinfo.mileage == prev.mileage) {
            if ((curRealinfo.speed.nonEmpty && curRealinfo.speed.contains(0)) |
              (curRealinfo.peaking.nonEmpty && curRealinfo.peaking.contains(0))) {
              invalidateRunWindowSlidingNum += 1
              invalidateRunWindowSlidingTime += timeDfMs
            }

            continueMileageWindowTotalNum += 1
          } else {
            if (invalidateRunWindowSlidingTime >= invalidateRunWindowMaxTime && (invalidateRunWindowSlidingNum >= continueMileageWindowTotalNum * invalidatePercent)) {
              accInvalidateTime += invalidateRunWindowSlidingTime
            }
            invalidateRunWindowSlidingNum = 0
            invalidateRunWindowSlidingTime = 0
            continueMileageWindowTotalNum = 0
          }
        case _ =>
      }

      prevRealinfo = Some(curRealinfo)

      if (startSoc == 0 && !curRealinfo.soc.contains(0)) startSoc = curRealinfo.soc.get
      if (!curRealinfo.soc.contains(0)) endSoc = curRealinfo.soc.get


      //计算快慢充
      if (quickWindowSlidingTime < quickWindowMaxTime) {
        quickWindowSlidingTime += timeDfMs
        if (curRealinfo.charge.exists(_ < quickThreshold)) quickChargeSlidingNum += 1
        quickWindowMaxNum += 1
      }

      //计算开始里程和结束里程
      curRealinfo.mileage match {
        case Some(mileage) if mileage != 0 =>
          if (mileage < startMileage) startMileage = mileage
          if (mileage > stopMileage) stopMileage = mileage
        case _ =>
      }


      //计算最大速度
      curRealinfo.speed match {
        case Some(speed) => if (speed > maxSpeed) maxSpeed = speed
        case None =>
      }

      //计算最大总电压和最小总电压
      val voltage = curRealinfo.totalVoltage
      voltage match {
        case Some(totalVoltage) if totalVoltage != 0 =>
          if (totalVoltage > maxTotalVoltage) maxTotalVoltage = totalVoltage
          if (totalVoltage < minTotalVoltage) minTotalVoltage = totalVoltage

        case _ =>
      }


      val current = curRealinfo.charge
      current match {
        case Some(totalEctriccurrent) =>
          if (totalEctriccurrent > maxTotalEctriccurrent) maxTotalEctriccurrent = totalEctriccurrent
          if (totalEctriccurrent < minTotalEctriccurrent) minTotalEctriccurrent = totalEctriccurrent
        case None =>
      }


      curRealinfo.secondaryCellMaxVoltage match {
        case Some(secondaryMaxVoltage) => if (secondaryMaxVoltage > maxSecondaryVolatage) maxSecondaryVolatage = secondaryMaxVoltage
        case None =>
      }

      curRealinfo.secondaryCellMinVoltage match {
        case Some(secondaryMinVoltage) if secondaryMinVoltage != 0 => if (secondaryMinVoltage < minSecondaryVolatage) minSecondaryVolatage = secondaryMinVoltage
        case _ =>
      }

      curRealinfo.accuisitionPointMaxTemp match {
        case Some(acquisitionMaxPointTemp) =>
          if (acquisitionMaxPointTemp > maxAcquisitionPointTemp) maxAcquisitionPointTemp = acquisitionMaxPointTemp
        case None =>
      }


      curRealinfo.accuisitionPointMinTemp match {
        case Some(acquisitionMinPointTemp) =>
          if (acquisitionMinPointTemp < minAcquisitionPointTemp) minAcquisitionPointTemp = acquisitionMinPointTemp
        case None =>
      }

      curRealinfo.engineTemp match {
        case Some(engineTemp) =>
          if (engineTemp > maxEngineTemp) maxEngineTemp = engineTemp
          if (engineTemp < minEngineTemp) minEngineTemp = engineTemp
        case None =>
      }


      curRealinfo.soc match {
        case Some(soc) =>
          if (soc > maxSoc) maxSoc = soc
          if (soc < minSoc) minSoc = soc
        case None =>
      }


      voltage match {
        case Some(totalVoltage) =>
          current match {
            case Some(totalCurrent) =>

              val charge: Double = DataPrecision.totalCharge(totalCurrent, totalVoltage, timeDfS)
              totalCharge += charge
            case None =>
          }
        case None =>
      }

      if (startLongitude == 0L) startLongitude = curRealinfo.longitude.getOrElse(0L)
      if (startLatitude == 0L) startLatitude = curRealinfo.latitude.getOrElse(0L)

      if (curRealinfo.longitude.getOrElse(0L) != 0) endLongitude = curRealinfo.longitude.getOrElse(0L)
      if (curRealinfo.latitude.getOrElse(0L) != 0) endLatitude = curRealinfo.latitude.getOrElse(0L)
    })

    if (invalidateRunWindowSlidingTime >= invalidateRunWindowMaxTime && (invalidateRunWindowSlidingNum >= continueMileageWindowTotalNum * invalidatePercent)) {
      accInvalidateTime += invalidateRunWindowSlidingTime
    }


    //计算平均速度
    def getAvgSpeed: Double = if (timeRange != 0) DataPrecision.mileage(stopMileage - startMileage) / DataPrecision.toHour(timeRange) else 0

    //计算剔除无效行驶后的实际运行时间
    val accRunTime = timeRange.toInt - accInvalidateTime

    // val accRunTimeHour = accRunTime / (1000 * 3600).toDouble

    //判断是否是快慢充
    val isQuickCharge = quickChargeSlidingNum >= (
      quickWindowMaxNum * stateConf.getOption("quick.percent").map(_.toDouble).getOrElse(0.7))

    def getOrInvalidate(v:Int):Int=if(v==Int.MaxValue||v==Int.MinValue) InvalidateValue.Value else v


    DetailModel(
      vid = window.head.vid,
      vin = window.head.vin,
      category = state,
      onlineTime = window.onLineTime.toInt,
      startTime = startTime,
      endTime = endTime,
      timeLeng = timeRange,
      accRunTime = timeRange.toInt - accInvalidateTime,
      startMileageOfCurrentDay = window.startMileage,
      endMileageOfCurrentDay = window.endMileage,
      startMileage = if (startMileage == Int.MaxValue) 0 else startMileage,
      stopMileage = stopMileage,
      gpsMileage = (gpsMileageM * 100).toInt,
      avgSpeed = getAvgSpeed,
      maxSpeed = maxSpeed,

      maxTotalVoltage = maxTotalVoltage,
      minTotalVoltage = Utils.roundMin(minTotalVoltage),

      maxTotalCurrent = maxTotalEctriccurrent,
      minTotalCurrent = Utils.roundMin(minTotalEctriccurrent),
      maxSecondaryVolatage = maxSecondaryVolatage,
      minSecondaryVolatage = Utils.roundMin(minSecondaryVolatage),
      maxAcquisitionPointTemp =if(maxAcquisitionPointTemp==Int.MinValue) None else Some(maxAcquisitionPointTemp),
      minAcquisitionPointTemp = Utils.roundMin(minAcquisitionPointTemp),
      maxEngineTemp = if(maxEngineTemp==Int.MinValue) None else Some(maxEngineTemp),
      minEngineTemp = Utils.roundMin(minEngineTemp),
      maxSoc = maxSoc,
      minSoc = Utils.roundMin(minSoc),
      startSoc = startSoc,
      endSoc = endSoc,
      startLongitude = startLongitude,
      startLatitude = startLatitude,
      endLongitude = endLongitude,
      endLatitude = endLatitude,
      totalCharge = totalCharge,
      Math.abs(startTime - prevChargeResult.map(_.endTime).getOrElse(startTime)).toInt,
      prevChargeResult.map(_.stopMileage).getOrElse(0),
      prevResult.map(_.endTime).getOrElse(0),
      prevResult.map(_.maxTotalCurrent).getOrElse(0),
      isQuickCharge = isQuickCharge,
      powerDistribution = chargeDistributed.getDistribution
    )
  }




  //无效行驶时间窗口
  lazy val invalidateRunWindowMaxTime:Int = stateConf.getOption("currentInvalidateTime") match {
    case Some(w) => TimeParser.parserAsMs(w).get
    case None => 7 * 60 * 1000
  }


  lazy val invalidatePercent:Double = stateConf.getOption("invalidatePercentInInvalidateWindow").map(_.toDouble).getOrElse(0.73D)


  def getDistanceM(longitudeA: Option[Long], latitudeA: Option[Long], longitudeB: Option[Long], latitudeB: Option[Long], timeDfS: Long): Double = {
    val diffMileage = if (latitudeA.nonEmpty && longitudeA.nonEmpty && latitudeB.nonEmpty && longitudeB.nonEmpty) {
      GpsDistance.getDistance(
        DataPrecision.latitude(longitudeA.get),
        DataPrecision.latitude(latitudeA.get),
        DataPrecision.latitude(latitudeB.get),
        DataPrecision.latitude(longitudeB.get)
      )
    } else {
      0D
    }

    val maxMileageStepM_S = stateConf.getOption("run.speed").map(_.toDouble).getOrElse(0.1) * 1000
    //如果每秒行驶的距离大于run.speed，那么认为无效
    val speedM_S = (diffMileage * 1000) / timeDfS
    if (speedM_S >= maxMileageStepM_S || speedM_S <= 0) 0 else diffMileage
  }
}



class ChargeDetailCompute(   stateConf:StateConf,
                             window: Window[RealinfoModel],
                             prevChargeResult:Option[DetailModel]) extends DetailCompute(stateConf,window,None,prevChargeResult) {}


class RunDetailCompute(   stateConf:StateConf,
                          window: Window[RealinfoModel],
                          prevResult:Option[DetailModel]) extends DetailCompute(stateConf,window,prevResult,None) {}


object DetailCompute {

  /**
    * 计算一辆车的每一个状态的明细报表
    *
    * @param windows :按照时间排序的状态集合。
    * @return 返回每一个状态的计算结果。
    **/
  def computeAllStates(stateConf: StateConf, windows: List[Window[RealinfoModel]]): List[DetailModel] = {
    val result = new ListBuffer[DetailModel]

    val oneValidateRunTime = stateConf.getOption("OneValidateRunTime") match {
      case Some(v) => TimeParser.parserAsMs(v).get
      case None => 3 * 60 * 1000
    }


      @tailrec
    def doGetResult(curIndex: Int, prevState: String, prevResult: Option[DetailModel], prevChargeResult: Option[DetailModel]): List[DetailModel] = {
      if (curIndex < windows.length) {
        val curWindow = windows(curIndex)
        val (curResult, curState) = curWindow match {
          case run: TravelWindow[RealinfoModel] =>
            //如果当前状态的上一个状态是充电状态，那么将会使用这个状态来计算当前结果，否则不会使用。
            val curResult = if (prevState == Constant.ChargeState) new RunDetailCompute(stateConf, curWindow, prevResult).compute()
            else new RunDetailCompute(stateConf, curWindow, None).compute()
            if (curResult.timeLeng >= oneValidateRunTime && curResult.accRunTime >= oneValidateRunTime && curResult.mileage > 0) {
              result.append(curResult)
              (Some(curResult), Constant.TravelState)
            }else (prevResult, prevState)
          case full: FullChargeWindow[RealinfoModel] =>
            val curResult = new DetailCompute(stateConf, curWindow, None, None).compute()
            result.append(curResult)
            (Some(curResult), Constant.FullChargeState)
          case e if e.isInstanceOf[ChargeWindow[RealinfoModel]] || e.isInstanceOf[ChargeChangeWindow[RealinfoModel]] =>
            //如果存在上一个充电状态，那么将会使用这个充电状态的结果来计算当前结果，比如计算两次充个电时间间隔就需要使用上一个充电结果。
            val curResult = if (prevChargeResult.isEmpty) new ChargeDetailCompute(stateConf, curWindow, None).compute()
            else new ChargeDetailCompute(stateConf, curWindow, prevChargeResult).compute()
            result.append(curResult)
            (Some(curResult), Constant.ChargeState)
          case _ =>
            throw new RuntimeException
        }

        //如果当前状态是充电状态，那么需要更新充电状态
        doGetResult(curIndex + 1, curState, curResult, if (curState == Constant.ChargeState) curResult else prevChargeResult)
      } else {
        result.toList
      }
    }

    doGetResult(0, "", None, None)
  }
}


//package com.bitnei.report.detail
//
//import java.util.Date
//
//import com.bitnei.report.detail.schema.DetailModel
//import org.scalatest.FunSuite
//
///**
//  * Created by wangbaosheng on 2017/5/9.
//  */
//class DetailHaseOutputTest extends FunSuite {
//  test("insert date to habse") {
//
////    def setFilter(): Scan = {
////      val scan = new Scan()
////      val startRow = Bytes.toBytes(s"12333_2222")
////      val stopRow = Bytes.toBytes(s"12333_2222")
////
////      scan.setStartRow(startRow)
////      scan.setStopRow(stopRow)
////
////      scan
////    }
////    val realinfoRDD = SparkHelper.readFromHbase(SparkHelper.getSparkSession(sparkMaster = Some("local")),
////      "192.168.2.70",
////      "2181",
////      "report_detail",
////      setFilter
////    )
////
////
////    realinfoRDD.foreach({case (w,r)=>
////        val cells=r.rawCells()
////        cells.foreach(cell=>{
////          val v=Bytes.toString(cell.getValueArray,cell.getValueOffset,cell.getValueLength)
////          println(v)
////        })
////    })
//
//    val output = new DetailHbaseOutput
//
//    output.insert("192.168.2.70,192.168.2.71,192.168.2.89", "2181", "report_detail",false,
//      Array(DetailModel("764a17fd-62c6-4f17-a4f5-94065043d782","vin", "RUN", 190000, new Date().getTime, new Date().getTime, 0,0,190000, 1000, 1100, 1.9, 10,100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,
//        10, 100D, 19000, 100, new Date().getTime, 100,null)).iterator,v=>v.vid)
//  }
//
//}
package com.bitnei.report.detail

import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.sparkhelper.HbaseHelper
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.{Connection, Row, Table}
import org.apache.hadoop.hbase.util.Bytes


/**
  * Created by wangbaosheng on 2017/5/9.
  */
class DetailHbaseOutput extends Serializable with Logging{
  /**
    * @param zkQuorum zookeeper address
    * @param zkPort zookeeper port
    * @param tableName hbase table ame
    * @param origin true if use raw data format
    * @param iter values
    * @param getRowKey get row key of the DetailModel
    * */
  def insert(zkQuorum: String, zkPort: String, tableName: String, origin:Boolean,iter: Iterator[DetailModel],getRowKey:(DetailModel)=>String): Unit = {
    var hbaseClient: Connection = null
    var table: Table = null

//    try {
//      hbaseClient = HbaseHelper.getConnection(zkQuorum, zkPort)
//      table = hbaseClient.getTable(TableName.valueOf(tableName))
//
//      def toRow(iter: Iterator[DetailModel]): java.util.List[Row] = {
//
//
//        val batch = new java.util.ArrayList[Row]
//
//        iter.foreach(v => {
//          val rowKey=Bytes.toBytes(getRowKey(v))
//
//          batch.add(HbaseHelper.createRow(rowKey, "other", "category",if(origin) v.category.toString else  v.category.toString))
//          batch.add(HbaseHelper.createRow(rowKey, "other", "onlineTime", if(origin)v.onlineTime.toString else Utils.timeToH(v.onlineTime).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "other", "endTime", v.endTime.toString))
//          batch.add(HbaseHelper.createRow(rowKey, "other", "timeLeng",if(origin) v.timeLeng.toString else  Utils.timeToH(v.timeLeng).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "other", "totalCharge", v.totalCharge.toString))
//          batch.add(HbaseHelper.createRow(rowKey, "other", "timeBetweenCharge",if(origin) v.timeBetweenCharge.toString else  Utils.timeToH(v.timeBetweenCharge).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "other", "stopMileageOfPrevCharge",if(origin)v.stopMileageOfPrevCharge.toString else  DataPrecision.mileage(v.stopMileageOfPrevCharge).toString))
//
//
//          //极值数据
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "maxTotalVoltage", if(origin) v.maxTotalVoltage.toString else DataPrecision.totalVoltage(v.maxTotalVoltage).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "minTotalVoltage", if(origin) v.minTotalVoltage.toString else DataPrecision.totalVoltage(v.minTotalVoltage).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "maxTotalCurrent", if(origin) v.maxTotalCurrent.toString else DataPrecision.totalCurrent(v.maxTotalCurrent).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "minTotalCurrent", if(origin)v.minTotalCurrent.toString else DataPrecision.totalCurrent(v.minTotalCurrent).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "maxSecondaryVolatage", if(origin) v.maxSecondaryVolatage.toString else DataPrecision.secondaryVoltage(v.maxSecondaryVolatage).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "minSecondaryVolatage", if(origin) v.minSecondaryVolatage.toString else DataPrecision.secondaryVoltage(v.minSecondaryVolatage).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "maxAcquisitionPointTemp", if(origin)v.maxAcquisitionPointTemp.toString else v.maxAcquisitionPointTemp.toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "minAcquisitionPointTemp", if(origin)v.minAcquisitionPointTemp.toString else v.minAcquisitionPointTemp.toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "maxEngineTemp", if(origin) v.maxEngineTemp.toString else v.maxEngineTemp.toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "minEngineTemp",if(origin)v.minEngineTemp.toString else  v.minEngineTemp.toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "maxSoc", if(origin)v.maxSoc.toString else DataPrecision.soc(v.maxSoc).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "minSoc",if(origin)v.minSoc.toString else  DataPrecision.soc(v.minSoc).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "limit", "startSoc", if(origin)v.startSoc.toString else DataPrecision.soc(v.startSoc).toString))
//
//          //行驶数据
//          batch.add(HbaseHelper.createRow(rowKey, "run", "startLongitude",if(origin)v.startLongitude.toString else DataPrecision.latitude(v.startLongitude).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "startLatitude", if(origin)v.startLatitude.toString else DataPrecision.latitude(v.startLatitude).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "endLongitude", if(origin)v.endLongitude.toString else DataPrecision.latitude(v.endLongitude).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "endLatitude", if(origin)v.endLatitude.toString else DataPrecision.latitude(v.endLatitude).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "startMileage",if(origin)v.startMileage.toString else  DataPrecision.mileage(v.startMileage).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "stopMileage",if(origin)v.stopMileage.toString else  DataPrecision.mileage(v.stopMileage).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "avgSpeed",if(origin)v.avgSpeed.toString else  DataPrecision.speed(v.avgSpeed).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "maxSpeed",if(origin)v.maxSpeed.toString else  DataPrecision.speed(v.maxSpeed).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "maxCurrentOfPrevCharge", if(origin)v.maxCurrentOfPrevCharge.toString else DataPrecision.totalCurrent(v.maxCurrentOfPrevCharge).toString))
//          batch.add(HbaseHelper.createRow(rowKey, "run", "prevChargeStopTime",if(origin)v.prevChargeStopTime.toString else  v.prevChargeStopTime.toString))
//        })
//
//        batch
//      }
//
//      table.batch(toRow(iter))
//    } catch {
//      case e:Exception=>
//        logError(e.getMessage)
//        iter.foreach(v=>logError(s"write to $tableName failed $v"))
//    }finally {
//      if (table != null) table.close()
//      if (hbaseClient != null) hbaseClient.close()
//    }
  }

}

package com.bitnei.report.detail

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{SystemClock, Utils}
import com.bitnei.report.{AutoPartition, Job, JobRunner}
import com.bitnei.report.detail.FaultDetail.FaultDetailCompute
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.{Dataset, SaveMode, SparkSession}

/**
  * Created by wangbaosheng on 2017/3/27.
  */
class DetailJob(  @transient sparkSession:SparkSession, stateConf:StateConf)  extends Serializable
  with Logging
  with Job
  with AutoPartition {
  override type R = DetailModel

  private val clock = new SystemClock
  private val sqlContext = sparkSession.sqlContext
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)

  import sparkSession.implicits._

  //输入表名
  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("realinfo")

  //输出表名
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("detail")

  def realinfoResult: Dataset[DetailModel] = doCompute()


  val enableEmergency=stateConf.getOption("emergency.enable").contains("true")
  val (emergencyDate)=stateConf.getOption("emergency.date") match {
    case Some(emergencyDate)=>
      Utils.parsetDate(emergencyDate,"yyyyMMdd").get
    case None=>
      val yesterday=new Date()
      yesterday.setDate(yesterday.getDate-1)
      yesterday
  }

  val (emergencyYear,emergencyMonth,emergencyDay)=(117,10,30)

  //注册输入表
  override def registerIfNeed(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,inputTableName)
  }

  override def doCompute[Product <: DetailModel](): Dataset[DetailModel] = {
    val whereCondition = SqlHelper.buildWhereConditionBasePartitionColumn(SparkHelper.getTableInfo(stateConf, inputTableName)).get

    val sql =
      s"""
      SELECT CAST (VID AS String),
             VIN,
             TIME,
             `2201` AS speed,
             `2614` AS charge,
             `2615` AS soc,
             `2613` AS totalVoltage,
             `2603` AS secondaryCellMaxVoltage,
             `2606` AS secondaryCellMinVoltage,
             `2304` AS engineTemp,
             `2609` AS accuisitionPointMaxTemp,
             `2612` AS accuisitionPointMinTemp,
             `2202` AS mileage,
             `2502` AS longitude,
             `2503` AS latitude,
             CAST(`2203` AS INT) AS peaking
       FROM $inputTableName
       WHERE vid IS NOT NULL AND $whereCondition
    """.stripMargin

    //读取输入数据
    val realinfoDs = sparkSession.sql(sql).as[RealinfoModel]

    val result = realinfoDs.groupByKey(_.vid)
      .flatMapGroups({ case (vid: String, rows: Iterator[RealinfoModel]) =>
        val values=if(enableEmergency){
          rows.map(r=>{
            val srcDate=Utils.parsetDate(r.time).get
            srcDate.setYear(emergencyYear)
            srcDate.setMonth(emergencyMonth)
            srcDate.setDate(emergencyDay)
            val toDate=Utils.formatDate(srcDate)
            r.copy(time = toDate)
          }).toArray
        }else{
          rows.toArray
        }

        //对数据按照时间排序并去重
        val sortedRows = Utils.strict[RealinfoModel, String](
          Utils.sortByDate(values, row => {
            if (row.time.trim.isEmpty) {
              logWarning(row.toString)
              None
            } else Some(row.time)
          }),
          row => Some(row.time)
        )

        val cmp: ReportDetailCompute = stateConf.getString("detail.compute") match {
          case "reportDetail" => new DetailComputeBase()
          case "faultDetail" => new FaultDetailCompute()
          case e => throw new RuntimeException(s"detail.compute=$e not support")
        }

        //明细计算
        val detailsResult = cmp.compute(stateConf, sortedRows)
          detailsResult
      })

    result.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(10))
  }

  override def write[Product <: DetailModel](result: Dataset[DetailModel]) = {
    val outptuModels = stateConf.getString("report.output").split(',')
    outptuModels.foreach(outputModel=>{
      //将计算结果写入到hdfs
      if(outputModel=="hdfs") SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), outputTableName)
      else if(outputModel=="oracle"||outputModel=="mysql"){
        stateConf.set("database",outputModel)

        //将计算结果写入到数据库
        val output = new DetailModelManager(stateConf)

        result.foreachPartition(par => {
          output.output(par.toIterable)
        })
      }
    })

  }

  override def getThreshold: Long = Utils.getParquetThreshold(stateConf, hadoopConfiguration)
}

object  DetailJob extends Logging {
  def main(args: Array[String]): Unit = {
   val sparkSession = SparkHelper.getSparkSession(None)
    val stateConf = new StateConf
    stateConf.add(args)


    new DetailJob(sparkSession, stateConf).compute()
  }

}
trait  JobOutput[T]{
  def foreach(values:Dataset[T]):Unit
}



/**
  * val pipe=Pipe.Creaet(Option)
  * pipe.source(path,format).compute().output(outputPath,...)
  * */
trait Pipe{
  type I
  type R
  def source(path:String,format:String,cmp:String,numPartiion:Int):I
  def compute():R
  def output(path:String,format:String,parColumn:String,numPartition:Int,cmp:String):Unit
}


//class Pipe extends Pipe{
//  override def source(path: String, format: String, cmp: String, numPartiion: Int) = ???
//}


//
//trait Input{
//  type R
//  def execute():R
//}
//
//trait Compute {
//  type I
//  type R
//  def execute(i:I): R
//}
//
//
//trait Output extends  Logging{
//  type R
//  def execute(result: R,
//            outputFormat: String,
//            cms: Option[String],
//            enableOutput: Option[String],
//            outputPath: String,
//            parColumn: Option[String],
//            parNum: Option[Int]):Unit
//}
//
//
//
////input,compute,output
//class DetailInput(inputFormat: String, inputPath: String, schema: Array[String], sqLContext: SQLContext) extends Input{
//  override type R = Dataset[RealinfoModel]
//   import sqLContext.implicits._
//  override def execute(): R = {
//    SparkHelper.parquetOrText(sqLContext, inputPath, inputFormat, schema).as[RealinfoModel]
//  }
//}
//
//
//
//
//
//class DetaiCompute(stateConf: StateConf,sparkSession:SparkSession) extends  Logging with Compute {
//  override type I=Dataset[RealinfoModel]
//  override type R = Dataset[DetailModel]
//
//  override def execute(input: I): R= {
//    input.createOrReplaceTempView("realinfo")
//    import sparkSession.sqlContext.implicits._
//    val realinfoDs = sparkSession.sql("SELECT VID,TIME,speed,charge,soc,totalVoltage,secondaryCellMaxVoltage,secondaryCellMinVoltage,engineTemp," +
//      "accuisitionPointMaxTemp,accuisitionPointMinTemp,mileage,longitude,latitude" +
//      "  FROM realinfo").as[RealinfoModel]
//
//
//    realinfoDs.groupByKey(_.vid)
//      .flatMapGroups({ case (vid: String, rows: Iterator[RealinfoModel]) =>
//
//        //按照时间排序并去重
//        val sortedRows = Utils.strict[RealinfoModel, String](
//          Utils.sortByDate(rows.toArray, row => {
//            if (row.time.trim.isEmpty) {
//              logInfo(row.toString)
//              None
//            } else Some(row.time)
//          }),
//          row => Some(row.time)
//        )
//        //状态划分
//        val stateGenerator = new StateGenerator[RealinfoModel](
//          stateConf,
//          getVid = row => row.vid,
//          getTime = row => row.time,
//          getCharge = row => row.charge.getOrElse(0),
//          getSoc = row => row.soc.getOrElse(0),
//          getSpeed = row => row.speed.getOrElse(0),
//          getRule=row=>"DB"
//        )
//
//        val chargeOrFullChargeOrTravelwindows = stateGenerator.handle(sortedRows)
//
//
//        //计算充电日报表
//
//
//        getResult(chargeOrFullChargeOrTravelwindows)
//      })
//  }
//
//  def getResult(windows:List[Window[RealinfoModel]]): List[DetailModel]= {
//    val result = new ListBuffer[DetailModel]
//
//    def doGetResult(curIndex: Int, prevState: String, prevResult: Option[DetailModel], prevChargeResult: Option[DetailModel]): List[DetailModel] = {
//      if (curIndex < windows.length) {
//        val curWindow = windows(curIndex)
//        val curState = Utils.windowIdToState(curWindow.windowId).getOrElse("")
//
//        if (curState == Constant.ChargeState) {
//          val curResult = if (prevChargeResult.isEmpty) new DetailCompute(stateConf, curWindow, None, None).result
//          else new DetailCompute(stateConf, curWindow, None, prevChargeResult).result
//          result.append(curResult)
//          doGetResult(curIndex + 1, curState, Some(curResult), Some(curResult))
//        } else if (curState == Constant.TravelState) {
//          val curResult = if (prevState == Constant.ChargeState) new DetailCompute(stateConf, curWindow, prevResult, None).result
//          else new DetailCompute(stateConf, curWindow, None, None).result
//          result.append(curResult)
//          doGetResult(curIndex + 1, curState, Some(curResult), prevChargeResult)
//        } else {
//          val curResult = new DetailCompute(stateConf, curWindow, None, None).result
//          result.append(curResult)
//          doGetResult(curIndex + 1, curState, Some(curResult), prevChargeResult)
//        }
//      } else {
//        result.toList
//      }
//    }
//
//    doGetResult(0, "", None, None)
//  }
//}
//
//
//class DetailOutput(stateConf: StateConf,sparkSession: SparkSession)extends Logging with Output{
//  override type R=Dataset[DetailModel]
//  override def execute(result:R,
//              outputFormat: String,
//              cms: Option[String],
//              enableOutput: Option[String],
//              outputPath: String,
//              parColumn: Option[String],
//              parNum: Option[Int]):Unit=enableOutput match {
//    case Some("true") =>
//      write(sparkSession.sqlContext,
//        result,
//        format = outputFormat,
//        compression = cms,
//        partitionColumn = parColumn,
//        partitionNum = parNum,
//        outputPath = outputPath)
//
//      logInfo(s"数据写入到${outputPath}完毕")
//    case _ =>
//      result.show(10)
//      logWarning("未开启hdfs输出")
//  }
//
//
//  def write(sQLContext: SQLContext, result: Dataset[DetailModel], format: String, compression: Option[String], partitionColumn: Option[String], partitionNum: Option[Int], outputPath: String) {
//    import sQLContext.implicits._
//    try {
//      if (format == "text" || format == "csv") {
//        val formated = if (format == "csv") {
//          result.map(v => StringParser.toCsv(v.toString))
//        } else {
//          result.map(_.toString)
//        }
//
//        val rePartitioned = if (partitionNum.nonEmpty) formated.repartition(partitionNum.get) else formated
//        val w = if (compression.nonEmpty) rePartitioned.write.format("text").option("compression", compression.get) else rePartitioned.write.format("text")
//        w.save(outputPath)
//      }
//      else {
//        Some({
//          partitionNum.map(result.repartition).getOrElse(result).write.format(format)
//        })
//          .map(w => compression.map(w.option("compression", _)).getOrElse(w))
//          .map(w => {
//            if (format == "parquet" && partitionColumn.nonEmpty) w.partitionBy(partitionColumn.get) else w
//          }).get.save(outputPath)
//      }
//
//      logInfo(s"$outputPath ok")
//    } catch {
//      case e: Throwable => throw e
//    }
//  }
//}
//
//
//
//class DetailJobBuilder(stateConf: StateConf,sparkSession: SparkSession) {
//  private var i: DetailInput = _
//  private var executor: DetaiCompute = _
//
//  def input(inputFormat: String, inputPath: String, schema: Array[String], sqLContext: SQLContext) = {
//    this.i = new DetailInput(inputFormat, inputPath, schema, sqLContext)
//    this
//  }
//
//  def compute() = {
//    this.executor = new DetaiCompute(stateConf, sparkSession)
//    this
//  }
//
//  def ouput(
//            outputFormat: String,
//            cms: Option[String],
//            enableOutput: Option[String],
//            outputPath: String,
//            parColumn: Option[String],
//            parNum: Option[Int]) = {
//
//    val r = executor.execute(i.execute())
//    new DetailOutput(stateConf, sparkSession).execute(r, outputFormat, cms, enableOutput, outputPath, parColumn, parNum)
//  }
//}


package com.bitnei.report.detail
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.constants.Constant
import com.bitnei.report.stateGenerate.Window
import org.scalatest.{BeforeAndAfter, FunSuite}

/**
  * Created by wangbaosheng on 2017/3/27.
  */
class DetailJobTest extends FunSuite with BeforeAndAfter {
  val stateConf = new StateConf()

  before {
    stateConf.set("state.database.remove.enable", "false")
    stateConf.set("detail.input.format", "text")
    stateConf.set(Constant.OutputDatabase, "false")
    stateConf.set(Constant.CheckPointPath, "")
  }


  test("detail compute") {
    val realinfoWindow = new Window[RealinfoModel]()
    realinfoWindow.append(createInvalidaateRealinfo("20170101000000"))
    realinfoWindow.append(createInvalidaateRealinfo("20170101000010"))
    realinfoWindow.append(createInvalidaateRealinfo("20170101000030"))
    realinfoWindow.append(createInvalidaateRealinfo("20170101000100"))
    realinfoWindow.append(createInvalidaateRealinfo("20170101000200"))
    realinfoWindow.append(createInvalidaateRealinfo("20170101000300"))
    realinfoWindow.append(createInvalidaateRealinfo("20170101000400"))
    realinfoWindow.append(createRealinfo("20170101000500"))
    realinfoWindow.append(createInvalidaateRealinfo("20170101000600"))
    realinfoWindow.append(createInvalidaateRealinfo("20170101000900"))

    def createInvalidaateRealinfo(time: String): RealinfoModel = {
      RealinfoModel(vid = "vid", vin = "vid", time = time, speed = Some(0), mileage = Some(1000), peaking = Some(0), charge = None, soc = None, totalVoltage = None, secondaryCellMaxVoltage = None, engineTemp = None, accuisitionPointMaxTemp = None, accuisitionPointMinTemp = None, longitude = None, latitude = None, secondaryCellMinVoltage = None)
    }

    def createRealinfo(time: String): RealinfoModel = {
      RealinfoModel(vid = "vid", vin = "vid", time = time, speed = Some(1), mileage = Some(1000), peaking = Some(0), charge = None, soc = None, totalVoltage = None, secondaryCellMaxVoltage = None, engineTemp = None, accuisitionPointMaxTemp = None, accuisitionPointMinTemp = None, longitude = None, latitude = None, secondaryCellMinVoltage = None)
    }

    val dc = new DetailCompute(stateConf, realinfoWindow, None, None)

    val result=dc.compute()
    println(result.timeLeng)
    println(result.accRunTime)

  }
}
//  test("left join "){
////    val sparkSession=SparkSession.builder().appName("spark单元测试").master("local").getOrCreate()
////    import  sparkSession.implicits._
////
////    sparkSession.createDataset(Array(DetailInput("no data","no data",Constant.TravelState,0,0,0, 0, 0, 0, 0, 0))).createOrReplaceTempView("detail")
////
////    val vehicle=Array(VehicleInfo("GJBCE50WCL1000000", "111", true, "京12345", "manu1", "model1", "type1", "西城", "use1",1,1,0,0),
////      VehicleInfo("GJBCE50WCL1000001", "111", true, "京12345", "manu1", "model1", "type1", "西城", "use1",1,1,0,0))
////    sparkSession.createDataset(vehicle).createOrReplaceTempView("vehicle")
////
////    val sql =
////      """
////       SELECT vehicle.vin as vin,
////       vehicle.vid as vid,
////       vehicle.licensePlate as licensePlate,
////       vehicle.manuUnit as manuUnit,
////       vehicle.vehicleMode as vehicleMode,
////       vehicle.vehicleType as vehicleType,
////       vehicle.area as area,
////       vehicle.use_unit AS useUnit,
////       vehicle.onlined as onlined,
////       vehicle.onlineDay as onlineDay,
////       vehicle.vehStatus as vehStatus,
////       vehicle.firstRegiste as firstRegiste,
////       vehicle.firstOnlineMileage as firstOnlineMileage,
////       d.category as category,
////       d.startTime as startTime,
////       d.endTime as endTime,
////       d.startMileage as startMileage,
////       d.stopMileage AS stopMileage,
////       d.timeLeng AS timeLeng,
////       d.avgSpeed AS avgSpeed
////       FROM  vehicle LEFT JOIN detail AS d  ON d.vin=vehicle.vin
////    """.stripMargin
////
////    val realinfoDs = sparkSession.sql(sql).as[OperationDetail]
////    realinfoDs.show(10)
//  }
//
//
//  test("no input data"){
//    val sparkSession=SparkSession.builder().appName("spark单元测试").master("local").getOrCreate()
//
//    val detailJob=new DetailJob(sparkSession,stateConf,"",Array(".\\src\\main\\resources\\dayreport\\01c3d73e-a3c5-4b85-aa2c-4d330c8c6015"))
//
//  }
//  val sparkSession=SparkSession.builder().appName("spark单元测试").master("local").getOrCreate()
//
//  val detailJob=new DetailJob(sparkSession,stateConf,"",Array(".\\src\\main\\resources\\dayreport\\01c3d73e-a3c5-4b85-aa2c-4d330c8c6015"))
//
//
//
//  test("test 01c3d73e-a3c5-4b85-aa2c-4d330c8c6015"){
//
//
//    /**
//      * 测试如下车辆的数据
//      * 0222510f-0261-4e82-ab33-71e81c1e9c30
//      * 1ef74cd7c-1043-49ae-aa94-84e28d9426f2
//      * 01c3d73e-a3c5-4b85-aa2c-4d330c8c6015"
//      * 02a82879-ffc4-4f54-8470-f4134259c70f
//      */
//
//
//    def assert01c3d73ea3c54b8aa2c4d330c8c6015(): Unit = {
//      stateConf.set("detail.input.path",".\\src\\main\\resources\\dayreport\\01c3d73e-a3c5-4b85-aa2c-4d330c8c6015")
//      stateConf.set("detail.output.path",".\\src\\main\\resources\\dayreport\\01c3d73e-a3c5-4b85-aa2c-4d330c8c6015.output")
//      stateConf.set("detail.output.format","text")
//
//      detailJob.registerIfNeed
//      val actualResults = detailJob.realinfoResult.collect().map(_.toString)
//
//      actualResults.foreach(println)
//
//      val exceptedRunResults = Array(
//        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913000026,20160913185710,22.77,70210,69920,60.00,572.4,419.4,439.8,-180.2,99.6,72.8,3.418,2.997,35,27,52,0",
//        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913200746,20160913221252,22.77,70220,70210,14.00,569.0,564.0,44.3,-42.8,99.6,99.6,3.393,3.343,35,32,48,43",
//        "run,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913221553,20160913235941,22.77,70220,70220,0.00,564.2,0.0,0.0,0.0,99.6,0.0,3.363,0.000,35,0,46,0")
//
//
//      val exceptedChargeResults = Array(
//        "charge,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913185740,20160913200616,1.14,14.67,1.14,603.9,563.7,-51.3,-67.3,99.2,72.8,3.651,3.348,35,32,44,0,14.67"
//      )
//
//      val exceptedFullChargeResults = Array(
//        "fullcharge,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,20160913200646,20160913200716,0.01,0.00,0.01,587.6,587.6,0.0,0.0,100.0,100.0,3.536,3.431,35,32,0,0,0.00")
//
//
//      val exceptedRunDayResult = Array("runDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,23.92,22.77,3,30.0,7.6,10.00,1.32,18.95,1.73,29.0,60.0,572.4,419.4,439.8,-180.2,99.6, 0.0,3.42,3.00,35,27,52,26")
//      val exceptedChargeDayResult = Array("chargeDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,1.14,1,14.67,48.90,1.1,1.1,603.9,563.7,-51.3,-67.3,99.2,72.8,3.65,3.35,35,32,44,0,14.67,14.67")
//      val exceptedFullChargeResult = Array("fullchargeDay,01c3d73e-a3c5-4b85-aa2c-4d330c8c6015,0.01,1,0.01,0.01,587.6, 587.6,3.54, 3.43,35, 32,0, 0")
//
//      val exceptedResults = Map(
//        "run" -> exceptedRunResults,
//        "charge" -> exceptedChargeResults,
//        "fullcharge" -> exceptedFullChargeResults
//      )
//      actualResults.foreach(println)
//
//
//
//
//    //  assertResult(actualResults, exceptedResults, "charge")
//    //  assertResult(actualResults, exceptedResults, "fullcharge")
//     // assertResult(actualResults, exceptedResults, "run")
//    }
//
//    def assert0222510f02614e82ab3371e81c1e9c30(): Unit = {
//      stateConf.set(Constant.RealinfoPath, "C:\\D\\dayreport\\src\\main\\resources\\dayreport\\0222510f-0261-4e82-ab33-71e81c1e9c30")
//
//      detailJob.registerIfNeed
//      val actualResults = detailJob.realinfoResult.collect().map(_.toString)
//
//      //actualResults.foreach(println)
//      val exceptedRunResults = Array(
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913002914,20160913003405,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913034459,20160913034929,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913094444,20160913094925,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913101646,20160913102406,1.71,4180,4170,10,28.10,512.7,483.6,84.2,-0.9,68.0,67.2,3.32,3.06,20,19,26,20",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913122953,20160913123434,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913133503,20160913133944,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913144000,20160913144451,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913154509,20160913155000,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913175524,20160913180015,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913190035,20160913190524,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913211050,20160913211538,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913221553,20160913222549,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647",
//      "run,0222510f-0261-4e82-ab33-71e81c1e9c30,20160913232608,20160913233058,1.71,0,0,0,0.00,0.0,214748368.0,0.0,0.0,0.0,0.0,0.00,0.00,0,2147483647,0,2147483647"
//      )
//
//      val exceptedChargeResults = Array.empty[String]
//
//      val exceptedFullChargeResults = Array.empty[String]
//
//
//      val exceptedResults = Map(
//        "run" -> exceptedRunResults,
//        "charge" -> exceptedChargeResults,
//        "fullcharge" -> exceptedFullChargeResults
//      )
//
//      actualResults.foreach(println)
//
//
//      assertResult(actualResults, exceptedResults, "charge")
//      assertResult(actualResults, exceptedResults, "fullcharge")
//      assertResult(actualResults, exceptedResults, "run")
//    }
//
//    def assertResult(actualResults:Array[String], exceptedResults:Map[String,Array[String]],state:String): Unit = {
//      exceptedResults.get(state) match {
//        case Some(excepted) =>
//          val actual = actualResults.filter(_.split(',')(0) == state)
//          assert(actual.length==excepted.length)
//
//          actual.indices.foreach(index=>{
//            val actualFields=actual(index).split(',')
//            val exceptedFields=excepted(index).split(',')
//
//            actualFields.indices.foreach(fieldIndex=>{
//              //测试run,vid,开始时间,结束时间
//              if(0<=fieldIndex&&fieldIndex<=3) {
//                assert(actualFields(fieldIndex) == exceptedFields(fieldIndex))
//                assert(actualFields(fieldIndex) == exceptedFields(fieldIndex))
//              }else {
//                // println(actualFields(fieldIndex)+":"+exceptedFields(fieldIndex))
//                // assert(Math.abs(actualFields(fieldIndex).toDouble-exceptedFields(fieldIndex).toDouble)<=0.01)
//              }
//            })
//          })
//        case None =>
//      }
//    }
//
//    assert01c3d73ea3c54b8aa2c4d330c8c6015()
//   // assert0222510f02614e82ab3371e81c1e9c30()
//  //  assert02a82879_ffc4_4f54_8470_f4134259c70f()
//  }
//
//
//}
package tempjob

import java.util.Date

import com.bitnei.report.common.utils.Utils

import scala.collection.mutable.ArrayBuffer

/**
  * Created by wangbaosheng on 2017/8/4.
  */
object DetailMerge {

  def main(args: Array[String]): Unit = {
    val inputFiles = Array("")

    inputFiles.foreach(inputFile => {
      processOne(inputFile)
    })
  }

  def processOne(inputFile: String) = {
    var prevLine: String = ""
    val result = new ArrayBuffer[String]()
    Utils.readAllLines(inputFile).zipWithIndex.foreach(p => {
      val (curLine, i) = p
      val mappedLine = if (prevLine == "") {
        curLine
      } else {
        belongOne(prevLine, curLine) match {
          case (true, mergedLine) => mergedLine
          case _ => curLine
        }
      }

      prevLine = curLine
      result.append(mappedLine)
    })
  }

  def belongOne(prev: String, cur: String): (Boolean, String) = {

//    val startDate:Date=Utils.formatDate("")
    (true, "")
  }

}
package com.bitnei.report.taxis


case class DetailModel(
                        vid:String,
                        category:String,
                        startTime:Long,
                        endTime:Long,
                        timeLeng:Long=0,

                        //实际行驶时间=timelenth-无效形式时间
                        accRunTime:Int=0,
                        //开始里程
                        startMileage:Int,
                        //结束里程
                        stopMileage:Int=Int.MaxValue,
                        maxSpeed:Int,

                        startSoc:Int,
                        endSoc:Int,

                        //行驶数据
                        startLongitude:Long,
                        startLatitude:Long,
                        endLongitude:Long,
                        endLatitude:Long,
                        minTotalCurrent:Int,
                        totalCharge:Double,
                        isQuickCharge:Option[Boolean]){}package com.bitnei.report.detail

import java.sql.{Date, Timestamp}

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.DataPrecision
import com.bitnei.report.constants.Constant

import scala.math.BigDecimal.RoundingMode

/**
  * Created by wangbaosheng on 2017/5/8.
  * 将明细数据输出到数据库
  */
class DetailModelManager(stateConf:StateConf) extends OutputManager with Serializable with Logging{

  //将行驶明细输出到数据库
  class RunManager extends Serializable{
    val table=stateConf.getOption("single_vehicle_run_detail_table").getOrElse("veh_day_single_run")
    def insert(vs: Iterable[DetailModel]): Unit = {
      val sql = stateConf.getOption("database") match {
        case Some("oracle") =>
          s" insert into $table (id,report_date,vid,start_time,end_time,hour_long,start_longitude,start_latitude,end_longitude,end_latitude,start_km,end_km,km,start_soc,end_soc) " +
            "VALUES(SEQ_VEH_DAY_SINGLE_RUN.Nextval,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
        case Some("mysql") =>
          s""" insert into $table(report_date,vid,start_time,end_time,hour_long,start_longitude,start_latitude,end_longitude,end_latitude,start_km,end_km,km,start_soc,end_soc)
                VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?);
              """.stripMargin
      }


      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        logInfo(s"writting $table,"+vs.head.toString)

        vs.foreach(v => {
          if (v.category == Constant.TravelState) {
            stmt.setDate(1, new Date(v.startTime)) //开始时间
            stmt.setString(2, v.vid)
            stmt.setTimestamp(3, new Timestamp(v.startTime)) //开始时间
            stmt.setTimestamp(4, new Timestamp(v.endTime)) //结束时间
            stmt.setDouble(5, DataPrecision.toHour(v.timeLeng))
            stmt.setDouble(6, DataPrecision.latitude(v.startLongitude))
            stmt.setDouble(7, DataPrecision.latitude(v.startLatitude))
            stmt.setDouble(8, DataPrecision.latitude(v.endLongitude))
            stmt.setDouble(9, DataPrecision.latitude(v.endLatitude))
            stmt.setDouble(10, DataPrecision.mileage(v.startMileage))
            stmt.setDouble(11, DataPrecision.mileage(v.stopMileage))
            stmt.setDouble(12, DataPrecision.mileage(v.stopMileage - v.startMileage))
            stmt.setDouble(13, DataPrecision.soc(v.startSoc))
            stmt.setDouble(14, DataPrecision.soc(v.endSoc))
            stmt.addBatch()
          }
        })
      })
    }

    def delete(vid: String, reportDate: String): Unit = {
      val sql = s"delete from $table where vid='$vid' and to_char(start_Time,'yyyy-mm-dd') ='$reportDate'"
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, x => {
        //x.setString(1, vid)
        //x.setString(2, reportDate)
        x.addBatch()
      })
    }

    def delete(reportDate: String): Unit = {
      val sql = s"delete from $table where to_char(start_Time,'yyyy-mm-dd') =?"
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, x => {
        x.setString(1, reportDate)
        x.addBatch()
      })
    }

  }

  //将充电明细输出到数据库
  class ChargeManager extends Serializable{
    val table=stateConf.getOption("single_vehicle_charge_detail_table").getOrElse("veh_day_single_charge")

    def insert(vs: Iterable[DetailModel]): Unit = {
      val sql = if (stateConf.getString("database").trim == "oracle") {
        s"insert into $table(id,report_date,vid,start_time,end_time,hour_long,longitude,latitude,start_soc,end_soc,power,max_current)" +
          "VALUES(SEQ_VEH_DAY_SINGLE_CHARGE.Nextval,?,?,?,?,?,?,?,?,?,?,?)"
      } else {
        s""" insert into $table(
            report_date,
            vid,
            start_time,
            end_time,
            hour_long,
            longitude,
            latitude,
            start_soc,
            end_soc,
            power,
            max_current)
                VALUES(?,?,?,?,?,?,?,?,?,?,?);
              """
      }
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        try {
          vs.foreach(v => {
            if (v.category == Constant.ChargeState) {
              stmt.setDate(1, new Date(v.startTime)) //开始时间
              stmt.setString(2, v.vid)
              stmt.setTimestamp(3, new Timestamp(v.startTime)) //开始时间
              stmt.setTimestamp(4, new Timestamp(v.endTime)) //结束时间
              stmt.setDouble(5, DataPrecision.toHour(v.timeLeng))
              stmt.setDouble(6, DataPrecision.latitude(v.startLongitude))
              stmt.setDouble(7, DataPrecision.latitude(v.startLatitude))
              stmt.setDouble(8, DataPrecision.soc(v.startSoc))
              stmt.setDouble(9, DataPrecision.soc(v.endSoc))
              stmt.setDouble(10, BigDecimal(v.totalCharge).setScale(2, RoundingMode.HALF_DOWN).toDouble) //总耗电量
              stmt.setDouble(11, DataPrecision.totalCurrent(v.maxTotalCurrent))
              stmt.addBatch()
            }
          })
        } catch {
          case e: Exception =>
            throw new Exception(s"throw en exception when writting $table", e)
        }
      })
    }

    def delete(vid: String, reportDate: String): Unit = {
      val sql = s"delete from $table where vid=? and to_char(start_Time,'yyyy-mm-dd') =?"
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, x => {
        x.setString(1, vid)
        x.setString(2, reportDate)
        x.addBatch()
      })
    }

    def delete(reportDate: String): Unit = {
      val sql = s"delete from $table where to_char(start_Time,'yyyy-mm-dd') =?"
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, x => {
        x.setString(1, reportDate)
        x.addBatch()
      })
    }
  }

  val runManager = new RunManager()

  val chargeManager = new ChargeManager()

  override type T = DetailModel

  override def output(vs: Iterable[DetailModel]) = {
    insertToChargeTable(vs)
    insertToRun(vs)
  }


  def insertToChargeTable(vs: Iterable[DetailModel]): Unit = {
    chargeManager.insert(vs)
  }

  def insertToRun(vs: Iterable[DetailModel]): Unit = {
    runManager.insert(vs)
  }

  def delete(vid: String, reportDate: String): Unit = {
    chargeManager.delete(vid, reportDate)
    runManager.delete(vid, reportDate)
  }

  def delete(reportDate: String): Unit = {
    chargeManager.delete(reportDate)
    runManager.delete(reportDate)
  }
}

package com.bitnei.report.detail

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.Job
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql.{Dataset, SparkSession}

import scala.reflect.runtime.universe

/**
  * Created by wangbaosheng on 2017/5/9.
  *将明细数据输出到数据库
  */
class DetailOutputJob(stateConf: StateConf, sparkSession: SparkSession) extends Serializable with Logging with Job {
  override type R = DetailModel
  private val sqlContext = sparkSession.sqlContext

  import  sqlContext.implicits._
  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("detail")

  override def registerIfNeed() = SparkHelper.createOrReplaceTempView(sparkSession,stateConf,inputTableName)

  override def unRegister() =sparkSession.catalog.dropTempView(inputTableName)


  override def doCompute[Product <: DetailModel]() = sqlContext.sql(s"SELECT * FROM $inputTableName ${SqlHelper.buildWhere(stateConf)}").as[DetailModel]


  override def write[Product <: DetailModel](result: Dataset[DetailModel]) = {
    val outputPaths = stateConf.getString("report.output").split(',')
    outputPaths.foreach({
      case e if e == "oracle" || e == "mysql" =>
        //输出到数据库
        stateConf.set("database", e)

        logInfo(s"writting detail to $e")
        //将明细数据输出到数据库
        result.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(10))
          .foreachPartition(par => {
          val output = new DetailModelManager(stateConf)
          output.output(par.toIterable)
        })
      case e =>
    })
  }


  def getLast(values: Iterator[DetailModel]): DetailModel = {
    var lastState: DetailModel = null
    values.foreach(v => {
      if (lastState == null) lastState = v
      else if (v.startTime > lastState.startTime) lastState = v
    })
    lastState
  }

}

object DetailOutputJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    new DetailOutputJob(stateConf, SparkHelper.getSparkSession(sparkMaster = None)).compute()
  }
}package com.bitnei.report.detail

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.utils.{DataPrecision, Utils}

//将充电行驶明细输出到数据库
class DetailRunChargeOutput(stateConf: StateConf)  extends OutputManager{
  override type T = ChargeRun

  override def output(vs:Iterable[ChargeRun]): Unit = {
    val table=stateConf.getOption("single_vehicle_charge_run_detail_table").getOrElse("veh_day_single_charge_run")

    val sql = stateConf.getString("database") match {
      case "oracle" =>
           s"""INSERT INTO $table (id,report_date,vid,start_km,end_km,total_km,run_single_hour_long1,run_single_hour_long2,run_single_hour_long3,run_single_hour_long4,run_single_hour_long5,run_single_hour_long6,run_single_hour_long7,run_single_hour_long8,run_half_hour_long1,run_half_hour_long2,run_half_hour_long3,run_half_hour_long4,run_half_hour_long5,run_half_hour_long6,run_half_hour_long7,run_half_hour_long8,run_half_hour_long9,run_half_hour_long10,run_half_hour_long11,run_half_hour_long12,run_half_hour_long13,run_half_hour_long14,run_half_hour_long15,run_half_hour_long16,run_half_hour_long17,run_half_hour_long18,run_half_hour_long19,run_half_hour_long20,run_half_hour_long21,run_half_hour_long22,run_half_hour_long23,run_half_hour_long24,run_half_hour_long25,run_half_hour_long26,run_half_hour_long27,run_half_hour_long28,run_half_hour_long29,run_half_hour_long30,run_half_hour_long31,run_half_hour_long32,run_half_hour_long33,run_half_hour_long34,run_half_hour_long35,run_half_hour_long36,run_half_hour_long37,run_half_hour_long38,run_half_hour_long39,run_half_hour_long40,run_half_hour_long41,run_half_hour_long42,run_half_hour_long43,run_half_hour_long44,run_half_hour_long45,run_half_hour_long46,run_half_hour_long47,run_half_hour_long48,charge_half_hour_long1,charge_half_hour_long2,charge_half_hour_long3,charge_half_hour_long4,charge_half_hour_long5,charge_half_hour_long6,charge_half_hour_long7,charge_half_hour_long8,charge_half_hour_long9,charge_half_hour_long10,charge_half_hour_long11,charge_half_hour_long12,charge_half_hour_long13,charge_half_hour_long14,charge_half_hour_long15,charge_half_hour_long16,charge_half_hour_long17,charge_half_hour_long18,charge_half_hour_long19,charge_half_hour_long20,charge_half_hour_long21,charge_half_hour_long22,charge_half_hour_long23,charge_half_hour_long24,charge_half_hour_long25,charge_half_hour_long26,charge_half_hour_long27,charge_half_hour_long28,charge_half_hour_long29,charge_half_hour_long30,charge_half_hour_long31,charge_half_hour_long32,charge_half_hour_long33,charge_half_hour_long34,charge_half_hour_long35,charge_half_hour_long36,charge_half_hour_long37,charge_half_hour_long38,charge_half_hour_long39,charge_half_hour_long40,charge_half_hour_long41,charge_half_hour_long42,charge_half_hour_long43,charge_half_hour_long44,charge_half_hour_long45,charge_half_hour_long46,charge_half_hour_long47,charge_half_hour_long48,charge_half_hour_power1,charge_half_hour_power2,charge_half_hour_power3,charge_half_hour_power4,charge_half_hour_power5,charge_half_hour_power6,charge_half_hour_power7,charge_half_hour_power8,charge_half_hour_power9,charge_half_hour_power10,charge_half_hour_power11,charge_half_hour_power12,charge_half_hour_power13,charge_half_hour_power14,charge_half_hour_power15,charge_half_hour_power16,charge_half_hour_power17,charge_half_hour_power18,charge_half_hour_power19,charge_half_hour_power20,charge_half_hour_power21,charge_half_hour_power22,charge_half_hour_power23,charge_half_hour_power24,charge_half_hour_power25,charge_half_hour_power26,charge_half_hour_power27,charge_half_hour_power28,charge_half_hour_power29,charge_half_hour_power30,charge_half_hour_power31,charge_half_hour_power32,charge_half_hour_power33,charge_half_hour_power34,charge_half_hour_power35,charge_half_hour_power36,charge_half_hour_power37,charge_half_hour_power38,charge_half_hour_power39,charge_half_hour_power40,charge_half_hour_power41,charge_half_hour_power42,charge_half_hour_power43,charge_half_hour_power44,charge_half_hour_power45,charge_half_hour_power46,charge_half_hour_power47,charge_half_hour_power48,sum_single_run_km)
        VALUES(SEQ_VEH_DAY_SINGLE_CHARGE_RUN.Nextval,?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?)
    """.stripMargin
      case "mysql" =>
        s"""
           INSERT INTO $table (report_date,vid,start_km,end_km,total_km,run_single_hour_long1,run_single_hour_long2,run_single_hour_long3,run_single_hour_long4,run_single_hour_long5,run_single_hour_long6,run_single_hour_long7,run_single_hour_long8,run_half_hour_long1,run_half_hour_long2,run_half_hour_long3,run_half_hour_long4,run_half_hour_long5,run_half_hour_long6,run_half_hour_long7,run_half_hour_long8,run_half_hour_long9,run_half_hour_long10,run_half_hour_long11,run_half_hour_long12,run_half_hour_long13,run_half_hour_long14,run_half_hour_long15,run_half_hour_long16,run_half_hour_long17,run_half_hour_long18,run_half_hour_long19,run_half_hour_long20,run_half_hour_long21,run_half_hour_long22,run_half_hour_long23,run_half_hour_long24,run_half_hour_long25,run_half_hour_long26,run_half_hour_long27,run_half_hour_long28,run_half_hour_long29,run_half_hour_long30,run_half_hour_long31,run_half_hour_long32,run_half_hour_long33,run_half_hour_long34,run_half_hour_long35,run_half_hour_long36,run_half_hour_long37,run_half_hour_long38,run_half_hour_long39,run_half_hour_long40,run_half_hour_long41,run_half_hour_long42,run_half_hour_long43,run_half_hour_long44,run_half_hour_long45,run_half_hour_long46,run_half_hour_long47,run_half_hour_long48,charge_half_hour_long1,charge_half_hour_long2,charge_half_hour_long3,charge_half_hour_long4,charge_half_hour_long5,charge_half_hour_long6,charge_half_hour_long7,charge_half_hour_long8,charge_half_hour_long9,charge_half_hour_long10,charge_half_hour_long11,charge_half_hour_long12,charge_half_hour_long13,charge_half_hour_long14,charge_half_hour_long15,charge_half_hour_long16,charge_half_hour_long17,charge_half_hour_long18,charge_half_hour_long19,charge_half_hour_long20,charge_half_hour_long21,charge_half_hour_long22,charge_half_hour_long23,charge_half_hour_long24,charge_half_hour_long25,charge_half_hour_long26,charge_half_hour_long27,charge_half_hour_long28,charge_half_hour_long29,charge_half_hour_long30,charge_half_hour_long31,charge_half_hour_long32,charge_half_hour_long33,charge_half_hour_long34,charge_half_hour_long35,charge_half_hour_long36,charge_half_hour_long37,charge_half_hour_long38,charge_half_hour_long39,charge_half_hour_long40,charge_half_hour_long41,charge_half_hour_long42,charge_half_hour_long43,charge_half_hour_long44,charge_half_hour_long45,charge_half_hour_long46,charge_half_hour_long47,charge_half_hour_long48,charge_half_hour_power1,charge_half_hour_power2,charge_half_hour_power3,charge_half_hour_power4,charge_half_hour_power5,charge_half_hour_power6,charge_half_hour_power7,charge_half_hour_power8,charge_half_hour_power9,charge_half_hour_power10,charge_half_hour_power11,charge_half_hour_power12,charge_half_hour_power13,charge_half_hour_power14,charge_half_hour_power15,charge_half_hour_power16,charge_half_hour_power17,charge_half_hour_power18,charge_half_hour_power19,charge_half_hour_power20,charge_half_hour_power21,charge_half_hour_power22,charge_half_hour_power23,charge_half_hour_power24,charge_half_hour_power25,charge_half_hour_power26,charge_half_hour_power27,charge_half_hour_power28,charge_half_hour_power29,charge_half_hour_power30,charge_half_hour_power31,charge_half_hour_power32,charge_half_hour_power33,charge_half_hour_power34,charge_half_hour_power35,charge_half_hour_power36,charge_half_hour_power37,charge_half_hour_power38,charge_half_hour_power39,charge_half_hour_power40,charge_half_hour_power41,charge_half_hour_power42,charge_half_hour_power43,charge_half_hour_power44,charge_half_hour_power45,charge_half_hour_power46,charge_half_hour_power47,charge_half_hour_power48,sum_single_run_km)
        VALUES(?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?,?,?,
         ?,?,?,?,?,?,?,?)
    """.stripMargin
    }

    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
     try {
       vs.foreach(v => {
         stmt.setDate(1, new java.sql.Date(v.startTime)) //开始时间
         stmt.setString(2, v.vid)

         stmt.setDouble(3, DataPrecision.mileage(v.startMileage.toInt))
         stmt.setDouble(4, DataPrecision.mileage(v.endMileage.toInt))
         stmt.setDouble(5, DataPrecision.mileage((v.endMileage - v.startMileage).toInt))

         //行驶时长分布, 0-1小时，1-2小时，....，9-10小时，10小时以上
         for (i <- 0 until 8) {
           stmt.setInt(6 + i, v.runTimeLenthDistribution(i))
         }

         //行驶时段分布
         for (i <- 0 until 48) {
           stmt.setInt(14 + i, v.runTimeRangeDistribution(i))
         }

         //充电时段分布
         for (i <- 0 until 48) {
           stmt.setInt(62 + i, v.chargeTimeRangeDistribution(i))
         }

         //耗电量分布
         for (i <- 0 until 48) {
           stmt.setDouble(110 + i, DataPrecision.charge(v.chargePowerDistribution(i)))
         }

         //实际行驶里程
         stmt.setDouble(158, DataPrecision.mileage(v.mileage.toInt))
         stmt.addBatch()
       })
     }catch {
       case e:Exception=>  throw new Exception(s"throw en exception when writting $table",e)
     }
    })
  }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}
import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}
import org.apache.spark.ml.param.{IntParam, ParamMap}
import org.apache.spark.ml.util.Identifiable
import org.apache.spark.sql.{Dataset, Row, SparkSession}

/**
 * A simple example demonstrating how to write your own learning algorithm using Estimator,
 * Transformer, and other abstractions.
 * This mimics [[org.apache.spark.ml.classification.LogisticRegression]].
 * Run with
 * {{{
 * bin/run-example ml.DeveloperApiExample
 * }}}
 */
object DeveloperApiExample {

  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("DeveloperApiExample")
      .getOrCreate()
    import spark.implicits._

    // Prepare training data.
    val training = spark.createDataFrame(Seq(
      LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)),
      LabeledPoint(0.0, Vectors.dense(2.0, 1.0, -1.0)),
      LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)),
      LabeledPoint(1.0, Vectors.dense(0.0, 1.2, -0.5))))

    // Create a LogisticRegression instance. This instance is an Estimator.
    val lr = new MyLogisticRegression()
    // Print out the parameters, documentation, and any default values.
    println("MyLogisticRegression parameters:\n" + lr.explainParams() + "\n")

    // We may set parameters using setter methods.
    lr.setMaxIter(10)

    // Learn a LogisticRegression model. This uses the parameters stored in lr.
    val model = lr.fit(training.toDF())

    // Prepare test data.
    val test = spark.createDataFrame(Seq(
      LabeledPoint(1.0, Vectors.dense(-1.0, 1.5, 1.3)),
      LabeledPoint(0.0, Vectors.dense(3.0, 2.0, -0.1)),
      LabeledPoint(1.0, Vectors.dense(0.0, 2.2, -1.5))))

    // Make predictions on test data.
    val sumPredictions: Double = model.transform(test)
      .select("features", "label", "prediction")
      .collect()
      .map { case Row(features: Vector, label: Double, prediction: Double) =>
        prediction
      }.sum
    assert(sumPredictions == 0.0,
      "MyLogisticRegression predicted something other than 0, even though all coefficients are 0!")

    spark.stop()
  }
}

/**
 * Example of defining a parameter trait for a user-defined type of [[Classifier]].
 *
 * NOTE: This is private since it is an example. In practice, you may not want it to be private.
 */
private trait MyLogisticRegressionParams extends ClassifierParams {

  /**
   * Param for max number of iterations
   *
   * NOTE: The usual way to add a parameter to a model or algorithm is to include:
   *   - val myParamName: ParamType
   *   - def getMyParamName
   *   - def setMyParamName
   * Here, we have a trait to be mixed in with the Estimator and Model (MyLogisticRegression
   * and MyLogisticRegressionModel). We place the setter (setMaxIter) method in the Estimator
   * class since the maxIter parameter is only used during training (not in the Model).
   */
  val maxIter: IntParam = new IntParam(this, "maxIter", "max number of iterations")
  def getMaxIter: Int = $(maxIter)
}

/**
 * Example of defining a type of [[Classifier]].
 *
 * NOTE: This is private since it is an example. In practice, you may not want it to be private.
 */
private class MyLogisticRegression(override val uid: String)
  extends Classifier[Vector, MyLogisticRegression, MyLogisticRegressionModel]
  with MyLogisticRegressionParams {

  def this() = this(Identifiable.randomUID("myLogReg"))

  setMaxIter(100) // Initialize

  // The parameter setter is in this class since it should return type MyLogisticRegression.
  def setMaxIter(value: Int): this.type = set(maxIter, value)

  // This method is used by fit()
  override protected def train(dataset: Dataset[_]): MyLogisticRegressionModel = {
    // Extract columns from data using helper method.
    val oldDataset = extractLabeledPoints(dataset)

    // Do learning to estimate the coefficients vector.
    val numFeatures = oldDataset.take(1)(0).features.size
    val coefficients = Vectors.zeros(numFeatures) // Learning would happen here.

    // Create a model, and return it.
    new MyLogisticRegressionModel(uid, coefficients).setParent(this)
  }

  override def copy(extra: ParamMap): MyLogisticRegression = defaultCopy(extra)
}

/**
 * Example of defining a type of [[ClassificationModel]].
 *
 * NOTE: This is private since it is an example. In practice, you may not want it to be private.
 */
private class MyLogisticRegressionModel(
    override val uid: String,
    val coefficients: Vector)
  extends ClassificationModel[Vector, MyLogisticRegressionModel]
  with MyLogisticRegressionParams {

  // This uses the default implementation of transform(), which reads column "features" and outputs
  // columns "prediction" and "rawPrediction."

  // This uses the default implementation of predict(), which chooses the label corresponding to
  // the maximum value returned by [[predictRaw()]].

  /**
   * Raw prediction for each possible label.
   * The meaning of a "raw" prediction may vary between algorithms, but it intuitively gives
   * a measure of confidence in each possible label (where larger = more confident).
   * This internal method is used to implement [[transform()]] and output [[rawPredictionCol]].
   *
   * @return  vector where element i is the raw prediction for label i.
   *          This raw prediction may be any real number, where a larger value indicates greater
   *          confidence for that label.
   */
  override protected def predictRaw(features: Vector): Vector = {
    val margin = BLAS.dot(features, coefficients)
    // There are 2 classes (binary classification), so we return a length-2 vector,
    // where index i corresponds to class i (i = 0, 1).
    Vectors.dense(-margin, margin)
  }

  /** Number of classes the label can take. 2 indicates binary classification. */
  override val numClasses: Int = 2

  /** Number of features the model was trained on. */
  override val numFeatures: Int = coefficients.size

  /**
   * Create a copy of the model.
   * The copy is shallow, except for the embedded paramMap, which gets a deep copy.
   *
   * This is used for the default implementation of [[transform()]].
   */
  override def copy(extra: ParamMap): MyLogisticRegressionModel = {
    copyValues(new MyLogisticRegressionModel(uid, coefficients), extra).setParent(parent)
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import java.io.File

import scala.io.Source._

import org.apache.spark.sql.SparkSession

/**
 * Simple test for reading and writing to a distributed
 * file system.  This example does the following:
 *
 *   1. Reads local file
 *   2. Computes word count on local file
 *   3. Writes local file to a DFS
 *   4. Reads the file back from the DFS
 *   5. Computes word count on the file using Spark
 *   6. Compares the word count results
 */
object DFSReadWriteTest {

  private var localFilePath: File = new File(".")
  private var dfsDirPath: String = ""

  private val NPARAMS = 2

  private def readFile(filename: String): List[String] = {
    val lineIter: Iterator[String] = fromFile(filename).getLines()
    val lineList: List[String] = lineIter.toList
    lineList
  }

  private def printUsage(): Unit = {
    val usage: String = "DFS Read-Write Test\n" +
    "\n" +
    "Usage: localFile dfsDir\n" +
    "\n" +
    "localFile - (string) local file to use in test\n" +
    "dfsDir - (string) DFS directory for read/write tests\n"

    println(usage)
  }

  private def parseArgs(args: Array[String]): Unit = {
    if (args.length != NPARAMS) {
      printUsage()
      System.exit(1)
    }

    var i = 0

    localFilePath = new File(args(i))
    if (!localFilePath.exists) {
      System.err.println("Given path (" + args(i) + ") does not exist.\n")
      printUsage()
      System.exit(1)
    }

    if (!localFilePath.isFile) {
      System.err.println("Given path (" + args(i) + ") is not a file.\n")
      printUsage()
      System.exit(1)
    }

    i += 1
    dfsDirPath = args(i)
  }

  def runLocalWordCount(fileContents: List[String]): Int = {
    fileContents.flatMap(_.split(" "))
      .flatMap(_.split("\t"))
      .filter(_.nonEmpty)
      .groupBy(w => w)
      .mapValues(_.size)
      .values
      .sum
  }

  def main(args: Array[String]): Unit = {
    parseArgs(args)

    println("Performing local word count")
    val fileContents = readFile(localFilePath.toString())
    val localWordCount = runLocalWordCount(fileContents)

    println("Creating SparkSession")
    val spark = SparkSession
      .builder
      .appName("DFS Read Write Test")
      .getOrCreate()

    println("Writing local file to DFS")
    val dfsFilename = dfsDirPath + "/dfs_read_write_test"
    val fileRDD = spark.sparkContext.parallelize(fileContents)
    fileRDD.saveAsTextFile(dfsFilename)

    println("Reading file from DFS and running Word Count")
    val readFileRDD = spark.sparkContext.textFile(dfsFilename)

    val dfsWordCount = readFileRDD
      .flatMap(_.split(" "))
      .flatMap(_.split("\t"))
      .filter(_.nonEmpty)
      .map(w => (w, 1))
      .countByKey()
      .values
      .sum

    spark.stop()

    if (localWordCount == dfsWordCount) {
      println(s"Success! Local Word Count ($localWordCount) " +
        s"and DFS Word Count ($dfsWordCount) agree.")
    } else {
      println(s"Failure! Local Word Count ($localWordCount) " +
        s"and DFS Word Count ($dfsWordCount) disagree.")
    }

  }
}
// scalastyle:on println
///*
// * Licensed to the Apache Software Foundation (ASF) under one or more
// * contributor license agreements.  See the NOTICE file distributed with
// * this work for additional information regarding copyright ownership.
// * The ASF licenses this file to You under the Apache License, Version 2.0
// * (the "License"); you may not use this file except in compliance with
// * the License.  You may obtain a copy of the License at
// *
// *    http://www.apache.org/licenses/LICENSE-2.0
// *
// * Unless required by applicable law or agreed to in writing, software
// * distributed under the License is distributed on an "AS IS" BASIS,
// * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// * See the License for the specific language governing permissions and
// * limitations under the License.
// */
//
//// scalastyle:off println
//package org.apache.spark.examples.streaming
//
//import kafka.serializer.StringDecoder
//
//import org.apache.spark.streaming._
//import org.apache.spark.streaming.kafka._
//import org.apache.spark.SparkConf
//
///**
// * Consumes messages from one or more topics in Kafka and does wordcount.
// * Usage: DirectKafkaWordCount <brokers> <topics>
// *   <brokers> is a list of one or more Kafka brokers
// *   <topics> is a list of one or more kafka topics to consume from
// *
// * Example:
// *    $ bin/run-example streaming.DirectKafkaWordCount broker1-host:port,broker2-host:port \
// *    topic1,topic2
// */
//object DirectKafkaWordCount {
//  def main(args: Array[String]) {
//    if (args.length < 2) {
//      System.err.println(s"""
//        |Usage: DirectKafkaWordCount <brokers> <topics>
//        |  <brokers> is a list of one or more Kafka brokers
//        |  <topics> is a list of one or more kafka topics to consume from
//        |
//        """.stripMargin)
//      System.exit(1)
//    }
//
//    StreamingExamples.setStreamingLogLevels()
//
//    val Array(brokers, topics) = args
//
//    // Create context with 2 second batch interval
//    val sparkConf = new SparkConf().setAppName("DirectKafkaWordCount")
//    val ssc = new StreamingContext(sparkConf, Seconds(2))
//
//    // Create direct kafka stream with brokers and topics
//    val topicsSet = topics.split(",").toSet
//    val kafkaParams = Map[String, String]("metadata.broker.list" -> brokers)
//    val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](
//      ssc, kafkaParams, topicsSet)
//
//    // Get the lines, split them into words, count the words and print
//    val lines = messages.map(_._2)
//    val words = lines.flatMap(_.split(" "))
//    val wordCounts = words.map(x => (x, 1L)).reduceByKey(_ + _)
//    wordCounts.print()
//
//    // Start the computation
//    ssc.start()
//    ssc.awaitTermination()
//  }
//}
//// scalastyle:on println
package com.bitnei.report.dayreport.distribution

class DiscreteDistribution {
  private val distributed: Array[Int] = Array.fill(13)(0)

  def add(timerange: Int): Unit = {
    val travelTimeLength: Double = timerange.toDouble / (1000 * 3600)
    if (0 <= travelTimeLength && travelTimeLength <= 0.50D) {
      distributed(0) += 1
    } else if (0.50D < travelTimeLength && travelTimeLength <= 1.00D) {
      distributed(1) += 1
    } else if (1.00D < travelTimeLength && travelTimeLength <= 1.50D) {
      distributed(2) += 1
    } else if (1.50D < travelTimeLength && travelTimeLength <= 2.00D) {
      distributed(3) += 1
    } else if (2.00D < travelTimeLength && travelTimeLength <= 2.50D) {
      distributed(4) += 1
    } else if (2.50D < travelTimeLength && travelTimeLength <= 3.00D) {
      distributed(5) += 1
    } else if (3.00D < travelTimeLength && travelTimeLength <= 3.50D) {
      distributed(6) += 1
    } else if (3.50D < travelTimeLength && travelTimeLength <= 4.00D) {
      distributed(7) += 1
    } else if (4.00D < travelTimeLength && travelTimeLength <= 4.50D) {
      distributed(8) += 1
    } else if (4.50D < travelTimeLength && travelTimeLength <= 5.00D) {
      distributed(9) += 1
    } else if (5.00D < travelTimeLength && travelTimeLength <= 5.50D) {
      distributed(10) += 1
    } else if (5.50D < travelTimeLength && travelTimeLength <= 6.00D) {
      distributed(11) += 1
    } else if (6.00D < travelTimeLength) {
      distributed(12) += 1
    }
  }

  def getDistribution: Array[Int] = distributed
}

object  DiscreteDistribution{
  def default: Array[Int] = Array.fill(13)(0)
}
package com.bitnei.report.distribute

trait Distribution {
  def interval: Int = 1

  def index(v: Long): Int = {
    if (v % interval == 0) {
      if (v == 0) 0
      else ((v - interval) / interval).toInt
    } else {
      (v / interval).toInt
    }
  }

  def getDistribution: Array[Int]
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import scala.collection.JavaConverters._

import org.apache.spark.util.Utils

/**
 * Prints out environmental information, sleeps, and then exits. Made to
 * test driver submission in the standalone scheduler.
 */
object DriverSubmissionTest {
  def main(args: Array[String]) {
    if (args.length < 1) {
      println("Usage: DriverSubmissionTest <seconds-to-sleep>")
      System.exit(0)
    }
    val numSecondsToSleep = args(0).toInt

    val env = System.getenv()
    val properties = Utils.getSystemProperties

    println("Environment variables containing SPARK_TEST:")
    env.asScala.filter { case (k, _) => k.contains("SPARK_TEST")}.foreach(println)

    println("System properties containing spark.test:")
    properties.filter { case (k, _) => k.toString.contains("spark.test") }.foreach(println)

    for (i <- 1 until numSecondsToSleep) {
      println(s"Alive for $i out of $numSecondsToSleep seconds")
      Thread.sleep(1000)
    }
  }
}
// scalastyle:on println
package com.bitnei.samples.dataset

import com.bitnei.samples.dataset.DSTest.Person
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.{Encoder, Encoders, SparkSession}
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.functions.expr
//import org.apache.spark.sql.{AggData, ClassBufferAggregator, ClassInputAgg, ComplexBufferAgg, _}

/**
  *
  * @author zhangyongtian
  * @define
  *
  * date: 2017-11-14
  *
  */
object DSTest {

  def main(args: Array[String]): Unit = {

    // TODO: 日志级别设置
    Logger.getLogger("org").setLevel(Level.ERROR)


    val spark = SparkSession
      .builder()
      .appName(this.getClass.getSimpleName)
      .config("spark.some.config.option", "some-value")
      .master("local[*]")
      .getOrCreate()
    import spark.implicits._

    //    val df = spark.read.json("data/people.json")
    //    df.select("name").show()
    //    df.select($"name", $"age" + 1).show()
    //    df.filter($"age" > 21).show()
    //    df.groupBy("age").count().show()
    //    df.createOrReplaceTempView("people")
    //    spark.sql("SELECT * FROM people").show()
    //    df.createGlobalTempView("people")
    //    spark.sql("SELECT * FROM global_temp.people").show()
    //    spark.newSession().sql("SELECT * FROM global_temp.people").show()


    //    val caseClassDS = Seq(Person("Andy", 32)).toDS()
    //    caseClassDS.show()

    // Returns: Array(2, 3, 4)

    //    val primitiveDS = Seq(1, 2, 3).toDS()
    //    primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)


    //    val ds = Seq(ClassData("a", 1), ClassData("a", 2)).toDS()
    //
    //    ds.groupByKey(d => ClassNullableData(d.a, null))
    //
    //      .mapGroups {
    //
    //        case (key, values) => key.a + values.map(_.b).sum
    //
    //      }
    //
    //      .show()


    //    val path = "data/people.json"
    //    val peopleDS = spark.read.json(path).as[Person]
    //    //    val peopleDS = Seq(Person("a", 1), Person("a", 2)).toDS()
    //    peopleDS
    //      .groupByKey(_.name)
    //      .mapGroups {
    //        case (name, persons) => (name, persons.toArray)
    //      }
    //      .show()


    //    val ds = spark.sql("SELECT 'one' AS b, 1 as a").as[AggData]
    //
    //    ds.select(ClassInputAgg.toColumn)
    //
    //    ds.select(expr("avg(a)").as[Double], ClassInputAgg.toColumn)
    //
    //
    //    ds.groupByKey(_.b).agg(ClassInputAgg.toColumn),
    //    ("one", 1))
    //


    val ds = Seq(AggData(1, "x"), AggData(2, "y"), AggData(3, "z")).toDS()

    //    ds.select(ClassBufferAggregator.toColumn)

    //    ds.select(ComplexBufferAgg.toColumn)

    //    var res = df.as[Data]
    //    var i = 0
    //    while (i < numChains) {
    //      res = res.map(func)
    //      i += 1
    //    }
    //    res.queryExecution.toRdd.foreach(_ => Unit)
    //    ,


    spark.stop

  }

  case class Person(name: String, age: Long)

  case class ClassData(a: String, b: Int)

  case class ClassNullableData(a: String, b: Integer)

  case class Man(name: String, toArray: Array[Person])

  case class AggData(a: Int, b: String)

  object ClassInputAgg extends Aggregator[AggData, Int, Int] {
    override def zero: Int = 0

    override def reduce(b: Int, a: AggData): Int = b + a.a

    override def finish(reduction: Int): Int = reduction

    override def merge(b1: Int, b2: Int): Int = b1 + b2

    override def bufferEncoder: Encoder[Int] = Encoders.scalaInt

    override def outputEncoder: Encoder[Int] = Encoders.scalaInt
  }

  case class Data(l: Long, s: String)

}

package com.bitnei.alarm

import java.text.SimpleDateFormat

import com.bitnei.alarm.EmptyDrive.{Input, Output, logInfo}
import com.bitnei.alarm.generator.{MileageRatioGenerator, ParkInfoGenerator, ParkInput}
import com.bitnei.common.constants.Constant
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.util.{NumberUtils, TimeUtils}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 动态里程核查三个指标 （能量消耗率 快充倍率 电池衰减率）
  *
  *                    create 2018-04-04 9:16
  *
  */
object DynamicMileage extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    var date = "20171102"

    //参数校验
    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)


    //空转判定参数
    var kongzhuan_gps_scale = stateConf.getOption("input.kongzhuan.gps.scale").getOrElse(5).toString.toInt
    var kongzhuan_record_count = stateConf.getOption("input.kongzhuan.record.count").getOrElse(10).toString.toInt
    var kongzhuan_record_duration = stateConf.getOption("input.kongzhuan.record.duration").getOrElse(30).toString.toInt

    //停靠点参数
    val MIN_DURATION = stateConf.getOption("input.min_duration").getOrElse(10 * 60).toString.toInt
    val PARK_RADIUS = stateConf.getOption("input.park_radius").getOrElse(1).toString.toInt

    //    val MAX_DISTANCE = stateConf.getOption("input.max_distance").getOrElse(4).toString.toInt
    //    if (MAX_DISTANCE < PARK_RADIUS) {
    //      throw new Exception("input MAX_DISTANCE < PARK_RADIUS error")
    //    }

    //输出参数
    var outputTargets = stateConf.getOption("output").getOrElse("console")
    var outFormat = stateConf.getOption("output.format").getOrElse("#")
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"



    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._

    val sqlContext = sparkSession.sparkContext


    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")

        sparkSession.read.json("data/detail/mock.txt").createOrReplaceTempView("detail")

        //注册config表
        sparkSession.read.parquet("data/dynamicmileageCheck/config/*.parquet").createOrReplaceTempView("config")


//        sparkSession.read.parquet("data/dynamicmileageCheck/config/*.parquet").toJSON.show(false)

      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/result/detail/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("detail")

        //注册config表
        sparkSession.read.parquet("/tmp/spark/vehicle/result/config").createOrReplaceTempView("config")


      }

    }


    ////////////////////////////////////////业务逻辑//////////////////////////////


    val sql =
      s"""
      SELECT
         CAST (d.VID AS String),
         d.startTime,
         d.endTime,
         CAST(d.startMileage * 0.1 AS double) AS startMileage,
         CAST(d.stopMileage * 0.1 AS double) AS stopMileage,
         d.startSoc,
         d.endSoc,
         d.totalCharge,
         d.category AS state,
         CAST(config.ENERGY_STORAGE_TOTAL_CAP AS DOUBLE) AS standendPower,
         CAST(config.chargePower AS DOUBLE ) AS chargePower
       FROM detail d INNER JOIN config ON d.VID=config.VID
       WHERE d.vid IS NOT NULL
      """

    //AND d.startTime like '${date}%

    val initDS = sparkSession.sql(sql).as[DetailModel]

    //过滤

    //获取输入数据
    val result =
      initDS.groupByKey(_.vid)
        .flatMapGroups({ case (vid: String, details: Iterator[DetailModel]) => {

          //首先按照时间排序
          val sortedDetails = details.toArray.sortBy(_.startTime)

          //计算crc闭包值
          val powerConsumptionResult = MileageRatioGenerator.computeCrc(sortedDetails)

          //计算rcr闭包值
          val rcrResult = MileageRatioGenerator.computeRcr(sortedDetails)

          //将crc闭包值和rcr闭包值合并到一起
          powerConsumptionResult ++ rcrResult
        }
        })




    ////////////////////////////////删除临时表#############################################

    sparkSession.catalog.dropTempView("detail")

    sparkSession.catalog.dropTempView("config")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
    }

    if (env.equals("prd")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      if (outputTargets.contains("hdfs")) {

        result.write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }


      //TODO:将SYS_VEH_CONFIG_NUM表cache到HDFS
      val url = stateConf.getString(Constant.JdbcUrl)
      val driver = stateConf.getString(Constant.JdbcDriver)
      val user = stateConf.getString(Constant.JdbcUserName)
      val password = stateConf.getString(Constant.JdbcPasswd)

      val sqlText =
        """
      SELECT
       DISTINCT(veh.UUID) AS VID,
       config.VEH_MODEL_ID,
       config.ENERGY_STORAGE_TOTAL_CAP
      FROM SYS_VEHICLE veh
      INNER JOIN
      SYS_VEH_CONFIG_NUM config
      ON veh.VEH_MODEL_ID=config.VEH_MODEL_ID
      WHERE config.ENERGY_STORAGE_TOTAL_CAP IS NOT NULL
    """.stripMargin

      val tableName = "config"

      sparkSession.read.format("jdbc").options(Map(
        "url" -> url,
        "driver" -> driver,
        "dbtable" -> s"($sqlText) $tableName",
        "user" -> user,
        "password" -> password,
        "numPartitions" -> 5.toString
      )).load.createOrReplaceTempView(tableName)


      sparkSession.sql("SELECT * FROM config")
        .write
        .mode(SaveMode.Overwrite)
        .save("/tmp/spark/vehicle/result/config")


    }

    sparkSession.stop()
  }
}

abstract class Detail extends Serializable {
  val state: String
}

case class DetailModel(vid: String,
                       //开始时间
                       startTime: Long = 0,
                       //结束时间
                       endTime: Long = 0,
                       //开始里程
                       startMileage: Double = 0,
                       //结束里程
                       stopMileage: Double = 0,

                       //开始，结束soc
                       startSoc: Int = 0,
                       endSoc: Int = 0,

                       //总电流
                       totalCharge: Double = 0,

                       //标称能量
                       standendPower: Double = 0,

                       //充电容量
                       chargePower: Double = 0,

                       //充电行驶状态
                       val state: String) extends Detail


/**
  * @param vid               车辆vid
  * @param startTime         开始时间
  * @param endTime           结束时间
  * @param closedType        闭包类型
  * @param socDiff           闭包内开始-结束 soc差值
  * @param mileageDiff       闭包内里程差值
  * @param powerConsumption  能量消耗率
  * @param maxTotalCurrent   闭包内最大总电流
  * @param minTotalCurrent   闭包内最小总电流
  * @param quickChargeFactor 快充倍率
  * @param m                 电池衰减
  **/
case class ClosedResult(
                         vid: String,
                         startTime: Long,
                         endTime: Long,
                         closedType: String, //crc,rcr
                         socDiff: Double,
                         mileageDiff: Double,
                         powerConsumption: Double,
                         maxTotalCurrent: Double,
                         minTotalCurrent: Double,
                         quickChargeFactor: Double,
                         chargePower: Double,
                         m: Double)

case class Config(ENERGY_STORAGE_TOTAL_CAP: Double, vid: String)

//case class Rreal(vid: String, startTime: Long,endTime:Long, startMileage: Double, stopMileage: Double, startSoc: Int, endSoc: Int, totalCharge: Double, category: String,year:Int,month:Int,day:Int)


/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.ElementwiseProduct
import org.apache.spark.mllib.linalg.Vectors
// $example off$

object ElementwiseProductExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("ElementwiseProductExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Create some vector data; also works for sparse vectors
    val data = sc.parallelize(Array(Vectors.dense(1.0, 2.0, 3.0), Vectors.dense(4.0, 5.0, 6.0)))

    val transformingVector = Vectors.dense(0.0, 1.0, 2.0)
    val transformer = new ElementwiseProduct(transformingVector)

    // Batch transform and per-row transform give the same results:
    val transformedData = transformer.transform(data)
    val transformedData2 = data.map(x => transformer.transform(x))
    // $example off$

    println("transformedData: ")
    transformedData.foreach(x => println(x))

    println("transformedData2: ")
    transformedData2.foreach(x => println(x))

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.alarm

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-01-31 11:43
  *
  */
object EmptyDrive extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
////////////////////////////////////////////////














    sparkSession.stop()
  }

}
package com.bitnei.tools.util

/**
  *
  * @author zhangyongtian
  * @define 编码工具类
  *
  * create 2017-12-05 11:24
  *
  */
object EncodUtils {

  def getEncoding(str: String): String = {
    var encode = "GB2312"
    try
        if (str == new String(str.getBytes(encode), encode)) {
          val s = encode
          System.out.println("*******---------------encodeing:" + encode)
          return s
        }
    catch {
      case exception: Exception =>

    }
    encode = "ISO-8859-1"
    try
        if (str == new String(str.getBytes(encode), encode)) {
          val s1 = encode
          System.out.println("*******---------------encodeing:" + encode)
          return s1
        }
    catch {
      case exception1: Exception =>

    }
    encode = "UTF-8"
    try
        if (str == new String(str.getBytes(encode), encode)) {
          val s2 = encode
          System.out.println("*******---------------encodeing:" + encode)
          return s2
        }
    catch {
      case exception2: Exception =>

    }
    encode = "GBK"
    try
        if (str == new String(str.getBytes(encode), encode)) {
          val s3 = encode
          System.out.println("*******---------------encodeing:" + encode)
          return s3
        }
    catch {
      case exception3: Exception =>

    }
    System.out.println("*******---------------encodeing:" + encode)
    ""
  }

  /**
    * @Title:string2HexString
    * @Description:字符串转16进制字符串
    * @param strPart
    * 字符串
    * @return 16进制字符串
    * @throws
    */
  def string2HexString(strPart: String): String = {
    val hexString = new StringBuffer
    var i = 0
    while ( {
      i < strPart.length
    }) {
      val ch = strPart.charAt(i).toInt
      val strHex = Integer.toHexString(ch)
      hexString.append(strHex)

      {
        i += 1;
        i - 1
      }
    }
    hexString.toString
  }

  /**
    * @Title:hexString2String
    * @Description:16进制字符串转字符串
    * @param src
    * 16进制字符串
    * @return 字节数组
    * @throws
    */
  def hexString2String(src: String): String = {
    var temp = ""
    var i = 0
    while ( {
      i < src.length / 2
    }) {
      temp = temp + Integer.valueOf(src.substring(i * 2, i * 2 + 2), 16).byteValue.toChar

      {
        i += 1;
        i - 1
      }
    }
    temp
  }

  /**
    * @Title:bytes2HexString
    * @Description:字节数组转16进制字符串
    * @param b
    * 字节数组
    * @return 16进制字符串
    * @throws
    */
  //  def bytes2HexString(b: Array[Byte]): String = {
  //    val result = new StringBuffer
  //    var hex = null
  //    var i = 0
  //    while ( {
  //      i < b.length
  //    }) {
  //      hex = Integer.toHexString(b(i) & 0xFF)
  //      if (hex.length == 1) hex = '0' + hex
  //      result.append(hex.toUpperCase)
  //
  //      {
  //        i += 1;
  //        i - 1
  //      }
  //    }
  //    result.toString
  //  }

  /**
    * @Title:hexString2Bytes
    * @Description:16进制字符串转字节数组
    * @param src
    * 16进制字符串
    * @return 字节数组
    * @throws
    */
  def hexString2Bytes(src: String): Array[Byte] = {
    val l = src.length / 2
    val ret = new Array[Byte](l)
    var i = 0
    while ( {
      i < l
    }) {
      ret(i) = Integer.valueOf(src.substring(i * 2, i * 2 + 2), 16).byteValue.toByte

      {
        i += 1;
        i - 1
      }
    }
    ret
  }


  /**
    * @Title:char2Byte
    * @Description:字符转成字节数据char-- >integer-->byte
    * @param src
    * @return
    * @throws
    */
  def char2Byte(src: Character): Byte = Integer.valueOf(src.asInstanceOf[Int]).byteValue

  /**
    * @Title:intToHexString
    * @Description:10进制数字转成16进制
    * @param a   转化数据
    * @param len 占用字节数
    * @return
    * @throws
    */
  //  private def intToHexString(a: Int, len: Int) = {
  //    len <<= 1
  //    var hexString = Integer.toHexString(a)
  //    val b = len - hexString.length
  //    if (b > 0) {
  //      var i = 0
  //      while ( {
  //        i < b
  //      }) {
  //        hexString = "0" + hexString
  //
  //        {
  //          i += 1;
  //          i - 1
  //        }
  //      }
  //    }
  //    hexString
  //  }


}
package com.bitnei.report

/*
* created by wangbaosheng on 2017/12/14
*/

trait EndoMonoid[A]{
  def op(a:A,b:A):A
  def zero():A
}



/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.Row
// $example off$
import org.apache.spark.sql.SparkSession

object EstimatorTransformerParamExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("EstimatorTransformerParamExample")
      .getOrCreate()

    // $example on$
    // Prepare training data from a list of (label, features) tuples.
    val training = spark.createDataFrame(Seq(
      (1.0, Vectors.dense(0.0, 1.1, 0.1)),
      (0.0, Vectors.dense(2.0, 1.0, -1.0)),
      (0.0, Vectors.dense(2.0, 1.3, 1.0)),
      (1.0, Vectors.dense(0.0, 1.2, -0.5))
    )).toDF("label", "features")

    // Create a LogisticRegression instance. This instance is an Estimator.
    val lr = new LogisticRegression()
    // Print out the parameters, documentation, and any default values.
    println("LogisticRegression parameters:\n" + lr.explainParams() + "\n")

    // We may set parameters using setter methods.
    lr.setMaxIter(10)
      .setRegParam(0.01)

    // Learn a LogisticRegression model. This uses the parameters stored in lr.
    val model1 = lr.fit(training)
    // Since model1 is a Model (i.e., a Transformer produced by an Estimator),
    // we can view the parameters it used during fit().
    // This prints the parameter (name: value) pairs, where names are unique IDs for this
    // LogisticRegression instance.
    println("Model 1 was fit using parameters: " + model1.parent.extractParamMap)

    // We may alternatively specify parameters using a ParamMap,
    // which supports several methods for specifying parameters.
    val paramMap = ParamMap(lr.maxIter -> 20)
      .put(lr.maxIter, 30)  // Specify 1 Param. This overwrites the original maxIter.
      .put(lr.regParam -> 0.1, lr.threshold -> 0.55)  // Specify multiple Params.

    // One can also combine ParamMaps.
    val paramMap2 = ParamMap(lr.probabilityCol -> "myProbability")  // Change output column name.
    val paramMapCombined = paramMap ++ paramMap2

    // Now learn a new model using the paramMapCombined parameters.
    // paramMapCombined overrides all parameters set earlier via lr.set* methods.
    val model2 = lr.fit(training, paramMapCombined)
    println("Model 2 was fit using parameters: " + model2.parent.extractParamMap)

    // Prepare test data.
    val test = spark.createDataFrame(Seq(
      (1.0, Vectors.dense(-1.0, 1.5, 1.3)),
      (0.0, Vectors.dense(3.0, 2.0, -0.1)),
      (1.0, Vectors.dense(0.0, 2.2, -1.5))
    )).toDF("label", "features")

    // Make predictions on test data using the Transformer.transform() method.
    // LogisticRegression.transform will only use the 'features' column.
    // Note that model2.transform() outputs a 'myProbability' column instead of the usual
    // 'probability' column since we renamed the lr.probabilityCol parameter previously.
    model2.transform(test)
      .select("features", "label", "myProbability", "prediction")
      .collect()
      .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =>
        println(s"($features, $label) -> prob=$prob, prediction=$prediction")
      }
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples

import org.apache.spark.sql.SparkSession

object ExceptionHandlingTest {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("ExceptionHandlingTest")
      .getOrCreate()

    spark.sparkContext.parallelize(0 until spark.sparkContext.defaultParallelism).foreach { i =>
      if (math.random > 0.75) {
        throw new Exception("Testing exception handling")
      }
    }

    spark.stop()
  }
}
package com.bitnei.report.tempjob

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.SparkHelper
import org.apache.spark.sql.{DataFrame, SparkSession}

import scala.io.Source

/**
  * Created by wangbaosheng on 2017/6/13.
  */
case class UUIDLicensePlate(vid:String,LICENSE_PLATE:String)
case class Result(LICENSE_PLATE:String,value:String)

class Export(stateConf:StateConf,sparkSession: SparkSession) extends  Serializable with Logging{

  import  sparkSession.implicits._

  def run(): Unit = {
    val inputDirectory = stateConf.getString("input.directory")
    val startDate = Utils.parsetDate(stateConf.getString("startDate"), "yyyyMMdd").get
    val endDate = Utils.parsetDate(stateConf.getString("endDate"), "yyyyMMdd").get
    registeUuidTable()

    Utils.buildPath(inputDirectory, startDate, endDate).foreach(inputPath => {
      try{
        logInfo(s"begining $inputPath")
        registeRealinfo(inputPath)

        stateConf.getOption("show") match {
          case Some("true") => sparkSession.sql("select * from uuidTable").show(10)
            sparkSession.sql("select * from realinfo").show(10)
          case _ =>
        }

        //数据项：车牌号，实时数据时间，速度，里程，SOC，剩余能量，总电流（排序不分前后）


        val result=compute()


        val tokens = inputPath.split('/')
        val outputDate = tokens(tokens.length - 3) + "/" + tokens(tokens.length - 2) + "/" + tokens(tokens.length - 1)
        val outputPath = stateConf.getString("output.directory") + "/" + outputDate

        save(result, outputPath)

        sparkSession.catalog.dropTempView("realinfo")
      }catch {
        case e:Exception=>
          logError(s"$inputPath error ${e.getMessage}")
      }
    })

  }

  def registeRealinfo(inputPath:String): Unit ={
    sparkSession.read.parquet(inputPath).createOrReplaceTempView("realinfo")
  }

  def registeUuidTable(): Unit ={
    val uuidTable=Source.fromFile(stateConf.getString("uuidTable")).getLines().map(line=>{
      val fields=line.split(',')
      UUIDLicensePlate(LICENSE_PLATE=fields(0),vid=fields(1))
    })

    sparkSession.createDataFrame(uuidTable.toSeq).createOrReplaceTempView("uuidTable")
  }

  def save(result:DataFrame,outputPath:String): Unit ={
    result.map(row=>{

      s"${row.getAs("LICENSE_PLATE")},${Utils.formatDate("yyyyMMddHHmmss","yyyy-MM-dd HH:mm:ss",row.getAs("time"))},${row.getAs("speed")},${row.getAs("mileage")},${row.getAs("soc")},${row.getAs("2616")},${row.getAs("charge")}"
    }).repartition(1).write.text(outputPath)
   // result.map().write.partitionBy("LICENSE_PLATE").text(outputPath)
  }

  def compute():DataFrame= {
    sparkSession.sql("select uuidtable.LICENSE_PLATE as LICENSE_PLATE,time,`2201` as speed,`2202` as mileage,`2615` as soc,`2616`,`2614` as charge  from realinfo join uuidtable on realinfo.vid=uuidtable.vid")

  }
}


object  Export{
  def main(args: Array[String]): Unit = {
    val stateConf=new StateConf
    stateConf.add(args)
    new Export(stateConf,SparkHelper.getSparkSession(sparkMaster = None)).run()
  }
}

package com.bitnei.report.tempjob

import java.util.Date
import java.util.concurrent.{ExecutorService, Executors}

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.{DataPrecision, FileWriter, StringParser, Utils}
import com.bitnei.report.tempjob.ExportCoor.setFilter
import com.bitnei.sparkhelper.HbaseHelper
import org.apache.hadoop.hbase.{Cell, CellUtil, TableName}
import org.apache.hadoop.hbase.client.{Scan, Table}
import org.apache.hadoop.hbase.util.Bytes

import scala.collection.mutable.ArrayBuffer

/**
  * Created by wangbaosheng on 2017/7/17.
  */
object ExportCoor {
  val stateConf = new StateConf
  def main(args: Array[String]): Unit = {

    stateConf.add(args)

    doRun(0)
  }

  def doRun(threadNum:Int): Unit = {
    var i = 0

    processEachVehicle((writer, uuid, number, startTime, endTime, table) => {
      println(s"begin process $i,$number....")
      val (firstMileage, firstOnlineTime, lastMileage, lastOnlineTime) = getResult(table, uuid, startTime, endTime)
      writer.write(
        //s","+s"$firstMileage,"+
        //s"${if(firstOnlineTime.nonEmpty) Utils.formatDate("yyyyMMddHHmmss", "yyyy-MM-dd HH:mm:ss",firstOnlineTime) else ""}"+
        s"$number," +
          s"${DataPrecision.mileage(lastMileage)}," +
          s"${if (lastOnlineTime.nonEmpty) Utils.formatDate("yyyyMMddHHmmss", "yyyy-MM-dd HH:mm:ss", lastOnlineTime) else ""}")
      i += 1
      writer.flush()
    })
  }

  def getResult(table:Table,uuid:String,startTime:Long,endTime:Long):(Int,String,Int,String)= {
    def getLast(startTime: Long, endTime: Long): (String, Int) = {
      var resultDate = toChecking(table,uuid,startTime, endTime, 360)
      val result = getBy(resultDate, endTime)
      println(s"getting ${Utils.formatDate(new Date(resultDate), "yyyy-MM-dd HH:mm:ss")},mileage=${result._3}")
      (result._4, result._3)
    }


    def getBy(startTime: Long, endTime: Long): (Int, String, Int, String) = {
      var firstMileage = 0
      var lastMileage = 0
      var firstOnlineTime = ""
      var lastOnlineTime = ""

      val scan = setFilter(uuid, startTime, endTime)
      scan.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("2202"))
      scan.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("2000"))

      val scanner = table.getScanner(scan)
      val iter = scanner.iterator()


      while (iter.hasNext) {
        val result = iter.next()
        val cells = result.rawCells()


        cells.foreach(cell => {
          val qualifier = Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength)

          if (qualifier == "2202") {
            val mileage = try {
              val v = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength).trim
              if (v.nonEmpty) v.toInt
              else 0
            } catch {
              case e: Exception =>
                println(e.getMessage)
                0
            }

            if (mileage != 0) {
              if (firstMileage == 0) {
                firstMileage = mileage
                firstOnlineTime = getTime(cells)
              }
              lastMileage = mileage
              lastOnlineTime = getTime(cells)

            }
          }
        })
      }

      scanner.close()
      (firstMileage, firstOnlineTime, lastMileage, lastOnlineTime)
    }


    def getTime(cells: Array[Cell]): String = {
      var time = ""
      cells.foreach(cell => {
        val qualifier = Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength)
        if (qualifier == "2000") {
          time = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)
        }
      })
      time
    }

    val (lastTime, lastMileage) = getLast(startTime, endTime)

    (0, "", lastMileage, lastTime)
  }

  def toChecking(table:Table,uuid:String,startTime: Long, endTime: Long, intervelDate: Int): Long = {
    val curDate = new Date(startTime)
    var haveValue = true

    var resultDate = startTime

    while (curDate.getTime < endTime && haveValue) {
      val x= haveDate(table, uuid,curDate.getTime, endTime)

      haveValue=x._1

      println(s"checking ${Utils.formatDate(curDate, "yyyy-MM-dd HH:mm:ss")},interval=$intervelDate,$haveValue,${x._2}")

      if (haveValue) {
        resultDate = curDate.getTime
      }

      curDate.setDate(curDate.getDate + intervelDate)
    }

    if (intervelDate <= 3) resultDate
    else toChecking(table, uuid, resultDate, endTime, intervelDate / 2)
  }

  def haveDate(table:Table,uuid:String,startTime: Long, endTime: Long): (Boolean,Int) = {
    var firstMileage = 0
    var lastMileage = 0
    var firstOnlineTime = ""
    var lastOnlineTime = ""

    val scan = setFilter(uuid, startTime, endTime)
    scan.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("2202"))
    scan.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("2000"))

    val scanner = table.getScanner(scan)
    val iter = scanner.iterator()

    var haveDate = false
    while (iter.hasNext && !haveDate) {
      val result = iter.next()
      val cells = result.rawCells()


      cells.foreach(cell => {
        val qualifier = Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength)

        if (qualifier == "2202") {
          val mileage = try {
            val v = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength).trim
            if (v.nonEmpty) v.toInt
            else 0
          } catch {
            case e: Exception =>
              println(e.getMessage)
              0
          }

          if (mileage != 0) {
            haveDate = true
            lastMileage=mileage
          }
        }
      })
    }

    scanner.close()
    (haveDate,lastMileage)
  }



  def processEachVehicle(f:(FileWriter,String,String,Long,Long,Table)=>Unit): Unit = {
    def splitToBatch(batchCount: Int): Array[Array[String]] = {
      val allUuids = Utils.readAllLines(stateConf.getString("uuidTable"))
      val uuidBatchs = new ArrayBuffer[ArrayBuffer[String]]()
      val batchLength = if (allUuids.length % batchCount == 0) allUuids.length / batchCount else allUuids.length / batchCount + 1


      (0 until batchCount).foreach(batchIndex => {
        uuidBatchs.append(new ArrayBuffer[String]())
        (batchIndex * batchLength until  (batchIndex+1) * batchLength ).foreach(i => {
          if(i<allUuids.length) uuidBatchs(batchIndex).append(allUuids(i))
        })
      })

      uuidBatchs.map(_.toArray).toArray
    }


    val batchs = splitToBatch(stateConf.getOption("batchCount").map(_.toInt).getOrElse(4))

    println(s"batch count:${batchs.length}")
    val threads = Executors.newFixedThreadPool(batchs.length)
    batchs.zipWithIndex.foreach(batchIndexPair => {
      val (batch,batchId)=batchIndexPair
      threads.execute(new Runnable {
        override def run(): Unit = {
          doRun(batchId,batch)
        }
      })
    })


    def doRun(batchId:Int,batch: Array[String]): Unit = {
      val hbaseConnection = HbaseHelper.getConnection(stateConf.getString("zk"), stateConf.getString("zkport"))
      val tableName = stateConf.getOption("hbase.table").getOrElse("realinfo")
      val table = hbaseConnection.getTable(TableName.valueOf(tableName))
      val writer = new FileWriter(stateConf.getString("output.directory") + s"_$batchId")

      batch.foreach(line => {
        val map = StringParser.toMap(line)
        val uuid = map("uuid").trim
        //转换为UTC
        val startTime = if (map.contains("starttime")) Utils.getTime(map("starttime")).get else 0
        val endTime = if (map.contains("endtime")) Utils.getTime(map("endtime")).get else Long.MaxValue

        val number = map("number").trim
        f(writer,uuid, number, startTime, endTime, table)
      })

      writer.flush()
      writer.close()
      table.close()
      hbaseConnection.close()
    }

    // doRun(Utils.readAllLines(stateConf.getString("uuidTable")))
  }


  def setFilter(uuid:String,startTime:Long,endTime:Long): Scan = {
    val scan = new Scan()
    val startRow = Bytes.toBytes(s"${uuid}_$startTime")
    val stopRow = Bytes.toBytes(s"${uuid}_$endTime")

    scan.setStartRow(startRow)
    scan.setStopRow(stopRow)

    //val rowFilter:Filter=new RowFilter(CompareFilter.CompareOp.GREATER_OR_EQUAL,new BinaryComparator(startRow))

    //  scan.setFilter(rowFilter)
    scan
  }

  def  byte2HexStr( b:Array[Byte]):String= byte2HexStr(b,0,b.length)

  def  byte2HexStr( b:Array[Byte],offset:Int,length:Int):String= {
    var hs = "";
    var stmp = "";
    var n = offset

    while (n < length) {
      stmp = Integer.toHexString(b(n) & 0XFF)
      if (stmp.length() == 1)
        hs = hs + "0" + stmp;
      else
        hs = hs + stmp;
      n += 1
    }
     hs.toUpperCase();
  }


}


package com.bitnei.report.tempjob

import com.bitnei.report.{Job, JobRunner}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.SystemClock
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.SparkHelper
import org.apache.spark.SparkConf
import org.apache.spark.sql.{Dataset, SparkSession}

import scala.io.Source
case class  TaxisAndRealInfo(    vid:String,
                                 time:String,
                                 soc:Option[Int],
                                 licensePlate:String,
                                 dictName:String,
                                 modelName:String,
                                 areaName:String,
                                 ruleId:String)

case class ExportGbExceptionResult(vid:String,licensePlate:String,ruleId:String,totalCount:Long,exceptionCount:Long,time:String,soc:Int){
  override def toString: String = s"$vid,$licensePlate,$ruleId,$exceptionCount,$time,$soc"
}
/**
  * Created by wangbaosheng on 2017/7/17.
  */
class ExportGbException(@transient sparkSession: SparkSession,stateConf:StateConf,inputPath:Array[String])
  extends Serializable with Logging with Job {
  private val clock = new SystemClock

  import sparkSession.implicits._

  @transient private val conf = stateConf.getOption(Constant.SPARK_DEPLOY_LOCAL) match {
    case Some(master) if master.startsWith("local") => new SparkConf().setMaster(master)
    case _ => new SparkConf()
  }


  @transient private val sqlContext = sparkSession.sqlContext

  override type R = ExportGbExceptionResult

  override def registerIfNeed(): Unit = {
    //uuid-><车牌，类别，类型，行政区域>
    def registerTaxisTable() {
      def getSql: String = {
        val sql = new StringBuffer()
        //<uuid,车牌，类别，类型，行政区域>
        val lines = Source.fromFile(stateConf.getString("temp.taxis.report.conf")).getLines()
        val values = lines.map(line => {
          val keyvalue = line.split('=')
          (keyvalue(0), keyvalue(1))
        }).toMap

        val vehType = values("vehicle.type")
        val vehModel = values("vehicle.model").split(',')
        val areaName = values("vehicle.area").split(',')

        //Sys_vehicle 车辆表
        //sys_dict 车辆类别表，比如出租车，环卫车等
        //sys_veh_model 车辆类型，比如E200EV电动出租车,首望500e电动出租车,迷迪电动出租车，ID列是整数。
        //sys_area 区域表
        sql.append("SELECT uuid, SYS_VEHICLE.LICENSE_PLATE AS licensePlate, SYS_DICT.NAME AS dictName, SYS_VEH_MODEL.VEH_MODEL_NAME AS modelName, SYS_AREA.NAME AS areaName,SYS_RULE.ID as ruleID ")
        sql.append(" FROM SYS_VEHICLE, SYS_DICT, SYS_AREA, SYS_VEH_MODEL,SYS_RULE")
        sql.append(" WHERE SYS_RULE.ID=SYS_VEHICLE.RULE_ID AND SYS_RULE.ID=21 AND SYS_VEHICLE.VEH_TYPE_ID = SYS_DICT.ID")
        sql.append(" AND SYS_DICT.NAME = '" + vehType + "'")
        sql.append(" AND SYS_VEHICLE.VEH_MODEL_ID = SYS_VEH_MODEL.ID")


        vehModel.indices.foreach(i => {
          sql.append(
            if (i == 0) " AND (SYS_VEH_MODEL.VEH_MODEL_NAME = '" + vehModel(i) + "'"
            else " OR SYS_VEH_MODEL.VEH_MODEL_NAME = '" + vehModel(i) + "'")
        })

        sql.append(")")

        sql.append(" AND SYS_VEHICLE.SYS_DIVISION_ID = SYS_AREA.ID")

        areaName.indices.foreach(i => {
          sql.append(
            if (i == 0) " AND (SYS_AREA.NAME = '" + areaName(0) + "'"
            else " OR SYS_AREA.NAME = '" + areaName(i) + "'")
        })

        sql.append(")")
        sql.toString
      }


      val url = stateConf.getString(Constant.JdbcUrl)
      val driver = stateConf.getString(Constant.JdbcDriver)
      val user = stateConf.getString(Constant.JdbcUserName)
      val password = stateConf.getString(Constant.JdbcPasswd)

      logInfo(s"url=$user,driver=$driver,user=$user,password=$password")
      SparkHelper.registerJdbcAsTempView(sqlContext, url, driver, "SYS_VEHICLE", user, password)
      SparkHelper.registerJdbcAsTempView(sqlContext, url, driver, "SYS_DICT", user, password)
      SparkHelper.registerJdbcAsTempView(sqlContext, url, driver, "SYS_AREA", user, password)
      SparkHelper.registerJdbcAsTempView(sqlContext, url, driver, "SYS_VEH_MODEL", user, password)
      SparkHelper.registerJdbcAsTempView(sqlContext, url, driver, "SYS_RULE", user, password)


      sqlContext.sql(getSql).createOrReplaceTempView("taxis")

    }

    def registerRealInfo() {
      val format = stateConf.getString("temp.taxis.report.input.format")
      val schema = Array(
        "name:VID,type:StringType,nullable:false,alias:VID,default: ",
        "name:TIME,type:StringType,nullable:false,alias:TIME,default: ",
        "name:2615,type:IntegerType,nullable:true,alias:soc ")
      SparkHelper.parquetOrText(sqlContext, inputPath, format, schema).createOrReplaceTempView("realinfo")
    }

    registerTaxisTable()
    registerRealInfo()
  }

  override def unRegister(): Unit = {
    sparkSession.catalog.dropTempView("taxis")
    sparkSession.catalog.dropTempView("realinfo")
  }


  override def doCompute[Product <: ExportGbExceptionResult](): Dataset[ExportGbExceptionResult] = {
    val realinfoDs = sqlContext.sql("SELECT t.licensePlate,t.dictName,t.modelName,t.areaName ,t.ruleID,  " +
      "r.VID,r.soc, r.TIME ,r.VID " +
      "FROM realinfo as r join taxis as t on r.vid=t.uuid").as[TaxisAndRealInfo]

   realinfoDs.groupByKey(_.vid)
      .flatMapGroups((vid: String, rows: Iterator[TaxisAndRealInfo]) => {
        var exceptionCount = 0
        var totalCount = 0

        var licensePlate = ""
        var time = ""
        var soc = 0
        var ruleId = "1"

        rows.foreach(row => {
          if (row.ruleId == "21" && row.soc.nonEmpty && row.soc.get > 100) {
            exceptionCount += 1
          }

          licensePlate = row.licensePlate
          time = row.time
          if (row.soc.nonEmpty) soc = row.soc.get
          totalCount += 1
          ruleId = row.ruleId
        })

        if (ruleId == "21") {
          Array(ExportGbExceptionResult(vid, licensePlate, ruleId, totalCount, exceptionCount, time, soc))
        } else Array.empty[ExportGbExceptionResult]
      })
  }


  override def write[Product <: ExportGbExceptionResult](result: Dataset[ExportGbExceptionResult]): Unit = {
    val hdfs = stateConf.getOption("hdfs").getOrElse("hdfs://nameservice1:8020")
    val outputPath = hdfs + stateConf.getString("temp.taxis.report.output.path")


    write(sqlContext, result,
      format = stateConf.getString("temp.taxis.report.output.format"),
      compression = stateConf.getOption("temp.taxis.report.output.compression"),
      partitionColumn = None,
      partitionNum = stateConf.getOption("temp.taxis.report.output.filecount").map(_.toInt),
      outputPath = outputPath)
  }
}


object  ExportGbException {
  def main(args: Array[String]): Unit = {
    new JobRunner(args).run(
      (stateConf, sparkSession, endDate, path) => new ExportGbException(sparkSession, stateConf, path))
  }
}package com.bitnei.report.tempjob

import java.text.SimpleDateFormat
import java.util.Date

import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.sparkhelper.SparkHelper
import org.apache.spark.sql.SparkSession

import scala.io.Source

/**
  * Created by wangbaosheng on 2017/6/28.
  */

case class RunDetail(vid:String,start_time:Long,end_time:Long)
case class Realinfo(vid:String,time:Long,longitude:Long,latitude:Long){
  override def toString: String = s"$vid,${Utils.formatDate(new Date(time))},${DataPrecision.latitude(longitude)},${DataPrecision.latitude(latitude)}"
}


object ExportLongtitude {

  val debug=false

  def toTime(x:String):Long={
    new SimpleDateFormat("yyyyMMddHHmmss").parse(x).getTime
  }

  def main(args: Array[String]): Unit = {
    val spark = SparkHelper.getSparkSession(sparkMaster =None)

    import spark.sqlContext.implicits._

    spark.udf.register("converttotime", (x: String)=>toTime(x))

    createRunDetailTable(spark)
    createRealinfoTable(spark)


    val fr=spark.sql(
      """
                    select rl.vid,rl.realtime as time,rl.longitude,rl.latitude
                    from realinfo rl
                     where rl.vid in(select vid from rundetail)
      """.stripMargin).as[Realinfo]


    fr.repartition(1).map(_.toString()).write.text("/spark/locus/20170627fulldata")
//
//    spark.sqlContext.sql(
//      """
//            select  fr.vid,fr.realtime as time,fr.longitude,fr.latitude
//            from fr join rundetail rd ON fr.vid=rd.vid AND fr.realtime>=rd.start_time AND fr.realtime<=rd.end_time
//      """.stripMargin).show(10)
//
//
//
//
//    val result = spark.sqlContext.sql(
//      """
//            select  fr.vid,fr.realtime as time,fr.longitude,fr.latitude
//            from(
//              select rl.vid,rl.realtime,rl.longitude,rl.latitude
//              from realinfo rl
//              where rl.vid in(select vid from rundetail)
//            ) fr join rundetail rd ON fr.vid=rd.vid AND fr.realtime>=rd.start_time AND fr.realtime<=rd.end_time
//      """.stripMargin).as[Realinfo]
//    result.show(10)
//    result.map(_.toString).repartition(1).write.text("/spark/locus/20170627")
  }

  def createRealinfoTable(spark:SparkSession): Unit = {
    import spark.sqlContext.implicits._
    if (debug) {
      spark.createDataset(Array(
        Realinfo("1", 1498514813000L, 10000, 10000),
        Realinfo("1", 1498514813001L, 10000, 10000),
        Realinfo("1", 1498514813003L, 10000, 10000),
        Realinfo("1", 1498514813004L, 10000, 10000),
        Realinfo("1", 1498514813005L, 10000, 10000),
        Realinfo("1", 1498514813006L, 10000, 10000),
        Realinfo("2", 0, 10000, 10000),
        Realinfo("2", 11, 10000, 10000))).createOrReplaceTempView("realinfo")
    } else {

      spark.read.parquet("/spark/vehicle/data/realinfo/2017/07/01")
        .withColumnRenamed("2502", "longitude")
        .withColumnRenamed("2503", "latitude").createOrReplaceTempView("realinfo1")

      val realinfo = spark.sqlContext.sql(" select vid,converttotime(time) as realtime,longitude,latitude from realinfo1")
       realinfo.show()
      realinfo.createOrReplaceTempView("realinfo")
    }
  }


  def createRunDetailTable(spark:SparkSession): Unit = {
    import spark.implicits._
    val rundetail = getRunDetail()
    spark.createDataset(rundetail).createOrReplaceTempView("rundetail")
    //spark.sqlContext.sql("select * from rundetail").show(5)
  }

  def getRunDetail(path: String = "/usr/local/sparkjob/tempjob/rundetail"): Array[RunDetail] = {
    if(debug){

          Array(RunDetail("1",0 , 10),
            RunDetail("1", 15, 20),
            RunDetail("1", 21, 30),
            RunDetail("2", 1, 3),
            RunDetail("2", 10, 30))
    }else {
      Source.fromFile(path).getLines().map(line => {
        val fields = line.split(',').map(_.trim)
        val v = RunDetail(vid = fields(0), start_time = Utils.getTime(fields(1)).get, end_time = Utils.getTime(fields(2)).get)
        println(v)
        v
      }).toArray
    }

  }
}
package com.bitnei.report.tempjob

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.{DataPrecision, FileWriter, Utils}
import com.bitnei.sparkhelper.HbaseHelper
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.{Scan, Table}
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp
import org.apache.hadoop.hbase.filter.{BinaryComparator, FilterList, RowFilter}
import org.apache.hadoop.hbase.util.Bytes

/**
  * Created by wangbaosheng on 2017/7/28.
  */
object ExportMileage {
  def main(args: Array[String]): Unit = {

    val stateConf = new StateConf
    stateConf.add(args)

    val uuidList = stateConf.getString("uuid").split(',')

    val endTime = Utils.getTime(stateConf.getString("endTime"), "yyyyMMddHHmmss").get
    val path = s"${stateConf.getString(s"output.path")}_$endTime"

    val fw = new FileWriter(path)
    val hbaseConnection = HbaseHelper.getConnection(stateConf.getString("zk"), stateConf.getString("zkport"))

    val tableName = stateConf.getOption("hbase.table").getOrElse("realinfo")
    val table = hbaseConnection.getTable(TableName.valueOf(tableName))

    uuidList.foreach(uuid => {
      write(stateConf, table, uuid, endTime, fw)
    })

    fw.flush()
    fw.close()

    table.close()
    hbaseConnection.close()
  }

  def write(stateConf: StateConf, table: Table, uuid: String, endTime: Long, fw: FileWriter): Unit = {
    val scanner = table.getScanner(setFilter(uuid, endTime))
    val iter = scanner.iterator()

    var continue = true
    while (iter.hasNext && continue) {
      val result = iter.next()

      val cells = result.rawCells()
      var mileage: Double = 0
      var realTime = ""

      cells.foreach(cell => {
        val qualifier = Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength)

        if (qualifier == "2202") {
          mileage = DataPrecision.mileage(Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength).toLong)
        } else if (qualifier == "2000") {
          realTime = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)
        }
      })


      //println(line)
      if (stateConf.getOption("show").contains("true")) {
        val rowKey = Bytes.toString(result.getRow)
        val t = rowKey.split('_')

        println(s"rowkey=${t(0)}_${Utils.formatDate(new Date(t(1).toLong)).format("yyyy-MM-dd HH:mm:ss")},value=$mileage")
      }

      if (mileage <= 0) {
        continue = true
      } else {
        continue = false
        fw.write(s"$realTime,$mileage")
      }
    }

    scanner.close()
  }

  def setFilter(uuid: String, endTime: Long): Scan = {
    val scan = new Scan()
    val stopRow = Bytes.toBytes(s"${uuid}_$endTime")


    val filterList = new FilterList()
    val rowFilter = new RowFilter(CompareOp.LESS_OR_EQUAL, new BinaryComparator(stopRow))
    filterList.addFilter(rowFilter)

    scan.setFilter(filterList)
    scan
  }
}
package com.bitnei.report.tempjob

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.tempjob.FaultValue.logInfo
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.util.Try

/*
* created by wangbaosheng on 2017/12/8
*/

case class VIN(vin:String)
case class MileageCheckResult(vin:Option[String],vid:Option[String],time:Option[String],v3201:Option[Long],v2201:Option[Long],v2202:Option[Long],v2613:Option[Long],
                              v2614:Option[Long],v2615:Option[Long],v2502:Option[Long],v2503:Option[Long]){
  override def toString: String = s"vid:${vid.getOrElse("")} time:${time.getOrElse("")} 3201:${v3201.map(_.toString).getOrElse("")} 2201:${v2201.map(_.toString).getOrElse("")} 2202:${v2202.map(_.toString).getOrElse("")} 2613:${v2613.map(_.toString).getOrElse("")} 2614:${v2614.map(x=>(x+10000).toString).getOrElse("")} 2615:${v2615.map(_.toString).getOrElse("")} 2502:${v2502.map(_.toString).getOrElse("")} 2503:${v2503.map(_.toString).getOrElse("")}"
}

object ExportMileageCheck extends Logging {
  val stateConf = new StateConf
  val spark = SparkSession.builder().getOrCreate()

  import spark.implicits._

  def main(args: Array[String]): Unit = {
    stateConf.add(args)

    val inputDirectory = stateConf.getString("vin.directory")
    val vinList = Utils.readAllLines(inputDirectory).map(VIN(_))

    spark.createDataset[VIN](vinList).createOrReplaceTempView("vintable")
    spark.read.parquet(stateConf.getString("realinfo.directory")).createOrReplaceTempView("realinfo")

    val startDate = Utils.parsetDate(stateConf.getOption("startDate").getOrElse("20160101000000")).get
    val endDate = Utils.parsetDate(stateConf.getOption("endDate").getOrElse("20171206000000")).get

    while (startDate.getTime < endDate.getTime) {
      val year = Utils.formatDate(startDate, "yyyy")
      val month = Utils.formatDate(startDate, "MM")
      val day = Utils.formatDate(startDate, "dd")

      logInfo(s"begin process $year-$month-$day")
      doCompute(year.toInt, month.toInt,day.toInt)
      startDate.setMonth(startDate.getMonth + 1)
    }
  }


  def doCompute(year: Int, month: Int,day:Int): Unit = {
    try {
      val result=spark.sql(
        s"""
        SELECT realinfo.vin,realinfo.VID,realinfo.TIME,
        CAST(`3201` AS Int) AS v3201,
        `2201` AS v2201,
        `2202` AS v2202,
        `2613` AS v2613,
        `2614` AS v2614,
        `2615` AS v2615,
        CAST(`2502` AS INT) AS v2502,
        `2503` AS v2503
        FROM realinfo INNER JOIN vintable on realinfo.vin=vintable.vin and realinfo.vin is not null
        WHERE year=$year and month=$month AND realinfo.vin IS NOT NULL
      """.stripMargin).as[MileageCheckResult].map(_.toString).repartition(1)
        .write.mode(SaveMode.Overwrite).text(s"${stateConf.getString("output.directory")}/$year/$month")
    }catch {
      case e:Exception=>
        logError(s"$year-$month-$day"+e)
        logError(s"$year-$month",e)
    }
  }
}
package com.bitnei.report.tempjob

import java.util.UUID

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.SparkHelper
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

import scala.collection.mutable.ArrayBuffer


trait Formatter{
  def format(row:Row):String
}

class CSVFormatter extends Formatter {
  override def format(row: Row): String = {
    var i = 0
    val result = new StringBuilder

    while (i < row.length) {
      result.append(s"${row.get(i)},")
      i += 1
    }
    if (result.lastOption.contains(',')) result.deleteCharAt(result.length - 1)
    result.toString()
  }
}


class SimpleJsonFormatter extends Formatter {
  override def format(row: Row): String = {
    var i = 0
    val result = new StringBuilder

    while (i < row.length) {
      result.append(s"${row.schema(i).name}:${row.get(i)},")
      i += 1
    }
    if (result.lastOption.contains(',')) result.deleteCharAt(result.length - 1)
    result.toString()
  }
}


/*
* created by wangbaosheng on 2017/12/11
*/
class ExportRealinfo(spark:SparkSession,stateConf:StateConf) extends Logging with Job {
  val vinFileParam = stateConf.getOption("--vinfile")
  val vidFileParam = stateConf.getOption("--vidfile")
  val numberFileParam = stateConf.getOption("--numberfile")
  val inputTable = stateConf.getOption("--inputtable").getOrElse("realinfo")
  val selectParam = stateConf.getOption("--select").getOrElse("*")
  val whereParam = stateConf.getOption("--where").getOrElse("1=1")
  val outputFormatParam = stateConf.getOption("--output-format") match {
    case Some("csv") => "com.bitnei.report.tempjob.CSVFormatter"
    case Some("simpleJson") => "com.bitnei.report.tempjob.SimpleJsonFormatter"
    case Some(fullNameOfFormatter) => fullNameOfFormatter
    case None => "com.bitnei.report.tempjob.CSVFormatter"
  }

  val outputFileDirectory = stateConf.getOption("--output-directory").getOrElse(s"/tmp/export/${UUID.randomUUID()}")
  val outputCategory = stateConf.getOption("--output-category").getOrElse("none")

  if (vinFileParam.isEmpty && vidFileParam.isEmpty && numberFileParam.isEmpty) {
    throw new IllegalArgumentException("the --vinfile --vidfile  --numberfile empty")
  }

  val (vehicleInfoList) = if (vinFileParam.nonEmpty) {
    logInfo("use --vinfile")

    val vinList = Utils.readAllLines(vinFileParam.get)
    getVehicleInfo("vin", vinList)
  } else if (vidFileParam.nonEmpty) {
    logInfo("use --vidfile")

    val vidList = Utils.readAllLines(vidFileParam.get)
    getVehicleInfo("vid", vidList)
  } else {
    logInfo("use --numberfile")

    val numberList = Utils.readAllLines(numberFileParam.get)
    getVehicleInfo("number", numberList)
  }


  override type R = String

  import spark.implicits._

  override def registerIfNeed(): Unit = {
    spark.createDataset(vehicleInfoList).createOrReplaceTempView("vehicle")
    SparkHelper.createOrReplaceTempView(spark, stateConf, inputTable)
  }


  override def doCompute[Product <: String](): Dataset[String] = {
    def compute(): DataFrame = {
      val sql =
        s"""
        SELECT $selectParam
        FROM vehicle INNER JOIN $inputTable AS t ON vehicle.vid=t.vid
        $whereParam
    """.stripMargin

      logInfo(sql)

      val result = spark.sql(sql)
      result
    }


    def format(result: DataFrame): Dataset[String] = {
      if (outputCategory == "none") {
        result.mapPartitions(values => {
          val formatter = Thread.currentThread().getContextClassLoader.loadClass(outputFormatParam).asInstanceOf[Formatter]
          values.map(formatter.format)
        }).repartition(10)
      } else if (outputCategory == "oneFileToOneVehicle") {
        result.groupByKey(_.getAs[String]("vid")).flatMapGroups({
          case (vid: String, values: Iterator[Row]) => {
            val formatter = Thread.currentThread().getContextClassLoader.loadClass(outputFormatParam).asInstanceOf[Formatter]
            values.map(formatter.format)
          }
        }).repartition(10)
      } else if (outputCategory == "oneFileToOneDay") {
        result.mapPartitions(values => {
          val formatter = Thread.currentThread().getContextClassLoader.loadClass(outputFormatParam).asInstanceOf[Formatter]
          values.map(formatter.format)
        }).repartition(10)
      } else {
        throw new IllegalArgumentException("")
      }
    }

    val result = compute()

    format(result)
  }




  override def write[Product <: String](result: Dataset[String]): Unit = {
    result.write.text(outputFileDirectory)
  }

  def getVehicleInfo(filterType: String, values: Array[String]): Array[VehicleInfo] = {
    val result = new ArrayBuffer[VehicleInfo]()

    Utils.batchProcessOfArray(values, 999, (offset, length) => {
      result ++= getVehicleSet(filterType, stateConf, values, offset, length)

      logInfo(s"offset=$offset,length=$length,${result.headOption},${result.lastOption}")
    })

    result.toArray
  }

  def getVehicleSet(filterType: String, stateConf: StateConf, values: Seq[String], offset: Int, length: Int): Seq[VehicleInfo] = {
    val whereFilter = new StringBuilder()

    if (filterType == "vin") {
      whereFilter.append(" WHERE vin IN (")
    } else if (filterType == "vid") {
      whereFilter.append(" WHERE VID IN (")
    } else {
      whereFilter.append(" WHERE license_plate IN (")
    }


    for (i <- offset until offset + length) {
      if (i < (offset + length) - 1) {
        whereFilter.append(s"'${values(i)}',")
      } else {
        whereFilter.append(s"'${values(i)}'")
      }
    }

    whereFilter.append(")")

    val sqlBuilder =
      s"""
          SELECT
           vin,
           uuid,
           license_plate
          FROM SYS_VEHICLE
          $whereFilter
""".stripMargin

    logInfo(sqlBuilder)

    val vehSet = new ArrayBuffer[VehicleInfo]()

    JdbcPoolHelper.getJdbcPoolHelper(stateConf).executeQuery(sqlBuilder, x => {
      while (x.next()) {
        val veh = VehicleInfo(
          vin = x.getString(1),
          vid = x.getString(2),
          number = x.getString(3)
        )

        vehSet.append(veh)
      }
    })

    vehSet
  }
}




object  ExportRealinfo extends Logging {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    stateConf.getOption("--help") match {
      case Some(helperParam) =>
        logInfo(
          """
             --vinfile=vinpath
            --vinfile=vin path
            --vidfile=vid path
            --numberfile=number path
            --inputtable=realinfo
            --select `1101`,`2201`
            --where `2201`>10
            --format csv
          """.stripMargin)
        System.exit(0)
      case None =>
        if (!stateConf.getString("--output-directory").startsWith("/tmp")) {
          logError("--output must start with /tmp")
          System.exit(1)
        }
    }

    new ExportRealinfo(SparkHelper.getSparkSession(sparkMaster = None),stateConf).compute()
  }
}


case class VehicleInfo(vid:String,vin:String,number:String)
package com.bitnei.report.tempjob

/**
  * Created by wangbaosheng on 2017/6/28.
  */
class ExportSingleVehiceLocus {

}
package com.bitnei.report.operationIndex.job

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.Utils
import org.apache.spark.sql.{Row, SaveMode, SparkSession}

/*
* created by wangbaosheng on 2017/12/8
*/

case class VIN(vin:String)
object ExportUUID {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().getOrCreate()

    val stateConf = new StateConf
    val vinList = Utils.readAllLines(stateConf.getString("input.directory")).map(VIN(_))

    import spark.implicits._

    spark.createDataset[VIN](vinList).createOrReplaceTempView("vintable")

    spark.read.parquet("/spark/vehicle/data/realinfo").createOrReplaceTempView("realinfo")

    spark.sql(
      """
        SELECT realinfo.*
        FROM VINTABLE INNER JOIN realinfo ON vintable.vin=realinfo.vin
      """.stripMargin).map(simpleJson).write.mode(SaveMode.Overwrite).text(stateConf.getString("output.directory"))
  }



  def simpleJson(row: Row): String = {
    var i = 0
    val result = new StringBuilder

    while (i < row.length) {
      val v = row.get(i)
      val columnName = row.schema(i).name
      result.append(s"$columnName:v,")
      i += 1
    }

    if (result.lastOption.contains(',')) result.deleteCharAt(result.length - 1)
    result.toString()
  }
}
package com.bitnei.report.tempjob


import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import org.apache.spark.sql.{Row, SaveMode, SparkSession}

/*
* created by wangbaosheng on 2017/12/8
*/



object ExportUUID extends Logging {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    val vinList = Utils.readAllLines(stateConf.getString("input.directory")).map(VIN(_))

    val startDate = Utils.parsetDate(stateConf.getOption("startDate").getOrElse("20160101000000"), "yyyyMMddHHmmss").get

    val endDate = Utils.parsetDate(stateConf.getOption("endDate").getOrElse("20171201000000"), "yyyyMMddHHmmss").get

    logInfo(startDate.toString)
    logInfo(endDate.toString)

    val spark = SparkSession.builder().getOrCreate()
    import spark.implicits._


    spark.createDataset[VIN](vinList).createOrReplaceTempView("vintable")

    spark.read.parquet("/spark/vehicle/data/realinfo").createOrReplaceTempView("realinfo")
    while (startDate.getTime < endDate.getTime) {
      val year = Utils.formatDate(startDate, "yyyy").toInt
      val month = Utils.formatDate(startDate, "MM").toInt

      logInfo(s"$year-$month")


      try {
        val result = spark.sql(
          s"""
        SELECT realinfo.*
        FROM VINTABLE INNER JOIN realinfo ON vintable.vin=realinfo.vin AND realinfo.vin IS NOT NULL
        WHERE year=$year AND month=$month AND realinfo.VIN IS NOT NULL
      """.stripMargin).map(simpleJson)

        // result.show(100)

        result.write.mode(SaveMode.Overwrite).text(stateConf.getString("output.directory") + s"/$year/$month")
      } catch {
        case e: Exception =>
      }


      startDate.setMonth(startDate.getMonth + 1)

    }
  }


  def simpleJson(row: Row): String = {
    var i = 0
    val result = new StringBuilder

    while (i < row.length) {
      val v = row.get(i)
      val columnName = row.schema(i).name
      result.append(s"$columnName:$v,")
      i += 1
    }

    if (result.lastOption.contains(',')) result.deleteCharAt(result.length - 1)
    result.toString()
  }
}
package com.bitnei.report.parquet


import java.io.{File, PrintWriter}
import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils._
import com.bitnei.sparkhelper.{HbaseHelper, Relation, Schema, SparkHelper}
import org.apache.hadoop.hbase.client.{Result, Scan}
import org.apache.hadoop.hbase.util.{Base64, Bytes}
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.hadoop.fs.FileSystem
import org.apache.hadoop.hbase.io.ImmutableBytesWritable

import scala.io.Source

/**
  * 导出hbase数据到hdfs。
  *
  **/
object ExtractHBaseDataUseSpark extends Logging {
  val OutputDirecory = "outputDirectory"
  val TableName = "tableName"
  val VehicleFileName = "vehicleFileName"
  val SchemaFileName = "schemaFileName"
  //spark在将hbase数据导出到hdfs的OutputDirectory之后，会将其下载到本地的DownloadDirectory中
  val DownloadDirectory = "downloadDirectory"
  val OutputModel = "outputModel"
  val TimeColumn = "TimeColumn"

  private val sparkSession = SparkHelper.getSparkSession(Some("local"), Some("ExtractHbaseRealInfoData"))

  case class VehicleUUID(uuid: String, starttime: Long, endtime: Long)

  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)


    //读取车辆表
    val vehicleList = Utils.readAllLines(stateConf.getString(VehicleFileName), line => {
      val map = StringParser.toMap(line)
      val uuid = map("uuid").trim
      //转换为UTC
      val startTime = if (map.contains("starttime")) Utils.getTime(map("starttime")).get else 0
      val endTime = if (map.contains("endtime")) Utils.getTime(map("endtime")).get else Long.MaxValue

      VehicleUUID(uuid, startTime, endTime)
    })


    //读取schema
    //srcColumnName:2201,dstColumnName:实时数据分析,decimal:0.01
    val schema = readScehmas(stateConf)
    printSchema(schema)


    var processedFileCount = 0
    vehicleList.foreach(vehicle => {
      processedFileCount += 1
      //将uuid的车辆数据写入到文本文件中。
      logInfo(s"开始处理第$processedFileCount 车辆数据,${vehicle.uuid}")
      writeToFile(sparkSession.sparkContext, stateConf, schema, vehicle)
    })
  }

  def readScehmas(stateConf: StateConf): Schema = {
    val relations = Utils.readAllLines(stateConf.getString(SchemaFileName), line => {
      val map = StringParser.toMap(line)
      println(map.get("srcColumnName"))

      val srcColumnName = map.get("srcColumnName").map(_.trim).get
      Relation(
        srcColumnName,
        map.get("dstColumnName").map(_.trim).getOrElse(srcColumnName),
        map.get("decimal").map(_.trim).getOrElse("0"),
        map.get("encode").map(_.trim).getOrElse(""))
    })

    Schema(stateConf.getString(TableName), stateConf.getString(OutputDirecory), relations)
  }


  def writeToFile(sc: SparkContext, stateConf: StateConf, schema: Schema, vehicle: VehicleUUID): Unit = {
    val uuid: String = vehicle.uuid
    val startTime: Long = vehicle.starttime
    val endTime: Long = vehicle.endtime

    def setFilter(): Scan = {
      val scan = new Scan()
      val startRow = Bytes.toBytes(s"${uuid}_$startTime")
      val stopRow = Bytes.toBytes(s"${uuid}_$endTime")

      scan.setStartRow(startRow)
      scan.setStopRow(stopRow)

      //val rowFilter:Filter=new RowFilter(CompareFilter.CompareOp.GREATER_OR_EQUAL,new BinaryComparator(startRow))

      //  scan.setFilter(rowFilter)
      scan
    }


    SparkHelper.readFromHbase(sparkSession,
      stateConf.getString("hbase.zookeeper.quorum"),
      stateConf.getString("hbase.zookeeper.property.clientPort"),
      schema.tableName,
      setFilter,
      realinfoRDD => processRealinfoRdd(stateConf, realinfoRDD, vehicle, schema)
    )
  }


  def processRealinfoRdd(stateConf: StateConf,
                         realinfoRDD: RDD[(ImmutableBytesWritable, Result)],
                         vehicle: VehicleUUID,
                         schema: Schema): Unit = {
    try {
      val uuid = vehicle.uuid
      val result = realinfoRDD.mapPartitions(part => {
        part.map(e => {
          def getOtherValue(relation: Relation, value: String): String = {
            val decimal = relation.decimal
            //如果精度为0位，那么不转换。
            if (decimal == "0") value
            else if (decimal.startsWith("srcDateFormat")) {
              val decimalMap = StringParser.toMap(decimal, "|", "=")
              val srcFormat = decimalMap.getOrElse("srcDateFormat", "yyyyMMddHHmmss")
              val dstFormat = decimalMap.getOrElse("dstDateFromat", "yyyy-MM-dd HH:mm:ss")
              val toValue = Utils.formatDate(srcFormat, dstFormat, value)
              println(toValue)
              toValue
            } else {
              try {
                (value.toInt / Math.pow(10, decimal.toDouble)).toString
              } catch {
                case e: Exception => value
              }
            }
          }

          def getValueOf2003(relation: Relation, value: String): String = {
            val keyValues = value.split(':')

            def getValue(): String = {
              val result = new StringBuilder
              if (keyValues.length == 2) {
                val value = keyValues(1)
                try {
                  value.split('_').map(_.toDouble / 1000).foreach(v => result.append(s"${v}_"))
                  if (result.last == '_') result.substring(0, result.length - 1) else result.toString()
                } catch {
                  case e: Exception =>
                    ""
                }
              } else {
                " "
              }
            }

            s"${keyValues(0)}:${getValue()}"
          }

          HbaseHelper.readRow(schema, e._2, (r, v) => {
            if (r.srcName == "2003") {
              getValueOf2003(r, v)
            } else {
              getOtherValue(r, v)
            }
          })
        })
      })
      val hdfs = stateConf.getOption("hdfs").getOrElse("hdfs://nameservice1:8020")
      output(stateConf, result, s"$hdfs/${stateConf.getString(OutputDirecory)}/$uuid")
    } catch {
      case e: Exception =>
        logError(s"${vehicle.uuid}处理失败")
        e.printStackTrace

    } finally {}
  }


  def output(stateConf: StateConf, result: RDD[String], path: String): Unit = {
    stateConf.getOption(OutputModel) match {
      case Some(outputModel) if outputModel == "debug" =>
        //result.foreach(println)


        result.foreachPartition(x => {
          val writer = new PrintWriter(s"C:\\Users\\francis\\Desktop\\singlevehicle+${new Date().getTime}")
          x.map(line => writer.write(line))
        })


      //Utils.write("C:\\Users\\francis\\Desktop\\singlevehicle",result.collect())

      case Some(outputModel) if outputModel.startsWith("groupBy") =>
        //val outputPath=
        result.groupBy(realinfo => {
          try {
            realinfo.split(',')(0).substring(0, 8)
          } catch {
            case e: Exception =>
              logError(e.getMessage + realinfo)
              ""
          }
        }) //按照日期分组
          .flatMap(_._2)
          .saveAsTextFile(path)
      case _ =>
        result.saveAsTextFile(path)
    }
  }


  def printSchema(schema: Schema): Unit = {
    logInfo(s"tablename:${schema.tableName}")
    logInfo(s"dstDirectory:${schema.dstDirectory}")
    schema.realations.foreach(relation => logInfo(s"${relation.srcName},${relation.dstName},${relation.decimal}"))
  }
}


class ExtractLocalRealinfoData(stateConf: StateConf) extends Logging {
  private val uuidTableFile = stateConf.getString("uuidTableFile")
  private val uuidTable = readUuidTable(uuidTableFile)
  private val schema = readSchema(stateConf.getString("schema"))


  def run(): Unit = {
    val inputDirectory = stateConf.getString("inputDirectory")
    readAllStartWith(new File(inputDirectory))
  }


  //提取所有以part开头的文件
  def readAllStartWith(inDirectory: File): Unit = {
    inDirectory.listFiles().foreach(file => {
      if (file.isDirectory) {
        readAllStartWith(file)
      } else {
        if (file.getName.startsWith("part")) {
          logInfo(s"begin ${file.getAbsoluteFile}")
          try {
            procesFile(file.getAbsolutePath, inDirectory.getName)
            logInfo(s"end ${file.getAbsoluteFile}")
          } catch {
            case e: Exception =>
              logError(e.getMessage)
              logError(s"${file.getAbsoluteFile},${inDirectory.getName}")

          }
        }
      }
    })
  }


  def readSchema(schemFile: String): String = {
    val lines = Source.fromFile(schemFile).getLines.toArray[String]
    lines(0)
  }


  def readUuidTable(uuidFile: String): Map[String, String] = {
    val lines = Source.fromFile(uuidFile).getLines()
    val uuidTable = scala.collection.mutable.Map[String, String]()
    lines.foreach(line => {
      val fields = StringParser.toMap(line)
      logInfo(line)
      uuidTable.put(fields("uuid"), fields("number"))
    })

    uuidTable.toMap
  }


  def procesFile(inFile: String, uuid: String) {
    splitFile(inFile).foreach(dateValuePair => {
      val date = dateValuePair._1
      val values = Utils.sortByDate[String](dateValuePair._2.toArray, row => {
        val fields = row.split(',')
        if (fields.length > 0) Some(fields(0)) else None
      }).map(line => {
        val fields = line.split(',')
        if (fields.length > 0) {
          val date = Utils.formatDate("yyyyMMddHHmmss", "yyyy-MM-dd HH:mm:ss", fields(0))

          s"$date${line.substring(line.indexOf(','))}"
        } else line
      })

      //创建新文件
      val number = uuidTable(uuid)
      //获取车牌号
      val rootDirectory = s"${stateConf.getString("outputDirectory")}"
      val outputFileName = s"$rootDirectory/$number/${number}_$date.csv"
      // Utils.createDirectory(rootDirectory,number)
      logInfo(s"begin $outputFileName")
      val outStream = new PrintWriter(createNewFile(outputFileName))

      //写入schema
      outStream.write(s"$schema${System.getProperty("line.separator")}")
      System.lineSeparator()

      //导出数据到新文件
      values.foreach(value => outStream.write(s"$value${System.getProperty("line.separator")}"))
      outStream.flush()
      outStream.close()
      logInfo(s"end $outputFileName")
    })
  }

  def splitFile(inFile: String): Map[String, scala.collection.mutable.ArrayBuffer[String]] = {
    val grouped = scala.collection.mutable.Map[String, scala.collection.mutable.ArrayBuffer[String]]()
    val lines = Source.fromFile(inFile).getLines()
    lines.foreach(line => {
      val fields = line.split(',')
      if (fields.length > 0 && fields(0).length >= 8) {
        val date = fields(0).substring(0, 8)
        grouped.get(date) match {
          case Some(value) => value += line
          case None =>
            val newValue = scala.collection.mutable.ArrayBuffer[String]()
            newValue += line
            grouped.put(date, newValue)
        }
      }
    })

    grouped.toMap
  }

  def createNewFile(fileName: String): File = {
    val out = new File(fileName)
    if (out.getParentFile != null && !out.getParentFile.exists()) {
      println(s"create a new directory ${out.getParent}")
      out.getParentFile.mkdirs()
    }

    if (out.exists()) {
      out.delete()
    } else out.createNewFile()
    out
  }


}

object ExtractLocalRealinfoData extends Logging {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    args.foreach(arg => {
      logInfo(arg)
      val param = arg.split('=')
      stateConf.set(param(0), param(1))
    })

    new ExtractLocalRealinfoData(stateConf).run()
  }
}


object GroupByDate {
  def main(args: Array[String]): Unit = {
    val inputFile = args(0)
    val outputFile = args(1)
    val sc = SparkSession.builder().getOrCreate().sparkContext
    val hadoopConfiguration = sc.hadoopConfiguration
    val fs = FileSystem.get(hadoopConfiguration)

    Utils.getChildFiles(fs, inputFile).foreach({ case (path, size) =>
      val realinfoRdd = sc.textFile(path)
      realinfoRdd.groupBy(line => {
        try {
          line.split(',')(0).substring(0, 8)
        } catch {
          case e: Exception => ""
        }
      })
        .flatMap(_._2)
        .saveAsTextFile(s"$outputFile/${Utils.getFileName(path)}")
    })
  }
}

package com.bitnei.common.utils

import java.io.File

import scala.io.Source

/**
  * Created by franciswang on 2016/11/30.
  */
object ExtractLog {
  def main(args: Array[String]) :Unit= {
    //val directoryName = args(0)
    val directoryName="D;/spark/logs"
    val directory=new File(directoryName)
    directory.listFiles().foreach(file=>{
      tranFile(file.getAbsolutePath,file.getAbsolutePath+".bak")
    })

    val str = "20161125-23:54:50--SEND--us_general:7--SEND--SUBMIT 0 LA81F1HT5GA101091 REALTIME {VID:92871294-304e-412c-8b72-d4615df45136,VTYPE:303,2000:20161125235451,2001:,2002:1,2003:MTo=,2101:0,2102:1,2103:MTo=,2201:0,2202:0,2203:0,2208:0,2209:0,2210:0,2301:255,2302:40,2303:0,2304:40,2305:0,2306:10000,2501:0,2502:116653603,2503:39671128,2504:0,2505:0,2601:,2602:,2603:0,2604:,2605:,2606:0,2607:,2608:1,2609:40,2610:,2611:1,2612:40,2613:0,2614:10000,2615:0,2616:0,2617:0,2801:0,2802:0,2804:0,2808:0}"
    println(tran(str))
  }


  def tranFile(src:String,dst:String): Unit ={
    try {
      val inFile = Source.fromFile(new File(src))
      val outFile = new java.io.FileWriter(dst, false)

      val lines = inFile.getLines()
      lines.foreach(line => {
        tran(line) match {
          case Some(tranline) => outFile.write(tranline)
          case _ =>
        }
      })

      outFile.close()
      inFile.close()
    }catch {
      case e:Exception=>
        println(s"转换${src}时出错！！")
        e.printStackTrace()
    }

  }

  def tran(line: String): Option[String] = {
    if (line.contains("SEND") && line.contains("VID") && line.contains("REALTIME")) {
      val vin = line.substring(54, 71)
      val startIndex = line.indexOf("{")
      val endIndex = line.indexOf("}")
      if (startIndex >= 0 && endIndex >= 0) {
        val vinfo = line.substring(startIndex+1, endIndex)
        val vid=vinfo.substring(0,vinfo.indexOf(','))


        vinfo.split(',').find(tuple => {
          val keyValuue = tuple.split(':')
          keyValuue.length == 2 && keyValuue(0) == "2000"
        }).map(_.split(':')(1)) match {
          case Some(time) =>
             val trimVid=vinfo.substring(vinfo.indexOf(',')+1)
             val trimTime=trimVid.replaceFirst(f",2000:\\d{14}","")
            Some(s"$vid,VIN:$vin,TIME:$time,MESSAGETYPE:REALTIME,$trimVid,2000:$time")
          case _=>None
        }
      }else{
        None
      }
    }else None
  }
}
package com.bitnei.report.detail.FaultDetail

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.detail._
import com.bitnei.report.stateGenerate.FaultStateGenerator
import com.bitnei.sparkhelper.HbaseHelper
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.{Get, Result}
import org.apache.hadoop.hbase.util.Bytes

import scala.collection.mutable.ArrayBuffer

/**
  * Created by wangbaosheng on 2017/5/9.
  */
class FaultDetailCompute extends ReportDetailCompute {
  override def compute(stateConf: StateConf, sortedRows: Seq[RealinfoModel]): List[DetailModel] = {
    //状态划分
    val stateGenerator = new FaultStateGenerator(stateConf) {
      override type T = RealinfoModel

      override def getVid = (row: RealinfoModel) => row.vid

      override def getTime = row => row.time

      override def getCharge = row => row.charge.getOrElse(0)

      override def getSoc = row => row.soc.getOrElse(0)

      override def getSpeed = row => row.speed.getOrElse(0)
      override def getMileage = row=>row.mileage.getOrElse(0)
    }


    //划分充电，行驶状态
    val windows = stateGenerator.handle(sortedRows)
   //计算每一个状态的明细数据
    val result = DetailCompute.computeAllStates(stateConf, windows)


    result match {
      case Nil => Nil
      case head :: tail => mergeFirstStateAndPrevCharge(
        () => {
          val con = HbaseHelper.getConnection(stateConf.getString("fault.hbase.zkquorm"), stateConf.getString("fault.hbase.zkport"))
          val table = con.getTable(TableName.valueOf("fault_detail_laststatus"))

          //获取上一个充电状态的明细数据
          val get = new Get(Bytes.toBytes(head.vid))
          val row = table.get(get)


          convertTo(head.vid, row)
        },
        head) :: tail
    }
  }

  /**
    * 将第一个状态及其上一个充电状态的明细数据进行合并，其中上一个状态的数据存储在hbase中
    * @param getPrevCharge 获取第一个状态的上一个状态对应明细数据
    * @param head 第一个状态的明细数据
    * @return 合并后的明细数据
    **/
  def mergeFirstStateAndPrevCharge(getPrevCharge:()=>Option[DetailModel],head: DetailModel): DetailModel = {
    def merge(first: DetailModel, prev: DetailModel): DetailModel = {
      //将当前状态和上一个状态的结果进行合并
      first.copy(timeBetweenCharge = prev.timeBetweenCharge, stopMileageOfPrevCharge = prev.stopMileageOfPrevCharge, prevChargeStopTime = prev.prevChargeStopTime, maxCurrentOfPrevCharge = prev.maxCurrentOfPrevCharge)
    }

    getPrevCharge() match {
      case Some(prevCharge) => merge(head, prevCharge)
      case None => head
    }
  }


  def convertTo(vid: String, r: Result): Option[DetailModel] = {
    val cells = r.rawCells()
    if (cells.isEmpty) None
    else {
      var category = ""
      var startTime = 0L
      var endTime = 0L
      var onlineTime = 0
      var timeLeng = 0L
      var accRunTime=0

      var totalCharge = 0D
      var timeBetweenCharge = 0
      var stopMileageOfPrevCharge = 0

      var maxTotalVoltage = 0
      var minTotalCurrent = 0
      var maxTotalCurrent = 0
      var minTotalVoltage = 0
      var maxSecondaryVolatage = 0
      var minSecondaryVolatage = 0
      var maxAcquisitionPointTemp = 0
      var minAcquisitionPointTemp = 0
      var maxEngineTemp = 0
      var minEngineTemp = 0
      var maxSoc = 0
      var minSoc = 0
      var startSoc = 0
      var endSoc=0

      var startLongitude = 0L
      var startLatitude = 0L
      var endLongitude = 0L
      var endLatitude = 0L
      var startMileage = 0
      var stopMileage = 0
      var avgSpeed = 0D
      var maxSpeed = 0
      var maxCurrentOfPrevCharge = 0
      var prevChargeStopTime = 0L
      var startMileageOfCurrentDay=0
      var endMileageOfCurrentDay=0
      var vin:String=""
      var gpsMileage:Int=0
      var year=""
      var month=""
      var day=""

      cells.foreach(cell => {
        val qualifier = Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength)
        val value = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)

        qualifier match {
          case "vin"=>vin=value.toString
          case "category" => category = value.toString
          case "onlineTime" => onlineTime = value.toInt
          case "startTime" => startTime = value.toLong
          case "endTime" => endTime = value.toLong
          case "timeLeng" => timeLeng = value.toInt
          case "accRunTime"=>accRunTime=value.toInt
          case "totalCharge" => totalCharge = value.toDouble
          case "timeBetweenCharge" => timeBetweenCharge = value.toInt
          case "stopMileageOfPrevCharge" => stopMileageOfPrevCharge = value.toInt


          case "maxTotalVoltage" => maxTotalVoltage = value.toInt
          case "minTotalVoltage" => minTotalVoltage = value.toInt
          case "maxTotalCurrent" => maxTotalCurrent = value.toInt
          case "minTotalCurrent" => minTotalCurrent = value.toInt
          case "maxSecondaryVolatage" => maxSecondaryVolatage = value.toInt
          case "minSecondaryVolatage" => minSecondaryVolatage = value.toInt
          case "maxAcquisitionPointTemp" => maxAcquisitionPointTemp = value.toInt
          case "minAcquisitionPointTemp" => minAcquisitionPointTemp = value.toInt
          case "maxEngineTemp" => maxEngineTemp = value.toInt
          case "minEngineTemp" => minEngineTemp = value.toInt
          case "maxSoc" => maxSoc = value.toInt
          case "minSoc" => minSoc = value.toInt
          case "startSoc" => startSoc = value.toInt
          case "endSoc"=>endSoc=value.toInt
          case "startLongitude" => startLongitude = value.toLong
          case "startLatitude" => startLatitude = value.toLong
          case "endLongitude" => endLongitude = value.toLong
          case "endLatitude" => endLatitude = value.toLong
          case "startMileage" => startMileage = value.toInt
          case "stopMileage" => stopMileage = value.toInt
          case "avgSpeed" => avgSpeed = value.toDouble
          case "maxSpeed" => maxSpeed = value.toInt
          case "maxCurrentOfPrevCharge" => maxCurrentOfPrevCharge = value.toInt
          case "prevChargeStopTime" => prevChargeStopTime = value.toLong
          case "startMileageOfCurrentDay"=>startMileageOfCurrentDay=value.toInt
          case "endMileageOfCurrentDay"=>endMileageOfCurrentDay=value.toInt
        }
      })

      null

//
//      Some(DetailModel(vid, vin=vin,
//        category = category,
////        year=year,
////        month=month,
////        day=day,
//        onlineTime = onlineTime, startTime = startTime, endTime = endTime, timeLeng = timeLeng,
//        accRunTime=accRunTime,
//        startMileageOfCurrentDay=startMileageOfCurrentDay,endMileageOfCurrentDay=endMileageOfCurrentDay,
//        startMileage = startMileage, stopMileage = stopMileage,gpsMileage=gpsMileage,
//        avgSpeed = avgSpeed, maxSpeed = maxSpeed, maxTotalVoltage = maxTotalVoltage, minTotalVoltage = minTotalVoltage,
//        maxTotalCurrent = maxTotalCurrent, minTotalCurrent = minTotalCurrent, maxSecondaryVolatage = maxSecondaryVolatage, minSecondaryVolatage = minSecondaryVolatage,
//        maxAcquisitionPointTemp = Some(maxAcquisitionPointTemp), minAcquisitionPointTemp = minAcquisitionPointTemp, maxEngineTemp = Some(maxEngineTemp), minEngineTemp = minEngineTemp, maxSoc = maxSoc,
//        minSoc = minSoc, startSoc = startSoc, endSoc=endSoc,startLongitude = startLongitude, startLatitude = startLatitude, endLongitude = endLongitude, endLatitude = endLatitude,
//        totalCharge = totalCharge, timeBetweenCharge = timeBetweenCharge, stopMileageOfPrevCharge = stopMileageOfPrevCharge, prevChargeStopTime = prevChargeStopTime, maxCurrentOfPrevCharge = maxCurrentOfPrevCharge,
//        isQuickCharge = false,
//        powerDistribution = null
//      ))
    }
  }
}


package com.bitnei.report.detail.FaultDetail

import com.bitnei.report.detail.DetailJob

/**
  * Created by wangbaosheng on 2017/6/2.
  */
object FaultDetailJOb {
  def main(args: Array[String]): Unit = {
    //1.计算故障明细，该计算会查询hbase数据库中fault_detail_laststatus表，并将计算结果写入到hdfs中
    DetailJob.main(args)
    //2.将步骤1的结果输出到hbase
    //3.将每一辆车的最后一个状态写入到hbase
    //DetailJobOutput.main(args)
  }
}
package com.bitnei.report.stateGenerate

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import scala.annotation.tailrec
import scala.collection.mutable.ListBuffer

/**
  * 专门用于故障时候追朔的状态生成器，在判断状态的时候，之判断充电和启动/熄火，不考虑满电。
  * */
abstract  class FaultStateGenerator(stateConf:StateConf) extends Logging with StateGenerator {
  private val chargeBeginLength = stateConf.getInt(Constant.StateWindowChargeBeginLength, 10)
  private val chargeEndLength = stateConf.getInt(Constant.StateWindowChargeEndLength, 25)
  private val fullChargeEndLength = stateConf.getInt(Constant.StateWindowFullChargeEndLength, 10)

  var chargeTime: Long = 0
  var fullChargeTime: Long = 0

  var startMielage=0
  var endMileage=0

  protected def  getVid:(T)=>String
  protected def getTime:(T)=>String
  protected def getSpeed:(T)=>Int
  protected def getCharge:(T)=>Int
  protected def getSoc:(T)=>Int
  protected def getMileage:(T)=>Int

 //在线时间
  var onlineTime: Long = 0

  private val onlineBound = stateConf.getLong(Constant.StateWindowLength, 180)


  /**
    * 为了减少bug和便于后续维护，比如添加新的判断条件，这里全部采用函数式实现(尾递归)，最大化的减少变量的修改，便于程序推断。
    **/
  def handle(source: Seq[T]):List[Window[T]] = {
    val windows = new ListBuffer[Window[T]]()

    startMielage=getMileage(source.head)
    endMileage=getMileage(source.last)

    @tailrec
    def splitWindow(curIndex: Int, curState: String): List[Window[T]] = {
      def beginChargeStrict(i: Int): Boolean = {
        val speed = getSpeed(source(i))
        val charge = getCharge(source(i))
        charge < 0 && speed <= 50
      }

      if (curIndex < source.length) {
        val (endIndex, state) = if (beginChargeStrict(curIndex) && beginCharge(source, curIndex)) {
          def append(v: T): ChargeWindow[T] = {
            val chargeWindow = ChargeWindow[T]()
            windows.append(chargeWindow)
            chargeWindow.append(v)
            chargeWindow
          }


          val chargeWindow = append(source(curIndex))

          (inCharge(source)(curIndex + 1, v => chargeWindow.append(v)), Constant.ChargeState)
        } else {
          val curMap = source(curIndex)
          val prevMap = if (curIndex >= 1) source(curIndex - 1) else curMap
          if (!online(curMap, prevMap)) {
            windows.append(TravelWindow().append(source(curIndex)))
          } else {
            onlineTime += getOnline(source, curIndex)
            windows.lastOption match {
              case Some(travelWindow: TravelWindow[T]) =>
                travelWindow.append(source(curIndex))
              case _ =>
                windows.append(TravelWindow().append(source(curIndex)))
            }
          }
          (curIndex + 1, Constant.TravelState)
        }

        setWindow(windows.last, state)
        splitWindow(endIndex, state)
      } else windows.toList
    }


     splitWindow(0,Constant.NoneState)
  }


  def online(cur: T, prev: T): Boolean = {
    if (prev == null) true
    else {
      val curTime = getTime(cur)
      val prevTime = getTime(prev)
      Utils.lessEq(curTime, prevTime, onlineBound)
    }
  }




  @tailrec
  private def inCharge(source: Seq[T])(cur: Int, f: (T) => Unit): Int = {
    @tailrec
    def inChargeTail(i: Int, curMap: T, prevMap: T, continuedInEndCharge: Int): Boolean = {
      if (i < source.length) {
        if (!online(curMap, prevMap)) {
          if (continuedInEndCharge == 0) false else continuedInEndCharge < chargeEndLength
        } else if (continuedInEndCharge < chargeEndLength) {
          val speed = getSpeed(curMap)
          val charge = getCharge(curMap)
          if (charge < 0 && speed <= 50) true
          else {
            val nextMap = if ((i + 1) < source.length) source(i + 1) else curMap
            inChargeTail(i + 1, nextMap, curMap, continuedInEndCharge + 1)
          }
        } else false
      } else {
        continuedInEndCharge < chargeEndLength
      }
    }

    if (cur < source.length && inChargeTail(cur, source(cur), source(cur - 1), 0)) {
      f(source(cur))
      onlineTime += getOnline(source, cur)
      inCharge(source)(cur + 1, f)
    } else {
      cur
    }
  }


  private def beginCharge(source: Seq[T], curIndex: Int): Boolean = {
    @tailrec
    def beginChargeTail(i: Int, validLength: Int): Boolean = {
      if (validLength == 0) {
        if (i >= source.length) false
        else {
          val map = source(i)
          val speed = getSpeed(map)
          val charge = getCharge(map)
          if (charge < 0 && speed <= 50) beginChargeTail(i + 1, validLength + 1)
          else false
        }
      } else {
        if (i < source.length) {
          val curMap = source(i)
          val prevMap = if ((i - 1) >= 0) source(i - 1) else curMap
          if (!online(curMap, prevMap) && validLength < chargeBeginLength) false
          else if (validLength < chargeBeginLength) {
            val map = source(i)
            val speed = getSpeed(map)
            val charge = getCharge(map)
            if (charge < 0 && speed <= 50) beginChargeTail(i + 1, validLength + 1)
            else false
          } else {
            true
          }
        } else {
          false
        }
      }
    }

    require(curIndex < source.length)
    beginChargeTail(curIndex, 0)
  }

  def getOnline(source: Seq[T], curIndex: Int): Long = {
    if (curIndex == 0 || curIndex >= source.length) 0
    else {
      (Utils.getTime(getTime(source(curIndex))),
        Utils.getTime(getTime(source(curIndex - 1)))) match {
        case (Some(curTime), Some(prevTime)) =>
          val timedif = curTime - prevTime
          if (timedif <= 3 * 60 * 1000) timedif
          else 0
        case _ => 0
      }
    }
  }



  def setWindow(window: Window[T], state: String): Unit = {

    def generateWindowId(head: T, last: T, state: String): String = s"${getVid(head)}|${getTime(head)}|${getTime(last)}|$state"


    val (startTime: Long, startH: Int) = Utils.parsetDate(getTime(window.head)) match {
      case Some(startDate) =>
        if (startDate.getMinutes == 0) (startDate.getTime, startDate.getHours)
        else (startDate.getTime, startDate.getHours + 1)
      case None =>
        (0, -1)
    }
    val (endTime: Long, endH: Int) = Utils.parsetDate(getTime(window.last)) match {
      case Some(endDate) =>
        if (endDate.getMinutes == 0) (endDate.getTime, endDate.getHours)
        else (endDate.getTime, endDate.getHours + 1)
      case None =>
        (0, -1)
    }

    window.onLineTime = onlineTime
    window.startMileage=startMielage
    window.endMileage=endMileage

    if (state == Constant.ChargeState) {
      chargeTime += (endTime - startTime)
    }else if (state == Constant.TravelState) {
      window.chargeTime = chargeTime
      window.fullChargeTime = fullChargeTime
    }
  }
}
package com.bitnei.report.tempjob

import java.text.SimpleDateFormat

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import org.apache.spark.sql.{SaveMode, SparkSession}

/*
* created by wangbaosheng on 2017/12/6
*/

object FaultValue extends Logging {
  def formatDate(d: String, f: String): String = {
    import java.text.SimpleDateFormat
    if (d == null || d.isEmpty) ""
    else {
      val format = new SimpleDateFormat("yyyyMMddHHmmss")
      new SimpleDateFormat(f).format(format.parse(d))
    }
  }


  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder().getOrCreate()
    val stateConf = new StateConf
    stateConf.add(args)


    spark.udf.register("formatDate", (d: String, f: String) => formatDate(d, f))

    import spark.implicits._

    spark.read.parquet(stateConf.getOption("input.directory").getOrElse("/spark/vehicle/data/realinfo")).createOrReplaceTempView("realinfo")

    val startDate = Utils.parsetDate(stateConf.getOption("startDate").getOrElse("20160101000000")).get
    val endDate = Utils.parsetDate(stateConf.getOption("endDate").getOrElse("20171206000000")).get


    //  spark.sql("select * from realinfo").show(false)

    while (startDate.getTime < endDate.getTime) {
      val year = Utils.formatDate(startDate, "yyyy")
      val month = Utils.formatDate(startDate, "MM")
      val day = Utils.formatDate(startDate, "dd")

      logInfo(s"begin process $year-$month-$day")
      process(year, month, day)
      startDate.setDate(startDate.getDate + 1)
    }

    spark.catalog.listTables()
"""

""".stripMargin
    def process(year: String, month: String, day: String): Unit = {
      val sqlLines=Utils.readAllLines(stateConf.getString("sqlfile"))
      val selectFromStatment=sqlLines(0)
      val whereStatement=if(sqlLines.length>=2) sqlLines(1) else "1=1"
       // s"select concat('vid',':',vid),concat('time',':',time),concat('3801',':',`3801`) from realinfo where year=$year and month=$month and day=$day")

      val sql=selectFromStatment+s" where year=$year and month=$month and day=$day AND $whereStatement"

      logInfo(sql)

      spark.sql(sql)
        .repartition(1)
        .write
        .mode(SaveMode.Overwrite)
        .text(s"${stateConf.getOption("output.directory").getOrElse("/spark/vehicle/result/fault")}/year=$year/month=$month/day=$day")
    }
  }
}
package com.bitnei.common.utils

import java.io.{File, PrintWriter}

import com.bitnei.report.common.log.Logging

import scala.collection.mutable.ArrayBuffer

/**
  * Created by wangbaosheng on 2017/7/27.
  */

class FileWriter(path:String,bufferLength:Int=4096) extends  Logging {
  private val buffer = ArrayBuffer.fill[String](bufferLength)("")
  private  val file = new File(path)

  if (!file.getParentFile.exists()) file.getParentFile.mkdir()

  if (!file.exists() && file.createNewFile()) {
    logInfo(s"a new file create sucessed  $path")
  } else {
    logInfo(s"a new file create failed  $path")
  }


  private val writer = new PrintWriter(path)
  private var i = 0
  private val MaxLength = bufferLength



  def write(line: String): Unit = {
    if (i == MaxLength) {
      doWrite()
      flush()
    }

    buffer(i) = line
    i += 1
  }


  def doWrite(): Unit ={
    for(index<-(0 until i)){
      writer.write(s"${buffer(index)}" + Utils.newLine)
    }
    i = 0
  }


  def flush(): Unit = {
    doWrite()
    writer.flush()
  }

  def close(): Unit = {
    writer.close()
  }
}package com.bitnei.tools.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.sparkhelper.HbaseHelper
import com.bitnei.tools.util.{JsonBase, MockDataProvider, ValidateUtils}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.hbase.util.Bytes
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

/**
  *
  * @author zhangyongtian
  * @define 统计车辆某天的第一个里程
  *
  * create 2017-11-27 9:21
  *
  */

case class FirstMileage(vid: String, time: String, mileage: String)


object FirstMileageJob extends Serializable with Logging with JsonBase[FirstMileage] {

  case class RealinfoInput(vid: String, time: String, mileage: String)


  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    println("appName:" + app)

    // TODO: 初始化参数集合
    println("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    println("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    println("运行模式：" + env)

    // TODO: 验证参数
    println("验证参数....")

    //时间参数 20170111
    var date = "20171102"

    //停车半径阈值 米
    var PARK_RADIUS = 1

    //秒
    var MIN_DURATION = 10 * 60
    //米
    var MAX_DISTANCE = 4

    if (MAX_DISTANCE < PARK_RADIUS) {
      throw new Exception("input MAX_DISTANCE < PARK_RADIUS error")
    }

    //输出参数
    var outFormat = "#"
    var outputTargets = "console"



    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
      PARK_RADIUS = stateConf.getOption("input.park_radius").get.toInt
      MIN_DURATION = stateConf.getOption("input.min_duration").get.toInt
      MAX_DISTANCE = stateConf.getOption("input.max_distance").get.toInt
      outputTargets = stateConf.getOption("output").get
      outFormat = stateConf.getOption("output.format").get
    }

    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)



    // TODO: 加载上下文
    println("加载上下文")

    //////////////////////////test//////////////////////////

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    import sparkSession.implicits._


    // TODO: 将数据注册成表
    logInfo("将数据注册成表")

    env match {
      case "local" => {
        MockDataProvider.realInfo(sparkSession)
      }

      case "dev" => {
        //研发环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/tmp/zyt/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }


       // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'

    var initDS = sparkSession.sql(s"SELECT VID,`2000` AS TIME,`2202` AS first_mile FROM realinfo where VID is not null and `2000` is not null and `2202` is not null ")
      .as[(String, String, String)]

    //    println("源数据量：" + initDS.count())

    //    year='${year}' and month='${month}' and day='${day}'

    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        ValidateUtils.isNumber(x._3)
      })

    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val time = x._2
          val first_mile = x._3
          RealinfoInput(vid: String, time: String, first_mile: String)
        })

    val result = mappedDS
      .groupByKey(_.vid)
      .mapGroups {
        case (vid, values: Iterator[RealinfoInput]) => {

          val sortedRealinfoInputs = values.toArray[RealinfoInput].sortBy(_.time)

          val first_time = sortedRealinfoInputs.head.time

          val first_mileage = sortedRealinfoInputs.head.mileage

          FirstMileage(vid, first_time, first_mileage)
        }

      }


    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      result.show(false)
      //      result.count()
    }

    if (outputTargets.contains("hdfs")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")


      //研发环境
      var hdfsPath = s"/tmp/zyt/data/${app}/year=${year}/month=${month}/day=${day}"

      //      /spark/vehicle/result/${app}
      val hdfsResult = result.mapPartitions(values => {
        toJson(values.toArray[FirstMileage]).toIterator
      }).toDF()

      if (env.equals("dev")) {
        //1
        hdfsResult.coalesce(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
      }

      if (env.equals("prd")) {
        //生产环境
        hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"
        hdfsResult.write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
      }

      logInfo("输出到HDFS end ....")

    }


    if (outputTargets.contains("hbase")) {
      //TODO: 输出到HBase
      logInfo("输出到HBase start....")

      //研发环境 	yf-nn2:2181,yf-dn1:2181,yf-dn2:2181
      var quorum = "192.168.6.103,192.168.6.104,192.168.6.105"
      var zkport = "2181"

      if (env.equals("prd")) {
        //生产环境
        quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89")
        zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")

        //        quorum = "192.168.2.70,192.168.2.71,192.168.2.89"
        //        zkport = "2181"
      }

      val htableName = s"${app}"

      println("zkQuorum========" + quorum)

      result.coalesce(80).foreachPartition(values => {
        HbaseHelper.bulkPut(quorum, zkport, htableName, (table) => {
          val mapper = new ObjectMapper()
          mapper.registerModule(DefaultScalaModule)
          values.foreach(value => {
            // 0000d218-44aa-4e15-be39-8f66c602218f_201710
            val rowKey = s"${value.vid}_${date}"
            //          table.delete(new Delete(Bytes.toBytes(rowKey)))
            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "firstmileage", toJson(mapper, value)))
          })
        })
      })
      logInfo("输出到HBase end ....")
    }


    logInfo("任务完成...")
    sparkSession.stop()
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.util.IntParam

/**
 *  Produces a count of events received from Flume.
 *
 *  This should be used in conjunction with an AvroSink in Flume. It will start
 *  an Avro server on at the request host:port address and listen for requests.
 *  Your Flume AvroSink should be pointed to this address.
 *
 *  Usage: FlumeEventCount <host> <port>
 *    <host> is the host the Flume receiver will be started on - a receiver
 *           creates a server and listens for flume events.
 *    <port> is the port the Flume receiver will listen on.
 *
 *  To run this example:
 *    `$ bin/run-example org.apache.spark.examples.streaming.FlumeEventCount <host> <port> `
 */
object FlumeEventCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println(
        "Usage: FlumeEventCount <host> <port>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    val Array(host, IntParam(port)) = args

    val batchInterval = Milliseconds(2000)

    // Create the context and set the batch size
    val sparkConf = new SparkConf().setAppName("FlumeEventCount")
    val ssc = new StreamingContext(sparkConf, batchInterval)

    // Create a flume stream
    val stream = FlumeUtils.createStream(ssc, host, port, StorageLevel.MEMORY_ONLY_SER_2)

    // Print out the count of events received from this server in each batch
    stream.count().map(cnt => "Received " + cnt + " flume events." ).print()

    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.util.IntParam

/**
 *  Produces a count of events received from Flume.
 *
 *  This should be used in conjunction with the Spark Sink running in a Flume agent. See
 *  the Spark Streaming programming guide for more details.
 *
 *  Usage: FlumePollingEventCount <host> <port>
 *    `host` is the host on which the Spark Sink is running.
 *    `port` is the port at which the Spark Sink is listening.
 *
 *  To run this example:
 *    `$ bin/run-example org.apache.spark.examples.streaming.FlumePollingEventCount [host] [port] `
 */
object FlumePollingEventCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println(
        "Usage: FlumePollingEventCount <host> <port>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    val Array(host, IntParam(port)) = args

    val batchInterval = Milliseconds(2000)

    // Create the context and set the batch size
    val sparkConf = new SparkConf().setAppName("FlumePollingEventCount")
    val ssc = new StreamingContext(sparkConf, batchInterval)

    // Create a flume stream that polls the Spark Sink running in a Flume agent
    val stream = FlumeUtils.createPollingStream(ssc, host, port)

    // Print out the count of events received from this server in each batch
    stream.count().map(cnt => "Received " + cnt + " flume events." ).print()

    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
package com.bitnei.report

/*
* created by wangbaosheng on 2017/12/15
* 一个基于列表第一个元素的foldable。
* 列表的初始元素是列表的第一个元素。
*/
trait FoldableBaseHead[F[_]] {
  def foldLeft[A, B](f: F[A], acc: FoldMonod[A, B]): B
}

package com.bitnei.report

/* created by wangbaosheng on 2017/11/2
*/

trait FoldMonod[A,B]{
  def fold(a:B,b:A):B
  def zero():B
}


trait Foldable[F[_]]{
  def foldLeft[A,B](f:F[A],acc:FoldMonod[A,B]):B
}


class ListUndoMonidFoldable extends Foldable[List] {
  override def foldLeft[A, B](f: List[A], acc: FoldMonod[A, B]): B = {
    f.foldLeft(acc.zero())(acc.fold)
  }
}


class ArrayUndoMonidFoldable extends Foldable[Array] {
  override def foldLeft[A, B](f: Array[A], acc: FoldMonod[A, B]): B = {
    f.foldLeft(acc.zero())(acc.fold)
  }
}
package com.bitnei.report.dayreport.validate

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.spark.sql.Row


class ForwardCompute(stateConf: StateConf, window: Iterator[Row]) extends Serializable with Logging {
  //VID:100027,VIN:LNBSCC3H2EM010308,TIME:20150909171422,TYPE:FORWARD,
  //RESULT:1,VTYPE:24,FORWARD_ID:EVDATA,FLAG:1,TYPE:1
  //转发结果1成功 2 失败

  val (validCount: Int, totalCount: Int) = getValidAndTotalCount()

  private def getValidAndTotalCount(): (Int, Int) = {
    var valid = 0
    var total = 0

    window.foreach(row => {
      if (row.getAs[String]("RESULT").trim() == "1") valid += 1
      total += 1
    })
    (valid, total)
  }


  def forwardValidityPercent: Float = if (totalCount > 0) validCount.toFloat / totalCount.toFloat else 0f

  def invalidCount: Int = totalCount - validCount

  def forwardInvalidityPercent: Float = 1 - forwardValidityPercent

  val platCount = 0
  val platFailCount = 0
  val platFailPercent: Float = 0
  val platSuccPercent: Float = 0
  val platSuccCount = 0


  //注意，下面五个值暂时不做:数据转发平台数,数据转发平台成功数,数据转发平台失败数,数据转发平台成功率,数据转发平台失败率
  override def toString: String = s"$totalCount,$validCount,${totalCount - validCount},$forwardValidityPercent,${1 - forwardValidityPercent},0,0,0,0,0"
}

object ForwardCompute {
  def defaultString = "0,0,0,0,0,0,0,0,0,0"
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.fpm.FPGrowth

/**
 * Example for mining frequent itemsets using FP-growth.
 * Example usage: ./bin/run-example mllib.FPGrowthExample \
 *   --minSupport 0.8 --numPartition 2 ./data/mllib/sample_fpgrowth.txt
 */
object FPGrowthExample {

  case class Params(
    input: String = null,
    minSupport: Double = 0.3,
    numPartition: Int = -1) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("FPGrowthExample") {
      head("FPGrowth: an example FP-growth app.")
      opt[Double]("minSupport")
        .text(s"minimal support level, default: ${defaultParams.minSupport}")
        .action((x, c) => c.copy(minSupport = x))
      opt[Int]("numPartition")
        .text(s"number of partition, default: ${defaultParams.numPartition}")
        .action((x, c) => c.copy(numPartition = x))
      arg[String]("<input>")
        .text("input paths to input data set, whose file format is that each line " +
          "contains a transaction with each item in String and separated by a space")
        .required()
        .action((x, c) => c.copy(input = x))
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"FPGrowthExample with $params")
    val sc = new SparkContext(conf)
    val transactions = sc.textFile(params.input).map(_.split(" ")).cache()

    println(s"Number of transactions: ${transactions.count()}")

    val model = new FPGrowth()
      .setMinSupport(params.minSupport)
      .setNumPartitions(params.numPartition)
      .run(transactions)

    println(s"Number of frequent itemsets: ${model.freqItemsets.count()}")

    model.freqItemsets.collect().foreach { itemset =>
      println(itemset.items.mkString("[", ",", "]") + ", " + itemset.freq)
    }

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.dayreport

import com.bitnei.report.dayreport.Model.FullChargeStateDayReportModel
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/10/22.
  */
class FullChargeDayReportTest extends FunSuite{
  test("insert to full charge day report table"){
    val values=Array(
      new FullChargeStateDayReportModel(
        id=0,
        report_time ="2016-10-22",
        vid = "728a83e0-d13f-4264-811b-2b4d7e6ba747",
        fullstate_time_sum=0,
        fullstate_times=0,
        fullstate_time_max=1,
        fullstate_time_avg=1,
        fullstate_vol_max=1,
        fullstate_vol_min=1,
        fullstate_svol_max=1,
        fullstate_svol_min=1,
        fullstate_cptemp_max=1,
        fullstate_cptemp_min=1,
        fullstate_temperature_max=1,
        fullstate_temperature_min=1
      )
    )


    //new FullChargeStateDayReportModelManager(new StateConf).insert(values)
    //new FullChargeStateDayReportModelManager(new StateConf).delete(values(0).vid)
  }
}
package com.bitnei.report.dayreport.Model

import java.sql.Date

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.realinfo.DayReportResult

/**
  * Created by franciswang on 2016/10/19.
  */
class FullChargeStateDayReportModel(
                                     val id:Int,
                                     val report_time:String,
                                     val vid:String,
                                     val fullstate_time_sum:Double,
                                     val fullstate_times  :Int,
                                     val fullstate_time_max:Double,
                                     val fullstate_time_avg :Double       ,
                                     val fullstate_vol_max        :Double ,
                                     val fullstate_vol_min        :Double ,
                                     val fullstate_svol_max        :Double,
                                     val fullstate_svol_min        :Double,
                                     val fullstate_cptemp_max      :Int,
                                     val fullstate_cptemp_min      :Int,
                                     val fullstate_temperature_max :Int,
                                     val fullstate_temperature_min :Int)extends  Serializable{

  def this(vi:String,reportDate:String)=this(0,reportDate,vi,0,0,0,0,0,0,0,0,0,0,0,0)
  override def toString: String = s""+
    s"FULLSTATE,$report_time,$vid,$fullstate_time_sum,$fullstate_times,$fullstate_time_max,$fullstate_time_avg,"+
    s"$fullstate_vol_max,$fullstate_vol_min,$fullstate_svol_max,$fullstate_svol_min,$fullstate_cptemp_max,$fullstate_cptemp_min,"+
    s"$fullstate_temperature_max,$fullstate_temperature_min,$fullstate_vol_max,$fullstate_vol_min,$fullstate_svol_max,$fullstate_svol_min,$fullstate_cptemp_max,"+
    s"$fullstate_cptemp_min,$fullstate_temperature_max,$fullstate_temperature_min"
}



class FullChargeStateDayReportModelManager(stateConf:StateConf) extends  Serializable with OutputManager with Logging{
  private val tableName=stateConf.getOption(Constant.FullChargeStateDayReportTable).getOrElse("veh_dayreport_fullstate")

  override type T = DayReportResult

  override def output(vs:Iterable[DayReportResult]):Unit= {
    val sql = stateConf.getString("database") match {
      case "oracle" => s"INSERT INTO $tableName " +
        "(id,report_time,vid,fullstate_time_sum,fullstate_times ,fullstate_time_max,fullstate_time_avg," +
        "fullstate_vol_max,fullstate_vol_min,fullstate_svol_max,fullstate_svol_min,fullstate_cptemp_max,fullstate_cptemp_min," +
        "fullstate_temperature_max,fullstate_temperature_min) VALUES(SEQ_VEH_REPORT.Nextval,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
      case "mysql" =>
        s"INSERT INTO $tableName " +
          "(report_time,vid,fullstate_time_sum,fullstate_times ,fullstate_time_max,fullstate_time_avg," +
          "fullstate_vol_max,fullstate_vol_min,fullstate_svol_max,fullstate_svol_min,fullstate_cptemp_max,fullstate_cptemp_min," +
          "fullstate_temperature_max,fullstate_temperature_min) VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
    }


    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setDate(1, new Date(v.reportDate))
          stmt.setString(2, v.vid)
          stmt.setDouble(3, DataPrecision.toHour(v.timeLeng))
          stmt.setInt(4, v.times)
          stmt.setDouble(5, DataPrecision.toHour(v.maxTime))
          stmt.setDouble(6,DataPrecision.toHour(v.avgTime))
          stmt.setDouble(7, DataPrecision.totalVoltage(v.maxTotalVoltage))
          stmt.setDouble(8, DataPrecision.totalVoltage(v.minTotalVoltage))
          stmt.setDouble(9, DataPrecision.secondaryVoltage(v.maxSecondaryVolatage))
          stmt.setDouble(10,DataPrecision.secondaryVoltage(v.minSecondaryVolatage))
          stmt.setInt(11, DataPrecision.temp(v.maxAcquisitionPointTemp))
          stmt.setInt(12, DataPrecision.temp(v.minAcquisitionPointTemp))
          stmt.setInt(13, DataPrecision.temp(v.maxEngineTemp))
          stmt.setInt(14, DataPrecision.temp(v.minEngineTemp))
          stmt.addBatch()
        })
      })
    } catch {
      case e: Exception =>
        logError(s"数据在写入到${tableName}中出现异常，${e.toString}")
        vs.foreach(v=>logInfo(v.toString))
        throw new Exception(s"throw en exception when writting $tableName",e)
    }
  }
  def delete(reportDate:String):Int={
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"DELETE FROM ${getTable()} WHERE report_time=to_date(?,'yyyy-mm-dd')",stmt=>{
        stmt.setString(1,reportDate)
        stmt.addBatch()
      })(0)
  }

  def getTable():String=stateConf.getString(Constant.FullChargeStateDayReportTable)
}


package com.bitnei.report.dayreport.Model

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant

/**
  * Created by franciswang on 2016/10/19.
  */
case class FullChargeStateModel( id: Int,
                                 start:String,
                                 end:String,
                                 vid: String,
                                 charge_time_sum: Double,
                                 charge_consume: Double,
                                 charge_time_max: Double,
                                 charge_vol_max: Double,
                                 charge_vol_min: Double,
                                 charge_cur_max: Double,
                                 charge_cur_min: Double,
                                 charge_soc_max: Double,
                                 charge_soc_min: Double,
                                 charge_svol_max: Double,
                                 charge_svol_min: Double,
                                 charge_cptemp_max: Int,
                                 charge_cptemp_min: Int,
                                 charge_engtemp_max: Int,
                                 charge_engtemp_min: Int,
                                 charge_sconsume_max: Double
                               ) extends  Serializable{}


class FullChargeStateManager(stateConf:StateConf) extends  Serializable with OutputManager with Logging{
  private val tableName=stateConf.getString(Constant.FullChargeStateTable)
  override type T = FullChargeStateModel

  override def output(vs:Iterable[FullChargeStateModel]): Unit={
    val sql: String = s"INSERT INTO $tableName " +
      "(id,start_time,end_time,vid, charge_time_sum ,charge_consume,charge_time_max" +
      ",charge_vol_max,charge_vol_min,charge_cur_max,charge_cur_min,charge_soc_max,charge_soc_min,charge_svol_max" +
      ",charge_svol_min,charge_cptemp_max,charge_cptemp_min,charge_engtemp_max,charge_engtemp_min, charge_sconsume_max)" +
      "  VALUES(SEQ_VEH_REPORT.Nextval,to_date(:start_time,'yyyy-mm-dd hh24:mi:ss'),to_date(:end_time,'yyyy-mm-dd hh24:mi:ss'),?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setString(1, v.start)
          stmt.setString(2, v.end)
          stmt.setString(3, v.vid)
          stmt.setDouble(4, v.charge_time_sum)
          stmt.setDouble(5, v.charge_consume)
          stmt.setDouble(6, v.charge_time_max)
          stmt.setDouble(7, v.charge_vol_max)
          stmt.setDouble(8, v.charge_vol_min)
          stmt.setDouble(9, v.charge_cur_max)
          stmt.setDouble(10, v.charge_cur_min)

          stmt.setDouble(11, v.charge_soc_max)
          stmt.setDouble(12, v.charge_soc_min)
          stmt.setDouble(13, v.charge_svol_max)
          stmt.setDouble(14, v.charge_svol_min)
          stmt.setDouble(15, v.charge_cptemp_max)
          stmt.setDouble(16, v.charge_cptemp_min)
          stmt.setDouble(17, v.charge_engtemp_max)
          stmt.setDouble(18, v.charge_engtemp_min)
          stmt.setDouble(19, v.charge_sconsume_max)

          stmt.addBatch()
        })

      })
    } catch {
      case e: Exception =>
        logError(s"数据在写入到${tableName}中出现异常，${e.toString}")
        Array()
    }
  }

  def delete(vid:String):Int={
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      "DELETE FROM %s WHERE  start_time>=to_date(?,'yyyy-mm-dd') and end_time<to_date(?,'yyyy-mm-dd')+1".format(stateConf.getString(Constant.FullChargeStateTable)),stmt=>{
        stmt.setString(1,vid)
        stmt.setString(2,vid)
        stmt.addBatch()
      })(0)
  }


  def getTable():String=stateConf.getString(Constant.FullChargeStateTable)
}
package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.dayreport.Model.{FullChargeStateManager, FullChargeStateModel}
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/10/22.
  */
class FullChargeStateTest extends FunSuite{
  test("insert to full charge table") {
    val values = Array(
       FullChargeStateModel(
        id = 0,
        vid = "728a83e0-d13f-4264-811b-2b4d7e6ba744",
        start = "2016-10-22 11:11:11",
        end = "2016-10-22 11:11:11",
        charge_time_sum = 1,
        charge_consume = 1,
        charge_time_max = 1,
        charge_vol_max = 1,
        charge_vol_min = 1,
        charge_cur_max = 1,
        charge_cur_min = 1,
        charge_soc_max = 1,
        charge_soc_min = 1,
        charge_svol_max = 1,
        charge_svol_min = 1,
        charge_cptemp_max = 1,
        charge_cptemp_min = 1,
        charge_engtemp_max = 1,
        charge_engtemp_min = 1,
        charge_sconsume_max = 0
      )
    )

    new FullChargeStateManager(new StateConf).output(values)
    new FullChargeStateManager(new StateConf).delete(values(0).vid)
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.clustering.{GaussianMixture, GaussianMixtureModel}
import org.apache.spark.mllib.linalg.Vectors
// $example off$

object GaussianMixtureExample {

  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName("GaussianMixtureExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load and parse the data
    val data = sc.textFile("data/mllib/gmm_data.txt")
    val parsedData = data.map(s => Vectors.dense(s.trim.split(' ').map(_.toDouble))).cache()

    // Cluster the data into two classes using GaussianMixture
    val gmm = new GaussianMixture().setK(2).run(parsedData)

    // Save and load model
    gmm.save(sc, "target/org/apache/spark/GaussianMixtureExample/GaussianMixtureModel")
    val sameModel = GaussianMixtureModel.load(sc,
      "target/org/apache/spark/GaussianMixtureExample/GaussianMixtureModel")

    // output parameters of max-likelihood model
    for (i <- 0 until gmm.k) {
      println("weight=%f\nmu=%s\nsigma=\n%s\n" format
        (gmm.weights(i), gmm.gaussians(i).mu, gmm.gaussians(i).sigma))
    }
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.parquet

import com.bitnei.report.common.{DbJobLog, jobLog}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.SparkHelper
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.sql.{Dataset, SQLContext}

import scala.math.BigDecimal.RoundingMode

/**
  * Created by wangbaosheng on 2017/6/2.
  * 将国标和地标数据的soc同意转换为100%，该工具只是一个临时使用的工具。
  *
  * 1.将2016.01-2016.09月中的soc全部乘以0.4
  * 2.2016.10-2017.06中的数据查询数据库，区分国标和地标，如果是地标，乘以0.4
  */
object ConvertGBDB  extends  Logging{
  def main(args: Array[String]): Unit = {

//        val sparkSession = SparkHelper.getSparkSession(sparkMaster = Some("local"))
//        import sparkSession.implicits._
//
//        val t1 = sparkSession.createDataset(Array(Rule("1", "1"))).withColumnRenamed("vid","2015").createOrReplaceTempView("rule")
//
//        sparkSession.udf.register("convertSoc",(soc:Int,ruleID:String)=>convertSoc(soc,ruleID))
//
//
//    sparkSession.sql("select * from rule").show()
//        sparkSession.sql("select `2015`*10 as `1111`,1 as ruleid2 from rule").show()


    val stateConf = new StateConf
    stateConf.add(args)

    val sparkSession = SparkHelper.getSparkSession(stateConf.getOption("spark.master"))
    val sqlContext: SQLContext = sparkSession.sqlContext


    sparkSession.udf.register("convertSoc", (soc: Int, ruleID: String) => convertSoc(soc, ruleID))
    convertGBDB(sqlContext, stateConf)
  }

  def convertSoc(soc: Int, ruleId: String): Double = {
    if (ruleId == "1") {
      BigDecimal(soc*0.4).setScale(1, RoundingMode.HALF_DOWN).toDouble
    } else soc
  }

  def convertGBDB(sqlContext: SQLContext, stateConf: StateConf): Unit = {

    import sqlContext.implicits._

    def getRuleTable(): Dataset[Rule] = {
      val url = stateConf.getString(Constant.JdbcUrl)
      val driver = stateConf.getString(Constant.JdbcDriver)
      val user = stateConf.getString(Constant.JdbcUserName)
      val password = stateConf.getString(Constant.JdbcPasswd)

      SparkHelper.registerJdbcAsTempView(sqlContext, url, driver, "SYS_VEHICLE", user, password)
      SparkHelper.registerJdbcAsTempView(sqlContext, url, driver, "SYS_RULE", user, password)

      if (stateConf.getOption("ruleTable.update").contains("true")) {
        sqlContext.sql("SELECT uuid AS vid,SYS_RULE.ID as ruleID " +
          " FROM SYS_VEHICLE JOIN SYS_RULE ON SYS_RULE.ID=SYS_VEHICLE.RULE_ID").write.parquet(stateConf.getString("ruleTable"))
      }

      sqlContext.read.parquet(stateConf.getString("ruleTable")).as[Rule]
    }



    val inputDirectory = stateConf.getString("input.directory")
    val pathArray = Utils.buildPath(inputDirectory,
      Utils.parsetDate(stateConf.getString("startdate"),"yyyyMMdd").get,
      Utils.parsetDate(stateConf.getString("enddate"),"yyyyMMdd").get)
    val jobServer: jobLog = new DbJobLog(stateConf)

    val ruleTale = getRuleTable()

    ruleTale.cache()

    ruleTale.createOrReplaceTempView("rule")

    pathArray.foreach(inputPath => {
      try {
        sqlContext.read.parquet(inputPath).createOrReplaceTempView("realinfo")
        val db = sqlContext.sql("SELECT realinfo.VID,TIME, `2201`,`2614`,convertSoc(`2615`,ruleId) as `2615`,`2613`,`2603`,`2606`,`2304`,`2609`,`2612`,`2202`,`2502`,`2503`,VIN,MESSAGETYPE,ISFILTER, `2210`,`2912`,`2608`,VTYPE, `2607`,`2002`,`2604`,`2003`,`2605`,`2602`,`2601`,`2910`,`2911`,`2505`,`2209`,`2208`,`2103`,`2101`,`2102`,`2203`,`2504`,`2501`,`2901`,`2902`,`2903`,`2904`,`2801`,`2905`,`2906`,`2616`,`2907`,`2617`,`2908`,`2611`,`2808`,`2802`,`2804`,`2610`,`10002`,`10005`,`10003`,`10004`,`2909`,`2303`,`2305`,`2306`,`2301`,`2302`,`2001`,`2000` " +
          " ,ruleId FROM rule  JOIN  realinfo   ON rule.vid=realinfo.vid WHERE rule.ruleId='1'")
        val tokens=inputPath.split('/')
        val outputDate=tokens(tokens.length-3)+"/"+tokens(tokens.length-2)+"/"+tokens(tokens.length-1)
        val outputPath=stateConf.getString("output.directory")+"/"+outputDate

        db.write.parquet(outputPath)
        sqlContext.sparkSession.catalog.dropTempView("realinfo")
        logInfo(s"$outputPath ok")
      } catch {
        case e: Exception => //写入书数据库
          val jobScript = stateConf.getString("jobscript")
          jobServer.log(sqlContext.sparkContext.applicationId, jobScript, sqlContext.sparkContext.appName, sqlContext.sparkContext.sparkUser, e.getMessage)
          throw  e
      }
    })

  }
}


object ConvertDB{
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    val sparkSession = SparkHelper.getSparkSession(stateConf.getOption("spark.master"))
    val sqlContext: SQLContext = sparkSession.sqlContext

    convertDB(sqlContext,stateConf)
  }



  def convertDB(sqlContext: SQLContext, stateConf: StateConf): Unit = {

    val inputDirectory = stateConf.getString("input.directory")
    val pathArray = Utils.buildPath(inputDirectory,
      Utils.parsetDate(stateConf.getString("startdate"),format = "yyyyMMdd").get,
      Utils.parsetDate(stateConf.getString("enddate"),"yyyyMMdd").get)
    val jobServer: jobLog = new DbJobLog(stateConf)
    val fs = FileSystem.get(sqlContext.sparkContext.hadoopConfiguration)
    pathArray.foreach(inputPath => {
      try {
        if(fs.exists(new Path(inputPath))){
          val source = sqlContext.read.parquet(inputPath).createOrReplaceTempView("realinfo")

          val result =sqlContext.sql("SELECT VID,TIME, `2201`,`2614`,cast(`2615` as int),`2613`,`2603`,`2606`,`2304`,`2609`,`2612`,`2202`,`2502`,`2503`,VIN,MESSAGETYPE,ISFILTER, `2210`,`2912`,`2608`,VTYPE, `2607`,`2002`,`2604`,`2003`,`2605`,`2602`,`2601`,`2910`,`2911`,`2505`,`2209`,`2208`,`2103`,`2101`,`2102`,`2203`,`2504`,`2501`,`2901`,`2902`,`2903`,`2904`,`2801`,`2905`,`2906`,`2616`,`2907`,`2617`,`2908`,`2611`,`2808`,`2802`,`2804`,`2610`,`10002`,`10005`,`10003`,`10004`,`2909`,`2303`,`2305`,`2306`,`2301`,`2302`,`2001`,`2000` "+
            " ,1 as ruleId FROM  realinfo")

          val tokens=inputPath.split('/')
          val outputDate=tokens(tokens.length-3)+"/"+tokens(tokens.length-2)+"/"+tokens(tokens.length-1)
          result.write.parquet(stateConf.getString("output.directory")+"/"+outputDate)
          sqlContext.sparkSession.catalog.dropTempView("realinfo")
        }

      } catch {
        case e: Exception => //写入书数据库
          try {
            val jobScript = stateConf.getString("jobscript")
            jobServer.log(sqlContext.sparkContext.applicationId, jobScript, sqlContext.sparkContext.appName, sqlContext.sparkContext.sparkUser, e.getMessage)
          }catch {
            case e:Exception=>
          }
          throw e
      }
    })
  }

}
case class Rule(vid:String,ruleId:String)/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

import scala.collection.mutable
import scala.language.reflectiveCalls

import scopt.OptionParser

import org.apache.spark.examples.mllib.AbstractParams
import org.apache.spark.ml.{Pipeline, PipelineStage}
import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}
import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer}
import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}
import org.apache.spark.sql.{DataFrame, SparkSession}


/**
 * An example runner for decision trees. Run with
 * {{{
 * ./bin/run-example ml.GBTExample [options]
 * }}}
 * Decision Trees and ensembles can take a large amount of memory. If the run-example command
 * above fails, try running via spark-submit and specifying the amount of memory as at least 1g.
 * For local mode, run
 * {{{
 * ./bin/spark-submit --class org.apache.spark.examples.ml.GBTExample --driver-memory 1g
 *   [examples JAR path] [options]
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object GBTExample {

  case class Params(
      input: String = null,
      testInput: String = "",
      dataFormat: String = "libsvm",
      algo: String = "classification",
      maxDepth: Int = 5,
      maxBins: Int = 32,
      minInstancesPerNode: Int = 1,
      minInfoGain: Double = 0.0,
      maxIter: Int = 10,
      fracTest: Double = 0.2,
      cacheNodeIds: Boolean = false,
      checkpointDir: Option[String] = None,
      checkpointInterval: Int = 10) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("GBTExample") {
      head("GBTExample: an example Gradient-Boosted Trees app.")
      opt[String]("algo")
        .text(s"algorithm (classification, regression), default: ${defaultParams.algo}")
        .action((x, c) => c.copy(algo = x))
      opt[Int]("maxDepth")
        .text(s"max depth of the tree, default: ${defaultParams.maxDepth}")
        .action((x, c) => c.copy(maxDepth = x))
      opt[Int]("maxBins")
        .text(s"max number of bins, default: ${defaultParams.maxBins}")
        .action((x, c) => c.copy(maxBins = x))
      opt[Int]("minInstancesPerNode")
        .text(s"min number of instances required at child nodes to create the parent split," +
        s" default: ${defaultParams.minInstancesPerNode}")
        .action((x, c) => c.copy(minInstancesPerNode = x))
      opt[Double]("minInfoGain")
        .text(s"min info gain required to create a split, default: ${defaultParams.minInfoGain}")
        .action((x, c) => c.copy(minInfoGain = x))
      opt[Int]("maxIter")
        .text(s"number of trees in ensemble, default: ${defaultParams.maxIter}")
        .action((x, c) => c.copy(maxIter = x))
      opt[Double]("fracTest")
        .text(s"fraction of data to hold out for testing. If given option testInput, " +
        s"this option is ignored. default: ${defaultParams.fracTest}")
        .action((x, c) => c.copy(fracTest = x))
      opt[Boolean]("cacheNodeIds")
        .text(s"whether to use node Id cache during training, " +
        s"default: ${defaultParams.cacheNodeIds}")
        .action((x, c) => c.copy(cacheNodeIds = x))
      opt[String]("checkpointDir")
        .text(s"checkpoint directory where intermediate node Id caches will be stored, " +
        s"default: ${
          defaultParams.checkpointDir match {
            case Some(strVal) => strVal
            case None => "None"
          }
        }")
        .action((x, c) => c.copy(checkpointDir = Some(x)))
      opt[Int]("checkpointInterval")
        .text(s"how often to checkpoint the node Id cache, " +
        s"default: ${defaultParams.checkpointInterval}")
        .action((x, c) => c.copy(checkpointInterval = x))
      opt[String]("testInput")
        .text(s"input path to test dataset. If given, option fracTest is ignored." +
        s" default: ${defaultParams.testInput}")
        .action((x, c) => c.copy(testInput = x))
      opt[String]("dataFormat")
        .text("data format: libsvm (default), dense (deprecated in Spark v1.1)")
        .action((x, c) => c.copy(dataFormat = x))
      arg[String]("<input>")
        .text("input path to labeled examples")
        .required()
        .action((x, c) => c.copy(input = x))
      checkConfig { params =>
        if (params.fracTest < 0 || params.fracTest >= 1) {
          failure(s"fracTest ${params.fracTest} value incorrect; should be in [0,1).")
        } else {
          success
        }
      }
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val spark = SparkSession
      .builder
      .appName(s"GBTExample with $params")
      .getOrCreate()

    params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir)
    val algo = params.algo.toLowerCase

    println(s"GBTExample with parameters:\n$params")

    // Load training and test data and cache it.
    val (training: DataFrame, test: DataFrame) = DecisionTreeExample.loadDatasets(params.input,
      params.dataFormat, params.testInput, algo, params.fracTest)

    // Set up Pipeline
    val stages = new mutable.ArrayBuffer[PipelineStage]()
    // (1) For classification, re-index classes.
    val labelColName = if (algo == "classification") "indexedLabel" else "label"
    if (algo == "classification") {
      val labelIndexer = new StringIndexer()
        .setInputCol("label")
        .setOutputCol(labelColName)
      stages += labelIndexer
    }
    // (2) Identify categorical features using VectorIndexer.
    //     Features with more than maxCategories values will be treated as continuous.
    val featuresIndexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("indexedFeatures")
      .setMaxCategories(10)
    stages += featuresIndexer
    // (3) Learn GBT.
    val dt = algo match {
      case "classification" =>
        new GBTClassifier()
          .setFeaturesCol("indexedFeatures")
          .setLabelCol(labelColName)
          .setMaxDepth(params.maxDepth)
          .setMaxBins(params.maxBins)
          .setMinInstancesPerNode(params.minInstancesPerNode)
          .setMinInfoGain(params.minInfoGain)
          .setCacheNodeIds(params.cacheNodeIds)
          .setCheckpointInterval(params.checkpointInterval)
          .setMaxIter(params.maxIter)
      case "regression" =>
        new GBTRegressor()
          .setFeaturesCol("indexedFeatures")
          .setLabelCol(labelColName)
          .setMaxDepth(params.maxDepth)
          .setMaxBins(params.maxBins)
          .setMinInstancesPerNode(params.minInstancesPerNode)
          .setMinInfoGain(params.minInfoGain)
          .setCacheNodeIds(params.cacheNodeIds)
          .setCheckpointInterval(params.checkpointInterval)
          .setMaxIter(params.maxIter)
      case _ => throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }
    stages += dt
    val pipeline = new Pipeline().setStages(stages.toArray)

    // Fit the Pipeline.
    val startTime = System.nanoTime()
    val pipelineModel = pipeline.fit(training)
    val elapsedTime = (System.nanoTime() - startTime) / 1e9
    println(s"Training time: $elapsedTime seconds")

    // Get the trained GBT from the fitted PipelineModel.
    algo match {
      case "classification" =>
        val rfModel = pipelineModel.stages.last.asInstanceOf[GBTClassificationModel]
        if (rfModel.totalNumNodes < 30) {
          println(rfModel.toDebugString) // Print full model.
        } else {
          println(rfModel) // Print model summary.
        }
      case "regression" =>
        val rfModel = pipelineModel.stages.last.asInstanceOf[GBTRegressionModel]
        if (rfModel.totalNumNodes < 30) {
          println(rfModel.toDebugString) // Print full model.
        } else {
          println(rfModel) // Print model summary.
        }
      case _ => throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }

    // Evaluate model on training, test data.
    algo match {
      case "classification" =>
        println("Training data results:")
        DecisionTreeExample.evaluateClassificationModel(pipelineModel, training, labelColName)
        println("Test data results:")
        DecisionTreeExample.evaluateClassificationModel(pipelineModel, test, labelColName)
      case "regression" =>
        println("Training data results:")
        DecisionTreeExample.evaluateRegressionModel(pipelineModel, training, labelColName)
        println("Test data results:")
        DecisionTreeExample.evaluateRegressionModel(pipelineModel, test, labelColName)
      case _ =>
        throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.regression.GeneralizedLinearRegression
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * An example demonstrating generalized linear regression.
 * Run with
 * {{{
 * bin/run-example ml.GeneralizedLinearRegressionExample
 * }}}
 */

object GeneralizedLinearRegressionExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("GeneralizedLinearRegressionExample")
      .getOrCreate()

    // $example on$
    // Load training data
    val dataset = spark.read.format("libsvm")
      .load("data/mllib/sample_linear_regression_data.txt")

    val glr = new GeneralizedLinearRegression()
      .setFamily("gaussian")
      .setLink("identity")
      .setMaxIter(10)
      .setRegParam(0.3)

    // Fit the model
    val model = glr.fit(dataset)

    // Print the coefficients and intercept for generalized linear regression model
    println(s"Coefficients: ${model.coefficients}")
    println(s"Intercept: ${model.intercept}")

    // Summarize the model over the training set and print out some metrics
    val summary = model.summary
    println(s"Coefficient Standard Errors: ${summary.coefficientStandardErrors.mkString(",")}")
    println(s"T Values: ${summary.tValues.mkString(",")}")
    println(s"P Values: ${summary.pValues.mkString(",")}")
    println(s"Dispersion: ${summary.dispersion}")
    println(s"Null Deviance: ${summary.nullDeviance}")
    println(s"Residual Degree Of Freedom Null: ${summary.residualDegreeOfFreedomNull}")
    println(s"Deviance: ${summary.deviance}")
    println(s"Residual Degree Of Freedom: ${summary.residualDegreeOfFreedom}")
    println(s"AIC: ${summary.aic}")
    println("Deviance Residuals: ")
    summary.residuals().show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.samples.geohash

import ch.hsr.geohash.GeoHash
import ch.hsr.geohash.WGS84Point
import com.spatial4j.core.context.SpatialContext
import com.spatial4j.core.distance.DistanceUtils

/**
  *
  * @author zhangyongtian
  * @define
  *
  * date: 2017-11-14
  *
  */
object GeoHashTest {

  def main(args: Array[String]): Unit = {
        val lat = 40.390943
        val lon = 75.9375
        val geoHash = GeoHash.withCharacterPrecision(lat, lon, 12)

        println(geoHash)
        println(geoHash.toBase32)
        val bbox = geoHash.getBoundingBox
        val decodedHash = GeoHash.fromGeohashString(geoHash.toBase32)
    //
    //
    //    val decodedCenter = decodedHash.getBoundingBoxCenterPoint
    //
    //    println("bbox " + bbox + " should contain the decoded center value " + decodedCenter, bbox
    //      .contains(decodedCenter))
    //
    //    bbox.contains(decodedCenter)

    //    val decodedBoundingBox = decodedHash.getBoundingBox


    // 移动设备经纬度// 移动设备经纬度

//    val lon = 116.312528
//    val lat = 39.983733
//    val geoHash = GeoHash.withCharacterPrecision(lat, lon, 10)
//
//    // N, NE, E, SE, S, SW, W, NW
//    val adjacent = geoHash.getAdjacent
//    for (hash <- adjacent) {
//      println(hash.toBase32)
//    }


    //    println(geoHash.ord())

    //    println(geoHash.significantBits)

    //    println(geoHash.toBase32)


    val lon1 = 116.3125333347639
    val lat1 = 39.98355521792821

    val lon2 = 116.312528
    val lat2 = 39.983733

    val geo = SpatialContext.GEO
    val distance = geo.calcDistance(geo.makePoint(lon1, lat1), geo.makePoint(lon2, lat2)) * DistanceUtils.DEG_TO_KM
    System.out.println(distance) // KM

  }

}
package com.bitnei.tools.util

import ch.hsr.geohash.GeoHash
import com.spatial4j.core.context.SpatialContext
import com.spatial4j.core.distance.DistanceUtils

import scala.math.BigDecimal.RoundingMode

/**
  *
  * @author zhangyongtian
  * @define 地理操作相关工具类
  *
  * create 2017-11-21 17:12
  *
  *
  */
object GeoUtils {


  /**
    * 计算经纬度坐标之间的距离
    *
    * @param lon1
    * @param lat1
    * @param lon2
    * @param lat2
    * @return 距离（米）
    */
  def getDistance(lon1: Double, lat1: Double, lon2: Double, lat2: Double): Double = {
    val geo = SpatialContext.GEO
    geo.calcDistance(geo.makePoint(lon1, lat1), geo.makePoint(lon2, lat2)) * DistanceUtils.DEG_TO_KM * 1000
  }


  /**
    * 计算经纬度坐标之间的距离
    *
    * @param lon1
    * @param lat1
    * @param lon2
    * @param lat2
    * @return 距离（米）
    */
  def getDistance(lon1: Long, lat1: Long, lon2: Long, lat2: Long): Double = {
    val geo = SpatialContext.GEO
    geo.calcDistance(geo.makePoint(coordLong2Double(lon1), coordLong2Double(lat1)), geo.makePoint(coordLong2Double(lon2), coordLong2Double(lat2))) * DistanceUtils.DEG_TO_KM * 1000
  }


  /**
    * 获取经纬坐标的GeoHash的Base32值
    *
    * @param lon
    * @param lat
    * @param numberOfCharacters GeoHash Length
    * @return
    */
  def getGeoHashOfBase32(lon: Double, lat: Double, numberOfCharacters: Int): String = {

    if (numberOfCharacters > 12) {
      throw new RuntimeException("geohash out of max of 12 !")
    }
    GeoHash.withCharacterPrecision(lat, lon, numberOfCharacters).toBase32
  }


  /**
    * 获取经纬坐标的GeoHash的Base32值
    *
    * @param lon
    * @param lat
    * @param numberOfCharacters GeoHash Length
    * @return
    */
  def getGeoHashOfBase32(lon: Long, lat: Long, numberOfCharacters: Int): String = {

    if (numberOfCharacters > 12) {
      throw new RuntimeException("geohash out of max of 12 !")
    }
    GeoHash.withCharacterPrecision(coordLong2Double(lat), coordLong2Double(lon), numberOfCharacters).toBase32
  }


  /** *
    * 将Long类型的坐标值转换为Double类型
    *
    * @param v
    * @return
    */
  def coordLong2Double(v: Long): Double = BigDecimal(v * 0.000001).setScale(6, RoundingMode.HALF_DOWN).toDouble

  def main(args: Array[String]): Unit = {

    val lon1 = 116.30174
    val lat1 = 39.979185

    val lon2 = 116.302142
    val lat2 = 39.977653

    //    println(getGeoHashOfBase32(lon1, lat1, 9))
    //
    println(getDistance(lon1, lat1, lon2, lat2))
    //
    //    println(getGeoHashOfBase32(lon2, lat2, 9))

    //    val 116.30174,39.979185

  }
}
package com.bitnei.report.local

import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-02-23 17:21
  *
  */
object GetExpect {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    Logger.getLogger("org").setLevel(Level.ERROR)
    val sparkConf = new SparkConf()

    val sparkSession = SparkSession.builder().config(sparkConf).master("local").appName(app).getOrCreate()
    import sparkSession.implicits._

    val sc = sparkSession.sparkContext

    /////////////////////////////////////////////////////////////////////////////////////////////////////////


    val sparkContext = sparkSession.sparkContext


    //    val rdd01 = sparkSession.read.textFile("data/jl-vid.txt").map(_.trim)
    //
    //    val rdd02 = sparkSession.read.textFile("data/jl-vid_finished.txt").map(_.trim)
    //
    //
    //    rdd01.except(rdd02).foreach(println(_))


    //TODO:报文导出工具
    //    val rdd01 = sc.textFile("data/tmp/info.txt")
    //      .map(x => {
    //        val parts = x.split(" ")
    //        val vin = parts(0)
    //        val vid = parts(1)
    //        (vin, vid)
    //      })
    //
    //
    //    val rdd02 = sc.textFile("data/tmp/vin.txt")
    //      .map(x => (x, 1))
    //
    //    rdd01.join(rdd02).map(_._2._1)
    //      .repartition(1).saveAsTextFile("/haha")

    //    sparkSession.read.parquet("data/part*").show(false)


//    sparkSession.read.textFile("data/vin.txt").rdd.map(x => ("\'" + x + "\'"))
//      .reduce((a, b) => {
//        a + "," + b
//      })
//
//      .foreach(print)


    sparkSession.read.parquet("data/realinfo/20171107/tmp/*.parquet")
      .map(x=>x.getAs[String]("VID")+","+x.getAs[String]("TIME")+","+x.getAs[String]("3201")+","+x.getAs[String]("2201")+","+x.getAs[String]("2614")+","+x.getAs[String]("2301")+","+x.getAs[String]("2615"))
      .write.text("/20171107_realinfo_4")

    sparkContext.stop()


  }
}
package com.bitnei.report.common

/*
* created by wangbaosheng on 2017/11/22
*/
object  GpsDistance{
  private val  EARTH_RADIUS = 6371D

  private def rad(d: Double): Double=d * Math.PI / 180.0


  /**
    * 根据两点间经纬度坐标（double值），计算两点间距离，单位为米
    */
  def getDistance(lng1:Double, lat1: Double, lng2: Double, lat2:Double): Double= {
    val radLat1 = lat1*Math.PI/180.0
    val radLat2 = lat2*Math.PI/180.0
    val a = Math.abs(radLat1 - radLat2)
    val b = Math.abs(lng1*Math.PI/180.0 - lng2*Math.PI/180.0)
    val dis = 2*Math.asin(
      Math.sqrt(
        Math.sin(a/2.0)*math.sin(a/2.0) +
          Math.cos(radLat1)*Math.cos(radLat2)*Math.sin(b/2.0)*Math.sin(b/2.0)
      )
    )
    dis*EARTH_RADIUS *1000
  }

  /**
    * 根据两点间经纬度坐标（double值），计算两点间距离，单位为千米。
    */
  def getDistanceKm(lng1:Double, lat1: Double, lng2: Double, lat2:Double): Double= {
    val radLat1 = lat1*Math.PI/180.0
    val radLat2 = lat2*Math.PI/180.0
    val a = Math.abs(radLat1 - radLat2)
    val b = Math.abs(lng1*Math.PI/180.0 - lng2*Math.PI/180.0)
    val dis = 2*Math.asin(
      Math.sqrt(
        Math.sin(a/2.0)*math.sin(a/2.0) +
          Math.cos(radLat1)*Math.cos(radLat2)*Math.sin(b/2.0)*Math.sin(b/2.0)
      )
    )
    dis*EARTH_RADIUS
  }
}package com.bitnei.report.common


/**
  * Created by wangbaosheng on 2017/8/23.
  */
object GpsMileage {
  def distance(longitudeA: Double, latitudeA: Double, latitudeB: Double, longitudeB: Double): Double = {
    val R = 6371D

    R * Math.acos(
      Math.cos(latitudeA * Math.PI / 180) * Math.cos(latitudeB * Math.PI / 180) * Math.cos(longitudeB * Math.PI / 180 - longitudeA * Math.PI / 180) +
        Math.sin(latitudeA * Math.PI / 180) * Math.sin(latitudeB * Math.PI / 180))
  }
}
package com.bitnei.alarm.generator

import com.bitnei.report.util.GeoUtils

/**
  *
  * @author zhangyongtian
  * @define 轨迹GPS 里程生成
  *
  *               create 2018-04-02 10:54
  *
  */
object GpsMileageGenerator {


  def handle(points: Array[RunStateInput]): Double = {

    //按时间排序
    val arr = points.sortBy(_.time)

    var res = 0D

    //累加相邻两帧之间的距离

    var flag = 0
    for (i <- 0 until arr.length if i > flag) {

      if (i != arr.length - 1) {
        val pre = arr(i)
        val post = arr(i + 1)

        res = res + GeoUtils.getDistance(pre.lon, pre.lat, post.lon, post.lat)

        flag = i + 1
      }
    }

    res
  }


}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}
// $example off$
import org.apache.spark.sql.SparkSession

object GradientBoostedTreeClassifierExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("GradientBoostedTreeClassifierExample")
      .getOrCreate()

    // $example on$
    // Load and parse the data file, converting it to a DataFrame.
    val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

    // Index labels, adding metadata to the label column.
    // Fit on whole dataset to include all labels in index.
    val labelIndexer = new StringIndexer()
      .setInputCol("label")
      .setOutputCol("indexedLabel")
      .fit(data)
    // Automatically identify categorical features, and index them.
    // Set maxCategories so features with > 4 distinct values are treated as continuous.
    val featureIndexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("indexedFeatures")
      .setMaxCategories(4)
      .fit(data)

    // Split the data into training and test sets (30% held out for testing).
    val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))

    // Train a GBT model.
    val gbt = new GBTClassifier()
      .setLabelCol("indexedLabel")
      .setFeaturesCol("indexedFeatures")
      .setMaxIter(10)

    // Convert indexed labels back to original labels.
    val labelConverter = new IndexToString()
      .setInputCol("prediction")
      .setOutputCol("predictedLabel")
      .setLabels(labelIndexer.labels)

    // Chain indexers and GBT in a Pipeline.
    val pipeline = new Pipeline()
      .setStages(Array(labelIndexer, featureIndexer, gbt, labelConverter))

    // Train model. This also runs the indexers.
    val model = pipeline.fit(trainingData)

    // Make predictions.
    val predictions = model.transform(testData)

    // Select example rows to display.
    predictions.select("predictedLabel", "label", "features").show(5)

    // Select (prediction, true label) and compute test error.
    val evaluator = new MulticlassClassificationEvaluator()
      .setLabelCol("indexedLabel")
      .setPredictionCol("prediction")
      .setMetricName("accuracy")
    val accuracy = evaluator.evaluate(predictions)
    println("Test Error = " + (1.0 - accuracy))

    val gbtModel = model.stages(2).asInstanceOf[GBTClassificationModel]
    println("Learned classification GBT model:\n" + gbtModel.toDebugString)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.feature.VectorIndexer
import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}
// $example off$
import org.apache.spark.sql.SparkSession

object GradientBoostedTreeRegressorExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("GradientBoostedTreeRegressorExample")
      .getOrCreate()

    // $example on$
    // Load and parse the data file, converting it to a DataFrame.
    val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

    // Automatically identify categorical features, and index them.
    // Set maxCategories so features with > 4 distinct values are treated as continuous.
    val featureIndexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("indexedFeatures")
      .setMaxCategories(4)
      .fit(data)

    // Split the data into training and test sets (30% held out for testing).
    val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))

    // Train a GBT model.
    val gbt = new GBTRegressor()
      .setLabelCol("label")
      .setFeaturesCol("indexedFeatures")
      .setMaxIter(10)

    // Chain indexer and GBT in a Pipeline.
    val pipeline = new Pipeline()
      .setStages(Array(featureIndexer, gbt))

    // Train model. This also runs the indexer.
    val model = pipeline.fit(trainingData)

    // Make predictions.
    val predictions = model.transform(testData)

    // Select example rows to display.
    predictions.select("prediction", "label", "features").show(5)

    // Select (prediction, true label) and compute test error.
    val evaluator = new RegressionEvaluator()
      .setLabelCol("label")
      .setPredictionCol("prediction")
      .setMetricName("rmse")
    val rmse = evaluator.evaluate(predictions)
    println("Root Mean Squared Error (RMSE) on test data = " + rmse)

    val gbtModel = model.stages(1).asInstanceOf[GBTRegressionModel]
    println("Learned regression GBT model:\n" + gbtModel.toDebugString)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.{Algo, BoostingStrategy}
import org.apache.spark.util.Utils

/**
 * An example runner for Gradient Boosting using decision trees as weak learners. Run with
 * {{{
 * ./bin/run-example mllib.GradientBoostedTreesRunner [options]
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 *
 * Note: This script treats all features as real-valued (not categorical).
 *       To include categorical features, modify categoricalFeaturesInfo.
 */
object GradientBoostedTreesRunner {

  case class Params(
      input: String = null,
      testInput: String = "",
      dataFormat: String = "libsvm",
      algo: String = "Classification",
      maxDepth: Int = 5,
      numIterations: Int = 10,
      fracTest: Double = 0.2) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("GradientBoostedTrees") {
      head("GradientBoostedTrees: an example decision tree app.")
      opt[String]("algo")
        .text(s"algorithm (${Algo.values.mkString(",")}), default: ${defaultParams.algo}")
        .action((x, c) => c.copy(algo = x))
      opt[Int]("maxDepth")
        .text(s"max depth of the tree, default: ${defaultParams.maxDepth}")
        .action((x, c) => c.copy(maxDepth = x))
      opt[Int]("numIterations")
        .text(s"number of iterations of boosting," + s" default: ${defaultParams.numIterations}")
        .action((x, c) => c.copy(numIterations = x))
      opt[Double]("fracTest")
        .text(s"fraction of data to hold out for testing.  If given option testInput, " +
          s"this option is ignored. default: ${defaultParams.fracTest}")
        .action((x, c) => c.copy(fracTest = x))
      opt[String]("testInput")
        .text(s"input path to test dataset.  If given, option fracTest is ignored." +
          s" default: ${defaultParams.testInput}")
        .action((x, c) => c.copy(testInput = x))
      opt[String]("dataFormat")
        .text("data format: libsvm (default), dense (deprecated in Spark v1.1)")
        .action((x, c) => c.copy(dataFormat = x))
      arg[String]("<input>")
        .text("input path to labeled examples")
        .required()
        .action((x, c) => c.copy(input = x))
      checkConfig { params =>
        if (params.fracTest < 0 || params.fracTest > 1) {
          failure(s"fracTest ${params.fracTest} value incorrect; should be in [0,1].")
        } else {
          success
        }
      }
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {

    val conf = new SparkConf().setAppName(s"GradientBoostedTreesRunner with $params")
    val sc = new SparkContext(conf)

    println(s"GradientBoostedTreesRunner with parameters:\n$params")

    // Load training and test data and cache it.
    val (training, test, numClasses) = DecisionTreeRunner.loadDatasets(sc, params.input,
      params.dataFormat, params.testInput, Algo.withName(params.algo), params.fracTest)

    val boostingStrategy = BoostingStrategy.defaultParams(params.algo)
    boostingStrategy.treeStrategy.numClasses = numClasses
    boostingStrategy.numIterations = params.numIterations
    boostingStrategy.treeStrategy.maxDepth = params.maxDepth

    val randomSeed = Utils.random.nextInt()
    if (params.algo == "Classification") {
      val startTime = System.nanoTime()
      val model = GradientBoostedTrees.train(training, boostingStrategy)
      val elapsedTime = (System.nanoTime() - startTime) / 1e9
      println(s"Training time: $elapsedTime seconds")
      if (model.totalNumNodes < 30) {
        println(model.toDebugString) // Print full model.
      } else {
        println(model) // Print model summary.
      }
      val trainAccuracy =
        new MulticlassMetrics(training.map(lp => (model.predict(lp.features), lp.label))).accuracy
      println(s"Train accuracy = $trainAccuracy")
      val testAccuracy =
        new MulticlassMetrics(test.map(lp => (model.predict(lp.features), lp.label))).accuracy
      println(s"Test accuracy = $testAccuracy")
    } else if (params.algo == "Regression") {
      val startTime = System.nanoTime()
      val model = GradientBoostedTrees.train(training, boostingStrategy)
      val elapsedTime = (System.nanoTime() - startTime) / 1e9
      println(s"Training time: $elapsedTime seconds")
      if (model.totalNumNodes < 30) {
        println(model.toDebugString) // Print full model.
      } else {
        println(model) // Print model summary.
      }
      val trainMSE = DecisionTreeRunner.meanSquaredError(model, training)
      println(s"Train mean squared error = $trainMSE")
      val testMSE = DecisionTreeRunner.meanSquaredError(model, test)
      println(s"Test mean squared error = $testMSE")
    }

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils
// $example off$

object GradientBoostingClassificationExample {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("GradientBoostedTreesClassificationExample")
    val sc = new SparkContext(conf)
    // $example on$
    // Load and parse the data file.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
    // Split the data into training and test sets (30% held out for testing)
    val splits = data.randomSplit(Array(0.7, 0.3))
    val (trainingData, testData) = (splits(0), splits(1))

    // Train a GradientBoostedTrees model.
    // The defaultParams for Classification use LogLoss by default.
    val boostingStrategy = BoostingStrategy.defaultParams("Classification")
    boostingStrategy.numIterations = 3 // Note: Use more iterations in practice.
    boostingStrategy.treeStrategy.numClasses = 2
    boostingStrategy.treeStrategy.maxDepth = 5
    // Empty categoricalFeaturesInfo indicates all features are continuous.
    boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]()

    val model = GradientBoostedTrees.train(trainingData, boostingStrategy)

    // Evaluate model on test instances and compute test error
    val labelAndPreds = testData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()
    println("Test Error = " + testErr)
    println("Learned classification GBT model:\n" + model.toDebugString)

    // Save and load model
    model.save(sc, "target/tmp/myGradientBoostingClassificationModel")
    val sameModel = GradientBoostedTreesModel.load(sc,
      "target/tmp/myGradientBoostingClassificationModel")
    // $example off$
  }
}
// scalastyle:on println


/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.tree.GradientBoostedTrees
import org.apache.spark.mllib.tree.configuration.BoostingStrategy
import org.apache.spark.mllib.tree.model.GradientBoostedTreesModel
import org.apache.spark.mllib.util.MLUtils
// $example off$

object GradientBoostingRegressionExample {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("GradientBoostedTreesRegressionExample")
    val sc = new SparkContext(conf)
    // $example on$
    // Load and parse the data file.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
    // Split the data into training and test sets (30% held out for testing)
    val splits = data.randomSplit(Array(0.7, 0.3))
    val (trainingData, testData) = (splits(0), splits(1))

    // Train a GradientBoostedTrees model.
    // The defaultParams for Regression use SquaredError by default.
    val boostingStrategy = BoostingStrategy.defaultParams("Regression")
    boostingStrategy.numIterations = 3 // Note: Use more iterations in practice.
    boostingStrategy.treeStrategy.maxDepth = 5
    // Empty categoricalFeaturesInfo indicates all features are continuous.
    boostingStrategy.treeStrategy.categoricalFeaturesInfo = Map[Int, Int]()

    val model = GradientBoostedTrees.train(trainingData, boostingStrategy)

    // Evaluate model on test instances and compute test error
    val labelsAndPredictions = testData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()
    println("Test Mean Squared Error = " + testMSE)
    println("Learned regression GBT model:\n" + model.toDebugString)

    // Save and load model
    model.save(sc, "target/tmp/myGradientBoostingRegressionModel")
    val sameModel = GradientBoostedTreesModel.load(sc,
      "target/tmp/myGradientBoostingRegressionModel")
    // $example off$
  }
}
// scalastyle:on println
package sample.hello

import akka.actor.Actor

object Greeter {
  case object Greet
  case object Done
}

class Greeter extends Actor {
  def receive = {
    case Greeter.Greet =>
      println("Hello World!")
      sender() ! Greeter.Done
  }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import java.util.Random

import org.apache.spark.sql.SparkSession

/**
 * Usage: GroupByTest [numMappers] [numKVPairs] [KeySize] [numReducers]
 */
object GroupByTest {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("GroupBy Test")
      .getOrCreate()

    val numMappers = if (args.length > 0) args(0).toInt else 2
    val numKVPairs = if (args.length > 1) args(1).toInt else 1000
    val valSize = if (args.length > 2) args(2).toInt else 1000
    val numReducers = if (args.length > 3) args(3).toInt else numMappers

    val pairs1 = spark.sparkContext.parallelize(0 until numMappers, numMappers).flatMap { p =>
      val ranGen = new Random
      val arr1 = new Array[(Int, Array[Byte])](numKVPairs)
      for (i <- 0 until numKVPairs) {
        val byteArr = new Array[Byte](valSize)
        ranGen.nextBytes(byteArr)
        arr1(i) = (ranGen.nextInt(Int.MaxValue), byteArr)
      }
      arr1
    }.cache()
    // Enforce that everything has been calculated and in cache
    pairs1.count()

    println(pairs1.groupByKey(numReducers).count())

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.noticecode

import java.text.SimpleDateFormat
import java.util.Calendar

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-29 14:16
  *
  */
object HahaTest {


  def getWeekOfYear(timeStr:String,pattern:String):Int={
    var cal:Calendar =Calendar.getInstance();

    val sdf = new SimpleDateFormat(pattern)
    cal.setTime(sdf.parse(timeStr))

//    cal.setFirstDayOfWeek(Calendar.MONDAY); //

    cal.get(Calendar.WEEK_OF_YEAR)
  }

  def main(args: Array[String]): Unit = {
    println(1E6)
//    println("01".toInt)
//    println(getWeekOfYear("20171228161936","yyyyMMddHHmmss"))
  }


}
package tempjob

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.HbaseHelper
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.Delete
import org.apache.hadoop.hbase.util.Bytes

import scala.collection.mutable.ArrayBuffer

object HbaseDelete {
  def main(args: Array[String]): Unit = {
    stateConf.add(args)
    configOracle()
    run()
  }

  val stateConf = new StateConf

  val con = HbaseHelper.getConnection(
    stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89"),
    stateConf.getOption("hbase.zkport").getOrElse("2181"))

  val table = con.getTable(TableName.valueOf("mileage_check_coords"))


  def run(): Unit = {
    val allVehicle = getAllVehicle

    var i = 0;

    allVehicle.foreach(vehicle => {
      i = i + 1
      println(s"delting $i")
      deleteRow(vehicle)
    })
  }

  def deleteRow(vehicle:String): Unit ={
   // delete(s"${vehicle}_201709")
    delete(s"${vehicle}_201711")
  }

  def getAllVehicle: Array[String] = {
    val allVehicle = new ArrayBuffer[String]()
    JdbcPoolHelper.getJdbcPoolHelper(stateConf)
      .executeQuery("select UUID from sys_vehicle", stmt => {
        while (stmt.next()) {
          allVehicle.append(stmt.getString(1))
        }
      })

    allVehicle.toArray
  }

  def delete(rowKey: String): Unit = {
    val deleteCommand = new Delete(Bytes.toBytes(rowKey))
    table.delete(deleteCommand)
  }

  def configOracle(): Unit = {
    stateConf.set(Constant.JdbcUserName, "ev")
    stateConf.set(Constant.JdbcPasswd, "ev")
    stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
    stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.2.51:1521:evmsc1")
    stateConf.set("database", "oracle")
  }
}
package com.bitnei.tools.export.hbase

import java.text.SimpleDateFormat

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
import com.fasterxml.jackson.annotation.JsonInclude
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.{Get, Scan}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.log4j.{Level, Logger}
import org.apache.hadoop.hbase.filter.CompareFilter
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.TableName
import org.apache.spark.sql.SaveMode
import org.json4s.jackson.Json

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer
import scala.io.Source

/**
  *
  * @author zhangyongtian
  * @define 根据VID文件 从hbase获取数据
  *
  *                 create 2017-11-24 9:16
  *
  */
object HBaseExporter extends Serializable with Logging {

  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    println("appName:" + app)

    // TODO: 参数集合
    println("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    println("日志级别：" + logLevel)

    logLevel match {
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }


    // TODO: 运行环境 （研发、测试、生产）
    var env = stateConf.getOption("env").getOrElse("local")
    println("运行模式：" + env)

    // TODO: 验证输入参数
    println("验证输入参数....")


    //查询条件数据 vids
    var selectDataPath = "data/export-vnum-uid/vids.txt"

    //时间参数 时间段
    var startTime = "20160101000000"
    var endTime = "20171122235959"

    if (startTime.length != 14 || endTime.length != 14) {
      throw new Exception("input.dates error")
    }

    //需要查询的字段
    var schema = Array[String](
      "VID",
      "VIN",
      "2502",
      "2503"
    )

    //需要查询的hbase表
    var htableName = "realinfo"


    //输出参数
    var outHdfsPath = ""
    var outHdfsFormat = ""


    if (!env.equals("local")) {
      selectDataPath = "file://" + stateConf.getOption("input.data").get

      startTime = stateConf.getOption("input.starttime").get
      endTime = stateConf.getOption("input.endtime").get

      ///usr/local/sparkjob/zyt/export/hbase
      val schemaDataPath = stateConf.getOption("input.schema").get

      println(schemaDataPath + "=============")
      schema = Source.fromFile(schemaDataPath).getLines().toArray

      htableName = stateConf.getOption("input.htableName").get

      outHdfsPath = stateConf.getString("output.hdfs.path")
      outHdfsFormat = stateConf.getString("output.format")

    }




    // TODO: 加载上下文
    println("加载上下文")
    //////////////////////////test//////////////////////////
    //    val sparkSession = SparkHelper.getSparkSession(sparkMaster = Option("local[*]"), sparkAppName = Option(app + "_" + startTime + "_" + endTime))
    //    import sparkSession.implicits._
    ////////////////////////////////////////////////////////

    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None, sparkAppName = Option(app + "_" + startTime + "_" + endTime))
    import sparkSession.implicits._

    //    val sparkConf = sparkSession.conf
    //    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");

    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //    sparkConf.set("spark.kryo.registrationRequired", "true")


    // TODO: 读取Hbase


    //研发环境 	yf-nn2:2181,yf-dn1:2181,yf-dn2:2181
    var zkquorum = "192.168.6.103,192.168.6.104,192.168.6.105"
    var zkport = "2181"

    if (env.equals("prd")) {
      //生产环境
      //        quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89")
      //        zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")

      zkquorum = "192.168.2.70,192.168.2.71,192.168.2.89"
      zkport = "2181"
    }


    val selectDataDS = sparkSession.read.textFile(selectDataPath).as[String]


    //给dataset map 返回值设置编码
    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[java.util.Map[String, String]]

    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    val result = selectDataDS

      .mapPartitions(vids => {

        import scala.collection.JavaConversions._
        val table = HbaseHelper.getConnection(zkquorum, zkport).getTable(TableName.valueOf(htableName))
        val scan = new Scan()

        vids.flatMap(vid => {

          val mapList = new mutable.ListBuffer[java.util.Map[String, String]]()

          //范围查询
          //	008fc677-c52d-481c-83f8-e6c38b97228e_1487635839000  （报文接收时间）
          //
          val startRowkey = vid + "_" + sdf.parse(startTime).getTime / 1000
          val stopRowkey = vid + "_" + sdf.parse(endTime).getTime / 1000
          scan.setStartRow(Bytes.toBytes(startRowkey))
          scan.setStopRow(Bytes.toBytes(stopRowkey))
          val resultScanner = table.getScanner(scan)

          for (res <- resultScanner) {
            //获取行
            val map = new mutable.HashMap[String, String]()

            for (col <- schema) {
              val family = Bytes.toBytes("cf")
              val bCol = Bytes.toBytes(col)
              val get = new Get(res.getRow).addColumn(family, bCol)
              val col_value = table.get(get).getValue(family, bCol);
              map.put(col, Option(Bytes.toString(col_value)).getOrElse(""))
              //              println(col + "-->" + Option(Bytes.toString(col_value)).getOrElse(""))
            }
            mapList.append(map)
          }
          mapList
        })
      }).map(x => {
      new ObjectMapper().writeValueAsString(x)
    })


    //      20171002070627,,2,690,214510,5300,1901,245,116033376,39708961

    // TODO: 输出
    if (env.equals("local")) {
      result.show(false)
      result.count()
    }

    //TODO: 输出到HDFS
    logInfo("输出到HDFS　start....")


    if (!env.equals("local")) {
      //研发环境
      var hdfsPath = s"/tmp/zyt/data/${app}/${startTime}_${endTime}"

      //生产环境
      if (env.equals("prd")) {
        hdfsPath = s"${outHdfsPath}/${app}/${startTime}_${endTime}"
      }

      //      /spark/vehicle/result/${app}
      //      val hdfsResult = result.mapPartitions(values => {
      //        toJson(values.toArray[ParkInfo]).toIterator
      //      }).toDF()

      if (env.equals("dev")) {
        result.coalesce(1).write.format(s"${outHdfsFormat}").mode(SaveMode.Overwrite).save(hdfsPath)
      }

      if (env.equals("prd")) {
        result.coalesce(20).write.format(s"${outHdfsFormat}").mode(SaveMode.Overwrite).save(hdfsPath)
      }

      logInfo("输出到HDFS end ....")

    }


    sparkSession.stop()

  }
}
package com.bitnei.sparkhelper

import java.io.{ByteArrayInputStream, ByteArrayOutputStream}
import java.util.zip.InflaterInputStream

import com.bitnei.report.common.log.Logging
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.{Base64, Bytes}
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}

import scala.collection.mutable.ArrayBuffer

/**
  * Created by wangbaosheng on 2017/4/10.
  */
object HbaseHelper extends Logging{
  def uncompress(message: Array[Byte], offset: Int, length: Int): String = {
    val bis = new ByteArrayInputStream(message, offset, length)
    val ii = new InflaterInputStream(bis)
    val baos = new ByteArrayOutputStream()
    val buffer = new ArrayBuffer[Byte]
    (0 until ii.available()).foreach(x => buffer.append(0))

    val buf = buffer.toArray
    var stop = false
    while (!stop) {
      val c = ii.read(buf)
      if (c != -1) baos.write(buf, 0, c)
      else stop = true
    }
    ii.close()
    baos.close()
    new String(baos.toByteArray, "UTF-8")
  }

  def decodeHbaseString(message: Array[Byte], offset: Int, length: Int): String = {
    new String(Base64.decode(uncompress(message, offset, length)))
  }


  def readRow(schema: Schema, r: Result, valueMap: (Relation, String) => String): String = {
    val cells = r.rawCells()

    def getCellValue(relation: Relation): String = {
      cells.find(cell => Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength) == relation.srcName) match {
        case Some(cell) =>
          val qualifier = Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength)
          //如果指定了编码
          if (relation.encode.nonEmpty) {
            decodeHbaseString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)
          } else {
            Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)
          }
        case None =>
          ""
      }
    }


    def getCellsValue(): String = {
      val resultBuilder = new StringBuilder
      cells.foreach(cell => {
        val qualifier = Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength)
        val value = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)
        resultBuilder.append(s"$qualifier:$value,")
      })

      if (resultBuilder.last == ',') resultBuilder.substring(0, resultBuilder.length - 1) else resultBuilder.toString()
    }


    if (schema.realations.length > 0) {
      val resultBuilder = new StringBuilder
      schema.realations.foreach(relation => {
        val value = valueMap(relation, getCellValue(relation))
        resultBuilder.append(s"$value,")
      })
      if (resultBuilder.last == ',') resultBuilder.substring(0, resultBuilder.length - 1) else resultBuilder.toString()
    } else {
      //如果没有指定schema，获取所有值
      getCellsValue()
    }
  }

  def getConnection(zkQuorum: String, zkPort: String): Connection = {
    val conf = HBaseConfiguration.create()
    conf.set("hbase.zookeeper.quorum", zkQuorum)
    conf.set("hbase.zookeeper.property.clientPort", zkPort)
    val con = ConnectionFactory.createConnection(conf)
    con
  }


  def bulkPut(zkQuorum: String, zkPort: String, tableName: String, f: (Table) => Unit): Unit = {
    val hbaseConnection = getConnection(zkQuorum, zkPort)
    var table: Table = null
    try {
      table = hbaseConnection.getTable(TableName.valueOf(tableName))
      f(table)
    }catch {
      case e:Exception=>
        logError(s"writing hbase throw en exception.zk:$zkQuorum:$zkPort,table=$table",e)
    }finally {
      if (table != null) table.close()
      if (hbaseConnection != null) hbaseConnection.close()
    }
  }


  def createRow(rowKey: Array[Byte], family: String, qualified: String, value: String): Put = {
    val put = new Put(rowKey)
    put.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualified), Bytes.toBytes(value))

    put
  }


  def get(table: Table, rowKey: String, quarlier: String): Option[String] = {
    val get = new Get(Bytes.toBytes(rowKey))
    val row = table.get(get)
    row.rawCells().find(cell => Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength) == quarlier)
      .map(cell => Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength))
  }
}

case class Relation(srcName:String,dstName:String,decimal:String,encode:String)  extends  Serializable
case class Schema (tableName:String,dstDirectory:String,realations:Array[Relation])  extends  Serializable
package com.bitnei.report.operationIndex.job

import java.util

import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.HbaseHelper
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.util.Bytes

/*
* created by wangbaosheng on 2017/12/21
*/
class HbaseOutput extends  Logging {
  def output(hbaseQuorum: String, hbaseZkport: String, values: Iterable[OperationDetailFormatValue]): Unit = {
    //HbaseHelper.select(hbaseQuorum,hbaseQuorum,"operationIndexDetail")
    //    HbaseHelper.bulkPut(hbaseQuorum, hbaseZkport, "operationIndexDetail", t => {
    ////      Utils.batchProcessOfArray(values,100,(offset,length)=>{
    ////        var i=offset
    ////        while(i<offset){
    ////          val v=values(i)
    ////        }
    ////        logInfo(v.toString)
    ////        val rowkey = Bytes.toBytes(v.rowKey)
    ////        val putList = new util.ArrayList[Put]()
    ////        putList.add(HbaseHelper.createRow(rowkey, "df", "type", v.detailType))
    ////        putList.add(HbaseHelper.createRow(rowkey, "df", "detail", v.detail))
    ////        t.put(putList)
    ////      })
    HbaseHelper.bulkPut(hbaseQuorum, hbaseZkport, "operationIndexDetail", t => {
      values.foreach(v => {
        //logInfo(v.toString)
        val rowkey = Bytes.toBytes(v.rowKey)
        val putList = new util.ArrayList[Put]()
        putList.add(HbaseHelper.createRow(rowkey, "df", "type", v.detailType))
        putList.add(HbaseHelper.createRow(rowkey, "df", "detail", v.detail))
        t.put(putList)
      })
    })
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import org.apache.spark.sql.SparkSession


object HdfsTest {

  /** Usage: HdfsTest [file] */
  def main(args: Array[String]) {
    if (args.length < 1) {
      System.err.println("Usage: HdfsTest <file>")
      System.exit(1)
    }
    val spark = SparkSession
      .builder
      .appName("HdfsTest")
      .getOrCreate()
    val file = spark.read.text(args(0)).rdd
    val mapped = file.map(s => s.length).cache()
    for (iter <- 1 to 10) {
      val start = System.currentTimeMillis()
      for (x <- mapped) { x + 2 }
      val end = System.currentTimeMillis()
      println("Iteration " + iter + " took " + (end-start) + " ms")
    }
    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
 * Counts words in new text files created in the given directory
 * Usage: HdfsWordCount <directory>
 *   <directory> is the directory that Spark Streaming will use to find and read new text files.
 *
 * To run this on your local machine on directory `localdir`, run this example
 *    $ bin/run-example \
 *       org.apache.spark.examples.streaming.HdfsWordCount localdir
 *
 * Then create a text file in `localdir` and the words in the file will get counted.
 */
object HdfsWordCount {
  def main(args: Array[String]) {
    if (args.length < 1) {
      System.err.println("Usage: HdfsWordCount <directory>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()
    val sparkConf = new SparkConf().setAppName("HdfsWordCount")
    // Create the context
    val ssc = new StreamingContext(sparkConf, Seconds(2))

    // Create the FileInputDStream on the directory and use the
    // stream to count words in new files created
    val lines = ssc.textFileStream(args(0))
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
package com.bitnei.samples.hello

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-08 16:24
  *
  */

import akka.actor.Actor
import akka.actor.ActorSystem
import akka.actor.Props

class HelloActor extends Actor {
  def receive = {
    case "hello" => println("hello back to you.")
    case _ => println("huh?")
  }
}

object Test1_HelloActor extends App {
  // actor need an ActorSystem
  val system = ActorSystem("HelloSystem")
  // create and start the actor
  val helloActor = system.actorOf(Props[HelloActor], name = "helloActor")
  // send two messages
  helloActor ! "hello"
  helloActor ! "what"
  // shutdown the actor system
  system.terminate()
}package sample.hello

import akka.actor.Actor
import akka.actor.Props

class HelloWorld extends Actor {

  override def preStart(): Unit = {
    // create the greeter actor
    val greeter = context.actorOf(Props[Greeter], "greeter")
    // tell it to perform the greeting
    greeter ! Greeter.Greet
  }

  def receive = {
    // when the greeter is done, stop this actor and with it the application
    case Greeter.Done => context.stop(self)
  }
}

package com.bitnei.alarm

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.util.NumberUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{Dataset, SaveMode, SparkSession}

object HistSpeedAndMileage extends Serializable with Logging {

  case class RealinfoReport(vid: String, vin: String, time: String, speed: Int, mileage: Int, lon: Double, lat: Double)

  case class Input(vid: String, time: String, mileage: Double, speed: Double)


  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)
    logInfo("参数集合....")

    val stateConf = new StateConf
    stateConf.add(args)

    //设置日志级别
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    //运行环境（研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("验证参数....")

    //输出参数
    var outFormat = "#"
    var outputTargets = "console"

    var date = "20171101"

    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get

      outputTargets = stateConf.getOption("output").get
      outFormat = stateConf.getOption("output.format").get
    }


    logInfo("加载上下文")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }
    val conf = new SparkConf()
    val sparkSession = SparkSession.builder().config(conf).master(sparkMaster).appName(this.getClass.getName).getOrCreate()
    import sparkSession.implicits._

    env match {
      case "local" => {
        sparkSession.read.json("data/realinfo/mock.txt").createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/").createOrReplaceTempView("realinfo")
      }
    }

    val sql =
      s"""
         SELECT VID,TIME,`2202` AS mileage,`2201` AS speed FROM realinfo where VID IS NOT NULL  AND `2202` IS NOT NULL AND `2201` IS NOT NULL
      """.stripMargin

    val initDS = sparkSession.sql(sql).as[(String, String, String, String)]

    //TODO:过滤
    val filterDS = initDS.filter(x => {

      val vid = x._1

      val time = x._2

      val milage = x._3.toDouble

      val speed = x._4.toDouble

      milage > 0 && speed > 0
    })

    //TODO:提取
    val mappedDS = filterDS.map(x => {
      val vid = x._1
      val time = x._2
      val milage = x._3.toDouble / 10
      val speed = x._4.toDouble / 10

      Input(vid, time, milage, speed)
    })


    val result = mappedDS.groupByKey(_.vid).mapGroups({
      case (vid: String, rows: Iterator[Input]) => {

        //        //TODO:计算里程的中位数
        val sortMileage = rows.toArray.map(_.mileage).sorted
        //val sortMileage = rows.toArray.map(_.mileage).sorted
        //        val time = rows.toArray.map(_.time)
        val sortMileagelength = sortMileage.length


        var medianMileage = NumberUtils.getMedianOfDouble(sortMileage)


        //TODO:计算速度的中位数
        val sortSpeed = rows.toArray.map(_.speed).sorted
        //val sortMileage = rows.toArray.map(_.mileage).sorted
        val speedslength = sortSpeed.length
        var medianSpeed = 0D
        if (speedslength % 2 != 0) {
          medianSpeed = sortMileage(speedslength / 2)
        } else {
          if (speedslength > 0){
            medianSpeed = (sortMileage(speedslength / 2) + sortMileage(speedslength / 2 - 1)) / 2
          }
        }

        Output(vid, medianMileage, medianSpeed)

      }
    })

    //      .filter(x => x.medianMileage > 0 && x.medianSpeed > 0)
    //      .filter(x => x.milean > 0 && x.medianSpeed > 0)

    if (env.equals("local")) {
      result.show(false)
    }

    if (env.equals("prd") && outputTargets.contains("hdfs")) {
      result.write.json("/spark/vehicle/result/median")
      //vid_mileageDS.write.mode(SaveMode.Overwrite).json("/spark/vehicle/result/median")
    }


    logInfo("任务完成...")


    sparkSession.stop()
  }

  case class Output(vid: String, medianMileage: Double, medianSpeed: Double)


}
package com.bitnei.report

import java.text.SimpleDateFormat
import java.util.Properties

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.KafkaUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define 根据历史结果HDFS文件生成到kafka消息
  * 生产历史结果文件到kafka
  * create 2017-12-27 17:11:08
  *
  */

object Hsdf2Kafka extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var date = "20180113"


    //报表类型
    val reportType = stateConf.getOption("input.report.type")
      .getOrElse("xx")

    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
    }

    if (date.length != 8) {
      throw new Exception("input.date error")
    }

    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    //       .set("spark.kryo.registrator", "MyRegistrator");
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    val sc = sparkSession.sparkContext
    import sparkSession.implicits._

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    ///vehicle/data/examine/2018/01


    ////////////////////////////////////业务逻辑/////////////////////////////////////////////


    var inputPath = env match {
      case "local" => reportType match {
        case "day" => "data/noticecode/dayres"
        case "week" => "data/noticecode/weekres"
      }
      case _ => reportType match {
        case "day" => s"${stateConf.getOption("input.data.path").get}/day/year=${year}/month=${month}/day=${day}"
        case "week" => s"${stateConf.getOption("input.data.path").get}/week/year=${year}/month=${month}/day=${day}"
      }
    }


    val result = sparkSession.read.json(inputPath).as[OutPut]



    ////////////////////////////////输出///////////////////////

    sparkSession.catalog.dropTempView("noticecode") //删除临时表

    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count
      //      result.count()
      //    result.printSchema()
      //    result.show(false)

      //      result
      //        .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
      //        .write
      //        .format("kafka")
      //        .option("kafka.bootstrap.servers", "192.168.6.105:9092,192.168.6.106:9092")
      //        .option("topic", "test")
      //        .save()
    }

    if (!env.equals("local")) {

      result.cache()

      if (stateConf.getString("out.target").contains("kafka")) {
        // TODO: 输出到kafka
        log.info("输出到kafka===========================")
        result.foreachPartition(outputs => {

          val props = new Properties()

          //        props.put("metadata.broker.list", "192.168.6.105:9092,192.168.6.106:9092")

          //        props.put("metadata.broker.list", "192.168.1.54:9092,192.168.1.55:9092")

          props.put("metadata.broker.list", stateConf.getString("output.kafka.brokers"))

          props.put("serializer.class", "kafka.serializer.StringEncoder")
          //        props.put("partitioner.class", classOf[HashPartitioner].getName)
          props.put("producer.type", "sync")
          props.put("batch.num.messages", "1")
          props.put("queue.buffering.max.messages", "1000000")
          props.put("queue.enqueue.timeout.ms", "20000000")

          val producer = KafkaUtils.getProducer(props);
          val topic = stateConf.getString("output.kafka.topic")

          outputs.foreach(output => {
            KafkaUtils.producerSendMessage(producer, topic, output.vinDate, output.jsonRes)
          })
          KafkaUtils.producerClose(producer)
        })
      }


      ///////////////////////////////////////////////////////////
      logInfo("任务完成...")

      sparkSession.stop()

    }

  }

  case class OutPut(vinDate: String, jsonRes: String)

}
package com.bitnei.tools.test

import com.bitnei.tools.util.ValidateUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  * @author zhangyongtian
  * @define
  * csv里面的event column 和 page_count column，
  * 求出每种event出现的次数
  * 这个event的page_count的平均数，
  *
  * event的名字需要normalized，
  * 有些event名字有标点的差别但是可以算成同一个
  * event名字里有非ASCII的字符
  * 这些字符要被删掉
  *
  * 然后和pdf说明上不一样的地方是，不需要再tokenize event，
  *
  * 只用取消标点，
  * 搞定空格，
  * 然后改成全小写之类的就行。就是说不用去发现相似的event了，比如lunch and breakfast 和 breakfast and lunch原本应该归为一类，现在可以当成两种名字了。
  *
  */
object HwSparkJob {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    Logger.getLogger("org").setLevel(Level.ERROR)

    val sparkConf = new SparkConf()

    var sparkMaster = "local[*]"

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._

    val initRDD = sparkSession.read.textFile("data/menu.csv").rdd

    initRDD
      .filter(!_.contains("id,name,sponsor,event"))
      .map(x => {
        val parts = x.split(",")
        (parts(3), parts(18))
      })

      .filter(x => {
        val ruler01 = !x._1.isEmpty
        val ruler02 = !x._2.isEmpty

        val ruler03 = ValidateUtils.isNumber(x._2)

        ruler01 && ruler02 && ruler03
      })
      .map(x => {
        val event = x._1.toString
        val pageCount = x._2.toString.toInt
        val e = event.trim.toLowerCase().replaceAll("\'|\\-", "").replaceAll("[\\pP‘’“”]", " ").trim
        (e, pageCount)
      }).filter(!_._1.isEmpty)
      .groupByKey()
      .map(x => {
        val event = x._1
        val arr = x._2
        (event, arr.size, arr.sum / arr.size)
      })
      .sortBy(x => x._1)

      .foreach(x => println(x))

    sparkSession.stop();

  }


  case class Input(event: String, page_count: String)

  //  case class Input(id: String, name: String, sponsor: String, event: String, venue: String, place: String, physical_description: String, occasion: String, notes: String, call_number: String, keywords: String, language: String, date: String, location: String, location_type: String, currency: String, currency_symbol: String, status: String, page_count: String, dish_count: String)

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.mllib.stat.test.ChiSqTestResult
import org.apache.spark.rdd.RDD
// $example off$

object HypothesisTestingExample {

  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName("HypothesisTestingExample")
    val sc = new SparkContext(conf)

    // $example on$
    // a vector composed of the frequencies of events
    val vec: Vector = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25)

    // compute the goodness of fit. If a second vector to test against is not supplied
    // as a parameter, the test runs against a uniform distribution.
    val goodnessOfFitTestResult = Statistics.chiSqTest(vec)
    // summary of the test including the p-value, degrees of freedom, test statistic, the method
    // used, and the null hypothesis.
    println(s"$goodnessOfFitTestResult\n")

    // a contingency matrix. Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))
    val mat: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))

    // conduct Pearson's independence test on the input contingency matrix
    val independenceTestResult = Statistics.chiSqTest(mat)
    // summary of the test including the p-value, degrees of freedom
    println(s"$independenceTestResult\n")

    val obs: RDD[LabeledPoint] =
      sc.parallelize(
        Seq(
          LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0)),
          LabeledPoint(1.0, Vectors.dense(1.0, 2.0, 0.0)),
          LabeledPoint(-1.0, Vectors.dense(-1.0, 0.0, -0.5)
          )
        )
      ) // (feature, label) pairs.

    // The contingency table is constructed from the raw (feature, label) pairs and used to conduct
    // the independence test. Returns an array containing the ChiSquaredTestResult for every feature
    // against the label.
    val featureTestResults: Array[ChiSqTestResult] = Statistics.chiSqTest(obs)
    featureTestResults.zipWithIndex.foreach { case (k, v) =>
      println("Column " + (v + 1).toString + ":")
      println(k)
    }  // summary of the test
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.rdd.RDD
// $example off$

object HypothesisTestingKolmogorovSmirnovTestExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("HypothesisTestingKolmogorovSmirnovTestExample")
    val sc = new SparkContext(conf)

    // $example on$
    val data: RDD[Double] = sc.parallelize(Seq(0.1, 0.15, 0.2, 0.3, 0.25))  // an RDD of sample data

    // run a KS test for the sample versus a standard normal distribution
    val testResult = Statistics.kolmogorovSmirnovTest(data, "norm", 0, 1)
    // summary of the test including the p-value, test statistic, and null hypothesis if our p-value
    // indicates significance, we can reject the null hypothesis.
    println(testResult)
    println()

    // perform a KS test using a cumulative distribution function of our making
    val myCDF = Map(0.1 -> 0.2, 0.15 -> 0.6, 0.2 -> 0.05, 0.3 -> 0.05, 0.25 -> 0.1)
    val testResult2 = Statistics.kolmogorovSmirnovTest(data, myCDF)
    println(testResult2)
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println

package com.bitnei.samples.implct

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-28 17:34
  *
  * 隐式解析机制

即编译器是如何查找到缺失信息的，解析具有以下两种规则：
1.首先会在当前代码作用域下查找隐式实体（隐式方法  隐式类 隐式对象）

2.如果第一条规则查找隐式实体失败，会继续在隐式参数的类型的作用域里查找(如上面代码中的extends部分)
类型的作用域是指与该类型相关联的全部伴生模块，一个隐式实体的类型T它的查找范围如下：
    （1）如果T被定义为T with A with B with C,那么A,B,C都是T的部分，在T的隐式解析过程中，它们的伴生对象都会被搜索
    （2）如果T是参数化类型，那么类型参数和与类型参数相关联的部分都算作T的部分，比如List[String]的隐式搜索会搜索List的
伴生对象和String的伴生对象
    （3） 如果T是一个单例类型p.T，即T是属于某个p对象内，那么这个p对象也会被搜索
    （4） 如果T是个类型注入S#T，那么S和T都会被搜索


  *
  */

object ImplVal {
  implicit val name: String = "reynold"
}

/**
  * 定义成trait的话，可以让ScalaDemo继承，这样就可以自动引入了
  */
trait ImplVal {
  implicit val name: String = "reynold"
}

object ImplicitHelper {

  /**
    * 隐式参数一般和柯里化进行结合,使用该函数不用给出implicit的值
    *
    * @param param
    * @param impl
    */
  def echo(param: String)(implicit impl: String): Unit = {
    println(param + "," + impl)
  }

  implicit def strToInt(str: String) = str.toInt

  /**
    * 隐式类有如下几个限制:
    * They must be defined inside of another trait/class/object.
    * They may only take one non-implicit argument in their constructor.
    * There may not be any method, member or object in scope with the same name as the implicit class.
    * Note: This means an implicit class cannot be a case class.
    * 隐式类的运作方式：
    * 隐式类的主构造函数只能有一个参数（有两个以上并不会报错，但是这个隐式类永远不会被编译器作为隐式类在隐式转化中使用）
    * 且这个参数的类型就是将要被转换的目标类型
    * 隐式转换类将包裹目标类型，隐式类的所有方法都会自动"附加"到目标类型上
    *
    * @param origin 隐式类构造函数参数
    */
  implicit class ImpAdd(origin: Int) {

    def add(param: Int) = origin + param

  }

//  def main(args: Array[String]): Unit = {
//
//    1.add(2)
//
//    strToInt("1")
//
//    math.max("1",2)
//
//    echo("hello")("word")
//
//    echo("hello")
//
////    implicit val impl = "implicit"
////    echo("hello")
//
//  }




}











package com.bitnei.samples.implct

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-28 11:24
  *
  */
object ImplicitTest {



  //  implicit final class ArrowAssoc[A](private val self: A) extends AnyVal {
  //    @inline def ->[B](y: B): Tuple2[A, B] = Tuple2(self, y)
  //
  //    def →[B](y: B): Tuple2[A, B] = ->(y)
  //  }


  // 定义一个操作符+%，将一个给定的百分比添加到某个值。
  // 举例来说，120 +% 10应得到132。提示：由于操作符的方法，而不是函数，你需要提供一个implicit。

//  object MyImplicitTypeConversion {
//    implicit def strToInt(str: String) = str.toInt
//  }

  def main(args: Array[String]): Unit = {
    //    val i = 120 +% 10
    //    println(i)

    //    println(5 !)

//    import MyImplicitTypeConversion.strToInt
//    val max = math.max("1",2)
//    println(max)

  }


  //  implicit def int2MyRichInt(n: Int): MyRichInt = {
  //    new MyRichInt(n)
  //  }


  //  class MyRichInt(val sef: Int) {
  //    def +%(n: Int): Int = {
  //      sef + (sef / n)
  //    }
  //  }


  //  implicit def int2MyRichInt(n: Int): MyRichInt = {
  //    new MyRichInt(n)
  //  }
  //
  //  class MyRichInt(val self: Int) {
  //
  //    def ! : Int = {
  //      1 to self product
  //    }
  //
  //  }


}
package com.bitnei.samples.implct

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-28 11:24
  *
  */
object ImplicitTest02 {





}
package com.bitnei.samples.implct

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-28 17:46
  *
  * 隐式类有如下几个限制：

They must be defined inside of another trait/class/object.

They may only take one non-implicit argument in their constructor.

There may not be any method, member or object in scope with the same name as the implicit class.
Note: This means an implicit class cannot be a case class.

隐式类与旧的隐式转换的语法（implicit def）是有细微的不同的，隐式类的运作方式是：隐式类的主构造函数只能有一个参数（有两个以上并不会报错，但是这个隐式类永远不会被编译器作为隐式类在隐式转化中使用），且这个参数的类型就是将要被转换的目标类型。从语义上这很自然：这个隐式转换类将包裹目标类型，隐式类的所有方法都会自动“附加”到目标类型上。


  *
  */
object ImplicitTest03 {

  //  implicit class MyImplicitTypeConversion(val str: String) {
  //    def strToInt = str.toInt
  //  }

  def main(args: Array[String]): Unit = {

    //    val max = math.max("1",2)
    //    println(max)
  }

//  scala.Predef.formatted("1")

}
package com.bitnei.samples.implct

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-28 17:46
  *
  * 隐式类有如下几个限制：
  * *
  * They must be defined inside of another trait/class/object.
  * *
  * They may only take one non-implicit argument in their constructor.
  * *
  * There may not be any method, member or object in scope with the same name as the implicit class.
  * Note: This means an implicit class cannot be a case class.
  * *
  * 隐式类与旧的隐式转换的语法（implicit def）是有细微的不同的，隐式类的运作方式是：隐式类的主构造函数只能有一个参数（有两个以上并不会报错，但是这个隐式类永远不会被编译器作为隐式类在隐式转化中使用），且这个参数的类型就是将要被转换的目标类型。从语义上这很自然：这个隐式转换类将包裹目标类型，隐式类的所有方法都会自动“附加”到目标类型上。
  *
  *
  *
  */
object ImplicitTest04 {

  object Greeter {
    def greet(name: String)(implicit prompt: String) {
      println("Welcome, " + name + ". The System is ready.")
      println(prompt)
    }
  }

  def main(args: Array[String]) {

    implicit val prompt = ">"

    Greeter.greet("admin")
  }


}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.attribute.Attribute
import org.apache.spark.ml.feature.{IndexToString, StringIndexer}
// $example off$
import org.apache.spark.sql.SparkSession

object IndexToStringExample {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("IndexToStringExample")
      .getOrCreate()

    // $example on$
    val df = spark.createDataFrame(Seq(
      (0, "a"),
      (1, "b"),
      (2, "c"),
      (3, "a"),
      (4, "a"),
      (5, "c")
    )).toDF("id", "category")

    val indexer = new StringIndexer()
      .setInputCol("category")
      .setOutputCol("categoryIndex")
      .fit(df)
    val indexed = indexer.transform(df)

    println(s"Transformed string column '${indexer.getInputCol}' " +
        s"to indexed column '${indexer.getOutputCol}'")
    indexed.show()

    val inputColSchema = indexed.schema(indexer.getOutputCol)
    println(s"StringIndexer will store labels in output column metadata: " +
        s"${Attribute.fromStructField(inputColSchema).toString}\n")

    val converter = new IndexToString()
      .setInputCol("categoryIndex")
      .setOutputCol("originalCategory")

    val converted = converter.transform(indexed)

    println(s"Transformed indexed column '${converter.getInputCol}' back to original string " +
        s"column '${converter.getOutputCol}' using labels in metadata")
    converted.select("id", "categoryIndex", "originalCategory").show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.Interaction
import org.apache.spark.ml.feature.VectorAssembler
// $example off$
import org.apache.spark.sql.SparkSession

object InteractionExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("InteractionExample")
      .getOrCreate()

    // $example on$
    val df = spark.createDataFrame(Seq(
      (1, 1, 2, 3, 8, 4, 5),
      (2, 4, 3, 8, 7, 9, 8),
      (3, 6, 1, 9, 2, 3, 6),
      (4, 10, 8, 6, 9, 4, 5),
      (5, 9, 2, 7, 10, 7, 3),
      (6, 1, 1, 4, 2, 8, 4)
    )).toDF("id1", "id2", "id3", "id4", "id5", "id6", "id7")

    val assembler1 = new VectorAssembler().
      setInputCols(Array("id2", "id3", "id4")).
      setOutputCol("vec1")

    val assembled1 = assembler1.transform(df)

    val assembler2 = new VectorAssembler().
      setInputCols(Array("id5", "id6", "id7")).
      setOutputCol("vec2")

    val assembled2 = assembler2.transform(assembled1).select("id1", "vec1", "vec2")

    val interaction = new Interaction()
      .setInputCols(Array("id1", "vec1", "vec2"))
      .setOutputCol("interactedCol")

    val interacted = interaction.transform(assembled2)

    interacted.show(truncate = false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.report

/*
* created by wangbaosheng on 2017/12/20
*/
object InvalidateValue {
  val Value = Int.MaxValue

  def validate(v: Int): Boolean = if (v == Int.MinValue || v == Int.MaxValue) false else true
  def validate(v: Long): Boolean = if (v == Long.MinValue || v == Long.MaxValue) false else true
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.regression.{IsotonicRegression, IsotonicRegressionModel}
import org.apache.spark.mllib.util.MLUtils
// $example off$

object IsotonicRegressionExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("IsotonicRegressionExample")
    val sc = new SparkContext(conf)
    // $example on$
    val data = MLUtils.loadLibSVMFile(sc,
      "data/mllib/sample_isotonic_regression_libsvm_data.txt").cache()

    // Create label, feature, weight tuples from input data with weight set to default value 1.0.
    val parsedData = data.map { labeledPoint =>
      (labeledPoint.label, labeledPoint.features(0), 1.0)
    }

    // Split data into training (60%) and test (40%) sets.
    val splits = parsedData.randomSplit(Array(0.6, 0.4), seed = 11L)
    val training = splits(0)
    val test = splits(1)

    // Create isotonic regression model from training data.
    // Isotonic parameter defaults to true so it is only shown for demonstration
    val model = new IsotonicRegression().setIsotonic(true).run(training)

    // Create tuples of predicted and real labels.
    val predictionAndLabel = test.map { point =>
      val predictedLabel = model.predict(point._2)
      (predictedLabel, point._1)
    }

    // Calculate mean squared error between predicted and real labels.
    val meanSquaredError = predictionAndLabel.map { case (p, l) => math.pow((p - l), 2) }.mean()
    println("Mean Squared Error = " + meanSquaredError)

    // Save and load model
    model.save(sc, "target/tmp/myIsotonicRegressionModel")
    val sameModel = IsotonicRegressionModel.load(sc, "target/tmp/myIsotonicRegressionModel")
    // $example off$
  }
}
// scalastyle:on println
package com.bitnei.report.common.jdbc

import java.sql.{Connection, PreparedStatement, ResultSet}

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import org.apache.commons.dbcp.BasicDataSource


class JdbcPoolHelper(stateConf:StateConf) extends  Serializable with Logging{
  private val dataSource=try {
    val ds=new BasicDataSource()
    val driveClass=stateConf.getString(Constant.JdbcDriver)
    ds.setDriverClassName(driveClass)
    ds.setUsername(stateConf.getString(Constant.JdbcUserName))
    ds.setPassword(stateConf.getString(Constant.JdbcPasswd))
    ds.setUrl(stateConf.getString(Constant.JdbcUrl))
    ds.setInitialSize(stateConf.getInt(Constant.JdbcPoolInitSize, 5))

    val maxActiveSize=stateConf.getInt(Constant.JdbcPoolMaxActive,5)
    logInfo("jdbc pool max active size is $")
    ds.setMaxActive(maxActiveSize)

    val maxIdelSize=stateConf.getInt(Constant.JdbcPoolMaxIdel,5)
    ds.setMaxIdle(maxIdelSize)

    val maxWaitSize=stateConf.getInt(Constant.JdbcPoolMaxWait,5)
    ds.setMaxWait(maxWaitSize)
    ds
  }catch {
    case e:Exception=>
      logError(s"加载jdbc出现错误${e.getMessage}")
      throw e
  }


  def getConnection:Connection=dataSource.getConnection


  def executeQuery(sql:String,fetchResult:(ResultSet)=>Unit):JdbcPoolHelper= {
    val con = getConnection
    val stmt = con.createStatement()
    val result = stmt.executeQuery(sql)
    try {
      fetchResult(result)
      result.close()
    } finally {
      if(!stmt.isClosed)stmt.close()
  //    if(!con.isClosed) con.close()
    }
    this
  }

  def execute(sql:String,addParam:(PreparedStatement)=>Unit):Array[Int]={
    val con = getConnection
    val stmt = con.prepareStatement(sql)
    try {
     stmt.clearBatch()
     addParam(stmt)
     val result = stmt.executeBatch()
     result
   }finally {
      if(!stmt.isClosed)stmt.close()
    //  if(!con.isClosed) con.close()
   }
  }
}


object  JdbcPoolHelper extends  Serializable with Logging{
  var poolHelper:Option[JdbcPoolHelper]=None

  def getJdbcPoolHelper(stateConf:StateConf):JdbcPoolHelper={
    poolHelper match {
      case Some(p)=> p
      case None=>
        poolHelper=Some(new JdbcPoolHelper(stateConf))
    }

    poolHelper.get
  }
}
package com.bitnei.util

import org.apache.spark.sql.types._

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-29 14:45
  *
  */
class JdbcUtils {


  import org.apache.spark.sql.jdbc.{JdbcDialects, JdbcType, JdbcDialect}
  import org.apache.spark.sql.jdbc.JdbcType

  val SQLServerDialect = new JdbcDialect {
    override def canHandle(url: String): Boolean = url.startsWith("jdbc:jtds:sqlserver") || url.contains("sqlserver")

    override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {
      case StringType => Some(JdbcType("VARCHAR(5000)", java.sql.Types.VARCHAR))
      case BooleanType => Some(JdbcType("BIT(1)", java.sql.Types.BIT))
      case IntegerType => Some(JdbcType("INTEGER", java.sql.Types.INTEGER))
      case LongType => Some(JdbcType("BIGINT", java.sql.Types.BIGINT))
      case DoubleType => Some(JdbcType("DOUBLE PRECISION", java.sql.Types.DOUBLE))
      case FloatType => Some(JdbcType("REAL", java.sql.Types.REAL))
      case ShortType => Some(JdbcType("INTEGER", java.sql.Types.INTEGER))
      case ByteType => Some(JdbcType("INTEGER", java.sql.Types.INTEGER))
      case BinaryType => Some(JdbcType("BINARY", java.sql.Types.BINARY))
      case TimestampType => Some(JdbcType("DATE", java.sql.Types.DATE))
      case DateType => Some(JdbcType("DATE", java.sql.Types.DATE))
      //      case DecimalType.Fixed(precision, scale) => Some(JdbcType("NUMBER(" + precision + "," + scale + ")", java.sql.Types.NUMERIC))
      case t: DecimalType => Some(JdbcType(s"DECIMAL(${t.precision},${t.scale})", java.sql.Types.DECIMAL))
      case _ => throw new IllegalArgumentException(s"Don't know how to save ${dt.json} to JDBC")
    }
  }

  JdbcDialects.registerDialect(SQLServerDialect)


}
package com.bitnei.report

import java.util.Date

import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{StringParser, Utils}
import org.apache.spark.sql.{DataFrame, Dataset, SQLContext, SparkSession}

/**
  * Created by wangbaosheng on 2017/4/6.
  */
trait Job extends  Logging{
  type R

  def compute(): Unit = registBlock(doCompute)(write)

  def doCompute[Product <: R](): Dataset[R]

  def write[Product <: R](result: Dataset[R]): Unit

  def registerIfNeed(): Unit={}

  def unRegister(): Unit={}

  def registBlock[Product <: R](c: () => Dataset[R])(w: (Dataset[R]) => Unit): Unit = {
    try {
      registerIfNeed()
      w(c())
    } finally {
      unRegister()
    }
  }

  def write[Product <: R](sQLContext: SQLContext, result: Dataset[R], format: String, compression: Option[String], partitionColumn: Option[String], partitionNum: Option[Int], outputPath: String) {
    import sQLContext.implicits._
    try {
      if (format == "text" || format == "csv") {
        val formated = if (format == "csv") {
          result.map(v => StringParser.toCsv(v.toString))
        } else {
          result.map(_.toString)
        }

        val rePartitioned = if (partitionNum.nonEmpty) formated.repartition(partitionNum.get) else formated
        val w = if (compression.nonEmpty) rePartitioned.write.format("text").option("compression", compression.get) else rePartitioned.write.format("text")
        w.save(outputPath)
      }
      else {
        Some({
          partitionNum.map(result.repartition).getOrElse(result).write.format(format)
        })
          .map(w => compression.map(w.option("compression", _)).getOrElse(w))
          .map(w => {
            if (format == "parquet" && partitionColumn.nonEmpty) w.partitionBy(partitionColumn.get) else w
          }).get.save(outputPath)
      }

      logInfo(s"$outputPath ok")
    }catch {
      case e:Throwable=>throw e
    }
  }
}


package com.bitnei.report

import com.bitnei.report.common.log.Logging

import scala.collection.mutable.ArrayBuffer

/**
  * Created by wangbaosheng on 2017/4/19.
  */
class JobContainer extends  Logging {
  private val jobContainer: ArrayBuffer[Job] = new ArrayBuffer[Job]()

  def add(job: Job): Unit = {
    jobContainer.append(job)
  }


  def run(): Unit = {
    jobContainer.zipWithIndex.foreach({ case (job, i) =>
      logInfo(s"start $i th job")
      try {
        job.compute()
      }catch {
        case e:Throwable=>
          logError(s"the $i th job throw an  exception",e)
      }
    })
  }
}

package com.bitnei.report.common

import java.io.ByteArrayInputStream

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper

/**
  * Created by wangbaosheng on 2017/4/24.
  */
trait jobLog {
  def log(jobId:String,jobScript:String,jobName:String,user:String,reason:String):Unit
}

class DbJobLog(stateConf: StateConf) extends jobLog {
  override def log(jobId: String, jobScript:String,jobName: String, user:String,reason: String):Unit = {
    val table = stateConf.getString("spark.jobserver.table")

    val sql = s"INSERT INTO $table (jobid,jobscript,jobname,jobuser,reason,job_date) VALUES(?,?,?,?,?,?)"
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, statement => {
      statement.setString(1, jobId)
      statement.setString(2,jobScript)
      statement.setString(3, jobName)
      statement.setString(4,user)

      val b=new ByteArrayInputStream(reason.getBytes("utf-8"))
      statement.setBlob(5,b)

      statement.setDate(6,new java.sql.Date(new java.util.Date().getTime))

      statement.addBatch()
    })
  }
}
package com.bitnei.report

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{ Utils}
import org.apache.spark.sql.SparkSession

/**
  * Created by wangbaosheng on 2017/4/19.
  */
class JobRunner(args: Array[String]) extends Logging {
  lazy val sparkSession:SparkSession =SparkSession.builder().getOrCreate()
  lazy val stateConf:StateConf = new StateConf

  args.foreach(arg => {
    if (arg.split('=').length != 2) {
      logWarning(f"StateCompute属性格式错误,$arg，正确的属性格式为:key=value")
    } else {
      val keyValue = arg.split('=')
      stateConf.set(keyValue(0), keyValue(1))
      logInfo(arg)
    }
  })

  def run(newJob: (StateConf, SparkSession, String, Array[String]) => Job): Unit = {
    val jobContainer = new JobContainer()

    val dateArray = stateConf.getString("report.date").split(',')

    dateArray.foreach(date => {
      val startDate = date.split('-')(0).trim
      val endDate = date.split('-')(1).trim
      val path = Utils.buildPath(stateConf.getString("input.directory"),
        Utils.parsetDate(startDate, "yyyyMMdd").get,
        Utils.parsetDate(endDate, "yyyyMMdd").get)

      logInfo(s"start date $startDate,$endDate")
      path.foreach(p => s"input path:${logInfo(p)}")

      jobContainer.add(newJob(stateConf, sparkSession, endDate, path))
    })

    jobContainer.run()
  }

  def run(newJob:Job): Unit ={
    newJob.compute()
  }

  def run(newJob: (StateConf, SparkSession) => Job): Unit ={
    newJob(stateConf,sparkSession).compute()
  }
}
package com.bitnei.samples

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.hbase.util.Bytes
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.{SaveMode, SparkSession}
import org.apache.spark.storage.StorageLevel

/**
  *
  * @author zhangyongtian
  * @define bitnei -job模板
  *
  * create 2017-11-21 14:45
  *
  */
object JobTemplate extends Serializable with Logging {


  def main(args: Array[String]): Unit = {

    // TODO: 日志级别设置
    Logger.getLogger("org").setLevel(Level.ERROR)

    // TODO: 参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)

    // TODO: 运行环境 （研发、测试、生产）
    var env = stateConf.getOption("env").getOrElse("local")

    // TODO: 验证输入参数
    logInfo("验证输入参数....")

    var outputTargets = stateConf.getOption("output").get

    if (env.equals("local")) {
      outputTargets = "console"
    }

    //时间参数
//    val yearMonth = stateConf.getOption("input.month")
    //      .get
    //
    //    if (yearMonth.length != 6) {
    //      throw new Exception("input.month error")
    //    }
    //
    //    val year = yearMonth.substring(0, 4)
    //
    //    val month = yearMonth.substring(4)


    // TODO: 加载上下文
    logInfo("加载上下文")
    //////////////////////////test//////////////////////////
    val sparkSession = SparkHelper.getSparkSession(sparkMaster = Option("local[*]"))
    import sparkSession.implicits._
    ////////////////////////////////////////////////////////

    //val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)

    val sparkConf = sparkSession.conf
    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
    //    sparkConf.set("spark.kryo.registrator", classOf[DayReport2].getName)

    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    sparkConf.set("spark.kryo.registrationRequired", "true")


    // TODO: 将数据注册成表
    logInfo("将数据注册成表")
    if (env.equals("local")) {
      MockDataProvider.realInfo(sparkSession)
      MockDataProvider.dayReport(sparkSession)

    }

    if (env.equals("dev")) {
      //研发环境
      sparkSession
        .read
        .format("parquet")
        .load(s"/tmp/zyt/data/realinfo/20171102").createOrReplaceTempView("realinfo")

    }

    if (env.equals("prd")) {
      //生产环境
      sparkSession
        .read
        .format("parquet")
        .load(s"/spark/vehicle/data/realinfo").createOrReplaceTempView("realinfo")
    }

    /////////////////////////////////业务逻辑//////////////////////////////////////////////


    ///////////////////////////////////////////////////////////////////////////////

    //    if (outputTargets.split(",").length > 1)
    //      result.persist(StorageLevel.MEMORY_ONLY_SER)

    ////////////////////////////////删除临时表#############################################
    //    sparkSession.catalog.dropTempView("realinfo")
    //    sparkSession.catalog.dropTempView("dayreport")


    // TODO: 输出
    if (env.equals("local")) {
      //        result.show(false)
    }


    if (outputTargets.contains("hdfs")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")


      //研发环境
      //var hdfsPath = s"/tmp/zyt/${this.getClass.getSimpleName}/${yearMonth}"

      //生产环境
      if (env.equals("prd")) {
        //hdfsPath = s"${stateConf.getString("output.hdfs.path")}/${year}/${month}"
      }

      ///spark/vehicle/result/mileagecheck
      //      result.mapPartitions(monthCoords => {
      //        toJson(monthCoords.toArray).toIterator
      //      }).toDF().write.format("text").mode(SaveMode.Overwrite).save(hdfsPath)

      logInfo("输出到HDFS end ....")

    }


    if (outputTargets.contains("hbase")) {
      //TODO: 输出到HBase
      logInfo("输出到HBase start....")

      //研发环境 	yf-nn2:2181,yf-dn1:2181,yf-dn2:2181
      var quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.6.103,192.168.6.104,192.168.6.105")
      var zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")

      if (env.equals("prd")) {
        //生产环境
         quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89")
         zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")
      }

      //      val htableName = ""

      //      result.coalesce(80).foreachPartition(monthCoords => {
      //        HbaseHelper.bulkPut(quorum, zkport, htableName, (table) => {
      //          val mapper = new ObjectMapper()
      //          mapper.registerModule(DefaultScalaModule)
      //          monthCoords.foreach(monthCoord => {
      //            // 0000d218-44aa-4e15-be39-8f66c602218f_201710
      //            val rowKey = s"${monthCoord.vid}_${monthCoord.date}"
      //            //          table.delete(new Delete(Bytes.toBytes(rowKey)))
      //            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "coord", toJson(mapper, monthCoord)))
      //          })
      //        })
      //      })
      logInfo("输出到HBase end ....")
    }


    logInfo("任务完成...")


    sparkSession.stop()

  }


}
package com.bitnei.tools.util

import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-11-23 14:21
  *
  */
trait JsonBase[T] {

  def toJson(mapper: ObjectMapper, obj: T): String = {
    val jsonValue = mapper.writeValueAsString(obj)
    jsonValue
  }

  def toJson(objs: Array[T]): Array[String] = {
    val mapper = new ObjectMapper()
    mapper.registerModule(DefaultScalaModule)
    objs.map(mapper.writeValueAsString(_))
  }
}
package com.bitnei.report.noticecode

import org.json4s.native.Serialization.write
import org.json4s._
import org.json4s.native.Serialization
import org.json4s.native.Serialization.{read, write}
/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-28 9:54
  *
  */
object JsonTest {


  def main(args: Array[String]): Unit = {

    implicit val formats = Serialization.formats(NoTypeHints)

    val ser = write(Array[Person](Person("Mary")))
    println(ser)
  }

  case class Person(str: String)

}
package samples

import org.junit._
import Assert._

@Test
class AppTest {

    @Test
    def testOK() = assertTrue(true)

//    @Test
//    def testKO() = assertTrue(false)

}


package com.bitnei.report.noticecode

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-29 18:21
  *
  */
class KafkaTest {

}
package com.bitnei.report.util

import java.util.Properties

import kafka.producer.ProducerConfig
import kafka.producer.Producer
import kafka.producer.KeyedMessage
import kafka.consumer.{Consumer, ConsumerConfig, ConsumerConnector}
import kafka.message.MessageAndMetadata
import org.apache.spark.HashPartitioner

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define kafka工具类
  *
  * create 2018-01-03 14:13
  *
  */
object KafkaUtils {


  def getProducer(props: Properties): Producer[String, String] = {

    //    props.put("metadata.broker.list", brokers)
    //    props.put("serializer.class", "kafka.serializer.StringEncoder")
    //    props.put("partitioner.class", classOf[HashPartitioner].getName)
    //    props.put("producer.type", "sync")
    //    props.put("batch.num.messages", "1")
    //    props.put("queue.buffering.max.messages", "1000000")
    //    props.put("queue.enqueue.timeout.ms", "20000000")

    //    props.put("bootstrap", brokers)
    //    props.put("client.id", "ScalaProducerExample")
    //    props.put("key.serializer", org.apache.kafka.common.serialization.StringSerializer)
    //    props.put("value.serializer", org.apache.kafka.common.serialization.StringDeserializer)
    //
    //    props.put("bootstrap.servers", "192.168.1.81:6667")
    //    props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
    //    props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")
    //    props.put("metadata.broker.list", "192.168.1.84:6667")
    //    props.put("group.id", "something")


    val config = new ProducerConfig(props)

    new Producer[String, String](config)

  }


  def producerSendMessage(producer: Producer[String, String], topic: String, key: String, value: String): Unit = {

    //    producer.send(new KeyedMessage[String, String](topic, "1", "test 1"))
    producer.send(new KeyedMessage[String, String](topic, key, value))

  }


  def producerClose(producer: Producer[String, String]): Unit = {
    producer.close()
  }

  /////////////////////////////////////////////////////////////////////////////////////////

  def getConsumer(brokers: String, props: Properties): ConsumerConnector = {

    //    val props = new Properties()
    //    props.put("zookeeper.connect", "192.168.1.151:2181,192.168.1.152:2181,192.168.1.153:2181")
    //    props.put("group.id", "")
    //    props.put("client.id", "test")
    //    props.put("consumer.id", "")
    //    props.put("auto.offset.reset", "smallest")
    //    props.put("auto.commit.enable", "true")
    //    props.put("auto.commit.interval.ms", "100")


    //    props.put("bootstrap", brokers)
    //    props.put("client.id", "ScalaProducerExample")
    //    props.put("key.serializer", org.apache.kafka.common.serialization.StringSerializer)
    //    props.put("value.serializer", org.apache.kafka.common.serialization.StringDeserializer)

    val consumerConfig = new ConsumerConfig(props)
    Consumer.create(consumerConfig)

  }


  def consumerMessage(consumerConn: ConsumerConnector, topic: String): ArrayBuffer[(String, String)] = {

    val msgArr = new ArrayBuffer[(String, String)]();

    val topicCountMap = Map(topic -> 1)

    val consumerMap = consumerConn.createMessageStreams(topicCountMap)

    val streams = consumerMap.get(topic).get

    for (stream <- streams) {
      val it = stream.iterator()
      while (it.hasNext()) {
        val messageAndMetadata = it.next()
        //        val message = s"Topic:${messageAndMetadata.topic}, GroupID:$groupid, Consumer ID:$consumerid, PartitionID:${messageAndMetadata.partition}, " +
        //          s"Offset:${messageAndMetadata.offset}, Message Key:${new String(messageAndMetadata.key())}, Message Payload: ${new String(messageAndMetadata.message())}"

        val key = new String(messageAndMetadata.key());

        val value = new String(messageAndMetadata.message());

        val msg = (key, value)

        msgArr.append(msg)
      }
    }

    msgArr
  }

  def consumerClose(consumerConn: ConsumerConnector): Unit = {
    consumerConn.commitOffsets(true)
  }

}


import kafka.producer.Partitioner
import scala.math._
import kafka.utils.VerifiableProperties

class HashPartitioner extends Partitioner {
  def this(verifiableProperties: VerifiableProperties) {
    this
  }

  override def partition(key: Any, numPartitions: Int): Int = {

    if (key.isInstanceOf[Int]) {
      abs(key.toString().toInt) % numPartitions
    }

    key.hashCode() % numPartitions
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import java.util.HashMap

import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}

import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka._

/**
 * Consumes messages from one or more topics in Kafka and does wordcount.
 * Usage: KafkaWordCount <zkQuorum> <group> <topics> <numThreads>
 *   <zkQuorum> is a list of one or more zookeeper servers that make quorum
 *   <group> is the name of kafka consumer group
 *   <topics> is a list of one or more kafka topics to consume from
 *   <numThreads> is the number of threads the kafka consumer should use
 *
 * Example:
 *    `$ bin/run-example \
 *      org.apache.spark.examples.streaming.KafkaWordCount zoo01,zoo02,zoo03 \
 *      my-consumer-group topic1,topic2 1`
 */
object KafkaWordCount {
  def main(args: Array[String]) {
    if (args.length < 4) {
      System.err.println("Usage: KafkaWordCount <zkQuorum> <group> <topics> <numThreads>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    val Array(zkQuorum, group, topics, numThreads) = args
    val sparkConf = new SparkConf().setAppName("KafkaWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(2))
    ssc.checkpoint("checkpoint")

    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap
    val lines = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap).map(_._2)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1L))
      .reduceByKeyAndWindow(_ + _, _ - _, Minutes(10), Seconds(2), 2)
    wordCounts.print()

    ssc.start()
    ssc.awaitTermination()
  }
}

// Produces some random words between 1 and 100.
object KafkaWordCountProducer {

  def main(args: Array[String]) {
    if (args.length < 4) {
      System.err.println("Usage: KafkaWordCountProducer <metadataBrokerList> <topic> " +
        "<messagesPerSec> <wordsPerMessage>")
      System.exit(1)
    }

    val Array(brokers, topic, messagesPerSec, wordsPerMessage) = args

    // Zookeeper connection properties
    val props = new HashMap[String, Object]()
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers)
    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
      "org.apache.kafka.common.serialization.StringSerializer")
    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
      "org.apache.kafka.common.serialization.StringSerializer")

    val producer = new KafkaProducer[String, String](props)

    // Send some messages
    while(true) {
      (1 to messagesPerSec.toInt).foreach { messageNum =>
        val str = (1 to wordsPerMessage.toInt).map(x => scala.util.Random.nextInt(10).toString)
          .mkString(" ")

        val message = new ProducerRecord[String, String](topic, null, str)
        producer.send(message)
      }

      Thread.sleep(1000)
    }
  }

}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.stat.KernelDensity
import org.apache.spark.rdd.RDD
// $example off$

object KernelDensityEstimationExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("KernelDensityEstimationExample")
    val sc = new SparkContext(conf)

    // $example on$
    // an RDD of sample data
    val data: RDD[Double] = sc.parallelize(Seq(1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 9))

    // Construct the density estimator with the sample data and a standard deviation
    // for the Gaussian kernels
    val kd = new KernelDensity()
      .setSample(data)
      .setBandwidth(3.0)

    // Find density estimates for the given values
    val densities = kd.estimate(Array(-1.0, 2.0, 5.0))
    // $example off$

    densities.foreach(println)

    sc.stop()
  }
}
// scalastyle:on println

//package com.bitnei.report.common.utils
//import org.apache.spark.sql.{DataFrame, Row, SQLContext, SaveMode}
//import org.apache.spark.sql.sources._
//import org.apache.spark.sql.types._
//
//object SchemaParser{
//  def toSparkSqlSchema(schema:Array[String]):StructType= {
//    StructType(parseSchema(schema).map(_._2))
//  }
//
//  def parseSchema(schema: Array[String]): Array[(String, StructField, Option[String])] = {
//    schema.map(field => {
//      val fieldMap = StringParser.toMap(field)
//      val name = fieldMap("name").trim
//      val fieldType = fieldMap("type").trim
//      val nullable = fieldMap("nullable").trim
//      //如果没有指定别名，那么输出列明就是原始列名
//      val alias = fieldMap.getOrElse("alias", name).trim
//
//      val default = fieldMap.get("detault")
//
//      (name, StructField(alias, typeTable(fieldType), nullable == "true"), default)
//    })
//  }
//
//  val typeTable: Map[String, DataType] = Map(
//    "StringType" -> StringType,
//    "ByteType" -> ByteType,
//    "ShortType" -> ShortType,
//    "IntegerType" -> IntegerType,
//    "LongType" -> LongType,
//    "FloatType" -> FloatType,
//    "DoubleType" -> DoubleType,
//    "BooleanType" -> BooleanType
//  )
//}
///**
//  * Created by wangbaosheng on 2017/9/22.
//  */
//class KeyValueRelation(path:String, userSchema: StructType)(@transient val sqlContext: SQLContext)
//  extends BaseRelation
//    with TableScan
//    with PrunedScan
//    with Serializable {
//  //定义schema
//  override def schema = {
//    userSchema
//  }
//
//  override def buildScan() = {
//    sqlContext.sparkContext.textFile(path)
//      .map(line => {
//        val fields = StringParser.toMap(line)
//        val convertedFields = schema.fields.map(fieldSchema => {
//          if (fields.contains(fieldSchema.name)) {
//            val value = fields(fieldSchema.name)
//            val x=castValue(fieldSchema.dataType, value)
//
//            x
//          } else {
//            None
//          }
//        })
//        Row.fromSeq(convertedFields)
//      })
//  }
//
//    def buildScan(requiredColumns: Array[String]) = {
//    sqlContext.sparkContext.textFile(path)
//      .map(line => {
//        val fields = StringParser.toMap(line)
//        val convertedFields = schema.fields
//          .filter(field => requiredColumns.contains(field.name))
//          .map(fieldSchema => {
//            if (fields.contains(fieldSchema.name)) {
//              val value = fields(fieldSchema.name)
//              val x = castValue(fieldSchema.dataType, value)
//              println(x)
//              x
//            } else {
//              None
//            }
//          })
//        Row.fromSeq(convertedFields)
//      })
//  }
//
//
//  def castValue(dataType: DataType,value:String):Option[Any]= {
//    try {
//      Some(if (dataType == StringType) {
//        value
//      } else if (dataType == IntegerType) {
//        value.toInt
//      } else if (dataType == LongType) {
//        value.toLong
//      } else if (dataType == FloatType) {
//        value.toFloat
//      } else if (dataType == DoubleType) {
//        value.toDouble
//      }
//      )
//    } catch {
//      case e: Exception =>
//        None
//    }
//  }
//}
//
//
//
///**
//  * SchemaRelationProvider ,sqlContext.read.schema(schema).load
//  **/
//class KeyValueDataSource
//  extends RelationProvider
//    with SchemaRelationProvider
//    with DataSourceRegister
//    with CreatableRelationProvider {
//  override def shortName() = "report"
//
//  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String]) = {
//    createRelation(sqlContext, parameters, null)
//  }
//
//  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String], schema: StructType) = {
//    val path = parameters("path")
//    new KeyValueRelation(path, schema)(sqlContext)
//  }
//
//  override def createRelation(sqlContext: SQLContext, mode: SaveMode, parameters: Map[String, String], data: DataFrame) = {
//    val rdd = data.rdd.map(line => {
//      line.schema.fields.foldLeft("")((acc, v) => {
//        if (acc.isEmpty) line.getAs(v.name) else s"$acc,${line.getAs(v.name)}"
//      })
//    })
//
//    rdd.saveAsTextFile(parameters("path"))
//    createRelation(sqlContext, parameters, data.schema)
//  }
//}
//
//
//object  Test {
//  def main(args: Array[String]): Unit = {
//    val sparkSession = SparkHelper.getSparkSession(sparkMaster = Some("local"))
//    val schema = SchemaParser.toSparkSqlSchema(Array("name:VID,type:StringType,nullable:false,alias:VID,default: ",
//      "name:VIN,type:StringType,nullable:true,alias:VIN,default: ",
//      "name:TIME,type:StringType,nullable:false,alias:TIME,default: ")
//    )
//
//    val dataset = sparkSession
//      .sqlContext
//      .read
//      .format("com.bitnei.report.common.utils.KeyValueDataSource")
//      .schema(schema)
//      .load("file:///C:\\D\\dayreport\\KeyValue.txt")
//
//    dataset.select("VID").show()
//
//    dataset.write
//     // .format("com.bitnei.report.common.utils.KeyValueDataSource")
//      .save("file:///C:\\D\\dayreport\\hehe1.txt")
//    dataset.printSchema()
//  }
//}
//
//
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.Vectors
// $example off$

object KMeansExample {

  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName("KMeansExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load and parse the data
    val data = sc.textFile("data/mllib/kmeans_data.txt")
    val parsedData = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble))).cache()

    // Cluster the data into two classes using KMeans
    val numClusters = 2
    val numIterations = 20
    val clusters = KMeans.train(parsedData, numClusters, numIterations)

    // Evaluate clustering by computing Within Set Sum of Squared Errors
    val WSSSE = clusters.computeCost(parsedData)
    println("Within Set Sum of Squared Errors = " + WSSSE)

    // Save and load model
    clusters.save(sc, "target/org/apache/spark/KMeansExample/KMeansModel")
    val sameModel = KMeansModel.load(sc, "target/org/apache/spark/KMeansExample/KMeansModel")
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.clustering.{DistributedLDAModel, LDA}
import org.apache.spark.mllib.linalg.Vectors
// $example off$

object LatentDirichletAllocationExample {

  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName("LatentDirichletAllocationExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load and parse the data
    val data = sc.textFile("data/mllib/sample_lda_data.txt")
    val parsedData = data.map(s => Vectors.dense(s.trim.split(' ').map(_.toDouble)))
    // Index documents with unique IDs
    val corpus = parsedData.zipWithIndex.map(_.swap).cache()

    // Cluster the documents into three topics using LDA
    val ldaModel = new LDA().setK(3).run(corpus)

    // Output topics. Each is a distribution over words (matching word count vectors)
    println("Learned topics (as distributions over vocab of " + ldaModel.vocabSize + " words):")
    val topics = ldaModel.topicsMatrix
    for (topic <- Range(0, 3)) {
      print("Topic " + topic + ":")
      for (word <- Range(0, ldaModel.vocabSize)) { print(" " + topics(word, topic)); }
      println()
    }

    // Save and load model.
    ldaModel.save(sc, "target/org/apache/spark/LatentDirichletAllocationExample/LDAModel")
    val sameModel = DistributedLDAModel.load(sc,
      "target/org/apache/spark/LatentDirichletAllocationExample/LDAModel")
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.classification.LogisticRegressionModel
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.optimization.{LBFGS, LogisticGradient, SquaredL2Updater}
import org.apache.spark.mllib.util.MLUtils
// $example off$

object LBFGSExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("LBFGSExample")
    val sc = new SparkContext(conf)

    // $example on$
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
    val numFeatures = data.take(1)(0).features.size

    // Split data into training (60%) and test (40%).
    val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)

    // Append 1 into the training data as intercept.
    val training = splits(0).map(x => (x.label, MLUtils.appendBias(x.features))).cache()

    val test = splits(1)

    // Run training algorithm to build the model
    val numCorrections = 10
    val convergenceTol = 1e-4
    val maxNumIterations = 20
    val regParam = 0.1
    val initialWeightsWithIntercept = Vectors.dense(new Array[Double](numFeatures + 1))

    val (weightsWithIntercept, loss) = LBFGS.runLBFGS(
      training,
      new LogisticGradient(),
      new SquaredL2Updater(),
      numCorrections,
      convergenceTol,
      maxNumIterations,
      regParam,
      initialWeightsWithIntercept)

    val model = new LogisticRegressionModel(
      Vectors.dense(weightsWithIntercept.toArray.slice(0, weightsWithIntercept.size - 1)),
      weightsWithIntercept(weightsWithIntercept.size - 1))

    // Clear the default threshold.
    model.clearThreshold()

    // Compute raw scores on the test set.
    val scoreAndLabels = test.map { point =>
      val score = model.predict(point.features)
      (score, point.label)
    }

    // Get evaluation metrics.
    val metrics = new BinaryClassificationMetrics(scoreAndLabels)
    val auROC = metrics.areaUnderROC()

    println("Loss of each step in training process")
    loss.foreach(println)
    println("Area under ROC = " + auROC)
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.log4j.{Level, Logger}
import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, RegexTokenizer, StopWordsRemover}
import org.apache.spark.ml.linalg.{Vector => MLVector}
import org.apache.spark.mllib.clustering.{DistributedLDAModel, EMLDAOptimizer, LDA, OnlineLDAOptimizer}
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.{Row, SparkSession}

/**
 * An example Latent Dirichlet Allocation (LDA) app. Run with
 * {{{
 * ./bin/run-example mllib.LDAExample [options] <input>
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object LDAExample {

  private case class Params(
      input: Seq[String] = Seq.empty,
      k: Int = 20,
      maxIterations: Int = 10,
      docConcentration: Double = -1,
      topicConcentration: Double = -1,
      vocabSize: Int = 10000,
      stopwordFile: String = "",
      algorithm: String = "em",
      checkpointDir: Option[String] = None,
      checkpointInterval: Int = 10) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("LDAExample") {
      head("LDAExample: an example LDA app for plain text data.")
      opt[Int]("k")
        .text(s"number of topics. default: ${defaultParams.k}")
        .action((x, c) => c.copy(k = x))
      opt[Int]("maxIterations")
        .text(s"number of iterations of learning. default: ${defaultParams.maxIterations}")
        .action((x, c) => c.copy(maxIterations = x))
      opt[Double]("docConcentration")
        .text(s"amount of topic smoothing to use (> 1.0) (-1=auto)." +
        s"  default: ${defaultParams.docConcentration}")
        .action((x, c) => c.copy(docConcentration = x))
      opt[Double]("topicConcentration")
        .text(s"amount of term (word) smoothing to use (> 1.0) (-1=auto)." +
        s"  default: ${defaultParams.topicConcentration}")
        .action((x, c) => c.copy(topicConcentration = x))
      opt[Int]("vocabSize")
        .text(s"number of distinct word types to use, chosen by frequency. (-1=all)" +
          s"  default: ${defaultParams.vocabSize}")
        .action((x, c) => c.copy(vocabSize = x))
      opt[String]("stopwordFile")
        .text(s"filepath for a list of stopwords. Note: This must fit on a single machine." +
        s"  default: ${defaultParams.stopwordFile}")
        .action((x, c) => c.copy(stopwordFile = x))
      opt[String]("algorithm")
        .text(s"inference algorithm to use. em and online are supported." +
        s" default: ${defaultParams.algorithm}")
        .action((x, c) => c.copy(algorithm = x))
      opt[String]("checkpointDir")
        .text(s"Directory for checkpointing intermediate results." +
        s"  Checkpointing helps with recovery and eliminates temporary shuffle files on disk." +
        s"  default: ${defaultParams.checkpointDir}")
        .action((x, c) => c.copy(checkpointDir = Some(x)))
      opt[Int]("checkpointInterval")
        .text(s"Iterations between each checkpoint.  Only used if checkpointDir is set." +
        s" default: ${defaultParams.checkpointInterval}")
        .action((x, c) => c.copy(checkpointInterval = x))
      arg[String]("<input>...")
        .text("input paths (directories) to plain text corpora." +
        "  Each text file line should hold 1 document.")
        .unbounded()
        .required()
        .action((x, c) => c.copy(input = c.input :+ x))
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  private def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"LDAExample with $params")
    val sc = new SparkContext(conf)

    Logger.getRootLogger.setLevel(Level.WARN)

    // Load documents, and prepare them for LDA.
    val preprocessStart = System.nanoTime()
    val (corpus, vocabArray, actualNumTokens) =
      preprocess(sc, params.input, params.vocabSize, params.stopwordFile)
    corpus.cache()
    val actualCorpusSize = corpus.count()
    val actualVocabSize = vocabArray.length
    val preprocessElapsed = (System.nanoTime() - preprocessStart) / 1e9

    println()
    println(s"Corpus summary:")
    println(s"\t Training set size: $actualCorpusSize documents")
    println(s"\t Vocabulary size: $actualVocabSize terms")
    println(s"\t Training set size: $actualNumTokens tokens")
    println(s"\t Preprocessing time: $preprocessElapsed sec")
    println()

    // Run LDA.
    val lda = new LDA()

    val optimizer = params.algorithm.toLowerCase match {
      case "em" => new EMLDAOptimizer
      // add (1.0 / actualCorpusSize) to MiniBatchFraction be more robust on tiny datasets.
      case "online" => new OnlineLDAOptimizer().setMiniBatchFraction(0.05 + 1.0 / actualCorpusSize)
      case _ => throw new IllegalArgumentException(
        s"Only em, online are supported but got ${params.algorithm}.")
    }

    lda.setOptimizer(optimizer)
      .setK(params.k)
      .setMaxIterations(params.maxIterations)
      .setDocConcentration(params.docConcentration)
      .setTopicConcentration(params.topicConcentration)
      .setCheckpointInterval(params.checkpointInterval)
    if (params.checkpointDir.nonEmpty) {
      sc.setCheckpointDir(params.checkpointDir.get)
    }
    val startTime = System.nanoTime()
    val ldaModel = lda.run(corpus)
    val elapsed = (System.nanoTime() - startTime) / 1e9

    println(s"Finished training LDA model.  Summary:")
    println(s"\t Training time: $elapsed sec")

    if (ldaModel.isInstanceOf[DistributedLDAModel]) {
      val distLDAModel = ldaModel.asInstanceOf[DistributedLDAModel]
      val avgLogLikelihood = distLDAModel.logLikelihood / actualCorpusSize.toDouble
      println(s"\t Training data average log likelihood: $avgLogLikelihood")
      println()
    }

    // Print the topics, showing the top-weighted terms for each topic.
    val topicIndices = ldaModel.describeTopics(maxTermsPerTopic = 10)
    val topics = topicIndices.map { case (terms, termWeights) =>
      terms.zip(termWeights).map { case (term, weight) => (vocabArray(term.toInt), weight) }
    }
    println(s"${params.k} topics:")
    topics.zipWithIndex.foreach { case (topic, i) =>
      println(s"TOPIC $i")
      topic.foreach { case (term, weight) =>
        println(s"$term\t$weight")
      }
      println()
    }
    sc.stop()
  }

  /**
   * Load documents, tokenize them, create vocabulary, and prepare documents as term count vectors.
   * @return (corpus, vocabulary as array, total token count in corpus)
   */
  private def preprocess(
      sc: SparkContext,
      paths: Seq[String],
      vocabSize: Int,
      stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {

    val spark = SparkSession
      .builder
      .sparkContext(sc)
      .getOrCreate()
    import spark.implicits._

    // Get dataset of document texts
    // One document per line in each text file. If the input consists of many small files,
    // this can result in a large number of small partitions, which can degrade performance.
    // In this case, consider using coalesce() to create fewer, larger partitions.
    val df = sc.textFile(paths.mkString(",")).toDF("docs")
    val customizedStopWords: Array[String] = if (stopwordFile.isEmpty) {
      Array.empty[String]
    } else {
      val stopWordText = sc.textFile(stopwordFile).collect()
      stopWordText.flatMap(_.stripMargin.split("\\s+"))
    }
    val tokenizer = new RegexTokenizer()
      .setInputCol("docs")
      .setOutputCol("rawTokens")
    val stopWordsRemover = new StopWordsRemover()
      .setInputCol("rawTokens")
      .setOutputCol("tokens")
    stopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++ customizedStopWords)
    val countVectorizer = new CountVectorizer()
      .setVocabSize(vocabSize)
      .setInputCol("tokens")
      .setOutputCol("features")

    val pipeline = new Pipeline()
      .setStages(Array(tokenizer, stopWordsRemover, countVectorizer))

    val model = pipeline.fit(df)
    val documents = model.transform(df)
      .select("features")
      .rdd
      .map { case Row(features: MLVector) => Vectors.fromML(features) }
      .zipWithIndex()
      .map(_.swap)

    (documents,
      model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary,  // vocabulary
      documents.map(_._2.numActives).sum().toLong) // total token count
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.log4j.{Level, Logger}
import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.optimization.{L1Updater, SimpleUpdater, SquaredL2Updater}
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.util.MLUtils

/**
 * An example app for linear regression. Run with
 * {{{
 * bin/run-example org.apache.spark.examples.mllib.LinearRegression
 * }}}
 * A synthetic dataset can be found at `data/mllib/sample_linear_regression_data.txt`.
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
@deprecated("Use ml.regression.LinearRegression or LBFGS", "2.0.0")
object LinearRegression {

  object RegType extends Enumeration {
    type RegType = Value
    val NONE, L1, L2 = Value
  }

  import RegType._

  case class Params(
      input: String = null,
      numIterations: Int = 100,
      stepSize: Double = 1.0,
      regType: RegType = L2,
      regParam: Double = 0.01) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("LinearRegression") {
      head("LinearRegression: an example app for linear regression.")
      opt[Int]("numIterations")
        .text("number of iterations")
        .action((x, c) => c.copy(numIterations = x))
      opt[Double]("stepSize")
        .text(s"initial step size, default: ${defaultParams.stepSize}")
        .action((x, c) => c.copy(stepSize = x))
      opt[String]("regType")
        .text(s"regularization type (${RegType.values.mkString(",")}), " +
        s"default: ${defaultParams.regType}")
        .action((x, c) => c.copy(regType = RegType.withName(x)))
      opt[Double]("regParam")
        .text(s"regularization parameter, default: ${defaultParams.regParam}")
      arg[String]("<input>")
        .required()
        .text("input paths to labeled examples in LIBSVM format")
        .action((x, c) => c.copy(input = x))
      note(
        """
          |For example, the following command runs this app on a synthetic dataset:
          |
          | bin/spark-submit --class org.apache.spark.examples.mllib.LinearRegression \
          |  examples/target/scala-*/spark-examples-*.jar \
          |  data/mllib/sample_linear_regression_data.txt
        """.stripMargin)
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"LinearRegression with $params")
    val sc = new SparkContext(conf)

    Logger.getRootLogger.setLevel(Level.WARN)

    val examples = MLUtils.loadLibSVMFile(sc, params.input).cache()

    val splits = examples.randomSplit(Array(0.8, 0.2))
    val training = splits(0).cache()
    val test = splits(1).cache()

    val numTraining = training.count()
    val numTest = test.count()
    println(s"Training: $numTraining, test: $numTest.")

    examples.unpersist(blocking = false)

    val updater = params.regType match {
      case NONE => new SimpleUpdater()
      case L1 => new L1Updater()
      case L2 => new SquaredL2Updater()
    }

    val algorithm = new LinearRegressionWithSGD()
    algorithm.optimizer
      .setNumIterations(params.numIterations)
      .setStepSize(params.stepSize)
      .setUpdater(updater)
      .setRegParam(params.regParam)

    val model = algorithm.run(training)

    val prediction = model.predict(test.map(_.features))
    val predictionAndLabel = prediction.zip(test.map(_.label))

    val loss = predictionAndLabel.map { case (p, l) =>
      val err = p - l
      err * err
    }.reduce(_ + _)
    val rmse = math.sqrt(loss / numTest)

    println(s"Test RMSE = $rmse.")

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

import scala.language.reflectiveCalls

import scopt.OptionParser

import org.apache.spark.examples.mllib.AbstractParams
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.sql.{DataFrame, SparkSession}

/**
 * An example runner for linear regression with elastic-net (mixing L1/L2) regularization.
 * Run with
 * {{{
 * bin/run-example ml.LinearRegressionExample [options]
 * }}}
 * A synthetic dataset can be found at `data/mllib/sample_linear_regression_data.txt` which can be
 * trained by
 * {{{
 * bin/run-example ml.LinearRegressionExample --regParam 0.15 --elasticNetParam 1.0 \
 *   data/mllib/sample_linear_regression_data.txt
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object LinearRegressionExample {

  case class Params(
      input: String = null,
      testInput: String = "",
      dataFormat: String = "libsvm",
      regParam: Double = 0.0,
      elasticNetParam: Double = 0.0,
      maxIter: Int = 100,
      tol: Double = 1E-6,
      fracTest: Double = 0.2) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("LinearRegressionExample") {
      head("LinearRegressionExample: an example Linear Regression with Elastic-Net app.")
      opt[Double]("regParam")
        .text(s"regularization parameter, default: ${defaultParams.regParam}")
        .action((x, c) => c.copy(regParam = x))
      opt[Double]("elasticNetParam")
        .text(s"ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. " +
        s"For alpha = 1, it is an L1 penalty. For 0 < alpha < 1, the penalty is a combination of " +
        s"L1 and L2, default: ${defaultParams.elasticNetParam}")
        .action((x, c) => c.copy(elasticNetParam = x))
      opt[Int]("maxIter")
        .text(s"maximum number of iterations, default: ${defaultParams.maxIter}")
        .action((x, c) => c.copy(maxIter = x))
      opt[Double]("tol")
        .text(s"the convergence tolerance of iterations, Smaller value will lead " +
        s"to higher accuracy with the cost of more iterations, default: ${defaultParams.tol}")
        .action((x, c) => c.copy(tol = x))
      opt[Double]("fracTest")
        .text(s"fraction of data to hold out for testing. If given option testInput, " +
        s"this option is ignored. default: ${defaultParams.fracTest}")
        .action((x, c) => c.copy(fracTest = x))
      opt[String]("testInput")
        .text(s"input path to test dataset. If given, option fracTest is ignored." +
        s" default: ${defaultParams.testInput}")
        .action((x, c) => c.copy(testInput = x))
      opt[String]("dataFormat")
        .text("data format: libsvm (default), dense (deprecated in Spark v1.1)")
        .action((x, c) => c.copy(dataFormat = x))
      arg[String]("<input>")
        .text("input path to labeled examples")
        .required()
        .action((x, c) => c.copy(input = x))
      checkConfig { params =>
        if (params.fracTest < 0 || params.fracTest >= 1) {
          failure(s"fracTest ${params.fracTest} value incorrect; should be in [0,1).")
        } else {
          success
        }
      }
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val spark = SparkSession
      .builder
      .appName(s"LinearRegressionExample with $params")
      .getOrCreate()

    println(s"LinearRegressionExample with parameters:\n$params")

    // Load training and test data and cache it.
    val (training: DataFrame, test: DataFrame) = DecisionTreeExample.loadDatasets(params.input,
      params.dataFormat, params.testInput, "regression", params.fracTest)

    val lir = new LinearRegression()
      .setFeaturesCol("features")
      .setLabelCol("label")
      .setRegParam(params.regParam)
      .setElasticNetParam(params.elasticNetParam)
      .setMaxIter(params.maxIter)
      .setTol(params.tol)

    // Train the model
    val startTime = System.nanoTime()
    val lirModel = lir.fit(training)
    val elapsedTime = (System.nanoTime() - startTime) / 1e9
    println(s"Training time: $elapsedTime seconds")

    // Print the weights and intercept for linear regression.
    println(s"Weights: ${lirModel.coefficients} Intercept: ${lirModel.intercept}")

    println("Training data results:")
    DecisionTreeExample.evaluateRegressionModel(lirModel, training, "label")
    println("Test data results:")
    DecisionTreeExample.evaluateRegressionModel(lirModel, test, "label")

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.regression.LinearRegression
// $example off$
import org.apache.spark.sql.SparkSession

object LinearRegressionWithElasticNetExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("LinearRegressionWithElasticNetExample")
      .getOrCreate()

    // $example on$
    // Load training data
    val training = spark.read.format("libsvm")
      .load("data/mllib/sample_linear_regression_data.txt")

    val lr = new LinearRegression()
      .setMaxIter(10)
      .setRegParam(0.3)
      .setElasticNetParam(0.8)

    // Fit the model
    val lrModel = lr.fit(training)

    // Print the coefficients and intercept for linear regression
    println(s"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}")

    // Summarize the model over the training set and print out some metrics
    val trainingSummary = lrModel.summary
    println(s"numIterations: ${trainingSummary.totalIterations}")
    println(s"objectiveHistory: [${trainingSummary.objectiveHistory.mkString(",")}]")
    trainingSummary.residuals.show()
    println(s"RMSE: ${trainingSummary.rootMeanSquaredError}")
    println(s"r2: ${trainingSummary.r2}")
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LinearRegressionModel
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
// $example off$

@deprecated("Use ml.regression.LinearRegression or LBFGS", "2.0.0")
object LinearRegressionWithSGDExample {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("LinearRegressionWithSGDExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load and parse the data
    val data = sc.textFile("data/mllib/ridge-data/lpsa.data")
    val parsedData = data.map { line =>
      val parts = line.split(',')
      LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
    }.cache()

    // Building the model
    val numIterations = 100
    val stepSize = 0.00000001
    val model = LinearRegressionWithSGD.train(parsedData, numIterations, stepSize)

    // Evaluate model on training examples and compute training error
    val valuesAndPreds = parsedData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val MSE = valuesAndPreds.map{ case(v, p) => math.pow((v - p), 2) }.mean()
    println("training Mean Squared Error = " + MSE)

    // Save and load model
    model.save(sc, "target/tmp/scalaLinearRegressionWithSGDModel")
    val sameModel = LinearRegressionModel.load(sc, "target/tmp/scalaLinearRegressionWithSGDModel")
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

/**
 * Uses GraphX to run PageRank on a LiveJournal social network graph. Download the dataset from
 * http://snap.stanford.edu/data/soc-LiveJournal1.html.
 */
object LiveJournalPageRank {
  def main(args: Array[String]) {
    if (args.length < 1) {
      System.err.println(
        "Usage: LiveJournalPageRank <edge_list_file>\n" +
          "    --numEPart=<num_edge_partitions>\n" +
          "        The number of partitions for the graph's edge RDD.\n" +
          "    [--tol=<tolerance>]\n" +
          "        The tolerance allowed at convergence (smaller => more accurate). Default is " +
          "0.001.\n" +
          "    [--output=<output_file>]\n" +
          "        If specified, the file to write the ranks to.\n" +
          "    [--partStrategy=RandomVertexCut | EdgePartition1D | EdgePartition2D | " +
          "CanonicalRandomVertexCut]\n" +
          "        The way edges are assigned to edge partitions. Default is RandomVertexCut.")
      System.exit(-1)
    }

    Analytics.main(args.patch(0, List("pagerank"), 0))
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import org.apache.commons.math3.linear._

/**
  * Alternating least squares matrix factorization.
  *
  * This is an example implementation for learning how to use Spark. For more conventional use,
  * please refer to org.apache.spark.ml.recommendation.ALS.
  */
object LocalALS {

  // Parameters set through command line arguments
  var M = 0 // Number of movies
  var U = 0 // Number of users
  var F = 0 // Number of features
  var ITERATIONS = 0
  val LAMBDA = 0.01 // Regularization coefficient

  def generateR(): RealMatrix = {
    val mh = randomMatrix(M, F)
    val uh = randomMatrix(U, F)
    mh.multiply(uh.transpose())
  }

  def rmse(targetR: RealMatrix, ms: Array[RealVector], us: Array[RealVector]): Double = {
    val r = new Array2DRowRealMatrix(M, U)
    for (i <- 0 until M; j <- 0 until U) {
      r.setEntry(i, j, ms(i).dotProduct(us(j)))
    }
    val diffs = r.subtract(targetR)
    var sumSqs = 0.0
    for (i <- 0 until M; j <- 0 until U) {
      val diff = diffs.getEntry(i, j)
      sumSqs += diff * diff
    }
    math.sqrt(sumSqs / (M.toDouble * U.toDouble))
  }

  def updateMovie(i: Int, m: RealVector, us: Array[RealVector], R: RealMatrix): RealVector = {
    var XtX: RealMatrix = new Array2DRowRealMatrix(F, F)
    var Xty: RealVector = new ArrayRealVector(F)
    // For each user that rated the movie
    for (j <- 0 until U) {
      val u = us(j)
      // Add u * u^t to XtX
      XtX = XtX.add(u.outerProduct(u))
      // Add u * rating to Xty
      Xty = Xty.add(u.mapMultiply(R.getEntry(i, j)))
    }
    // Add regularization coefficients to diagonal terms
    for (d <- 0 until F) {
      XtX.addToEntry(d, d, LAMBDA * U)
    }
    // Solve it with Cholesky
    new CholeskyDecomposition(XtX).getSolver.solve(Xty)
  }

  def updateUser(j: Int, u: RealVector, ms: Array[RealVector], R: RealMatrix): RealVector = {
    var XtX: RealMatrix = new Array2DRowRealMatrix(F, F)
    var Xty: RealVector = new ArrayRealVector(F)
    // For each movie that the user rated
    for (i <- 0 until M) {
      val m = ms(i)
      // Add m * m^t to XtX
      XtX = XtX.add(m.outerProduct(m))
      // Add m * rating to Xty
      Xty = Xty.add(m.mapMultiply(R.getEntry(i, j)))
    }
    // Add regularization coefficients to diagonal terms
    for (d <- 0 until F) {
      XtX.addToEntry(d, d, LAMBDA * M)
    }
    // Solve it with Cholesky
    new CholeskyDecomposition(XtX).getSolver.solve(Xty)
  }

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of ALS and is given as an example!
        |Please use org.apache.spark.ml.recommendation.ALS
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {

    args match {
      case Array(m, u, f, iters) =>
        M = m.toInt
        U = u.toInt
        F = f.toInt
        ITERATIONS = iters.toInt
      case _ =>
        System.err.println("Usage: LocalALS <M> <U> <F> <iters>")
        System.exit(1)
    }

    showWarning()

    println(s"Running with M=$M, U=$U, F=$F, iters=$ITERATIONS")

    val R = generateR()

    // Initialize m and u randomly
    var ms = Array.fill(M)(randomVector(F))
    var us = Array.fill(U)(randomVector(F))

    // Iteratively update movies then users
    for (iter <- 1 to ITERATIONS) {
      println(s"Iteration $iter:")
      ms = (0 until M).map(i => updateMovie(i, ms(i), us, R)).toArray
      us = (0 until U).map(j => updateUser(j, us(j), ms, R)).toArray
      println("RMSE = " + rmse(R, ms, us))
      println()
    }
  }

  private def randomVector(n: Int): RealVector =
    new ArrayRealVector(Array.fill(n)(math.random))

  private def randomMatrix(rows: Int, cols: Int): RealMatrix =
    new Array2DRowRealMatrix(Array.fill(rows, cols)(math.random))

}

// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import java.util.Random

import breeze.linalg.{DenseVector, Vector}

/**
 * Logistic regression based classification.
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.ml.classification.LogisticRegression.
 */
object LocalFileLR {
  val D = 10   // Number of dimensions
  val rand = new Random(42)

  case class DataPoint(x: Vector[Double], y: Double)

  def parsePoint(line: String): DataPoint = {
    val nums = line.split(' ').map(_.toDouble)
    DataPoint(new DenseVector(nums.slice(1, D + 1)), nums(0))
  }

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of Logistic Regression and is given as an example!
        |Please use org.apache.spark.ml.classification.LogisticRegression
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {

    showWarning()

    val lines = scala.io.Source.fromFile(args(0)).getLines().toArray
    val points = lines.map(parsePoint _)
    val ITERATIONS = args(1).toInt

    // Initialize w to a random value
    var w = DenseVector.fill(D) {2 * rand.nextDouble - 1}
    println("Initial w: " + w)

    for (i <- 1 to ITERATIONS) {
      println("On iteration " + i)
      var gradient = DenseVector.zeros[Double](D)
      for (p <- points) {
        val scale = (1 / (1 + math.exp(-p.y * (w.dot(p.x)))) - 1) * p.y
        gradient += p.x * scale
      }
      w -= gradient
    }

    println("Final w: " + w)
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import java.util.Random

import scala.collection.mutable.HashMap
import scala.collection.mutable.HashSet

import breeze.linalg.{squaredDistance, DenseVector, Vector}

/**
 * K-means clustering.
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.ml.clustering.KMeans.
 */
object LocalKMeans {
  val N = 1000
  val R = 1000    // Scaling factor
  val D = 10
  val K = 10
  val convergeDist = 0.001
  val rand = new Random(42)

  def generateData: Array[DenseVector[Double]] = {
    def generatePoint(i: Int): DenseVector[Double] = {
      DenseVector.fill(D) {rand.nextDouble * R}
    }
    Array.tabulate(N)(generatePoint)
  }

  def closestPoint(p: Vector[Double], centers: HashMap[Int, Vector[Double]]): Int = {
    var index = 0
    var bestIndex = 0
    var closest = Double.PositiveInfinity

    for (i <- 1 to centers.size) {
      val vCurr = centers.get(i).get
      val tempDist = squaredDistance(p, vCurr)
      if (tempDist < closest) {
        closest = tempDist
        bestIndex = i
      }
    }

    bestIndex
  }

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of KMeans Clustering and is given as an example!
        |Please use org.apache.spark.ml.clustering.KMeans
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {

    showWarning()

    val data = generateData
    var points = new HashSet[Vector[Double]]
    var kPoints = new HashMap[Int, Vector[Double]]
    var tempDist = 1.0

    while (points.size < K) {
      points.add(data(rand.nextInt(N)))
    }

    val iter = points.iterator
    for (i <- 1 to points.size) {
      kPoints.put(i, iter.next())
    }

    println("Initial centers: " + kPoints)

    while(tempDist > convergeDist) {
      var closest = data.map (p => (closestPoint(p, kPoints), (p, 1)))

      var mappings = closest.groupBy[Int] (x => x._1)

      var pointStats = mappings.map { pair =>
        pair._2.reduceLeft [(Int, (Vector[Double], Int))] {
          case ((id1, (p1, c1)), (id2, (p2, c2))) => (id1, (p1 + p2, c1 + c2))
        }
      }

      var newPoints = pointStats.map {mapping =>
        (mapping._1, mapping._2._1 * (1.0 / mapping._2._2))}

      tempDist = 0.0
      for (mapping <- newPoints) {
        tempDist += squaredDistance(kPoints.get(mapping._1).get, mapping._2)
      }

      for (newP <- newPoints) {
        kPoints.put(newP._1, newP._2)
      }
    }

    println("Final centers: " + kPoints)
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import java.util.Random

import breeze.linalg.{DenseVector, Vector}

/**
 * Logistic regression based classification.
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.ml.classification.LogisticRegression.
 */
object LocalLR {
  val N = 10000  // Number of data points
  val D = 10   // Number of dimensions
  val R = 0.7  // Scaling factor
  val ITERATIONS = 5
  val rand = new Random(42)

  case class DataPoint(x: Vector[Double], y: Double)

  def generateData: Array[DataPoint] = {
    def generatePoint(i: Int): DataPoint = {
      val y = if (i % 2 == 0) -1 else 1
      val x = DenseVector.fill(D) {rand.nextGaussian + y * R}
      DataPoint(x, y)
    }
    Array.tabulate(N)(generatePoint)
  }

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of Logistic Regression and is given as an example!
        |Please use org.apache.spark.ml.classification.LogisticRegression
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {

    showWarning()

    val data = generateData
    // Initialize w to a random value
    var w = DenseVector.fill(D) {2 * rand.nextDouble - 1}
    println("Initial w: " + w)

    for (i <- 1 to ITERATIONS) {
      println("On iteration " + i)
      var gradient = DenseVector.zeros[Double](D)
      for (p <- data) {
        val scale = (1 / (1 + math.exp(-p.y * (w.dot(p.x)))) - 1) * p.y
        gradient +=  p.x * scale
      }
      w -= gradient
    }

    println("Final w: " + w)
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import scala.math.random

object LocalPi {
  def main(args: Array[String]) {
    var count = 0
    for (i <- 1 to 100000) {
      val x = random * 2 - 1
      val y = random * 2 - 1
      if (x*x + y*y < 1) count += 1
    }
    println("Pi is roughly " + 4 * count / 100000.0)
  }
}
// scalastyle:on println
package com.bitnei.report.common.log

import com.bitnei.common.utils.Utils
import org.apache.log4j.{LogManager, PropertyConfigurator}
import org.slf4j.impl.StaticLoggerBinder
import org.slf4j.{Logger, LoggerFactory}

trait Logging {
  // Make the log field transient so that objects with Logging can
  // be serialized and used on another machine
  @transient private var log_ : Logger = _

  // Method to get the logger name for this object
  protected def logName:String = {
    // Ignore trailing $'s in the class names for Scala objects
    this.getClass.getName.stripSuffix("$")
  }

  // Method to get or create the logger for this object
  protected def log: Logger = {
    if (log_ == null) {
      initializeIfNecessary()
      println(logName)
      log_ = LoggerFactory.getLogger(logName)
    }
    log_
  }

  // Log methods that take only a String
  protected def logInfo(msg: => String) {
    if (log.isInfoEnabled) log.info(msg)
  }

  protected def logDebug(msg: => String) {
    if (log.isDebugEnabled) log.debug(msg)
  }

  protected def logTrace(msg: => String) {
    if (log.isTraceEnabled) log.trace(msg)
  }

  protected def logWarning(msg: => String) {
    if (log.isWarnEnabled) log.warn(msg)
  }

  protected def logError(msg: => String) {
    if (log.isErrorEnabled) log.error(msg)
  }

  // Log methods that take Throwables (Exceptions/Errors) too
  protected def logInfo(msg: => String, throwable: Throwable) {
    if (log.isInfoEnabled) log.info(msg, throwable)
  }

  protected def logDebug(msg: => String, throwable: Throwable) {
    if (log.isDebugEnabled) log.debug(msg, throwable)
  }

  protected def logTrace(msg: => String, throwable: Throwable) {
    if (log.isTraceEnabled) log.trace(msg, throwable)
  }

  protected def logWarning(msg: => String, throwable: Throwable) {
    if (log.isWarnEnabled) log.warn(msg, throwable)
  }

  protected def logError(msg: => String, throwable: Throwable) {
    if (log.isErrorEnabled) log.error(msg, throwable)
  }

  protected def isTraceEnabled(): Boolean = {
    log.isTraceEnabled
  }

  private def initializeIfNecessary() {
    if (!Logging.initialized) {
      Logging.initLock.synchronized {
        if (!Logging.initialized) {
          initializeLogging()
        }
      }
    }
  }

  private def initializeLogging() {
    // Don't use a logger in here, as this is itself occurring during initialization of a logger
    // If Log4j 1.2 is being used, but is not initialized, load a default properties file
    val binderClass = StaticLoggerBinder.getSingleton.getLoggerFactoryClassStr
    // This distinguishes the log4j 1.2 binding, currently
    // org.slf4j.impl.Log4jLoggerFactory, from the log4j 2.0 binding, currently
    // org.apache.logging.slf4j.Log4jLoggerFactory
    val usingLog4j12 = "org.slf4j.impl.Log4jLoggerFactory".equals(binderClass)
    if (usingLog4j12) {
      System.err.println("usinglog4j12")
      val log4j12Initialized = LogManager.getRootLogger.getAllAppenders.hasMoreElements
      if (!log4j12Initialized) {
        // scalastyle:off println
        if (Utils.isInInterpreter) {
          val replDefaultLogProps = "log4j.properties"
          PropertyConfigurator.configure(replDefaultLogProps)
        } else {
          val defaultLogProps = "log4j.properties"
          Option(Utils.getSparkClassLoader.getResource(defaultLogProps)) match {
            case Some(url) =>
              PropertyConfigurator.configure(url)
              System.err.println(s"Using Spark's default log4j profile: $defaultLogProps")
            case None =>
              System.err.println(s"Spark was unable to load $defaultLogProps")
          }
        }
        // scalastyle:on println
      }
    }
    Logging.initialized = true

    // Force a call into slf4j to initialize it. Avoids this happening from multiple threads
    // and triggering this: http://mailman.qos.ch/pipermail/slf4j-dev/2010-April/002956.html
    log
  }
}

private object Logging {
  @volatile private var initialized = false
  val initLock = new Object()
  try {
    // We use reflection here to handle the case where users remove the
    // slf4j-to-jul bridge order to route their logs to JUL.
    val bridgeClass = Utils.classForName("org.slf4j.bridge.SLF4JBridgeHandler")
    bridgeClass.getMethod("removeHandlersForRootLogger").invoke(null)
    val installed = bridgeClass.getMethod("isInstalled").invoke(null).asInstanceOf[Boolean]
    if (!installed) {
      bridgeClass.getMethod("install").invoke(null)
    }
  } catch {
    case e: ClassNotFoundException => // can't log anything yet so just fail silently
  }
}

package com.bitnei.report.dayreport.validate

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import com.bitnei.report.stateGenerate.Window

import scala.annotation.tailrec


case class LoginModel(vid:String,result:Option[String])
/**
  * Created by franciswang on 2016/11/3.
  */
class LoginCompute[T](stateConf:StateConf,window:Window[T],getResult:(T)=>Option[String])extends  Serializable with Logging{
  class LoginResult(var login:Int, var invalidLogin:Int,var sucessLogin:Int,var failedLogin:Int, var errorLogin:Int){
    def +(that:LoginResult):LoginResult={
      new LoginResult(
        this.login+that.login,
        this.invalidLogin+that.invalidLogin,
        this.sucessLogin+that.sucessLogin,
        this.failedLogin+that.failedLogin,
        this.errorLogin+that.errorLogin
      )
    }
    def sucessPercent:Double=if(login>0) sucessLogin.toDouble/login.toDouble else 0D
    def failedPercent:Double=if(login>0) failedLogin.toDouble/login.toDouble else 0D


    override def toString():String=s"$login,$sucessLogin,$invalidLogin,$failedLogin,$errorLogin"
  }


  val loginResult:LoginResult=compute()


  def compute():LoginResult= {

    @tailrec def tailCompute(i:Int,result:LoginResult):LoginResult ={
      if(i<0||i>=window.length) result
      else {
        result.login+=1
        val a=getResult(window(i))
        a match {
          case Some(Constant.LogginInvaid)=>result.invalidLogin+=1
          case Some(Constant.LoginSucess)=>result.sucessLogin+=1
          case Some(Constant.LogginFailed)=>result.failedLogin+=1
          case _=>result.errorLogin+=1
        }

        tailCompute(i+1,result)
      }
    }

    val result=new LoginResult(0,0,0,0,0)
    tailCompute(0,result)
  }



  def loginCount:Int=loginResult.login
  def invalidLoginCount:Int=loginResult.invalidLogin
  def validLoginCount:Int=loginResult.sucessLogin
  def succPercent:Float=if(loginResult.login>0) loginResult.sucessLogin.toFloat/loginResult.login.toFloat else 0f
  def failPercent:Float=1-succPercent


  override def toString: String =loginResult.toString()
}

object LoginCompute{
  def defaultString="0,0,0,0,0"
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

import scala.collection.mutable
import scala.language.reflectiveCalls

import scopt.OptionParser

import org.apache.spark.examples.mllib.AbstractParams
import org.apache.spark.ml.{Pipeline, PipelineStage}
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.sql.{DataFrame, SparkSession}

/**
 * An example runner for logistic regression with elastic-net (mixing L1/L2) regularization.
 * Run with
 * {{{
 * bin/run-example ml.LogisticRegressionExample [options]
 * }}}
 * A synthetic dataset can be found at `data/mllib/sample_libsvm_data.txt` which can be
 * trained by
 * {{{
 * bin/run-example ml.LogisticRegressionExample --regParam 0.3 --elasticNetParam 0.8 \
 *   data/mllib/sample_libsvm_data.txt
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object LogisticRegressionExample {

  case class Params(
      input: String = null,
      testInput: String = "",
      dataFormat: String = "libsvm",
      regParam: Double = 0.0,
      elasticNetParam: Double = 0.0,
      maxIter: Int = 100,
      fitIntercept: Boolean = true,
      tol: Double = 1E-6,
      fracTest: Double = 0.2) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("LogisticRegressionExample") {
      head("LogisticRegressionExample: an example Logistic Regression with Elastic-Net app.")
      opt[Double]("regParam")
        .text(s"regularization parameter, default: ${defaultParams.regParam}")
        .action((x, c) => c.copy(regParam = x))
      opt[Double]("elasticNetParam")
        .text(s"ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. " +
        s"For alpha = 1, it is an L1 penalty. For 0 < alpha < 1, the penalty is a combination of " +
        s"L1 and L2, default: ${defaultParams.elasticNetParam}")
        .action((x, c) => c.copy(elasticNetParam = x))
      opt[Int]("maxIter")
        .text(s"maximum number of iterations, default: ${defaultParams.maxIter}")
        .action((x, c) => c.copy(maxIter = x))
      opt[Boolean]("fitIntercept")
        .text(s"whether to fit an intercept term, default: ${defaultParams.fitIntercept}")
        .action((x, c) => c.copy(fitIntercept = x))
      opt[Double]("tol")
        .text(s"the convergence tolerance of iterations, Smaller value will lead " +
        s"to higher accuracy with the cost of more iterations, default: ${defaultParams.tol}")
        .action((x, c) => c.copy(tol = x))
      opt[Double]("fracTest")
        .text(s"fraction of data to hold out for testing. If given option testInput, " +
        s"this option is ignored. default: ${defaultParams.fracTest}")
        .action((x, c) => c.copy(fracTest = x))
      opt[String]("testInput")
        .text(s"input path to test dataset. If given, option fracTest is ignored." +
        s" default: ${defaultParams.testInput}")
        .action((x, c) => c.copy(testInput = x))
      opt[String]("dataFormat")
        .text("data format: libsvm (default), dense (deprecated in Spark v1.1)")
        .action((x, c) => c.copy(dataFormat = x))
      arg[String]("<input>")
        .text("input path to labeled examples")
        .required()
        .action((x, c) => c.copy(input = x))
      checkConfig { params =>
        if (params.fracTest < 0 || params.fracTest >= 1) {
          failure(s"fracTest ${params.fracTest} value incorrect; should be in [0,1).")
        } else {
          success
        }
      }
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val spark = SparkSession
      .builder
      .appName(s"LogisticRegressionExample with $params")
      .getOrCreate()

    println(s"LogisticRegressionExample with parameters:\n$params")

    // Load training and test data and cache it.
    val (training: DataFrame, test: DataFrame) = DecisionTreeExample.loadDatasets(params.input,
      params.dataFormat, params.testInput, "classification", params.fracTest)

    // Set up Pipeline.
    val stages = new mutable.ArrayBuffer[PipelineStage]()

    val labelIndexer = new StringIndexer()
      .setInputCol("label")
      .setOutputCol("indexedLabel")
    stages += labelIndexer

    val lor = new LogisticRegression()
      .setFeaturesCol("features")
      .setLabelCol("indexedLabel")
      .setRegParam(params.regParam)
      .setElasticNetParam(params.elasticNetParam)
      .setMaxIter(params.maxIter)
      .setTol(params.tol)
      .setFitIntercept(params.fitIntercept)

    stages += lor
    val pipeline = new Pipeline().setStages(stages.toArray)

    // Fit the Pipeline.
    val startTime = System.nanoTime()
    val pipelineModel = pipeline.fit(training)
    val elapsedTime = (System.nanoTime() - startTime) / 1e9
    println(s"Training time: $elapsedTime seconds")

    val lorModel = pipelineModel.stages.last.asInstanceOf[LogisticRegressionModel]
    // Print the weights and intercept for logistic regression.
    println(s"Weights: ${lorModel.coefficients} Intercept: ${lorModel.intercept}")

    println("Training data results:")
    DecisionTreeExample.evaluateClassificationModel(pipelineModel, training, "indexedLabel")
    println("Test data results:")
    DecisionTreeExample.evaluateClassificationModel(pipelineModel, test, "indexedLabel")

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}
// $example off$
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.max

object LogisticRegressionSummaryExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("LogisticRegressionSummaryExample")
      .getOrCreate()
    import spark.implicits._

    // Load training data
    val training = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

    val lr = new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.3)
      .setElasticNetParam(0.8)

    // Fit the model
    val lrModel = lr.fit(training)

    // $example on$
    // Extract the summary from the returned LogisticRegressionModel instance trained in the earlier
    // example
    val trainingSummary = lrModel.summary

    // Obtain the objective per iteration.
    val objectiveHistory = trainingSummary.objectiveHistory
    println("objectiveHistory:")
    objectiveHistory.foreach(loss => println(loss))

    // Obtain the metrics useful to judge performance on test data.
    // We cast the summary to a BinaryLogisticRegressionSummary since the problem is a
    // binary classification problem.
    val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]

    // Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.
    val roc = binarySummary.roc
    roc.show()
    println(s"areaUnderROC: ${binarySummary.areaUnderROC}")

    // Set the model threshold to maximize F-Measure
    val fMeasure = binarySummary.fMeasureByThreshold
    val maxFMeasure = fMeasure.select(max("F-Measure")).head().getDouble(0)
    val bestThreshold = fMeasure.where($"F-Measure" === maxFMeasure)
      .select("threshold").head().getDouble(0)
    lrModel.setThreshold(bestThreshold)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.classification.LogisticRegression
// $example off$
import org.apache.spark.sql.SparkSession

object LogisticRegressionWithElasticNetExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("LogisticRegressionWithElasticNetExample")
      .getOrCreate()

    // $example on$
    // Load training data
    val training = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

    val lr = new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.3)
      .setElasticNetParam(0.8)

    // Fit the model
    val lrModel = lr.fit(training)

    // Print the coefficients and intercept for logistic regression
    println(s"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}")

    // We can also use the multinomial family for binary classification
    val mlr = new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.3)
      .setElasticNetParam(0.8)
      .setFamily("multinomial")

    val mlrModel = mlr.fit(training)

    // Print the coefficients and intercepts for logistic regression with multinomial family
    println(s"Multinomial coefficients: ${mlrModel.coefficientMatrix}")
    println(s"Multinomial intercepts: ${mlrModel.interceptVector}")
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.classification.{LogisticRegressionModel, LogisticRegressionWithLBFGS}
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
// $example off$

object LogisticRegressionWithLBFGSExample {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("LogisticRegressionWithLBFGSExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load training data in LIBSVM format.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")

    // Split data into training (60%) and test (40%).
    val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
    val training = splits(0).cache()
    val test = splits(1)

    // Run training algorithm to build the model
    val model = new LogisticRegressionWithLBFGS()
      .setNumClasses(10)
      .run(training)

    // Compute raw scores on the test set.
    val predictionAndLabels = test.map { case LabeledPoint(label, features) =>
      val prediction = model.predict(features)
      (prediction, label)
    }

    // Get evaluation metrics.
    val metrics = new MulticlassMetrics(predictionAndLabels)
    val accuracy = metrics.accuracy
    println(s"Accuracy = $accuracy")

    // Save and load model
    model.save(sc, "target/tmp/scalaLogisticRegressionWithLBFGSModel")
    val sameModel = LogisticRegressionModel.load(sc,
      "target/tmp/scalaLogisticRegressionWithLBFGSModel")
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import org.apache.spark.{SparkConf, SparkContext}

/**
 * Executes a roll up-style query against Apache logs.
 *
 * Usage: LogQuery [logFile]
 */
object LogQuery {
  val exampleApacheLogs = List(
    """10.10.10.10 - "FRED" [18/Jan/2013:17:56:07 +1100] "GET http://images.com/2013/Generic.jpg
      | HTTP/1.1" 304 315 "http://referall.com/" "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1;
      | GTB7.4; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30; .NET CLR 3.0.04506.648; .NET CLR
      | 3.5.21022; .NET CLR 3.0.4506.2152; .NET CLR 1.0.3705; .NET CLR 1.1.4322; .NET CLR
      | 3.5.30729; Release=ARP)" "UD-1" - "image/jpeg" "whatever" 0.350 "-" - "" 265 923 934 ""
      | 62.24.11.25 images.com 1358492167 - Whatup""".stripMargin.lines.mkString,
    """10.10.10.10 - "FRED" [18/Jan/2013:18:02:37 +1100] "GET http://images.com/2013/Generic.jpg
      | HTTP/1.1" 304 306 "http:/referall.com" "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1;
      | GTB7.4; .NET CLR 2.0.50727; .NET CLR 3.0.04506.30; .NET CLR 3.0.04506.648; .NET CLR
      | 3.5.21022; .NET CLR 3.0.4506.2152; .NET CLR 1.0.3705; .NET CLR 1.1.4322; .NET CLR
      | 3.5.30729; Release=ARP)" "UD-1" - "image/jpeg" "whatever" 0.352 "-" - "" 256 977 988 ""
      | 0 73.23.2.15 images.com 1358492557 - Whatup""".stripMargin.lines.mkString
  )

  def main(args: Array[String]) {

    val sparkConf = new SparkConf().setAppName("Log Query")
    val sc = new SparkContext(sparkConf)

    val dataSet =
      if (args.length == 1) sc.textFile(args(0)) else sc.parallelize(exampleApacheLogs)
    // scalastyle:off
    val apacheLogRegex =
      """^([\d.]+) (\S+) (\S+) \[([\w\d:/]+\s[+\-]\d{4})\] "(.+?)" (\d{3}) ([\d\-]+) "([^"]+)" "([^"]+)".*""".r
    // scalastyle:on
    /** Tracks the total query count and number of aggregate bytes for a particular group. */
    class Stats(val count: Int, val numBytes: Int) extends Serializable {
      def merge(other: Stats): Stats = new Stats(count + other.count, numBytes + other.numBytes)
      override def toString: String = "bytes=%s\tn=%s".format(numBytes, count)
    }

    def extractKey(line: String): (String, String, String) = {
      apacheLogRegex.findFirstIn(line) match {
        case Some(apacheLogRegex(ip, _, user, dateTime, query, status, bytes, referer, ua)) =>
          if (user != "\"-\"") (ip, user, query)
          else (null, null, null)
        case _ => (null, null, null)
      }
    }

    def extractStats(line: String): Stats = {
      apacheLogRegex.findFirstIn(line) match {
        case Some(apacheLogRegex(ip, _, user, dateTime, query, status, bytes, referer, ua)) =>
          new Stats(1, bytes.toInt)
        case _ => new Stats(1, 0)
      }
    }

    dataSet.map(line => (extractKey(line), extractStats(line)))
      .reduceByKey((a, b) => a.merge(b))
      .collect().foreach{
        case (user, query) => println("%s\t%s".format(user, query))}

    sc.stop()
  }
}
// scalastyle:on println
package sample.hello

object Main {

  def main(args: Array[String]): Unit = {
    akka.Main.main(Array(classOf[HelloWorld].getName))
  }

}package sample.hello

import akka.actor.ActorSystem
import akka.actor.Props
import akka.actor.ActorRef
import akka.actor.Actor
import akka.actor.ActorLogging
import akka.actor.Terminated

object Main2 {

  def main(args: Array[String]): Unit = {
    val system = ActorSystem("Hello")
    val a = system.actorOf(Props[HelloWorld], "helloWorld")
    system.actorOf(Props(classOf[Terminator], a), "terminator")
  }

  class Terminator(ref: ActorRef) extends Actor with ActorLogging {
    context watch ref

    def receive = {
      case Terminated(_) =>
        log.info("{} has terminated, shutting down system", ref.path)
        context.system.terminate()
    }
  }

}package com.bitnei.util.linear

import breeze.linalg.{DenseMatrix, min}
import breeze.numerics.abs

import scala.collection.mutable.ArrayBuffer

/**
  * 矩阵工具类
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-26 15:03
  *
  */
object MatrixUtils {

  /**
    * 通过两个向量生成距离矩阵
    *
    * @param seq0
    * @param seq1
    * @return
    */
  def vectors2DistanceMatrix(seq0: Array[Int], seq1: Array[Int]): Array[Array[Int]] = {

    val res = new ArrayBuffer[Array[Int]]()

    for (i <- 0 until seq1.length) {

      val row = new ArrayBuffer[Int]()

      for (j <- 0 until seq0.length) {

        val diff = abs(seq1(i) - seq0(j))

        row.append(diff)
      }
      res.append(row.toArray)
    }
    res.toArray

  }


  /**
    * 生成损失矩阵
    *
    * @param M 距离矩阵
    * @return
    */
  def getMcMatrix(M: Array[Array[Int]]): Array[Array[Int]] = {

    val MC = new ArrayBuffer[Array[Int]]()

    //付初始值
    for (i <- 0 until M.length) {
      val col = new ArrayBuffer[Int]()
      for (j <- 0 until M(i).length) {
        col.append(0)
      }
      MC.append(col.toArray)
    }

    for (i <- 0 until M.length) {

      for (j <- 0 until M(i).length) {
        if (i == 0 && j == 0) {
          MC(i)(j) = M(0)(0)
        } else if (i == 0 && j != 0) {
          MC(i)(j) = M(i)(j) + MC(i)(j - 1)
        } else if (i != 0 && j == 0) {
          MC(i)(j) = M(i)(j) + MC(i - 1)(j)
        } else {
          val data = M(i)(j) + min(MC(i - 1)(j), MC(i)(j - 1), MC(i - 1)(j - 1))
          MC(i)(j) = data
        }
      }
    }

    MC.toArray
  }


  /**
    * 计算相似度
    *
    * @param M 距离矩阵
    * @return
    */
  def getMcMatrixValue(M: Array[Array[Int]]): Int = {

    val mc = getMcMatrix(M)

    val len = mc.length

    mc(len - 1)(mc(0).length - 1)
  }















  def main(args: Array[String]): Unit = {

    val m = vectors2DistanceMatrix(Array(1, -1, 1), Array(-1, 1, 1))

    for (i <- 0 until m.length) {
      for (j <- 0 until m(i).length) {
        print(m(i)(j) + " ")
      }
      println()
    }

    val mc = getMcMatrix(m)

    println(mc)

    for (i <- 0 until mc.length) {
      for (j <- 0 until mc(i).length) {
        print(mc(i)(j) + " ")
      }
      println()
    }


    //    val createFunctionMatrix= new DenseMatrix[Double](3, 2, Array(1.0, 4.0, 7.0, 3.0, 6.0, 9.0))


    val m1 = DenseMatrix((1.0, 2.0), (3.0, 4.0))
    val m2 = DenseMatrix((0.5, 0.5), (0.5, 0.5))


    //    println(getMcMatrix(m))

  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.MaxAbsScaler
import org.apache.spark.ml.linalg.Vectors
// $example off$
import org.apache.spark.sql.SparkSession

object MaxAbsScalerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("MaxAbsScalerExample")
      .getOrCreate()

    // $example on$
    val dataFrame = spark.createDataFrame(Seq(
      (0, Vectors.dense(1.0, 0.1, -8.0)),
      (1, Vectors.dense(2.0, 1.0, -4.0)),
      (2, Vectors.dense(4.0, 10.0, 8.0))
    )).toDF("id", "features")

    val scaler = new MaxAbsScaler()
      .setInputCol("features")
      .setOutputCol("scaledFeatures")

    // Compute summary statistics and generate MaxAbsScalerModel
    val scalerModel = scaler.fit(dataFrame)

    // rescale each feature to range [-1, 1]
    val scaledData = scalerModel.transform(dataFrame)
    scaledData.select("features", "scaledFeatures").show()
    // $example off$

    spark.stop()
  }
}
package com.bitnei.report.detail

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{Utils}
import com.bitnei.sparkhelper.SparkHelper
import org.apache.hadoop.fs.FileSystem

/**
  * Created by wangbaosheng on 2017/8/23.
  */
object MergeDetail extends Logging {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    val sparkSession = SparkHelper.getSparkSession(None)

    val startdate = Utils.parsetDate(stateConf.getOption("startDate").getOrElse("2016-01-01"), "yyyy-MM-dd").get
    val endDate = Utils.parsetDate(stateConf.getOption("endDate").getOrElse("2017-08-22"), "yyyy-MM-dd").get

    val fs = FileSystem.get(sparkSession.sparkContext.hadoopConfiguration)

    import sparkSession.implicits._

    SparkHelper.unionDataFrames(Utils.getValidPath(fs, "/spark/vehicle/result/detail/day", startdate, endDate)
      .map(p => {
        logInfo(s"begin merge $p.")
        sparkSession.read.parquet(p)
      })
    ).as[DetailModel].repartition(1).write.partitionBy("category").parquet(s"/tmp/spark/vehicle/result/detail/month/${Utils.formatDate(startdate,"yyyy/MM")}")
  }
}
package com.bitnei.report.tempjob

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.{StringParser, Utils}
import com.bitnei.sparkhelper.SparkHelper
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession

/**
  * Created by wangbaosheng on 2017/10/12.
  */
class MergeFile(@transient spark: SparkSession, stateConf: StateConf) {
  @transient private val hadoopConfiguration = spark.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)

  def saveLzo(rdd: RDD[String], outputPath: String) {
    import org.apache.hadoop.io.compress.CompressionCodec
    spark.sparkContext.getConf.set("io.compression.codecs", "com.hadoop.compression.lzo.LzopCodec")
    Thread.currentThread().getContextClassLoader.loadClass("com.hadoop.compression.lzo.LzopCodec") match {
      case lzo: Class[CompressionCodec] => rdd.saveAsTextFile(outputPath, lzo)
      case _ =>
    }
  }

  def run(): Unit = {
    stateConf.getOption("input.directory") match {
      case Some(inputDirectory) =>
        val startDate = Utils.parsetDate(stateConf.getOption("input.startDate").getOrElse("20160101"), "yyyyMMdd").get
        val endDate = Utils.parsetDate(stateConf.getOption("intput.endDate").getOrElse("20171010"), "yyyyMMdd").get

        Utils.getValidPath(fs, inputDirectory, startDate, endDate, "yyy/MM/dd")
          .foreach(p => {
            val outputPath = stateConf.getOption("output.directory").getOrElse("/tmp")
            merge(Array(p), s"$outputPath/$p")
          })
      case None =>
    }
  }

  def merge(path: Array[String], outputPath: String): Unit = {
    val validateRdds = path.map(p => {
      spark.sparkContext.textFile(p).filter(line => {
        try {
          StringParser.toMap(line)
          true
        } catch {
          case e: Exception => false
        }
      })
    })

    val resultRdd = validateRdds.reduce((a, b) => a.union(b)).repartition(10)
    if (stateConf.getOption("compression").contains("lzo"))
      saveLzo(resultRdd, outputPath)
    else resultRdd.saveAsTextFile(outputPath)
  }
}


object MergeFile {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    new MergeFile(SparkHelper.getSparkSession(sparkMaster = None), stateConf).run()
  }
}
package com.bitnei.report

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper, SqlHelper}

object  MileageCheck extends Logging {
  val stateConf = new StateConf
  val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)
  def main(args: Array[String]): Unit = {
    stateConf.add(args)
    stateConf.getOption("onlyOutputNotCompute") match {
      case Some("true") =>
        logInfo("only output but not compute")
        //只输出不计算
        onlyOutput()
      case _ =>
        //计算并输出
        computeAndOutput()
    }
  }

  def onlyOutput(): Unit ={
    def computeEveryDay(stateConf: StateConf,startDate: Date, endDate: Date): Unit = {
      val dayCoordJob = new DayCoordOutputJob(stateConf, sparkSession)
      //注册表
      dayCoordJob.registerIfNeed()
      //输出每一天的日轨迹
      foreachEveryDay(startDate, endDate, curDate => {
        val year = Utils.formatDate(curDate, "yyyy")
        val month = Utils.formatDate(curDate, "MM")
        val day = Utils.formatDate(curDate, "dd")
        logInfo(s"begin compute $year-$month-$day")
        stateConf.set("date", s"$year$month$day")

        SparkHelper.setPartitionValue(stateConf, "mileageCheckDayCoord", Array(year, month, day))
        //输出
        dayCoordJob.write(dayCoordJob.doCompute())
      })
    }

    def computeMonthMileage(startDate: Date, endDate: Date): Unit ={
      val year=Utils.formatDate(startDate,"yyyy")
      val month=Utils.formatDate(startDate,"MM")
      SparkHelper.setPartitionValue(stateConf,"mileageCheckDeadline",Array(year,month))
      new DeadlineMileageOutputJob(stateConf, sparkSession).compute()
    }

    //获取开始日期
    val startDate = Utils.parsetDate(stateConf.getString("monthDate"), "yyyyMM").get
    //获取结束日期
    val endDate = new Date(startDate.getTime)
    endDate.setMonth(startDate.getMonth + 1)
    endDate.setDate(endDate.getDate-1)

    logInfo(s"begin compute $startDate ")

    if (stateConf.getOption("daycoord.enable").getOrElse("true") == "true") {
      //计算每一天的轨迹
      computeEveryDay(stateConf, startDate, endDate)
    }

    //计算月里程
    logInfo("begin compute deadlinemileage")
    computeMonthMileage(startDate, endDate)
  }


  def computeAndOutput(): Unit ={
    def computeEveryDay(stateConf: StateConf,startDate: Date, endDate: Date): Unit ={
      val dayCoordJob = new DayCoordJob(stateConf, sparkSession)
      //注册表
      dayCoordJob.registerIfNeed()



      //计算每一天的日轨迹
      foreachEveryDay(startDate, endDate, curDate => {

          val year = Utils.formatDate(curDate, "yyyy")
          val month = Utils.formatDate(curDate, "MM")
          val day = Utils.formatDate(curDate, "dd")
          logInfo(s"begin compute $year-$month-$day")
          stateConf.set("date", s"$year$month$day")

          SparkHelper.setPartitionValue(stateConf, "mileageCheckDayCoord", Array(year, month, day))
          //计算
          dayCoordJob.write(dayCoordJob.doCompute())


      })
    }

    def computeMonthMileage(startDate: Date, endDate: Date): Unit ={
      val year=Utils.formatDate(startDate,"yyyy")
      val month=Utils.formatDate(startDate,"MM")
      SparkHelper.setPartitionValue(stateConf,"mileageCheckDeadline",Array(year,month))
      new DeadlineMileageJob(stateConf, sparkSession).compute()
    }

    //获取开始日期
    val startDate = Utils.parsetDate(stateConf.getString("monthDate"), "yyyyMM").get
    //获取结束日期
    val endDate = new Date(startDate.getTime)
    endDate.setMonth(startDate.getMonth + 1)
    endDate.setDate(endDate.getDate-1)
    logInfo(s"begin compute $startDate ")

    if (stateConf.getOption("daycoord.enable").getOrElse("true") == "true") {
      //计算每一天的轨迹
      computeEveryDay(stateConf, startDate, endDate)
    }

    //计算月里程
    logInfo("begin compute deadlinemileage")
    computeMonthMileage(startDate, endDate)
  }

  //计算每一天数据
  def foreachEveryDay(startDate: Date, endDate: Date, f: (Date) => Unit): Unit = {
    endDate.setMonth(startDate.getMonth + 1)

    while (startDate.getTime <= endDate.getTime) {
      f(startDate)
      startDate.setDate(startDate.getDate + 1)
    }
  }
}
package com.bitnei.report

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.{Delete, Scan}
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp
import org.apache.hadoop.hbase.filter.{CompareFilter, RowFilter, SubstringComparator}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.{SaveMode, SparkSession}
import org.apache.spark.storage.StorageLevel

import scala.collection.mutable.ArrayBuffer


/**
  * 需求：
  * "vid":"01e2868d-e308-4209-ac9f-9bec6314ac0c"
  * "vin":"LA9CA8N02GBBFC423"
  * "date":"201711"
  * "mielage":277.1
  * "hours":15.45
  * "days":4
  * "coords":[[0.0] [0.0] [0.0] [0.0] [0.0] [0.0]]
  *
  */

case class DayReport2(vid: String, totalMileage: Option[Int], runtime: Option[Int], dayNum: Option[Int])

case class RealinfoInput2(vid: String, vin: String, time: Option[String], longitude: Option[String], latitude: Option[String])

case class RealCoords2(vid: String, vin: String, windows: Array[Array[Array[Double]]])

case class MonthAndCoordReport(vid: String, vin: String, date: String, mielage: Double, hours: Double, days: Int, windows: Array[Array[Array[Double]]])


/**
  *
  * @author zhangyongtian
  * @define 计算里程核查的轨迹和里程
  * create 2017-11-15 9:07
  *
  */
object MileageCheckJob extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    // TODO: 日志级别设置
    Logger.getLogger("org").setLevel(Level.ERROR)

    // TODO: 参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)

    // TODO: 验证参数
    logInfo("验证参数....")

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env")

    val outputTargets = stateConf.getOption("output")
            .get
      ///////////////////////////test//////////////////
//      .getOrElse("unknow")
    ///////////////////////////test//////////////////


    val yearMonth = stateConf.getOption("input.month")
            .get

      ////////////////////////test///////////////
//      .getOrElse("201711")
    ///////////////////////////////////////////////

    if (yearMonth.length != 6) {
      throw new Exception("input.month error")
    }

    val year = yearMonth.substring(0, 4)

    val month = yearMonth.substring(4)


    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)

    ////////////////////////test////////////////////////////
//    val sparkSession = SparkHelper.getSparkSession(sparkMaster = Option("local[*]"))
    ////////////////////////////////////////////////////////

    val sparkConf = sparkSession.conf
    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
    sparkConf.set("spark.kryo.registrator", classOf[DayReport2].getName)
    sparkConf.set("spark.kryo.registrator", classOf[RealinfoInput2].getName)
    sparkConf.set("spark.kryo.registrator", classOf[RealCoords2].getName)
    sparkConf.set("spark.kryo.registrator", classOf[MonthAndCoordReport].getName)

    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    sparkConf.set("spark.kryo.registrationRequired", "true")

    import sparkSession.implicits._

    //    ////////////////////////test//////////////////////////////////
    //
    // TODO: 注册模拟数据
//    MockDataProvider.realInfo(sparkSession)
//    MockDataProvider.dayReport(sparkSession)
    //
    //    ////////////////////////////////////////////////


    ////////////////////////////////////////////////
    // TODO: 将数据注册成表
    logInfo("将数据注册成表")


    /////////////////////研发环境///////////////////////////////

    if (env.get.equals("dev")) {

      sparkSession
        .read
        .format("parquet")
        .load(s"/tmp/zyt/data/realinfo/20171102").createOrReplaceTempView("realinfo")


      sparkSession
        .read
        .format("parquet")
        .load(s"/tmp/zyt/data/dayreport/20171102").createOrReplaceTempView("dayreport")
    }
    else {
      //生产环境
      sparkSession
        .read
        .format("parquet")
        .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}").createOrReplaceTempView("realinfo")


      sparkSession
        .read
        .format("parquet")
        .load(s"/spark/vehicle/result/dayreport/year=${year}/month=${month}").createOrReplaceTempView("dayreport")

    }

    ////////////////////////////////////////////////////////
    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

    //"vid":"010a0660-338d-407b-9cee-84b7dfeba355", "reportDate":1509552001000,"totalMileage":1319,"timeLeng":3584000,"reportDate":1509552001000
    // TODO: 日报表数据计算
    logInfo("日报表数据计算")
    val dayReportDS = sparkSession.sql(
      s"""
         |SELECT
         | VID,
         | CAST(SUM(totalMileage) AS int) AS totalMileage,
         | CAST(SUM(timeLeng) AS int) AS runtime,
         | CAST(COUNT(distinct date_format(from_unixtime(reportDate/1000), 'yyyyMMdd')) AS int) AS dayNum
         | FROM dayreport where category = '${Constant.TravelState}'  GROUP BY VID Having totalMileage > 0 and runtime > 0 and dayNum > 0
              """.stripMargin).as[DayReport2]

    //////////////////////test/////////////
    //        dayReportDS.show(false)
    /////////////////////////test///////////////////

    // TODO: 实时数据计算
    logInfo("实时数据计算")

    val coordDiffHigh = stateConf.getOption("coord.hightThreshold").map(_.toInt).getOrElse(10000)
    val coordDiffLow = stateConf.getOption("coord.lowThreshold").map(_.toInt).getOrElse(100)

    val minL = stateConf.getOption("window.minL").map(_.toInt).getOrElse(1)
    val maxL = stateConf.getOption("window.maxL").map(_.toInt).getOrElse(10000)
    val minWindowLength = stateConf.getOption("window.minWindowLength").map(_.toInt).getOrElse(2)


    //VID:27E2ADD692D67EB1E0533C02A8C0B094  VIN:LNBSCB3F1ED143229   TIME:20171101234917  2502:116702658   2503:39882063
    val realinfoDS = sparkSession.sql(s"SELECT VID,VIN,TIME,`2502` AS longitude,`2503` AS latitude FROM realinfo where VID is not null and VIN is not null and TIME is not null and `2502` is not null and `2503` is not null ").as[RealinfoInput2]
      .groupByKey(_.vid)
      .mapGroups {
        case (vid, values: Iterator[RealinfoInput2]) => {

          val realinfoInputs = values.toArray[RealinfoInput2].sortBy(_.time)

          //          println("-----------------"+realinfoInputs.length)

          // TODO: 获取坐标集合
          var coords = new ArrayBuffer[Array[Long]]()

          realinfoInputs.foreach(realinfoInput => {
            coords.append(Array(realinfoInput.longitude.getOrElse(0L).toString.toLong, realinfoInput.latitude.getOrElse(0L).toString.toLong))
          })

          coords = coords.filter(x => !x.contains(0L))


          // TODO: 窗口切分算法处理
          var windows: Array[Array[Array[Double]]] = null

          if (coords.length > 0) {
            windows = CoordsHandler.splitByWindow(coords, minL, maxL, minWindowLength)
          }

          val vin = realinfoInputs.head.vin

          RealCoords2(vid, vin, windows)
        }
      }
      .filter(x => {
        x.windows != null && x.windows.length > 0
      }).coalesce(100)

    //////////////////////////////////////test/////////////////////////
    //            realinfoDS.count
//    realinfoDS.show(false)
    /////////////////////////////test/////////////////////////


    // TODO: 关联
    logInfo("关联")
    val result = dayReportDS.joinWith(realinfoDS, dayReportDS("vid").equalTo(realinfoDS("vid")), "inner")
      .map {

        case (dayReport, realCoords) =>

          MonthAndCoordReport(
            realCoords.vid,
            realCoords.vin,
            yearMonth,
            DataPrecision.mileage(dayReport.totalMileage.getOrElse(0)),
            DataPrecision.toHour(dayReport.runtime.getOrElse(0)),
            dayReport.dayNum.getOrElse(0),
            realCoords.windows
          )
      }

    //////////////////////////////////////test/////////////////////////
    //    result.count
    //    result.show(false)
    //////////////////////////////////////test/////////////////////////

    if (outputTargets.split(",").length > 1)
      result.persist(StorageLevel.MEMORY_ONLY_SER)

    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")
    sparkSession.catalog.dropTempView("dayreport")


    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////


    //    val quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89")
    //    val zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")
    //    val htableName = "mileage_check_coords"

    ///////////////////////////////////tmp删除该月的所有row/////////////////

    //    logInfo("tmp删除该月的所有row  start .............")

    //方案一
    //    sparkSession.sparkContext.textFile("/tmp/zyt/uuids.txt")
    //      .map(x => {
    //        x + s"_${yearMonth}"
    //      })
    //      .foreachPartition(rows => {
    //        val table = HbaseHelper.getConnection(quorum, zkport).getTable(TableName.valueOf(htableName))
    //        rows.foreach(row => {
    //          table.delete(new Delete(Bytes.toBytes(row)))
    //        })
    //      })

    //方案二
    //    import org.apache.hadoop.hbase.filter.CompareFilter
    //    import org.apache.hadoop.hbase.util.Bytes
    //    import org.apache.hadoop.hbase.TableName
    //    import java.util
    //    import scala.collection.JavaConversions._
    //
    //
    //    val table = HbaseHelper.getConnection(quorum, zkport).getTable(TableName.valueOf(htableName))
    //    val filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(s"${yearMonth}"))
    //    val scan = new Scan()
    //    scan.setFilter(filter)
    //
    //    val resultScanner = table.getScanner(scan);
    //
    //    val deletes = new util.ArrayList[Delete]
    //
    //    for (res <- resultScanner) {
    //      deletes.add(new Delete(res.getRow))
    //    }
    //    table.delete(deletes);

    //    logInfo("tmp删除该月的所有row end .............")

    ///////////////////////////////////tmp删除该月的所有row/////////////////

    if (outputTargets.contains("hdfs")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      val hdfsPath = s"${stateConf.getString("output.hdfs.path")}/${year}/${month}"

      //////////////////test/////////////////////////
      //    val hdfsPath = s"/tmp/zyt/${this.getClass.getSimpleName}/${yearMonth}"
      ///////////////////////////////////////////////////////

      ///spark/vehicle/result/mileagecheck
      result.mapPartitions(monthCoords => {
        toJson(monthCoords.toArray).toIterator
      }).toDF().write.format("text").mode(SaveMode.Overwrite).save(hdfsPath)

      logInfo("输出到HDFS end ....")

    }


    if (outputTargets.contains("hbase")) {

      //生产环境
      var quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89")
      var zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")

      //研发环境 	yf-nn2:2181,yf-dn1:2181,yf-dn2:2181
      if (env.get.equals("dev")) {
        quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.6.103,192.168.6.104,192.168.6.105")
        zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")
      }


      val htableName = "mileage_check_coords"

      //TODO: 输出到HBase
      logInfo("输出到HBase start....")
      result.coalesce(80).foreachPartition(monthCoords => {
        HbaseHelper.bulkPut(quorum, zkport, htableName, (table) => {
          val mapper = new ObjectMapper()
          mapper.registerModule(DefaultScalaModule)
          monthCoords.foreach(monthCoord => {
            // 0000d218-44aa-4e15-be39-8f66c602218f_201710
            val rowKey = s"${monthCoord.vid}_${monthCoord.date}"
            //          table.delete(new Delete(Bytes.toBytes(rowKey)))
            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "coord", toJson(mapper, monthCoord)))
          })
        })
      })
      logInfo("输出到HBase end ....")
    }

    logInfo("任务完成...")

    ///////////////////////test 截取天数////////////////////
    //    sparkSession.read.textFile(hdfsPath).as[String]
    //      .map(x => {
    //        val i = x.indexOf("days")
    //        x.substring(i, i + 10)
    //      }).repartition(1)
    //      .write.text(s"/tmp/zyt/days/${yearMonth}/${System.currentTimeMillis()}")
    ///////////////////////test截取天数////////////////////


    sparkSession.stop()
  }


  /////////////////////////////////////////////////////////////////////////////////////////////////////


  /**
    * 对象转换成Json字符串
    *
    * @param mapper
    * @param monthCoord
    * @return
    */
  def toJson(mapper: ObjectMapper, monthCoord: MonthAndCoordReport): String = {
    val jsonValue = mapper.writeValueAsString(monthCoord)
    jsonValue
  }

  /**
    * 对象数组转换成Json字符串数组
    *
    * @param monthCoords
    * @return
    */
  def toJson(monthCoords: Array[MonthAndCoordReport]): Array[String] = {
    val mapper = new ObjectMapper()
    mapper.registerModule(DefaultScalaModule)
    monthCoords.map(mapper.writeValueAsString(_))
  }


}



//package com.bitnei.report.local
//
//import java.util.Date
//
//import com.bitnei.report.MileageCheckJob.{MonthCoord, RealinfoInput}
//import com.bitnei.report.{MonthAndCoordReport, RealinfoCoord}
//import com.bitnei.report.common.configuration.StateConf
//import com.bitnei.report.common.utils.{DataPrecision, Utils}
//import com.bitnei.report.constants.Constant
//import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
//import com.fasterxml.jackson.databind.ObjectMapper
//import com.fasterxml.jackson.module.scala.DefaultScalaModule
//import org.apache.hadoop.hbase.util.Bytes
//import org.apache.spark.sql._
//import org.apache.log4j.{Level, Logger}
//import org.apache.spark.storage.StorageLevel
//
//
///**
//  * 需求：
//  * "vid":"01e2868d-e308-4209-ac9f-9bec6314ac0c"
//  * "vin":"LA9CA8N02GBBFC423"
//  * "date":"201711"
//  * "mielage":277.1
//  * "hours":15.45
//  * "days":4
//  * "coords":[[0.0] [0.0] [0.0] [0.0] [0.0] [0.0]]
//  *
//  */
//object MileageCheckJobTest {
//
//  case class RealinfoInput(vid: String, vin: String, time: String, longitude: Long, latitude: Long)
//
//  case class MonthCoord(vid: String, vin:String,date: String, mielage: Double, hours: Double, days: Int, coords: Array[Array[Double]])
//
//  def main(args: Array[String]): Unit = {
//
//    // TODO: 参数集合
//    val stateConf = new StateConf
//    stateConf.add(args)
//
//    // TODO: 验证参数
//    //    val dateList = stateConf.getOption("report.date") match {
//    //      case Some(reportDateList) => reportDateList.split('-')
//    //      case None => throw new RuntimeException("report.date error")
//    //    }
//
//    //    val dateList = stateConf.getOption("report.date") match {
//    //      case Some(reportDateList) => reportDateList.split('-')
//    //      case None => throw new Exception("report.date error")
//    //    }
//
//    // TODO: 获取参数
////    val (startDate, endDate) = stateConf.getOption("input.date").map(_.split('_')) match {
////      case Some(dateRange) if dateRange.length == 2 =>
////        (Utils.parsetDate(dateRange(0), "yyyyMMdd").get, Utils.parsetDate(dateRange(1), "yyyyMMdd").get)
////      case _ =>
////        val yesterday = new Date
////        yesterday.setDate(yesterday.getDate - 1)
////        (yesterday, yesterday)
////    }
//
//
//    val (startDate, endDate) = ("2017-11-10","2017-11-13")
//
//    println(startDate)
//
//    // TODO: 日志级别设置
//    Logger.getLogger("org").setLevel(Level.ERROR)
//
//
//    // TODO: 上下文
//    val sparkSession = SparkHelper.getSparkSession(sparkMaster = Option("local[*]"))
//    import sparkSession.implicits._
//
//    // TODO: 定义 Encoder
//    //    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
//
//    //    implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()
//
//    ////////////////////////test//////////////////////////////////
//
//    // TODO: 注册模拟数据
//    MockDataProvider.realInfo(sparkSession)
//    MockDataProvider.dayReport(sparkSession)
//
//    ////////////////////////////////////////////////
//
////    sparkSession.sqlContext
////      .read
////      .format("parquet")
////      .load("/spark/vehicle/data/realinfo/year=2017/month=09").persist(StorageLevel.MEMORY_AND_DISK_SER).createOrReplaceTempView("realinfo")
////
////
////    sparkSession.sqlContext
////      .read
////      .format("parquet")
////      .load("/spark/vehicle/data/dayreport/year=2017/month=09").persist(StorageLevel.MEMORY_AND_DISK_SER).createOrReplaceTempView("dayreport")
//
//
////    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,"realinfo")
////    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,"dayreport")
//
//
//    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//
//    // TODO: 实时数据计算
//
//
//    val realinfoDF = sparkSession.sql(s"SELECT VID,VIN,TIME,`2502` AS longitude,`2503` AS latitude FROM realinfo ")
//
//    val realinfoRDD = realinfoDF
////      .filter(x=>{
////        x.getAs[String]("longitude").matches("^(\\d+)$") &&  x.getAs[String]("latitude").matches("^(\\d+)$")
////      })
//
//      .map(x => {
//        val vid = x.getAs[String]("VID")
//        val vin = x.getAs[String]("VIN")
//        val time = x.getAs[String]("TIME")
//
//        val longitude = x.getAs[String]("longitude").toLong
//
//        val latitude = x.getAs[String]("latitude").toLong
//
//        RealinfoInput(vid, vin, time, longitude, latitude)
//      })
//
//        realinfoRDD.count()
//
////    // TODO: 分组
////    val realinfoGroupRDD = realinfoRDD.groupBy(x => {
////      x.vid + "\t" + x.vin
////    })
////
////    val realinfoResult = realinfoGroupRDD
////      .map(x => {
////        val cols = x._1.toString.split("\t")
////        val vid = cols(0)
////        val vin = cols(1)
////
////        // TODO: 按时间排序
////        //      val realinfoIter: Iterable[MileageCheckJobTest.RealinfoInput] = x._2
////        val sortedRealinfoArr = x._2.toArray.sortBy(x => x.time)
////
////        // TODO: 验证经纬度的正确性
////        var prevLontitude = 0L
////        var prevLatitude = 0L
////
////        val hightThreshold = 10000
////        val lowThreshold = 100
////
////        val filterRealInfoArr = sortedRealinfoArr.filter(realinfo => {
////
////          val longitude = realinfo.longitude
////          //                .getOrElse(0L)
////          val latitude = realinfo.latitude
////          //                .getOrElse(0L)
////
////          //          val ruler01 = (prevLatitude == 0 || prevLontitude == 0)
////
////          val longitudeDiff = Math.abs(longitude - prevLontitude)
////          val latitudeDiff = Math.abs(latitude - prevLatitude)
////
////          val ruler02 = longitudeDiff >= lowThreshold && latitudeDiff >= lowThreshold
////
////          val ruler03 = longitudeDiff <= hightThreshold && latitudeDiff <= hightThreshold
////
////          prevLontitude = longitude
////          prevLatitude = latitude
////
////          //          ruler01 &&
////          ruler02 && ruler03
////        })
////        println(vid + "----filtered row num : " + (sortedRealinfoArr.length - filterRealInfoArr.length))
////
////        (vid, vin, filterRealInfoArr)
////
////      })
////
////      .map(x => {
////        val vid = x._1
////        val vin = x._2
////        // TODO: 转换经纬度类型
////        val coords = x._3
////          .map(row => {
////            Array[Double](DataPrecision.latitude(row.longitude), DataPrecision.latitude(row.latitude))
////          })
////        (vid.toString, (vin, coords))
////      })
////
////
////    // TODO: 日报表数据计算
////    val dayReportDF = sparkSession.sql(
////      s"""
////         |SELECT
////         | VID,
////         | CAST(SUM(totalMileage) AS int) AS totalMileage,
////         | CAST(SUM(timeLeng) AS int) AS runtime,
////         | CAST(COUNT(*) AS int) AS dayNum
////         | FROM dayreport where category = '${Constant.TravelState}'  GROUP BY VID
////          """.stripMargin)
////
////    val dayReportResult =
////      dayReportDF.rdd.map(x => {
////        val vid = x.getAs[String]("VID")
////        val totalMileage = x.getAs[Int]("totalMileage")
////        val runtime = x.getAs[Int]("runtime")
////        val dayNum = x.getAs[Int]("dayNum")
////
////        (vid.toString, (totalMileage, runtime, dayNum))
////      })
////
////    // TODO: 关联
////    val result = realinfoResult.leftOuterJoin(dayReportResult).map(x => {
////      val vid = x._1
////
////      val left = x._2._1
////      val vin = left._1
////      val coords = left._2
////
////      val right = x._2._2
////
////      var totalMileage = 0D
////      var runtime = 0D
////      var dayNum = 0
////
////
////      if (!right.isEmpty) {
////        totalMileage = right.get._1.toString.toDouble
////        runtime = right.get._2.toString.toDouble
////        dayNum = right.get._3.toString.toInt
////      }
////
////
////      runtime = DataPrecision.toHour(runtime)
////
////      totalMileage = DataPrecision.mileage(totalMileage)
////
////      MonthCoord(
////        vid,
////        vin,
////        "201709",
////        totalMileage,
////        runtime,
////        dayNum,
////        coords
////      )
////    }).distinct
////
////    result.count()
//
//    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//    //    // TODO: 输出到HBase
//    //    val quorum = stateConf.getOption("hbase.quorum").get
//    //    val zkport = stateConf.getOption("hbase.zkport").get
//    //    val htableName = "mileage_check_coords"
//    //
//    //    result.foreachPartition(monthCoords => {
//    //      HbaseHelper.bulkPut(quorum, zkport, htableName, (table) => {
//    //        val mapper = new ObjectMapper()
//    //        mapper.registerModule(DefaultScalaModule)
//    //        monthCoords.foreach(monthCoord => {
//    //          val rowKey = s"${monthCoord.vid}_${monthCoord.date}"
//    //          table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "coord", toJson(mapper, monthCoord)))
//    //        })
//    //      })
//    //    })
//    //
//    //
//    //    // TODO: 输出到HDFS
//    //    ///spark/vehicle/result/mileagecheck
//    //    val path = s"${stateConf.getString("output.hdfs.path")}/${Utils.formatDate(startDate, "yyyy/MM")}"
//    //
//    //    val numPartitions = stateConf.getOption("output.hdfs.file.num").map(_.toInt).getOrElse(10)
//    //
//    //    result.mapPartitions(monthCoords => {
//    //      toJson(monthCoords.toArray).toIterator
//    //    }).repartition(numPartitions)
//    //      .toDF().write.format("text").mode(SaveMode.Overwrite).save(path)
//    //
//    //    s"${stateConf.getString("output.hdfs.path")}/${Utils.formatDate(startDate, "yyyy/MM")}"
//
//
//    sparkSession.stop()
//  }
//
//
//  /////////////////////////////////////////////////////////////////////////////////////////////////////
//
//  /**
//    * 对象转换成Json字符串
//    *
//    * @param mapper
//    * @param monthCoord
//    * @return
//    */
//  def toJson(mapper: ObjectMapper, monthCoord: MonthCoord): String = {
//    val jsonValue = mapper.writeValueAsString(monthCoord)
//    jsonValue
//  }
//
//  /**
//    * 对象数组转换成Json字符串数组
//    *
//    * @param monthCoords
//    * @return
//    */
//  def toJson(monthCoords: Array[MonthCoord]): Array[String] = {
//    val mapper = new ObjectMapper()
//    mapper.registerModule(DefaultScalaModule)
//    monthCoords.map(mapper.writeValueAsString(_))
//  }
//
//
//}
//
package com.bitnei.report.common.mileage

/*
* created by wangbaosheng on 2017/12/15
*/

object MileageCheckPath {
  val Unkonw: Int = Int.MaxValue
  val EmptyPercentTooHigh=0
  val ExceptionPercentTooHigh=1
  val OnlineMileageAndGPSMileageRelativeErrorTooHigh=2
  val OnlineMileageAndValidateMileageRelativeError=3
  val OnlineMileage=4

  def parsePath(v: MileageResult, path: Int): String = {
    if (path == EmptyPercentTooHigh) s"数据缺失率太高(${v.counter.emptyPercent > v.checkMileageThreshold.A}),核算里程为0"
    else if (path == ExceptionPercentTooHigh) s"数据异常率太高(${v.counter.exceptionPercent}>${v.checkMileageThreshold.B}),核算里程为0"
    else if (path == OnlineMileageAndGPSMileageRelativeErrorTooHigh) {
      val m = if (v.onlineMileage < v.gpsMileage) s"在线里程 ${v.onlineMileage}" else s"轨迹里程 ${v.gpsMileage}"
      s"上线里程(${v.onlineMileage})和轨迹里程(${v.gpsMileage})相对误差太大(${v.onlineMileageAndGPSMileageRelativeError}>${v.checkMileageThreshold.C}),核算里程为$m"
    }
    else if (path == OnlineMileageAndValidateMileageRelativeError) {
      val m = if (v.onlineMileage < v.validateMileage) s"在线里程 ${v.onlineMileage}" else s"有效里程 ${v.validateMileage}"
      s"上线里程(${v.onlineMileage})和有效里程(${v.validateMileage})相对误差太大(${v.onlineMileageAndValidateMileageRelativeError}>${v.checkMileageThreshold.D}),核算里程为$m"
    }
    else if (path == OnlineMileage) "核算里程为在线里程"
    else "核算里程异常"
  }
}
//package com.bitnei.report
//
//import com.bitnei.common.configuration.StateConf
//import com.bitnei.common.utils.SparkHelper
//import com.bitnei.report.constants.Constant
//import org.scalatest.FunSuite
//
///**
//  * Created by wangbaosheng on 2017/7/31.
//  */
//
//case class DayReport(vid:String,timeleng:Int,totalMileage:Int,category:String)
//
//class MileageCheckTest extends FunSuite {
//
//  test("test mileage check"){
//    val sparkSession=SparkHelper.getSparkSession(sparkMaster = Some("local"))
//    val stateConf=new StateConf
//
//    stateConf.set("input.date","20170101_20170131")
//    import sparkSession.implicits._
//
//    val mc=new MileageCheck(stateConf,sparkSession)
//
//    sparkSession.createDataset(
//      Array(
//        RealinfoInput("vid1","vin1",Some("20170101000000"),Some(116400791),Some(39899761)),
//        RealinfoInput("vid1","vin1",Some("20170101000010"),Some(111111111),Some(13222111)),
//        RealinfoInput("vid1","vin1",Some("20170101000010"),Some(111111111),Some(13222111)),
//        RealinfoInput("vid1","vin1",Some("20170101000010"),Some(111111111),Some(13222112)),
//        RealinfoInput("vid2","vin1",Some("20170101000010"),None,Some(13222333)),
//        RealinfoInput("vid3","vin1",Some("20170101000010"),Some(111133333),Some(13222111))
//      )
//    ).createOrReplaceTempView("realinfo")
//
//
//    sparkSession.createDataset(Array(
//      DayReport("vid1",100000,10,Constant.TravelState),
//      DayReport("vid1",100000,11,Constant.TravelState),
//      DayReport("vid1",100000,11,Constant.TravelState),
//      DayReport("vid2",100000,11,Constant.TravelState)
//    )).createOrReplaceTempView("dayreport")
//
//
//    mc.doCompute().foreachPartition(v=>{
//      mc.toJson(v.toArray).foreach(println)
//    })
//  }
//}
package com.bitnei.report.common.mileage

import java.util.Date

import com.bitnei.report.common.GpsDistance
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils

import scala.annotation.tailrec
import scala.collection.mutable.ArrayBuffer

/*
* created by wangbaosheng on 2017/12/11
* 核算里程计算
*/
class MileageComputer(stateConf: StateConf) extends Logging with Serializable {
  //默认为2KM
  private val stepMileageThreshold = stateConf.getOption("stepMileageThresholdKm").map(_.toDouble).getOrElse(2D)

  //连续电流窗口内，任意一条数据的电流都>continueCurrentWindowThresholdA，并且相等。
  private val continueCurrentWindowThresholdA = stateConf.getOption("continueCurrentWindowThresholdA").map(_.toDouble).getOrElse(10D)
  //连续电流最小窗口大小
  private val continueCurrentWindowMinLength = stateConf.getOption("continueCurrentWindowMinLength").map(_.toInt).getOrElse(3)

  //连续电流明细
  private val continuCurrentStepMileageWindows = new ArrayBuffer[String]()

  //跳变明细
  private val stepMileageWindows = new ArrayBuffer[String]()

  def getStepMileageWindows:Array[String]=stepMileageWindows.toArray

  //速度最低阈值
  private val speedLow=stateConf.getInt("speedLow",0)
  //速度最高阈值
  private val speedHigh=stateConf.getInt("speedHigh",220)

  private val totalVoltageLow=stateConf.getInt("totalVoltageLow",0)
  private val totalVoltageHigh=stateConf.getInt("totalVoltageHigh",1000)

  private val totalChargeLow=stateConf.getInt("totalChargeLow",-1000)
  private val totalChargeHigh=stateConf.getInt("totalChargeHigh",1000)

  private val socLow=stateConf.getInt("socLow",0)
  private val socHigh=stateConf.getInt("socHigh",100)

  private val longitudeLow=stateConf.getInt("longitudeLow",73)
  private val longitudeHigh=stateConf.getInt("longitudeHigh",135)

  private val latitudeLow=stateConf.getInt("latitudeLow",4)
  private val latitudeHigh=stateConf.getInt("latitudeHigh",53)

  //核算里程阈值
  private val checkMileageThreshold=CheckMileageThreshold(
    stateConf.getOption("A").map(_.toDouble).getOrElse(1D) ,
    stateConf.getOption("B").map(_.toDouble).getOrElse(1D),
    stateConf.getOption("C").map(_.toDouble).getOrElse(0.6),
    stateConf.getOption("D").map(_.toDouble).getOrElse(0.6)
  )

  def compute(values: Array[Realinfo]): MileageResult = {
    val vid = values.headOption.map(_.vid).getOrElse("")
    val vin = values.headOption.map(_.vin).getOrElse("")

    //获取第一帧有效里程，最后一帧有效里程，计数指标，剔除缺失值和异常值后的数据
    val (firstWithValidateMileage, lastWithValidateMielage, counter, nonExceptionValues) = getValidateValues(values)
    val firstMileage: Double = firstWithValidateMileage.map(_.mileage.getOrElse(0D)).getOrElse(0D)
    val lastMileage: Double = lastWithValidateMielage.map(_.mileage.getOrElse(0D)).getOrElse(0D)

    //上线里程
    val onlineMileage = lastMileage - firstMileage

    val (gpsMileage, totalGpsRepeationNum, stepMileage, stepWindowNum, stepNum, continueCurrentStepMileage, validateMileage, checkMileage, path) = if (nonExceptionValues.nonEmpty) {
      //计算基础里程值，基础里程值有：(gps里程，gps轨迹重复计次，里程跳变扣除里程，跳变计次，连续电流扣除里程)
      val (gpsMileage, totalGpsRepeationNum, stepMileage, stepWindowNum, stepNum, continueCurrentStepMileage) = getBaseMileage(nonExceptionValues)

      //有效里程=上线里程-里程跳变扣除里程-连续电流跳变扣除里程
      val validateMileage = MileageUtil.validateMileage(onlineMileage, stepMileage, continueCurrentStepMileage)

      //核算里程和核算路径。
      val (checkMileage, path) = MileageUtil.getCheckMileage(onlineMileage, validateMileage, gpsMileage, counter, checkMileageThreshold)

      (gpsMileage, totalGpsRepeationNum, stepMileage, stepWindowNum, stepNum, continueCurrentStepMileage, validateMileage, checkMileage, path)
    } else {
      // logWarning(s"vid:${vid},vin:$vin no validate values")
      (0D, 0, 0D, 0, 0, 0D, 0D, 0D, MileageCheckPath.Unkonw)
    }

    MileageResult(
      vid = vid,
      vin = vin,
      firstValidateMileage = firstWithValidateMileage.map(_.mileage).getOrElse(None),
      lastValidateMileage = lastWithValidateMielage.map(_.mileage).getOrElse(None),
      firstTimeWithValidateMileage = firstWithValidateMileage.map(_.time),
      lastTimeWithValidateMileage = lastWithValidateMielage.map(_.time),
      stepMileageThreshold = stepMileageThreshold,
      checkMileagePath = path,
      checkMileage = checkMileage,
      onlineMileage = onlineMileage,
      gpsMileage = gpsMileage,
      totalGpsRepeationNum = totalGpsRepeationNum,
      stepMileage = stepMileage,
      stepNum = stepNum,
      stepWindowNum = stepWindowNum,
      continueCurrentStepMileage = continueCurrentStepMileage,
      validateMileage = validateMileage,

      onlineMileageAndValidateMileageRelativeError = (onlineMileage - validateMileage) / onlineMileage,
      onlineMileageAndGPSMileageRelativeError = (onlineMileage - gpsMileage) / onlineMileage,

      stepMileageDetail = stepMileageWindows.toArray,
      continueCurrentWindowThreshold = continueCurrentWindowThresholdA,
      continueCurrentWindowMinLength = continueCurrentWindowMinLength + 1,
      continueCurrentStepMileageNum = continuCurrentStepMileageWindows.length,
      continueCurrentStepMileageDetail = continuCurrentStepMileageWindows.toArray,

      checkMileageThreshold = checkMileageThreshold,

      counter = counter
    )
  }

  /**
    * @return (第一条有效仪表里程，最后一条有效仪表里程，统计值，剔除缺失和异常值后的数据)
    **/
  def getValidateValues(values: Array[Realinfo]): (Option[Realinfo], Option[Realinfo], Counter, Array[Realinfo]) = {
    //获取首次和末次有效里程
    val (firstWithValidateMileage, lastWithValidateMileage) = getFirstAndLastWithValidateMielage(values)

    //获取非缺失数据
    val (speedEmptyCount,
    mileageEmptyCount,
    voltageEmptyCount,
    currentEmptyCount,
    socEmptyCount,
    longitudeEmptyCount,
    latitudeEmptyCount,
    flameoutCount,
    emptyCount,
    totalCharge,
    avgCharge,
    nonEmptyValues) = getNonEmptyValues(values)

    //获取非异常数据
    val (speedExceptionCount,
    mileageExceptionCount,
    voltageExceptionCount,
    currentExceptionCount,
    socExceptionCount,
    longitudeExceptionCount,
    latitudeExceptionCount,
    exceptionCount,
    nonExceptionValues) = if (nonEmptyValues.nonEmpty && firstWithValidateMileage.nonEmpty) {
      getNonExceptionValus(nonEmptyValues,
        firstWithValidateMileage.map(_.mileage.getOrElse(0D)).getOrElse(0D),
        lastWithValidateMileage.map(_.mileage.getOrElse(0D)).getOrElse(0D))
    } else {
      (0, 0, 0, 0, 0, 0, 0, 0, Array.empty[Realinfo])
    }

    //    logInfo(
    //      s"""
    //        |${values.headOption.map(_.vin).getOrElse("")}
    //        |${nonEmptyValues.length}
    //        |${nonExceptionValues.length}
    //        |${firstWithValidateMileage}
    //      """.stripMargin)

    val counter = Counter(
      values.length,
      speedEmptyCount,
      mileageEmptyCount,
      voltageEmptyCount,
      currentEmptyCount,
      socEmptyCount,
      longitudeEmptyCount,
      latitudeEmptyCount,
      flameoutCount,
      emptyCount,

      nonEmptyValues.length,
      speedExceptionCount,
      mileageExceptionCount,
      voltageExceptionCount,
      currentExceptionCount,
      socExceptionCount,
      longitudeExceptionCount,
      latitudeExceptionCount,
      exceptionCount,
      nonExceptionValues.length,
      okPercent = (values.length - flameoutCount-emptyCount - exceptionCount) / values.length.toDouble)

    (firstWithValidateMileage,
      lastWithValidateMileage,
      counter,
      nonExceptionValues)
  }

  case class Range(start:Int,end:Int)
  object Range{
    def Empty:Range=Range(-1,-1)
  }

  private val problemDetails:ArrayBuffer[Detail]=new ArrayBuffer[Detail]()

  case class Detail(detailType:Int,startTime:Long,endTime:Long,length:Int,startMileage:Option[Double],endMileage:Option[Double])
  object Detail {
    val Speedempty = 1
    val Mileageempty = 2
    val Voltageempty = 3
    val Currentempty = 4
    val Socempty = 5
    val Longitudeempty = 6
    val Latitudeempty = 7
  }


  /*
  *
  * i<0,socEmptyBegin=-1,socEmptyBegin=-1
  * i>=0,
  * 如果i异常，如果是第一次异常(socEmptyBegin=-1) socEmptyBegin=i，否则socEmptyEnd=i
  * 如果i不异常了 (socEcmptyBegin,socEmptyEnd) ,然后set (socEcmptyBegin,socEmptyEnd) =(-1,-1)
  * */
//  def getValidateAndNonValidateValues(fullValues:Array[Realinfo],firstValidateMileage:Double, lastValidateMielage:Double): Unit = {
//    val validateValues=new ArrayBuffer[Realinfo]
//    def doCompute(i: Int,
//                  ranges:Array[Range],
//                  getRangeIfStoppeds: Array[(Boolean, Range, Int) => (Boolean, Range)],
//                  f: (Int, Range) => Unit): Unit = {
//      if (i < fullValues.length) {
//        val v = fullValues(i)
//
//        val (speedempty,
//        mileageempty,
//        voltageempty,
//        currentempty,
//        socempty,
//        longitudeempty,
//        latitudeempty) = checkEmpty(v)
//
//        //如果某一项缺失，则计数
//        //如果缺失，
//
//
////
////        val allNonEmpty = !(speedempty || mileageempty || voltageempty || currentempty || socempty ||
////          longitudeempty || latitudeempty)
////
////        val (speedException,
////        mileageException,
////        voltageException,
////        currentException,
////        socException,
////        longitudeException,
////        latitudeException) = checkException(v, firstValidateMileage, lastValidateMielage)
////
////        val allAreOk = !speedException && !mileageException && !voltageException && !currentException && !socException &&
////          !longitudeException && !latitudeException
////
////
////        //如果没有熄火，没有缺失，并且没有异常，那么是有效数据
////        if(allNonEmpty &&allAreOk){
////          validateValues.append(fullValues(i))
////        }
//
//        val s=getRangeIfStoppeds.map(getRangeIfStopped=>getRangeIfStopped(ranges(0))).map(_._2)
//        doCompute(
//          i + 1,
//          s,
//          getRangeIfStoppeds,
//          f
//        )
//      } else {
//
//      }
//    }
//
//
//    def getRangeOfProblem(hasProblem: Boolean, range: Range, i: Int): (Boolean, Range) = {
//      if (hasProblem) {
//        if (range.start == -1) (false, Range(i, i))
//        else (false, Range(range.start, i))
//      } else {
//        if (range.start != -1) (true, range)
//        else (false, Range.Empty)
//      }
//    }
//
//
//    def processSpeedRange()=     {
//      getRangeOfProblem(speedempty, speedRange, i) match {
//        case (true, range) =>
//          f(Detail.Speedempty, range)
//          Range.Empty
//        case v => v._2
//      }
//    }
//    doCompute(0,
//      Array(
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty),
//      Array(
//        getRangeIfStopped(speedempty, speedRange, i) match {
//          case (true, range) =>
//            f(Detail.Speedempty, range)
//            Range.Empty
//          case v => v._2
//        },
//        getRangeIfStopped(mileageempty, mileageRange, i) match {
//          case (true, range) =>
//            f(Detail.Mileageempty, range)
//            Range.Empty
//          case v => v._2
//        },
//        getRangeIfStopped(voltageempty, voltageRange, i) match {
//          case (true, range) =>
//            f(Detail.Voltageempty, Detail, range)
//            Range.Empty
//          case v => v._2
//        },
//        getRangeIfStopped(currentempty, currentRange, i) match {
//          case (true, range) =>
//            f(Detail.Currentempty, range)
//            Range.Empty
//          case v => v._2
//        },
//
//        getRangeIfStopped(socempty, socRange, i) match {
//          case (true, range) =>
//            f(Detail.Socempty, range)
//            Range.Empty
//          case v => v._2
//        },
//        getRangeIfStopped(longitudeempty, longitudeRange, i) match {
//          case (true, range) =>
//            f(Detail.Longitudeempty, range)
//            Range.Empty
//          case v => v._2
//        },
//
//        getRangeIfStopped(latitudeempty, latitudeRange, i) match {
//          case (true, range) =>
//            f(Detail.Latitudeempty, range)
//            Range.Empty
//          case v => v._2
//        }
//      )
//      (prooblemType, range) => {
//        val startValue = fullValues(range.start)
//        val endValue = fullValues(range.end)
//        //将range转换为明细数据
//        val detail = Detail(prooblemType, startValue.time, endValue.time, range.end - range.start + 1, startValue.mileage, endValue.mileage)
//        //处理明细数据
//        problemDetails.append(detail)
//      }
//    )
//  }
//

//  def getValidateAndNonValidateValues(fullValues:Array[Realinfo],firstValidateMileage:Double, lastValidateMielage:Double): Unit = {
//    val validateValues=new ArrayBuffer[Realinfo]
//    def doCompute(i: Int,
//                  speedRange: Range,
//                  mileageRange: Range,
//                  voltageRange: Range,
//                  currentRange: Range,
//                  socRange: Range,
//                  longitudeRange: Range,
//                  latitudeRange: Range,
//                  getRangeIfStoppeds: Array[(Boolean, Range, Int) => (Boolean, Range)],
//                  f: (Int, Range) => Unit): Unit = {
//      if (i < fullValues.length) {
//        val v = fullValues(i)
//
//        val (speedempty,
//        mileageempty,
//        voltageempty,
//        currentempty,
//        socempty,
//        longitudeempty,
//        latitudeempty) = checkEmpty(v)
//
//        //如果某一项缺失，则计数
//        //如果缺失，
//
//
//        //
//        //        val allNonEmpty = !(speedempty || mileageempty || voltageempty || currentempty || socempty ||
//        //          longitudeempty || latitudeempty)
//        //
//        //        val (speedException,
//        //        mileageException,
//        //        voltageException,
//        //        currentException,
//        //        socException,
//        //        longitudeException,
//        //        latitudeException) = checkException(v, firstValidateMileage, lastValidateMielage)
//        //
//        //        val allAreOk = !speedException && !mileageException && !voltageException && !currentException && !socException &&
//        //          !longitudeException && !latitudeException
//        //
//        //
//        //        //如果没有熄火，没有缺失，并且没有异常，那么是有效数据
//        //        if(allNonEmpty &&allAreOk){
//        //          validateValues.append(fullValues(i))
//        //        }
//
//        getRangeIfStoppeds.zipWithIndex
//        doCompute(
//          i + 1,
//          getRangeIfStopped(speedempty, speedRange, i) match {
//            case (true, range) =>
//              f(Detail.Speedempty, range)
//              Range.Empty
//            case v => v._2
//          },
//          getRangeIfStopped(mileageempty, mileageRange, i) match {
//            case (true, range) =>
//              f(Detail.Mileageempty, range)
//              Range.Empty
//            case v => v._2
//          },
//          getRangeIfStopped(voltageempty, voltageRange, i) match {
//            case (true, range) =>
//              f(Detail.Voltageempty, Detail, range)
//              Range.Empty
//            case v => v._2
//          },
//          getRangeIfStopped(currentempty, currentRange, i) match {
//            case (true, range) =>
//              f(Detail.Currentempty, range)
//              Range.Empty
//            case v => v._2
//          },
//
//          getRangeIfStopped(socempty, socRange, i) match {
//            case (true, range) =>
//              f(Detail.Socempty, range)
//              Range.Empty
//            case v => v._2
//          },
//          getRangeIfStopped(longitudeempty, longitudeRange, i) match {
//            case (true, range) =>
//              f(Detail.Longitudeempty, range)
//              Range.Empty
//            case v => v._2
//          },
//
//          getRangeIfStopped(latitudeempty, latitudeRange, i) match {
//            case (true, range) =>
//              f(Detail.Latitudeempty, range)
//              Range.Empty
//            case v => v._2
//          },
//
//          getRangeIfStopped,
//          f
//        )
//      } else {
//
//      }
//    }
//
//
//    def getRangeOfProblem(hasProblem: Boolean, range: Range, i: Int): (Boolean, Range) = {
//      if (hasProblem) {
//        if (range.start == -1) (false, Range(i, i))
//        else (false, Range(range.start, i))
//      } else {
//        if (range.start != -1) (true, range)
//        else (false, Range.Empty)
//      }
//    }
//
//
//    doCompute(0,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      Range.Empty,
//      (hasProblem, range, i) => getRangeOfProblem(hasProblem, range, i),
//      (prooblemType, range) => {
//        val startValue = fullValues(range.start)
//        val endValue = fullValues(range.end)
//        //将range转换为明细数据
//        val detail = Detail(prooblemType, startValue.time, endValue.time, range.end - range.start + 1, startValue.mileage, endValue.mileage)
//        //处理明细数据
//        problemDetails.append(detail)
//      }
//    )
//  }



  private  val detail=new ArrayBuffer[String]()


  /** *
    * 剔除缺失值
    * */
  def getNonEmptyValues(values: Array[Realinfo]): (Int, Int, Int, Int, Int, Int, Int, Int, Int, Double, Double, Array[Realinfo]) = {
    var emptyCount = 0

    var (speedemptyCount,
    mileageemptyCount,
    voltageemptyCount,
    currentemptyCount,
    socemptyCount,
    longitudeemptyCount,
    latitudeemptyCount,
    flameOutCount) = (0, 0, 0, 0, 0, 0, 0, 0)


    var totalCharge = 0D



    val nonEmptyValues = values.filter(v => {
      if (v.state.contains(2)) {
        flameOutCount += 1
        false
      } else {
        //异常计数
        val (speedempty,
        mileageempty,
        voltageempty,
        currentempty,
        socempty,
        longitudeempty,
        latitudeempty) = checkEmpty(v)

        if (speedempty) speedemptyCount += 1
        if (mileageempty) {

          mileageemptyCount += 1
        }else{

        }

        if (voltageempty) voltageemptyCount += 1
        if (currentempty) currentemptyCount += 1
        if (socempty) socemptyCount += 1
        if (longitudeempty) longitudeemptyCount += 1
        if (latitudeempty) latitudeemptyCount += 1


        val allNonEmpty = !(speedempty || mileageempty || voltageempty || currentempty || socempty ||
          longitudeempty || latitudeempty)


        if (!allNonEmpty) emptyCount += 1

        if (allNonEmpty) {
          totalCharge += v.totalCharge.getOrElse(0D)
        }

        //返回非缺失，非异常数据
        allNonEmpty
      }
    })

    val avgCurrenct = if (nonEmptyValues.length <= 0) 0D else totalCharge / nonEmptyValues.length


    (speedemptyCount,
      mileageemptyCount,
      voltageemptyCount,
      currentemptyCount,
      socemptyCount,
      longitudeemptyCount,
      latitudeemptyCount,
      flameOutCount,
      emptyCount,
      totalCharge,
      avgCurrenct,
      nonEmptyValues)
  }


  /**
    * 剔除异常值
    *
    * @param nonEmptyValues       ,剔除缺失值后的数据
    * @param firstValidateMileage 第一帧有效仪表里程
    * @param lastValidateMielage  最后一帧有效仪表里程
    **/
  def getNonExceptionValus(nonEmptyValues: Array[Realinfo],
                           firstValidateMileage: Double,
                           lastValidateMielage: Double): (
    Int, Int, Int, Int, Int, Int, Int, Int, Array[Realinfo]) = {
    var exceptionCount = 0

    var (speedExceptionCount,
    mileageExceptionCount,
    voltageExceptionCount,
    currentExceptionCount,
    socExceptionCount,
    longitudeExceptionCount,
    latitudeExceptionCount) = (0, 0, 0, 0, 0, 0, 0)


    val validateValues = nonEmptyValues.filter(v => {
      //异常计数
      val (speedException,
      mileageException,
      voltageException,
      currentException,
      socException,
      longitudeException,
      latitudeException) = checkException(v, firstValidateMileage, lastValidateMielage)

      if (speedException) speedExceptionCount += 1
      if (mileageException) mileageExceptionCount += 1
      if (voltageException) voltageExceptionCount += 1
      if (currentException) currentExceptionCount += 1
      if (socException) socExceptionCount += 1
      if (longitudeException) longitudeExceptionCount += 1
      if (latitudeException) latitudeExceptionCount += 1


      val allAreOk = !speedException && !mileageException && !voltageException && !currentException && !socException &&
        !longitudeException && !latitudeException

      if (!allAreOk) exceptionCount += 1

      //返回非缺失，非异常数据
      allAreOk
    })


    (speedExceptionCount,
      mileageExceptionCount,
      voltageExceptionCount,
      currentExceptionCount,
      socExceptionCount,
      longitudeExceptionCount,
      latitudeExceptionCount,
      exceptionCount,
      validateValues)
  }


  //计算第一帧有效里程和最后一帧有效里程对应的数据
  def getFirstAndLastWithValidateMielage(values: Array[Realinfo]): (Option[Realinfo], Option[Realinfo]) = {
    @tailrec
    def doGetFirstWithValidateMileage(i: Int, firstWithValidateMileage: Realinfo): Realinfo = {
      if (i == values.length) firstWithValidateMileage
      else if (values(i).mileage.exists(_ >= 1)) {
        values(i)
      } else {
        doGetFirstWithValidateMileage(i + 1, firstWithValidateMileage)
      }
    }

    @tailrec
    def doGetLastWithValidateMileage(i: Int, lastWithValidateMileage: Realinfo): Realinfo = {
      if (i < 0) lastWithValidateMileage
      else if (values(i).mileage.exists(_ >=1)) values(i)
      else {
        doGetLastWithValidateMileage(i - 1, lastWithValidateMileage)
      }
    }

    if (values.nonEmpty) {
      val (firstWithValidateMileage, lastWithValidateMileage) = (
        doGetFirstWithValidateMileage(0, values.head),
        doGetLastWithValidateMileage(values.length - 1, values.last)
      )


      if (firstWithValidateMileage.mileage.exists(_>=1))
        (Some(firstWithValidateMileage), Some(lastWithValidateMileage))
      else (None, None)
    } else {
      (None, None)
    }
  }


  //数据缺失检测
  def checkEmpty(v: Realinfo): (Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean) = {
    (v.speed.isEmpty,
      v.mileage.isEmpty,
      v.totalVoltage.isEmpty,
      v.totalCharge.isEmpty,
      v.soc.isEmpty,
      v.longitude.isEmpty,
      v.latitude.isEmpty
    )
  }


  /**
    * 数据异常检测
    * */
  def checkException(v: Realinfo,
                     firstValidateMileage: Double,
                     lastValidateMielage: Double): (Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean) = {
    (!v.speed.exists(x => x >= speedLow && x <= speedHigh),
      !v.mileage.exists(x => x >= firstValidateMileage && x <= lastValidateMielage),
      !v.totalVoltage.exists(x => x > totalVoltageLow && x <= totalVoltageHigh),
      !v.totalCharge.exists(x => x >= totalChargeLow && x <= totalChargeHigh),
      !v.soc.exists(x => x >=socLow && x <= socHigh),
      !v.longitude.exists(x => x > longitudeLow && x < longitudeHigh),
      !v.latitude.exists(x => x > latitudeLow && x < latitudeHigh)
    )
  }

  /**
    * @param nonExceptionValues ,剔除缺失和异常的数据，也就是有效数据
    * @return (gps里程，gps重复次数，跳变里程，跳变窗口个数，跳变次数，连续电流扣除里程)
    **/
  def getBaseMileage(nonExceptionValues: Array[Realinfo]): (Double, Int, Double,Int, Int, Double) = {
    val (stepMileage, stepWindowNum,stepNum, gpsMileage, totalGpsRepeationNum) = getStepAndGpsMileage(nonExceptionValues)
    val continueCurrentStepMileage = getContinueCurrentStepMileage(nonExceptionValues)

    (gpsMileage, totalGpsRepeationNum, stepMileage, stepWindowNum,stepNum, continueCurrentStepMileage)
  }


  /**
    * 计算跳变里程和gps里程
    *
    * @param validateValues 剔除掉缺失和异常之后的有效数据。
    * @return (跳变里程，跳变窗口个数，跳变总次数，gps里程，gps重复次数)
    **/
  def getStepAndGpsMileage(validateValues: Array[Realinfo]): (Double, Int, Int,Double, Int) = {

    /***
      * @param indexOfEndStepMileage 跳变窗口内最后一个数据的索引。
      * */
    @tailrec
    def doGetStepMileageAndGpsMileage(i:Int,indexOfStartStepMileage:Int,indexOfEndStepMileage:Int,stepMileage:Double, stepNum:Int,gpsMileage:Double,gpsRepeationNum:Int):(Int,Int,Double,Int,Double,Int)={
     if((i+1)==validateValues.length){
       (indexOfStartStepMileage,indexOfEndStepMileage,stepMileage,stepNum,gpsMileage,gpsRepeationNum)
     }else{
       val curValue = validateValues(i)
       val nextValue = validateValues(i + 1)
       //计算gps重复次数，重复次数太多了，gps里程就很低。
       val gpsRepeation = if (Math.abs(curValue.longitude.get - nextValue.longitude.get) <= 0.000001D &&
         Math.abs(curValue.longitude.get - nextValue.longitude.get) <= 0.000001D)
         1
       else 0

       val mileageDiff = nextValue.mileage.get - curValue.mileage.get
       val gpsDiff=GpsDistance.getDistanceKm(curValue.longitude.get, curValue.latitude.get, nextValue.longitude.get, nextValue.latitude.get)

       if (Math.abs(mileageDiff) > stepMileageThreshold)
         doGetStepMileageAndGpsMileage(
           i+1,
           indexOfStartStepMileage,
           i+1,
           stepMileage+mileageDiff,
           stepNum+1,
           gpsMileage+gpsDiff,
           gpsRepeationNum+gpsRepeation)
       else
         (indexOfStartStepMileage,indexOfEndStepMileage,stepMileage,stepNum,gpsMileage+gpsDiff,gpsRepeationNum)
     }
    }

    @tailrec
    def getAllStepMileageAndGpsMileage(i: Int, totalStepMileage: Double, stepWindowNum:Int,totalStepNum: Int, totalGpsMileage: Double, totalGpsRepeationNum: Int): (Double, Int,Int, Double, Int) = {
      if (i >= validateValues.length) {
        (totalStepMileage, stepWindowNum,totalStepNum, totalGpsMileage, totalGpsRepeationNum)
      } else {
        val (indexOfStartStepMileage, indexOfEndStepMileage, stepMileage, stepNum, gpsMileage, gpsRepeationNum) = doGetStepMileageAndGpsMileage(i, i, i, 0, 0, 0, 0)

        val step=if (indexOfStartStepMileage != indexOfEndStepMileage) {
          val detail = newDetail(validateValues, indexOfStartStepMileage, indexOfEndStepMileage, stepMileage)
          stepMileageWindows.append(detail)
          1
        }else 0

        getAllStepMileageAndGpsMileage(
          indexOfEndStepMileage + 1,
          totalStepMileage + stepMileage,
          stepWindowNum+step,
          totalStepNum + stepNum+step,
          totalGpsMileage + gpsMileage,
          totalGpsRepeationNum + gpsRepeationNum
        )
      }
    }

    getAllStepMileageAndGpsMileage(0, 0, 0,0, 0, 0)
  }

  /**
    * 计算连续电流扣除里程
    *
    * @param validateValues 剔除掉缺失和异常之后的有效数据。
    * @return ( 连续电流扣除里程)
    **/
  def getContinueCurrentStepMileage(validateValues: Array[Realinfo]): Double = {
    @tailrec
    def getMileageInMaxContinueCurent(fromIndex: Int, continueLenght: Int, continueMileage: Double): (Double, Int) = {
      val i = fromIndex + continueLenght

      if (i == validateValues.length - 1) {
        if (continueLenght >= continueCurrentWindowMinLength) (continueMileage, i)
        else (0, fromIndex)
      } else if (validateValues(i).totalCharge.get == validateValues(i + 1).totalCharge.get) {
        getMileageInMaxContinueCurent(fromIndex,
          continueLenght + 1,
          continueMileage + validateValues(i + 1).mileage.get - validateValues(i).mileage.get)
      } else {
        if ((continueLenght + 1) >= continueCurrentWindowMinLength) (continueMileage, i)
        else (0, fromIndex)
      }
    }

    @tailrec
    def doGetContinueCurrentStepMileage(i: Int, totalContinueCurrentStepMileage: Double): Double = {
      if (i == validateValues.length - 1) totalContinueCurrentStepMileage
      else if (validateValues(i).totalCharge.get >continueCurrentWindowThresholdA) {
        val (continueCurrentStepMileage, lastContinue) = getMileageInMaxContinueCurent(i, 0, 0)
        //发现了一个新的满足条件的连续电流窗口
        if (i != lastContinue) {
          continuCurrentStepMileageWindows.append(newDetail(validateValues,i,lastContinue,continueCurrentStepMileage))
          doGetContinueCurrentStepMileage(lastContinue, totalContinueCurrentStepMileage + continueCurrentStepMileage)
        } else {
          doGetContinueCurrentStepMileage(i + 1, totalContinueCurrentStepMileage)
        }
      } else {
        doGetContinueCurrentStepMileage(i + 1, totalContinueCurrentStepMileage)
      }
    }

    doGetContinueCurrentStepMileage(0, 0)
  }


  def newDetail(values:Array[Realinfo],startIndex:Int,endIndex:Int,mileage:Double): String = {
//    val newContinueCurrentWindow = s"${Utils.formatDate(new Date(values(startIndex).time), "yyyyMMddHHmmss")}," +
//      s"${Utils.formatDate(new Date(values(endIndex).time), "yyyyMMddHHmmss")}," +
//    s"${Utils.formatDate(new Date(values(startIndex).time), "yyyyMMddHHmmss")}," +
//      s"${Utils.formatDate(new Date(values(endIndex).time), "yyyyMMddHHmmss")}," +
      s"${values(startIndex).time},"+
      s"${values(endIndex).time},"+
      s"${endIndex - startIndex + 1}," +
      s"${values(startIndex).mileage.get},"+
      s"${values(endIndex).mileage.get},"+
      s"${mileage}"

  }

  //输出连续电流窗口扣除电压的调试信息。
  private def continueCurrentWindowString: String = {
    val windowString = new StringBuilder
    var i = 0

    while (i < continuCurrentStepMileageWindows.length) {
      val window = continuCurrentStepMileageWindows(i)
      windowString.append(s"窗口$i:${window.toString} ${Utils.newLine}")
      i += 1
    }

    s"""
      连续电流窗口个数:${continuCurrentStepMileageWindows.length}
      连续电流条件:电流>=${continueCurrentWindowThresholdA}A,窗口长度>=$continueCurrentWindowMinLength
      $windowString
    """.stripMargin
  }
}

object MileageComputer {

}

package com.bitnei.report.operationIndex

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.mileage._
import org.scalatest.{FunSuite, Suite}

/*
* created by wangbaosheng on 2017/11/21
*/
class MileageCounterTest extends FunSuite {
  test("test empty and exception") {
    val config = new StateConf
    val computer = new MileageComputer(config)

    def assertAllAreEmpty(): Unit = {
      val result = computer.getValidateValues(Array(Realinfo("vid1", "vin1", 0, None, None, None, None, None, None, None, None)))
      //第一条有效仪表里程，最后一条有效仪表里程，统计值，剔除缺失和异常值后的数据)
      assert(result._1.isEmpty)
      assert(result._2.isEmpty)
      assert(result._3 == Counter(1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))
      assert(result._4.isEmpty)
    }

    def assertSomeisEmpty(): Unit = {
      val result = computer.getValidateValues(Array(Realinfo("vid1", "vin1", 0, None, Some(10), None, None, None, None, None, None)))
      //第一条有效仪表里程，最后一条有效仪表里程，统计值，剔除缺失和异常值后的数据)
      assert(result._1.isEmpty)
      assert(result._2.isEmpty)
      assert(result._3 == Counter(1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))
      assert(result._4.isEmpty)
    }


    def assertAllAreNoneEmpty(): Unit = {
      val result = computer.getValidateValues(Array(
        Realinfo("vid1", "vin1", 0,
          mileage = Some(1000),
          totalVoltage = Some(10),
          totalCharge = Some(10),
          soc = Some(10),
          speed = Some(10),
          longitude = Some(80),
          latitude = Some(50),
          state = None),
        Realinfo("vid1", "vin1", 0,
          mileage = Some(1100),
          totalVoltage = Some(10),
          totalCharge = Some(10),
          soc = Some(10),
          speed = Some(10),
          longitude = Some(80),
          latitude = Some(50), state = None)
      ))
      //第一条有效仪表里程，最后一条有效仪表里程，统计值，剔除缺失和异常值后的数据)
      assert(result._1.get.mileage.get == 1000)
      assert(result._2.get.mileage.get == 1100)
      assert(result._3 == Counter(2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1))
      assert(result._4.length == 2)
    }


    def assertSomeIsException(): Unit = {
      val result = computer.getValidateValues(Array(
        Realinfo("vid1", "vin1", 0,
          mileage = Some(1000),
          totalVoltage = Some(10),
          totalCharge = Some(10),
          soc = Some(10),
          speed = Some(10),
          longitude = Some(80),
          latitude = Some(50),
          state = None),
        Realinfo("vid1", "vin1", 0,
          mileage = Some(1100),
          totalVoltage = Some(10),
          totalCharge = Some(10),
          soc = Some(10),
          speed = Some(10),
          longitude = Some(0), //异常值
          latitude = Some(50),
          state = None)))
      //第一条有效仪表里程，最后一条有效仪表里程，统计值，剔除缺失和异常值后的数据)
      assert(result._1.get.mileage.contains(1000))
      assert(result._2.get.mileage.contains(1100))
      assert(result._3 == Counter(2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0.5))
      assert(result._4.length == 1)
    }

    assertAllAreEmpty()

    assertSomeisEmpty()

    assertAllAreNoneEmpty()
    assertSomeIsException()
  }


  test("test step mileage and continue current mileage") {
    val config = new StateConf
    val computer = new MileageComputer(config)


    val values = Array(
      newValue(1000, 10),
      newValue(1003, 11),
      newValue(1002, 10),
      newValue(1003, 10),
      newValue(1000, 10),
      newValue(1004, 10),
      newValue(1009, 11))

    //测试跳变里程
    //   里程序列：1000，1003（异常），1002，1003，1000，1004(异常),1009(异常)
    //测试连续电流扣除里程
    //   <aoc,里程>序列：<10,1000>，<13,1003>，<10,1002>（开始），<10,1003>，<10,1000>，<10,1004>(结束),<11，1009>
    // (gps里程，跳变里程，连续电流扣除里程)
    val (gpsMileage, totalGpsRepeationNum, stepMileage, stepWindowNum, stepNum, continueCurrentStepMileage) = computer.getBaseMileage(values)

    assert(stepNum == 6)
    assert(stepWindowNum == computer.getStepMileageWindows.length)

    val stepMileageWindows = computer.getStepMileageWindows
    stepMileageWindows(0) == "0,0,2,1000.0,1003.0,3.0"
    stepMileageWindows(1) == "0,0,4,1003.0,1009.0,6.0"

    assert(stepMileage == 9)
    assert(continueCurrentStepMileage == 2)
  }

  def newValue(mileage: Int, current: Int): Realinfo = {
    Realinfo("vid1", "vin1", 0,
      mileage = Some(mileage),
      totalVoltage = Some(10),
      totalCharge = Some(current),
      soc = Some(10),
      speed = Some(10),
      longitude = Some(80),
      latitude = Some(50),
      state = Some(1))
  }

  def newEmptyValue(): Realinfo = {
    Realinfo("vid1", "vin1", 0,
      mileage = None,
      totalVoltage = Some(10),
      totalCharge = Some(10),
      soc = None,
      speed = Some(10),
      longitude = Some(80),
      latitude = Some(50),
      state = Some(1))
  }


  test("test compute") {
    val config = new StateConf
    val computer = new MileageComputer(config)


    val values = Array(
      newValue(1000, 10),
      newValue(1003, 11),
      newValue(1002, 10),
      newValue(1003, 10),
      newValue(1000, 10),
      newValue(1004, 10),
      newValue(1009, 11),
      newValue(1010, 30),
      newEmptyValue(),
      newValue(1011, 10),
      newValue(1012, 10),
      newValue(1013, 10),
      newValue(1015, 10),
      newValue(1017, 11),
      newValue(1019, 10),
      newValue(1021, 11),
      newValue(1023, 10),
      newValue(1025, 11)
    )

    val result = computer.compute(values)
    assert(result.firstValidateMileage.contains(values.head.mileage.get))
    assert(result.lastValidateMileage.contains(values.last.mileage.get))
    assert(result.onlineMileage == result.lastValidateMileage.get - result.firstValidateMileage.get)

    assert(result.validateMileage == result.onlineMileage - result.stepMileage - result.continueCurrentStepMileage)
    assert(result.onlineMileageAndValidateMileageRelativeError == MileageUtil.onlineMileageAndValidateMileageRelativeError(result.onlineMileage, result.validateMileage))
    assert(result.onlineMileageAndGPSMileageRelativeError == MileageUtil.onlineMileageAndGPSMileageRelativeError(result.onlineMileage, result.gpsMileage))

    assert(result.checkMileage == 0)
    println(MileageCheckPath.parsePath(result, result.checkMileagePath))
    println(result.toString)


    //测试跳变里程
    assert(result.stepMileage == 9)
    assert(result.stepNum == 6)
    assert(result.stepWindowNum == computer.getStepMileageWindows.length && result.stepWindowNum == 2)

    val stepMileageWindows = result.stepMileageDetail
    stepMileageWindows(0) == "0,0,2,1000.0,1003.0,3.0"
    stepMileageWindows(1) == "0,0,4,1003.0,1009.0,6.0"


    //测试连续电流里程
    assert(result.continueCurrentStepMileage == 6)
    assert(result.continueCurrentStepMileageDetail.length == result.continueCurrentStepMileageNum && result.continueCurrentStepMileageNum == 2)
    assert(result.continueCurrentStepMileageDetail(0) == "0,0,4,1002.0,1004.0,2.0")
    assert(result.continueCurrentStepMileageDetail(1) == "0,0,4,1011.0,1015.0,4.0")

    //测试技术器
    assert(result.counter.totalCount == values.length)
    assert(result.counter.validateCount == values.length - 1)


    assert(result.counter == Counter(values.length, 0, 1, 0, 0, 1, 0, 0, 0, 1, result.validateCount, 0, 0, 0, 0, 0, 0, 0, 0, result.validateCount, result.validateCount.toDouble / result.counter.totalCount.toDouble))
  }


  test("multi month mileage") {
    val values = Array(
      MileageResult(
        vin = "",
        vid = "",
        licensePlate = None,
        firstValidateMileage = None,
        lastValidateMileage = None,
        firstTimeWithValidateMileage = None,
        lastTimeWithValidateMileage = None,
        onlineMileage = 0,
        stepMileage = 100,
        continueCurrentStepMileage = 200,
        validateMileage=1000-100-200,
        gpsMileage = 1000,
        checkMileage = 1000,
        stepNum = 1,
        stepWindowNum = 1,
        continueCurrentStepMileageNum =1,
        counter = Counter(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
      ),
      MileageResult(
        vin = "",
        vid = "",
        licensePlate = None,
        firstValidateMileage = Some(1000),
        lastValidateMileage = Some(2000),
        firstTimeWithValidateMileage = None,
        lastTimeWithValidateMileage = None,
        onlineMileage = 1000,
        stepMileage = 100,
        continueCurrentStepMileage = 200,
        validateMileage=1000-100-200,
        gpsMileage = 1000,
        checkMileage = 1000,
        stepNum = 1,
        stepWindowNum = 1,
        continueCurrentStepMileageNum =1,
        counter = Counter(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
      ),
      MileageResult(
        vin = "",
        vid = "",
        licensePlate = None,
        firstValidateMileage = None,
        lastValidateMileage = None,
        firstTimeWithValidateMileage = None,
        lastTimeWithValidateMileage = None,
        onlineMileage = 0,
        stepMileage = 100,
        continueCurrentStepMileage = 200,
        validateMileage=1000-100-200,
        gpsMileage = 1000,
        checkMileage = 1000,
        stepNum = 1,
        stepWindowNum = 1,
        continueCurrentStepMileageNum =1,
        counter = Counter(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
      ),
      MileageResult(
        vin = "",
        vid = "",
        licensePlate = None,
        firstValidateMileage = Some(2000),
        lastValidateMileage = Some(3000),
        firstTimeWithValidateMileage = None,
        lastTimeWithValidateMileage = None,
        onlineMileage = 1000,
        stepMileage = 100,
        continueCurrentStepMileage = 200,
        validateMileage=1000-100-200,
        gpsMileage = 1000,
        checkMileage = 1000,
        stepNum = 1,
        stepWindowNum = 1,
        continueCurrentStepMileageNum =1,
        counter = Counter(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
      ),
      MileageResult(
        vin = "",
        vid = "",
        licensePlate = None,
        firstValidateMileage = Some(3000),
        lastValidateMileage = Some(4000),
        firstTimeWithValidateMileage = None,
        lastTimeWithValidateMileage = None,
        onlineMileage = 1000,
        stepMileage = 100,
        continueCurrentStepMileage = 200,
        validateMileage=1000-100-200,
        gpsMileage = 1000,
        checkMileage = 1000,
        stepNum = 1,
        stepWindowNum = 1,
        continueCurrentStepMileageNum =1,
        counter = Counter(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
      ),
      MileageResult(
        vin = "",
        vid = "",
        licensePlate = None,
        firstValidateMileage = None,
        lastValidateMileage = None,
        firstTimeWithValidateMileage = None,
        lastTimeWithValidateMileage = None,
        onlineMileage = 0,
        stepMileage = 100,
        continueCurrentStepMileage = 200,
        validateMileage=1000-100-200,
        gpsMileage = 1000,
        checkMileage = 1000,
        stepNum = 1,
        stepWindowNum = 1,
        continueCurrentStepMileageNum =1,
        counter = Counter(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
      ))



    val r=new MileageResultMonidBaseOnHeadFoldable().foldLeft(values,new MileageResultMonidBaseHead(values.head))

    assert(r.onlineMileage==3000)
    assert(r.firstValidateMileage.contains(1000))
    assert(r.lastValidateMileage.contains(4000))
    assert(r.stepMileage==600)
    assert(r.continueCurrentStepMileage==1200)
    assert(r.stepNum==6)
    assert(r.stepWindowNum==6)

   // new MileageResultFoldable().foldLeft(Array.empty[MileageResult],new MileageResultMonid(values.))
  }

  test("multi one mileage") {
    val values = Array(
      MileageResult(
        vin = "",
        vid = "",
        licensePlate = None,
        firstValidateMileage = Some(1000),
        lastValidateMileage = Some(2000),
        firstTimeWithValidateMileage = None,
        lastTimeWithValidateMileage = None,
        onlineMileage = 1000,
        stepMileage = 100,
        continueCurrentStepMileage = 200,
        validateMileage=1000-100-200,
        gpsMileage = 1000,
        checkMileage = 1000,
        stepNum = 1,
        stepWindowNum = 1,
        continueCurrentStepMileageNum =1,
        counter = Counter(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
      ))



    val r=new MileageResultMonidBaseOnHeadFoldable().foldLeft(values,new MileageResultMonidBaseHead(values.head))

    assert(r.onlineMileage==1000)
    assert(r.firstValidateMileage.contains(1000))
    assert(r.lastValidateMileage.contains(2000))
    assert(r.stepMileage==100)
    assert(r.continueCurrentStepMileage==200)
    assert(r.stepNum==1)
    assert(r.stepWindowNum==1)

    println("""
      |h
      |hh
    """.stripMargin)
    // new MileageResultFoldable().foldLeft(Array.empty[MileageResult],new MileageResultMonid(values.))
  }
}
package com.bitnei.report.dayreport.distribution

import com.bitnei.report.distribute.Distribution

class MileageDistribution extends Distribution {
  val start: Int = 0
  val end: Int = 1000

  override def interval = 100

  //0到100Km分布，间距为10Km，用户输入为km
  private val mileage0_1000M: Array[Int] = Array.fill(10)(0)
  private val mileage1000_3000M: Array[Int] = Array.fill(4)(0)
  private var mileage3000M: Int = 0

  def add(mileageM: Int) {
    if (start <= mileageM && mileageM <= end) {
      mileage0_1000M(index(mileageM)) += 1
    } else if (1000 < mileageM && mileageM <= 3000) {
      if (mileageM > 1000 && mileageM <= 1500) mileage1000_3000M(0) += 1
      else if (mileageM > 1500 && mileageM <= 2000) mileage1000_3000M(1) += 1
      else if (mileageM > 2000 && mileageM <= 2500) mileage1000_3000M(2) += 1
      else if (mileageM > 2500 && mileageM < 3000) mileage1000_3000M(3) += 1
    }
    else mileage3000M += (if (mileageM > 3000) 1 else 0)
  }

  override def getDistribution: Array[Int] = mileage0_1000M ++ mileage1000_3000M ++ Array(mileage3000M)
}

object MileageDistribution {
  def default: Array[Int] = Array.fill(15)(0)
}package tempjob

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{FileWriter, Utils}
import com.bitnei.sparkhelper.HbaseHelper
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.{Delete, Result, Scan}
import org.apache.hadoop.hbase.util.Bytes

import scala.collection.mutable.ArrayBuffer
import scala.util.{Failure, Success, Try}

/*
* created by wangbaosheng on 2017/12/4
*/

case class MonthCoord(vid: String, vin:String,date: String, mielage: Double, hours: Double, days: Int, coords: Array[Array[Array[Double]]])

object MileageErrorCheck extends  Logging {
  val stateConf = new StateConf
  val accMileageThreshold = stateConf.getOption("accMileageThreshold").map(_.toInt).getOrElse(1000000)
  val totalMileageThreshold = stateConf.getOption("totalMileageThreshold").map(_.toInt).getOrElse(accMileageThreshold)

  val errorRowkeyFilePath = stateConf.getOption("output.directory").getOrElse("/tmp/mileage_check") + "/error"
  val reasonRowkeyFilePath = stateConf.getOption("output.directory").getOrElse("/tmp/mileage_check") + "/reason"

  def main(args: Array[String]): Unit = {
    stateConf.add(args)
    run()
  }

  def run(): Unit = {
    logInfo(
      s"""
        |accMileageThreshold:[0,$accMileageThreshold]
        |totalMileageThreshold:[0,$totalMileageThreshold]
      """.stripMargin)

    stateConf.getOption("check") match {
      case Some("true") =>
        logInfo("begin checking")
        check()
      case _ => logWarning("no checking")
    }



    stateConf.getOption("deleteError") match {
      case Some("true") => deleteErrorIfNeed()
      case Some("false") => logInfo("the deleteError=false,exited.")
      case _ => logError("the deleteError param error.")
    }
  }


  def check(): Unit = {
    val jsonParser = new ObjectMapper()
    val errorWriter = new FileWriter(errorRowkeyFilePath)
    val reasonWriter = new FileWriter(reasonRowkeyFilePath)
    var errorCount = 0


    var num=0

    readHbase(v => {
      val (row, accMileage, totalMileage, coord) = getRow(v)
      logInfo(s"begin process the $num th,rowkey=$row")
      hasError(jsonParser, row, accMileage, totalMileage, coord) match {
        case (true, reason) =>
          errorCount += 1
          println(s"$row error......")
          errorWriter.write(s"$row ${Utils.newLine}")
          reasonWriter.write(s"$row,${reason} ${Utils.newLine}")
        case _ =>
      }
      num+=1
    })

    errorWriter.close()
    reasonWriter.close()
  }


  def hasError(mapper: ObjectMapper, row: String, accMileage: String, totalMileage: String, coord: String): (Boolean, String) = {
    var coordError = false
    var reason = new StringBuilder

    var accMileageError = Try {
      accMileage.toDouble
    } match {
      case Success(v) if (v > 0 && v <= accMileageThreshold) =>
        false
      case Success(v) =>
        reason.append(s"accMileage=$accMileage,")
        true
      case Failure(e) =>
        reason.append(s"accMileage=$accMileage,")
        true
    }


    val totalMileageError = Try {
      totalMileage.toDouble
    } match {
      case Success(v) if (v > 0 && v <= totalMileageThreshold) =>
        false
      case Success(v) =>
        reason.append(s"totalMileage=$totalMileage,")
        true
      case Failure(e) =>
        reason.append(s"totalMileage=$totalMileage,")
        true
    }


    val (monthHour, monthDayNum, monthMileage) = asMonthCoord(mapper, coord)


    if (monthHour > 31 * 24) {
      coordError = true
      reason.append(s"hour=$monthHour,")
    }

    if (monthDayNum > 31) {
      coordError = true
      reason.append(s"dayNum=$monthHour,")
    }

    if (monthMileage > accMileageThreshold) {
      coordError = true
      reason.append(s"monthMileage=$monthMileage,")
    }

    (accMileageError || totalMileageError || coordError, reason.toString())
  }


  def asMonthCoord(mapper: ObjectMapper, jsonValue: String): (Double, Double, Double) = {
    try {
      val v = mapper.readTree(jsonValue)

      val hours = v.get("hours").asDouble()
      val dayNum = v.get("days").asDouble()
      val mileage = v.get("mielage").asDouble()
      (hours, dayNum, mileage)
    } catch {
      case e: Exception =>
        (-1, -1, -1)
    }
  }


  def readHbase(f: (Result) => Unit): Unit = {
    val con = HbaseHelper.getConnection(
      stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89"),
      stateConf.getOption("hbase.zkport").getOrElse("2181"))

    val table = con.getTable(TableName.valueOf("mileage_check_coords"))

    val scan = new Scan
    val scanner = table.getScanner(scan)
    val iter = scanner.iterator()
    while (iter.hasNext) {
      val v = iter.next()
      f(v)
    }
    scanner.close()
    table.close()
    con.close()
  }

  def getRow(v: Result): (String, String, String, String) = {
    val row = Bytes.toString(v.getRow)
    var accMileage = ""
    var totalMileage = ""
    var coord = ""

    v.rawCells().foreach(cell => {
      val qualifier = Bytes.toString(cell.getQualifierArray, cell.getQualifierOffset, cell.getQualifierLength)

      if (qualifier == "accMileage") {
        accMileage = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)
      } else if (qualifier == "totalMileage") {
        totalMileage = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)

      } else if (qualifier == "coord") {
        coord = Bytes.toString(cell.getValueArray, cell.getValueOffset, cell.getValueLength)
      }
    })


    (row, accMileage, totalMileage, coord)
  }


  def deleteErrorIfNeed(): Unit = {

    val con = HbaseHelper.getConnection(
      stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89"),
      stateConf.getOption("hbase.zkport").getOrElse("2181"))

    val table = con.getTable(TableName.valueOf("mileage_check_coords"))


    def delete(rowKey: String): Unit = {
      val deleteCommand = new Delete(Bytes.toBytes(rowKey))
      table.delete(deleteCommand)
    }


    Utils.readAllLines(errorRowkeyFilePath).filter(_.trim.nonEmpty).foreach(rowKey => {
      println(s"deleting $rowKey")
      delete(rowKey.trim)
    })

    table.close()
    con.close()
  }
}
package com.bitnei.alarm.generator

import com.bitnei.alarm.{ClosedResult, Detail, DetailModel}
import com.bitnei.common.constants.Constant

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 动态里程核查指标算法 （能量消耗率 快充倍率 电池衰减率）
  *
  * create 2018-04-04 9:27
  *
  */
object MileageRatioGenerator {


  /**
    * 计算能量消耗率
    * @param sortedDetails
    * @return
    */
  //计算(充电，行驶，充电)闭包值
  def computeCrc(sortedDetails: Array[DetailModel]): Array[ClosedResult] = {
    val powerConsumptionResult = new ArrayBuffer[ClosedResult]()

    //遍历每个(充电-行驶-充电)闭包
    foreachCRC(sortedDetails, (start, end) => {
      //闭包内第一个行驶
      val firstRun = sortedDetails(start + 1)
      //闭包内最后一个行驶
      val lastRun = sortedDetails(end - 1)
      //闭包soc差值,因为行驶的soc是递减的，所以这里用开始-结束
      val socDiff = firstRun.startSoc - lastRun.endSoc
      //闭包里程差值
      val mileageDiff = lastRun.stopMileage - firstRun.startMileage
      //最大峰值电流
      val (maxTotalCurrent, minTotalCurrent) = getMaxAndMinTotalCurrent(sortedDetails, start, end)

      //车辆能量消耗率
      val powerConsumption = if (mileageDiff <= 0) 0 else socDiff * firstRun.standendPower / mileageDiff

      var chargePower = 0D
      (start + 1 to end - 1).foreach(i => {
        chargePower += sortedDetails(i).chargePower
      })

      powerConsumptionResult.append(
        ClosedResult(
          vid = firstRun.vid,
          startTime = sortedDetails(start).startTime,
          endTime = sortedDetails(end).endTime,
          closedType = "crc",
          socDiff = socDiff,
          mileageDiff = mileageDiff,
          powerConsumption = powerConsumption,
          maxTotalCurrent = maxTotalCurrent,
          minTotalCurrent = minTotalCurrent,
          quickChargeFactor = 0,
          chargePower = chargePower,
          m = 0))
    })
    powerConsumptionResult.toArray
  }




  //计算(行驶,充电,行驶) 闭包值
  def computeRcr(sortedDetails: Array[DetailModel]): Array[ClosedResult] = {
    val result = new ArrayBuffer[ClosedResult]()

    //遍历每个(充电-行驶-充电)闭包
    foreachRCR(sortedDetails, (start, end) => {
      //闭包内第一个行驶
      val firstCharge = sortedDetails(start + 1)
      //闭包内最后一个行驶
      val lastCharge = sortedDetails(end - 1)
      //闭包soc差值
      val socDiff = lastCharge.endSoc - firstCharge.startSoc
      //闭包里程差值
      val mileageDiff = lastCharge.stopMileage - firstCharge.startMileage
      //最大峰值电流
      val (maxTotalCurrent, minTotalCurrent) = getMaxAndMinTotalCurrent(sortedDetails, start, end)

      //车辆能量消耗率
      val powerConsumption = if (mileageDiff <= 0) 0 else socDiff * firstCharge.standendPower / mileageDiff

      //计算快充倍率
      val quickChargeFactor = maxTotalCurrent / 1

      var chargePower = 0D
      (start + 1 to end - 1).foreach(i => {
        chargePower += sortedDetails(i).chargePower
      })
      //计算电池衰减=soc差值/充电容量
      val m = chargePower / socDiff
      result.append(
        ClosedResult(
          vid = firstCharge.vid,
          startTime = sortedDetails(start).startTime,
          endTime = sortedDetails(end).endTime,
          closedType = "rcr",
          socDiff = socDiff,
          mileageDiff = mileageDiff,
          powerConsumption = 0,
          maxTotalCurrent = maxTotalCurrent,
          minTotalCurrent = minTotalCurrent,
          quickChargeFactor = quickChargeFactor,
          chargePower = chargePower,
          m = m))
    })
    result.toArray
  }


  //遍历每一个RCR闭包。
  //每一个RCR闭包长度至少为3.
  def foreachRCR[T <: Detail](details: Seq[T], f: (Int, Int) => Unit): Unit = {
    foreachClosed(Constant.TravelState, Constant.ChargeState)(details, f)
  }

  //遍历每一个CRC闭包。
  //每一个CRC闭包长度至少为3.
  def foreachCRC[T <: Detail](details: Seq[T], f: (Int, Int) => Unit): Unit = {
    foreachClosed(Constant.ChargeState, Constant.TravelState)(details, f)
  }


  /**
    * @param closed  闭包开始和结束元素的状态。
    * @param mid     闭包中间元素的状态
    * @param details 输入集合。
    * @param f       一个用于处理闭包的函数。 第一个参数是闭包开始下标，第二个参数是闭包结束下标
    *
    *                在details序列中查找闭包(closed)，闭包是detail的一个子序列，这个子序列满足如下条件：
    * 1.闭包的第一个元素和最后一个的状态相同。
    * 2.闭包的中间元素状态相同
    *                例如(充电，行驶，行驶，行驶，充电)就是一个闭包，这个闭包我们叫做crc闭包。c是充电的意思，r是行驶的意思。
    *                再比如，(行驶，充电，行驶)也是一个闭包,z这个闭包叫做rcr.
    *                事实上，（行驶，行驶，行驶）也是一个闭包，只不过这个闭包中的所有detail的状态都是行驶。
    **/
  def foreachClosed[T <: Detail](closed: String, mid: String)(details: Seq[T], f: (Int, Int) => Unit): Unit = {
    if (details.size < 3) {
      //do nothing
    } else {
      //闭包开始索引
      var start = -1
      //闭包结束索引
      var end = -1

      var i = 0

      while (i < details.size - 1) {
        if (details(i).state == closed && details(i + 1).state == mid) {
          start = i
        } else if (details(i).state == mid && details(i + 1).state == closed) {
          end = i + 1
        }

        //如果有闭包开始索引和结束索引，那么[start,end]内的所有数据形成了一个闭包。
        //然后调用f来处理这个闭包。
        if (start != -1 && end != -1) {
          f(start, end)

          start = -1
          end = -1
        }
        i += 1
      }
    }
  }


  //获取最大/最小总电流
  def getMaxAndMinTotalCurrent(details: Array[DetailModel], start: Int, end: Int): (Double, Double) = {
    //最大峰值电流
    var maxTotalCurrent: Double = 0

    //最小峰值电流
    var minTotalCurrent: Double = 10000

    //计算峰值电流
    (start + 1 until end).foreach(i => {
      val detail = details(i)
      if (detail.totalCharge <= minTotalCurrent) minTotalCurrent = detail.totalCharge
      else if (detail.totalCharge >= maxTotalCurrent) maxTotalCurrent = detail.totalCharge
    })
    (maxTotalCurrent, minTotalCurrent)
  }


}
package com.bitnei.report.common.mileage

import java.util.Date

import com.bitnei.report.common.utils.Utils

/*
* created by wangbaosheng on 2017/11/10
*/


///核算=在线里程-跳变里程，连续电流
/**
  * @param vin 车辆vin
  * @param checkMileage 核算里程
  * @param onlineMileage 仪表里程
  * @param validateMileage 有效里程
  * @param gpsMileage 轨迹里程
  * @param stepMileage 跳变里程
  * @param continueCurrentStepMileage 连续电流扣除里程
  */
case class MileageResult(
                          vid:String,
                          vin:String,
                        //车牌号
                          licensePlate:Option[String]=None,

                          //核算里程
                          checkMileage:Double=0,
                          checkMileagePath:Int=0,
                          firstValidateMileage:Option[Double]=None, //第一帧有效仪表里程

                        //最后一帧有效里程
                          lastValidateMileage:Option[Double]=None, //
                          //第一帧有效里程对应的时间
                          firstTimeWithValidateMileage:Option[Long]=None,

                          //最后一帧有效里程对应的时间
                          lastTimeWithValidateMileage:Option[Long]=None,

                          onlineMileage:Double=0, //上线里程
                          validateMileage:Double=0, //有效里程

                          totalGpsRepeationNum:Int=0,
                          gpsMileage:Double=0, //轨迹里程

                          onlineMileageAndValidateMileageRelativeError:Double=0,
                          onlineMileageAndGPSMileageRelativeError:Double=0,

                          stepMileageThreshold:Double=0,
                          stepWindowNum:Int=0,//跳变窗口个数
                          stepNum:Int=0,//跳变计次
                          stepMileage:Double=0, //跳变里程


                          continueCurrentStepMileage:Double=0, //连续电流扣除里程
                          continueCurrentStepMileageNum:Int=0,//连续电流扣除计次
                          continueCurrentWindowThreshold:Double=0,//连续电流扣除阈值
                          continueCurrentWindowMinLength:Int=0,//连续电流窗口最小长度

                          stepMileageDetail:Array[String]=Array.empty[String],//跳变里程明细
                          continueCurrentStepMileageDetail:Array[String]=Array.empty[String],//连续电流跳变里程明细

                          //核算里程阈值
                          checkMileageThreshold:CheckMileageThreshold=CheckMileageThreshold(1,1,0.6,0.6),
                          counter:Counter=Counter(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)//缺失，异常等统计信息。
                               ) {
  override def toString: String = humanString

  def humanString: String = {
    s"""
        vid:$vid
      vin:$vin
      车牌号:${licensePlate.getOrElse("")}
      第一帧:${firstTimeWithValidateMileage.map(v=>Utils.formatDate(new Date(v),"yyyy-MM-dd HH:mm:ss")).getOrElse("")},${firstValidateMileage.getOrElse("")}Km
      最后一帧:${lastTimeWithValidateMileage.map(v=>Utils.formatDate(new Date(v),"yyyy-MM-dd HH:mm:ss")).getOrElse("")},${lastValidateMileage.getOrElse("")}Km
      上线里程:$onlineMileage
      有效里程:$validateMileage
      轨迹里程:$gpsMileage ${if (gpsRepeationPercent >= 0.7D) "警告：GPS重复率高于70%" else ""}
      核算里程:$checkMileage(原因:${MileageCheckPath.parsePath(this,checkMileagePath)})
      核算里程阈值:(${checkMileageThreshold.toString})
      在线里程和有效里程相对误差:${onlineMileageAndValidateMileageRelativeError}
      在线里程和轨迹里程相对误差:${onlineMileageAndGPSMileageRelativeError}
      跳变里程(>$stepMileageThreshold KM):$stepMileage ${if (stepPercent >= 0.7D) "警告:里程跳变率高于70%" else ""}
      连续电流扣除里程:$continueCurrentStepMileage
      数据准确率:${counter.okPercent}

      总条数:${counter.totalCount}
      有效率:$validatePercent  ${if (validatePercent <= 0.3D) "警告:数据有效率低于于30%" else ""}
      缺失条数:${counter.emptyCount}
      熄火条数(3201=2):${counter.flameoutCount}
      过滤缺失和熄火后:${counter.totalCount-counter.emptyCount-counter.flameoutCount}
      异常条数:${counter.exceptionCount}
      有效条数:$validateCount

      缺失比率${counter.emptyPercent} ${if (counter.emptyPercent >= 0.7D) "警告:数据缺失率高于70%" else ""}
      速度缺失条数:${counter.speedEmptyCount}
      里程缺失条数:${counter.mileageEmptyCount}
      电压缺失条数:${counter.voltageEmptyCount}
      电流缺失条数:${counter.currentEmptyCount}
      soc缺失条数 :${counter.socEmptyCount}
      经度缺失条数:${counter.longitudeEmptyCount}
      纬度缺失条数:${counter.latitudeEmptyCount}

      异常比率:${counter.exceptionPercent} ${if (counter.exceptionPercent >= 0.7D) "警告:数据异常率高于70%" else ""}
      速度异常条数:${counter.speedExceptionCount}
      里程异常条数:${counter.mileageExceptionCount}
      电压异常条数:${counter.voltageExceptionCount}
      电流异常条数:${counter.currentExceptionCount}
      有效电流区间:
      soc异常条数 :${counter.socExceptionCount}
      经度异常条数:${counter.longitudeExceptionCount}
      纬度异常条数:${counter.latitudeExceptionCount}

      轨迹里程详细信息：
      轨迹重复次数:$totalGpsRepeationNum
      轨迹重复率:$gpsRepeationPercent

      里程跳变详细信息：
      跳变里程:${stepMileage}
      里程跳变次数：${stepMileageDetail.length}
      里程跳变窗口个数:${stepWindowNum}
      里程跳变率:$stepPercent
       ${stepMileageDetail.reduceOption((a,b)=>s"$a ${Utils.newLine} $b").getOrElse("")}
      连续电流窗口信息：
      连续电流窗口个数：${continueCurrentStepMileageDetail.length}
      连续电流扣除里程：${continueCurrentStepMileage}
       ${continueCurrentStepMileageDetail.reduceOption((a, b) => s"$a ${Utils.newLine} $b").getOrElse("")}
      """.stripMargin
  }


  def validateCount: Int = counter.validateCount

  def validatePercent: Double = if (counter.totalCount == 0) 0 else validateCount.toDouble / counter.totalCount.toDouble

  def gpsRepeationPercent: Double = if (counter.totalCount == 0) 0 else totalGpsRepeationNum.toDouble / counter.totalCount.toDouble

  def stepPercent:Double=    if (counter.totalCount == 0) 0 else {stepMileageDetail.length}.toDouble / counter.totalCount.toDouble
}


case class Counter(
                    //数据总条数据
                    totalCount:Int,

                   //速度缺失个数
                    speedEmptyCount:Int,
                    mileageEmptyCount:Int,
                    voltageEmptyCount:Int,
                    currentEmptyCount:Int,
                    socEmptyCount:Int,
                    longitudeEmptyCount:Int,
                    latitudeEmptyCount:Int,
                    //熄火个数
                    flameoutCount:Int,
                    //缺失条数
                    emptyCount:Int,
                    nonEmptyCount:Int,

                   //速度异常个数
                    speedExceptionCount:Int,
                    mileageExceptionCount:Int,
                    voltageExceptionCount:Int,
                    currentExceptionCount:Int,
                    socExceptionCount:Int,
                    longitudeExceptionCount:Int,
                    latitudeExceptionCount:Int,
                    exceptionCount:Int,
                    nonExceptionCount:Int,

                   //数据有效率
                    okPercent:Double) {
  def emptyPercent: Double = emptyCount.toDouble / totalCount.toDouble

  def exceptionPercent: Double = exceptionCount.toDouble / totalCount.toDouble

  def validateCount=totalCount-emptyCount-exceptionCount-flameoutCount

  def checkEmptyCount: Boolean = {
    emptyCount == Array(speedEmptyCount,
      mileageEmptyCount,
      voltageEmptyCount,
      currentEmptyCount,
      socEmptyCount,
      longitudeEmptyCount,
      latitudeEmptyCount).max
  }

  def checkExceptionCount: Boolean = {
    exceptionCount == Array(speedExceptionCount,
      mileageExceptionCount,
      voltageExceptionCount,
      currentExceptionCount,
      socExceptionCount,
      longitudeExceptionCount,
      latitudeExceptionCount).max
  }
}

case class CheckMileageThreshold(A:Double,B:Double,C:Double,D:Double){
  override def toString: String = s"$A,$B,$C,$D"
}package com.bitnei.report.common.mileage

import com.bitnei.report.{EndoMonoid, FoldMonod}

/*
* created by wangbaosheng on 2017/12/14
* 对多个里程核查结果进行累计，该累加并不是完全foldable的。
* 该累计的初始元素是列表的第一个元素。
*/
class MileageResultMonidBaseHead(head:MileageResult) extends Serializable with FoldMonod[MileageResult,MileageResult] {
  override def zero(): MileageResult = head

  override def fold(a: MileageResult, b: MileageResult): MileageResult = {
    val onlineMileage = a.onlineMileage + (if (b.onlineMileage >= 0) b.onlineMileage else 0)
    val stepMileage = a.stepMileage + b.stepMileage
    val continueCurrentStepMileage = a.continueCurrentStepMileage + b.continueCurrentStepMileage
    val validateMileage = MileageUtil.validateMileage(onlineMileage ,stepMileage,continueCurrentStepMileage)
    val gpsMileage = a.gpsMileage + b.gpsMileage
    val counter=new CounterMonid().fold(a.counter,b.counter)
    val (checkMileage,path)=MileageUtil.getCheckMileage(onlineMileage,validateMileage,gpsMileage,counter,a.checkMileageThreshold)

    MileageResult(
      vid = head.vid,
      vin = head.vin,
      licensePlate = head.licensePlate,
      //基础里程
      onlineMileage = onlineMileage,
      firstValidateMileage = head.firstValidateMileage,
      firstTimeWithValidateMileage = head.firstTimeWithValidateMileage,
      lastValidateMileage =if(b.lastValidateMileage.isEmpty) head.lastValidateMileage else  b.lastValidateMileage,
      lastTimeWithValidateMileage =if(b.lastTimeWithValidateMileage.isEmpty) head.lastTimeWithValidateMileage else  b.lastTimeWithValidateMileage,

      //跳变里程
      stepMileage = stepMileage,
      stepMileageThreshold = a.stepMileageThreshold,
      stepNum = a.stepNum + b.stepNum,
      stepWindowNum = a.stepWindowNum + b.stepWindowNum,

      //连续电流扣除里程
      continueCurrentStepMileage = continueCurrentStepMileage,
      continueCurrentWindowThreshold = a.continueCurrentWindowThreshold, //连续电流扣除阈值
      continueCurrentWindowMinLength = a.continueCurrentWindowMinLength,
      continueCurrentStepMileageNum = a.continueCurrentStepMileageNum + b.continueCurrentStepMileageNum,

      //有效里程
      validateMileage = validateMileage,

      //gps里程
      totalGpsRepeationNum = a.totalGpsRepeationNum + b.totalGpsRepeationNum,
      gpsMileage = gpsMileage,

      //核算里程
      checkMileage = checkMileage,
      checkMileagePath=path,
      checkMileageThreshold=a.checkMileageThreshold,
      onlineMileageAndValidateMileageRelativeError = MileageUtil.onlineMileageAndValidateMileageRelativeError(onlineMileage,validateMileage),
      onlineMileageAndGPSMileageRelativeError = MileageUtil.onlineMileageAndGPSMileageRelativeError(onlineMileage,gpsMileage),

      counter = counter)
  }
}
package com.bitnei.report.common.mileage

import com.bitnei.report.{FoldMonod, FoldableBaseHead}

/*
* created by wangbaosheng on 2017/12/19
*/
class MileageResultMonidBaseOnHeadFoldable {
   def foldLeft(f: Array[MileageResult], acc: MileageResultMonidBaseHead): MileageResult = {
    //对每日的运营指标查询结果累加。
    var result: MileageResult = acc.zero()

    //  def firstAndLast():(Option[Dou])
    var firstValidateMileage: Option[Double] = None
    var lastValidateMileage: Option[Double] = None
    var firstTimeWithValidateMileage: Option[Long] = None
    var lastTimeWithValidateMileage: Option[Long] = None

    f.foreach(v => {
      if (firstValidateMileage.isEmpty && v.firstValidateMileage.nonEmpty) {
        firstTimeWithValidateMileage = v.firstTimeWithValidateMileage
        firstValidateMileage = v.firstValidateMileage
      }

      if (v.lastValidateMileage.nonEmpty) {
        lastValidateMileage = v.lastValidateMileage
        lastTimeWithValidateMileage = v.lastTimeWithValidateMileage
      }
    })

    f.tail
      .foldLeft(acc.zero())(acc.fold).copy(
      firstTimeWithValidateMileage = firstTimeWithValidateMileage,
      firstValidateMileage = firstValidateMileage,
      lastValidateMileage = lastValidateMileage,
      lastTimeWithValidateMileage = lastTimeWithValidateMileage
    )
  }
}package com.bitnei.report.tempjob

/**
  * Created by wangbaosheng on 2017/6/23.
  */
class MileageTopK {
  val yarn=2016

  val sql =
    s"""
        select sys_dict.NAME,LICENSE_PLATE,mileage
        from (
          select VEH_TYPE_ID,LICENSE_PLATE,mileage
          from
          (
            --按照类别分组排序取前10
            select LICENSE_PLATE, VEH_TYPE_ID ,vid,rank() over(partition by VEH_TYPE_ID order by mileage desc) rk,mileage
            from
            (
              select SYS_VEHICLE.LICENSE_PLATE as LICENSE_PLATE,SYS_VEHICLE.VEH_TYPE_ID as VEH_TYPE_ID, t.vid,t.mileage
              from
              (
                -- 获取每一辆车的总里程，剔除600公里以上的数据
                select dy.VID as vid,sum(dy.RUN_KM) as  mileage
                from veh_dayreport_runstate dy
                where to_char(dy.REPORT_TIME,'YYYY')='${yarn}'' AND RUN_KM<600
                group by dy.vid
              ) t join SYS_VEHICLE ON t.vid=SYS_VEHICLE.uuid
            ) mileageTable
          ) aggt
          where aggt.rk<=10
          order by aggt.mileage desc
        ) top10 join sys_dict ON top10.VEH_TYPE_ID=sys_dict.id

""".stripMargin



  """
     select vid,to_char(report_time,'YYYYMM') as rt,sum(run_km),sum(run_time_sum),count(*)
     from veh_dayreport_runstate
     where vid in(
       select uuid from sys_vehicle where LICENSE_PLATE IN ('京Q0QW75',
       '京Q2N0P8',
       '京Q8GG18',
       '京Q9KL03',
       '甘J39963',
       '皖S1D893',
       '云FY2396',
       '闽DKL181',
       '闽DKL168',
       '京AAG901')
     ) and to_char(report_time,'YYYY')='2017'
     group by vid,to_char(report_time,'YYYYMM')
     order by vid,rt;


    select LICENSE_PLATE, uuid from sys_vehicle where LICENSE_PLATE IN ('京Q0QW75',
    '京Q2N0P8',
    '京Q8GG18',
    '京Q9KL03',
    '甘J39963',
    '皖S1D893',
    '云FY2396',
    '闽DKL181',
    '闽DKL168',
    '京AAG901');
  """.stripMargin
}
package com.bitnei.report.common.mileage

/*
* created by wangbaosheng on 2017/12/15
*/
object MileageUtil {

  /**
    * 计算核算里程和数据准确性
    *
    * @param onlineMileage   上线里程
    * @param validateMileage 有效里程
    * @param gpsMileage      gps里程
    * @return 核算里程，数据准确性
    */
  def getCheckMileage(onlineMileage: Double,
                      validateMileage: Double,
                      gpsMileage: Double,
                      counter: Counter,
                      checkMileageThreshold: CheckMileageThreshold)(): (Double, Int) = {
    val (checkMileage, path) = if (counter.emptyPercent > checkMileageThreshold.A) (0D, MileageCheckPath.EmptyPercentTooHigh)
    else if (counter.exceptionPercent > checkMileageThreshold.B) (0D, MileageCheckPath.ExceptionPercentTooHigh)
    else if (onlineMileageAndGPSMileageRelativeError(onlineMileage, gpsMileage) >= checkMileageThreshold.C)
      (Math.min(onlineMileage, gpsMileage), MileageCheckPath.OnlineMileageAndGPSMileageRelativeErrorTooHigh)
    else if (onlineMileageAndValidateMileageRelativeError(onlineMileage, validateMileage) >= checkMileageThreshold.D)
      (Math.min(onlineMileage, validateMileage), MileageCheckPath.OnlineMileageAndValidateMileageRelativeError)
    else (onlineMileage, MileageCheckPath.OnlineMileage)

    (checkMileage, path)
  }


  def validateMileage(onlineMileage: Double, stepMileage: Double, continueCurrentStepMileage: Double): Double = onlineMileage - stepMileage - continueCurrentStepMileage


  def onlineMileageAndValidateMileageRelativeError(onlineMileage: Double, validateMileage: Double): Double = {
    (onlineMileage - validateMileage) / (onlineMileage)
  }

  def onlineMileageAndGPSMileageRelativeError(onlineMileage: Double, gpsMileage: Double): Double = {
    (onlineMileage - gpsMileage) / (onlineMileage)
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.MinHashLSH
import org.apache.spark.ml.linalg.Vectors
// $example off$
import org.apache.spark.sql.SparkSession

object MinHashLSHExample {
  def main(args: Array[String]): Unit = {
    // Creates a SparkSession
    val spark = SparkSession
      .builder
      .appName("MinHashLSHExample")
      .getOrCreate()

    // $example on$
    val dfA = spark.createDataFrame(Seq(
      (0, Vectors.sparse(6, Seq((0, 1.0), (1, 1.0), (2, 1.0)))),
      (1, Vectors.sparse(6, Seq((2, 1.0), (3, 1.0), (4, 1.0)))),
      (2, Vectors.sparse(6, Seq((0, 1.0), (2, 1.0), (4, 1.0))))
    )).toDF("id", "keys")

    val dfB = spark.createDataFrame(Seq(
      (3, Vectors.sparse(6, Seq((1, 1.0), (3, 1.0), (5, 1.0)))),
      (4, Vectors.sparse(6, Seq((2, 1.0), (3, 1.0), (5, 1.0)))),
      (5, Vectors.sparse(6, Seq((1, 1.0), (2, 1.0), (4, 1.0))))
    )).toDF("id", "keys")

    val key = Vectors.sparse(6, Seq((1, 1.0), (3, 1.0)))

    val mh = new MinHashLSH()
      .setNumHashTables(3)
      .setInputCol("keys")
      .setOutputCol("values")

    val model = mh.fit(dfA)

    // Feature Transformation
    model.transform(dfA).show()
    // Cache the transformed columns
    val transformedA = model.transform(dfA).cache()
    val transformedB = model.transform(dfB).cache()

    // Approximate similarity join
    model.approxSimilarityJoin(dfA, dfB, 0.6).show()
    model.approxSimilarityJoin(transformedA, transformedB, 0.6).show()
    // Self Join
    model.approxSimilarityJoin(dfA, dfA, 0.6).filter("datasetA.id < datasetB.id").show()

    // Approximate nearest neighbor search
    model.approxNearestNeighbors(dfA, key, 2).show()
    model.approxNearestNeighbors(transformedA, key, 2).show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.MinMaxScaler
import org.apache.spark.ml.linalg.Vectors
// $example off$
import org.apache.spark.sql.SparkSession

object MinMaxScalerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("MinMaxScalerExample")
      .getOrCreate()

    // $example on$
    val dataFrame = spark.createDataFrame(Seq(
      (0, Vectors.dense(1.0, 0.1, -1.0)),
      (1, Vectors.dense(2.0, 1.1, 1.0)),
      (2, Vectors.dense(3.0, 10.1, 3.0))
    )).toDF("id", "features")

    val scaler = new MinMaxScaler()
      .setInputCol("features")
      .setOutputCol("scaledFeatures")

    // Compute summary statistics and generate MinMaxScalerModel
    val scalerModel = scaler.fit(dataFrame)

    // rescale each feature to range [min, max].
    val scaledData = scalerModel.transform(dataFrame)
    println(s"Features scaled to range: [${scaler.getMin}, ${scaler.getMax}]")
    scaledData.select("features", "scaledFeatures").show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.alarm

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.stat.Statistics
import org.apache.spark.mllib.stat.test.ChiSqTestResult
import org.apache.spark.rdd.RDD
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.linalg.distributed.RowMatrix
/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-20 9:41
  *
  */
object MLlibTest {

  def main(args: Array[String]): Unit = {

    val sparkConf = new SparkConf()

    sparkConf.setAppName("MLlibTest")
    sparkConf.setMaster("local[*]")
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("ERROR")


    import org.apache.spark.mllib.linalg.{Vector, Vectors}

    //    Local vector
    // Create a dense vector (1.0, 0.0, 3.0).
    val dv: Vector = Vectors.dense(1.0, 0.0, 3.0)
    // Create a sparse vector (1.0, 0.0, 3.0) by specifying its indices and values corresponding to nonzero entries.
    val sv1: Vector = Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0))
    // Create a sparse vector (1.0, 0.0, 3.0) by specifying its nonzero entries.
    val sv2: Vector = Vectors.sparse(3, Seq((0, 1.0), (2, 3.0)))


    //    Labeled point
    // Create a labeled point with a positive label and a dense feature vector.
    val pos = LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0))

    // Create a labeled point with a negative label and a sparse feature vector.
    val neg = LabeledPoint(0.0, Vectors.sparse(3, Array(0, 2), Array(1.0, 3.0)))


    import org.apache.spark.mllib.regression.LabeledPoint
    import org.apache.spark.mllib.util.MLUtils
    import org.apache.spark.rdd.RDD

    //label index1:value1 index2:value2 ...
    val examples: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")



    //    Local matrix
    import org.apache.spark.mllib.linalg.{Matrix, Matrices}

    // Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))
    val dm: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))

    // Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))
    val sm: Matrix = Matrices.sparse(3, 2, Array(0, 1, 3), Array(0, 2, 1), Array(9, 6, 8))


    dm.asML.transpose

//    RowMatrix



////    val rows: RDD[Vector] = ... // an RDD of local vectors
//    // Create a RowMatrix from an RDD[Vector].
//    val mat: RowMatrix = new RowMatrix(rows)
//
//    // Get its size.
//    val m = mat.numRows()
//    val n = mat.numCols()
//
//    // QR decomposition
//    val qrResult = mat.tallSkinnyQR(true)



    import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix, RowMatrix}

//    val rows: RDD[IndexedRow] = ... // an RDD of indexed rows
//    // Create an IndexedRowMatrix from an RDD[IndexedRow].
//    val mat: IndexedRowMatrix = new IndexedRowMatrix(rows)
//
//     Get its size.
//    val m = mat.numRows()
//    val n = mat.numCols()
//
//     Drop its row indices.
//    val rowMat: RowMatrix = mat.toRowMatrix()






























    //    val observations = sc.parallelize(
    //      Seq(
    //        Vectors.dense(1.0, 10.0, 100.0),
    //        Vectors.dense(2.0, 20.0, 200.0),
    //        Vectors.dense(3.0, 30.0, 300.0)
    //      )
    //    )
    //
    //    val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)
    //    println(summary.mean) // a dense vector containing the mean value for each column
    //    println(summary.variance) // column-wise variance
    //    println(summary.numNonzeros) // number of nonzeros in each column
    //
    //
    //    val seriesX: RDD[Double] = sc.parallelize(Array(1, 2, 3, 3, 5)) // a series
    //    // must have the same number of partitions and cardinality as seriesX
    //    val seriesY: RDD[Double] = sc.parallelize(Array(11, 22, 33, 33, 555))
    //
    //    // compute the correlation using Pearson's method. Enter "spearman" for Spearman's method. If a
    //    // method is not specified, Pearson's method will be used by default.
    //    val correlation: Double = Statistics.corr(seriesX, seriesY, "pearson")
    //    println(s"Correlation is: $correlation")
    //
    //    val data: RDD[Vector] = sc.parallelize(
    //      Seq(
    //        Vectors.dense(1.0, 10.0, 100.0),
    //        Vectors.dense(2.0, 20.0, 200.0),
    //        Vectors.dense(5.0, 33.0, 366.0))
    //    ) // note that each Vector is a row and not a column
    //
    //    // calculate the correlation matrix using Pearson's method. Use "spearman" for Spearman's method
    //    // If a method is not specified, Pearson's method will be used by default.
    //    val correlMatrix: Matrix = Statistics.corr(data, "pearson")
    //    println(correlMatrix.toString)
    //
    //
    //    // an RDD[(K, V)] of any key value pairs
    //    val data2 = sc.parallelize(
    //      Seq((1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')))
    //
    //    // specify the exact fraction desired from each key
    //    val fractions = Map(1 -> 0.1, 2 -> 0.6, 3 -> 0.3)
    //
    //    // Get an approximate sample from each stratum
    //    val approxSample = data2.sampleByKey(withReplacement = false, fractions = fractions)
    //    // Get an exact sample from each stratum
    //    val exactSample = data2.sampleByKeyExact(withReplacement = false, fractions = fractions)
    //    //
    //
    //
    //    // a vector composed of the frequencies of events
    //    val vec: Vector = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25)
    //
    //    // compute the goodness of fit. If a second vector to test against is not supplied
    //    // as a parameter, the test runs against a uniform distribution.
    //    val goodnessOfFitTestResult = Statistics.chiSqTest(vec)
    //    // summary of the test including the p-value, degrees of freedom, test statistic, the method
    //    // used, and the null hypothesis.
    //    println(s"$goodnessOfFitTestResult\n")
    //
    //    // a contingency matrix. Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))
    //    val mat: Matrix = Matrices.dense(3, 2, Array(1.0, 3.0, 5.0, 2.0, 4.0, 6.0))
    //
    //    // conduct Pearson's independence test on the input contingency matrix
    //    val independenceTestResult = Statistics.chiSqTest(mat)
    //    // summary of the test including the p-value, degrees of freedom
    //    println(s"$independenceTestResult\n")
    //
    //    val obs: RDD[LabeledPoint] =
    //      sc.parallelize(
    //        Seq(
    //          LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0)),
    //          LabeledPoint(1.0, Vectors.dense(1.0, 2.0, 0.0)),
    //          LabeledPoint(-1.0, Vectors.dense(-1.0, 0.0, -0.5)
    //          )
    //        )
    //      ) // (feature, label) pairs.
    //
    //    // The contingency table is constructed from the raw (feature, label) pairs and used to conduct
    //    // the independence test. Returns an array containing the ChiSquaredTestResult for every feature
    //    // against the label.
    //    val featureTestResults: Array[ChiSqTestResult] = Statistics.chiSqTest(obs)
    //    featureTestResults.zipWithIndex.foreach { case (k, v) =>
    //      println("Column " + (v + 1).toString + ":")
    //      println(k)
    //    } // summary of the test


    sc.stop()


  }
}
package com.bitnei.tools.util

import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define
  *
  * date: 2017-11-09
  *
  */
object MockDataProvider extends Serializable {


  // TODO: 实时数据
  def realInfo(sparkSession: SparkSession): Unit = {

//    val realInfoDS = sparkSession.read.json("data/realinfo/mock.txt")

    val realInfoDS = sparkSession.read.parquet("data/realinfo/20171102/*.parquet")

    //    val realInfoDS = sparkSession.read.json("data/realinfo/year=2017/month=11/day=02/mock.txt")

    //    val realInfoDS = sparkSession.sqlContext.read.parquet("data/realinfo/year=2017/month=11/day=02/*.parquet")

    //    val realInfoDS = sparkSession.sqlContext.read
    //      .format("parquet")
    //      .load("data/realinfo/year=2017/month=11/day=02/*.parquet")

    realInfoDS.createOrReplaceTempView("realinfo")
  }

  // TODO: 单车日报数据
  def dayReport(sparkSession: SparkSession): Unit = {

//    val realInfoDS = sparkSession.read.json("data/dayreport/mock.txt")

        val realInfoDS = sparkSession.read.parquet("data/dayreport/*.parquet")

    //    realInfoDS.printSchema()

    realInfoDS.createOrReplaceTempView("dayreport")
  }

  // TODO: 月报表数据
  def monthReport(sparkSession: SparkSession): Unit = {

    val realInfoDS = sparkSession.read.json("data/monthreport/mock.txt")




    //    val realInfoDS = sparkSession.read.parquet("data/dayreport/*.parquet")

    //    realInfoDS.printSchema()

//    realInfoDS.createOrReplaceTempView("monthreport")
  }



}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
import org.apache.spark.sql.Row
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * A simple example demonstrating model selection using CrossValidator.
 * This example also demonstrates how Pipelines are Estimators.
 *
 * Run with
 * {{{
 * bin/run-example ml.ModelSelectionViaCrossValidationExample
 * }}}
 */
object ModelSelectionViaCrossValidationExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("ModelSelectionViaCrossValidationExample")
      .getOrCreate()

    // $example on$
    // Prepare training data from a list of (id, text, label) tuples.
    val training = spark.createDataFrame(Seq(
      (0L, "a b c d e spark", 1.0),
      (1L, "b d", 0.0),
      (2L, "spark f g h", 1.0),
      (3L, "hadoop mapreduce", 0.0),
      (4L, "b spark who", 1.0),
      (5L, "g d a y", 0.0),
      (6L, "spark fly", 1.0),
      (7L, "was mapreduce", 0.0),
      (8L, "e spark program", 1.0),
      (9L, "a e c l", 0.0),
      (10L, "spark compile", 1.0),
      (11L, "hadoop software", 0.0)
    )).toDF("id", "text", "label")

    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
    val tokenizer = new Tokenizer()
      .setInputCol("text")
      .setOutputCol("words")
    val hashingTF = new HashingTF()
      .setInputCol(tokenizer.getOutputCol)
      .setOutputCol("features")
    val lr = new LogisticRegression()
      .setMaxIter(10)
    val pipeline = new Pipeline()
      .setStages(Array(tokenizer, hashingTF, lr))

    // We use a ParamGridBuilder to construct a grid of parameters to search over.
    // With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,
    // this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.
    val paramGrid = new ParamGridBuilder()
      .addGrid(hashingTF.numFeatures, Array(10, 100, 1000))
      .addGrid(lr.regParam, Array(0.1, 0.01))
      .build()

    // We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.
    // This will allow us to jointly choose parameters for all Pipeline stages.
    // A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.
    // Note that the evaluator here is a BinaryClassificationEvaluator and its default metric
    // is areaUnderROC.
    val cv = new CrossValidator()
      .setEstimator(pipeline)
      .setEvaluator(new BinaryClassificationEvaluator)
      .setEstimatorParamMaps(paramGrid)
      .setNumFolds(2)  // Use 3+ in practice

    // Run cross-validation, and choose the best set of parameters.
    val cvModel = cv.fit(training)

    // Prepare test documents, which are unlabeled (id, text) tuples.
    val test = spark.createDataFrame(Seq(
      (4L, "spark i j k"),
      (5L, "l m n"),
      (6L, "mapreduce spark"),
      (7L, "apache hadoop")
    )).toDF("id", "text")

    // Make predictions on test documents. cvModel uses the best model found (lrModel).
    cvModel.transform(test)
      .select("id", "text", "probability", "prediction")
      .collect()
      .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
        println(s"($id, $text) --> prob=$prob, prediction=$prediction")
      }
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * A simple example demonstrating model selection using TrainValidationSplit.
 *
 * Run with
 * {{{
 * bin/run-example ml.ModelSelectionViaTrainValidationSplitExample
 * }}}
 */
object ModelSelectionViaTrainValidationSplitExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("ModelSelectionViaTrainValidationSplitExample")
      .getOrCreate()

    // $example on$
    // Prepare training and test data.
    val data = spark.read.format("libsvm").load("data/mllib/sample_linear_regression_data.txt")
    val Array(training, test) = data.randomSplit(Array(0.9, 0.1), seed = 12345)

    val lr = new LinearRegression()
        .setMaxIter(10)

    // We use a ParamGridBuilder to construct a grid of parameters to search over.
    // TrainValidationSplit will try all combinations of values and determine best model using
    // the evaluator.
    val paramGrid = new ParamGridBuilder()
      .addGrid(lr.regParam, Array(0.1, 0.01))
      .addGrid(lr.fitIntercept)
      .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))
      .build()

    // In this case the estimator is simply the linear regression.
    // A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.
    val trainValidationSplit = new TrainValidationSplit()
      .setEstimator(lr)
      .setEvaluator(new RegressionEvaluator)
      .setEstimatorParamMaps(paramGrid)
      // 80% of the data will be used for training and the remaining 20% for validation.
      .setTrainRatio(0.8)

    // Run train validation split, and choose the best set of parameters.
    val model = trainValidationSplit.fit(training)

    // Make predictions on test data. model is the model with combination of parameters
    // that performed best.
    model.transform(test)
      .select("features", "label", "prediction")
      .show()
    // $example off$

    spark.stop()
  }
}
package com.bitnei.report.tempjob

trait Monoid [A] {
  def op(left: A, right: A): A

  def zero(): A
  def foldRight[B](z: B)(f: (A, B) => B): B
}


//class StringMonoid extends  Monoid[String]{
//  override def op(left: String, right: String): String = left+right
//
//  override def zero(): String = ""
//
//  override def foldRight[B](z: B)(f: (String, B) => B): B = f(z,)
//}


trait Functor[F[_]] {
  def map[A, B](fa: F[A])(f: A => B): F[B]
  def distribute[A,B](fab: F[(A, B)]): (F[A], F[B]) =  (map(fab)(_._1), map(fab)(_._2))
}



object Main{
  def main(args: Array[String]): Unit = {
    val listFunctor=new Functor[List] {
      override def map[A, B](fa: List[A])(f: A => B): List[B] = fa map f
    }

    val x=listFunctor.distribute[Int,Int](List((1,3),(2,4)))
    listFunctor.map(List((1,3),(2,4)))(x=>x).foreach(x=>println(x))

    println(x._1)
    println(x._2)
  }
}
package com.bitnei.report.dayreport

import com.bitnei.report.{Job, JobContainer, JobRunner}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.realinfo.{DayReportResult, DayReportResultFoldMonoid, DayReportResultMonid}
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.hadoop.fs.Path
import org.apache.spark.sql.{Dataset, SparkSession}

/**
  * Created by wangbaosheng on 2017/8/18.
  * 月报作业
  */
class MonthReportJob (stateConf:StateConf,sparkSession: SparkSession) extends Serializable with Logging with Job {
  override type R = DayReportResult

  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConfiguration)

  import sparkSession.implicits._
  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("dayreport")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("monthreport")

  override def registerIfNeed():Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,inputTableName)
  }


  override def unRegister() = {}

  override def doCompute[Product <: DayReportResult]() = {
    val sql = s"SELECT * FROM $inputTableName ${SqlHelper.buildWhere(stateConf)}"
    sparkSession.sql(sql).as[DayReportResult]
      .groupByKey(_.vid)
      .flatMapGroups({ case (vid: String, values: Iterator[DayReportResult]) =>

        val result=new scala.collection.mutable.ArrayBuffer[DayReportResult]()

        //dayreportValues.foreach(line=>logInfo(line.toString))

        val dayreportValues=values.toIterable

        val chargeDayReports = dayreportValues.filter(_.category.trim == Constant.ChargeState)
        val chargeMonthReport = chargeDayReports.reduceOption((a, b) => new DayReportResultMonid(a.vid, a.category, a.reportDate).op(a, b))

        val fullChargeDayReports = dayreportValues.filter(_.category.trim == Constant.FullChargeState)
        val fullChargeMonthReport = fullChargeDayReports.reduceOption((a, b) => new DayReportResultMonid(a.vid, a.category, a.reportDate).op(a, b))

        val runDayReports = dayreportValues.filter(_.category.trim == Constant.TravelState)
        val runMonthReport = runDayReports.reduceOption((a, b) => new DayReportResultMonid(a.vid, a.category, a.reportDate).op(a, b))


        if(chargeMonthReport.nonEmpty){
          result.append(chargeMonthReport.get)
        }

        if(fullChargeMonthReport.nonEmpty){
          result.append(fullChargeMonthReport.get)
        }

        if(runMonthReport.nonEmpty){
          result.append(runMonthReport.get)

        }
        result
      })
  }

  override def write[Product <: DayReportResult](result: Dataset[DayReportResult]) = {
    SparkHelper.saveToPartition(sparkSession,stateConf,result.toDF(),outputTableName)
  }
}

object MonthReportJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    val sparkSession = SparkHelper.getSparkSession(None)

    new MonthReportJob(stateConf, sparkSession).compute()
  }
}package com.bitnei.report.dayreport

import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.realinfo.DayReportResult
import com.bitnei.sparkhelper.SparkHelper
import org.scalatest.{BeforeAndAfter, FunSuite}

/**
  * Created by wangbaosheng on 2017/8/21.
  */
class MonthReportJobTest  extends FunSuite{
  test("test month report"){

    val stateConf=new StateConf
    val sparkSession=SparkHelper.getSparkSession(Some("local"))

    import  sparkSession.implicits._

//    val job=new MonthReportJob(stateConf,sparkSession)
//    sparkSession.createDataset(Array(
//      DayReportResult(vid="vid1",category=Constant.TravelState,reportDate=new Date().getTime,startMileage = 10,stopMileage = 100,totalMileage = 70)  ,
//      DayReportResult(vid="vid1",category=Constant.TravelState,reportDate=new Date().getTime+1,startMileage = 101,stopMileage = 200,totalMileage = 70)
//    )).createOrReplaceTempView("dayreport")
//
//    job.doCompute().collect().foreach(x=>println(x.startMileage,x.stopMileage,x.totalMileage))
//    //assert()

  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import scala.collection.mutable

import org.apache.log4j.{Level, Logger}
import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.recommendation.{ALS, MatrixFactorizationModel, Rating}
import org.apache.spark.rdd.RDD

/**
 * An example app for ALS on MovieLens data (http://grouplens.org/datasets/movielens/).
 * Run with
 * {{{
 * bin/run-example org.apache.spark.examples.mllib.MovieLensALS
 * }}}
 * A synthetic dataset in MovieLens format can be found at `data/mllib/sample_movielens_data.txt`.
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object MovieLensALS {

  case class Params(
      input: String = null,
      kryo: Boolean = false,
      numIterations: Int = 20,
      lambda: Double = 1.0,
      rank: Int = 10,
      numUserBlocks: Int = -1,
      numProductBlocks: Int = -1,
      implicitPrefs: Boolean = false) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("MovieLensALS") {
      head("MovieLensALS: an example app for ALS on MovieLens data.")
      opt[Int]("rank")
        .text(s"rank, default: ${defaultParams.rank}")
        .action((x, c) => c.copy(rank = x))
      opt[Int]("numIterations")
        .text(s"number of iterations, default: ${defaultParams.numIterations}")
        .action((x, c) => c.copy(numIterations = x))
      opt[Double]("lambda")
        .text(s"lambda (smoothing constant), default: ${defaultParams.lambda}")
        .action((x, c) => c.copy(lambda = x))
      opt[Unit]("kryo")
        .text("use Kryo serialization")
        .action((_, c) => c.copy(kryo = true))
      opt[Int]("numUserBlocks")
        .text(s"number of user blocks, default: ${defaultParams.numUserBlocks} (auto)")
        .action((x, c) => c.copy(numUserBlocks = x))
      opt[Int]("numProductBlocks")
        .text(s"number of product blocks, default: ${defaultParams.numProductBlocks} (auto)")
        .action((x, c) => c.copy(numProductBlocks = x))
      opt[Unit]("implicitPrefs")
        .text("use implicit preference")
        .action((_, c) => c.copy(implicitPrefs = true))
      arg[String]("<input>")
        .required()
        .text("input paths to a MovieLens dataset of ratings")
        .action((x, c) => c.copy(input = x))
      note(
        """
          |For example, the following command runs this app on a synthetic dataset:
          |
          | bin/spark-submit --class org.apache.spark.examples.mllib.MovieLensALS \
          |  examples/target/scala-*/spark-examples-*.jar \
          |  --rank 5 --numIterations 20 --lambda 1.0 --kryo \
          |  data/mllib/sample_movielens_data.txt
        """.stripMargin)
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"MovieLensALS with $params")
    if (params.kryo) {
      conf.registerKryoClasses(Array(classOf[mutable.BitSet], classOf[Rating]))
        .set("spark.kryoserializer.buffer", "8m")
    }
    val sc = new SparkContext(conf)

    Logger.getRootLogger.setLevel(Level.WARN)

    val implicitPrefs = params.implicitPrefs

    val ratings = sc.textFile(params.input).map { line =>
      val fields = line.split("::")
      if (implicitPrefs) {
        /*
         * MovieLens ratings are on a scale of 1-5:
         * 5: Must see
         * 4: Will enjoy
         * 3: It's okay
         * 2: Fairly bad
         * 1: Awful
         * So we should not recommend a movie if the predicted rating is less than 3.
         * To map ratings to confidence scores, we use
         * 5 -> 2.5, 4 -> 1.5, 3 -> 0.5, 2 -> -0.5, 1 -> -1.5. This mappings means unobserved
         * entries are generally between It's okay and Fairly bad.
         * The semantics of 0 in this expanded world of non-positive weights
         * are "the same as never having interacted at all".
         */
        Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble - 2.5)
      } else {
        Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble)
      }
    }.cache()

    val numRatings = ratings.count()
    val numUsers = ratings.map(_.user).distinct().count()
    val numMovies = ratings.map(_.product).distinct().count()

    println(s"Got $numRatings ratings from $numUsers users on $numMovies movies.")

    val splits = ratings.randomSplit(Array(0.8, 0.2))
    val training = splits(0).cache()
    val test = if (params.implicitPrefs) {
      /*
       * 0 means "don't know" and positive values mean "confident that the prediction should be 1".
       * Negative values means "confident that the prediction should be 0".
       * We have in this case used some kind of weighted RMSE. The weight is the absolute value of
       * the confidence. The error is the difference between prediction and either 1 or 0,
       * depending on whether r is positive or negative.
       */
      splits(1).map(x => Rating(x.user, x.product, if (x.rating > 0) 1.0 else 0.0))
    } else {
      splits(1)
    }.cache()

    val numTraining = training.count()
    val numTest = test.count()
    println(s"Training: $numTraining, test: $numTest.")

    ratings.unpersist(blocking = false)

    val model = new ALS()
      .setRank(params.rank)
      .setIterations(params.numIterations)
      .setLambda(params.lambda)
      .setImplicitPrefs(params.implicitPrefs)
      .setUserBlocks(params.numUserBlocks)
      .setProductBlocks(params.numProductBlocks)
      .run(training)

    val rmse = computeRmse(model, test, params.implicitPrefs)

    println(s"Test RMSE = $rmse.")

    sc.stop()
  }

  /** Compute RMSE (Root Mean Squared Error). */
  def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], implicitPrefs: Boolean)
    : Double = {

    def mapPredictedRating(r: Double): Double = {
      if (implicitPrefs) math.max(math.min(r, 1.0), 0.0) else r
    }

    val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))
    val predictionsAndRatings = predictions.map{ x =>
      ((x.user, x.product), mapPredictedRating(x.rating))
    }.join(data.map(x => ((x.user, x.product), x.rating))).values
    math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).mean())
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession


/**
 * Usage: MultiBroadcastTest [slices] [numElem]
 */
object MultiBroadcastTest {
  def main(args: Array[String]) {

    val spark = SparkSession
      .builder
      .appName("Multi-Broadcast Test")
      .getOrCreate()

    val slices = if (args.length > 0) args(0).toInt else 2
    val num = if (args.length > 1) args(1).toInt else 1000000

    val arr1 = new Array[Int](num)
    for (i <- 0 until arr1.length) {
      arr1(i) = i
    }

    val arr2 = new Array[Int](num)
    for (i <- 0 until arr2.length) {
      arr2(i) = i
    }

    val barr1 = spark.sparkContext.broadcast(arr1)
    val barr2 = spark.sparkContext.broadcast(arr2)
    val observedSizes: RDD[(Int, Int)] = spark.sparkContext.parallelize(1 to 10, slices).map { _ =>
      (barr1.value.length, barr2.value.length)
    }
    // Collect the small RDD so we can print the observed sizes locally.
    observedSizes.collect().foreach(i => println(i))

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.classification.LogisticRegression
// $example off$
import org.apache.spark.sql.SparkSession

object MulticlassLogisticRegressionWithElasticNetExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("MulticlassLogisticRegressionWithElasticNetExample")
      .getOrCreate()

    // $example on$
    // Load training data
    val training = spark
      .read
      .format("libsvm")
      .load("data/mllib/sample_multiclass_classification_data.txt")

    val lr = new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.3)
      .setElasticNetParam(0.8)

    // Fit the model
    val lrModel = lr.fit(training)

    // Print the coefficients and intercept for multinomial logistic regression
    println(s"Coefficients: \n${lrModel.coefficientMatrix}")
    println(s"Intercepts: ${lrModel.interceptVector}")
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.util.MLUtils
// $example off$

object MulticlassMetricsExample {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("MulticlassMetricsExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load training data in LIBSVM format
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_multiclass_classification_data.txt")

    // Split data into training (60%) and test (40%)
    val Array(training, test) = data.randomSplit(Array(0.6, 0.4), seed = 11L)
    training.cache()

    // Run training algorithm to build the model
    val model = new LogisticRegressionWithLBFGS()
      .setNumClasses(3)
      .run(training)

    // Compute raw scores on the test set
    val predictionAndLabels = test.map { case LabeledPoint(label, features) =>
      val prediction = model.predict(features)
      (prediction, label)
    }

    // Instantiate metrics object
    val metrics = new MulticlassMetrics(predictionAndLabels)

    // Confusion matrix
    println("Confusion matrix:")
    println(metrics.confusionMatrix)

    // Overall Statistics
    val accuracy = metrics.accuracy
    println("Summary Statistics")
    println(s"Accuracy = $accuracy")

    // Precision by label
    val labels = metrics.labels
    labels.foreach { l =>
      println(s"Precision($l) = " + metrics.precision(l))
    }

    // Recall by label
    labels.foreach { l =>
      println(s"Recall($l) = " + metrics.recall(l))
    }

    // False positive rate by label
    labels.foreach { l =>
      println(s"FPR($l) = " + metrics.falsePositiveRate(l))
    }

    // F-measure by label
    labels.foreach { l =>
      println(s"F1-Score($l) = " + metrics.fMeasure(l))
    }

    // Weighted stats
    println(s"Weighted precision: ${metrics.weightedPrecision}")
    println(s"Weighted recall: ${metrics.weightedRecall}")
    println(s"Weighted F1 score: ${metrics.weightedFMeasure}")
    println(s"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}")
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.evaluation.MultilabelMetrics
import org.apache.spark.rdd.RDD
// $example off$

object MultiLabelMetricsExample {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("MultiLabelMetricsExample")
    val sc = new SparkContext(conf)
    // $example on$
    val scoreAndLabels: RDD[(Array[Double], Array[Double])] = sc.parallelize(
      Seq((Array(0.0, 1.0), Array(0.0, 2.0)),
        (Array(0.0, 2.0), Array(0.0, 1.0)),
        (Array.empty[Double], Array(0.0)),
        (Array(2.0), Array(2.0)),
        (Array(2.0, 0.0), Array(2.0, 0.0)),
        (Array(0.0, 1.0, 2.0), Array(0.0, 1.0)),
        (Array(1.0), Array(1.0, 2.0))), 2)

    // Instantiate metrics object
    val metrics = new MultilabelMetrics(scoreAndLabels)

    // Summary stats
    println(s"Recall = ${metrics.recall}")
    println(s"Precision = ${metrics.precision}")
    println(s"F1 measure = ${metrics.f1Measure}")
    println(s"Accuracy = ${metrics.accuracy}")

    // Individual label stats
    metrics.labels.foreach(label =>
      println(s"Class $label precision = ${metrics.precision(label)}"))
    metrics.labels.foreach(label => println(s"Class $label recall = ${metrics.recall(label)}"))
    metrics.labels.foreach(label => println(s"Class $label F1-score = ${metrics.f1Measure(label)}"))

    // Micro stats
    println(s"Micro recall = ${metrics.microRecall}")
    println(s"Micro precision = ${metrics.microPrecision}")
    println(s"Micro F1 measure = ${metrics.microF1Measure}")

    // Hamming loss
    println(s"Hamming loss = ${metrics.hammingLoss}")

    // Subset accuracy
    println(s"Subset accuracy = ${metrics.subsetAccuracy}")
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.classification.MultilayerPerceptronClassifier
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * An example for Multilayer Perceptron Classification.
 */
object MultilayerPerceptronClassifierExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("MultilayerPerceptronClassifierExample")
      .getOrCreate()

    // $example on$
    // Load the data stored in LIBSVM format as a DataFrame.
    val data = spark.read.format("libsvm")
      .load("data/mllib/sample_multiclass_classification_data.txt")

    // Split the data into train and test
    val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)
    val train = splits(0)
    val test = splits(1)

    // specify layers for the neural network:
    // input layer of size 4 (features), two intermediate of size 5 and 4
    // and output of size 3 (classes)
    val layers = Array[Int](4, 5, 4, 3)

    // create the trainer and set its parameters
    val trainer = new MultilayerPerceptronClassifier()
      .setLayers(layers)
      .setBlockSize(128)
      .setSeed(1234L)
      .setMaxIter(100)

    // train the model
    val model = trainer.fit(train)

    // compute accuracy on the test set
    val result = model.transform(test)
    val predictionAndLabels = result.select("prediction", "label")
    val evaluator = new MulticlassClassificationEvaluator()
      .setMetricName("accuracy")

    println("Test set accuracy = " + evaluator.evaluate(predictionAndLabels))
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer
import org.apache.spark.mllib.util.MLUtils

/**
 * An example app for summarizing multivariate data from a file. Run with
 * {{{
 * bin/run-example org.apache.spark.examples.mllib.MultivariateSummarizer
 * }}}
 * By default, this loads a synthetic dataset from `data/mllib/sample_linear_regression_data.txt`.
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object MultivariateSummarizer {

  case class Params(input: String = "data/mllib/sample_linear_regression_data.txt")
    extends AbstractParams[Params]

  def main(args: Array[String]) {

    val defaultParams = Params()

    val parser = new OptionParser[Params]("MultivariateSummarizer") {
      head("MultivariateSummarizer: an example app for MultivariateOnlineSummarizer")
      opt[String]("input")
        .text(s"Input path to labeled examples in LIBSVM format, default: ${defaultParams.input}")
        .action((x, c) => c.copy(input = x))
      note(
        """
        |For example, the following command runs this app on a synthetic dataset:
        |
        | bin/spark-submit --class org.apache.spark.examples.mllib.MultivariateSummarizer \
        |  examples/target/scala-*/spark-examples-*.jar \
        |  --input data/mllib/sample_linear_regression_data.txt
        """.stripMargin)
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"MultivariateSummarizer with $params")
    val sc = new SparkContext(conf)

    val examples = MLUtils.loadLibSVMFile(sc, params.input).cache()

    println(s"Summary of data file: ${params.input}")
    println(s"${examples.count()} data points")

    // Summarize labels
    val labelSummary = examples.aggregate(new MultivariateOnlineSummarizer())(
      (summary, lp) => summary.add(Vectors.dense(lp.label)),
      (sum1, sum2) => sum1.merge(sum2))

    // Summarize features
    val featureSummary = examples.aggregate(new MultivariateOnlineSummarizer())(
      (summary, lp) => summary.add(lp.features),
      (sum1, sum2) => sum1.merge(sum2))

    println()
    println(s"Summary statistics")
    println(s"\tLabel\tFeatures")
    println(s"mean\t${labelSummary.mean(0)}\t${featureSummary.mean.toArray.mkString("\t")}")
    println(s"var\t${labelSummary.variance(0)}\t${featureSummary.variance.toArray.mkString("\t")}")
    println(
      s"nnz\t${labelSummary.numNonzeros(0)}\t${featureSummary.numNonzeros.toArray.mkString("\t")}")
    println(s"max\t${labelSummary.max(0)}\t${featureSummary.max.toArray.mkString("\t")}")
    println(s"min\t${labelSummary.min(0)}\t${featureSummary.min.toArray.mkString("\t")}")
    println()

    sc.stop()
  }
}
// scalastyle:on println
//package com.bitnei.report.zyt
//
//import org.apache.spark.sql.expressions.Aggregator
//import org.apache.spark.sql.Encoder
//import org.apache.spark.sql.Encoders
//import org.apache.spark.sql.SparkSession
//
//case class Employee(name: String, salary: Long)
//
//case class Average(var sum: Long, var count: Long)
//
//object MyAverage2 extends Aggregator[Employee, Average, Double] {
////  // A zero value for this aggregation. Should satisfy the property that any b + zero = b
////  def zero: Average = Average(0L, 0L)
////
////  // Combine two values to produce a new value. For performance, the function may modify `buffer`
////  // and return it instead of constructing a new object
////  def reduce(buffer: Average, employee: Employee): Average = {
////    buffer.sum += employee.salary
////    buffer.count += 1
////    buffer
////  }
////
////  // Merge two intermediate values
////  def merge(b1: Average, b2: Average): Average = {
////    b1.sum += b2.sum
////    b1.count += b2.count
////    b1
////  }
////
////  // Transform the output of the reduction
////  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
////
////  // Specifies the Encoder for the intermediate value type
////  def bufferEncoder: Encoder[Average] = Encoders.product
////
////  // Specifies the Encoder for the final output value type
////  def outputEncoder: Encoder[Double] = Encoders.scalaDouble
//}
//
////val ds = spark.read.json ("examples/src/main/resources/employees.json").as[Employee]
////ds.show ()
////// +-------+------+
////// |   name|salary|
////// +-------+------+
////// |Michael|  3000|
////// |   Andy|  4500|
////// | Justin|  3500|
////// |  Berta|  4000|
////// +-------+------+
////
////// Convert the function to a `TypedColumn` and give it a name
////val averageSalary = MyAverage.toColumn.name ("average_salary")
////val result = ds.select (averageSalary)
////result.show ()
//
//// +--------------+
//// |average_salary|
//// +--------------+
//// |        3750.0|
//// +--------------+
//
//}package com.bitnei.alarm

import org.specs._
import org.specs.runner.{ConsoleRunner, JUnit4}

class MySpecTest extends JUnit4(MySpec)
//class MySpecSuite extends ScalaTestSuite(MySpec)
object MySpecRunner extends ConsoleRunner(MySpec)

object MySpec extends Specification {
  "This wonderful system" should {
    "save the world" in {
      val list = Nil
      list must beEmpty
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}
import org.apache.spark.mllib.util.MLUtils
// $example off$

object NaiveBayesExample {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("NaiveBayesExample")
    val sc = new SparkContext(conf)
    // $example on$
    // Load and parse the data file.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")

    // Split data into training (60%) and test (40%).
    val Array(training, test) = data.randomSplit(Array(0.6, 0.4))

    val model = NaiveBayes.train(training, lambda = 1.0, modelType = "multinomial")

    val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))
    val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()

    // Save and load model
    model.save(sc, "target/tmp/myNaiveBayesModel")
    val sameModel = NaiveBayesModel.load(sc, "target/tmp/myNaiveBayesModel")
    // $example off$
  }
}

// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
 * Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 *
 * Usage: NetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.NetworkWordCount localhost 9999`
 */
object NetworkWordCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: NetworkWordCount <hostname> <port>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    // Create the context with a 1 second batch size
    val sparkConf = new SparkConf().setAppName("NetworkWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(1))

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.NGram
// $example off$
import org.apache.spark.sql.SparkSession

object NGramExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("NGramExample")
      .getOrCreate()

    // $example on$
    val wordDataFrame = spark.createDataFrame(Seq(
      (0, Array("Hi", "I", "heard", "about", "Spark")),
      (1, Array("I", "wish", "Java", "could", "use", "case", "classes")),
      (2, Array("Logistic", "regression", "models", "are", "neat"))
    )).toDF("id", "words")

    val ngram = new NGram().setN(2).setInputCol("words").setOutputCol("ngrams")

    val ngramDataFrame = ngram.transform(wordDataFrame)
    ngramDataFrame.select("ngrams").show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.Normalizer
import org.apache.spark.mllib.util.MLUtils
// $example off$

object NormalizerExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("NormalizerExample")
    val sc = new SparkContext(conf)

    // $example on$
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")

    val normalizer1 = new Normalizer()
    val normalizer2 = new Normalizer(p = Double.PositiveInfinity)

    // Each sample in data1 will be normalized using $L^2$ norm.
    val data1 = data.map(x => (x.label, normalizer1.transform(x.features)))

    // Each sample in data2 will be normalized using $L^\infty$ norm.
    val data2 = data.map(x => (x.label, normalizer2.transform(x.features)))
    // $example off$

    println("data1: ")
    data1.foreach(x => println(x))

    println("data2: ")
    data2.foreach(x => println(x))

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.alarm

import org.apache.spark.ml.feature.Normalizer
import org.apache.spark.sql.SparkSession
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.ml.feature.StandardScaler
import org.apache.spark.ml.feature.MinMaxScaler


/**
  * 归一化测试
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-27 10:11
  *
  */
object NormalTest {

  def main(args: Array[String]): Unit = {

    val sparkSession = SparkSession.builder().master("local[*]").appName(this.getClass.getSimpleName).getOrCreate()


    val dataFrame = sparkSession.createDataFrame(Seq(

      (0, Vectors.dense(1.0, 0.5, -1.0)),
      (1, Vectors.dense(2.0, 1.0, 1.0)),
      (2, Vectors.dense(4.0, 10.0, 2.0))

    )).toDF("id", "features")


    dataFrame.show(false)


    //Normalizer

    //正则化每个向量到1阶范数
    //    使每一个行向量的范数变换为一个单位范数
    val normalizer =
    new Normalizer()
      .setInputCol("features")
      .setOutputCol("normFeatures")
      .setP(1.0)

    val l1NormData = normalizer.transform(dataFrame)

    println("Normalized using L^1 norm")

    l1NormData.show()

    //正则化每个向量到无穷阶范数
    // 向量的无穷阶范数即向量中所有值中的最大值

    val lInfNormData = normalizer.transform(dataFrame,
      normalizer.p -> Double.PositiveInfinity)

    println("Normalized using L^inf norm")

    lInfNormData.show(false)



//    StandardScaler处理的对象是每一列，
    // 也就是每一维特征，将特征标准化为单位标准差或是0均值，
    // 或是0均值单位标准差。
//    主要有两个参数可以设置：
//    - withStd: 默认为真。将数据标准化到单位标准差。
//    - withMean: 默认为假。是否变换为0均值。
//
//    StandardScaler需要fit数据，获取每一维的均值和标准差，来缩放每一维特征。

    val scaler = new StandardScaler()
      .setInputCol("features")
      .setOutputCol("scaledFeatures")
      .setWithStd(true)
      .setWithMean(false)

    // Compute summary statistics by fitting the StandardScaler.
    val scalerModel = scaler.fit(dataFrame)


    // Normalize each feature to have unit standard deviation.
    //// 将每一列的标准差缩放到1。

    val scaledData = scalerModel.transform(dataFrame)

//    scaledData.show

































    sparkSession.stop()
  }

}
package com.bitnei.report

import java.sql.{DriverManager, Types}
import java.text.SimpleDateFormat
import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.TimeUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.jdbc.JdbcDialect
import org.apache.spark.sql.types.{DataType, DataTypes, MetadataBuilder}
import org.apache.spark.sql.{SaveMode, SparkSession}


/**
  *
  * @author zhangyongtian
  * @define 统计车辆的报警指标初始化 第一次统计
  *
  *                      oracle 6.146 表 SYS_CHECK_ITEM_PASSBY_NEW
  *                      首次上线时间
  *                      自然统计天数
  *                      上报统计天数
  *                      错误检测数量
  *                      更新时间
  *
  *                      "VIN" VARCHAR2(255 BYTE) NOT NULL ,
  *                      "FIRST_TIME" DATE NULL ,
  *                      "DAY_OF_NATURE" NUMBER NULL ,
  *                      "DAY_OF_REPORT" NUMBER NULL ,
  *                      "CHECK_ITEM_NUMS" NUMBER NULL ,
  *                      "UPDATE_TIME" DATE NULL
  *
  *                      create 2018-01-05 9:25
  *
  */

object NoticeCodeNextStatInit extends Serializable with Logging {

  def oracleInit(): Unit = {
    val dialect = new JdbcDialect() {
      override def canHandle(url: String) = {
        url.startsWith("jdbc:oracle");
      }

      override def getCatalystType(sqlType: Int, typeName: String, size: Int, md: MetadataBuilder): Option[DataType] = {
        if (sqlType == Types.DATE && typeName.equals("DATE") && size == 0)
          return Option(DataTypes.TimestampType);
        //      return Option.empty();
        None;
      }

      //      override def getJDBCType(dt: DataType): Option[JdbcType] = {
      //        if (DataTypes.StringType.isInstanceOf[dt.type ] {
      //          None
      //        }

      //用于写Oracle数据库时数据类型的转换
      //      @Override
      //      public Option<JdbcType> getJDBCType(DataType dt) {
      //        if (DataTypes.StringType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("VARCHAR2(255)", Types.VARCHAR));
      //        } else if (DataTypes.BooleanType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("NUMBER(1)", Types.NUMERIC));
      //        } else if (DataTypes.IntegerType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("NUMBER(10)", Types.NUMERIC));
      //        } else if (DataTypes.LongType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("NUMBER(19)", Types.NUMERIC));
      //        } else if (DataTypes.DoubleType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("NUMBER(19,4)", Types.NUMERIC));
      //        } else if (DataTypes.FloatType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("NUMBER(19,4)", Types.NUMERIC));
      //        } else if (DataTypes.ShortType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("NUMBER(5)", Types.NUMERIC));
      //        } else if (DataTypes.ByteType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("NUMBER(3)", Types.NUMERIC));
      //        } else if (DataTypes.BinaryType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("BLOB", Types.BLOB));
      //        } else if (DataTypes.TimestampType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("DATE", Types.DATE));
      //        } else if (DataTypes.DateType.sameType(dt)) {
      //          return Option.apply(
      //            new JdbcType("DATE", Types.DATE));
      //        } else if (DataTypes.createDecimalType()
      //          .sameType(dt)) { //unlimited
      //          /*                    return DecimalType.Fixed(precision, scale)
      //                                      =>Some(JdbcType("NUMBER(" + precision + "," + scale + ")",
      //                                      java.sql.Types.NUMERIC))*/
      //          return Option.apply(
      //            new JdbcType("NUMBER(38,4)", Types.NUMERIC));
      //        }
      //        return Option.empty();
      //      }
      //    };

      //      JdbcDialects.registerDialect(dialect)
      //        JdbcDialects.unregisterDialect(dialect);

    }

  }

  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var date = new SimpleDateFormat("yyyyMMdd").format(new Date())
    //    var date = "20180109"

    if (date.length != 8) {
      throw new Exception("input.date error")
    }

    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)


    var user = stateConf.getString("jdbc.user")
    var password = stateConf.getString("jdbc.password")
    var ip = stateConf.getString("jdbc.ip")
    var port = stateConf.getString("jdbc.port")
    var server = stateConf.getString("jdbc.server")


    var tableName = stateConf.getString("jdbc.tableName")


    var outputPath = stateConf.getOption("output.data.path").getOrElse("/spark/vehicle/result/noticecode_stat/next_stat")


    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    val sc = sparkSession.sparkContext
    import sparkSession.implicits._

    val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
    val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConfiguration)

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////

    // TODO: 数据源
    logInfo("数据源:将JSON数据注册成表")

    env match {
      case "local" => {

        val initRDD = sparkSession
          .read
          .format("json")
          .load(s"data/noticecode/day/*.json")
          .select("jsonRes").as[String].rdd

        val tmp = sparkSession.read.json(initRDD).select("value").as[String].rdd

        sparkSession.read.json(tmp).createOrReplaceTempView("ncday")

      }

      //      case "dev" => {
      //
      //        val initRDD = sparkSession
      //          .read
      //          .format("json")
      //          .load(s"/spark/vehicle/result/noticecode_stat/day")
      //          .select("jsonRes").as[String].rdd
      //
      //        val tmp = sparkSession.read.json(initRDD).select("value").as[String].rdd
      //
      //        sparkSession.read.json(tmp).createOrReplaceTempView("ncday")
      //
      //        //        sparkSession.sql("select vin from noticecode").count()
      //        //        return
      //
      //      }


      case "prd" => {

        val initRDD = sparkSession
          .read
          .format("json")
          .load(stateConf.getString("input.data.path"))
          .select("jsonRes").as[String].rdd

        val tmp = sparkSession.read.json(initRDD).select("value").as[String].rdd

        sparkSession.read.json(tmp).createOrReplaceTempView("ncday")

        //        sparkSession.sql("select vin from noticecode").count()
        //        return

      }

    }


    ////////////////////////////////////业务逻辑/////////////////////////////////////////////

    val initDS = sparkSession.sql(s"SELECT vin, date, noticeCode, totalCount FROM ncday where vin is not null and date is not null and noticeCode is not null and totalCount is not null ").as[(String, String, String, String)]


    val filterDS = initDS
      .filter(x => {
        val vin = x._1
        val date = x._2
        val noticecode = x._3
        val total_count = x._4
        !vin.isEmpty
      })

    val mappedDS = filterDS.map(x => {
      val vin = x._1
      val date = x._2
      val noticecode = x._3
      val total_count = x._4.toLong

      Input(vin, date, noticecode, total_count)
    })


    val sdf = new SimpleDateFormat("yyyyMMdd HH:mm:ss")

    val result =
      mappedDS
        .groupByKey(_.vin)
        .mapGroups {
          case (vin, inputs: Iterator[Input]) => {
            val arr = inputs.toArray.sortBy(_.date)

            //            首次上线时间
            //            * 自然统计天数
            //              * 上报统计天数
            //              * 错误检测数量
            //              * 更新时间

            //            "VIN" VARCHAR2(255 BYTE) NOT NULL ,
            //            * "FIRST_TIME" DATE NULL ,
            //            * "DAY_OF_NATURE" NUMBER NULL ,
            //            * "DAY_OF_REPORT" NUMBER NULL ,
            //            * "CHECK_ITEM_NUMS" NUMBER NULL ,
            //            * "UPDATE_TIME" DATE NULL


            val first_time = arr.head.date.toString

            val day_of_nature = TimeUtils.getDaysToNow(first_time, "yyyyMMdd") + 1

            val day_of_report = arr.map(_.date).distinct.length

            val check_item_nums = arr.map(_.total_count).sum

            val update_time = sdf.format(new Date())

            Output(vin, first_time, day_of_nature, day_of_report, check_item_nums, update_time)
          }
        }


    ////////////////////////////////输出///////////////////////

    sparkSession.catalog.dropTempView("ncday") //删除临时表

    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      //      result.show(100, false)
      //      result.count()
      //    result.printSchema()
      //      result.show(false)

      //      result.show(false)k
      // TODO: 输出到oracle
      log.info("输出到oracle====================================")

      user = "ev"
      password = "ev"
      ip = "192.168.6.146"
      port = "1521"
      server = "evmsc1"
      val url = s"jdbc:oracle:thin:@$ip:$port:" + server
      tableName = "SYS_CHECK_ITEM_PASSBY_NEW_test"

      //      user = "ev"
      //      password = "ev"
      //      ip = "192.168.6.146"
      //      port = "1521"
      //      url = s"jdbc:oracle:thin:$user/$password@//$ip:$port/evmsc1"
      //      tableName = "SYS_CHECK_ITEM_PASSBY_NEW_test"

      val prop = new java.util.Properties
      prop.setProperty("user", user)
      prop.setProperty("password", password)

      result.write.option(JDBCOptions.JDBC_TRUNCATE, "true").mode(SaveMode.Overwrite).jdbc(url, tableName, prop);

    }


    if (env.equals("prd")) {
      //      result.show(false)
      // TODO: 输出到oracle
      log.info("输出到oracle====================================")

      //      val user = "ev"
      //      val password = "ev"
      //      val ip = "192.168.2.51"
      //      val port = "1521"
      //      val url = s"jdbc:oracle:thin:@$ip:$port:evmsc1"
      //      val tableName = "SYS_CHECK_ITEM_PASSBY_NEW"

      val prop = new java.util.Properties
      prop.setProperty("user", user)
      prop.setProperty("password", password)

      //      val url = s"jdbc:oracle:thin:@$ip:$port:"+server

      val url = s"jdbc:oracle:thin:@//$ip:$port/" + server


      //      val url = "jdbc:oracle:thin:@//10.10.11.56:1521/evmsc"

      println("url=" + url)

      //      result.write.option(JDBCOptions.JDBC_TRUNCATE, "true").mode(SaveMode.Overwrite).jdbc(url, tableName, prop);

      //SYS_CHECK_ITEM_PASSBY_NEW
      //        192.168.1.49 ev1209/ev


      //删除
      val delSql =
        s"""
          DROP TABLE $tableName;
        """.stripMargin


      //新增
      val createSql =
        s"""
           CREATE TABLE $tableName (
           "VIN" VARCHAR2(255 BYTE) NULL ,
           "FIRST_TIME" VARCHAR2(255 BYTE) NULL ,
           "DAY_OF_NATURE" NUMBER(10) NULL ,
           "DAY_OF_REPORT" NUMBER(10) NULL ,
           "CHECK_ITEM_NUMS" NUMBER(19) NULL ,
           "UPDATE_TIME" VARCHAR2(255 BYTE) NULL
          )
        """.stripMargin

      val truncatSql =
        s"""
         TRUNCATE TABLE $tableName
        """.stripMargin

      val connection = DriverManager.getConnection(url, user, password)

      val stmt = connection.createStatement()

      try {
        stmt.clearBatch()
        stmt.addBatch(truncatSql)
        stmt.executeBatch()
      } catch {
        case e: Exception =>
          logError(s"数据在写入到${tableName}中出现异常")
          throw new Exception(s"throw en exception when writting $tableName", e)
      } finally {
        if (!stmt.isClosed) stmt.close()
        if (!connection.isClosed) connection.close()
      }


      //插入
      result.foreachPartition(values => {
        val sql =
          s"""
            INSERT INTO $tableName
              (VIN,FIRST_TIME,DAY_OF_NATURE,DAY_OF_REPORT,CHECK_ITEM_NUMS,UPDATE_TIME)
             VALUES (?,?,?,?,?,?)
          """.stripMargin

        val connection = DriverManager.getConnection(url, user, password)

        val stmt = connection.prepareStatement(sql)

        try {
          stmt.clearBatch()
          values.foreach(v => {
            stmt.setString(1, v.VIN)
            stmt.setString(2, v.FIRST_TIME)
            stmt.setInt(3, v.DAY_OF_NATURE)
            stmt.setInt(4, v.DAY_OF_REPORT)
            stmt.setLong(5, v.CHECK_ITEM_NUMS)
            stmt.setString(6, v.UPDATE_TIME)
            stmt.addBatch()
          })
          stmt.executeBatch()
        } catch {
          case e: Exception =>
            logError(s"数据在写入到${tableName}中出现异常")
            throw new Exception(s"throw en exception when writting $tableName", e)
        } finally {
          if (!stmt.isClosed) stmt.close()
          if (!connection.isClosed) connection.close()
        }
      }
      )



      //TODO 输出到HDFS
      log.info("输出到HDFS====================================")


      result.repartition(1).write.json(outputPath + "/" + date)

    }


    //    jdbc:oracle:thin:@localhost:1521:orcl
    //    jdbc:oracle:thin:@//localhost:1521/orcl.city.com
    //    jdbc:oracle:thin:@TNS_ALIAS_NAME

    logInfo("任务完成...")

    sparkSession.stop()


  }


  case class Input(vin: String, date: String, noticecode: String, total_count: Long)

  case class Output(VIN: String, FIRST_TIME: String, DAY_OF_NATURE: Int, DAY_OF_REPORT: Int, CHECK_ITEM_NUMS: Long, UPDATE_TIME: String)

}

package com.bitnei.report

import java.text.SimpleDateFormat
import java.util.Properties

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.{KafkaUtils, RedisClient, TimeUtils}
import com.bitnei.sparkhelper.SparkHelper
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.jdbc.{JdbcDialect, JdbcDialects, JdbcType}
import org.apache.spark.sql.types.{DataType, DataTypes, MetadataBuilder}
import org.apache.spark.sql.{SaveMode, SparkSession}
import org.json4s.jackson.Serialization
import java.sql.Types

import com.bitnei.report.util.redis.RedisPool
import org.slf4j
import redis.clients.jedis.{Jedis, JedisPoolConfig, Response}

/**
  *
  * @author zhangyongtian
  * @define Noticecode报警编号按日统计 输出到kafka
  *                            create 2017-12-27 17:11:08
  *
  *                            2018-03-23：新增字段
  *                            select p.MANU_UNIT_ID as "厂商",p.VEH_MODEL_ID as "车型",p.BATCH_NUMBER as "批次" from SYS_VEHICLE p null -1
  *
  *  2.51 oracle
  *
  */

object NoticecodeStatDay extends Serializable with Logging {

  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var date = "20180113"

    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
    }


    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    //       .set("spark.kryo.registrator", "MyRegistrator");
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    val sc = sparkSession.sparkContext
    import sparkSession.implicits._

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    ///vehicle/data/examine/2018/01

    // TODO: 数据源
    logInfo("数据源:将JSON数据注册成表")

    env match {
      case "local" => {

        sparkSession
          .read
          .format("json")
          .load(s"data/noticecode/mock.txt").createOrReplaceTempView("noticecode")


        //        val redisHost = "192.168.6.106"
        //        val redisPort = 6379
        //        val redisPassword = ""
        //        val db = 1
        //
        //        val rc = new Jedis(redisHost, redisPort)
        //        rc.auth(redisPassword)

        //Redis连接池管理类
        //        val redisClient = new RedisClient(redisHost); //创建redis连接池管理类

        //广播Reids连接池管理对象
        //        val broadcastRedis = sc.broadcast(redisClient);

        //    RedisClient redisClient=broadcastRedis.getValue();
        //    val jedis = redisClient.getResource();
        //    redisClient.returnResource(jedis);

        //        rc.select(1)
        //        rc.hgetAll("XNY_LC9KE3EM8GENJL029")
        //        val map = rc.get("XNY_LC9KE3EM8GENJL029")

        //        println(rc.hgetAll("XNY_LC9KE3EM8GENJL029"))

        // echo "hgetall XNY_LC9KE3EM8GENJL029" |redis-cli -n 1

        //        var tempRedisRes = Map[String, Response[String]]()
        //        val keys = Array("XNY_LJVH84A57HS003193", "key2", "key3", "key4")
        //        val pp = rc.pipelined()
        //        for (key <- keys) {
        //          tempRedisRes ++= Map(key -> pp.get(key))
        //        }
        //        pp.sync()
        //
        //        println(tempRedisRes.get("XNY_LJVH84A57HS003193").get.get())

        //    var tempRedisRes = Map[String, Response[String]]()
        //    val keys = Array("key1", "key2", "key3", "key4")
        //    var tryTimes = 2
        //    var flag = false
        //    while (tryTimes > 0 && !flag) {
        //      try {
        //        val pp = redisClient.pipelined()
        //        for (key <- keys) {
        //          tempRedisRes ++= Map(key -> pp.get(key))
        //        }
        //        pp.sync()
        //        flag = true
        //      } catch {
        //        case e: Exception => {
        //          flag = false
        //          println("Redis-Timeout" + e)
        //          tryTimes = tryTimes - 1
        //        }
        //      } finally {
        //        redisClient.disconnect()
        //      }
        //    }


      }

      case "dev" => {
        val initRDD = SparkHelper.readHadoopTextLzoFile(sc, s"/vehicle/data/examine/${year}/${month}/${day}/*.lzo")
        sparkSession.read.json(initRDD).createOrReplaceTempView("noticecode")
        //        sparkSession.sql("select vin from noticecode").count()
        //        return

      }

      case "prd" => {
        val initRDD = SparkHelper.readHadoopTextLzoFile(sc, stateConf.getOption("input.data.path").get + s"/${year}/${month}/${day}/*.lzo")
        sparkSession.read.json(initRDD).createOrReplaceTempView("noticecode")
        //        sparkSession.sql("select vin from noticecode").count()
        //        return

        val vid_record_count = ""


      }

    }

    ////////////////////////////////////业务逻辑/////////////////////////////////////////////

    //    var initDS = sparkSession.sql(s"SELECT vin,VIN,`2000` AS TIME,cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat FROM realinfo where vin is not null and `2000` like '${date}%'  and `2502` is not null and `2503` is not null ").as[(String, String, String, String, String)]
    //
    val initDS = sparkSession.sql(s"SELECT vin,noticecode,count,terminalSTime,terminalETime FROM noticecode where vin is not null and noticecode is not null and count is not null and terminalSTime is not null and terminalETime is not null and noticeTime like '${date}%' ").as[(String, Long, Long, String, String)]


    val filterDS = initDS
      .filter(x => {
        val vin = x._1
        val noticecode = x._2
        val count = x._3
        val stime = x._4
        val etime = x._5
        !vin.isEmpty && !count.equals("")
      })

    val mappedDS = filterDS.map(x => {
      val vin = x._1
      val noticecode = x._2.toInt
      val count = x._3
      val stime = x._4
      val etime = x._5

      Input(vin, noticecode, count, stime, etime)
    })

    val result1 = mappedDS
      .groupByKey(x => {
        (x.vin, x.noticecode)
      }).mapGroups {
      case ((vin, noticecode), inputs: Iterator[Input]) => {
        val arr = inputs.toArray
        val counts = arr.map(_.count)
        val total_count = counts.sum
        val max_count = counts.max

        val sdf = new SimpleDateFormat("yyyyMMddHHmmss")
        val timeLens = arr.map(x => {
          val stimestamp = sdf.parse(x.stime).getTime
          val etimestamp = sdf.parse(x.etime).getTime

          (etimestamp - stimestamp) / 1000
        })
        val total_time_len = timeLens.sum
        val max_time_len = timeLens.max

        val avg_time_len = total_time_len / total_count

        Output1(vin, date, noticecode, total_count, max_count, total_time_len, max_time_len, avg_time_len)
      }
    }


    val result2 = result1.groupByKey(_.vin).mapGroups {
      case (vin, outputs: Iterator[Output1]) => {

        val arr = outputs.toArray

        val max = arr.map(_.max_count).max

        val sum = arr.map(_.total_count).sum


        Output2(vin, max, sum)
      }
    }


    val result3 = result1.joinWith(result2, result1("vin").equalTo(result2("vin")), "inner")
      .map {
        case (input1, intput2) => {
          val vin = input1.vin
          val date = input1.date
          val noticecode = input1.noticecode
          val total_count = input1.total_count
          val max_count = input1.max_count
          val total_time_len = input1.total_time_len
          val max_time_len = input1.total_time_len
          val avg_time_len = input1.avg_time_len

          val all_noticecode_max_count = intput2.max
          val all_noticecode_total_count = intput2.all_noticecode_total_count

          Output3(vin, noticecode, total_count, max_count, total_time_len, max_time_len, avg_time_len, all_noticecode_max_count, all_noticecode_total_count)
        }
      }
    val result4 = result3.mapPartitions(output3s => {

      // TODO: 从redis中读取总报文条数
      val config = new JedisPoolConfig
      val redisHost = stateConf.getOption("input.redis.host").getOrElse("192.168.6.106")
      val redisPort = stateConf.getOption("input.redis.port").getOrElse(6379).toString.toInt
      val db = stateConf.getOption("input.redis.db").getOrElse(1).toString.toInt

      val pool = new RedisPool(config, redisHost, redisPort)

      val jedis = pool.getJedis

      jedis.select(db)

      output3s.map(output3 => {

        val list = jedis.hmget("XNY_" + output3.vin, date)

        val record_count = (list == null || list.isEmpty) match {
          case true => -1
          case false => {
            if (list.get(0) == null)
              -1
            else
              list.get(0).toInt
          }
        }

        //          val record_count = jedis.hmget("XNY_LC9KE3EM8GENJL029","20180101")


        Output4(output3.vin, output3.noticecode, output3.total_count: Long, output3.max_count: Long, output3.total_time_len: Long, output3.max_time_len: Long, output3.avg_time_len: Long, output3.all_noticecode_max_count: Long, output3.all_noticecode_total_count: Long, record_count)
      })

    }).as("a");


    //TODO 读取厂商 车型 批次字段  select p.MANU_UNIT_ID as "厂商",p.VEH_MODEL_ID as "车型",p.BATCH_NUMBER as "批次" from SYS_VEHICLE p null -1

    val user = stateConf.getOption("jdbc.user").getOrElse("ev")
    val password = stateConf.getOption("jdbc.password").getOrElse("ev")
    val ip = stateConf.getOption("jdbc.ip").getOrElse("192.168.6.146")
    val port = stateConf.getOption("jdbc.port").getOrElse("1521")
    val server = stateConf.getOption("jdbc.server").getOrElse("evmsc1")


    val tableName = stateConf.getOption("jdbc.tableName").getOrElse("SYS_VEHICLE")

    val prop = new java.util.Properties
    prop.setProperty("user", user)
    prop.setProperty("password", password)
    prop.put("oracle.jdbc.mapDateToTimestamp", "false")

    val url = s"jdbc:oracle:thin:@$ip:$port:" + server

    val addDs = sparkSession.read.format("jdbc")
      .options(
        Map(
          //"jdbc:oracle:thin:username/password@//192.168.0.89:1521/epps"
          JDBCOptions.JDBC_URL -> s"jdbc:oracle:thin:$user/$password@//$ip/$server",
          JDBCOptions.JDBC_TABLE_NAME -> s"(SELECT T.VIN, T.MANU_UNIT_ID AS manu, T.VEH_MODEL_ID AS MODEL, T.BATCH_NUMBER AS BATCH, U.PATH AS PATH FROM SYS_VEHICLE T JOIN SYS_UNIT U ON T.MANU_UNIT_ID = U.ID  ) a",
          JDBCOptions.JDBC_DRIVER_CLASS -> s"oracle.jdbc.driver.OracleDriver"
          //          JDBCOptions.JDBC_PARTITION_COLUMN -> "REPORT_TIME", //必须是数字列
          //          JDBCOptions.JDBC_LOWER_BOUND -> "1",
          //          JDBCOptions.JDBC_UPPER_BOUND -> "1000",
          //          JDBCOptions.JDBC_NUM_PARTITIONS -> "5",
          //          JDBCOptions.JDBC_BATCH_FETCH_SIZE -> "10",
          //          JDBCOptions.JDBC_TRUNCATE -> "false",
          //          JDBCOptions.JDBC_CREATE_TABLE_OPTIONS -> "CREATE TABLE t (name string) ENGINE=InnoDB DEFAULT CHARSET=utf8",
          //          JDBCOptions.JDBC_BATCH_INSERT_SIZE -> "1000",
          //          JDBCOptions.JDBC_TXN_ISOLATION_LEVEL -> "READ_UNCOMMITTED"

        )
      )
      .load().as("b");

    val result5 = result4
      .joinWith(addDs, $"a.vin" === $"b.vin", "inner")
      .map(x => {

        val output4 = x._1

        val vin = output4.vin
        val noticecode = output4.noticecode
        val total_count = output4.total_count
        val max_count = output4.max_count
        val total_time_len = output4.total_time_len
        val max_time_len = output4.max_time_len
        val avg_time_len = output4.avg_time_len
        val all_noticecode_max_count = output4.all_noticecode_max_count
        val all_noticecode_total_count = output4.all_noticecode_total_count
        val record_count = output4.record_count

        //        var manu = "unknow"
        //        var model = "unknow"
        //        var batch = "unknow"

        var manu = x._2.getAs[String]("MANU")
        var model = x._2.getAs[String]("MODEL")
        var batch = x._2.getAs[String]("BATCH")
        var path = x._2.getAs[String]("PATH")

        if (manu == null || manu.isEmpty()) {
          manu = "unknow"
        }

        if (model == null || model.isEmpty()) {
          model = "unknow"
        }

        if (batch == null || batch.isEmpty()) {
          batch = "unknow"
        }

        if (path == null || path.isEmpty()) {
          path = "/#/"
        }


        Output5(vin: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long, all_noticecode_max_count: Long, all_noticecode_total_count: Long, record_count: Int, manu: String, model: String, batch: String, path: String)
      })


    //TODO 计算结果
    val result = result5.map(x => {
      import org.json4s.JsonDSL._
      import org.json4s._
      import org.json4s.jackson.JsonMethods._

      implicit val formats = Serialization.formats(ShortTypeHints(List()))

      //key
      val vinDateJobject =
        ("vin" -> x.vin) ~ ("date" -> date)
      val vinDate = compact(render(vinDateJobject))
      val manuPaths = x.path.split("\\/").filter(x=>{
        !x.isEmpty && !x.equals("#")
      }).toList

      val value =
        ("vin" -> x.vin) ~
          ("date" -> date) ~
          ("noticeCode" -> x.noticecode) ~
          ("totalCount" -> x.total_count) ~
          ("maxCount" -> x.max_count) ~
          ("totalTime" -> x.total_time_len) ~
          ("maxTime" -> x.max_time_len) ~
          ("avgTime" -> x.avg_time_len) ~
          ("recordCount" -> x.record_count) ~
          ("manu" -> x.manu) ~
          ("model" -> x.model) ~
          ("batch" -> x.batch) ~ {
          "manuPath" -> manuPaths
           }
      //        ~
      //            ("all_noticecode_max_count" -> x.all_noticecode_max_count) ~
      //            ("all_noticecode_total_count" -> x.all_noticecode_total_count)

      val jsonRes = compact(render(value))

      //type 1表示日报
      val resJobject =
        ("type" -> 1) ~ ("value" -> jsonRes)
      val res = compact(render(resJobject))

      OutPut(vinDate, res)

    })




    //      .groupByKey(_.vin)
    //      .mapGroups {
    //        case (vin, output3s: Iterator[Output3]) => {
    //
    //          import org.json4s.JsonDSL._
    //          import org.json4s._
    //          import org.json4s.jackson.JsonMethods._
    //
    //          val jObjects = output3s.toList.map { x =>
    //            (
    //              ("vin" -> x.vin) ~
    //                ("noticecode" -> x.noticecode) ~
    //                ("total_count" -> x.total_count) ~
    //                ("max_count" -> x.max_count) ~
    //                ("total_time_len" -> x.total_time_len) ~
    //                ("avg_time_len" -> x.avg_time_len) ~
    //                ("all_noticecode_max_count" -> x.all_noticecode_max_count)
    //              )
    //          }
    //
    //
    //          implicit val formats = Serialization.formats(ShortTypeHints(List()))
    //
    //          val vinDateJobject: JsonAST.JObject =
    //            ("vin" -> vin) ~ ("date" -> date)
    //          val vinDate = compact(render(vinDateJobject))
    //
    //          val str: List[JsonAST.JObject] = jObjects
    //          val jsonRes = compact(render(str))
    //          //          val jsonClob = classOf[java.sql.Clob].cast(jsonRes)
    //          OutPut(vinDate, jsonRes)
    //        }
    //      }


    ////////////////////////////////输出///////////////////////

    sparkSession.catalog.dropTempView("noticecode") //删除临时表

    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      //      result.show(false)
      //      result.count
      //      result.count()
      //    result.printSchema()
      result.show(false)

      //      result
      //        .selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
      //        .write
      //        .format("kafka")
      //        .option("kafka.bootstrap.servers", "192.168.6.105:9092,192.168.6.106:9092")
      //        .option("topic", "test")
      //        .save()
    }

    if (!env.equals("local")) {

      result.cache()

      if (stateConf.getString("out.target").contains("kafka")) {
        // TODO: 输出到kafka
        log.info("输出到kafka===========================")
        result.foreachPartition(outputs => {

          val props = new Properties()

          //        props.put("metadata.broker.list", "192.168.6.105:9092,192.168.6.106:9092")

          //        props.put("metadata.broker.list", "192.168.1.54:9092,192.168.1.55:9092")

          props.put("metadata.broker.list", stateConf.getString("output.kafka.brokers"))

          props.put("serializer.class", "kafka.serializer.StringEncoder")
          //        props.put("partitioner.class", classOf[HashPartitioner].getName)
          props.put("producer.type", "sync")
          props.put("batch.num.messages", "1")
          props.put("queue.buffering.max.messages", "1000000")
          props.put("queue.enqueue.timeout.ms", "20000000")

          val producer = KafkaUtils.getProducer(props);
          val topic = stateConf.getString("output.kafka.topic")

          outputs.foreach(output => {
            KafkaUtils.producerSendMessage(producer, topic, output.vinDate, output.jsonRes)
          })
          KafkaUtils.producerClose(producer)
        })
      }


      // TODO: 输出到hdfs
      log.info("输出到hdfs===========================")
      if (stateConf.getString("out.target").contains("hdfs")) {
        result.repartition(1).write.json(stateConf.getString("output.hdfs.path") + s"/year=${year}/month=${month}/day=${day}")
      }

      //      // TODO: 输出到Oracle
      //      val user = stateConf.getString("jdbc.user")
      //      val password = stateConf.getString("jdbc.password")
      //      val ip = stateConf.getString("jdbc.ip")
      //      val port = stateConf.getString("jdbc.port")
      //      val server = stateConf.getString("jdbc.server")
      //      val url = s"jdbc:oracle:thin:$user/$password@//$ip:$port/"+server
      //      val tableName = stateConf.getString("jdbc.tableName")
      //
      //
      //      //      val user = "ev"
      //      //      val password = "ev"
      //      //      val ip = "192.168.2.51"
      //      //      val port = "1521"
      //      //      val url = s"jdbc:oracle:thin:$user/$password@//$ip:$port/evmsc1"
      //      //      val tableName = "SYS_CHECK_ITEM_PASSBY_NEW"
      //
      //      val prop = new java.util.Properties
      //
      //      result.write.option(JDBCOptions.JDBC_TRUNCATE, "true").mode(SaveMode.Overwrite).jdbc(url, tableName, prop);


      /////////////////////////////////////////////////////////////////////
      //TODO:统计车辆的报警指标更新

      //      val init_rdd = result.select("jsonRes").as[String].rdd
      //      sparkSession.read.json(init_rdd).createOrReplaceTempView("ncday")
      //
      //      val init_DS = sparkSession.sql(s"SELECT vin, date, noticecode, total_count FROM ncday where vin is not null and date is not null and noticecode is not null and total_count is not null ").as[(String, String, String, String)]
      //      val filter_DS = init_DS
      //        .filter(x => {
      //          val vin = x._1
      //          val date = x._2
      //          val noticecode = x._3
      //          val total_count = x._4
      //          !vin.isEmpty
      //        })
      //      val mapped_DS = filter_DS.map(x => {
      //        val vin = x._1
      //        val date = x._2
      //        val noticecode = x._3
      //        val total_count = x._4.toLong
      //        NextInput(vin, date, noticecode, total_count)
      //      })
      //
      //      val sdf = new SimpleDateFormat("yyyyMMdd HH:mm:ss")
      //
      //      val old_DS = sparkSession.read.json("/spark/vehicle/result/noticecode_stat/next_stat/" + TimeUtils.getYesterDay(date, "yyyyMMdd")).as[NextOutput2].collect()
      //      //      val old_DS = sparkSession.read.json("data/noticecode/next_stat/*").as[NextOutput2].collect()
      //      val old_DSBc = sc.broadcast(old_DS)
      //
      //      val rs =
      //        mapped_DS
      //          .groupByKey(x => {
      //            x.vin
      //          })
      //          .mapGroups {
      //            case (vin, inputs: Iterator[NextInput]) => {
      //              val arr = inputs.toArray.sortBy(_.date)
      //
      //              val oldNextArr = old_DSBc.value
      //              var first_time = ""
      //              var day_of_nature = 0
      //              var day_of_report = 0
      //              var check_item_nums = 0L
      //              if (oldNextArr.map(_.vin).contains(vin)) {
      //                oldNextArr.filter(_.vin.equals(vin)).foreach(x => {
      //                  first_time = x.first_time
      //                  day_of_nature = x.day_of_nature.toInt + 1
      //                  day_of_report = x.day_of_report.toInt + 1
      //                  check_item_nums = x.check_item_nums + arr.map(_.total_count).sum
      //                })
      //              } else {
      //                first_time = arr.head.date.toString
      //                day_of_nature = TimeUtils.getDaysToNow(first_time, "yyyyMMdd") + 1
      //                day_of_report = 1
      //                check_item_nums = arr.map(_.total_count).sum
      //              }
      //
      //              val update_time = sdf.format(new Date())
      //              NextOutput(vin, first_time, day_of_nature, day_of_report, check_item_nums, update_time)
      //            }
      //          }
      //
      //      // TODO: 更新到Oracle
      //      val user = "ev"
      //      val password = "ev"
      //      val ip = "192.168.6.146"
      //      val port = "1521"
      //      val url = s"jdbc:oracle:thin:$user/$password@//$ip:$port/evmsc1"
      //      val tableName = "SYS_CHECK_ITEM_PASSBY_NEW_test"
      //      val prop = new java.util.Properties
      //
      //      rs.write.option(JDBCOptions.JDBC_TRUNCATE, "true").mode(SaveMode.Append).jdbc(url, tableName, prop);
      //
      //      //TODO 输出到HDFS
      //      rs.repartition(1)
      //        .write
      //        .json("/spark/vehicle/result/noticecode_stat/next_stat/" + date)
      /////////////////////////////////////////////////////////////////////

    }

    //      result
    //        .selectExpr("CAST(vinDate AS STRING)", "CAST(jsonRes AS STRING)")
    //        .write
    //        .format("kafka")
    //        .option("kafka.bootstrap.servers", "192.168.6.105:9092,192.168.6.106:9092")
    //        .option("topic", "test")
    //        .save()
    //        .option("checkpointLocation", "/path/to/HDFS/dir") \

    sparkSession.catalog.dropTempView("ncday") //删除临时表

    logInfo("任务完成...")

    sparkSession.stop()

  }


  case class Input(vin: String, noticecode: Int, count: Long, stime: String, etime: String)

  case class Output1(vin: String, date: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long)

  case class Output2(vin: String, max: Long, all_noticecode_total_count: Long)

  case class Output3(vin: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long, all_noticecode_max_count: Long, all_noticecode_total_count: Long)

  case class Output4(vin: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long, all_noticecode_max_count: Long, all_noticecode_total_count: Long, record_count: Int)

  case class Output5(vin: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long, all_noticecode_max_count: Long, all_noticecode_total_count: Long, record_count: Int, manu: String, model: String, batch: String, path: String)


  case class OutPut(vinDate: String, jsonRes: String)

  case class NextInput(vin: String, date: String, noticecode: Int, total_count: Long)

  case class NextOutput(vin: String, first_time: String, day_of_nature: Int, day_of_report: Int, check_item_nums: Long, update_time: String)

  case class NextOutput2(vin: String, first_time: String, day_of_nature: Long, day_of_report: Long, check_item_nums: Long, update_time: String)

}
package com.bitnei.report

import java.text.SimpleDateFormat
import java.util.{Calendar, Properties}

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.redis.RedisPool
import com.bitnei.report.util.{KafkaUtils, TimeUtils}
import com.bitnei.sparkhelper.SparkHelper
import org.apache.hadoop.fs.Path
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.{SaveMode, SparkSession}
import org.json4s.jackson.Serialization
import redis.clients.jedis.JedisPoolConfig

import scala.collection.mutable.ArrayBuffer


/**
  *
  * @author zhangyongtian
  * @define Noticecode报警编号按周统计
  *
  * create 2017-12-27 17:11:08
  *
  */

object NoticecodeStatWeek extends Serializable with Logging {

  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var date = "20180115"

    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
    }


    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    var year = date.substring(0, 4)
    var month = date.substring(4, 6)
    var day = date.substring(6)

    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    val sc = sparkSession.sparkContext
    import sparkSession.implicits._

    val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
    val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConfiguration)

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////

    // TODO: 数据源
    logInfo("数据源:将JSON数据注册成表")

    env match {
      case "local" => {

        sparkSession
          .read
          .format("json")
          .load(s"data/noticecode/mock.txt")
          //          .filter(x => {
          //            TimeUtils.getWeekOfYear(x.getAs[String]("noticeTime"), "yyyyMMddHHmmss") == TimeUtils.getWeekOfYear(date, "yyyyMMdd")
          //          })
          .createOrReplaceTempView("noticecode")

        //        sparkSession.sql("select date_format('20180101', 'w') from noticecode").show(false)
        ///vehicle/data/examine/2018/01
      }


      case "dev" => {

        //获取一周的第一天和最后一天
        var initRDD = SparkHelper.readHadoopTextLzoFile(sc, s"/vehicle/data/examine/${year}/${month}/${day}")

        //上周五开始 到这周四 周五定时任务 当前时间-7
        val allDaysOfWeek = TimeUtils.getMyAllDaysOfWeek(date, "yyyyMMdd")

        log.info("这周的日期:")
        allDaysOfWeek.foreach(println)

        for (oneDay <- allDaysOfWeek if (!oneDay.equals(date))) {
          val year = oneDay.substring(0, 4)
          val month = oneDay.substring(4, 6)
          val day = oneDay.substring(6)

          val path = s"/vehicle/data/examine/${year}/${month}/${day}"
          if (fs.exists(new Path(path))) {
            initRDD = initRDD.union(SparkHelper.readHadoopTextLzoFile(sc, path))
          }

        }

        sparkSession.read.json(initRDD)
          .createOrReplaceTempView("noticecode")

        //        sparkSession.sql("select vin from noticecode").count()
        //        return

      }
      ///tmp/zyt/data/realinfo

      case "prd" => {

        //获取一周的第一天和最后一天

        var initRDD = SparkHelper.readHadoopTextLzoFile(sc, stateConf.getOption("input.data.path").get + s"/${year}/${month}/${day}")

        //上周五开始 到这周四 周五定时任务 当前时间-7
        val allDaysOfWeek = TimeUtils.getMyAllDaysOfWeek(date, "yyyyMMdd")

        log.info("这周的日期:")
        allDaysOfWeek.foreach(println)

        for (oneDay <- allDaysOfWeek if (!oneDay.equals(date))) {
          val year = oneDay.substring(0, 4)
          val month = oneDay.substring(4, 6)
          val day = oneDay.substring(6)

          val path = s"/vehicle/data/examine/${year}/${month}/${day}"
          if (fs.exists(new Path(path))) {
            initRDD = initRDD.union(SparkHelper.readHadoopTextLzoFile(sc, path))
          }

        }

        sparkSession.read.json(initRDD)
          .createOrReplaceTempView("noticecode")

        //        sparkSession.sql("select vin from noticecode").count()
        //        return

      }


    }

    ////////////////////////////////////业务逻辑/////////////////////////////////////////////

    // TODO: 改变标识日期
    date = TimeUtils.addDays(date, "yyyyMMdd", 7)
    year = date.substring(0, 4)
    month = date.substring(4, 6)
    day = date.substring(6)

    //    var initDS = sparkSession.sql(s"SELECT vin,VIN,`2000` AS TIME,cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat FROM realinfo where vin is not null and `2000` like '${date}%'  and `2502` is not null and `2503` is not null ").as[(String, String, String, String, String)]
    //
    val initDS = sparkSession.sql(s"SELECT vin,noticecode,count,terminalSTime,terminalETime FROM noticecode where vin is not null and noticecode is not null and count is not null and terminalSTime is not null and terminalETime is not null and noticeTime is not null  ").as[(String, Long, Long, String, String)]


    val filterDS = initDS
      .filter(x => {
        val vin = x._1
        val noticecode = x._2
        val count = x._3
        val stime = x._4
        val etime = x._5
        !vin.isEmpty
      })

    val mappedDS = filterDS.map(x => {
      val vin = x._1
      val noticecode = x._2.toInt
      val count = x._3
      val stime = x._4
      val etime = x._5

      Input(vin, noticecode, count, stime, etime)
    })

    val result1 = mappedDS
      .groupByKey(x => {
        (x.vin, x.noticecode)
      }).mapGroups {
      case ((vin, noticecode), inputs: Iterator[Input]) => {
        val arr = inputs.toArray
        val counts = arr.map(_.count)
        val total_count = counts.sum
        val max_count = counts.max

        val sdf = new SimpleDateFormat("yyyyMMddHHmmss")
        val timeLens = arr.map(x => {
          val stimestamp = sdf.parse(x.stime).getTime
          val etimestamp = sdf.parse(x.etime).getTime

          (etimestamp - stimestamp) / 1000
        })
        val total_time_len = timeLens.sum
        val max_time_len = timeLens.max

        val avg_time_len = total_time_len / total_count

        Output1(vin, date, noticecode, total_count, max_count, total_time_len, max_time_len, avg_time_len)
      }
    }

    val result2 = result1.groupByKey(_.vin).mapGroups {
      case (vin, outputs: Iterator[Output1]) => {

        val arr = outputs.toArray

        val max = arr.map(_.max_count).max

        val sum = arr.map(_.total_count).sum

        Output2(vin, max, sum)
      }
    }


    val result3 = result1.joinWith(result2, result1("vin").equalTo(result2("vin")), "inner")
      .map {
        case (input1, intput2) => {
          val vin = input1.vin
          val date = input1.date
          val noticecode = input1.noticecode
          val total_count = input1.total_count
          val max_count = input1.max_count
          val total_time_len = input1.total_time_len
          val max_time_len = input1.total_time_len
          val avg_time_len = input1.avg_time_len

          val all_noticecode_max_count = intput2.max
          val all_noticecode_total_count = intput2.all_noticecode_total_count


          Output3(vin, noticecode, total_count, max_count, total_time_len, max_time_len, avg_time_len, all_noticecode_max_count, all_noticecode_total_count)
        }
      }


    val result4 = result3.mapPartitions(output3s => {

      // TODO: 从redis中读取总报文条数
      val config = new JedisPoolConfig
      val redisHost = stateConf.getOption("input.redis.host").getOrElse("192.168.6.106")
      val redisPort = stateConf.getOption("input.redis.port").getOrElse(6379).toString.toInt
      val db = stateConf.getOption("input.redis.db").getOrElse(1).toString.toInt

      val pool = new RedisPool(config, redisHost, redisPort)

      val jedis = pool.getJedis

      jedis.select(db)

      output3s.map(output3 => {

        val allDaysOfWeek = TimeUtils.getMyAllDaysOfWeek(date, "yyyyMMdd")

        var record_count = 0

        allDaysOfWeek.foreach(d => {

          val list = jedis.hmget("XNY_" + output3.vin, d)

          val cnt = (list == null || list.isEmpty) match {
            case true => 0
            case false => {
              if (list.get(0) == null)
                0
              else
                list.get(0).toInt
            }
          }

          record_count = record_count + cnt

        })


        //          val record_count = jedis.hmget("XNY_LC9KE3EM8GENJL029","20180101")


        Output4(output3.vin, output3.noticecode, output3.total_count: Long, output3.max_count: Long, output3.total_time_len: Long, output3.max_time_len: Long, output3.avg_time_len: Long, output3.all_noticecode_max_count: Long, output3.all_noticecode_total_count: Long, record_count)
      })

    }).as("a");

    //TODO 读取厂商 车型 批次字段  select p.MANU_UNIT_ID as "厂商",p.VEH_MODEL_ID as "车型",p.BATCH_NUMBER as "批次" from SYS_VEHICLE p null -1
    val user = stateConf.getOption("jdbc.user").getOrElse("ev")
    val password = stateConf.getOption("jdbc.password").getOrElse("ev")
    val ip = stateConf.getOption("jdbc.ip").getOrElse("192.168.6.146")
    val port = stateConf.getOption("jdbc.port").getOrElse("1521")
    val server = stateConf.getOption("jdbc.server").getOrElse("evmsc1")


    val tableName = stateConf.getOption("jdbc.tableName").getOrElse("SYS_VEHICLE")

    val prop = new java.util.Properties
    prop.setProperty("user", user)
    prop.setProperty("password", password)
    prop.put("oracle.jdbc.mapDateToTimestamp", "false")

    val url = s"jdbc:oracle:thin:@$ip:$port:" + server

    val addDs = sparkSession.read.format("jdbc")
      .options(
        Map(
          //"jdbc:oracle:thin:username/password@//192.168.0.89:1521/epps"
          JDBCOptions.JDBC_URL -> s"jdbc:oracle:thin:$user/$password@//$ip/$server",
          JDBCOptions.JDBC_TABLE_NAME -> s"(SELECT T.VIN, T.MANU_UNIT_ID AS manu, T.VEH_MODEL_ID AS MODEL, T.BATCH_NUMBER AS BATCH, U.PATH AS PATH FROM SYS_VEHICLE T JOIN SYS_UNIT U ON T.MANU_UNIT_ID = U.ID ) a",
          JDBCOptions.JDBC_DRIVER_CLASS -> s"oracle.jdbc.driver.OracleDriver"
          //          JDBCOptions.JDBC_PARTITION_COLUMN -> "REPORT_TIME", //必须是数字列
          //          JDBCOptions.JDBC_LOWER_BOUND -> "1",
          //          JDBCOptions.JDBC_UPPER_BOUND -> "1000",
          //          JDBCOptions.JDBC_NUM_PARTITIONS -> "5",
          //          JDBCOptions.JDBC_BATCH_FETCH_SIZE -> "10",
          //          JDBCOptions.JDBC_TRUNCATE -> "false",
          //          JDBCOptions.JDBC_CREATE_TABLE_OPTIONS -> "CREATE TABLE t (name string) ENGINE=InnoDB DEFAULT CHARSET=utf8",
          //          JDBCOptions.JDBC_BATCH_INSERT_SIZE -> "1000",
          //          JDBCOptions.JDBC_TXN_ISOLATION_LEVEL -> "READ_UNCOMMITTED"

        )
      )
      .load().as("b");

    val result5 = result4
      .joinWith(addDs, $"a.vin" === $"b.vin", "inner")
      .map(x => {

        val output4 = x._1

        val vin = output4.vin
        val noticecode = output4.noticecode
        val total_count = output4.total_count
        val max_count = output4.max_count
        val total_time_len = output4.total_time_len
        val max_time_len = output4.max_time_len
        val avg_time_len = output4.avg_time_len
        val all_noticecode_max_count = output4.all_noticecode_max_count
        val all_noticecode_total_count = output4.all_noticecode_total_count
        val record_count = output4.record_count

        //        var manu = "unknow"
        //        var model = "unknow"
        //        var batch = "unknow"

        var manu = x._2.getAs[String]("MANU")
        var model = x._2.getAs[String]("MODEL")
        var batch = x._2.getAs[String]("BATCH")
        var path = x._2.getAs[String]("PATH")

        if (manu == null || manu.isEmpty()) {
          manu = "unknow"
        }

        if (model == null || model.isEmpty()) {
          model = "unknow"
        }

        if (batch == null || batch.isEmpty()) {
          batch = "unknow"
        }

        if (path == null || path.isEmpty()) {
          path = "/#/"
        }


        Output5(vin: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long, all_noticecode_max_count: Long, all_noticecode_total_count: Long, record_count: Int, manu: String, model: String, batch: String, path: String)
      })


    //TODO 计算结果
    val result = result5.map(x => {
      import org.json4s.JsonDSL._
      import org.json4s._
      import org.json4s.jackson.JsonMethods._

      implicit val formats = Serialization.formats(ShortTypeHints(List()))

      //key
      val vinDateJobject =
        ("vin" -> x.vin) ~ ("date" -> date)
      val vinDate = compact(render(vinDateJobject))
      val manuPaths = x.path.split("\\/").filter(x=>{
        !x.isEmpty && !x.equals("#")
      }).toList

      val value =
        ("vin" -> x.vin) ~
          ("date" -> date) ~
          ("noticeCode" -> x.noticecode) ~
          ("totalCount" -> x.total_count) ~
          ("maxCount" -> x.max_count) ~
          ("totalTime" -> x.total_time_len) ~
          ("maxTime" -> x.max_time_len) ~
          ("avgTime" -> x.avg_time_len) ~
          ("recordCount" -> x.record_count) ~
          ("manu" -> x.manu) ~
          ("model" -> x.model) ~
          ("batch" -> x.batch) ~ {
          "manuPath" -> manuPaths
        }
      //        ~
      //            ("all_noticecode_max_count" -> x.all_noticecode_max_count) ~
      //            ("all_noticecode_total_count" -> x.all_noticecode_total_count)

      val jsonRes = compact(render(value))

      //type 2表示周报
      val resJobject: JsonAST.JObject =
        ("type" -> 2) ~ ("value" -> jsonRes)
      val res = compact(render(resJobject))

      OutPut(vinDate, res)

    })




    ////////////////////////////////输出///////////////////////

    sparkSession.catalog.dropTempView("noticecode") //删除临时表


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
      //    result.printSchema()
      //    result.show(false)
      //
      //      val user = "ev"
      //      val password = "ev"
      //      val ip = "192.168.6.146"
      //      val port = "1521"
      //      val url = s"jdbc:oracle:thin:$user/$password@//$ip:$port/evmsc1"
      //      val tableName = "noticecodeStat"
      //      val prop = new java.util.Properties
      //
      //      result.write.mode(SaveMode.Append).jdbc(url, tableName, prop);
    }

    if (!env.equals("local")) {

      result.cache()

      if (stateConf.getString("out.target").contains("kafka")) {

        // TODO: 输出到kafka
        log.info("输出到kafka======================================")
        result.foreachPartition(outputs => {

          val props = new Properties()

          //        props.put("metadata.broker.list", "192.168.6.105:9092,192.168.6.106:9092")

          //        props.put("metadata.broker.list", "192.168.1.54:9092,192.168.1.55:9092")

          //        props.put("metadata.broker.list", "192.168.2.94:9092,192.168.2.97:9092")

          // zookeeper   192.168.2.70:2181,192.168.2.71:2181,192.168.2.89:2181

          props.put("metadata.broker.list", stateConf.getString("output.kafka.brokers"))

          props.put("serializer.class", "kafka.serializer.StringEncoder")
          //        props.put("partitioner.class", classOf[HashPartitioner].getName)
          props.put("producer.type", "sync")
          props.put("batch.num.messages", "1")
          props.put("queue.buffering.max.messages", "1000000")
          props.put("queue.enqueue.timeout.ms", "20000000")

          val producer = KafkaUtils.getProducer(props);
          val topic = stateConf.getString("output.kafka.topic")

          outputs.foreach(output => {
            KafkaUtils.producerSendMessage(producer, topic, output.vinDate, output.jsonRes)
          })
          KafkaUtils.producerClose(producer)
        })
      }

      // TODO: 输出到hdfs
      log.info("输出到hdfs======================================")
      if (stateConf.getString("out.target").contains("hdfs")) {
        result.repartition(1).write.json(stateConf.getString("output.hdfs.path") + s"/year=${year}/month=${month}/day=${day}")
      }
      // TODO: 输出到Oracle
      //      val user = stateConf.getString("jdbc.user")
      //      val password = stateConf.getString("jdbc.password")
      //      val ip = stateConf.getString("jdbc.ip")
      //      val port = stateConf.getString("jdbc.port")
      //      val server = stateConf.getString("jdbc.server")
      //      val url = s"jdbc:oracle:thin:$user/$password@//$ip:$port/"+server
      //      val tableName = stateConf.getString("jdbc.tableName")
      //
      //
      //      //      val user = "ev"
      //      //      val password = "ev"
      //      //      val ip = "192.168.2.51"
      //      //      val port = "1521"
      //      //      val url = s"jdbc:oracle:thin:$user/$password@//$ip:$port/evmsc1"
      //      //      val tableName = "SYS_CHECK_ITEM_PASSBY_NEW"
      //
      //      val prop = new java.util.Properties
      //
      //      result.write.option(JDBCOptions.JDBC_TRUNCATE, "true").mode(SaveMode.Overwrite).jdbc(url, tableName, prop);

    }


    logInfo("任务完成...")

    sparkSession.stop()


  }


  case class Input(vin: String, noticecode: Int, count: Long, stime: String, etime: String)

  case class Output1(vin: String, date: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long)

  case class Output2(vin: String, max: Long, all_noticecode_total_count: Long)

  case class Output3(vin: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long, all_noticecode_max_count: Long, all_noticecode_total_count: Long)

  case class Output4(vin: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long, all_noticecode_max_count: Long, all_noticecode_total_count: Long, record_count: Int)

  case class Output5(vin: String, noticecode: Int, total_count: Long, max_count: Long, total_time_len: Long, max_time_len: Long, avg_time_len: Long, all_noticecode_max_count: Long, all_noticecode_total_count: Long, record_count: Int, manu: String, model: String, batch: String, path: String)



  case class OutPut(vinDate: String, jsonRes: String)


}

package com.bitnei.util

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-19 9:41
  *
  */

import java.math.BigDecimal

import scala.collection.mutable.ArrayBuffer

object NumberUtils {

  /**
    * 截取小数到指定位数
    *
    * @param number 小数
    * @param scale  小数点后的位数
    * @return
    */
  def toDoubleOfScale(number: Double, scale: Int): Double = {
    new BigDecimal(number).setScale(scale, BigDecimal.ROUND_HALF_UP).doubleValue
    //    val ll = Double.doubleToLongBits(pDouble)

  }

  def getMedianOfInt(arr: ArrayBuffer[Int]): Int = {

    val sortedArr = arr.sorted
    val len = sortedArr.length
    var res = 0

    if (len % 2 != 0) {
      res = sortedArr(len / 2)
    } else {
      if (len > 0) {
        res = (sortedArr(len / 2) + sortedArr(len / 2 - 1)) / 2
      }
    }
    res
  }

  def getMedianOfInt(arr: Array[Int]): Int = {

    val sortedArr = arr.sorted
    val len = sortedArr.length
    var res = 0

    if (len % 2 != 0) {
      res = sortedArr(len / 2)
    } else {
      if (len > 0) {
        res = (sortedArr(len / 2) + sortedArr(len / 2 - 1)) / 2
      }
    }
    res
  }


  def getMedianOfDouble(arr: ArrayBuffer[Double]): Double = {
    val sortedArr = arr.sorted
    val len = sortedArr.length
    var res = 0D

    if (len % 2 != 0) {
      res = sortedArr(len / 2)
    } else {
      if (len > 0) {
        res = (sortedArr(len / 2) + sortedArr(len / 2 - 1)) / 2
      }
    }
    res
  }


  def getMedianOfDouble(arr: Array[Double]): Double = {
    val sortedArr = arr.sorted
    val len = sortedArr.length
    var res = 0D

    if (len % 2 != 0) {
      res = sortedArr(len / 2)
    } else {
      if (len > 0) {
        res = (sortedArr(len / 2) + sortedArr(len / 2 - 1)) / 2
      }
    }
    res
  }


  def main(args: Array[String]): Unit = {
    val res = getMedianOfDouble(ArrayBuffer(1D, 2D, 3D))
    println(res)
  }

}
package com.bitnei.alarm

import java.text.SimpleDateFormat

import com.bitnei.alarm.generator.{GpsMileageGenerator, RunStateInput, RunStateOrbitSplitGenerator}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.GeoUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.annotation.tailrec
import scala.collection.mutable
import scala.collection.mutable.{ArrayBuffer, ListBuffer}

/**
  *
  * @author zhangyongtian
  * @define 一车多终端判断算法
  *
  * 多个vid 在一定范围内 一定时间内 轨迹 时间 里程  soc 相似
  *
  * 轨迹相似
  * 里程数组 是不是相似
  * 时间
  * 速度
  * 电压
  * 电流
  *
  * 停靠点少
  *
  * create 2018-01-31 11:43
  *
  *
  * output:
  * 3. 一车多终端明细表（VIN、开始时间、结束时间、行驶时长、行驶里程、绑定组编号）(ORACLE)（23号初步结果）；
  *
  *
  */


object One2manyTerminal extends Serializable with Logging {


  /**
    * 时间近似处理
    *
    * @param input               输入数据
    * @param sampleTimeThreshold 相同阈值
    * @return
    */
  def comparaTime(input: Array[GroupedOutput], sampleTimeThreshold: Int): Array[GroupedOutput] = {

    val res = new ArrayBuffer[GroupedOutput]

    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    val arr = input.sortBy(_.startTime)

    var flag = 0
    for (i <- 0 until arr.length if i > flag) {

      if (i != arr.length - 1) {
        val pre = arr(i)
        val post = arr(i + 1)
        val startTimeDiff = (sdf.parse(pre.startTime).getTime - sdf.parse(post.startTime).getTime) / 1000
        val stopTimeDiff = (sdf.parse(pre.stopTime).getTime - sdf.parse(post.stopTime).getTime) / 1000
        if (Math.abs(startTimeDiff) <= sampleTimeThreshold && Math.abs(stopTimeDiff) <= sampleTimeThreshold) {
          res.append(pre)
          res.append(post)
          flag = i + 1
        }
      }

    }

    //    @tailrec
    //    def handlerTailRec(curIndex: Int): Unit = {
    //
    //      if (curIndex < res.length) {
    //
    //        handlerTailRec(curIndex + 1)
    //      } else {
    //
    //        return
    //      }

    res.toArray
  }

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    var date = stateConf.getOption("input.date").getOrElse("20180209")

    //参数校验
    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    val geoHashLen = stateConf.getOption("input.geohash.len").getOrElse("8").toInt

    val sampleDiffMileageRatio = stateConf.getOption("input.sample.mileage.ratio").getOrElse("10").toInt

    val sampleTimeThreshold = stateConf.getOption("input.sample.time.ratio").getOrElse("30").toInt

    //输出参数
    var outputTargets = stateConf.getOption("output").getOrElse("console")
    var outFormat = stateConf.getOption("output.format").getOrElse("#")
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
    ////////////////////////////////////////////////

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")
        //        sparkSession.read.json("data/realinfo/mock.txt").createOrReplaceTempView("realinfo")
        sparkSession.read.json("data/realinfo/terminal_orbit_similar.json").createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")

        //        val initDs = sparkSession
        //          .read
        //          .format("parquet")
        //          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}")
        //
        //        initDs.filter(x=>{
        //          val arr= Array("LVCB4L4D8HM004392","LVCB4L4D5HM004379")
        //          val vin = x.getAs[String]("VIN")
        //          arr.contains(vin)
        //        }).toJSON.repartition(1).write.json("/tmp/similar_realinfo")

      }

    }


    ////////////////////////////////////////业务逻辑//////////////////////////////


    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'

    val sql =
      s"""
        SELECT
        VID,
        VIN,
        `2000` AS TIME,
        `2201` AS speed,
        `2202` As mileage,
        `2615` As soc,
        `3201` As isstart,
        cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,
        cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat
        FROM realinfo
        where VID is not null and `2000` like '${date}%'  and `2201` is not null and `2202` is not null  and `2615` is not null  and `3201` is not null  and `2502` is not null and `2503` is not null
      """.stripMargin


    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String, String, String, String)]

    //    initDS.show(false)
    //    if (env.equals("prd")) {
    //      initDS = sparkSession.sql(s"SELECT VID,VIN,`2000` AS TIME,cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat FROM realinfo where VID is not null  and `2000` is not null and `2502` is not null and `2503` is not null and year='${year}' and month='${month}' and day='${day}'").as[(String, String, String, String, String)]
    //    }

    //    logInfo("源数据量：" + initDS.count())

    //    year='${year}' and month='${month}' and day='${day}'

    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        val lon = x._8
        val lat = x._9
        val cond01 = (lon != null && lon.nonEmpty && lat != null && lat.nonEmpty)
        var cond02 = false
        if (cond01) {
          val lonD = lon.toDouble
          val latD = lat.toDouble
          //在中国境内
          cond02 = (lonD <= 136 && lonD >= 73 && latD <= 54 && latD >= 3)
        }
        val speed = x._4.toString.toDouble
        val mileage = x._5.toString.toDouble
        val soc = x._6

        val vin = x._2


        cond02
        //        && speed > 0 && mileage > 0
      })


    //    Array("LVCB4L4DXHM004409","LVCB4L4D8HM004392","LVCB4L4D0HM004399","LVCB4L4D6HM004374","LVCB4L4D5HM004379","LVCB4L4D4HM004406","LVCB4L4D0HM004385","LVCB4L4D8HM004411","LVCB4L4D8HM004392","LVCB4L4D6HM004374","LZYTBGBW4F1052790","LZYTBGBW3F1052781","LZYTBGBW5F1052782","LZYTBGBW0F1052785","LZYTBGBW4F1052787","LZYTBGBW3F1038041","LZYTBGBW6F1052788","LZYTBGBW9F1052784","LZYTBGBW2F1052786","LZYTBGBW5F1052779","LZYTBGBW7F1052783","LZYTBGBW1F1052780","LZYTBGBW8F1052789","LZYTBGBW9E1023896","LZYTBGBW0E1023897")
    //
    //    filteredDS.show(false)

    //    return


    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val vin = x._2
          val time = x._3
          val speed = x._4.toString.toDouble / 10
          val mileage = x._5.toString.toDouble / 10
          val soc = x._6
          val isstart = x._7 //车辆状态 1.启动 2.熄火
          val lon = x._8.toDouble
          val lat = x._9.toDouble

          Input(vid, vin, time, speed, mileage, soc, isstart, lon, lat)
        })


    val runPointDS = mappedDS
      .groupByKey(_.vid)
      .flatMapGroups {
        case (vid, inputs) => {
          val arr = inputs.toArray.sortBy(_.time)
          val vin = arr.head.vin

          //TODO:轨迹切分
          val runstateInputs = arr.map(x => RunStateInput(x.vid, x.vin, x.time, x.speed, x.mileage, x.soc.toDouble, x.isstart.toInt, x.lon, x.lat))
          val runPoints = RunStateOrbitSplitGenerator.handle(runstateInputs)
          runPoints
        }
      }

    //    runPointDS.show(false)
    //    return

    //TODO:获取GPS区域中心和行驶里程
    val diffMileageGpoints = runPointDS.map(points => {

      //中心点 geohash
      val startPoint = points.head

      val stopPoint = points.last

      val minGPSLon = points.map(_.lon).min
      val maxGPSLon = points.map(_.lon).max

      val minGPSLat = points.map(_.lat).min
      val maxGPSLat = points.map(_.lat).max


      val centerLon = minGPSLon + ((maxGPSLon - minGPSLon) / 2)
      val centerLat = minGPSLat + ((maxGPSLat - minGPSLat) / 2)

      val geoHash = GeoUtils.getGeoHashOfBase32(centerLon, centerLat, geoHashLen)

      //里程
      val startMileage = points.head.mileage
      val stopMileage = points.last.mileage

      var diffMileage = 0

      if (stopMileage > startMileage) {
        diffMileage = ((stopMileage - startMileage) / sampleDiffMileageRatio).toInt
      }

      //给每个轨迹加上geohash值和轨迹
      GroupOrbit(geoHash, diffMileage, points.toArray)
    })


    //TODO:根据GeoHash 相同里程 筛选相似轨迹
    val diffMileageSampeGpoints =
      diffMileageGpoints
        .groupByKey(x => (x.geoHash, x.diffMileage))
        .mapGroups {
          case ((geoHash, diffMileage), gpointss: Iterator[GroupOrbit]) => {
            gpointss.toList
          }
        }
        .filter(x => x.length > 1)
        .flatMap(x => x)


    //TODO:输出相似组结果
    val result = diffMileageSampeGpoints.map(x => {

      //TODO:标识相似组

      val groupId = x.geoHash + "#" + x.diffMileage

      val vin = x.points.head.vin

      val vid = x.points.head.vid

      val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

      val startTime = x.points.head.time

      val stopTime = x.points.last.time

      val duration = (sdf.parse(stopTime).getTime - sdf.parse(startTime).getTime) / 1000

      val mileage = x.points.last.mileage - x.points.head.mileage

      val stopMileage = x.points.last.mileage

      val gpsmileage: Double = GpsMileageGenerator.handle(x.points) / 1000D

      GroupedOutput(groupId, vid, vin, startTime, stopTime, duration, mileage, gpsmileage)
    })
      .groupByKey(_.groupId).flatMapGroups {
      case (groupid, outputs) => {

        val arr = outputs.toArray

        //        val res = arr.groupBy(x => {
        //          val sdf = new SimpleDateFormat("yyyyMMddHHmmss")
        //          val starTime = sdf.parse(x.startTime).getTime / 1000 / sampleDiffTimeRatio
        //          val stopTime = sdf.parse(x.stopTime).getTime / 1000 / sampleDiffTimeRatio
        //          (starTime.toInt, stopTime.toInt)
        //        }).map {
        //          case ((startGroupTime, stopGroupTime), outputs) => {
        //            outputs.toList
        //          }
        //        }.filter(_.length > 1)

//                arr.toList
        comparaTime(arr, sampleTimeThreshold).toList
      }
    }.filter(_.mileage >= 1)






    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
    }

    if (env.equals("prd")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      if (outputTargets.contains("hdfs")) {

        result.repartition(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }

    }

    sparkSession.stop()
  }


  case class Input(vid: String, vin: String, time: String, speed: Double, mileage: Double, soc: String, isstart: String, lon: Double, lat: Double)


  case class GroupOrbit(geoHash: String, diffMileage: Int, points: Array[RunStateInput])

  case class GroupedOutput(groupId: String, vid: String, vin: String, startTime: String, stopTime: String, duration: Long, mileage: Double, gpsmileage: Double)

}package com.bitnei.alarm

import java.text.SimpleDateFormat

import breeze.linalg.{DenseVector, normalize}
import breeze.numerics.abs
import com.bitnei.alarm.generator.{GpsMileageGenerator, RunStateInput, RunStateOrbitSplitGenerator}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.GeoUtils
import com.bitnei.util.linear.{MatrixUtils, VectorUtil}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.mllib.feature.Normalizer
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.sql.{Encoder, Encoders, SaveMode, SparkSession}

import scala.annotation.tailrec
import scala.collection.mutable
import scala.collection.mutable.{ArrayBuffer, ListBuffer}

/**
  *
  * @author zhangyongtian
  * @define 一车多终端判断算法
  *
  * 多个vid 在一定范围内 一定时间内 轨迹 时间 里程  soc 相似
  *
  * 轨迹相似
  * 里程数组 是不是相似
  * 时间
  * 速度
  * 电压
  * 电流
  *
  * 停靠点少
  *
  * create 2018-01-31 11:43
  * output:
  * 3. 一车多终端明细表（VIN、开始时间、结束时间、行驶时长、行驶里程、绑定组编号）(ORACLE)（23号初步结果）；
  *
  */


object One2manyTerminalFinal extends Serializable with Logging {


  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    var date = stateConf.getOption("input.date").getOrElse("20171102")

    //参数校验
    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    val geoHashLen = stateConf.getOption("input.geohash.len").getOrElse("8").toInt

    val sampleDiffMileageRatio = stateConf.getOption("input.sample.mileage.ratio").getOrElse("10").toInt

    val sampleTimeThreshold = stateConf.getOption("input.sample.time.ratio").getOrElse("30").toInt

    val loss_matrix_threshold = stateConf.getOption("input.loss.matrix.threshold").getOrElse("5").toInt
    val sameSpeedCount = stateConf.getOption("input.sampe.speed.count").getOrElse("3").toInt

    //输出参数
    var outputTargets = stateConf.getOption("output").getOrElse("console")
    var outFormat = stateConf.getOption("output.format").getOrElse("#")
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()

    val sc = sparkSession.sparkContext

    val sqlContext = sparkSession.sqlContext

    import sparkSession.implicits._
    ////////////////////////////////////////////////

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")
        sparkSession.read.json("data/realinfo/mock.txt").createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }

    ////////////////////////////////////////业务逻辑//////////////////////////////


    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'

    val sql =
      s"""
        SELECT
        VID,
        VIN,
        `2000` AS TIME,
        `2201` AS speed,
        `2202` As mileage,
        `2615` As soc,
        `3201` As isstart,
        cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,
        cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat
        FROM realinfo
        where VID is not null and `2000` like '${date}%'  and `2201` is not null and `2202` is not null  and `2615` is not null  and `3201` is not null  and `2502` is not null and `2503` is not null
      """.stripMargin


    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String, String, String, String)]

    //    initDS.show(false)
    //    if (env.equals("prd")) {
    //      initDS = sparkSession.sql(s"SELECT VID,VIN,`2000` AS TIME,cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat FROM realinfo where VID is not null  and `2000` is not null and `2502` is not null and `2503` is not null and year='${year}' and month='${month}' and day='${day}'").as[(String, String, String, String, String)]
    //    }

    //    logInfo("源数据量：" + initDS.count())

    //    year='${year}' and month='${month}' and day='${day}'

    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        val lon = x._8
        val lat = x._9
        val cond01 = (lon != null && lon.nonEmpty && lat != null && lat.nonEmpty)
        var cond02 = false
        if (cond01) {
          val lonD = lon.toDouble
          val latD = lat.toDouble
          //在中国境内
          cond02 = (lonD <= 136 && lonD >= 73 && latD <= 54 && latD >= 3)
        }
        val speed = x._4.toString.toDouble
        val mileage = x._5.toString.toDouble
        val soc = x._6
        cond02 && speed > 0 && mileage > 0
      })


    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val vin = x._2
          val time = x._3
          val speed = x._4.toString.toDouble / 10
          val mileage = x._5.toString.toDouble / 10
          val soc = x._6
          val isstart = x._7 //车辆状态 1.启动 2.熄火
          val lon = x._8.toDouble
          val lat = x._9.toDouble

          Input(vid, vin, time, speed, mileage, soc, isstart, lon, lat)
        })


    val runPointDS = mappedDS
      .groupByKey(_.vid)
      .flatMapGroups {
        case (vid, inputs) => {
          val arr = inputs.toArray.sortBy(_.time)
          val vin = arr.head.vin

          //TODO:轨迹切分
          val runstateInputs = arr.map(x => RunStateInput(x.vid, x.vin, x.time, x.speed, x.mileage, x.soc.toDouble, x.isstart.toInt, x.lon, x.lat))
          val runPoints = RunStateOrbitSplitGenerator.handle(runstateInputs)
          runPoints
        }
      }


    //TODO:获取GPS区域中心和行驶里程
    val diffMileageGpoints = runPointDS.map(points => {

      //中心点 geohash
      val startPoint = points.head

      val stopPoint = points.last

      val minGPSLon = points.map(_.lon).min
      val maxGPSLon = points.map(_.lon).max

      val minGPSLat = points.map(_.lat).min
      val maxGPSLat = points.map(_.lat).max


      val centerLon = minGPSLon + ((maxGPSLon - minGPSLon) / 2)
      val centerLat = minGPSLat + ((maxGPSLat - minGPSLat) / 2)


      val geoHash = GeoUtils.getGeoHashOfBase32(centerLon, centerLat, geoHashLen)

      //里程
      val startMileage = points.head.mileage
      val stopMileage = points.last.mileage

      var diffMileage = 0

      if (stopMileage > startMileage) {
        diffMileage = ((stopMileage - startMileage) / sampleDiffMileageRatio).toInt
      }

      //给每个轨迹加上geohash值和轨迹
      GroupOrbit(geoHash, diffMileage, points.toArray)
    })


    //TODO:根据GeoHash 相同里程 筛选相似轨迹
    val diffMileageSampeGpoints =
      diffMileageGpoints
        .groupByKey(x => (x.geoHash, x.diffMileage))
        .mapGroups {
          case ((geoHash, diffMileage), gpointss: Iterator[GroupOrbit]) => {
            gpointss.toList
          }
        }
        .filter(x => x.length > 1 && x.head.diffMileage != 0)
        .flatMap(x => x)


    //TODO:输出相似组结果
    val groupResult = diffMileageSampeGpoints.map(x => {

      //TODO:标识相似组
      val groupId = x.geoHash + "#" + x.diffMileage

      GroupOrbitWithGid(groupId, x.points.head.vin, x.points)
    })
      .groupByKey(_.groupId).flatMapGroups {
      case (groupid, grouporbitwithgids) => {

        val arr = grouporbitwithgids.toArray

        //TODO:时间相近的轨迹
        comparaTime(arr, sampleTimeThreshold).toList
      }
    }.filter(x => {
      val startMilage = x.points.head.mileage
      val stopMilage = x.points.last.mileage

      (stopMilage - startMilage) >= 1
    })


    //TODO:获取车辆行驶方向序列
    //1.轨迹转化成向量
    val orbitDirectSeqDS =
    groupResult
      .groupByKey(_.vin)
      .flatMapGroups {
        case (vin, gowgs) => {
          val arr = gowgs.toArray
          arr.map(x => {
            val orbitId = x.vin + "#" + x.points.head.time

            val orbit = x.points

            val directSeq = new ArrayBuffer[Int]()

            //满足条件的连续帧数
            var count = 0

            for (i <- 0 until orbit.length) {
              if (i + 2 < orbit.length) {

                val gps0_lon = orbit(i).lon
                val gps0_lat = orbit(i).lat

                val gps1_lon = orbit(i + 1).lon
                val gps1_lat = orbit(i + 1).lat

                val gps2_lon = orbit(i + 2).lon
                val gps2_lat = orbit(i + 2).lat


                //2.单位化向量
                val d_lon_0 = gps1_lon - gps0_lon
                val d_lat_0 = gps1_lat - gps0_lat

                val v0 = VectorUtil.normalize(Array(d_lon_0, d_lat_0))

                val d_lon_1 = gps2_lon - gps1_lon
                val d_lat_1 = gps2_lat - gps1_lat

                val v1 = VectorUtil.normalize(Array(d_lon_1, d_lat_1))

                //向量叉乘
                val crossRes = VectorUtil.cross(v0, v1)

                //若在第n帧存在根号3/2<|叉乘|<=1且从第n帧开始后的6帧连续满足 记录一次行驶状态改变
                //n<=1 && n>0.5
                if (abs(crossRes) > 0.5 && abs(crossRes) <= 1) {
                  count = count + 1
                  if (count > 5) {
                    //  [1]向右侧行驶判断：n<0，行驶状态记为-1；
                    //  [2]向左侧行驶弯判断：n>0，行驶状态记为1；
                    if (crossRes > 0) {
                      directSeq.append(1)
                    }
                    if (crossRes < 0) {
                      directSeq.append(-1)
                    }
                    count = 0
                  }
                }
              }
            }
            OrbitDirectSeq(vin, orbit, directSeq)
          })
        }
      }


    //将不同车辆的行驶状态序列进行两条两条对比,生成序列距离矩阵M
    val orbitDirectSeqArr = orbitDirectSeqDS.collect().sortBy(_.orbit.head.time)

    val similarOrbits = new ArrayBuffer[OrbitDirectSeq]()

    var flag = 0

    for (i <- 0 until orbitDirectSeqArr.length if i > flag) {

      if (i != orbitDirectSeqArr.length - 1) {
        val pre = orbitDirectSeqArr(i)
        val post = orbitDirectSeqArr(i + 1)

        //不同车辆
        if (!orbitDirectSeqArr(i).vin.equals(orbitDirectSeqArr(i + 1).vin)) {

          val left = orbitDirectSeqArr(i).directSeq.toArray
          val right = orbitDirectSeqArr(i + 1).directSeq.toArray
          //形成距离矩阵
          val m = MatrixUtils.vectors2DistanceMatrix(left, right)

          //计算损失矩阵
          val mc_value = MatrixUtils.getMcMatrixValue(m)
          if (mc_value < loss_matrix_threshold) {
            similarOrbits.append(orbitDirectSeqArr(i))
            similarOrbits.append(orbitDirectSeqArr(i + 1))
          }

        }
        flag = i + 1
      }
    }


    val similarOrbitDirectSeqs = new ArrayBuffer[OrbitDirectSeq]()

    //TODO:判断速度相同个数
    similarOrbits.foreach(x => {

      //取速度数据中的第一条、四分之一分位点、二分之一分位点、四分之三分位点和最后一条进行对比
      val arr = x.orbit.map(_.speed).sorted

      val ds = new org.apache.commons.math3.stat.descriptive.DescriptiveStatistics

      val firstSpeed = arr.head

      val quarterSpeed = ds.getPercentile(25)

      val halfSpeed = ds.getPercentile(50)

      val threeQuarterSpeed = ds.getPercentile(75)

      val lastSpeed = arr.last

      val res = Array[Double](firstSpeed, quarterSpeed, halfSpeed, threeQuarterSpeed, lastSpeed)

      val setLen = res.toSet.size

      //如果五个点中有m个相同则判断为相似轨迹
      if ((res.length - setLen) >= sameSpeedCount) {
        similarOrbitDirectSeqs.append(x)
      }

    })


    //输出
    //    val similarOrbitDirectSeqDS = sparkSession.createDataset(similarOrbitDirectSeqs)

    //    implicit val mapEncoder = org.apache.spark.sql.Encoders.javaSerialization[OrbitDirectSeq]

    val resultArr = similarOrbitDirectSeqs.map(x => {

      val points = x.orbit.sortBy(_.time)

      val vid = points.head.vid

      val vin = points.head.vin

      val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

      val startTime = points.head.time
      val stopTime = points.last.time

      //秒
      val duration = (sdf.parse(stopTime).getTime - sdf.parse(startTime).getTime) / 1000

      val mileage = points.last.mileage - points.head.mileage

      val gpsmileage: Double = GpsMileageGenerator.handle(points) / 1000D

      Output(vid, vin, startTime, stopTime, duration, mileage, gpsmileage)
    })

    val encoder = Encoders.kryo[Output]
    val result = sparkSession.createDataset(resultArr)(encoder)


    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
    }

    if (env.equals("prd")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      if (outputTargets.contains("hdfs")) {

        result.repartition(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //                parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }

    }

    sparkSession.stop()
  }


  /**
    * 时间近似处理
    * 优化方法
    *
    * @param input               输入数据
    * @param sampleTimeThreshold 相同阈值
    * @return
    */
  def comparaTime(input: Array[GroupOrbitWithGid], sampleTimeThreshold: Int): Array[GroupOrbitWithGid] = {

    val res = new ArrayBuffer[GroupOrbitWithGid]

    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    var flag = 0

    val arr = input.sortBy(_.points.head.time)

    for (i <- 0 until arr.length if i > flag) {

      if (i != arr.length - 1) {
        val pre = arr(i)
        val post = arr(i + 1)

        val startTimeDiff = (sdf.parse(pre.points.head.time).getTime - sdf.parse(post.points.head.time).getTime) / 1000
        val stopTimeDiff = (sdf.parse(pre.points.last.time).getTime - sdf.parse(post.points.last.time).getTime) / 1000

        if (Math.abs(startTimeDiff) <= sampleTimeThreshold && Math.abs(stopTimeDiff) <= sampleTimeThreshold) {
          res.append(pre)
          res.append(post)
          flag = i + 1
        }
      }

    }

    res.toArray
  }


}


case class Input(vid: String, vin: String, time: String, speed: Double, mileage: Double, soc: String, isstart: String, lon: Double, lat: Double)

case class GroupOrbit(geoHash: String, diffMileage: Int, points: Array[RunStateInput])

case class GroupOrbitWithGid(groupId: String, vin: String, points: Array[RunStateInput])

case class OrbitDirectSeq(vin: String, orbit: Array[RunStateInput], directSeq: ArrayBuffer[Int])

case class Output(vid: String, vin: String, startTime: String, stopTime: String, duration: Long, mileage: Double, gpsmileage: Double)
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}
// $example off$
import org.apache.spark.sql.SparkSession

object OneHotEncoderExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("OneHotEncoderExample")
      .getOrCreate()

    // $example on$
    val df = spark.createDataFrame(Seq(
      (0, "a"),
      (1, "b"),
      (2, "c"),
      (3, "a"),
      (4, "a"),
      (5, "c")
    )).toDF("id", "category")

    val indexer = new StringIndexer()
      .setInputCol("category")
      .setOutputCol("categoryIndex")
      .fit(df)
    val indexed = indexer.transform(df)

    val encoder = new OneHotEncoder()
      .setInputCol("categoryIndex")
      .setOutputCol("categoryVec")

    val encoded = encoder.transform(indexed)
    encoded.show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.alarm

import java.sql.Timestamp
import java.text.SimpleDateFormat
import java.util.{Date, UUID}

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.sun.org.apache.xalan.internal.xsltc.compiler.util.IntType
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Encoders, Row, SaveMode, SparkSession}

/**
  *
  * @author zhangyongtian
  * @define 将结果插入表格
  *
  * create 2018-01-31 11:43
  *
  */
object OneMoreTerminalInsertToOracle extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()

    val sqlContext = sparkSession.sqlContext

    import sqlContext.implicits._

    val sc = sparkSession.sparkContext

    import sparkSession.implicits._
    ////////////////////////////////////////////////

    sparkSession.read.json("data/one2manyterminal.txt").createOrReplaceTempView("onemoreterminal")

    //        source.map(_.vin).map(x => ("\'" + x + "\'"))
    //          .reduce((a, b) => {
    //            a + "," + b
    //          }).foreach(print)


    sparkSession.sql("select distinct vin from onemoreterminal").show(100,false)


    sparkSession.read.json("data/vin2vid.txt").createOrReplaceTempView("vidvin")


    val sql =
      """
            select v.vid,o.* from onemoreterminal o left join vidvin v on (o.vin=v.vin)
          """.stripMargin

    val rdd = sparkSession.sql(sql).rdd
      .map(row => {

        val ID = UUID.randomUUID().toString

        val VID = Option(row.getAs[String]("vid")).getOrElse("xx")
        val VIN = row.getAs[String]("vin")

        val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

        val sdf2 = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss")

        val startDate = sdf.parse(row.getAs[String]("startTime"))

        val stopDate = sdf.parse(row.getAs[String]("stopTime"))

        val START_TIME = Timestamp.valueOf(sdf2.format(startDate))

        val END_TIME = Timestamp.valueOf(sdf2.format(stopDate))

        //仪表里程
        val KILOMETRES = 0D
        val KILOMETRES_DRIVING = row.getAs[Double]("mileage")
        val TIMES = row.getAs[Long]("duration") / 60
        val STATUS = 0
        val IS_DELETE = "N"
        val MODIFY_SYS = 1
        val MODIFY_TIME = Timestamp.valueOf(sdf2.format(new Date()))

        val REMARK = ""
        val MORE_TERMINAL_ID = row.getAs[String]("groupId")

        Row(ID, VID, VIN, START_TIME, END_TIME, KILOMETRES, KILOMETRES_DRIVING, TIMES, STATUS, IS_DELETE, MODIFY_SYS, MODIFY_TIME, REMARK, MORE_TERMINAL_ID)
      })

    val dfschema = StructType(
      Array(
        StructField("ID", StringType),
        StructField("VID", StringType),
        StructField("VIN", StringType),
        StructField("START_TIME", TimestampType),
        StructField("END_TIME", TimestampType),
        StructField("KILOMETRES", DoubleType),
        StructField("KILOMETRES_DRIVING", DoubleType),
        StructField("TIMES", LongType),
        StructField("STATUS", IntegerType),
        StructField("IS_DELETE", StringType),
        StructField("MODIFY_SYS", IntegerType),
        StructField("MODIFY_TIME", TimestampType),
        StructField("REMARK", StringType),
        StructField("MORE_TERMINAL_ID", StringType)

      ))

    val result = sparkSession.createDataFrame(rdd, dfschema)


    //    result.show(false)
    /////////////////////////////////////////////////////////////////////////////////////

    //
    //    val result = sqlContext.createDataFrame(rdd.collect())
    //
    //
    //    //    val result = rdd.toDF()
    //
    //    //    val encoder = Encoders.kryo[Output]
    //    //    val result = sparkSession.createDataset(rdd.collect())(encoder)
    //
    //    //    val encoder = Encoders.kryo[Output]
    //
    //    //    result.foreach(println)
    //
    //    result.show(100, false)


    ////////////////////////输出//////////////////////
    //    ID	VARCHAR2	36	0	True
    //      VID	VARCHAR2	36	0	True
    //      VIN	VARCHAR2	18	0	True
    //      START_TIME	TIMESTAMP	6	0	True
    //      END_TIME	TIMESTAMP	6	0	True
    //      KILOMETRES	NUMBER	8	2	True
    //      KILOMETRES_DRIVING	NUMBER	8	2	True
    //      TIMES	NUMBER	8	2	True
    //      STATUS	NUMBER	1	0	False
    //      IS_DELETE	CHAR	1	0	True
    //      MODIFY_SYS	NUMBER	8	0	True
    //      MODIFY_TIME	TIMESTAMP	6	0	True
    //      REMARK	VARCHAR2	256	0	False
    //      MORE_TERMINAL_ID	VARCHAR2	36	0	True


    val user = "ev"
    val password = "ev"
    val ip = "192.168.6.146"
    val port = "1521"
    val server = "evmsc1"
    val url = s"jdbc:oracle:thin:@$ip:$port:" + server
    val tableName = "CHECK_MORE_TERMINAL"

    val prop = new java.util.Properties
    prop.setProperty("user", user)
    prop.setProperty("password", password)
    prop.put("oracle.jdbc.mapDateToTimestamp", "false")

    result.write.option(JDBCOptions.JDBC_TRUNCATE, "true").mode(SaveMode.Overwrite).jdbc(url, tableName, prop);

    sparkSession.stop()

  }


  case class Iput(groupId: String, vin: String, startTime: String, stopTime: String, duration: Long, mileage: Double)

  case class Otput(ID: String, VID: String, VIN: String, START_TIME: Long, END_TIME: Long, KILOMETRES: Int, KILOMETRES_DRIVING: Double, TIMES: Long, STATUS: Int, IS_DELETE: Char, MODIFY_SYS: Int, MODIFY_TIME: Long, REMARK: String, MORE_TERMINAL_ID: String)


}



/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * An example of Multiclass to Binary Reduction with One Vs Rest,
 * using Logistic Regression as the base classifier.
 * Run with
 * {{{
 * ./bin/run-example ml.OneVsRestExample
 * }}}
 */

object OneVsRestExample {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName(s"OneVsRestExample")
      .getOrCreate()

    // $example on$
    // load data file.
    val inputData = spark.read.format("libsvm")
      .load("data/mllib/sample_multiclass_classification_data.txt")

    // generate the train/test split.
    val Array(train, test) = inputData.randomSplit(Array(0.8, 0.2))

    // instantiate the base classifier
    val classifier = new LogisticRegression()
      .setMaxIter(10)
      .setTol(1E-6)
      .setFitIntercept(true)

    // instantiate the One Vs Rest Classifier.
    val ovr = new OneVsRest().setClassifier(classifier)

    // train the multiclass model.
    val ovrModel = ovr.fit(train)

    // score the model on test data.
    val predictions = ovrModel.transform(test)

    // obtain evaluator.
    val evaluator = new MulticlassClassificationEvaluator()
      .setMetricName("accuracy")

    // compute the classification error on test data.
    val accuracy = evaluator.evaluate(predictions)
    println(s"Test Error = ${1 - accuracy}")
    // $example off$

    spark.stop()
  }

}
// scalastyle:on println
package com.bitnei.report.operationIndex.job

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper, SqlHelper}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.sql.{Dataset, SparkSession}

import scala.collection.mutable.ArrayBuffer

/*
* created by wangbaosheng on 2017/12/14
*/

case class OperationDetailFormatValue(rowKey:String,detailType:String,detail:String)
case class OperationDetail(
                            vin:String,
                            firstTimeWithValidateMileage:Option[Long],
                            stepMileageDetail:Array[String]=Array.empty[String],
                            continueCurrentStepMileageDetail:Array[String]=Array.empty[String])

class OperationDetailJob  (@transient sparkSession:SparkSession,stateConf:StateConf) extends Job with Serializable {
  override type R = OperationDetailFormatValue

  import  sparkSession.implicits._

  private val operationIndexMileageTableName = stateConf.getOption("input.table.name").getOrElse("operationIndexMileage")
  private val operationIndexDetailFormatTableName = stateConf.getOption("operationIndexDetailFormat.table.name").getOrElse("operationIndexDetail")


  override def registerIfNeed(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, operationIndexMileageTableName)
  }

  override def doCompute[Product <: OperationDetailFormatValue](): Dataset[OperationDetailFormatValue] = {
    sparkSession.sql(s"SELECT vin,firstTimeWithValidateMileage,stepMileageDetail,continueCurrentStepMileageDetail FROM $operationIndexMileageTableName as operationMileage ${SqlHelper.buildWhere(stateConf)}")
      .as[OperationDetail]
      .mapPartitions(values=>{
        val format=new OperationIndexDetailFormat
        format.foramt(values.toIterable).toIterator
      })
  }

  override def write[Product <: OperationDetailFormatValue](result: Dataset[OperationDetailFormatValue]): Unit = {
    val outputModels=stateConf.getOption("report.output").getOrElse("hdfs").split(',')

    if(outputModels.length>1)  result.cache()

    outputModels.foreach(output=>{
      if(output=="hdfs"){
        SparkHelper.saveToPartition(sparkSession,stateConf,result.toDF(),operationIndexDetailFormatTableName)
      }else if(output=="hbase"){
        (stateConf.getOption("hbase.quorum"), stateConf.getOption("hbase.zkport")) match {
          case (Some(quorm), Some(zkport)) =>
            result.foreachPartition(values=>{
              new HbaseOutput().output(quorm,zkport,values.toIterable)
            })
          case _=>
        }
      }
    })
  }
}

object OperationDetailJob extends  Logging{
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    batchProcess(stateConf, (spark, stateConf) => new OperationDetailJob(spark, stateConf).compute())
  }

  def batchProcess(stateConf: StateConf,f:(SparkSession,StateConf)=>Unit): Unit ={
    val spark = SparkHelper.getSparkSession(sparkMaster = None)
    if (stateConf.getOption("batchProcess.enable").contains("true")) {
      val startDate = Utils.parsetDate(stateConf.getString("startDate"), "yyyyMMdd").get
      val endDate = Utils.parsetDate(stateConf.getString("endDate"), "yyyyMMdd").get

      while (startDate.getTime <= endDate.getTime) {
        val year = Utils.formatDate(startDate, "yyyy")
        val month = Utils.formatDate(startDate, "MM")
        val day = Utils.formatDate(startDate, "dd")

        SparkHelper.setPartitionValue(stateConf, "operationIndexMileage", Array(year, month, day))
        SparkHelper.setPartitionValue(stateConf, "operationIndexDetail", Array(year, month, day))

        val s=s"${year}${month}${day}"
        stateConf.set("report.date",s"$s-$s")

        logInfo(s"begin execute $year-$month-$day")
        f(spark,stateConf)
        startDate.setDate(startDate.getDate + 1)
      }
    } else {
      f(spark,stateConf)
    }
  }
}
package com.bitnei.report.operationIndex.job
import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper, SqlHelper}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.sql.{Dataset, SparkSession}

/*
* created by wangbaosheng on 2017/12/14
*/


class OperationDetailOutputJob  (@transient sparkSession:SparkSession,stateConf:StateConf) extends Job with Serializable {
  override type R = OperationDetailFormatValue

  import sparkSession.implicits._

  private val operationIndexDetailFormatTableName = stateConf.getOption("operationIndexDetailFormat.table.name").getOrElse("operationIndexDetail")


  override def registerIfNeed(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, operationIndexDetailFormatTableName)
  }

  override def doCompute[Product <: OperationDetailFormatValue](): Dataset[OperationDetailFormatValue] = {
    sparkSession.sql(s"SELECT * FROM $operationIndexDetailFormatTableName  ${SqlHelper.buildWhere(stateConf)}")
      .as[OperationDetailFormatValue]
  }

  override def write[Product <: OperationDetailFormatValue](result: Dataset[OperationDetailFormatValue]): Unit = {

    (stateConf.getOption("hbase.quorum"), stateConf.getOption("hbase.zkport")) match {
      case (Some(quorm), Some(zkport)) =>
        result.foreachPartition(values => {
          new HbaseOutput().output(quorm,zkport,values.toIterable)
        })
      case  _=>logWarning("no hbase.quorum or hbase.zkport")
    }
  }
}

object OperationDetailOutputJob extends  Logging{
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    batchProcess(stateConf, (spark, stateConf) => new OperationDetailOutputJob(spark, stateConf).compute())
  }

  def batchProcess(stateConf: StateConf,f:(SparkSession,StateConf)=>Unit): Unit ={
    val spark = SparkHelper.getSparkSession(sparkMaster = None)
    if (stateConf.getOption("batchProcess.enable").contains("true")) {
      val startDate = Utils.parsetDate(stateConf.getString("startDate"), "yyyyMMdd").get
      val endDate = Utils.parsetDate(stateConf.getString("endDate"), "yyyyMMdd").get

      while (startDate.getTime <= endDate.getTime) {
        val year = Utils.formatDate(startDate, "yyyy")
        val month = Utils.formatDate(startDate, "MM")
        val day = Utils.formatDate(startDate, "dd")

        SparkHelper.setPartitionValue(stateConf, "operationIndexDetail", Array(year, month, day))

        val s=s"${year}${month}${day}"
        stateConf.set("report.date",s"$s-$s")

        logInfo(s"begin execute $year-$month-$day")
        f(spark,stateConf)
        startDate.setDate(startDate.getDate + 1)
      }
    } else {
      f(spark,stateConf)
    }
  }
}

package com.bitnei.report.operationIndex.job

/*
* created by wangbaosheng on 2017/12/22
*/

object OperationIndexChecker {

}
package com.bitnei.report.operationIndex.job

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.SparkHelper
import org.apache.spark.sql.{Dataset, SparkSession}

/*
* created by wangbaosheng on 2017/12/27
* 里程核查数据一致性校验
*/
//class OperationIndexConsistencyCheck(@transient sparkSession:SparkSession,stateConf:StateConf)
//  extends Job with Serializable {
//    override type R = String
//  //里程表
//  private val operationIndexMileageTableName = stateConf.getOption("operationIndexMileage.table.name").getOrElse("operationIndexMileage")
//
//  //明细表
//  private val operationIndexDetailFormatTableName = stateConf.getOption("operationIndexDetailFormat.table.name").getOrElse("operationIndexDetail")
//
//  override def registerIfNeed(): Unit = {
//    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,operationIndexMileageTableName)
//    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,operationIndexDetailFormatTableName)
//  }
//
//  override def doCompute[Product <: String](): Dataset[String] = {
//    val date=Utils.parsetDate(stateConf.getString("checkdate"),"yyyyMMdd").get
//
//    val year=Utils.formatDate(date,"yyyy")
//    val month=Utils.formatDate(date,"MM")
//    val day=Utils.formatDate(date,"dd")
//
//    s"""
//      SELECT
//        mileage.stepMileageDetail,
//        mileage.continueCurrentStepMileageDetail,
//        detail.
//      FROM $operationIndexMileageTableName mileage INNER JOIN $operationIndexDetailFormatTableName detail ON mileage.vid=detail.vid
//    """.stripMargin
//    null
//  }
//
//  def check(): Unit ={
//    //读取某一天的日里程数据
//    //读取某一天的日明细数据
//    //校验1：日里程数据中跳变里程校验
//    //检验2：日里程中的连续电流扣除里程校验
//
//
//  }
//}
package com.bitnei.report.operationIndex.job

import com.bitnei.report.common.mileage.MileageResult

import scala.collection.mutable.ArrayBuffer

/*
* created by wangbaosheng on 2017/12/13
*/

class OperationIndexDetailFormat {
  def mapOperationValueToDetailValues(operationValues: Iterable[MileageResult]): Array[OperationDetailFormatValue] = {
    val result = new ArrayBuffer[OperationDetailFormatValue]()

    operationValues.foreach(operationValue => {
      detail(operationValue.vin,
        operationValue.stepMileageDetail,
        operationValue.continueCurrentStepMileageDetail
        , v => result.append(v))
    })

    result.toArray
  }

  def foramt(operationValues: Iterable[OperationDetail]): Array[OperationDetailFormatValue] = {
    val result = new ArrayBuffer[OperationDetailFormatValue]()

    operationValues.foreach(operationValue => {
      detail(operationValue.vin,
        operationValue.stepMileageDetail,
        operationValue.continueCurrentStepMileageDetail,
        v => result.append(v))
    })

    result.toArray
  }

  def detail(vin: String,
             stepMileageDetail: Array[String],
             continueCurrentStepMileageDetail: Array[String],
             f: (OperationDetailFormatValue) => Unit): Unit = {
    stepMileageDetail.foreach(detail => {
      val date = detail.split(',').head
      val detailType = 1
      f(
        OperationDetailFormatValue(
          rowKey = s"${vin}_${detailType}_$date",
          detailType = detailType.toString,
          detail = detail
        ))
    })

    continueCurrentStepMileageDetail.foreach(continueCurrent => {
      val date = continueCurrent.split(',').head
      val detailType = 2
      f(
        OperationDetailFormatValue(
          rowKey = s"${vin}_${detailType}_$date",
          detailType = detailType.toString,
          detail = continueCurrent
        ))
    })
  }
}
package com.bitnei.report.operationIndex.job

import java.util.Date

import com.bitnei.report.{Job}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.mileage._
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.operationIndex.output.OperationIndexResultOutput
import com.bitnei.report.operationIndex.queryParam.{QueryParam, VehicleInfo}
import com.bitnei.sparkhelper.SparkHelper
import org.apache.spark.sql.{Dataset, SparkSession}

import scala.collection.mutable.ArrayBuffer
/*
* created by wangbaosheng on 2017/11/8
*/
class OperationIndexJob(@transient sparkSession:SparkSession,stateConf:StateConf,queryParam: QueryParam)
  extends Job with Serializable {
  override type R = OperationIndexResult

  private val docId = queryParam.docId
  private val queryStartTime=queryParam.startTime.get
  private val queryEndTime=queryParam.endTime.get

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("operationIndexMileage")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("operationIndex")

  private val vinTableName = "vin_table"

  private  val enableDebugLog=stateConf.getOption("enableDebugLog").getOrElse("true")=="true"
  private  val gpsMileageFactor=stateConf.getInt("gpsMileageFactor",1)

  import sparkSession.implicits._

  override def registerIfNeed(): Unit = {
    sparkSession.udf.register("parse_time", (strTime: String) => parseTime(strTime))

    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)

    sparkSession.createDataset(queryParam.allVehicleList).createOrReplaceTempView(vinTableName)
  }

  def parseTime(strTime: String): Long = {
    Utils.parsetDate(strTime, "yyyyMMddHHmmss").map(_.getTime).getOrElse(0L)
  }

  override def unRegister(): Unit = sparkSession.catalog.dropTempView(inputTableName)




  //运营指标查询计算
  override def doCompute[Product <: OperationIndexResult](): Dataset[OperationIndexResult] = {
    val vehicleTable = sparkSession.sql(s"SELECT * FROM $vinTableName").as[VehicleInfo]
    val operationTable = sparkSession.sql(s"select * FROM $inputTableName where ${OperationIndexJob.buildDateWhere(queryStartTime, queryEndTime)}").as[MileageResult]

    //先进行join，然后按照vin进行分组。
    val result = vehicleTable.joinWith(operationTable, vehicleTable("vin") === operationTable("vin"), "left")
      .groupByKey(_._1.vin)
      .mapGroups({
        case (vin: String, valuesIterator: Iterator[(VehicleInfo, MileageResult)]) => {
          val values = valuesIterator.toArray

          //因为是左连接，所以这里过滤掉那些为null的值
          val sortedValus = Utils.sortByDate2[MileageResult](values.filter(v => v._2 != null && v._2.firstTimeWithValidateMileage.nonEmpty).map(_._2), x => x.firstTimeWithValidateMileage)

          val head = values.head._1

          val result = sortedValus.headOption
            .map(h => new MileageResultMonidBaseOnHeadFoldable().foldLeft(sortedValus, new MileageResultMonidBaseHead(h)))
            .getOrElse(MileageResult(vin = vin, vid = head.vid, licensePlate = Some(head.licensePlate)))

          //如果开启了debug模式，那么输出详细日志
          if(enableDebugLog){
            stateConf.getOption("debug.vid") match {
              case Some(vid)=>
                //如果配置了debug.vid,那么只输出该vid的日志
                if(vid==head.vid) logDebug(sortedValus,result)
              case None=>
                logDebug(sortedValus,result)
            }
          }

          val onlineDayNum = sortedValus.count(_.onlineMileage > 0)
          val totalOnlineDayNum: Int = {
            (result.firstTimeWithValidateMileage, result.lastTimeWithValidateMileage) match {
              case (Some(firstTime), Some(lastTime)) =>
                val q = (lastTime - firstTime).toInt % (1000 * 60 * 60 * 24)
                (lastTime - firstTime).toInt / (1000 * 60 * 60 * 24) + (if (q > 0) 1 else 0)
              case _ => 0
            }
          }

          val unOnlineDayNum = totalOnlineDayNum - onlineDayNum
          OperationIndexResult(
            vid = result.vid,
            vin = result.vin,
            licensePlate = Some(head.licensePlate),
            checkMileage = result.checkMileage,
            checkMileagePath = result.checkMileagePath,
            firstValidateMileage = result.firstValidateMileage,
            lastValidateMileage = result.lastValidateMileage,
            firstTimeWithValidateMileage = result.firstTimeWithValidateMileage,
            lastTimeWithValidateMileage = result.lastTimeWithValidateMileage,
            onlineMileage = result.onlineMileage,
            validateMileage = result.validateMileage,
            totalGpsRepeationNum = result.totalGpsRepeationNum,
            gpsMileage = result.gpsMileage*gpsMileageFactor,
            onlineMileageAndValidateMileageRelativeError = result.onlineMileageAndValidateMileageRelativeError,
            onlineMileageAndGPSMileageRelativeError = result.onlineMileageAndGPSMileageRelativeError,
            stepMileage = result.stepMileage,
            stepMileageThreshold = result.stepMileageThreshold,
            stepWindowNum = result.stepWindowNum,
            stepNum = result.stepWindowNum,
            continueCurrentStepMileage = result.continueCurrentStepMileage,
            continueCurrentStepMileageNum = result.continueCurrentStepMileageNum,
            unOnlineDayNum = unOnlineDayNum,
            counter = result.counter)
        }
      }).repartition(5)
    result
  }

  def logDebug(mileageValues:Array[MileageResult],result:MileageResult)={
    mileageValues.map(v=>{
      logInfo(v.humanString)
    })

    logInfo("全局信息")
    logInfo(result.humanString)
  }

  override def write[Product <: OperationIndexResult](result: Dataset[OperationIndexResult]): Unit = {
    val outptuModels = stateConf.getString("report.output").split(',')

    if(outptuModels.length>1) result.cache()

    outptuModels.foreach(outputModel => {
      if (outputModel == "hdfs") SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), outputTableName)
      else if (outputModel == "oracle" || outputModel == "mysql") {
        stateConf.set("database", outputModel)
        result.foreachPartition(par => {
          val output = new OperationIndexResultOutput(docId, stateConf)
          output.output(par.toIterable)
        })
      }
    })
  }

  def stop(): Unit ={
    logInfo(s"begin stoppin operationIndex for ${queryParam.docId}")
    sparkSession.stop()
  }
}


object OperationIndexJob {
  def main(args: Array[String]): Unit = {
    println(buildDateWhere(
      Utils.parsetDate("2017-01-20", "yyyy-MM-dd").get.getTime,
      Utils.parsetDate("2017-01-20", "yyyy-MM-dd").get.getTime)
    )

    println(buildDateWhere(
      Utils.parsetDate("2017-01-20", "yyyy-MM-dd").get.getTime,
      Utils.parsetDate("2017-01-21", "yyyy-MM-dd").get.getTime)
    )
    println(buildDateWhere(
      Utils.parsetDate("2017-01-20", "yyyy-MM-dd").get.getTime,
      Utils.parsetDate("2017-02-21", "yyyy-MM-dd").get.getTime)
    )
    println(buildDateWhere(
      Utils.parsetDate("2017-01-20", "yyyy-MM-dd").get.getTime,
      Utils.parsetDate("2017-05-21", "yyyy-MM-dd").get.getTime)
    )


    println(buildDateWhere(
      Utils.parsetDate("2017-01-01", "yyyy-MM-dd").get.getTime,
      Utils.parsetDate("2017-12-07 23:59:59", "yyyy-MM-dd hh:mm:ss").get.getTime)
    )
  }

  def buildDateWhere(startTime: Long, endTime: Long): String = {
    def buildMonth(startTime: Long): ArrayBuffer[String] = {
      val reportDateString = new ArrayBuffer[String]()
      val curDate = new Date(startTime)
      while (curDate.getTime <= endTime) {
        val dateStr=Utils.formatDate(curDate,"yyyy-MM")
        val year=dateStr.split('-')(0)
        val month=dateStr.split('-')(1)
        reportDateString.append(s"year=$year AND month=$month")
        curDate.setMonth(curDate.getMonth + 1)
      }
      reportDateString
    }

    def replaceFirstMonth(startTime: Long): String = {
      val reportDateString = new StringBuilder()
      val curDate = new Date(startTime)

      var endDate = new Date(startTime)
      endDate.setMonth(endDate.getMonth + 1)
      endDate.setDate(0)
      endDate.setHours(0)
      endDate.setMinutes(0)

      endDate = new Date(Math.min(endDate.getTime, endTime))

      val s = Utils.formatDate(endDate, "yyyy-MM-dd HH:mm:ss")
      while (curDate.getTime <= endDate.getTime) {
        val dateStr = Utils.formatDate(curDate, "yyyy-MM-dd")
        val year = dateStr.split('-')(0)
        val month = dateStr.split('-')(1)
        val date = dateStr.split('-')(2)
        reportDateString.append(s"(year=$year AND month=$month AND day=$date)OR")
        curDate.setDate(curDate.getDate + 1)
      }
      reportDateString.substring(0, reportDateString.length - 2)
    }


    def replaceLastMonth(endTime: Long): String = {
      val reportDateString = new StringBuilder
      val endDate = new Date(endTime)
      var curDate = new Date((endTime))
      curDate.setDate(1)

      curDate = new Date(Math.max(curDate.getTime, startTime))

      val s = Utils.formatDate(curDate, "yyyy-MM-dd HH:mm:ss")
      while (curDate.getTime <= endDate.getTime) {
        val dateStr = Utils.formatDate(curDate, "yyyy-MM-dd")
        val year = dateStr.split('-')(0)
        val month = dateStr.split('-')(1)
        val date = dateStr.split('-')(2)
        reportDateString.append(s"(year=$year AND month=$month AND day=$date)OR")
        curDate.setDate(curDate.getDate + 1)
      }
      if (reportDateString.length > 2) reportDateString.substring(0, reportDateString.length - 2) else ""
    }

    //2017-10-28,2017-12-01

    val monthCondition = buildMonth(startTime)


    //将第一个月份替换为：[startTime,第一个月最后一天】
    //将最后一个月分替换为：[最后一个月份的第一天，endtime]


    val firstMonth = monthCondition.head
    val replacedFirstMonth = replaceFirstMonth(startTime)

    val lastMonth = monthCondition.last
    val replacedLastMonth = replaceLastMonth(endTime)


    monthCondition.remove(0)

    if(monthCondition.nonEmpty) monthCondition.remove(monthCondition.length - 1)

    if (monthCondition.nonEmpty) {
      val body = monthCondition.foldLeft("")((a, b) => {
        if (a.isEmpty) b
        else {
          s"($a) OR ($b)"
        }
      })
      s"$replacedFirstMonth OR $body OR $replacedLastMonth"
    } else {
      if(replacedFirstMonth==replacedLastMonth) replacedFirstMonth
      else s"$replacedFirstMonth OR $replacedLastMonth"
    }
  }
}
package com.bitnei.report.operationIndex.job

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.mileage.MileageResult
import com.bitnei.report.operationIndex.output.OperationIndexResultOutput
import com.bitnei.report.operationIndex.queryParam.{DbQueryParamParser, QueryParam, VehicleInfo}
import com.bitnei.sparkhelper.SparkHelper

import scala.util.{Failure, Success, Try}

/*
* created by wangbaosheng on 2017/11/10
*/
/**
  * 模块1，读取数据库，获取作业所需要的查询参数
  *   QueryParamParser,QueryParam
  * 模块2，根绝查询参数，读取hdfs数据，根绝算法计算结果，输出到hdfs。
  *
  * 模块3，输出模块
  * 模块4，算法模块，里程核查核心算法
  * */

object ErrorCode {
  val SystemFault = 1
  val ComputeException = 2
  val UNKNOW=3
  val OK:Int = 0
}


class OperationIndexMain(stateConf: StateConf) extends  Logging {
  def run(queryId: String): Int = {
    var exit=ErrorCode.OK
    val queryResultOutput = new OperationIndexResultOutput(queryId, stateConf)

    new DbQueryParamParser(stateConf).parse(queryId) match {
      case Success(Some(queryParam)) =>
        logInfo(queryParam.toString)

        logInfo(s"delteing data from sys_operation_result $queryId")
        queryResultOutput.delete(queryId)

        if (queryParam.registedCount > 0) {
          Try {
            SparkHelper.getSparkSession(None)
          }  match {
            case Success(sparkSession)=>
              try {
                val operationJob = new OperationIndexJob(sparkSession, stateConf, queryParam)
                operationJob.compute()
              } catch {
                case e: Exception =>
                  logError(s"operationIndex for ${queryParam.docId} throw an exception,so stopping SparkSession", e)
                  sparkSession.stop()
                  exit=ErrorCode.ComputeException
                  notifySuccessed(queryResultOutput,queryParam)
                  throw e
              }
            case Failure(e) =>
              logError(s"Operation for ${queryParam.docId} throw an system fault",e)
              exit=ErrorCode.SystemFault
              notifyFailed(queryResultOutput,queryParam)
              throw e
          }
        } else {
          logInfo("no registed vehicle....")
          queryResultOutput.output(
            queryParam.allVehicleList.map(vehicle => OperationIndexResult(vid=vehicle.vid, vin=vehicle.vin, licensePlate=Some(vehicle.licensePlate)))
          )
        }

        logInfo("begin udpate sys_operation_doc")
        //将一些全局结果写入到数据库
        notifySuccessed(queryResultOutput,queryParam)
      case Success(None) =>
        logError(s"$queryId, no such queryid:$queryId")
      case Failure(e) =>
        logError(s"$queryId,compute failed.", e)
    }
    exit
  }

  def notifyFailed(queryResultOutput:OperationIndexResultOutput,queryParam: QueryParam): Unit ={
    queryResultOutput.insert(
      queryParam.allVehicleList.length,
      queryParam.unregistedCount,
      queryParam.registedButUnloadCount,
      4)
  }

  def notifySuccessed(queryResultOutput:OperationIndexResultOutput,queryParam: QueryParam): Unit ={
    queryResultOutput.insert(
      queryParam.allVehicleList.length,
      queryParam.unregistedCount,
      queryParam.registedButUnloadCount,
      2)
  }
}


object OperationIndexMain extends  Logging {
  //系统故障：1 ，spark启动失败
  //计算异常：2，

  def main(args: Array[String]): Unit = {
    try {
      val stateConf = new StateConf
      stateConf.add(args)

      val exit = stateConf.getOption("queryid") match {
        case Some(queryId) =>
          log.info("queryid:" + queryId)
          new OperationIndexMain(stateConf).run(queryId)
        case None =>
          logError("no queryid.")
          ErrorCode.OK
      }
      System.exit(exit)
    } catch {
      case e: Exception =>
        System.exit(ErrorCode.UNKNOW)
    }
  }
}package com.bitnei.report.operationIndex.job

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.mileage.{MileageComputer, MileageResult, Realinfo}
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper, SqlHelper}
import org.apache.hadoop.hbase.client.Table
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.sql.{Dataset, SparkSession}
import org.apache.spark.storage.StorageLevel

/*
* created by wangbaosheng on 2017/12/11
*/
class OperationIndexMileageJob (@transient sparkSession:SparkSession,stateConf:StateConf)
  extends Job with Serializable {
  override type R = MileageResult

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("realinfo")
  //里程表
  private val operationIndexMileageTableName = stateConf.getOption("output.table.name").getOrElse("operationIndexMileage")

  //明细表
  private val operationIndexDetailFormatTableName = stateConf.getOption("operationIndexDetailFormat.table.name").getOrElse("operationIndexDetail")

  //是否开启调试
  private  val enableDebugLog=stateConf.getOption("enableDebugLog").getOrElse("true")=="true"

  import sparkSession.implicits._


  override def registerIfNeed(): Unit = {
    sparkSession.udf.register("parse_time", (strTime: String) => parseTime(strTime))

    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)
  }

  def parseTime(strTime: String): Long = {
    Utils.parsetDate(strTime, "yyyyMMddHHmmss").map(_.getTime).getOrElse(0L)
  }

  override def unRegister(): Unit = sparkSession.catalog.dropTempView(inputTableName)


  override def doCompute[Product <: MileageResult](): Dataset[MileageResult] = {
    val operationIndexResult = getRealinfo.groupByKey(_.vin).mapGroups({
      case (vin: String, realinfoValues: Iterator[Realinfo]) => {
        //按照时间排序
        val sortedValues = Utils.sortByDate2[Realinfo](realinfoValues.toArray, x =>
          if (x.time == 0) None else Some(x.time))

        //里程计算
        val result = new MileageComputer(stateConf).compute(sortedValues)

        //如果开启了debug模式，那么输出详细日志
        if(enableDebugLog){
          stateConf.getOption("debug.vid") match {
            case Some(vid)=>
              //如果配置了debug.vid,那么只输出该vid的日志
              if(vid==result.vid) logDebug(sortedValues,result)
            case None=>
              logDebug(sortedValues,result)
          }
        }

        result
      }
    }).filter(_.counter.totalCount >= 0) //过滤掉没有数据的里程


    //因为计算车辆很少，并且每一辆车最后只有一条数据，所以没有必要使用太多的分区。
    operationIndexResult.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(5))
  }

  def logDebug(mileageValues:Array[Realinfo],result:MileageResult)={
    mileageValues.map(v=>{
      logInfo(v.toString)
    })

    logInfo("全局信息")
    logInfo(result.humanString)
  }

  def getRealinfo(): Dataset[Realinfo] = {
    val sql =
      s"""
       SELECT
         r.vid,
         r.vin,
         parse_time(r.time) AS time,
         CAST((r.`2202`* 0.1) AS DOUBLE) AS mileage,
         CAST(r.`2613` * 0.1 AS DOUBLE) AS totalVoltage,
         CAST(r.`2614` *0.1 AS DOUBLE) AS totalCharge,
         CAST(r.`2502` * 0.000001 AS DOUBLE) AS longitude,
         CAST(r.`2503` * 0.000001 AS DOUBLE) AS latitude ,
         CASE WHEN `3201` is not null THEN CAST(`2615` AS INT ) ELSE CAST(`2615` * 0.4 AS INT) END AS soc,
         CAST(r.`2201` *0.1 AS DOUBLE) AS speed,
         CAST(r.`3201` AS  Int) AS state
         FROM $inputTableName AS r
         ${SqlHelper.buildWhere(stateConf)} AND r.vin IS NOT NULL
        """.stripMargin
    sparkSession.sql(sql).as[Realinfo]
  }

  override def write[Product <: MileageResult](result: Dataset[MileageResult]): Unit = {
    def saveOperationResult(): Unit = {
      SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), operationIndexMileageTableName)
    }

    //存储明细数据到hdfs和hbase
    def saveOperationDetailFormat(detailDs: Dataset[OperationDetailFormatValue]): Unit = {
      SparkHelper.saveToPartition(sparkSession, stateConf, detailDs.toDF(), operationIndexDetailFormatTableName)

      (stateConf.getOption("hbase.quorum"), stateConf.getOption("hbase.zkport")) match {
        case (Some(quorm), Some(zkport)) =>
          logInfo(s"writting to hbase,$quorm:$zkport")
          detailDs.foreachPartition(values => new HbaseOutput().output(quorm, zkport, values.toIterable))
        case _ =>
          throw new RuntimeException("hbase.quorum or hbase.zkport not exists.")
      }
    }


    val outputModels = stateConf.getOption("report.output").getOrElse("hdfs")
      .split(",")

    if (outputModels.length > 1) result.cache()
    outputModels.foreach(outputModel => {
      if (outputModel == "hdfs") {
        saveOperationResult()
      } else if (outputModel == "hbase") {
        //如果开启明细计算，那么计算明细
        if (stateConf.getOption("computeDetail").contains("true")) {
          saveOperationDetailFormat(result.mapPartitions(values => {
            val format = new OperationIndexDetailFormat
            //格式化明细数据
            format.mapOperationValueToDetailValues(values.toIterable).toIterator
          }))
        }
      }
    })
  }


  def stop(): Unit = {
    sparkSession.stop()
  }
}


object OperationIndexMileageJob extends  Logging{
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    batchProcess(stateConf, (spark, stateConf, operationIndex) => {
      operationIndex.write(operationIndex.doCompute())
    })
  }

  def batchProcess(stateConf: StateConf,f:(SparkSession,StateConf,OperationIndexMileageJob)=>Unit): Unit ={
    val spark = SparkHelper.getSparkSession(sparkMaster = None)
    val com = new OperationIndexMileageJob(spark, stateConf)
    com.registerIfNeed()
    if (stateConf.getOption("batchProcess.enable").contains("true")) {
      val startDate = Utils.parsetDate(stateConf.getString("startDate"), "yyyyMMdd").get
      val endDate = Utils.parsetDate(stateConf.getString("endDate"), "yyyyMMdd").get


      while (startDate.getTime <= endDate.getTime) {
        val year = Utils.formatDate(startDate, "yyyy")
        val month = Utils.formatDate(startDate, "MM")
        val day = Utils.formatDate(startDate, "dd")

        SparkHelper.setPartitionValue(stateConf, "operationIndexMileage", Array(year, month, day))
        SparkHelper.setPartitionValue(stateConf, "operationIndexDetail", Array(year, month, day))

        val s=s"${year}${month}${day}"
        stateConf.set("report.date",s"$s-$s")

        logInfo(s"begin execute $year-$month-$day")

        f(spark,stateConf,com)
        startDate.setDate(startDate.getDate + 1)
      }
    } else {
      f(spark,stateConf,com)
    }
  }
}package com.bitnei.report.operationIndex.job

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.mileage.MileageResult
import com.bitnei.report.operationIndex.output.{ OperationIndexResultOutput}
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql.{Dataset, SparkSession}
import org.apache.spark.storage.StorageLevel

/*
* created by wangbaosheng on 2017/11/10
*/

class OperationIndexOutputJob(sparkSession: SparkSession,stateConf: StateConf) extends Logging with Job {
  override type R = OperationIndexResult
  private val sqlContext = sparkSession.sqlContext

  import sqlContext.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("operationIndex")

  override def registerIfNeed(){
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)
  }

  override def unRegister() {
    sparkSession.catalog.dropTempView(inputTableName)
  }


  private val queryId =stateConf.getString("queryId")

  override def doCompute[Product <: OperationIndexResult]():Dataset[OperationIndexResult] = {
    sqlContext.sql(s"SELECT * FROM $inputTableName where queryId=$queryId").as[OperationIndexResult]
  }


  override def write[Product <: OperationIndexResult](result: Dataset[OperationIndexResult]) {
    val outputModels = stateConf.getString("report.output").split(',')


    outputModels.foreach({
      case e if e == "oracle" || e == "mysql" =>
        //输出到数据库
        stateConf.set("database", e)

        logInfo(s"writting operation index result  to $e")
        result.foreachPartition(par => {
          val output = new OperationIndexResultOutput(queryId,stateConf)
          output.output(par.toIterable)
        })
      case e => throw new Exception(s"report.output=$e")
    })
  }
}


object OperationIndexOutputJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    new OperationIndexOutputJob(SparkHelper.getSparkSession(sparkMaster = None), stateConf).compute()
  }
}
package com.bitnei.report.operationIndex.job

import com.bitnei.report.common.mileage.{CheckMileageThreshold, Counter}

/*
* created by wangbaosheng on 2017/12/21
*/
case class OperationIndexResult(
                                 vid:String,
                                 vin:String,
                                 //车牌号
                                 licensePlate:Option[String]=None,

                                 //核算里程
                                 checkMileage:Double=0,
                                 checkMileagePath:Int=0,
                                 firstValidateMileage:Option[Double]=None, //第一帧有效仪表里程

                                 //最后一帧有效里程
                                 lastValidateMileage:Option[Double]=None, //
                                 //第一帧有效里程对应的时间
                                 firstTimeWithValidateMileage:Option[Long]=None,

                                 //最后一帧有效里程对应的时间
                                 lastTimeWithValidateMileage:Option[Long]=None,

                                 onlineMileage:Double=0, //上线里程
                                 validateMileage:Double=0, //有效里程

                                 totalGpsRepeationNum:Int=0,
                                 gpsMileage:Double=0, //轨迹里程

                                 onlineMileageAndValidateMileageRelativeError:Double=0,
                                 onlineMileageAndGPSMileageRelativeError:Double=0,

                                 stepMileageThreshold:Double=0,
                                 stepWindowNum:Int=0, //跳变窗口个数
                                 stepNum:Int=0, //跳变计次
                                 stepMileage:Double=0, //跳变里程

                                 continueCurrentStepMileage:Double=0, //连续电流扣除里程
                                 continueCurrentStepMileageNum:Int=0, //连续电流扣除计次

                                 unOnlineDayNum:Int=0,
                                 counter:Counter=Counter(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0) //缺失，异常等统计信息。
                        )package com.bitnei.report.operationIndex.output

import java.sql.Date
import java.util.UUID

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.DataPrecision
import com.bitnei.report.operationIndex.job.OperationIndexResult
import com.bitnei.report.operationIndex.queryParam.QueryParam

/*
* created by wangbaosheng on 2017/11/9
*/


class OperationIndexResultOutput(queryId:String,stateConf: StateConf)
  extends  OutputManager
  with Serializable
    with Logging {
  override type T = OperationIndexResult

  override def output(values: Iterable[OperationIndexResult]): Unit = {
    if(stateConf.getOption("operationResultOutput.version").contains("1")) {
     version1(values)
    }else{
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
        """
       INSERT INTO SYS_OPERATION_RESULT (
       ID,
       DOC_ID,
       VIN,
       LICENSE_PLATE,
       START_TIME,
       END_TIME,
       START_MILEAGE,
       END_MILEAGE,
       ONLINE_MILEAGE,
       VALIDATE_MILEAGE,
       GPS_MILEAGE,
       HOP_MILEAGE,
       ELECTRICITY_MILEAGE,
       CHECK_COUNT,
       INVALID_COUNT,
       EXCEPTION_COUNT,
       ELECTRICITY_COUNT,
       HOP_COUNT,
       INVALID_PERCENTAGE,
       EXCEPTION_PERCENTAGE,
       ONLINE_VALIDATE_DEVIATION,
       ONLINE_GPS_DEVIATION,
       CHECK_MILEAGE)
       VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
      """.stripMargin, stmt => {
          values.foreach(v => {
            logInfo(v.toString)
            stmt.setString(1, UUID.randomUUID().toString)
            stmt.setString(2, queryId)
            stmt.setString(3, v.vin)
            stmt.setString(4, v.licensePlate.getOrElse(""))
            stmt.setDate(5, v.firstTimeWithValidateMileage.map(new Date(_)).orNull)
            stmt.setDate(6, v.lastTimeWithValidateMileage.map(new Date(_)).orNull)

            stmt.setObject(7, v.firstValidateMileage.orNull)

            //第一帧有效里程
            stmt.setObject(8, v.lastValidateMileage.orNull)

            //在线里程
            stmt.setDouble(9, v.onlineMileage)
            //有效里程
            stmt.setDouble(10, v.validateMileage)
            //gls里程
            stmt.setDouble(11, v.gpsMileage)
            //跳变里程
            stmt.setDouble(12, v.stepMileage)

            //连续电流扣除里程
            stmt.setDouble(13, v.continueCurrentStepMileage)

            stmt.setInt(14, v.counter.totalCount)

            stmt.setInt(15, v.counter.emptyCount)
            stmt.setInt(16, v.counter.exceptionCount)
            //连续电流条数
            stmt.setInt(17, v.continueCurrentStepMileageNum)
            stmt.setInt(18, v.stepWindowNum)
            //数据缺失比率
            stmt.setInt(19, (v.counter.emptyPercent * 100).toInt)

            //数据异常比率
            stmt.setInt(20, (v.counter.exceptionPercent * 100).toInt)

            //上线里程和有效里程相对误差
            stmt.setInt(21, (v.onlineMileageAndValidateMileageRelativeError * 100).toInt)

            //上线里程和轨迹里程相对误差
            stmt.setInt(22, (v.onlineMileageAndGPSMileageRelativeError * 100).toInt)

            //核算里程
            stmt.setDouble(23, v.checkMileage)

            stmt.addBatch()
          })

          stmt.executeBatch()
        })
    }
  }

  def version1(values: Iterable[OperationIndexResult]): Unit ={
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      """
       INSERT INTO SYS_OPERATION_RESULT (
       ID,
       DOC_ID,
       VIN,
       LICENSE_PLATE,
       START_TIME,
       END_TIME,
       START_MILEAGE,
       END_MILEAGE,
       ONLINE_MILEAGE,
       VALIDATE_MILEAGE,
       GPS_MILEAGE,
       HOP_MILEAGE,
       ELECTRICITY_MILEAGE,
       CHECK_COUNT,
       INVALID_COUNT,
       EXCEPTION_COUNT,
       ELECTRICITY_COUNT,
       HOP_COUNT,
       INVALID_PERCENTAGE,
       EXCEPTION_PERCENTAGE,
       ONLINE_VALIDATE_DEVIATION,
       ONLINE_GPS_DEVIATION,
       CHECK_MILEAGE,
       NOT_ONLINE_COUNT)
       VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
      """.stripMargin, stmt => {
        values.foreach(v => {
          logInfo(v.toString)
          stmt.setString(1, UUID.randomUUID().toString)
          stmt.setString(2, queryId)
          stmt.setString(3, v.vin)
          stmt.setString(4, v.licensePlate.getOrElse(""))
          stmt.setDate(5, v.firstTimeWithValidateMileage.map(new Date(_)).orNull)
          stmt.setDate(6, v.lastTimeWithValidateMileage.map(new Date(_)).orNull)

          stmt.setObject(7, v.firstValidateMileage.orNull)

          //第一帧有效里程
          stmt.setObject(8, v.lastValidateMileage.orNull)

          //在线里程
          stmt.setDouble(9, v.onlineMileage)
          //有效里程
          stmt.setDouble(10, v.validateMileage)
          //gls里程
          stmt.setDouble(11, v.gpsMileage)
          //跳变里程
          stmt.setDouble(12, v.stepMileage)

          //连续电流扣除里程
          stmt.setDouble(13, v.continueCurrentStepMileage)

          stmt.setInt(14, v.counter.totalCount)

          stmt.setInt(15, v.counter.emptyCount)
          stmt.setInt(16, v.counter.exceptionCount)
          //连续电流条数
          stmt.setInt(17, v.continueCurrentStepMileageNum)
          stmt.setInt(18, v.stepWindowNum)
          //数据缺失比率
          stmt.setInt(19, (v.counter.emptyPercent * 100).toInt)

          //数据异常比率
          stmt.setInt(20, (v.counter.exceptionPercent * 100).toInt)

          //上线里程和有效里程相对误差
          stmt.setInt(21, (v.onlineMileageAndValidateMileageRelativeError * 100).toInt)

          //上线里程和轨迹里程相对误差
          stmt.setInt(22, (v.onlineMileageAndGPSMileageRelativeError * 100).toInt)

          //核算里程
          stmt.setDouble(23, v.checkMileage)

          stmt.setInt(24, v.unOnlineDayNum)
          stmt.addBatch()
        })

        stmt.executeBatch()
      })
  }


  def insert(totalCount: Int, unregisteredCount: Int, notUploadCount: Int, status: Int): Unit = {
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"""
        update SYS_OPERATION_DOC
        SET
          TOTAL_COUNT=$totalCount,
          UNREGISTERED_COUNT=$unregisteredCount,
          NOT_UPLOAD_COUNT=$notUploadCount,
          STATUS=$status
          WHERE id='$queryId' AND status IN(1,2,4)
      """.stripMargin, stmt => {
        stmt.addBatch()
      })
  }


  def delete(queryId: String): Unit = {
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"DELETE FROM sys_operation_result WHERE doc_id=?", stmt => {
        stmt.setString(1,queryId)
        stmt.addBatch()
      })
  }
}
package com.bitnei.report.operationIndex.job

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.mileage.MileageResult
import com.bitnei.report.constants.Constant
import com.bitnei.report.operationIndex.output.OperationIndexResultOutput
import com.bitnei.report.operationIndex.queryParam.DbQueryParamParser

/*
* created by wangbaosheng on 2017/11/14
*/

object OperationTest {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf

    def configOracle(): Unit = {
      stateConf.set(Constant.JdbcUserName, "EV1209")
      stateConf.set(Constant.JdbcPasswd, "ev1209")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.1.49:1521:evmsc")
      stateConf.set("database", "oracle")
    }

    //conifgMysql();
    configOracle()


    val queryid="402881b2606e2983016071f62cb70019"
    val queryParamPaser = new DbQueryParamParser(stateConf)
    val queryParam = queryParamPaser.parse(queryid)

    val o = new OperationIndexResultOutput(queryid, stateConf)
    val q=queryParam.get.get
    o.insert(q.allVehicleList.length,q.unregistedCount,q.registedButUnloadCount,2)

    o.output(q.allVehicleList.map(v=>OperationIndexResult(vin=v.vin,vid=v.vid,licensePlate=Some(v.licensePlate),unOnlineDayNum = 1)))
  }
}
package com.bitnei.alarm

import java.text.SimpleDateFormat

import com.bitnei.alarm.generator.{GpsMileageGenerator, RunStateInput, RunStateOrbitSplitGenerator}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.GeoUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 一车多终端判断算法
  *
  * 多个vid 在一定范围内 一定时间内 轨迹 时间 里程  soc 相似
  *
  * 轨迹相似
  * 里程数组 是不是相似
  * 时间
  * 速度
  * 电压
  * 电流
  *
  * 停靠点少
  *
  * create 2018-01-31 11:43
  *
  *
  * output:
  * 3. 一车多终端明细表（VIN、开始时间、结束时间、行驶时长、行驶里程、绑定组编号）(ORACLE)（23号初步结果）；
  *
  *
  */


object OrbitSimilarTerminal extends Serializable with Logging {


  /**
    * 时间近似处理
    *
    * @param input               输入数据
    * @param sampleTimeThreshold 相同阈值
    * @return
    */
  def comparaTime(input: Array[GroupedOutput], sampleTimeThreshold: Int): Array[GroupedOutput] = {

    val res = new ArrayBuffer[GroupedOutput]

    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    val arr = input.sortBy(_.startTime)

    var flag = 0
    for (i <- 0 until arr.length if i > flag) {

      if (i != arr.length - 1) {
        val pre = arr(i)
        val post = arr(i + 1)
        val startTimeDiff = (sdf.parse(pre.startTime).getTime - sdf.parse(post.startTime).getTime) / 1000
        val stopTimeDiff = (sdf.parse(pre.stopTime).getTime - sdf.parse(post.stopTime).getTime) / 1000
        if (Math.abs(startTimeDiff) <= sampleTimeThreshold && Math.abs(stopTimeDiff) <= sampleTimeThreshold) {
          res.append(pre)
          res.append(post)
          flag = i + 1
        }
      }

    }

    //    @tailrec
    //    def handlerTailRec(curIndex: Int): Unit = {
    //
    //      if (curIndex < res.length) {
    //
    //        handlerTailRec(curIndex + 1)
    //      } else {
    //
    //        return
    //      }

    res.toArray
  }

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    var date = stateConf.getOption("input.date").getOrElse("20180209")

    //参数校验
    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    val geoHashLen = stateConf.getOption("input.geohash.len").getOrElse("8").toInt

    val sampleDiffMileageRatio = stateConf.getOption("input.sample.mileage.ratio").getOrElse("10").toInt

    val sampleTimeThreshold = stateConf.getOption("input.sample.time.ratio").getOrElse("30").toInt

    //输出参数
    var outputTargets = stateConf.getOption("output").getOrElse("console")
    var outFormat = stateConf.getOption("output.format").getOrElse("#")
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
    ////////////////////////////////////////////////

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")
        //        sparkSession.read.json("data/realinfo/mock.txt").createOrReplaceTempView("realinfo")
        sparkSession.read.json("data/realinfo/terminal_orbit_similar.json").createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")

        //        val initDs = sparkSession
        //          .read
        //          .format("parquet")
        //          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}")
        //
        //        initDs.filter(x=>{
        //          val arr= Array("LVCB4L4D8HM004392","LVCB4L4D5HM004379")
        //          val vin = x.getAs[String]("VIN")
        //          arr.contains(vin)
        //        }).toJSON.repartition(1).write.json("/tmp/similar_realinfo")

      }

    }


    ////////////////////////////////////////业务逻辑//////////////////////////////


    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'

    val sql =
      s"""
        SELECT
        VID,
        VIN,
        `2000` AS TIME,
        `2201` AS speed,
        `2202` As mileage,
        `2615` As soc,
        `3201` As isstart,
        cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,
        cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat
        FROM realinfo
        where VID is not null and `2000` like '${date}%'  and `2201` is not null and `2202` is not null  and `2615` is not null  and `3201` is not null  and `2502` is not null and `2503` is not null
      """.stripMargin


    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String, String, String, String)]

    //    initDS.show(false)
    //    if (env.equals("prd")) {
    //      initDS = sparkSession.sql(s"SELECT VID,VIN,`2000` AS TIME,cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat FROM realinfo where VID is not null  and `2000` is not null and `2502` is not null and `2503` is not null and year='${year}' and month='${month}' and day='${day}'").as[(String, String, String, String, String)]
    //    }

    //    logInfo("源数据量：" + initDS.count())

    //    year='${year}' and month='${month}' and day='${day}'

    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        val lon = x._8
        val lat = x._9
        val cond01 = (lon != null && lon.nonEmpty && lat != null && lat.nonEmpty)
        var cond02 = false
        if (cond01) {
          val lonD = lon.toDouble
          val latD = lat.toDouble
          //在中国境内
          cond02 = (lonD <= 136 && lonD >= 73 && latD <= 54 && latD >= 3)
        }
        val speed = x._4.toString.toDouble
        val mileage = x._5.toString.toDouble
        val soc = x._6

        val vin = x._2


        cond02
        //        && speed > 0 && mileage > 0
      })


    //    Array("LVCB4L4DXHM004409","LVCB4L4D8HM004392","LVCB4L4D0HM004399","LVCB4L4D6HM004374","LVCB4L4D5HM004379","LVCB4L4D4HM004406","LVCB4L4D0HM004385","LVCB4L4D8HM004411","LVCB4L4D8HM004392","LVCB4L4D6HM004374","LZYTBGBW4F1052790","LZYTBGBW3F1052781","LZYTBGBW5F1052782","LZYTBGBW0F1052785","LZYTBGBW4F1052787","LZYTBGBW3F1038041","LZYTBGBW6F1052788","LZYTBGBW9F1052784","LZYTBGBW2F1052786","LZYTBGBW5F1052779","LZYTBGBW7F1052783","LZYTBGBW1F1052780","LZYTBGBW8F1052789","LZYTBGBW9E1023896","LZYTBGBW0E1023897")
    //
    //    filteredDS.show(false)

    //    return


    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val vin = x._2
          val time = x._3
          val speed = x._4.toString.toDouble / 10
          val mileage = x._5.toString.toDouble / 10
          val soc = x._6
          val isstart = x._7 //车辆状态 1.启动 2.熄火
          val lon = x._8.toDouble
          val lat = x._9.toDouble

          Input(vid, vin, time, speed, mileage, soc, isstart, lon, lat)
        })


    val runPointDS = mappedDS
      .groupByKey(_.vid)
      .flatMapGroups {
        case (vid, inputs) => {
          val arr = inputs.toArray.sortBy(_.time)
          val vin = arr.head.vin

          //TODO:轨迹切分
          val runstateInputs = arr.map(x => RunStateInput(x.vid, x.vin, x.time, x.speed, x.mileage, x.soc.toDouble, x.isstart.toInt, x.lon, x.lat))
          val runPoints = RunStateOrbitSplitGenerator.handle(runstateInputs)
          runPoints
        }
      }

    //    runPointDS.show(false)
    //    return

    //TODO:获取GPS区域中心和行驶里程
    val diffMileageGpoints = runPointDS.map(points => {

      //中心点 geohash
      val startPoint = points.head

      val stopPoint = points.last

      val minGPSLon = points.map(_.lon).min
      val maxGPSLon = points.map(_.lon).max

      val minGPSLat = points.map(_.lat).min
      val maxGPSLat = points.map(_.lat).max


      val centerLon = minGPSLon + ((maxGPSLon - minGPSLon) / 2)
      val centerLat = minGPSLat + ((maxGPSLat - minGPSLat) / 2)

      val geoHash = GeoUtils.getGeoHashOfBase32(centerLon, centerLat, geoHashLen)

      //里程
      val startMileage = points.head.mileage
      val stopMileage = points.last.mileage

      var diffMileage = 0

      if (stopMileage > startMileage) {
        diffMileage = ((stopMileage - startMileage) / sampleDiffMileageRatio).toInt
      }

      //给每个轨迹加上geohash值和轨迹
      GroupOrbit(geoHash, diffMileage, points.toArray)
    })


    //TODO:根据GeoHash 相同里程 筛选相似轨迹
    val diffMileageSampeGpoints =
      diffMileageGpoints
        .groupByKey(x => (x.geoHash, x.diffMileage))
        .mapGroups {
          case ((geoHash, diffMileage), gpointss: Iterator[GroupOrbit]) => {
            gpointss.toList
          }
        }
        .filter(x => x.length > 1)
        .flatMap(x => x)


    //TODO:输出相似组结果
    val result = diffMileageSampeGpoints.map(x => {

      //TODO:标识相似组

      val groupId = x.geoHash + "#" + x.diffMileage

      val vin = x.points.head.vin

      val vid = x.points.head.vid

      val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

      val startTime = x.points.head.time

      val stopTime = x.points.last.time

      val duration = (sdf.parse(stopTime).getTime - sdf.parse(startTime).getTime) / 1000

      val mileage = x.points.last.mileage - x.points.head.mileage

      val stopMileage = x.points.last.mileage

      val gpsmileage: Double = GpsMileageGenerator.handle(x.points) / 1000D

      GroupedOutput(groupId, vid, vin, startTime, stopTime, duration, mileage, gpsmileage)
    })
      .groupByKey(_.groupId).flatMapGroups {
      case (groupid, outputs) => {

        val arr = outputs.toArray

        //        val res = arr.groupBy(x => {
        //          val sdf = new SimpleDateFormat("yyyyMMddHHmmss")
        //          val starTime = sdf.parse(x.startTime).getTime / 1000 / sampleDiffTimeRatio
        //          val stopTime = sdf.parse(x.stopTime).getTime / 1000 / sampleDiffTimeRatio
        //          (starTime.toInt, stopTime.toInt)
        //        }).map {
        //          case ((startGroupTime, stopGroupTime), outputs) => {
        //            outputs.toList
        //          }
        //        }.filter(_.length > 1)

                arr.toList
//        comparaTime(arr, sampleTimeThreshold).toList
      }
    }.filter(_.mileage >= 1)


    //TODO:比较全天的轨迹


    //TODO：结果需要排除公交车的情况

    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
    }

    if (env.equals("prd")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      if (outputTargets.contains("hdfs")) {

        result.repartition(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }

    }

    sparkSession.stop()
  }


  case class Input(vid: String, vin: String, time: String, speed: Double, mileage: Double, soc: String, isstart: String, lon: Double, lat: Double)


  case class GroupOrbit(geoHash: String, diffMileage: Int, points: Array[RunStateInput])

  case class GroupedOutput(groupId: String, vid: String, vin: String, startTime: String, stopTime: String, duration: Long, mileage: Double, gpsmileage: Double)

}package com.bitnei.report.taxis
import java.sql.{BatchUpdateException, Date, Timestamp}

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant


class TaxisOutputManager(stateConf:StateConf,reportType:String) {
  val chargeOutputManager = new TaxisChargeOutputManager(stateConf, reportType)
  val runOutputManager = new TaxisRunOutputManager(stateConf, reportType)

  def output(values: Iterable[TaxisResult]): Unit = {
    stateConf.getOption("output.table").getOrElse("charge,run").split(",").foreach({
      case "charge" => chargeOutputManager.output(values)
      case "run" => runOutputManager.output(values)
    })
  }
}



class TaxisChargeOutputManager(stateConf:StateConf,reportType:String) extends OutputManager with Logging {
  val tableName = if (reportType == "day") {
    "veh_report_taxis_charge_week"
  } else if (reportType == "month") {
    "veh_report_taxis_charge_month"
  } else if (reportType == "year") {
    "veh_report_taxis_charge_year"
  } else throw new Exception(s"$reportType must be day ,month,year")

  override type T = TaxisResult

  val insertSql =
    s"""
       INSERT INTO $tableName(
       vid,
       report_date,
       category,
       start_Time,
       end_Time,
       acc_Run_Time ,
       start_Mileage ,
       end_Mileage ,
       mileage ,
       start_Soc ,
       end_Soc ,
       start_Longitude ,
       start_Latitude ,
       stop_Longitude ,
       stop_Latitude ,
       time_Between_Charge,
       charge,
       prev_Charge_End_Mileage,
       max_Curent,
       charge_hour,
       mileage_Between_Charge)
       VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
    """.stripMargin

  override def output(values: Iterable[T]): Unit = {
//    val sortedValues = Utils.strict[T](
//      values.toArray.sortWith((a, b) => a.vid == b.vid && a.startTime < b.startTime),
//      (curV, nextV) => curV.vid == nextV.vid && curV.startTime == nextV.startTime
//    )

    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(insertSql, stmt => {
        values.foreach(v => {
          if (v.category == Constant.ChargeState) {
            stmt.setString(1, v.vid)
            stmt.setDate(2, new Date(v.startTime)) //报表日期
            stmt.setInt(3, if (v.isQuickCharge.contains(true)) 1 else 0)
            stmt.setTimestamp(4, new Timestamp(v.startTime)) //开始时间
            stmt.setTimestamp(5, new Timestamp(v.endTime)) //结束时间
            stmt.setDouble(6, DataPrecision.toHour(v.accRunTime))
            stmt.setDouble(7, DataPrecision.mileage(v.startMileage))
            stmt.setDouble(8, DataPrecision.mileage(v.endMileage))
            stmt.setDouble(9, DataPrecision.mileage(v.endMileage - v.startMileage))
            stmt.setDouble(10, DataPrecision.soc(v.startSoc))
            stmt.setDouble(11, DataPrecision.soc(v.endSoc))
            stmt.setDouble(12, DataPrecision.latitude(v.startLongitude))
            stmt.setDouble(13, DataPrecision.latitude(v.startLatitude))
            stmt.setDouble(14, DataPrecision.latitude(v.stopLongitude))
            stmt.setDouble(15, DataPrecision.latitude(v.stopLatitude))
            stmt.setDouble(16, DataPrecision.toHour(v.timeBetweenCharge))
            stmt.setDouble(17, DataPrecision.charge(v.charge))
            stmt.setDouble(18, DataPrecision.mileage(v.prevChargeEndMileage))
            stmt.setDouble(19, DataPrecision.charge(v.maxCurent))
            stmt.setDouble(20, DataPrecision.toHour(v.endTime - v.startTime))
            stmt.setDouble(21, v.mileageBetweenCharge.getOrElse(0))
            stmt.addBatch()
          }
        })
        stmt.executeBatch()
      })
    } catch {
      case e: BatchUpdateException => logError("update error", e)
        values.filter(_.category==Constant.ChargeState).foreach(v => logError(v.toString))
        throw e
      case e: Exception => throw e
    }
  }
}



class TaxisRunOutputManager(val stateConf:StateConf,reportType:String) extends OutputManager with Logging{
  val tableName=if(reportType=="day"){
    "veh_report_taxis_run_week"
  }else if(reportType=="month"){
    "veh_report_taxis_run_month"
  }else if(reportType=="year"){
    "veh_report_taxis_run_year"
  }else throw new Exception(s"$reportType must be day ,month,year")

  override type T = TaxisResult
  val sql=
    s"""
       INSERT INTO $tableName(
      vid,
      report_date,
      start_Time,
      end_Time,
      acc_Run_Time ,
      start_Mileage ,
      end_Mileage ,
      mileage ,
      start_Soc ,
      end_Soc ,
      start_Longitude ,
      start_Latitude ,
      stop_Longitude ,
      stop_Latitude ,
       avg_Speed,
       prev_Charge_End_Time,
       prev_Charge_Max_Current,
       run_time_length)
       VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)
    """.stripMargin
  override def output(values: Iterable[T]): Unit = {
    try{
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        values.foreach(v => {
          if (v.category == Constant.TravelState) {
            stmt.setString(1, v.vid)
            stmt.setDate(2, new Date(v.startTime)) //报表日期
            stmt.setTimestamp(3, new Timestamp(v.startTime)) //开始时间
            stmt.setTimestamp(4, new Timestamp(v.endTime)) //结束时间
            stmt.setDouble(5, DataPrecision.toHour(v.accRunTime))
            stmt.setDouble(6, DataPrecision.mileage(v.startMileage))
            stmt.setDouble(7, DataPrecision.mileage(v.endMileage))
            stmt.setDouble(8, DataPrecision.mileage(v.endMileage - v.startMileage))
            stmt.setDouble(9, DataPrecision.soc(v.startSoc))
            stmt.setDouble(10, DataPrecision.soc(v.endSoc))
            stmt.setDouble(11, DataPrecision.latitude(v.startLongitude))
            stmt.setDouble(12, DataPrecision.latitude(v.startLatitude))
            stmt.setDouble(13, DataPrecision.latitude(v.stopLongitude))
            stmt.setDouble(14, DataPrecision.latitude(v.stopLatitude))
            stmt.setDouble(15, DataPrecision.speed(v.avgSpeed))
            stmt.setTimestamp(16, new Timestamp(v.prevChargeEndTime.getOrElse(0)))
            stmt.setDouble(17, DataPrecision.totalCurrent(v.prevChargeMaxCurrent.getOrElse(0)))
            stmt.setDouble(18, DataPrecision.toHour(v.endTime-v.startTime))
            stmt.addBatch()
          }
        })
        stmt.executeBatch()
      })
    }catch {
      case e:BatchUpdateException=> logError("update error",e)
      case e:Exception=>throw e
    }
  }
}package com.bitnei.tools.export.hbase

import java.text.SimpleDateFormat
import java.util

import breeze.util.ArrayUtil
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.sparkhelper.HbaseHelper
import com.bitnei.tools.util.{Base64Utils, EncodUtils, ValidateUtils}
import org.apache.commons.codec.binary.Base64
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase._
import org.apache.hadoop.hbase.codec.BaseEncoder
import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp
import org.apache.hadoop.hbase.filter.{CompareFilter, RowFilter, SingleColumnValueFilter, SubstringComparator}
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.{IdentityTableMapper, TableInputFormat, TableMapReduceUtil}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.io.compress.{CompressionCodec, GzipCodec}
import org.apache.hadoop.io.{LongWritable, Text}
import org.apache.hadoop.mapred.JobConf
import org.apache.hadoop.mapred.lib.MultipleTextOutputFormat
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat
import org.apache.log4j.{Level, Logger}
import org.apache.spark._
import org.apache.spark.sql.SparkSession
import org.json4s.jackson.Serialization
import org.spark_project.guava.io.BaseEncoding
import shapeless.ops.nat.GT.>

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 从hbase packet表 导出原始报文 归档压缩
  *
  *                输入：hbase packet
  *
  *                输出：
  *                文件名称：厂商编号/2017/10/10/AB3CDXJU5HR170044（VIN）
  *                内容：20171010121314,IyME/kxFM01HWEpVMkhSMTcwMDY1AQAIEQoTCjYtAAKy
  *                压缩：厂商编号/2017/10/10.tar.gz
  *
  *                问题1：一次性导出多久的数据
  *                问题2：厂商编号、vin 从哪里获取
  *                问题3：导出后，如何存储到硬盘中
  *
  *                10开始到34个字符
  *
  *                17给字节
  *
  *                create 2017-12-05 9:44
  *
  */
object PacketExporter extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数

    //查询条件数据 vids
    var selectDataPath = "data/vids.txt"

    val schema = Array[String](
      "key",
      "stime",
      "type",
      "verify",
      "data"
    )


    var startDate = "0"

    //时间参数 导出数据的截止日期
    var deadLineDate = "20180125"

    //需要查询的hbase表
    var htableName = stateConf.getString("input.htable")
    var zkquorum = stateConf.getString("input.zk.quorum")
    var zkport = stateConf.getString("input.zk.port")

    if (env.equals("local")) {
      htableName = "packet"
      //      zkquorum = "192.168.6.103,192.168.6.104,192.168.6.105"
      zkquorum = "192.168.2.128,192.168.2.129,192.168.2.140"
      zkport = "2181"
    }


    //输出参数
    var outputPath = "/tmp/zyt/packet-export" + System.currentTimeMillis()

    //参数处理
    if (!env.equals("local")) {
      startDate = stateConf.getOption("input.date.start").get

      deadLineDate = stateConf.getOption("input.date.end").get
      //      selectDataPath = "file://" + stateConf.getOption("input.data").get
      selectDataPath = stateConf.getOption("input.data").get

      if (env.equals("prd")) {
        outputPath = stateConf.getString("output.hdfs.path")
      }
    }

    //参数校验
    if (deadLineDate.length != 8) {
      throw new Exception("input.date error")
    }
    val year = deadLineDate.substring(0, 4)
    val month = deadLineDate.substring(4, 6)
    val day = deadLineDate.substring(6)


    ///////////////////////////////////////////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + deadLineDate).getOrCreate()
    import sparkSession.implicits._

    /////////////////////////////////////////////////////////////////////////////////////////////////////////

    val sparkContext = sparkSession.sparkContext

    val vidsArr = sparkContext.textFile(selectDataPath).collect().sortBy(x => x)

    //    vidsArr.foreach(println)

    //    println("需要导出数据的vid==>" + vidsArr.mkString(","))

    //    val subVidsArr = vidsArr.getSubArr(arr:Array,startInex)

    //读取Vids文件
    //    val selectDataDS = sparkContext.textFile(vidsArr)

    var step = stateConf.getInt("input.data.step", 1)
    println("step:" + step)


    var index = 0
    while (index <= vidsArr.length) {
      val start = index
      val next = index + step
      val subVids = vidsArr.slice(start, next)
      logInfo(start + "====>" + next)
      runjob(env, startDate, deadLineDate, step, schema, zkquorum, zkport, htableName, outputPath + "_from" + start, sparkContext, subVids)
      logInfo("子任务完成...")
      index = next
    }

    //////////////////////////////////////////////////////////////////////////////////////////////////////////
    logInfo("任务完成...")
    sparkSession.stop()

  }


  private def getDataFromHBaseByVids(hConn: Connection, table: Table, schema: Array[String], sdf: SimpleDateFormat, vids: Iterator[String], scan: Scan, startDate: String, deadLineDate: String) = {

    val sdf2 = new SimpleDateFormat("yyyyMMdd hh:mm:ss")

    val mapList = new mutable.ListBuffer[java.util.Map[String, String]]()

    import scala.collection.JavaConversions._

    vids.foreach(vid => {

      //startDate
      //范围查询 2018-01-26 20180113-20180119
      //	 0000d218-44aa-4e15-be39-8f66c602218f_1510172312000_2

      var startRowkey = vid + "_" + startDate + "_" + 0
      //      val startRowkey = vid + "_" + sdf.parse(deadLineDate).getTime + "_" + 0

      if (!startDate.equals("0")) {
        startRowkey = vid + "_" + sdf2.parse(startDate + " 00:00:00").getTime + "_" + 0
      }
      val stopRowkey = vid + "_" + sdf2.parse(deadLineDate + " 23:59:59").getTime + "_" + 99

      //      println(startRowkey)
      scan.setStartRow(Bytes.toBytes(startRowkey))
      scan.setStopRow(Bytes.toBytes(stopRowkey))

      scan.setMaxVersions()


      //testing
//      val filter = new SingleColumnValueFilter(Bytes.toBytes("cf"), Bytes.toBytes("stime"), CompareOp.EQUAL, Bytes.toBytes("20170830182428"))
//      scan.setFilter(filter)

      val resultScanner = table.getScanner(scan)

      for (res <- resultScanner) {

        val rowkey = Option(Bytes.toString(res.getRow)).getOrElse("")

        val get = new Get(res.getRow).setMaxVersions()

        val result = table.get(get)

        val b_stime = result.getValue(Bytes.toBytes("cf"), Bytes.toBytes("stime"))

        val stime = Bytes.toString(b_stime)

        val b_type = result.getValue(Bytes.toBytes("cf"), Bytes.toBytes("type"))

        val stype = Bytes.toString(b_type)

        val b_verify = result.getValue(Bytes.toBytes("cf"), Bytes.toBytes("verify"))

        val verify = Bytes.toString(b_verify)

        //data多版本
        val list: java.util.List[KeyValue] = result.getColumn(Bytes.toBytes("cf"), Bytes.toBytes("data"))

        //获取行
        for (kv <- list) {
          val map = new mutable.HashMap[String, String]
          val data = Option(new String(Base64.encodeBase64(kv.getValue))).getOrElse("")
          map.put("key", rowkey + "#" + kv.getTimestamp)
          map.put("data", data)
          map.put("stime", stime)
          map.put("type", stype)
          map.put("verify", verify)
          mapList.append(map)
        }



      }
    })

    table.close()
    hConn.close()

    mapList.toIterator
  }


  private def runjob(env: String, startDate: String, deadLineDate: String, step: Int, schema: Array[String], zkquorum: String, zkport: String, htableName: String, outputPath: String, sparkContext: SparkContext, vidsArr: Array[String]) = {

    val rdd = sparkContext.makeRDD(vidsArr)
      .map(x => (x, ""))

    val selectDataRDD = rdd.partitionBy(new RangePartitioner(step, rdd))
      .map(x => x._1)

    //val selectDataDS = sparkSession.read.textFile(selectDataPath).as[String]
    val sdf = new SimpleDateFormat("yyyyMMdd")


    // TODO: 数据源
    logInfo("读取hbase")

    logInfo("zkquorum:" + zkquorum)
    logInfo("zkport:" + zkport)


    val initRDD =
      selectDataRDD
        .mapPartitions(vids => {
          val conf = HBaseConfiguration.create()
          conf.set("hbase.zookeeper.quorum", zkquorum)
          conf.set("hbase.zookeeper.property.clientPort", zkport)
          conf.setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 300000)
          val hConn = ConnectionFactory.createConnection(conf)
          val table = hConn.getTable(TableName.valueOf(htableName))
          val scan = new Scan()

          //TODO:设定最大版本数
          scan.setMaxVersions()
          getDataFromHBaseByVids(hConn, table, schema, sdf, vids, scan, startDate, deadLineDate)
        })
    //    initRDD.count

    val result = initRDD
      .map(x => {
        val arr = new ArrayBuffer[String]()
        //	 0000d218-44aa-4e15-be39-8f66c602218f_1510172312000_2
        //        获取vid
        val key = x.get("key")
        val vid = key.split("_")(0)

        //服务器时间
        val jsonRes = x.get("stime") + "," + x.get("type") + "," + x.get("verify") + "," + x.get("data")
        (vid, jsonRes)
      })

    //    initDS.unpersist()

    //////////////////////输出/////////////
    // TODO: 输出

    if (env.equals("local")) {
      //      result.count
      result.foreach(println)
    } else {
      val jobConf = new JobConf
      //          jobConf.set("mapred.output.compress", "true")
      //          jobConf.set("mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec")
      //
      //设置MapReduce的输出的分隔符为逗号
      jobConf.set("mapred.textoutputformat.ignoreseparator", "true")
      jobConf.set("mapred.textoutputformat.separator", ",")

      result.saveAsHadoopFile(
        outputPath,
        classOf[String],
        classOf[String],
        classOf[RDDMultipleTextOutputFormat[_, _]], jobConf)

    }
  }

  class RDDMultipleTextOutputFormat[K, V]() extends MultipleTextOutputFormat[K, V]() {

    override def generateFileNameForKeyValue(key: K, value: V, name: String): String = {
      key.toString
    }
  }

}
package com.bitnei.tools.export.hbase

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.hadoop.mapred.JobConf
import org.apache.hadoop.mapred.lib.MultipleTextOutputFormat
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.{Partitioner, RangePartitioner, SparkConf, SparkContext}

import scala.util.parsing.json.JSON

/**
  *
  * @author zhangyongtian
  * @define 从hbase packet表 json 格式转换成
  *
  *                第一列是报文接收服务器时间，stime字段
  *                第二列是报文类型，type字段
  *                第三列是校验代码，verify字段
  *                第四列是报文数据，data字段，base64(原始报文)
  *
  *                create 2017-12-05 9:44
  *
  */
object PacketExporterConvert extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    var hdfsPath = "data/packet/hbase"

    if (!env.equals("local")) {
      hdfsPath = "/spark/hbase2hdfs/*/*"
    }

    ///////////////////////////////////////////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._

    val sc = sparkSession.sparkContext

    val hadoopConfiguration = sc.hadoopConfiguration
    val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConfiguration)

    ///////////////////////////////读取数据////////////////////////////////

    val initRDD = sc.textFile(hdfsPath)

    val result = initRDD.map(x => {

      val jsonStr = x.substring(x.indexOf("{"))

      val map: Map[String, Any] = JSON.parseFull(jsonStr).get.asInstanceOf[Map[String, Any]]

      val vid = map.get("vid").get

      val stime = map.get("stime").get
      val typeStr = map.get("type").get
      val verify = map.get("verify").get
      val data = map.get("data").get

      (vid, stime + "," + typeStr + "," + verify + "," + data)
    })

    //      .foreach(println)

    /////////////////////////////////////////////////////////////////////////////////////////////////////////

    var outputPath =  "/tmp/2018-02-26"
    // TODO: 输出

    val jobConf = new JobConf
    //          jobConf.set("mapred.output.compress", "true")
    //          jobConf.set("mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec")
    //
    //设置MapReduce的输出的分隔符为逗号
    jobConf.set("mapred.textoutputformat.ignoreseparator", "true")
    jobConf.set("mapred.textoutputformat.separator", ",")

    result.saveAsHadoopFile(
      outputPath,
      classOf[String],
      classOf[String],
      classOf[RDDMultipleTextOutputFormat[_, _]], jobConf)

    class RDDMultipleTextOutputFormat[K, V]() extends MultipleTextOutputFormat[K, V]() {

      override def generateFileNameForKeyValue(key: K, value: V, name: String): String = {
        key.toString
      }

    }



    def recursionFile(path:String): Unit ={



    }

    ////////////////////////////////////////////////////////////////////////////////
    sparkSession.stop()

    //自定义分区类，需继承Partitioner类
    //  class UsridPartitioner(numParts: Int) extends Partitioner {
    //    //覆盖分区数
    //    override def numPartitions: Int = numParts
    //
    //    //覆盖分区号获取函数
    //    override def getPartition(key: Any): Int = {
    //      key.toString.toInt
    //    }
    //  }

  }
}package com.bitnei.tools.export.hbase

import java.text.SimpleDateFormat
import java.util
import java.util.concurrent.atomic.AtomicInteger
import java.util.concurrent.{ExecutorService, Executors}

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.tools.export.hbase.PacketExporter.runjob
import org.apache.commons.codec.binary.Base64
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.client.{ConnectionFactory, Get, Scan}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, HConstants, TableName}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.{RangePartitioner, SparkConf, SparkContext}

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer
import scala.io.Source

/**
  *
  * @author zhangyongtian
  * @define 从hbase packet表 导出原始报文 归档压缩
  *
  *                输入：hbase packet
  *                输出：
  *                文件名称：厂商编号/2017/10/10/AB3CDXJU5HR170044（VIN）
  *                内容：20171010121314,IyME/kxFM01HWEpVMkhSMTcwMDY1AQAIEQoTCjYtAAKy
  *                压缩：厂商编号/2017/10/10.tar.gz
  *                问题1：一次性导出多久的数据
  *                问题2：厂商编号、vin 从哪里获取
  *                问题3：导出后，如何存储到硬盘中
  *                10开始到34个字符
  *                17给字节
  *                create 2017-12-05 9:44
  *
  */

//arg0:vids
//arg1:20171130
//scala -cp Tools-1.0.0.jar com.bitnei.tools.export.hbase.PacketExporterLocal
object PacketExporterLocal extends Serializable with Logging {

  def main(args: Array[String]): Unit = {


    val schema = Array[String](
      "key",
      "stime",
      "type",
      "verify",
      "data"
    )
    val sdf = new SimpleDateFormat("yyyyMMdd")

    //    val vids = Source.fromFile(args(0)).getLines().toArray
    //    时间参数 导出数据的截止日期
    //    var deadLineDate = args(1)


    val vids = Source.fromFile("data/export-vnum-uid/vids.txt").getLines().toArray
    var deadLineDate = "20160101"


    val htableName = "packet"

    //研发环境 	yf-nn2:2181,yf-dn1:2181,yf-dn2:2181
//    var zkquorum = "192.168.6.103,192.168.6.104,192.168.6.105"
//    var zkport = "2181"

    //生产环境
        val zkquorum = "192.168.2.70,192.168.2.71,192.168.2.89"
        val zkport = "2181"

    val conf = HBaseConfiguration.create()
    conf.set("hbase.zookeeper.quorum", zkquorum)
    conf.set("hbase.zookeeper.property.clientPort", zkport)
    conf.setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 300000)


    //    var step = stateConf.getInt("input.data.step", 1)
    var step = new AtomicInteger(1);
    println("step:" + step)

    //创建线程池
    //    val threadPool: ExecutorService = Executors.newFixedThreadPool((vids.length / step) + 1)


    val index = new AtomicInteger(0);

    while (index.get() <= vids.length) {
      val start = index
      val next = new AtomicInteger(start.addAndGet(step.get()))
      val subVids = vids.slice(start.get(), next.get()).toVector

      println(start + "====>" + next)

      this.synchronized {
        val myThread = new Thread(new Runnable {
          override def run(): Unit = {
//                      runjob(schema, htableName, conf, subVids, sdf, deadLineDate)
            println(index + "子任务完成...")
          }
        })

        //      myThread.setDaemon(true)
        myThread.start()

      }


      index.set(next.get())
    }
    //////////////////////////////////////////////////////////////////////////////////////////////////////////
    println("任务完成...")


  }


  private def runjob(schema: Array[String], htableName: String, conf: Configuration, subVids: Array[String], sdf: SimpleDateFormat, deadLineDate: String) = {
    import scala.collection.JavaConversions._
    val con = ConnectionFactory.createConnection(conf)
    val table = con.getTable(TableName.valueOf(htableName))
    val scan = new Scan()

    val result = subVids
      .flatMap(vid => {
        val mapList = new mutable.ListBuffer[util.Map[String, String]]()

        //范围查询
        //	 0000d218-44aa-4e15-be39-8f66c602218f_1510172312000_2
        val startRowkey = vid + "_" + 0 + "_" + 0
        val stopRowkey = vid + "_" + sdf.parse(deadLineDate).getTime + "_" + 99

        scan.setStartRow(Bytes.toBytes(startRowkey))
        scan.setStopRow(Bytes.toBytes(stopRowkey))

        val resultScanner = table.getScanner(scan)

        for (res <- resultScanner) {
          //获取行
          val map = new mutable.HashMap[String, String]()

          for (col <- schema) {
            if (col.equals("key")) {
              map.put("key", Option(Bytes.toString(res.getRow)).getOrElse(""))
            } else {
              val family = Bytes.toBytes("cf")
              val bCol = Bytes.toBytes(col)
              val get = new Get(res.getRow).addColumn(family, bCol)
              val col_value = table.get(get).getValue(family, bCol);
              map.put(col, Option(Bytes.toString(col_value)).getOrElse(""))
            }
          }
          //        println(map)
          mapList.append(map)
        }
        mapList
      }
      )
      .map(x => {
        val arr = new ArrayBuffer[String]()
        for (col <- schema) {
          var value = x.get(col)
          //转成Hbase 64
          if (col.equals("data")) {
            val byteBase64Data = Base64.encodeBase64(value.getBytes("UTF-8"))
            value = new String(byteBase64Data)
            //            println(value)
          }
          arr.append(value)
        }
        //	 0000d218-44aa-4e15-be39-8f66c602218f_1510172312000_2
        //        获取vid
        val key = x.get("key")
        val vid = key.split("_")(0)
        //println(vid)
        (vid, arr.mkString(","))
      })
      .groupBy(_._1)
      .foreach(x => {
        val vid = x._1
        val arr = x._2.map(_._2)

        var out = new java.io.FileWriter("./" + vid + ".txt", true)
        arr.foreach(x => {
          out.write(x.toString() + "\n")

        })
        out close
      })

    table.close()
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

// $example on$
import org.apache.spark.graphx.GraphLoader
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * A PageRank example on social network dataset
 * Run with
 * {{{
 * bin/run-example graphx.PageRankExample
 * }}}
 */
object PageRankExample {
  def main(args: Array[String]): Unit = {
    // Creates a SparkSession.
    val spark = SparkSession
      .builder
      .appName(s"${this.getClass.getSimpleName}")
      .getOrCreate()
    val sc = spark.sparkContext

    // $example on$
    // Load the edges as a graph
    val graph = GraphLoader.edgeListFile(sc, "data/graphx/followers.txt")
    // Run PageRank
    val ranks = graph.pageRank(0.0001).vertices
    // Join the ranks with the usernames
    val users = sc.textFile("data/graphx/users.txt").map { line =>
      val fields = line.split(",")
      (fields(0).toLong, fields(1))
    }
    val ranksByUsername = users.join(ranks).map {
      case (id, (username, rank)) => (username, rank)
    }
    // Print the result
    println(ranksByUsername.collect().mkString("\n"))
    // $example off$
    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming.clickstream

import java.io.PrintWriter
import java.net.ServerSocket
import java.util.Random

/** Represents a page view on a website with associated dimension data. */
class PageView(val url: String, val status: Int, val zipCode: Int, val userID: Int)
    extends Serializable {
  override def toString(): String = {
    "%s\t%s\t%s\t%s\n".format(url, status, zipCode, userID)
  }
}

object PageView extends Serializable {
  def fromString(in: String): PageView = {
    val parts = in.split("\t")
    new PageView(parts(0), parts(1).toInt, parts(2).toInt, parts(3).toInt)
  }
}

// scalastyle:off
/**
 * Generates streaming events to simulate page views on a website.
 *
 * This should be used in tandem with PageViewStream.scala. Example:
 *
 * To run the generator
 * `$ bin/run-example org.apache.spark.examples.streaming.clickstream.PageViewGenerator 44444 10`
 * To process the generated stream
 * `$ bin/run-example \
 *    org.apache.spark.examples.streaming.clickstream.PageViewStream errorRatePerZipCode localhost 44444`
 *
 */
// scalastyle:on
object PageViewGenerator {
  val pages = Map("http://foo.com/" -> .7,
                  "http://foo.com/news" -> 0.2,
                  "http://foo.com/contact" -> .1)
  val httpStatus = Map(200 -> .95,
                       404 -> .05)
  val userZipCode = Map(94709 -> .5,
                        94117 -> .5)
  val userID = Map((1 to 100).map(_ -> .01): _*)

  def pickFromDistribution[T](inputMap: Map[T, Double]): T = {
    val rand = new Random().nextDouble()
    var total = 0.0
    for ((item, prob) <- inputMap) {
      total = total + prob
      if (total > rand) {
        return item
      }
    }
    inputMap.take(1).head._1 // Shouldn't get here if probabilities add up to 1.0
  }

  def getNextClickEvent(): String = {
    val id = pickFromDistribution(userID)
    val page = pickFromDistribution(pages)
    val status = pickFromDistribution(httpStatus)
    val zipCode = pickFromDistribution(userZipCode)
    new PageView(page, status, zipCode, id).toString()
  }

  def main(args: Array[String]) {
    if (args.length != 2) {
      System.err.println("Usage: PageViewGenerator <port> <viewsPerSecond>")
      System.exit(1)
    }
    val port = args(0).toInt
    val viewsPerSecond = args(1).toFloat
    val sleepDelayMs = (1000.0 / viewsPerSecond).toInt
    val listener = new ServerSocket(port)
    println("Listening on port: " + port)

    while (true) {
      val socket = listener.accept()
      new Thread() {
        override def run(): Unit = {
          println("Got client connected from: " + socket.getInetAddress)
          val out = new PrintWriter(socket.getOutputStream(), true)

          while (true) {
            Thread.sleep(sleepDelayMs)
            out.write(getNextClickEvent())
            out.flush()
          }
          socket.close()
        }
      }.start()
    }
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming.clickstream

import org.apache.spark.examples.streaming.StreamingExamples
import org.apache.spark.streaming.{Seconds, StreamingContext}

// scalastyle:off
/**
 * Analyses a streaming dataset of web page views. This class demonstrates several types of
 * operators available in Spark streaming.
 *
 * This should be used in tandem with PageViewStream.scala. Example:
 * To run the generator
 * `$ bin/run-example org.apache.spark.examples.streaming.clickstream.PageViewGenerator 44444 10`
 * To process the generated stream
 * `$ bin/run-example \
 *    org.apache.spark.examples.streaming.clickstream.PageViewStream errorRatePerZipCode localhost 44444`
 */
// scalastyle:on
object PageViewStream {
  def main(args: Array[String]) {
    if (args.length != 3) {
      System.err.println("Usage: PageViewStream <metric> <host> <port>")
      System.err.println("<metric> must be one of pageCounts, slidingPageCounts," +
                         " errorRatePerZipCode, activeUserCount, popularUsersSeen")
      System.exit(1)
    }
    StreamingExamples.setStreamingLogLevels()
    val metric = args(0)
    val host = args(1)
    val port = args(2).toInt

    // Create the context
    val ssc = new StreamingContext("local[2]", "PageViewStream", Seconds(1),
      System.getenv("SPARK_HOME"), StreamingContext.jarOfClass(this.getClass).toSeq)

    // Create a ReceiverInputDStream on target host:port and convert each line to a PageView
    val pageViews = ssc.socketTextStream(host, port)
                       .flatMap(_.split("\n"))
                       .map(PageView.fromString(_))

    // Return a count of views per URL seen in each batch
    val pageCounts = pageViews.map(view => view.url).countByValue()

    // Return a sliding window of page views per URL in the last ten seconds
    val slidingPageCounts = pageViews.map(view => view.url)
                                     .countByValueAndWindow(Seconds(10), Seconds(2))


    // Return the rate of error pages (a non 200 status) in each zip code over the last 30 seconds
    val statusesPerZipCode = pageViews.window(Seconds(30), Seconds(2))
                                      .map(view => ((view.zipCode, view.status)))
                                      .groupByKey()
    val errorRatePerZipCode = statusesPerZipCode.map{
      case(zip, statuses) =>
        val normalCount = statuses.count(_ == 200)
        val errorCount = statuses.size - normalCount
        val errorRatio = errorCount.toFloat / statuses.size
        if (errorRatio > 0.05) {
          "%s: **%s**".format(zip, errorRatio)
        } else {
          "%s: %s".format(zip, errorRatio)
        }
    }

    // Return the number unique users in last 15 seconds
    val activeUserCount = pageViews.window(Seconds(15), Seconds(2))
                                   .map(view => (view.userID, 1))
                                   .groupByKey()
                                   .count()
                                   .map("Unique active users: " + _)

    // An external dataset we want to join to this stream
    val userList = ssc.sparkContext.parallelize(Seq(
      1 -> "Patrick Wendell",
      2 -> "Reynold Xin",
      3 -> "Matei Zaharia"))

    metric match {
      case "pageCounts" => pageCounts.print()
      case "slidingPageCounts" => slidingPageCounts.print()
      case "errorRatePerZipCode" => errorRatePerZipCode.print()
      case "activeUserCount" => activeUserCount.print()
      case "popularUsersSeen" =>
        // Look for users in our existing dataset and print it out if we have a match
        pageViews.map(view => (view.userID, 1))
          .foreachRDD((rdd, time) => rdd.join(userList)
            .map(_._2._2)
            .take(10)
            .foreach(u => println("Saw user %s at time %s".format(u, time))))
      case _ => println("Invalid metric entered: " + metric)
    }

    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
package com.bitnei.alarm.generator

import java.text.SimpleDateFormat

import com.bitnei.report.util.GeoUtils

import scala.annotation.tailrec
import scala.collection.mutable.ArrayBuffer

/**
  * 车辆停靠算法
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-19 18:32
  *
  */

case class ParkInput(time: String, speed: Double, lon: Double, lat: Double)

case class ParkOutput(vid: String, startTime: String, stopTime: String, coords: Array[Array[Double]], geohash: String)

object ParkInfoGenerator {



  /**
    * 车辆数据处理，生成停靠点信息
    *
    * @param vid          车辆vid
    * @param inputArr     输入一辆车的实时数据
    * @param PARK_RADIUS  停车半径
    * @param MIN_DURATION 停车的最小时间
    * @return
    */
  def handler(vid: String, inputArr: Array[ParkInput], PARK_RADIUS: Double, MIN_DURATION: Int): ArrayBuffer[ParkOutput] = {

    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    val parkInfos = new ArrayBuffer[ParkOutput]

    import scala.util.control.Breaks._

    var flag = 0

    //排序
    val arr = inputArr.sortBy(_.time)

    for (i <- 0 until arr.length if i >= flag) {

      val startLon = arr(i).lon
      val startLat = arr(i).lat

      //数据格式：20171101434937  格式化
      val startTimeStr = arr(i).time
      val startTime = sdf.parse(startTimeStr).getTime

      var endIndex = 0

      breakable(
        for (j <- (i + 1) until arr.length) {

          val nextLon = arr(j).lon
          val nextLat = arr(j).lat

          val start2NextDistance = GeoUtils.getDistance(startLon, startLat, nextLon, nextLat)


          //判定最后一个停车行为（条件：距离半径）
          //                val cond01 = (j == (arr.length - 1) && start2NextDistance <= PARK_RADIUS)

          val cond01 = (j == (arr.length - 1))

          //判断停车行为（条件：距离半径）
          val start2LastDistance = GeoUtils.getDistance(startLon, startLat, arr(j - 1).lon, arr(j - 1).lat)


          val cond02 = (j != (arr.length - 1) && start2LastDistance <= PARK_RADIUS && start2NextDistance > PARK_RADIUS)

          //    logInfo("start2NextDistance:" + start2NextDistance)

          //找到第一个符合距离条件的结束点
          if (cond01 || cond02) {
            //                  if (cond01) {
            //                    logInfo(arr.length)
            //                    logInfo(j)
            //                    logInfo("start2LastDistance:" + start2LastDistance)
            //                    logInfo("start2EndDistance:" + start2NextDistance)
            //                  }
            endIndex = j
            flag = j
            break()
          }

        })

      if (endIndex != 0) {

        //结束坐标点
        val endLon = arr(endIndex).lon
        val endLat = arr(endIndex).lat

        val stopTimeStr = arr(endIndex).time
        val stopTime = sdf.parse(stopTimeStr).getTime

        val duration = (stopTime - startTime) / 1000

        //判定条件2 时间差 秒 大于最小时间
        if (duration >= MIN_DURATION) {
          //加入结果集
          val startCoord = Array(startLon, startLat)
          val stopCoord = Array(endLon, endLat)
          val coords: Array[Array[Double]] = Array(startCoord, stopCoord)

          //获取geoHash值 精确到${MAX_DISTANCE} 用于合并停车点
          val geohash = GeoUtils.getGeoHashOfBase32(startLon, startLat, 6)
          parkInfos.append(ParkOutput(vid: String, startTimeStr: String, stopTimeStr: String, coords: Array[Array[Double]], geohash: String))
        }
      }
    }

    parkInfos
  }


  //连续10帧车速为0，判断车辆处于停靠状态 未完成
  def handler2(vid: String, inputArr: Array[ParkInput], PARK_RADIUS: Double, MIN_DURATION: Int): ArrayBuffer[ParkOutput] = {

    val res = new ArrayBuffer[ParkOutput]

    var count = 0

    var startTime = ""
    var stopTime = ""

    @tailrec
    def getParkinfoTailRec(curIndex: Int, parkInput: ParkInput): ArrayBuffer[ParkOutput] = {

      if (curIndex < inputArr.length) {
        if (count == 10) {
          var stopTime = parkInput.time
          res.append(ParkOutput(vid, parkInput.time, parkInput.time, Array(Array(parkInput.lon, parkInput.lat)), "123"))
          count = 0
          startTime = ""
          stopTime = ""
        }
        if (inputArr(curIndex).speed == 0) {
          count = count + 1
          startTime = parkInput.time
        }else{
          count=0
        }
        getParkinfoTailRec(curIndex + 1, inputArr(curIndex + 1))
      } else {
        res
      }
    }

    res
  }


}
package com.bitnei.report

import java.text.SimpleDateFormat

import ch.hsr.geohash.GeoHash
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.{GeoUtils, JsonBase, MockDataProvider}
import com.bitnei.sparkhelper.{HbaseHelper, SparkHelper}
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import com.twitter.chill.Kryo
import org.apache.hadoop.hbase.util.Bytes
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.serializer.KryoRegistrator
import org.apache.spark.sql.{SaveMode, SparkSession}
import org.apache.spark.storage.StorageLevel

import scala.collection.mutable.ArrayBuffer


case class ParkInfo(vid: String, startTime: String, stopTime: String, coords: Array[Array[Double]], geohash: String)

case class Coord(lon: Double, lat: Double)

case class StationAndParkInfo(station: Coord, parkInfo: ParkInfo)

case class ParkHeat(station: Coord, heat: Long, parkDetails: Array[ParkDetail])

case class ParkDetail(vid: String, durations: Long, startCoords: Array[Coord])

case class OutPut(center: String, heat: Long)

/**
  *
  *
  *
  * 3月
      至少 30分钟 停车
  1
    一次 停车时长 停车经纬度  初始点
  10米

  2

  3


  * @author zhangyongtian
  * @define 停车热力图
  *
  * create 2017-11-21 14:45
  *
  */
object ParkingHeat extends Serializable with Logging with JsonBase[ParkDetail] {

  case class RealinfoInput(vid: String, vin: String, time: String, lon: Double, lat: Double)


  class MyRegistrator extends KryoRegistrator {

    override def registerClasses(kryo: Kryo) {
      kryo.register(classOf[String])
      kryo.register(classOf[RealinfoInput])
      kryo.register(classOf[ParkInfo])
    }
  }


  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    var date = "20171102"

    //停车半径阈值 米
    var PARK_RADIUS = 1

    //秒
    var MIN_DURATION = 10 * 60
    //米
    var MAX_DISTANCE = 4

    if (MAX_DISTANCE < PARK_RADIUS) {
      throw new Exception("input MAX_DISTANCE < PARK_RADIUS error")
    }
    //充电站数据存放路径
    val stationDataPath = stateConf.getString("input.station.data.path")

    //充电站周边停车点统计范围（米）
    var PARK_RANGE = 500

    //输出参数
    var outFormat = "#"
    var outputTargets = "console"


    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
      PARK_RADIUS = stateConf.getOption("input.park_radius").get.toInt
      MIN_DURATION = stateConf.getOption("input.min_duration").get.toInt
      MAX_DISTANCE = stateConf.getOption("input.max_distance").get.toInt
      PARK_RANGE = stateConf.getOption("input.park_range").get.toInt

      outputTargets = stateConf.getOption("output").get
      outFormat = stateConf.getOption("output.format").get
    }

    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)





    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    import sparkSession.implicits._

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        MockDataProvider.realInfo(sparkSession)
      }

      case "dev" => {
        //研发环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }
        ///tmp/zyt/data/realinfo

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }


    /////////////////////////////////业务逻辑//////////////////////////////////////////////

    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'

    var initDS = sparkSession.sql(s"SELECT VID,VIN,`2000` AS TIME,cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat FROM realinfo where VID is not null and `2000` like '${date}%'  and `2502` is not null and `2503` is not null ").as[(String, String, String, String, String)]

    //    initDS.show(false)
    //    if (env.equals("prd")) {
    //      initDS = sparkSession.sql(s"SELECT VID,VIN,`2000` AS TIME,cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat FROM realinfo where VID is not null  and `2000` is not null and `2502` is not null and `2503` is not null and year='${year}' and month='${month}' and day='${day}'").as[(String, String, String, String, String)]
    //    }

    //    logInfo("源数据量：" + initDS.count())

    //    year='${year}' and month='${month}' and day='${day}'

    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        val lon = x._4
        val lat = x._5
        val cond01 = (lon != null && lon.nonEmpty && lat != null && lat.nonEmpty)
        var cond02 = false
        if (cond01) {
          val lonD = lon.toDouble
          val latD = lat.toDouble
          //在中国境内
          cond02 = (lonD <= 136 && lonD >= 73 && latD <= 54 && latD >= 3)
        }
        cond02
      })

    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val vin = x._2
          val time = x._3
          val lon = x._4.toDouble
          val lat = x._5.toDouble
          RealinfoInput(vid: String, vin: String, time: String, lon: Double, lat: Double)
        })

    //计算停车点
    val parkingInfoDS = mappedDS
      .groupByKey(_.vid)
      .flatMapGroups {
        case (vid, values: Iterator[RealinfoInput]) => {

          val sortedRealinfoInputs = values.toArray[RealinfoInput].sortBy(_.time)

          val vin = sortedRealinfoInputs.head.vin

          val parkInfos = new ArrayBuffer[ParkInfo]

          // TODO: 停车判定

          //坐标按时间排序，两点GPS距离在${MAX_DISTANCE}米以内 且 时长在MIN_DURATION分钟以上 判定为停车状态
          //bug:没计算经过的点与开始点的距离范围

          import scala.util.control.Breaks._

          var flag = 0

          for (i <- 0 until sortedRealinfoInputs.length if i >= flag) {

            val startLon = sortedRealinfoInputs(i).lon
            val startLat = sortedRealinfoInputs(i).lat

            //数据格式：20171101434937  格式化
            val startTimeStr = sortedRealinfoInputs(i).time
            val startTime = sdf.parse(startTimeStr).getTime

            var endIndex = 0

            breakable(
              for (j <- (i + 1) until sortedRealinfoInputs.length) {

                val nextLon = sortedRealinfoInputs(j).lon
                val nextLat = sortedRealinfoInputs(j).lat

                val start2NextDistance = GeoUtils.getDistance(startLon, startLat, nextLon, nextLat)


                //判定最后一个停车行为（条件：距离半径）
                //                val cond01 = (j == (sortedRealinfoInputs.length - 1) && start2NextDistance <= PARK_RADIUS)

                val cond01 = (j == (sortedRealinfoInputs.length - 1))

                //判断停车行为（条件：距离半径）
                val start2LastDistance = GeoUtils.getDistance(startLon, startLat, sortedRealinfoInputs(j - 1).lon, sortedRealinfoInputs(j - 1).lat)


                val cond02 = (j != (sortedRealinfoInputs.length - 1) && start2LastDistance <= PARK_RADIUS && start2NextDistance > PARK_RADIUS)

                //    logInfo("start2NextDistance:" + start2NextDistance)

                //找到第一个符合距离条件的结束点
                if (cond01 || cond02) {
                  //                  if (cond01) {
                  //                    logInfo(sortedRealinfoInputs.length)
                  //                    logInfo(j)
                  //                    logInfo("start2LastDistance:" + start2LastDistance)
                  //                    logInfo("start2EndDistance:" + start2NextDistance)
                  //                  }
                  endIndex = j
                  flag = j
                  break()
                }

              })

            if (endIndex != 0) {

              //结束坐标点
              val endLon = sortedRealinfoInputs(endIndex).lon
              val endLat = sortedRealinfoInputs(endIndex).lat

              val stopTimeStr = sortedRealinfoInputs(endIndex).time
              val stopTime = sdf.parse(stopTimeStr).getTime

              val duration = (stopTime - startTime) / 1000

              //判定条件2 时间差 秒 大于最小时间
              if (duration >= MIN_DURATION) {
                //加入结果集
                val startCoord = Array(startLon, startLat)
                val stopCoord = Array(endLon, endLat)
                val coords: Array[Array[Double]] = Array(startCoord, stopCoord)

                //获取geoHash值 精确到${MAX_DISTANCE} 用于合并停车点
                val geohash = GeoUtils.getGeoHashOfBase32(startLon, startLat, 6)
                parkInfos.append(ParkInfo(vid: String, startTimeStr: String, stopTimeStr: String, coords: Array[Array[Double]], geohash: String))
              }
            }
          }

          parkInfos
        }
      }


    // TODO: 将地图划分为边长500米的方格 计算方格内所有的停车时长
    val result = parkingInfoDS.map(x => {

      val startTime = sdf.parse(x.startTime).getTime
      val stopTime = sdf.parse(x.stopTime).getTime

      val duration = (stopTime - startTime) / 1000


//      x.coords(0).foreach(println)

      val startLon = x.coords(0)(0)
      val startLat = x.coords(0)(1)


      ParkDetail(x.vid,duration,Array(Coord(startLon,startLat)))
    })

    // TODO: 转换为停车明细
    //    parkingInfoDS
    //      .groupByKey(_.vid)
    //      .mapGroups{
    //        case (vid, values: Iterator[ParkInfo]) => {
    //          //一辆车所有的停车时长
    //          var duration =
    //
    //        }
    //      }

//    logInfo("station data path:" + stationDataPath)

    //      .toJSON

    //      .show(false)


    //    //join条件：distance（充电站坐标，停车开始坐标） <= 500
    //    parkingInfoDS.joinWith(chargeStationDS, condition=>{
    //      false
    //    })


    ///////////////////////////////////////////////////////////////////////////////

    if (outputTargets.split(",").length > 1)
      parkingInfoDS.persist(StorageLevel.MEMORY_ONLY_SER)

    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
    }

    if (outputTargets.contains("hdfs")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")


      //研发环境
//      var hdfsPath = s"/tmp/zyt/data/${app}/year=${year}/month=${month}/day=${day}"
      var hdfsPath = s"/tmp/zyt/data/${app}/year=${year}/month=${month}/day=${day}"

      //      /spark/vehicle/result/${app}
      val hdfsResult = result.mapPartitions(values => {
        toJson(values.toArray[ParkDetail]).toIterator
      }).toDF()

      if (env.equals("dev")) {
        //1
        hdfsResult.coalesce(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
      }

      if (env.equals("prd")) {
        //生产环境
        hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"
        hdfsResult.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }

      logInfo("输出到HDFS end ....")

    }


    if (outputTargets.contains("hbase")) {
      //TODO: 输出到HBase
      logInfo("输出到HBase start....")

      //研发环境 	yf-nn2:2181,yf-dn1:2181,yf-dn2:2181
      var quorum = "192.168.6.103,192.168.6.104,192.168.6.105"
      var zkport = "2181"

      if (env.equals("prd")) {
        //生产环境
        quorum = stateConf.getOption("hbase.quorum").getOrElse("192.168.2.70,192.168.2.71,192.168.2.89")
        zkport = stateConf.getOption("hbase.zkport").getOrElse("2181")

        //        quorum = "192.168.2.70,192.168.2.71,192.168.2.89"
        //        zkport = "2181"
      }

      val htableName = s"${app}"

      logInfo("zkQuorum========" + quorum)

      //      result.coalesce(80).foreachPartition(values => {
      //        HbaseHelper.bulkPut(quorum, zkport, htableName, (table) => {
      //          val mapper = new ObjectMapper()
      //          mapper.registerModule(DefaultScalaModule)
      //          values.foreach(value => {
      //            // 0000d218-44aa-4e15-be39-8f66c602218f_201710
      //            val rowKey = s"${value.vid}_${date}"
      //            //          table.delete(new Delete(Bytes.toBytes(rowKey)))
      //            table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "parkingheat", toJson(mapper, value)))
      //          })
      //        })
      //      })


      logInfo("输出到HBase end ....")
    }


    logInfo("任务完成...")


    sparkSession.stop()

  }

  //  def toJson(mapper: ObjectMapper, locus: ParkInfo): String = {
  //    val jsonValue = mapper.writeValueAsString(locus)
  //    jsonValue
  //  }
  //
  //  def toJson(locuses: Array[ParkInfo]): Array[String] = {
  //    val mapper = new ObjectMapper()
  //    mapper.registerModule(DefaultScalaModule)
  //    locuses.map(mapper.writeValueAsString(_))
  //  }


}
package com.bitnei.report

import scala.annotation.tailrec
import scala.collection.mutable.ArrayBuffer
import scala.util.control.Breaks.breakable
import scala.util.control.Breaks._

/**
  * Created by xiaoyuzhou on 2017/11/22.
  */
object ParkingHeatTest {
  def main(args: Array[String]): Unit = {
    val arr = Range(1, 9).toArray

    //    arr.foreach(println)
    //    java.util.Arrays.copyOfRange(arr, 0, arr.length).foreach(println)


    //    val res = test(0, arr)

    //    res.foreach(e => e.foreach(println))
    //    res.foreach(e => {
    //      e.foreach(print(_))
    //      println("---------------")
    //    })

    val MAX_DISTANCE = 1

    var flag = 0

    for (i <- 0 until arr.length) {

      println("i-------" + arr(i))

      breakable(
        for (j <- i until arr.length) {

          //        println(arr(j))

          if (i >= flag && j + 1 < arr.length && arr(j + 1) - arr(i) > MAX_DISTANCE) {
            println(arr(i), arr(j))
            flag = j + 1
            break()
          }

          if (i >= flag && (j + 1 == (arr.length - 1)) && arr(j + 1) - arr(i) <= MAX_DISTANCE) {
            println(arr(i), arr(j + 1))
            break()
          }


        })

    }

  }


}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.PCA
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}
// $example off$

@deprecated("Deprecated since LinearRegressionWithSGD is deprecated.  Use ml.feature.PCA", "2.0.0")
object PCAExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("PCAExample")
    val sc = new SparkContext(conf)

    // $example on$
    val data = sc.textFile("data/mllib/ridge-data/lpsa.data").map { line =>
      val parts = line.split(',')
      LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))
    }.cache()

    val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
    val training = splits(0).cache()
    val test = splits(1)

    val pca = new PCA(training.first().features.size / 2).fit(data.map(_.features))
    val training_pca = training.map(p => p.copy(features = pca.transform(p.features)))
    val test_pca = test.map(p => p.copy(features = pca.transform(p.features)))

    val numIterations = 100
    val model = LinearRegressionWithSGD.train(training, numIterations)
    val model_pca = LinearRegressionWithSGD.train(training_pca, numIterations)

    val valuesAndPreds = test.map { point =>
      val score = model.predict(point.features)
      (score, point.label)
    }

    val valuesAndPreds_pca = test_pca.map { point =>
      val score = model_pca.predict(point.features)
      (score, point.label)
    }

    val MSE = valuesAndPreds.map { case (v, p) => math.pow((v - p), 2) }.mean()
    val MSE_pca = valuesAndPreds_pca.map { case (v, p) => math.pow((v - p), 2) }.mean()

    println("Mean Squared Error = " + MSE)
    println("PCA Mean Squared Error = " + MSE_pca)
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.distributed.RowMatrix
// $example off$

object PCAOnRowMatrixExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("PCAOnRowMatrixExample")
    val sc = new SparkContext(conf)

    // $example on$
    val data = Array(
      Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),
      Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),
      Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0))

    val dataRDD = sc.parallelize(data, 2)

    val mat: RowMatrix = new RowMatrix(dataRDD)

    // Compute the top 4 principal components.
    // Principal components are stored in a local dense matrix.
    val pc: Matrix = mat.computePrincipalComponents(4)

    // Project the rows to the linear space spanned by the top 4 principal components.
    val projected: RowMatrix = mat.multiply(pc)
    // $example off$
    val collect = projected.rows.collect()
    println("Projected Row Matrix of principal component:")
    collect.foreach { vector => println(vector) }
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.PCA
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.rdd.RDD
// $example off$

object PCAOnSourceVectorExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("PCAOnSourceVectorExample")
    val sc = new SparkContext(conf)

    // $example on$
    val data: RDD[LabeledPoint] = sc.parallelize(Seq(
      new LabeledPoint(0, Vectors.dense(1, 0, 0, 0, 1)),
      new LabeledPoint(1, Vectors.dense(1, 1, 0, 1, 0)),
      new LabeledPoint(1, Vectors.dense(1, 1, 0, 0, 0)),
      new LabeledPoint(0, Vectors.dense(1, 0, 0, 0, 0)),
      new LabeledPoint(1, Vectors.dense(1, 1, 0, 0, 0))))

    // Compute the top 5 principal components.
    val pca = new PCA(5).fit(data.map(_.features))

    // Project vectors to the linear space spanned by the top 5 principal
    // components, keeping the label
    val projected = data.map(p => p.copy(features = pca.transform(p.features)))
    // $example off$
    val collect = projected.collect()
    println("Projected vector of principal component:")
    collect.foreach { vector => println(vector) }
  }
}
// scalastyle:on println
package com.bitnei.alarm

import java.text.SimpleDateFormat

import com.bitnei.alarm.SpeedPowerConsumer.{Output, getSpeedInterval}
import com.bitnei.alarm.generator.{RunStateInput, RunStateOrbitSplitGenerator}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.util.TimeUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{Row, SaveMode, SparkSession}

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 早 、晚高峰低速行驶下电耗与平均电耗对比
  *
  *
  *           *
  *           年月	省	市	第一部分分类内容	……
  *           *
  *           ===========================
  *           *
  *           早高峰平均车速
  *           *
  *           早高峰行驶时间
  *           *
  *           早高峰行驶里程
  *           *
  *           早高峰累计消耗SOC%
  *           *
  *           晚高峰平均车速
  *           *
  *           晚高峰行驶时间
  *           *
  *           晚高峰行驶里程
  *           *
  *           晚高峰累计消耗SOC%
  *           *
  *           早高峰行驶车速30Km/h以下时间
  *           *
  *           早高峰行驶车速30Km/h以下里程
  *           *
  *           早高峰车速30Km/h以下累计消耗SOC%
  *           *
  *           晚高峰行驶车速30Km/h以下时间
  *           *
  *           晚高峰行驶车速30Km/h以下里程
  *           *
  *           晚高峰车速30Km/h以下累计消耗SOC%
  *           *
  *
  *           统计方法：删除为0的数据；只统计连续有效数据不间断的；连续数据取值范围是指最少具备连续起止两条有效数据值；将有效值累计
  *
  *           ================
  *           高峰区间
  *           车速
  *           车速区间 30KM以下
  *           行驶时间
  *           行驶里程
  *           累计消耗SOC%
  *
  *
  *
  *
  *
  *
  *
  */


object PeekLowSpeedPowerConsumer extends Serializable with Logging {

  def getSpeedInterval(SPEED_INTERVALS: Map[String, Array[Double]], speed: Double): String = {

    var res = "#"

    SPEED_INTERVALS.foreach(x => {
      val arr = x._2
      if (speed >= arr(0) && speed <= arr(1)) {
        res = x._1
      }
    })

    res

  }

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    var date = stateConf.getOption("input.date").getOrElse("20180216")

    //参数校验
    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    val geoHashLen = stateConf.getOption("input.geohash.len").getOrElse("8").toInt

    val sampleDiffMileageRatio = stateConf.getOption("input.sample.mileage.ratio").getOrElse("10").toInt

    val sampleTimeThreshold = stateConf.getOption("input.sample.time.ratio").getOrElse("30").toInt

    //输出参数
    var outputTargets = stateConf.getOption("output").getOrElse("console")
    var outFormat = stateConf.getOption("output.format").getOrElse("#")
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
    ////////////////////////////////////////////////

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")
        //        sparkSession.read.json("data/realinfo/mock.txt").createOrReplaceTempView("realinfo")
        //        sparkSession.read.json("data/realinfo/20180216-2-one-car.json").createOrReplaceTempView("realinfo")
        sparkSession.read.json("data/realinfo/mock2.txt").createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")

        //        val initDs = sparkSession
        //          .read
        //          .format("parquet")
        //          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}")
        //
        //        initDs.filter(x=>{
        //          val arr= Array("LVCB4L4D8HM004392","LVCB4L4D5HM004379")
        //          val vin = x.getAs[String]("VIN")
        //          arr.contains(vin)
        //        }).toJSON.repartition(1).write.json("/tmp/similar_realinfo")

      }

    }


    ////////////////////////////////////////业务逻辑//////////////////////////////
    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'

    val sql =
      s"""
        SELECT
        VID,
        VIN,
        `2000` AS TIME,
        `2201` AS speed,
        `2202` As mileage,
        `2615` As soc,
        `3201` As isstart
        FROM realinfo
        where VID is not null and `2000` like '${date}%' and `2201` is not null and `2202` is not null  and `2615` is not null  and `3201` is not null
      """.stripMargin

    //`2201` is not null and `2202` is not null  and `2615` is not null  and `3201` is not null

    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String, String)]

    //    print(initDS.count())

    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        true
      })


    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val vin = x._2
          val time = x._3
          val speed = x._4.toString.toDouble / 10
          val mileage = x._5.toString.toDouble / 10

          val soc = x._6
          val isstart = x._7 //车辆状态 1.启动 2.熄火

          Input(vid, vin, time, speed, mileage, soc, isstart)
        })

    //TODO:结论
    val result = mappedDS
      .groupByKey(_.vid)
      .flatMapGroups {
        case (vid, inputs) => {
          val arr = inputs.toArray.sortBy(_.time)
          val vin = arr.head.vin


          val res = new ArrayBuffer[Output]()

          //TODO:轨迹切分（行驶状态）
          val runstateInputs = arr.map(x => RunStateInput(x.vid, x.vin, x.time, x.speed, x.mileage, x.soc.toDouble, x.isstart.toInt))

          val runPoints =
            RunStateOrbitSplitGenerator.handle(runstateInputs)

          //          val SPEED_INTERVALS = Array(
          //            SPEED_INTERVALS(0)
          //            SPEED_INTERVALS(0)
          //            SPEED_INTERVALS(0)
          //          )


          val SPEED_INTERVALS_MAP = Map[String, Array[Double]](
            "0-30" -> Array(0.001D, 30D)
          )

          val SPEED_INTERVALS = SPEED_INTERVALS_MAP.keySet.toArray


          //          val res = scala.collection.mutable.Map[String, Output]()


          //          SPEED_INTERVALS.foreach(x => {
          //            val runDuration = 0D
          //            val runMileage = 0D
          //            val consumSoc = 0D
          //
          //            val output = Output(vid, vin, x._1, runDuration, runMileage, consumSoc)
          //            res += (x._1 -> output)
          //          })

          //维度

          var morningPeak_runDuration = 0D
          var morningPeak_runMileage = 0D
          var morningPeak_consumSoc = 0D

          var nightPeak_runDuration = 0D
          var nightPeak_runMileage = 0D
          var nightPeak_consumSoc = 0D

          var morningPeak_runDuration0T30 = 0D
          var morningPeak_runMileage0T30 = 0D
          var morningPeak_consumSoc0T30 = 0D

          var nightPeak_runDuration0T30 = 0D
          var nightPeak_runMileage0T30 = 0D
          var nightPeak_consumSoc0T30 = 0D

          //          高峰区间
          //          车速
          //          车速区间 30KM以下
          //           行驶时间
          //           行驶里程
          //           累计消耗SOC%

          runPoints.foreach(orbit => {

            //如果跨区间，算后一帧的区间
            orbit.reduce((pre, post) => {

              val speedInterval: String = getSpeedInterval(SPEED_INTERVALS_MAP, post.speed)

              var diffMileage = post.mileage - pre.mileage

              val lastTimesatmp = sdf.parse(pre.time).getTime
              val curTimesatmp = sdf.parse(post.time).getTime
              val diffTimes = (curTimesatmp - lastTimesatmp).toDouble / 1000

              val diffSoc = pre.soc - post.soc

              //保证是行驶状态
              if (diffMileage > 0 && diffSoc >= 0) {

                if (TimeUtils.isMorningPeak(post.time)) {
                  morningPeak_runDuration += diffTimes
                  morningPeak_runMileage += diffMileage
                  morningPeak_consumSoc += diffSoc

                }

                if (TimeUtils.isnightPeak(post.time)) {
                  nightPeak_runDuration += diffTimes
                  nightPeak_runMileage += diffMileage
                  nightPeak_consumSoc += diffSoc

                }


                if (speedInterval.equals(SPEED_INTERVALS(0)) && TimeUtils.isMorningPeak(post.time)) {
                  morningPeak_runDuration0T30 += diffTimes
                  morningPeak_runMileage0T30 += diffMileage
                  morningPeak_consumSoc0T30 += diffSoc

                }

                if (speedInterval.equals(SPEED_INTERVALS(0)) && TimeUtils.isnightPeak(post.time)) {
                  nightPeak_runDuration0T30 += diffTimes
                  nightPeak_runMileage0T30 += diffMileage
                  nightPeak_consumSoc0T30 += diffSoc
                }

              }

              post
            })

          })


          var morning_avgSpeed = 0D
          if (morningPeak_runDuration != 0) {
            morning_avgSpeed = morningPeak_runMileage / (morningPeak_runDuration / 3600)
          }


          var night_avgSpeed = 0D
          if (nightPeak_runDuration != 0) {
            night_avgSpeed = nightPeak_runMileage / (nightPeak_runDuration / 3600)
          }


          if (morningPeak_runDuration > 0 || nightPeak_runDuration > 0){

            res += Output(vid, vin, date, "早高峰", morning_avgSpeed, morningPeak_runDuration, morningPeak_runMileage, morningPeak_consumSoc, morningPeak_runDuration0T30, morningPeak_runMileage0T30, morningPeak_consumSoc0T30)

            res += Output(vid, vin, date, "晚高峰", night_avgSpeed, nightPeak_runDuration, nightPeak_runMileage, nightPeak_consumSoc, nightPeak_runDuration0T30, nightPeak_runMileage0T30, nightPeak_consumSoc0T30)

          }



          //          * 车速区间
          //            * 行驶时间
          //            * 行驶里程
          //            * 消耗SOC数值
          res.toList
        }
      }



    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
    }

    if (env.equals("prd")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      if (outputTargets.contains("hdfs")) {

        result.repartition(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }

    }

    sparkSession.stop()
  }

  case class Input(vid: String, vin: String, time: String, speed: Double, mileage: Double, soc: String, isstart: String)


  case class Output(vid: String, vin: String, date: String, peakType: String, avgSpeed: Double, runDuration: Double, runMileage: Double, consumSoc: Double, runDuration0T30: Double, runMileage0T30: Double, consumSoc0T30: Double)

}package samples

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-15 11:03
  *
  */
//成员balance状态一旦被赋值，便不能更改
//因而它也是线程安全的
class Person(val age: Integer) {
  def getAge() = age
}

object Person {
  //创建新的对象来实现对象状态修改
  def increment(person: Person): Person = {
    new Person(person.getAge() + 1)
  }

}
//package com.bitnei.samples.akka
//
///**
//  *
//  * @author zhangyongtian
//  * @define akka 计算api
//  *
//  * create 2017-12-08 15:17
//  *
//  */
//
//import akka.actor._
//import akka.routing.RoundRobinRouter
//import akka.util.Duration
//import akka.util.duration._
//
//import scala.concurrent.duration.Duration
//
//object Pi extends App {
//
//  calculate(nrOfWorkers = 4, nrOfElements = 10000, nrOfMessages = 10000)
//
//  sealed trait PiMessage
//
//  case object Calculate extends PiMessage
//
//  case class Work(start: Int, nrOfElements: Int) extends PiMessage
//
//  case class Result(value: Double) extends PiMessage
//
//  case class PiApproximation(pi: Double, duration: Duration)
//
//  class Worker extends Actor {
//
//    def calculatePiFor(start: Int, nrOfElements: Int): Double = {
//      var acc = 0.0
//      for (i ← start until (start + nrOfElements))
//        acc += 4.0 * (1 - (i % 2) * 2) / (2 * i + 1)
//      acc
//    }
//
//    def receive = {
//      case Work(start, nrOfElements) ⇒
//        sender ! Result(calculatePiFor(start, nrOfElements)) // perform the work
//    }
//  }
//
//  class Master(nrOfWorkers: Int, nrOfMessages: Int, nrOfElements: Int, listener: ActorRef)
//    extends Actor {
//
//    var pi: Double = _
//    var nrOfResults: Int = _
//    val start: Long = System.currentTimeMillis
//
//    val workerRouter = context.actorOf(
//      Props[Worker].withRouter(RoundRobinRouter(nrOfWorkers)), name = "workerRouter")
//
//    def receive = {
//      case Calculate ⇒
//        for (i ← 0 until nrOfMessages) workerRouter ! Work(i * nrOfElements, nrOfElements)
//      case Result(value) ⇒
//        pi += value
//        nrOfResults += 1
//        if (nrOfResults == nrOfMessages) {
//          // Send the result to the listener
//          listener ! PiApproximation(pi, duration = (System.currentTimeMillis - start).millis)
//          // Stops this actor and all its supervised children
//          context.stop(self)
//        }
//    }
//
//  }
//
//  class Listener extends Actor {
//    def receive = {
//      case PiApproximation(pi, duration) ⇒
//        println("\n\tPi approximation: \t\t%s\n\tCalculation time: \t%s"
//          .format(pi, duration))
//        context.system.shutdown()
//    }
//  }
//
//
//  def calculate(nrOfWorkers: Int, nrOfElements: Int, nrOfMessages: Int) {
//    // Create an Akka system
//    val system = ActorSystem("PiSystem")
//
//    // create the result listener, which will print the result and shutdown the system
//    val listener = system.actorOf(Props[Listener], name = "listener")
//
//    // create the master
//    val master = system.actorOf(Props(new Master(
//      nrOfWorkers, nrOfMessages, nrOfElements, listener)),
//      name = "master")
//
//    // start the calculation
//    master ! Calculate
//
//  }
//}
//
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.ml.linalg.Vector
import org.apache.spark.sql.Row
// $example off$
import org.apache.spark.sql.SparkSession

object PipelineExample {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("PipelineExample")
      .getOrCreate()

    // $example on$
    // Prepare training documents from a list of (id, text, label) tuples.
    val training = spark.createDataFrame(Seq(
      (0L, "a b c d e spark", 1.0),
      (1L, "b d", 0.0),
      (2L, "spark f g h", 1.0),
      (3L, "hadoop mapreduce", 0.0)
    )).toDF("id", "text", "label")

    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.
    val tokenizer = new Tokenizer()
      .setInputCol("text")
      .setOutputCol("words")
    val hashingTF = new HashingTF()
      .setNumFeatures(1000)
      .setInputCol(tokenizer.getOutputCol)
      .setOutputCol("features")
    val lr = new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.001)
    val pipeline = new Pipeline()
      .setStages(Array(tokenizer, hashingTF, lr))

    // Fit the pipeline to training documents.
    val model = pipeline.fit(training)

    // Now we can optionally save the fitted pipeline to disk
    model.write.overwrite().save("/tmp/spark-logistic-regression-model")

    // We can also save this unfit pipeline to disk
    pipeline.write.overwrite().save("/tmp/unfit-lr-model")

    // And load it back in during production
    val sameModel = PipelineModel.load("/tmp/spark-logistic-regression-model")

    // Prepare test documents, which are unlabeled (id, text) tuples.
    val test = spark.createDataFrame(Seq(
      (4L, "spark i j k"),
      (5L, "l m n"),
      (6L, "spark hadoop spark"),
      (7L, "apache hadoop")
    )).toDF("id", "text")

    // Make predictions on test documents.
    model.transform(test)
      .select("id", "text", "probability", "prediction")
      .collect()
      .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
        println(s"($id, $text) --> prob=$prob, prediction=$prediction")
      }
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
// $example off$

object PMMLModelExportExample {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("PMMLModelExportExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load and parse the data
    val data = sc.textFile("data/mllib/kmeans_data.txt")
    val parsedData = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble))).cache()

    // Cluster the data into two classes using KMeans
    val numClusters = 2
    val numIterations = 20
    val clusters = KMeans.train(parsedData, numClusters, numIterations)

    // Export to PMML to a String in PMML format
    println("PMML Model:\n" + clusters.toPMML)

    // Export the model to a local file in PMML format
    clusters.toPMML("/tmp/kmeans.xml")

    // Export the model to a directory on a distributed file system in PMML format
    clusters.toPMML(sc, "/tmp/kmeans")

    // Export the model to the OutputStream in PMML format
    clusters.toPMML(System.out)
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report

import scala.annotation.tailrec
import scala.collection.mutable.ArrayBuffer


/*
* created by wangbaosheng on 2017/11/15
*/

class PointWindow[T] extends ArrayBuffer[T] {}


/**
  * 轨迹切分。
  *
  * @param minL            相邻两点间最小距离
  * @param maxL            相邻两点间最大距离
  * @param minWindowLength 轨迹窗口内轨迹点最小个数。
  *                        详细介绍可以看文档
  **/
class PointSplit[Point](minL: Double, maxL: Double, minWindowLength: Int) {
  /*
  * 轨迹计算入口
  * */
  def split(X: Array[Point])(L: (Point, Point) => Double): Array[PointWindow[Point]] = {

    var coordWindows = splitWindow(X)(L)

//    coordWindows = removeExceptionWindow(coordWindows)(L)
//    coordWindows = correctWindow(coordWindows)(L)
//    coordWindows = mergeWindow(coordWindows)(L)

    coordWindows
  }

  /** *
    * 窗口切分
    *
    * @param X 按照时间排序的轨迹序列
    * @param L 距离函数
    * @return 轨迹窗口序列。
    */
  def splitWindow(X: Array[Point])(L: (Point, Point) => Double): Array[PointWindow[Point]] = {
    @tailrec
    def doSplitWindow(i: Int, windows: ArrayBuffer[PointWindow[Point]]): Array[PointWindow[Point]] = {
      if (i == X.length) {
        windows.toArray
      } else {
        //初始化
        if (i == 0) {
          val newWindow = new PointWindow[Point]
          newWindow.append(X(i))
          windows.append(newWindow)
          windows.toArray
        }

        val curCoord = X(i)
        val nextCoord = if (i + 1 < X.length) X(i + 1) else X(i)
        if (L(curCoord, nextCoord) <= maxL) {
          windows.last.append(nextCoord)
        }
        else {
          createWindow(windows).append(nextCoord)
        }
        doSplitWindow(i + 1, windows)
      }
    }

    val windows = new ArrayBuffer[PointWindow[Point]]()
    doSplitWindow(0, windows)
  }

  def createWindow(windows: ArrayBuffer[PointWindow[Point]]): PointWindow[Point] = {
    val newWindow = new PointWindow[Point]
    windows.append(newWindow)
    newWindow
  }

  /** *
    * 生成有效轨迹窗口序列
    *
    * @param coordWindows 轨迹窗口序列
    * @return 有效轨迹窗口序列
    * */
  def removeExceptionWindow(coordWindows: Array[PointWindow[Point]])(L: (Point, Point) => Double): Array[PointWindow[Point]] = {
    def validateWindow(coordWindow: PointWindow[Point]): Boolean = coordWindow.length > minWindowLength

    coordWindows.filter(validateWindow)
  }


  /** *
    * 修正轨迹窗口序列
    *
    * @param validateWindows 有效轨迹窗口序列。
    * @return 修正后的轨迹窗口序列
    */
  def correctWindow(validateWindows: Array[PointWindow[Point]])(L: (Point, Point) => Double): Array[PointWindow[Point]] = {
    validateWindows.map(doCorrectWindow(_)(L))
  }

  /**
    * 修正轨迹窗口
    *
    * @param validateWindow 有效轨迹窗口
    * @return 修正后的轨迹窗口
    **/
  def doCorrectWindow(validateWindow: PointWindow[Point])(L: (Point, Point) => Double): PointWindow[Point] = {
    @tailrec
    def doCompute(i: Int, k: Int, correctWindow: PointWindow[Point]): PointWindow[Point] = {
      if (i == validateWindow.length) correctWindow
      else {
        val curCoord = validateWindow(i)
        if (i + k < validateWindow.length) {
          val nextCoord = validateWindow(i + k)
          if (L(curCoord, nextCoord) >= minL) {
            correctWindow.append(nextCoord)
            doCompute(i + k, 1, correctWindow)
          } else {
            doCompute(i, k + 1, correctWindow)
          }
        } else {
          correctWindow
        }
      }
    }

    val correctWindow = new PointWindow[Point]
    correctWindow.append(validateWindow(0))
    doCompute(0, 1, correctWindow)
  }


  /**
    * 合并轨迹窗口
    *
    * @param correctWindows 修正轨迹窗口序列
    * @return 合并后的轨迹窗口序列
    **/
  def mergeWindow(correctWindows: Array[PointWindow[Point]])(L: (Point, Point) => Double): Array[PointWindow[Point]] = {

    @tailrec
    def doMergeWindow(i: Int, k: Int, windows: ArrayBuffer[PointWindow[Point]]): Array[PointWindow[Point]] = {
      if (i == correctWindows.length) {
        windows.toArray
      } else {
        val curWindow = correctWindows(i)

        if ((i + k) == correctWindows.length) {
          windows.append(curWindow)
          windows.toArray
        } else {
          val nextWindow = correctWindows(i + k)

          if (L(curWindow.last, nextWindow.head) <= maxL) {
            nextWindow.foreach(x => curWindow.append(x))
            doMergeWindow(i, k + 1, windows)
          } else {
            windows.append(curWindow)
            doMergeWindow(i + k, 1, windows)
          }
        }
      }
    }

    if (correctWindows.nonEmpty)
      doMergeWindow(0, 1, new ArrayBuffer[PointWindow[Point]]())
    else
      correctWindows
  }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.PolynomialExpansion
import org.apache.spark.ml.linalg.Vectors
// $example off$
import org.apache.spark.sql.SparkSession

object PolynomialExpansionExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("PolynomialExpansionExample")
      .getOrCreate()

    // $example on$
    val data = Array(
      Vectors.dense(2.0, 1.0),
      Vectors.dense(0.0, 0.0),
      Vectors.dense(3.0, -1.0)
    )
    val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF("features")

    val polyExpansion = new PolynomialExpansion()
      .setInputCol("features")
      .setOutputCol("polyFeatures")
      .setDegree(3)

    val polyDF = polyExpansion.transform(df)
    polyDF.show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.detail.distribution

import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.detail.RealinfoModel

/**
  * Created by franciswang on 2016/10/15.
  */
class PowerDistribution() {
  private val distributed: Array[Double] = Array.fill(48)(0D)

  def add(prevChargeValue: Option[RealinfoModel], currentChargeValue: Option[RealinfoModel]): Unit = {
    (prevChargeValue, currentChargeValue) match {
      case (Some(prev), Some(curv)) =>
        val curDate = Utils.parsetDate(curv.time).get
        val curMinute = curDate.getHours * 60 + curDate.getMinutes + (if (curDate.getMinutes == 0) 0 else 1)
        val timeDiffS= Utils.timeDiff(curv.time, prev.time).toInt / 1000
        val charge = DataPrecision.totalCharge(curv.charge.getOrElse(0), curv.totalVoltage.getOrElse(0), timeDiffS)
       val i=curMinute / 30
        if(i<distributed.length) distributed(i) += charge
      case _ =>
    }
  }



  def add(distribution:Array[Double]): Unit = {
    distribution.indices.foreach(index => {
      if (index < distributed.length) distributed(index) += distribution(index)
    })
  }


  def getDistribution: Array[Double] = this.distributed
}

object PowerDistribution{
  def default: Array[Double] = Array.fill(48)(0D)
}package com.bitnei.report.detail.distribution

import java.util.Date

import com.bitnei.report.distribute.Distribution

/*
* created by wangbaosheng on 2017/12/27
*/
class PowerDistributionBaseMean( ) extends PowerDistribution  {
  private val distributed: Array[Double] = Array.fill(48)(0D)


  override def add(distribution: Array[Double]): Unit = super.add(distribution)

  def add(totalCharge:Double,startTime:Long,endTime:Long):Unit={
    compute(totalCharge,startTime,endTime)
  }

  override def getDistribution: Array[Double] = distributed


  //获取第i个区间的开始值
  def getStartOf(i: Int): Int = i * interval

  //获取第i个区间的结束值
  def getEndOf(i: Int): Int = (i + 1) * interval

  def compute(totalCharge:Double,startTime:Long,endTime:Long): Unit = {
    val duration: Double = (endTime - startTime).toDouble / (1000 * 60)
    val mean: Double = totalCharge / duration
    val startDate = new Date(startTime)
    val startM = startDate.getHours * 60 + startDate.getMinutes + (if (startDate.getMinutes > 30) 1 else 0)

    val endDate = new Date(endTime)
    val endM = endDate.getHours * 60 + endDate.getMinutes + (if (endDate.getMinutes > 30) 1 else 0)
    val startIndex = index(startM)
    val endIndex = index(endM)

    (startIndex to endIndex).foreach(i => {
      if (0 <= i && i < distributed.length) {
        //获取增量
        val x = if (startIndex == endIndex) endM - startM else if (i == startIndex) getStartOf(i) + interval - startM
        else if (i == endIndex) endM - getStartOf(i)
        else interval

        distributed(i) += mean * x
      }
    })
  }


  def index(v: Long): Int = {
    if (v % interval == 0) {
      if (v == 0) 0
      else ((v - interval) / interval).toInt
    } else {
      (v / interval).toInt
    }
  }

  def interval: Int = 30
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.log4j.{Level, Logger}
import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.clustering.PowerIterationClustering
// $example off$
import org.apache.spark.rdd.RDD

/**
 * An example Power Iteration Clustering http://www.icml2010.org/papers/387.pdf app.
 * Takes an input of K concentric circles and the number of points in the innermost circle.
 * The output should be K clusters - each cluster containing precisely the points associated
 * with each of the input circles.
 *
 * Run with
 * {{{
 * ./bin/run-example mllib.PowerIterationClusteringExample [options]
 *
 * Where options include:
 *   k:  Number of circles/clusters
 *   n:  Number of sampled points on innermost circle.. There are proportionally more points
 *      within the outer/larger circles
 *   maxIterations:   Number of Power Iterations
 * }}}
 *
 * Here is a sample run and output:
 *
 * ./bin/run-example mllib.PowerIterationClusteringExample -k 2 --n 10 --maxIterations 15
 *
 * Cluster assignments: 1 -> [0,1,2,3,4,5,6,7,8,9],
 *   0 -> [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29]
 *
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object PowerIterationClusteringExample {

  case class Params(
      k: Int = 2,
      numPoints: Int = 10,
      maxIterations: Int = 15
    ) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("PowerIterationClusteringExample") {
      head("PowerIterationClusteringExample: an example PIC app using concentric circles.")
      opt[Int]('k', "k")
        .text(s"number of circles (clusters), default: ${defaultParams.k}")
        .action((x, c) => c.copy(k = x))
      opt[Int]('n', "n")
        .text(s"number of points in smallest circle, default: ${defaultParams.numPoints}")
        .action((x, c) => c.copy(numPoints = x))
      opt[Int]("maxIterations")
        .text(s"number of iterations, default: ${defaultParams.maxIterations}")
        .action((x, c) => c.copy(maxIterations = x))
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf()
      .setMaster("local")
      .setAppName(s"PowerIterationClustering with $params")
    val sc = new SparkContext(conf)

    Logger.getRootLogger.setLevel(Level.WARN)

    // $example on$
    val circlesRdd = generateCirclesRdd(sc, params.k, params.numPoints)
    val model = new PowerIterationClustering()
      .setK(params.k)
      .setMaxIterations(params.maxIterations)
      .setInitializationMode("degree")
      .run(circlesRdd)

    val clusters = model.assignments.collect().groupBy(_.cluster).mapValues(_.map(_.id))
    val assignments = clusters.toList.sortBy { case (k, v) => v.length }
    val assignmentsStr = assignments
      .map { case (k, v) =>
        s"$k -> ${v.sorted.mkString("[", ",", "]")}"
      }.mkString(", ")
    val sizesStr = assignments.map {
      _._2.length
    }.sorted.mkString("(", ",", ")")
    println(s"Cluster assignments: $assignmentsStr\ncluster sizes: $sizesStr")
    // $example off$

    sc.stop()
  }

  def generateCircle(radius: Double, n: Int): Seq[(Double, Double)] = {
    Seq.tabulate(n) { i =>
      val theta = 2.0 * math.Pi * i / n
      (radius * math.cos(theta), radius * math.sin(theta))
    }
  }

  def generateCirclesRdd(
      sc: SparkContext,
      nCircles: Int,
      nPoints: Int): RDD[(Long, Long, Double)] = {
    val points = (1 to nCircles).flatMap { i =>
      generateCircle(i, i * nPoints)
    }.zipWithIndex
    val rdd = sc.parallelize(points)
    val distancesRdd = rdd.cartesian(rdd).flatMap { case (((x0, y0), i0), ((x1, y1), i1)) =>
      if (i0 < i1) {
        Some((i0.toLong, i1.toLong, gaussianSimilarity((x0, y0), (x1, y1))))
      } else {
        None
      }
    }
    distancesRdd
  }

  /**
   * Gaussian Similarity:  http://en.wikipedia.org/wiki/Radial_basis_function_kernel
   */
  def gaussianSimilarity(p1: (Double, Double), p2: (Double, Double)): Double = {
    val ssquares = (p1._1 - p2._1) * (p1._1 - p2._1) + (p1._2 - p2._2) * (p1._2 - p2._2)
    math.exp(-ssquares / 2.0)
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.fpm.PrefixSpan
// $example off$

object PrefixSpanExample {

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("PrefixSpanExample")
    val sc = new SparkContext(conf)

    // $example on$
    val sequences = sc.parallelize(Seq(
      Array(Array(1, 2), Array(3)),
      Array(Array(1), Array(3, 2), Array(1, 2)),
      Array(Array(1, 2), Array(5)),
      Array(Array(6))
    ), 2).cache()
    val prefixSpan = new PrefixSpan()
      .setMinSupport(0.5)
      .setMaxPatternLength(5)
    val model = prefixSpan.run(sequences)
    model.freqSequences.collect().foreach { freqSequence =>
      println(
        freqSequence.sequence.map(_.mkString("[", ", ", "]")).mkString("[", ", ", "]") +
          ", " + freqSequence.freq)
    }
    // $example off$
  }
}
// scalastyle:off println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.QuantileDiscretizer
// $example off$
import org.apache.spark.sql.SparkSession

object QuantileDiscretizerExample {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("QuantileDiscretizerExample")
      .getOrCreate()

    // $example on$
    val data = Array((0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.2))
    val df = spark.createDataFrame(data).toDF("id", "hour")
    // $example off$
    // Output of QuantileDiscretizer for such small datasets can depend on the number of
    // partitions. Here we force a single partition to ensure consistent results.
    // Note this is not necessary for normal use cases
        .repartition(1)

    // $example on$
    val discretizer = new QuantileDiscretizer()
      .setInputCol("hour")
      .setOutputCol("result")
      .setNumBuckets(3)

    val result = discretizer.fit(df).transform(df)
    result.show()
    // $example off$

    spark.stop()
  }
}
package com.bitnei.report.operationIndex.queryParam

import java.util.Date

import com.bitnei.report.common.utils.Utils

/*
* created by wangbaosheng on 2017/11/8
*/


case class QueryParam(
                       docId:String,
                       startTime:Option[Long],
                       endTime:Option[Long],
                       //所有车辆列表
                       @transient allVehicleList:Array[VehicleInfo],
                       registedCount:Int, //已注册车辆数
                       unregistedCount:Int,
                       registedButUnloadCount:Int //已注册未上传车辆数
                     ) {
  def getUnregistedVehicleList: Iterable[VehicleInfo] = {
    allVehicleList.filter(vehicle=>vehicle.vid.isEmpty)
  }




  override def toString: String = {
    s"""
        queryId:$docId,
        startTime:${startTime.map(st => Utils.formatDate(new Date(st), "yyyy-MM-dd HH:mm:ss")).getOrElse("")}
        endTime:${endTime.map(st => Utils.formatDate(new Date(st), "yyyy-MM-dd HH:mm:ss")).getOrElse("")}
        vehicleCount:${allVehicleList.length}
        registedCount:${registedCount}
        unregistedCount:${allVehicleList.length - registedCount}
        registedButUnloadCount:${registedButUnloadCount}
      """.stripMargin
  }
}

case class VehicleInfo(vin:String,vid:String,licensePlate:String,onlined:Boolean)package com.bitnei.report.operationIndex.queryParam

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils

import scala.collection.mutable.ArrayBuffer
import scala.util.Try

/*
* created by wangbaosheng on 2017/11/9
*/


trait QueryParamParser {
  type T
  def parse(queryId: String): Try[Option[T]]
}



class DbQueryParamParser(stateConf: StateConf) extends QueryParamParser with Logging {
  override type T = QueryParam

  override def parse(queryId: String): Try[Option[QueryParam]] = {
    Try({
      val (docId, startTime, endTime, allVinList) = getDocInfo(queryId)
      if (docId.isEmpty) {
        None
      } else {
        val registedVehicleInfoList = getRegistedVehicleInfoList(allVinList)

        val registedCount = registedVehicleInfoList.length
        val unregistedCount = allVinList.length - registedCount
        val registedButUnloadCount = registedVehicleInfoList.count(_.onlined)

        val allVehicleList=allVinList.map(vin=>{
          registedVehicleInfoList.find(_.vin==vin) match {
            case Some(finded)=>finded
            case None=>VehicleInfo(vin,"","",false)
          }
        })

        Some(QueryParam(docId, startTime, endTime, allVehicleList, registedCount, unregistedCount, registedButUnloadCount))
      }
    })
  }

  def getDocInfo(queryId: String): (String, Option[Long], Option[Long], Array[String]) = {
    val sql =
      s"""
        SELECT
           item.DOC_ID,
           item.START_TIME,
           item.END_TIME,
           item.VINS
        FROM SYS_OPERATION_DOC doc INNER JOIN SYS_OPERATION_ITEM item ON doc.id=item.doc_id
        where DOC.id='$queryId'
  """.stripMargin


    var docId = ""
    var startTime: Option[Long] = None
    var endTime: Option[Long] = None
    var vins = ""

    JdbcPoolHelper.getJdbcPoolHelper(stateConf).executeQuery(sql, stmt => {
      if (stmt.next()) {
        docId = stmt.getString(1)
        startTime = if (stmt.getTimestamp(2) != null) Some(stmt.getTimestamp(2).getTime) else None
        endTime = if (stmt.getTimestamp(3) != null) Some(stmt.getTimestamp(3).getTime) else None
        vins = stmt.getString(4)
      }
    })

    (docId, startTime, endTime, vins.split(',').filter(_.nonEmpty))
  }

  def getRegistedVehicleInfoList(vinList: Array[String]): Array[VehicleInfo] = {
    val result = new ArrayBuffer[VehicleInfo]()

    Utils.batchProcessOfArray(vinList, 999, (offset, length) => {
      result ++= getVehicleSet(stateConf, vinList, offset, length)

      logInfo(s"offset=$offset,length=$length,${result.headOption},${result.lastOption}")
    })

    result.toArray
  }

  def getVehicleSet(stateConf: StateConf, vinSet: Seq[String], offset: Int, length: Int): Seq[VehicleInfo] = {
    val whereFilter = new StringBuilder()

    whereFilter.append(" WHERE vin IN (")

    for (i <- offset until offset + length) {
      if (i < (offset + length) - 1) {
        whereFilter.append(s"'${vinSet(i)}',")
      } else {
        whereFilter.append(s"'${vinSet(i)}'")
      }
    }

    whereFilter.append(")")

    val sqlBuilder =
      s"""
          SELECT
           vin,
           uuid,
           license_plate,
           onlined
          FROM SYS_VEHICLE
          $whereFilter
""".stripMargin

    logInfo(sqlBuilder)

    val vehSet = new ArrayBuffer[VehicleInfo]()

    JdbcPoolHelper.getJdbcPoolHelper(stateConf).executeQuery(sqlBuilder, x => {
      while (x.next()) {
        val veh = VehicleInfo(
          vin = x.getString(1),
          vid = x.getString(2),
          licensePlate = x.getString(3),
          onlined = x.getBoolean(4)
        )

        vehSet.append(veh)
      }
    })

    vehSet
  }
}package com.bitnei.report.operationIndex

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.constants.Constant
import com.bitnei.report.operationIndex.queryParam.DbQueryParamParser
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.catalyst.expressions.Add
import org.scalatest.FunSuite

case class Realinfo(str: String, age: Int)

/*
* created by wangbaosheng on 2017/11/10
*/
class QueryParamTest extends FunSuite{
  test("test query param parser"){
//    val stateConf=new StateConf
//    def configOracle(): Unit = {
//      stateConf.set(Constant.JdbcUserName, "ev")
//      stateConf.set(Constant.JdbcPasswd, "ev")
//      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
//      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.6.146:1521:evmsc1")
//      stateConf.set("database", "oracle")
//    }
//
//    //conifgMysql();
//    configOracle()
//
//
//    val queryParamPaser=new DbQueryParamParser(stateConf)
//    queryParamPaser.parse("queryid1")



//    val sparkSession=SparkSession.builder().master("local").config("spark.testing.memory","471859200").getOrCreate()
//    import  sparkSession.implicits._
//
//    val planstring=sparkSession.createDataset(Array(Realinfo("vid1",12))).queryExecution.toString()
//    println(planstring)

    import org.apache.spark.sql.catalyst.expressions.Expression
    import org.apache.spark.sql.catalyst.expressions.Literal


    val e:Expression=Add(Literal(1),Literal(3))
    println(e.sql)
    println(e.eval())


  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.streaming

import scala.collection.mutable.Queue

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.{Seconds, StreamingContext}

object QueueStream {

  def main(args: Array[String]) {

    StreamingExamples.setStreamingLogLevels()
    val sparkConf = new SparkConf().setAppName("QueueStream")
    // Create the context
    val ssc = new StreamingContext(sparkConf, Seconds(1))

    // Create the queue through which RDDs can be pushed to
    // a QueueInputDStream
    val rddQueue = new Queue[RDD[Int]]()

    // Create the QueueInputDStream and use it do some processing
    val inputStream = ssc.queueStream(rddQueue)
    val mappedStream = inputStream.map(x => (x % 10, 1))
    val reducedStream = mappedStream.reduceByKey(_ + _)
    reducedStream.print()
    ssc.start()

    // Create and push some RDDs into rddQueue
    for (i <- 1 to 30) {
      rddQueue.synchronized {
        rddQueue += ssc.sparkContext.makeRDD(1 to 1000, 10)
      }
      Thread.sleep(1000)
    }
    ssc.stop()
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.util.MLUtils
// $example off$

object RandomForestClassificationExample {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("RandomForestClassificationExample")
    val sc = new SparkContext(conf)
    // $example on$
    // Load and parse the data file.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
    // Split the data into training and test sets (30% held out for testing)
    val splits = data.randomSplit(Array(0.7, 0.3))
    val (trainingData, testData) = (splits(0), splits(1))

    // Train a RandomForest model.
    // Empty categoricalFeaturesInfo indicates all features are continuous.
    val numClasses = 2
    val categoricalFeaturesInfo = Map[Int, Int]()
    val numTrees = 3 // Use more in practice.
    val featureSubsetStrategy = "auto" // Let the algorithm choose.
    val impurity = "gini"
    val maxDepth = 4
    val maxBins = 32

    val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,
      numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)

    // Evaluate model on test instances and compute test error
    val labelAndPreds = testData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()
    println("Test Error = " + testErr)
    println("Learned classification forest model:\n" + model.toDebugString)

    // Save and load model
    model.save(sc, "target/tmp/myRandomForestClassificationModel")
    val sameModel = RandomForestModel.load(sc, "target/tmp/myRandomForestClassificationModel")
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}
// $example off$
import org.apache.spark.sql.SparkSession

object RandomForestClassifierExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("RandomForestClassifierExample")
      .getOrCreate()

    // $example on$
    // Load and parse the data file, converting it to a DataFrame.
    val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

    // Index labels, adding metadata to the label column.
    // Fit on whole dataset to include all labels in index.
    val labelIndexer = new StringIndexer()
      .setInputCol("label")
      .setOutputCol("indexedLabel")
      .fit(data)
    // Automatically identify categorical features, and index them.
    // Set maxCategories so features with > 4 distinct values are treated as continuous.
    val featureIndexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("indexedFeatures")
      .setMaxCategories(4)
      .fit(data)

    // Split the data into training and test sets (30% held out for testing).
    val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))

    // Train a RandomForest model.
    val rf = new RandomForestClassifier()
      .setLabelCol("indexedLabel")
      .setFeaturesCol("indexedFeatures")
      .setNumTrees(10)

    // Convert indexed labels back to original labels.
    val labelConverter = new IndexToString()
      .setInputCol("prediction")
      .setOutputCol("predictedLabel")
      .setLabels(labelIndexer.labels)

    // Chain indexers and forest in a Pipeline.
    val pipeline = new Pipeline()
      .setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))

    // Train model. This also runs the indexers.
    val model = pipeline.fit(trainingData)

    // Make predictions.
    val predictions = model.transform(testData)

    // Select example rows to display.
    predictions.select("predictedLabel", "label", "features").show(5)

    // Select (prediction, true label) and compute test error.
    val evaluator = new MulticlassClassificationEvaluator()
      .setLabelCol("indexedLabel")
      .setPredictionCol("prediction")
      .setMetricName("accuracy")
    val accuracy = evaluator.evaluate(predictions)
    println("Test Error = " + (1.0 - accuracy))

    val rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]
    println("Learned classification forest model:\n" + rfModel.toDebugString)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

import scala.collection.mutable
import scala.language.reflectiveCalls

import scopt.OptionParser

import org.apache.spark.examples.mllib.AbstractParams
import org.apache.spark.ml.{Pipeline, PipelineStage}
import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}
import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer}
import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}
import org.apache.spark.sql.{DataFrame, SparkSession}


/**
 * An example runner for decision trees. Run with
 * {{{
 * ./bin/run-example ml.RandomForestExample [options]
 * }}}
 * Decision Trees and ensembles can take a large amount of memory. If the run-example command
 * above fails, try running via spark-submit and specifying the amount of memory as at least 1g.
 * For local mode, run
 * {{{
 * ./bin/spark-submit --class org.apache.spark.examples.ml.RandomForestExample --driver-memory 1g
 *   [examples JAR path] [options]
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object RandomForestExample {

  case class Params(
      input: String = null,
      testInput: String = "",
      dataFormat: String = "libsvm",
      algo: String = "classification",
      maxDepth: Int = 5,
      maxBins: Int = 32,
      minInstancesPerNode: Int = 1,
      minInfoGain: Double = 0.0,
      numTrees: Int = 10,
      featureSubsetStrategy: String = "auto",
      fracTest: Double = 0.2,
      cacheNodeIds: Boolean = false,
      checkpointDir: Option[String] = None,
      checkpointInterval: Int = 10) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("RandomForestExample") {
      head("RandomForestExample: an example random forest app.")
      opt[String]("algo")
        .text(s"algorithm (classification, regression), default: ${defaultParams.algo}")
        .action((x, c) => c.copy(algo = x))
      opt[Int]("maxDepth")
        .text(s"max depth of the tree, default: ${defaultParams.maxDepth}")
        .action((x, c) => c.copy(maxDepth = x))
      opt[Int]("maxBins")
        .text(s"max number of bins, default: ${defaultParams.maxBins}")
        .action((x, c) => c.copy(maxBins = x))
      opt[Int]("minInstancesPerNode")
        .text(s"min number of instances required at child nodes to create the parent split," +
        s" default: ${defaultParams.minInstancesPerNode}")
        .action((x, c) => c.copy(minInstancesPerNode = x))
      opt[Double]("minInfoGain")
        .text(s"min info gain required to create a split, default: ${defaultParams.minInfoGain}")
        .action((x, c) => c.copy(minInfoGain = x))
      opt[Int]("numTrees")
        .text(s"number of trees in ensemble, default: ${defaultParams.numTrees}")
        .action((x, c) => c.copy(numTrees = x))
      opt[String]("featureSubsetStrategy")
        .text(s"number of features to use per node (supported:" +
        s" ${RandomForestClassifier.supportedFeatureSubsetStrategies.mkString(",")})," +
        s" default: ${defaultParams.numTrees}")
        .action((x, c) => c.copy(featureSubsetStrategy = x))
      opt[Double]("fracTest")
        .text(s"fraction of data to hold out for testing. If given option testInput, " +
        s"this option is ignored. default: ${defaultParams.fracTest}")
        .action((x, c) => c.copy(fracTest = x))
      opt[Boolean]("cacheNodeIds")
        .text(s"whether to use node Id cache during training, " +
        s"default: ${defaultParams.cacheNodeIds}")
        .action((x, c) => c.copy(cacheNodeIds = x))
      opt[String]("checkpointDir")
        .text(s"checkpoint directory where intermediate node Id caches will be stored, " +
        s"default: ${
          defaultParams.checkpointDir match {
            case Some(strVal) => strVal
            case None => "None"
          }
        }")
        .action((x, c) => c.copy(checkpointDir = Some(x)))
      opt[Int]("checkpointInterval")
        .text(s"how often to checkpoint the node Id cache, " +
        s"default: ${defaultParams.checkpointInterval}")
        .action((x, c) => c.copy(checkpointInterval = x))
      opt[String]("testInput")
        .text(s"input path to test dataset. If given, option fracTest is ignored." +
        s" default: ${defaultParams.testInput}")
        .action((x, c) => c.copy(testInput = x))
      opt[String]("dataFormat")
        .text("data format: libsvm (default), dense (deprecated in Spark v1.1)")
        .action((x, c) => c.copy(dataFormat = x))
      arg[String]("<input>")
        .text("input path to labeled examples")
        .required()
        .action((x, c) => c.copy(input = x))
      checkConfig { params =>
        if (params.fracTest < 0 || params.fracTest >= 1) {
          failure(s"fracTest ${params.fracTest} value incorrect; should be in [0,1).")
        } else {
          success
        }
      }
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val spark = SparkSession
      .builder
      .appName(s"RandomForestExample with $params")
      .getOrCreate()

    params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir)
    val algo = params.algo.toLowerCase

    println(s"RandomForestExample with parameters:\n$params")

    // Load training and test data and cache it.
    val (training: DataFrame, test: DataFrame) = DecisionTreeExample.loadDatasets(params.input,
      params.dataFormat, params.testInput, algo, params.fracTest)

    // Set up Pipeline.
    val stages = new mutable.ArrayBuffer[PipelineStage]()
    // (1) For classification, re-index classes.
    val labelColName = if (algo == "classification") "indexedLabel" else "label"
    if (algo == "classification") {
      val labelIndexer = new StringIndexer()
        .setInputCol("label")
        .setOutputCol(labelColName)
      stages += labelIndexer
    }
    // (2) Identify categorical features using VectorIndexer.
    //     Features with more than maxCategories values will be treated as continuous.
    val featuresIndexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("indexedFeatures")
      .setMaxCategories(10)
    stages += featuresIndexer
    // (3) Learn Random Forest.
    val dt = algo match {
      case "classification" =>
        new RandomForestClassifier()
          .setFeaturesCol("indexedFeatures")
          .setLabelCol(labelColName)
          .setMaxDepth(params.maxDepth)
          .setMaxBins(params.maxBins)
          .setMinInstancesPerNode(params.minInstancesPerNode)
          .setMinInfoGain(params.minInfoGain)
          .setCacheNodeIds(params.cacheNodeIds)
          .setCheckpointInterval(params.checkpointInterval)
          .setFeatureSubsetStrategy(params.featureSubsetStrategy)
          .setNumTrees(params.numTrees)
      case "regression" =>
        new RandomForestRegressor()
          .setFeaturesCol("indexedFeatures")
          .setLabelCol(labelColName)
          .setMaxDepth(params.maxDepth)
          .setMaxBins(params.maxBins)
          .setMinInstancesPerNode(params.minInstancesPerNode)
          .setMinInfoGain(params.minInfoGain)
          .setCacheNodeIds(params.cacheNodeIds)
          .setCheckpointInterval(params.checkpointInterval)
          .setFeatureSubsetStrategy(params.featureSubsetStrategy)
          .setNumTrees(params.numTrees)
      case _ => throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }
    stages += dt
    val pipeline = new Pipeline().setStages(stages.toArray)

    // Fit the Pipeline.
    val startTime = System.nanoTime()
    val pipelineModel = pipeline.fit(training)
    val elapsedTime = (System.nanoTime() - startTime) / 1e9
    println(s"Training time: $elapsedTime seconds")

    // Get the trained Random Forest from the fitted PipelineModel.
    algo match {
      case "classification" =>
        val rfModel = pipelineModel.stages.last.asInstanceOf[RandomForestClassificationModel]
        if (rfModel.totalNumNodes < 30) {
          println(rfModel.toDebugString) // Print full model.
        } else {
          println(rfModel) // Print model summary.
        }
      case "regression" =>
        val rfModel = pipelineModel.stages.last.asInstanceOf[RandomForestRegressionModel]
        if (rfModel.totalNumNodes < 30) {
          println(rfModel.toDebugString) // Print full model.
        } else {
          println(rfModel) // Print model summary.
        }
      case _ => throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }

    // Evaluate model on training, test data.
    algo match {
      case "classification" =>
        println("Training data results:")
        DecisionTreeExample.evaluateClassificationModel(pipelineModel, training, labelColName)
        println("Test data results:")
        DecisionTreeExample.evaluateClassificationModel(pipelineModel, test, labelColName)
      case "regression" =>
        println("Training data results:")
        DecisionTreeExample.evaluateRegressionModel(pipelineModel, training, labelColName)
        println("Test data results:")
        DecisionTreeExample.evaluateRegressionModel(pipelineModel, test, labelColName)
      case _ =>
        throw new IllegalArgumentException("Algo ${params.algo} not supported.")
    }

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.util.MLUtils
// $example off$

object RandomForestRegressionExample {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("RandomForestRegressionExample")
    val sc = new SparkContext(conf)
    // $example on$
    // Load and parse the data file.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")
    // Split the data into training and test sets (30% held out for testing)
    val splits = data.randomSplit(Array(0.7, 0.3))
    val (trainingData, testData) = (splits(0), splits(1))

    // Train a RandomForest model.
    // Empty categoricalFeaturesInfo indicates all features are continuous.
    val numClasses = 2
    val categoricalFeaturesInfo = Map[Int, Int]()
    val numTrees = 3 // Use more in practice.
    val featureSubsetStrategy = "auto" // Let the algorithm choose.
    val impurity = "variance"
    val maxDepth = 4
    val maxBins = 32

    val model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo,
      numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)

    // Evaluate model on test instances and compute test error
    val labelsAndPredictions = testData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()
    println("Test Mean Squared Error = " + testMSE)
    println("Learned regression forest model:\n" + model.toDebugString)

    // Save and load model
    model.save(sc, "target/tmp/myRandomForestRegressionModel")
    val sameModel = RandomForestModel.load(sc, "target/tmp/myRandomForestRegressionModel")
    // $example off$
  }
}
// scalastyle:on println

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.feature.VectorIndexer
import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}
// $example off$
import org.apache.spark.sql.SparkSession

object RandomForestRegressorExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("RandomForestRegressorExample")
      .getOrCreate()

    // $example on$
    // Load and parse the data file, converting it to a DataFrame.
    val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

    // Automatically identify categorical features, and index them.
    // Set maxCategories so features with > 4 distinct values are treated as continuous.
    val featureIndexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("indexedFeatures")
      .setMaxCategories(4)
      .fit(data)

    // Split the data into training and test sets (30% held out for testing).
    val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))

    // Train a RandomForest model.
    val rf = new RandomForestRegressor()
      .setLabelCol("label")
      .setFeaturesCol("indexedFeatures")

    // Chain indexer and forest in a Pipeline.
    val pipeline = new Pipeline()
      .setStages(Array(featureIndexer, rf))

    // Train model. This also runs the indexer.
    val model = pipeline.fit(trainingData)

    // Make predictions.
    val predictions = model.transform(testData)

    // Select example rows to display.
    predictions.select("prediction", "label", "features").show(5)

    // Select (prediction, true label) and compute test error.
    val evaluator = new RegressionEvaluator()
      .setLabelCol("label")
      .setPredictionCol("prediction")
      .setMetricName("rmse")
    val rmse = evaluator.evaluate(predictions)
    println("Root Mean Squared Error (RMSE) on test data = " + rmse)

    val rfModel = model.stages(1).asInstanceOf[RandomForestRegressionModel]
    println("Learned regression forest model:\n" + rfModel.toDebugString)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.random.RandomRDDs
import org.apache.spark.rdd.RDD

/**
 * An example app for randomly generated RDDs. Run with
 * {{{
 * bin/run-example org.apache.spark.examples.mllib.RandomRDDGeneration
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object RandomRDDGeneration {

  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName(s"RandomRDDGeneration")
    val sc = new SparkContext(conf)

    val numExamples = 10000 // number of examples to generate
    val fraction = 0.1 // fraction of data to sample

    // Example: RandomRDDs.normalRDD
    val normalRDD: RDD[Double] = RandomRDDs.normalRDD(sc, numExamples)
    println(s"Generated RDD of ${normalRDD.count()}" +
      " examples sampled from the standard normal distribution")
    println("  First 5 samples:")
    normalRDD.take(5).foreach( x => println(s"    $x") )

    // Example: RandomRDDs.normalVectorRDD
    val normalVectorRDD = RandomRDDs.normalVectorRDD(sc, numRows = numExamples, numCols = 2)
    println(s"Generated RDD of ${normalVectorRDD.count()} examples of length-2 vectors.")
    println("  First 5 samples:")
    normalVectorRDD.take(5).foreach( x => println(s"    $x") )

    println()

    sc.stop()
  }

}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

// $example on$
import org.apache.spark.mllib.evaluation.{RankingMetrics, RegressionMetrics}
import org.apache.spark.mllib.recommendation.{ALS, Rating}
// $example off$
import org.apache.spark.sql.SparkSession

object RankingMetricsExample {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("RankingMetricsExample")
      .getOrCreate()
    import spark.implicits._
    // $example on$
    // Read in the ratings data
    val ratings = spark.read.textFile("data/mllib/sample_movielens_data.txt").rdd.map { line =>
      val fields = line.split("::")
      Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble - 2.5)
    }.cache()

    // Map ratings to 1 or 0, 1 indicating a movie that should be recommended
    val binarizedRatings = ratings.map(r => Rating(r.user, r.product,
      if (r.rating > 0) 1.0 else 0.0)).cache()

    // Summarize ratings
    val numRatings = ratings.count()
    val numUsers = ratings.map(_.user).distinct().count()
    val numMovies = ratings.map(_.product).distinct().count()
    println(s"Got $numRatings ratings from $numUsers users on $numMovies movies.")

    // Build the model
    val numIterations = 10
    val rank = 10
    val lambda = 0.01
    val model = ALS.train(ratings, rank, numIterations, lambda)

    // Define a function to scale ratings from 0 to 1
    def scaledRating(r: Rating): Rating = {
      val scaledRating = math.max(math.min(r.rating, 1.0), 0.0)
      Rating(r.user, r.product, scaledRating)
    }

    // Get sorted top ten predictions for each user and then scale from [0, 1]
    val userRecommended = model.recommendProductsForUsers(10).map { case (user, recs) =>
      (user, recs.map(scaledRating))
    }

    // Assume that any movie a user rated 3 or higher (which maps to a 1) is a relevant document
    // Compare with top ten most relevant documents
    val userMovies = binarizedRatings.groupBy(_.user)
    val relevantDocuments = userMovies.join(userRecommended).map { case (user, (actual,
    predictions)) =>
      (predictions.map(_.product), actual.filter(_.rating > 0.0).map(_.product).toArray)
    }

    // Instantiate metrics object
    val metrics = new RankingMetrics(relevantDocuments)

    // Precision at K
    Array(1, 3, 5).foreach { k =>
      println(s"Precision at $k = ${metrics.precisionAt(k)}")
    }

    // Mean average precision
    println(s"Mean average precision = ${metrics.meanAveragePrecision}")

    // Normalized discounted cumulative gain
    Array(1, 3, 5).foreach { k =>
      println(s"NDCG at $k = ${metrics.ndcgAt(k)}")
    }

    // Get predictions for each data point
    val allPredictions = model.predict(ratings.map(r => (r.user, r.product))).map(r => ((r.user,
      r.product), r.rating))
    val allRatings = ratings.map(r => ((r.user, r.product), r.rating))
    val predictionsAndLabels = allPredictions.join(allRatings).map { case ((user, product),
    (predicted, actual)) =>
      (predicted, actual)
    }

    // Get the RMSE using regression metrics
    val regressionMetrics = new RegressionMetrics(predictionsAndLabels)
    println(s"RMSE = ${regressionMetrics.rootMeanSquaredError}")

    // R-squared
    println(s"R-squared = ${regressionMetrics.r2}")
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming._
import org.apache.spark.util.IntParam

/**
 * Receives text from multiple rawNetworkStreams and counts how many '\n' delimited
 * lines have the word 'the' in them. This is useful for benchmarking purposes. This
 * will only work with spark.streaming.util.RawTextSender running on all worker nodes
 * and with Spark using Kryo serialization (set Java property "spark.serializer" to
 * "org.apache.spark.serializer.KryoSerializer").
 * Usage: RawNetworkGrep <numStreams> <host> <port> <batchMillis>
 *   <numStream> is the number rawNetworkStreams, which should be same as number
 *               of work nodes in the cluster
 *   <host> is "localhost".
 *   <port> is the port on which RawTextSender is running in the worker nodes.
 *   <batchMillise> is the Spark Streaming batch duration in milliseconds.
 */
object RawNetworkGrep {
  def main(args: Array[String]) {
    if (args.length != 4) {
      System.err.println("Usage: RawNetworkGrep <numStreams> <host> <port> <batchMillis>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    val Array(IntParam(numStreams), host, IntParam(port), IntParam(batchMillis)) = args
    val sparkConf = new SparkConf().setAppName("RawNetworkGrep")
    // Create the context
    val ssc = new StreamingContext(sparkConf, Duration(batchMillis))

    val rawStreams = (1 to numStreams).map(_ =>
      ssc.rawSocketStream[String](host, port, StorageLevel.MEMORY_ONLY_SER_2)).toArray
    val union = ssc.union(rawStreams)
    union.filter(_.contains("the")).count().foreachRDD(r =>
      println("Grep count: " + r.collect().mkString))
    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.sql

import org.apache.spark.sql.SaveMode
// $example on:init_session$
import org.apache.spark.sql.SparkSession
// $example off:init_session$

// One method for defining the schema of an RDD is to make a case class with the desired column
// names and types.
case class Record(key: Int, value: String)

object RDDRelation {
  def main(args: Array[String]) {
    // $example on:init_session$
    val spark = SparkSession
      .builder
      .appName("Spark Examples")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    // Importing the SparkSession gives access to all the SQL functions and implicit conversions.
    import spark.implicits._
    // $example off:init_session$

    val df = spark.createDataFrame((1 to 100).map(i => Record(i, s"val_$i")))
    // Any RDD containing case classes can be used to create a temporary view.  The schema of the
    // view is automatically inferred using scala reflection.
    df.createOrReplaceTempView("records")

    // Once tables have been registered, you can run SQL queries over them.
    println("Result of SELECT *:")
    spark.sql("SELECT * FROM records").collect().foreach(println)

    // Aggregation queries are also supported.
    val count = spark.sql("SELECT COUNT(*) FROM records").collect().head.getLong(0)
    println(s"COUNT(*): $count")

    // The results of SQL queries are themselves RDDs and support all normal RDD functions. The
    // items in the RDD are of type Row, which allows you to access each column by ordinal.
    val rddFromSql = spark.sql("SELECT key, value FROM records WHERE key < 10")

    println("Result of RDD.map:")
    rddFromSql.rdd.map(row => s"Key: ${row(0)}, Value: ${row(1)}").collect().foreach(println)

    // Queries can also be written using a LINQ-like Scala DSL.
    df.where($"key" === 1).orderBy($"value".asc).select($"key").collect().foreach(println)

    // Write out an RDD as a parquet file with overwrite mode.
    df.write.mode(SaveMode.Overwrite).parquet("pair.parquet")

    // Read in parquet file.  Parquet files are self-describing so the schema is preserved.
    val parquetFile = spark.read.parquet("pair.parquet")

    // Queries can be run using the DSL on parquet files just like the original RDD.
    parquetFile.where($"key" === 1).select($"value".as("a")).collect().foreach(println)

    // These files can also be used to create a temporary view.
    parquetFile.createOrReplaceTempView("parquetFile")
    spark.sql("SELECT * FROM parquetFile").collect().foreach(println)

    spark.stop()
  }
}
// scalastyle:on println
package tempjob

import org.apache.spark.sql.SparkSession

/*
* created by wangbaosheng on 2017/11/17
*/

case class Realinfo(id:String,name:String)
object ReaderTest {
  def main(args: Array[String]): Unit = {
    val sparkSession=SparkSession.builder().master("local").config("spark.testing.memory","471859200").getOrCreate()
    sparkSession.read.csv("csvdata").show(false)
    import sparkSession.implicits._

    sparkSession.createDataset(Array(Realinfo("sss","23"))).write.csv("file:///C:\\D\\report\\statistics\\report-master\\dayreport\\csv11")
  }
}
package com.bitnei.report.cellVoltageConsist

/*
* created by wangbaosheng on 2017/11/2
*/
case class Realinfo(vid: String,
                    time: String,
                    soc: Option[Int],
                    cellVoltage: Option[String] = None,
                    elecFlow: Option[Int]){
  override def toString: String =
    s"""
      vid=$vid
      time=$time
      soc=${soc.getOrElse("")}
      cellVoltage:${cellVoltage.getOrElse("")}
      elecFlow:${elecFlow.getOrElse("")}
    """.stripMargin
}
package com.bitnei.report

import java.text.SimpleDateFormat

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.detail.FaultDetail.FaultDetailCompute
import com.bitnei.report.detail.{DetailComputeBase, RealinfoModel, ReportDetailCompute}
import com.bitnei.report.handler.StateGenerator
import com.bitnei.report.stateGenerate.ChargeChangeGenerator
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

import scala.annotation.tailrec
import scala.collection.mutable.ArrayBuffer
import scala.reflect.ClassTag

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-01-31 11:43
  *
  */
object RealInfo2Detail extends Serializable with Logging {

  case class RealinfoModel(
                            vid: String,
                            vin: String,
                            time: String,
                            speed: Option[Int] = None,
                            //电流
                            charge: Option[Int] = None,
                            //soc
                            soc: Option[Int] = None,
                            //总电压
                            totalVoltage: Option[Int] = None,
                            //最大单体电压
                            secondaryCellMaxVoltage: Option[Int] = None,
                            secondaryCellMinVoltage: Option[Int] = None,
                            //发电机温度
                            engineTemp: Option[Int] = None,
                            //最大采集温度
                            accuisitionPointMaxTemp: Option[Int] = None,
                            //最小采集温度
                            accuisitionPointMinTemp: Option[Int] = None,
                            //里程
                            mileage: Option[Int] = None,
                            //经度
                            longitude: Option[Long] = None,
                            //纬度
                            latitude: Option[Long] = None,
                            //挡位
                            peaking: Option[Int] = None)


  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")
    //时间参数 20170111
    var date = "20171102"

    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")
    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
    ////////////////////////////////////////////////

    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        val realInfoDS = sparkSession.read.parquet("data/realinfo/*.parquet")
        realInfoDS.createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }
    ////////////////业务逻辑////////////////////////

    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    val sql =
      s"""
      SELECT CAST (VID AS String),
              VIN,
             `2000` AS TIME,
             `2201` AS speed,
             `2614` AS charge,
             `2615` AS soc,
             `2613` AS totalVoltage,
             `2603` AS secondaryCellMaxVoltage,
             `2606` AS secondaryCellMinVoltage,
             `2304` AS engineTemp,
             `2609` AS accuisitionPointMaxTemp,
             `2612` AS accuisitionPointMinTemp,
             `2202` AS mileage,
             `2502` AS longitude,
             `2503` AS latitude,
             CAST(`2203` AS INT) AS peaking
       FROM realinfo
       WHERE vid IS NOT NULL AND `2000` like '${date}%'  and `2201` is not null and `2614` is not null  and `2615` is not null and `2613` is not null  and `2603` is not null and `2606` is not null  and `2304` is not null and `2609` is not null  and `2612` is not null and `2202` is not null  and `2502` is not null and `2503` is not null
    """.stripMargin

    val input = sparkSession.sql(sql).as[RealinfoModel]

    val enableEmergency = stateConf.getOption("emergency.enable").contains("true")

//    val res = input.groupByKey(_.vid)
//      .flatMapGroups {
//        case (vid: String, realinfos: Iterator[RealinfoModel]) => {
//
//          //          realinfos.map(r=>{
//          //            val enableEmergency = true
//          //            if(enableEmergency){
//          //
//          ////              val year = date.substring(0, 4)
//          ////              val month = date.substring(4, 6)
//          ////              val day = date.substring(6)
//          //
//          //            }
//          //          })
//
//
//          val sortedRealInfos = realinfos.toArray.sortBy(r => r.time)
//
//          val distinctRealInfos: Array[RealinfoModel] = distinct(sortedRealInfos, realinfo => Some(realinfo.time))
//
//          stateConf.getString("detail.compute") match {
//            case "reportDetail" => {
//              val flag = stateConf.getString("chargeChange.enable")
//
//              if(flag.equals("true")){
//
//                new StateGenerator(stateConf) {
//
//                  override type T = RealinfoModel
//
//                  override def getVid = (row: RealinfoModel) => row.vid
//
//                  override def getTime = row => row.time
//
//                  override def getCharge = row => row.charge.getOrElse(0)
//
//                  override def getSoc = row => row.soc.getOrElse(0)
//
//                  override def getSpeed = row => row.speed.getOrElse(0)
//
//                  override def getMileage = row => row.mileage.getOrElse(0)
//
//                }.handle(distinctRealInfos)
//
//              }else{
//
//                new StateGeneratorBase(stateConf) {
//                  override type T = RealinfoModel
//
//                  override def getVid = (row: RealinfoModel) => row.vid
//
//                  override def getTime = row => row.time
//
//                  override def getCharge = row => row.charge.getOrElse(0)
//
//                  override def getSoc = row => row.soc.getOrElse(0)
//
//                  override def getSpeed = row => row.speed.getOrElse(0)
//
//                  override def getMileage = row => row.mileage.getOrElse(0)
//                }.handle(sortedRows.filter(_.soc.nonEmpty))
//
//              }
//            }
//            case "faultDetail" => {
//
//            }
//            case e => throw new RuntimeException(s"detail.compute=$e not support")
//          }
//
//
//          distinctDS
//        }
//      }
//
//
//    res.show(false)

    ///////////////////////////////////
    sparkSession.stop()
  }

  def distinct[T: ClassTag, U](sortedRealInfos: Array[T], getField: (T) => Option[U]): Array[T] = {

    val strictData = new ArrayBuffer[T]()

    @tailrec
    def distinctTailRec(curIndex: Int, prev: Option[T]): ArrayBuffer[T] = {
      if (curIndex < sortedRealInfos.length) {
        if (prev.isEmpty || getField(sortedRealInfos(curIndex)) != getField(prev.get)) {
          strictData += sortedRealInfos(curIndex)
        }
        distinctTailRec(curIndex + 1, Some(sortedRealInfos(curIndex)))
      } else {
        strictData
      }
    }

    distinctTailRec(0, None).toArray[T]

  }

}
package com.bitnei.report.year


import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession


/**
  *
  * @author zhangyongtian
  * @define 导出异常数据
  * 工作日：指法定上班日期；
  *
  * 早高峰：早上7:00-9:00；（4）晚高峰：下午5:00-7:00；
  *
  * 车牌  	总行驶里程	总行驶时长	总充电时长	总上线天数	平均车速	总充电量	早高峰平均车速	晚高峰平均车速
  *
  * 充电状态判断：电流 负数 车速 <=0.5 m
  *
  * create 2018-01-03 16:05
  *
  */
object RealinfoExceptionData extends Serializable with Logging {


  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var date = "20171102"

    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
    }


    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    val sc = sparkSession.sparkContext
    import sparkSession.implicits._

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////


    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")

      }

      case "dev" => {
        //研发环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }
      ///tmp/zyt/data/realinfo

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }


    /////////////////////////////////业务逻辑//////////////////////////////////////////////

    val sql =
      s"""
         SELECT
           VID,
         `2000` AS TIME,
         `2202` AS Mileage,
         `2201` As speed,
         `2613` As dy,
         `2614` As dl
         FROM realinfo
         where VID is not null
         and `2000` like '${date}%'
         and `2202` is not null
         and `2201` is not null
         and `2613` is not null
         and `2614` is not null
      """.stripMargin
    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String)]


    // TODO: 过滤
    val result = initDS
      .filter(x => {
        val vid = x._1
        val time = x._2
        val mileage = x._3.toDouble

        val cond01 = mileage >= 0 && mileage <= 9999999

        val speed = x._4.toDouble
        val cond02 = speed >= 0 && speed <= 2200

        val dy = x._5.toDouble
        val cond03 = dy >= 0 && dy <= 10000

        //parquet数据已经减去了偏移量
        val dl = x._6.toDouble
        //[-10000,10000]
        val cond04 = dl >= -10000 && dl <= 10000

        //功率过滤
        val cond05 = dl < 0 && (dy/10 * Math.abs(dl)/10) / 1000 > 200

        cond01 && cond02 && cond03 && cond04 && cond05
      })


    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")

    // TODO: 输出
    if (env.equals("local")) {


      result.show(false)


      //      parkingInfoDS.show(false)
      //      result.show(100, false)

      //      result.count()

      //    result.printSchema()

      //    result.show(false)


    }

    // TODO: 输出到
    if (env.equals("dev")) {
      result.repartition(1).write.json(s"/spark/vehicle/result/yearReport/exceptionData/year=${year}/month=${month}/day=${day}")
    }

    if (env.equals("prd")) {
    }

    logInfo("任务完成...")

    sparkSession.stop()
  }

  def isMorningPeak(time: String): Boolean = {
    val clock = time.substring(8, 10).toInt
    clock >= 7 && clock <= 9
  }

  def isnightPeak(time: String): Boolean = {
    val clock = time.substring(8, 10).toInt
    clock >= 17 && clock <= 19
  }


  def getChargeRuler(input: Input): Boolean = {
    input.dl < 0 && (input.speed * 1000) <= 0.5
    //    && (input.dy > 0)
  }

  def getRunRuler(input: Input): Boolean = {
    input.speed * 1000 > 0.5 && input.dl > 0
    //    !getChargeRuler(input)
  }


  case class Input(vid: String, time: String, mileage: Double, speed: Double, dy: Double, dl: Double, timeStamp: Long)


  case class Output(vid: String, date: String, totalMilage: String, run_duration: String, avg_speed: String, charge_duration: String, total_charge_num: String, morningPeakMileage: String, morningPeakRunDuration: String, morningAvgSpeed: String, nightPeakMileage: String, nightPeakRunDuration: String, nightAvgSpeed: String)

}

//.formatted("%.3f")package com.bitnei.tools.export.hdfs

import java.text.SimpleDateFormat
import java.util
import java.util.Date

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.tools.dayreport.StartStopMileage2Oracle.logInfo
import com.bitnei.tools.util.MockDataProvider
import org.apache.commons.codec.binary.Base64
import org.apache.hadoop.fs.Path
import org.apache.hadoop.hbase.client._
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, HConstants, TableName}
import org.apache.hadoop.mapred.JobConf
import org.apache.hadoop.mapred.lib.MultipleTextOutputFormat
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.storage.StorageLevel
import org.apache.spark.{RangePartitioner, SparkConf, SparkContext}

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define 从hbase 按vid从hdfs导出realinfo数据
  *
  */
object RealInfoExporter extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数

    //查询条件数据 vids
    var selectDataPath = "data/export-vnum-uid/vids.txt"

    //TODO 开始有数据的第一天
    //时间参数 导出数据的截止日期
    var startDate = "20150101"

    //输出参数
    var outputPath = s"/tmp/zyt/${app}" + System.currentTimeMillis()

    //参数处理
    if (!env.equals("local")) {
      startDate = stateConf.getOption("input.date").get
      //      selectDataPath = "file://" + stateConf.getOption("input.data").get
      selectDataPath = stateConf.getOption("input.data").get

      if (env.equals("prd")) {
        outputPath = stateConf.getString("output.hdfs.path")
      }
    }

    //参数校验
    if (startDate.length != 8) {
      throw new Exception("input.date error")
    }


    ///////////////////////////////////////////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

      .set("spark.yarn.executor.memoryoverhead", "4420")

      .set("spark.default.parallelism", "100")

      .set("spark.rdd.compress", "true")
      .set("spark.storage.memoryFraction", "0.5")
      .set("spark.memory.storageFraction", "0.2")
      .set("spark.memory.useLegacyMode", "true")
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.shuffle.consolidateFiles", "true")
      .set("spark.shuffle.io.maxRetries", "10")
      .set("spark.shuffle.io.retryWait", "30")
      .set("spark.files.fetchTimeout", "360")
      .set("spark.reducer.maxSizeInFligth", "24")
      .set("spark.network.timeout", "360")
      .set("spark.sql.shuffle.partitions", "100")
      .set("mapreduce.reduce.memory.mb", "8192")
      .set("spark.sql.codegen", "true")
      .set("spark.sql.parquet.compression.codec", "snappy")
      .set("spark.cleaner.ttl", "240000")
      .set("spark.kryoserializer.buffer.mb", "10")
      .set("spark.reducer.maxSizeInFligth", "12")
      .set("spark.executor.extraJavaOptions", "-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+UseG1GC")

      .set("spark.speculation", "true")
      .set("spark.blacklist.enabled", "true")

      .set("spark.scheduler.mode", "FAIR")




    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + startDate).getOrCreate()

    /////////////////////////////////////////////////////////////////////////////////////////////////////////

    //数据源：/spark/vehicle/data/realinfo/

    //    sparkSession.sparkContext.hadoopConfiguration


    val vidsArr = sparkSession.sparkContext.textFile(selectDataPath).collect().sortBy(x => x)


    val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
    val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConfiguration)


    val sdf = new SimpleDateFormat("yyyyMMdd")

    //结束
    val now = new Date()
    val nowStr = sdf.format(now)

    var dayTime = sdf.parse(startDate).getTime

    val year = startDate.substring(0, 4)
    val month = startDate.substring(4, 6)
    val day = startDate.substring(6)


    // TODO: 遍历到目前为止的所有天
    while (dayTime <= now.getTime) {

      val dayStr = sdf.format(dayTime)


      val year = dayStr.substring(0, 4)
      val month = dayStr.substring(4, 6)
      val day = dayStr.substring(6)

      var path =
        env match {
          case "local" => "data/realinfo/*.parquet"
          case "dev" => s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}"
          case "prd" => s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}"
        }

      //        if (!startDate.equals(dayStr) && fs.exists(new Path(path))) {
      //          val newDS = sparkSession.read.parquet(path).filter(_.getString(0).contains(vid))
      //          resDS = resDS.union(newDS)
      //          resDS.cache()
      //        }

      log.info("输入路径：" + path)

      if (env.equals("local") || fs.exists(new Path(path))) {

        println(dayStr)

        val result =
          sparkSession.read.parquet(path)
            .rdd
            .map(line => {
              val vid = line.getString(0)
              val arr = new ArrayBuffer[String]()
              for (i <- 0 until line.size - 1) {
                val col = Option(line.get(i)).getOrElse("").toString
                //                println(col)
                arr.append(col)
              }
              arr.mkString(",")
            })


        //////////////////////输出/////////////
        if (env.equals("local")) {
          println(result.count())
          //    result.take(1).foreach(println)
          //    result.show(false)
          //    result.count()
          //    result.show(false)
        }

        // TODO: 按天输出
        if (!env.equals("local")) {


          //          val jobConf = new JobConf
          //    jobConf.set("mapred.output.compress", "true")
          //    jobConf.set("mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec")

          //设置MapReduce的输出的分隔符为逗号
          //          jobConf.set("mapred.textoutputformat.ignoreseparator", "true")
          //          jobConf.set("mapred.textoutputformat.separator", "")
          //
          //          result.saveAsHadoopFile(
          //            outputPath + s"/${year}/${month}/${day}",
          //            classOf[String],
          //            classOf[String],
          //            classOf[RDDMultipleTextOutputFormat[_, _]], jobConf)

          //        resDS.repartition(1).write.json(outputPath)

          //          result.persist(StorageLevel.MEMORY_AND_DISK_SER)

          for (vid <- vidsArr) {
            result.filter(_.contains(vid)).saveAsTextFile(outputPath + s"/daily/${year}/${month}/${day}/" + vid)
          }
        }

      }

      dayTime = dayTime + 24 * 60 * 60 * 1000
    }


    //    var dayTime2 = sdf.parse(startDate).getTime
    //
    //
    //    // TODO: 遍历
    //    for (vid <- vidsArr) {
    //
    //      var rdd = sparkSession.sparkContext.makeRDD(Array(""))
    //
    //      // TODO:按VID合并
    //      while (dayTime2 <= now.getTime) {
    //
    //        val dayStr = sdf.format(dayTime2)
    //        val year = dayStr.substring(0, 4)
    //        val month = dayStr.substring(4, 6)
    //        val day = dayStr.substring(6)
    //
    //        val path = outputPath + s"/daily/${year}/${month}/${day}/" + vid
    //
    //        if (fs.exists(new Path(path))) {
    //          val newRdd = sparkSession.sparkContext.textFile(path)
    //          rdd = rdd.union(newRdd)
    //
    //        }
    //
    //        dayTime2 = dayTime2 + 24 * 60 * 60 * 1000
    //
    //      }
    //
    //      rdd.repartition(1).saveAsTextFile(outputPath + s"/${startDate}-${nowStr}/${vid}")
    //
    //    }


    //    logInfo("============"+result.count())
    //    result.saveAsTextFile(outputPath)

    //    val jobConf = new JobConf
    //    jobConf.set("mapred.output.compress", "true")
    //    jobConf.set("mapred.output.compression.codec", "org.apache.hadoop.io.compress.GzipCodec")


    //设置MapReduce的输出的分隔符为逗号
    //    jobConf.set("mapred.textoutputformat.ignoreseparator", "true")
    //    jobConf.set("mapred.textoutputformat.separator", "")
    //
    //    result.saveAsHadoopFile(
    //      outputPath,
    //      classOf[String],
    //      classOf[String],
    //      classOf[RDDMultipleTextOutputFormat[_, _]], jobConf)


    //
    //  class RDDMultipleTextOutputFormat[K, V]() extends MultipleTextOutputFormat[K, V]() {
    //
    //    override def generateFileNameForKeyValue(key: K, value: V, name: String): String = {
    //      key.toString
    //    }
    //  }


    // TODO: 删除按天数据
    //    fs.delete(new Path(outputPath + "/daily"), true)

    //////////////////////////////////////////////////////////////////////////////////////////////////////////
    logInfo("任务完成...")
    sparkSession.stop()

  }

  class RDDMultipleTextOutputFormat[K, V]() extends MultipleTextOutputFormat[K, V]() {

    override def generateFileNameForKeyValue(key: K, value: V, name: String): String = {
      key.toString
    }
  }

}
package com.bitnei.report.dayreport.realinfo

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.Model._
import com.bitnei.report.dayreport.distribution.{ContinueDistributionCounter, DiscreteDistribution, MileageDistribution, SocDistribution}
import com.bitnei.report.distribute._
import com.bitnei.report.{ArrayUndoMonidFoldable, AutoPartition, Job}
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}

import scala.collection.mutable.ArrayBuffer


//存储每一次充电/行驶/满点的明细数据。
case class DetailModel(
                        vid:String,
                        vin:String,
                        category:String,
                        onlineTime:Int=0,
                        startTime:Long=0,
                        endTime:Long=0,
                        timeLeng:Long=0,

                        //实际行驶时间=timelenth-无效形式时间
                        accRunTime:Int=0,
                        //当日开始里程
                        startMileageOfCurrentDay:Int=0,
                        //当日结束里程
                        endMileageOfCurrentDay:Int=0,
                        //开始里程
                        startMileage:Int=0,
                        //结束里程
                        stopMileage:Int=Int.MaxValue,
                        gpsMileage: Int=0,
                        avgSpeed:Double=0,
                        maxSpeed:Int=0,

                        //极值数据
                        maxTotalVoltage:Int=0,
                        minTotalVoltage:Int=Int.MaxValue,
                        maxTotalCurrent:Int=0,
                        minTotalCurrent:Int=Int.MaxValue,
                        maxSecondaryVolatage:Int=0,
                        minSecondaryVolatage:Int=Int.MaxValue,
                        maxAcquisitionPointTemp:Option[Int]=None,
                        minAcquisitionPointTemp:Int=Int.MaxValue,
                        maxEngineTemp:Option[Int]=None,
                        minEngineTemp:Int=Int.MaxValue,
                        maxSoc:Int=0,
                        minSoc:Int=Int.MaxValue,
                        startSoc:Int=0,
                        endSoc:Int=0,

                        //行驶数据
                        startLongitude:Long=0,
                        startLatitude:Long=0,
                        endLongitude:Long=0,
                        endLatitude:Long=0,

                        totalCharge:Double=0,
                        timeBetweenCharge:Int=0,
                        stopMileageOfPrevCharge:Int=0,
                        prevChargeStopTime:Long=0,
                        maxCurrentOfPrevCharge:Int=0)
/**
  * Created by wangbaosheng on 2017/4/6.
  * 日报表作业
  */
class RealinfoJob(@transient sparkSession:SparkSession, stateConf:StateConf) extends Serializable with Logging with Job with AutoPartition {
  @transient private val sqlContext = sparkSession.sqlContext
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConfiguration)

  import sparkSession.implicits._

  override type R = DayReportResult
  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("detail")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("dayreport")

  override def registerIfNeed(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)
  }

  def getInputTable(): DataFrame = {
    val whereCondition = SqlHelper.buildWhereConditionBasePartitionColumn(
      SparkHelper.getTableInfo(stateConf, inputTableName),
      (partitionColumnName, partitionColumnValue) => s"$partitionColumnName=${partitionColumnValue}"
    ).get

    val input = sparkSession.sql(s"SELECT *  FROM $inputTableName WHERE vid IS NOT NULL AND $whereCondition")
    input
  }


  override def doCompute[Product <: DayReportResult](): Dataset[DayReportResult] = {
    val result=getInputTable().as[DetailModel]
      .groupByKey(_.vid)
      .flatMapGroups({ case (vid: String, rows: Iterator[DetailModel]) =>
        val details = rows.toArray
        val reportDate = details.head.startTime
        val onlineTime= details.head.onlineTime
        //计算充电日报表
        val chargeDayReportResult = compute(vid, Constant.ChargeState, reportDate,onlineTime, 0, 0, details.filter(_.category == Constant.ChargeState))

        //计算满电日报表
        val fullChargeDayReportResult = compute(vid, Constant.FullChargeState, reportDate, onlineTime,0, 0, details.filter(_.category == Constant.FullChargeState))

        //计算行驶日报表
        val runDayReportResult = compute(vid,
          Constant.TravelState,
          reportDate,
          onlineTime,
          chargeDayReportResult.timeLeng.toInt,
          fullChargeDayReportResult.timeLeng.toInt,
          details.filter(_.category == Constant.TravelState)
        )

        //计算百公里耗电量
        val c = chargeDayReportResult.setChargePer100Km(
          if (runDayReportResult.totalMileage > 0)
            chargeDayReportResult.totalCharge / runDayReportResult.totalMileage * 1000
          else 0)

        Array(c, fullChargeDayReportResult, runDayReportResult)
      })



    result.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(5))
  }

  def compute(vid:String,category:String,reportDate:Long,onlineTime:Int,chargeTime:Int,fullChargeTime:Int,detailValues: Array[DetailModel]): DayReportResult = {
    if (detailValues.nonEmpty) {
      val foldMonid = new ArrayUndoMonidFoldable
      //计算日报表
      val dayreport = foldMonid.foldLeft(detailValues, new DayReportResultFoldMonoid(vid, category, reportDate, chargeTime, fullChargeTime))
      //计算日报表中的分布信息：比如充电开始时间分布，soc分布等
      distribution(detailValues, dayreport)
    } else {
      DayReportResult.zero(vid=vid, category=category, reportDate=reportDate,onlineTime = onlineTime,chargeTime=chargeTime,fullChargeTime=fullChargeTime)
    }
  }

  //分布计算
  def distribution(detailValues:Array[DetailModel],dayreport:DayReportResult): DayReportResult = {
    val chargeTimeRangeDistribution = new ContinueDistributionCounter
    val runTimeDis = new ContinueDistributionCounter
    val socDistributed = new SocDistribution
    val runTimeLenthDistributed = new DiscreteDistribution
    val chargeTimeLenthDistributed = new DiscreteDistribution
    val mileageDistributed = new MileageDistribution

    detailValues.foreach(that => {
      that.category match {
        case Constant.ChargeState =>
          //充电时段分布计算
          chargeTimeRangeDistribution.add(that.startTime, that.endTime)
          //soc分布计算
          socDistributed.add(that.startSoc)
          //充电时长分布计算
          chargeTimeLenthDistributed.add((that.endTime - that.startTime).toInt)
        case Constant.TravelState =>
          //行驶时段分布计算
          runTimeDis.add(that.startTime, that.endTime)
          //行驶时长分布计算
          runTimeLenthDistributed.add((that.endTime - that.startTime).toInt)
          //里程分布计算
          mileageDistributed.add(that.stopMileage - that.startMileage)
        case Constant.FullChargeState =>
        case _ =>
      }
    })

    dayreport.copy(
      chargeSocDistribution = socDistributed.getDistribution,
      chargeTimeRangeDistribution = chargeTimeRangeDistribution.getDistribution,
      /*充电时段*/
      runTimeRangeDistribution = runTimeDis.getDistribution,
      runTimeLengthDistribution = runTimeLenthDistributed.getDistribution,
      runMileageDistribution = mileageDistributed.getDistribution,
      chargeTimeLengthDistribution = chargeTimeLenthDistributed.getDistribution)
  }

  override def unRegister(): Unit = {}

  override def write[Product <: DayReportResult](result: Dataset[DayReportResult]): Unit = {
    val ouputModel = stateConf.getString("report.output").split(',')
    if (ouputModel.length > 1) result.cache()

    if (ouputModel.contains("hdfs")) {
      SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), outputTableName)
    } else if (ouputModel.contains("debug")) {
      result.show(10)
      logWarning("未开启hdfs输出")
    }

    //输出到数据库
    ouputModel.find(x => x == "oracle" || x == "mysql") match {
      case Some(e) if e == "oracle" || e == "mysql" =>
        stateConf.set("database", e)
        logInfo("开启数据库输出")
        outputDatabase(result)
      case _ =>
        logWarning("没有开启数据库输出功能")
    }
  }

  def outputDatabase(result: Dataset[DayReportResult]): Unit = {
    def outputRealinfoReport(): Unit = {
      result.repartition(4).foreachPartition(par => {
        val chargeDatResuls = new ArrayBuffer[DayReportResult]()
        val fullchargeDayResuls = new ArrayBuffer[DayReportResult]()
        val runDayResuls = new ArrayBuffer[DayReportResult]()
        par.foreach(result => {
          result.category match {
            case Constant.ChargeState =>
              chargeDatResuls += result
            case Constant.FullChargeState =>
              fullchargeDayResuls += result
            case Constant.TravelState =>
              runDayResuls += result
            case _ =>
          }
        })

        if (chargeDatResuls.nonEmpty) new ChargeStateDayReportManager(stateConf).output(chargeDatResuls)
        if (fullchargeDayResuls.nonEmpty) new FullChargeStateDayReportModelManager(stateConf).output(fullchargeDayResuls)
        if (runDayResuls.nonEmpty) new RunStateDayReportManager(stateConf).output(runDayResuls)
      })
    }


    outputRealinfoReport()
  }


  override def getThreshold: Long = Utils.getParquetThreshold(stateConf, hadoopConfiguration)
}


object RealinfoJob extends Logging {
  def main(args: Array[String]): Unit = {
    val sparkSession = SparkHelper.getSparkSession(None)
    val stateConf = new StateConf
    stateConf.add(args)

    new RealinfoJob(sparkSession, stateConf).compute()
  }
}package com.bitnei.report.detail

/**
  * Created by francis on 2017/2/9.
  */
case class RealinfoModel(
                          vid:String,
                          vin:String,
                          time:String,
//                          year:String,
//                          month:String,
//                          day:String,
                        //  ruleId:String="1",
                        //速度
                          speed:Option[Int]=None,
                          //电流
                          charge:Option[Int]=None,
                          //soc
                          soc:Option[Int]=None,
                          //总电压
                          totalVoltage:Option[Int]=None,
                          //最大单体电压
                          secondaryCellMaxVoltage:Option[Int]=None,
                          secondaryCellMinVoltage:Option[Int]=None,

                        //发电机温度
                          engineTemp:Option[Int]=None,
                          //最大采集温度
                          accuisitionPointMaxTemp:Option[Int]=None,
                          //最小采集温度
                          accuisitionPointMinTemp:Option[Int]=None,

                        //里程
                          mileage:Option[Int]=None,
                        //经度
                          longitude:Option[Long]=None,
                          //纬度
                          latitude:Option[Long]=None,
                          //挡位
                          peaking:Option[Int]=None) {

}
package com.bitnei.report.dayreport.realinfo
import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.Model.{ChargeStateDayReportManager, FullChargeStateDayReportModelManager, RunStateDayReportManager}
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql.{Dataset, SparkSession}

import scala.collection.mutable.ArrayBuffer
/*
* created by wangbaosheng on 2017/11/2
* 输出到数据库
*/
class RealinfoOutputJob(stateConf: StateConf, sparkSession: SparkSession) extends Serializable with Logging with Job {
  override type R = DayReportResult
  private val sqlContext = sparkSession.sqlContext

  import sqlContext.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("dayreport")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("detail")

  override def registerIfNeed() = SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)

  override def unRegister() = sparkSession.catalog.dropTempView(inputTableName)


  override def doCompute[Product <: DayReportResult]() = sqlContext.sql(s"SELECT * FROM $inputTableName ${SqlHelper.buildWhere(stateConf)}").as[DayReportResult]


  override def write[Product <: DayReportResult](result: Dataset[DayReportResult]) = {
    val outputPaths = stateConf.getString("report.output").split(',')
    outputPaths.foreach({
      case e if e == "oracle" || e == "mysql" =>
        //输出到数据库
        stateConf.set("database", e)
        result.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(10)).foreachPartition(par => {
          val chargeDatResuls = new ArrayBuffer[DayReportResult]()
          val fullchargeDayResuls = new ArrayBuffer[DayReportResult]()
          val runDayResuls = new ArrayBuffer[DayReportResult]()
          par.foreach(result => {
            result.category match {
              case Constant.ChargeState =>
                chargeDatResuls += result
              case Constant.FullChargeState =>
                fullchargeDayResuls += result
              case Constant.TravelState =>
                runDayResuls += result
              case _ =>
            }
          })

          if (chargeDatResuls.nonEmpty) new ChargeStateDayReportManager(stateConf).output(chargeDatResuls)
          if (fullchargeDayResuls.nonEmpty) new FullChargeStateDayReportModelManager(stateConf).output(fullchargeDayResuls)
          if (runDayResuls.nonEmpty) new RunStateDayReportManager(stateConf).output(runDayResuls)
        })
      case e =>
    })
  }
}

object RealinfoOutputJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    new RealinfoOutputJob(stateConf, SparkHelper.getSparkSession(sparkMaster = None)).compute()
  }
}package com.bitnei.report.local

import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-02-23 17:21
  *
  */
object RealInfoSomeExporter {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    Logger.getLogger("org").setLevel(Level.ERROR)
    val sparkConf = new SparkConf()

    val sparkSession = SparkSession.builder().config(sparkConf).master("local").appName(app).getOrCreate()
    import sparkSession.implicits._

    val sc = sparkSession.sparkContext

    /////////////////////////////////////////////////////////////////////////////////////////////////////////


    val sparkContext = sparkSession.sparkContext

    sparkSession.read.textFile("data/vin.txt").rdd.map(x => ("\'" + x + "\'"))
      .reduce((a, b) => {
        a + "," + b
      })

      .foreach(print)

      sparkSession.read.parquet("/spark/vehicle/data/realinfo/year=2017/month=11/day=02").createOrReplaceTempView("realinfo")

    sparkSession.sql(
      """
            select concat( coalesce(`VIN`,' '),",",coalesce(`2000`,' '),",",coalesce(`3201`,' '),",",coalesce(`2301`,' '),",",coalesce(`2201`,' '),",",coalesce(`2202`,' '),",",coalesce(`2613`,' '),",",coalesce(`2614`,' '),",",coalesce(`2615`,' '),",",coalesce(`2617`,' ')) from realinfo where VIN in (
            'LZYTBTBW9G1066310','LZYTBTBW3G1072071','LZYTBTBW1G1072070','LZYTDGAW0G1047731','LZYTDGAW0G1047728','LZYTBTBWXG1068325','LZYTBTBWXG1067918','LZYTBTBW8G1067917','LZYTDGAW9G1047730','LZYTBGBW2G1072215','LZYTBGBW1G1067913','LZYTBGBW0G1072214','LZYTDGAW9G1072482','LZYTDGAW9G1072479','LZYTDGAW9G1042530','LZYTDGAW9G1042527','LZYTDGAW8G1072506','LZYTDGAW8G1072490','LZYTDGAW8G1072487','LZYTDGAW8G1072473','LZYTDGAW8G1042535','LZYTDGAW8G1042521','LZYTDGAW8G1042518','LZYTDGAW7G1072514','LZYTDGAW7G1072500','LZYTDGAW7G1072495','LZYTDGAW7G1072481','LZYTDGAW7G1042526','LZYTDGAW6G1072519','LZYTDGAW6G1072505','LZYTDGAW6G1072486','LZYTDGAW6G1072472','LZYTDGAW6G1042534','LZYTDGAWXG1064925','LZYTDGAW6G1042520','LZYTDGAW6G1042517','LZYTDGAW5G1072513','LZYTDGAW5G1072494','LZYTDGAW5G1072480','LZYTDGAW5G1072477','LZYTDGAW5G1042525','LZYTDGAW4G1072521','LZYTDGAW4G1072518','LZYTDGAW4G1072504','LZYTDGAW4G1072499','LZYTDGAW4G1072485','LZYTDGAW4G1072471','LZYTDGAW4G1042533','LZYTDGAW4G1042516','LZYTDGAW3G1072512','LZYTDGAW3G1072509','LZYTDGAW3G1072493','LZYTDGAW3G1072476','LZYTDGAW3G1042524','LZYTDGAW2G1072520','LZYTDGAW2G1072517','LZYTDGAWXG1064911','LZYTDGAWXG1064908','LZYTDGAW9G1064916','LZYTDGAW8G1064924','LZYTDGAW8G1064910','LZYTDGAW8G1064907','LZYTDGAW7G1064915','LZYTDGAW6G1064923','LZYTDGAW6G1064906','LZYTDGAW5G1064914','LZYTDGAW4G1064922','LZYTDGAW4G1064919','LZYTDGAW4G1064905','LZYTDGAW2G1064921','LZYTDGAW2G1064918','LZYTDGAW1G1064926','LZYTDGAW1G1064912','LZYTDGAW1G1064909','LZYTDGAW0G1064920','LZYTDGAW2G1072503','LZYTDGAW2G1072498','LZYTDGAW2G1072484','LZYTDGAW2G1042532','LZYTDGAW2G1042529','LZYTDGAW1G1072511','LZYTDGAW1G1072508','LZYTDGAW1G1072492','LZYTDGAW1G1072489','LZYTDGAW1G1072475','LZYTDGAW1G1042523','LZYTDGAW0G1072516','LZYTDGAW0G1072497','LZYTDGAW0G1072483','LZYTDGAW0G1042528','LZYTBTBW9G1065352','LZYTBTBW9G1063844','LZYTBTBW6G1065356','LZYTBTBW6G1064563','LZYTBTBW6G1063851','LZYTBTBW5G1063842','LZYTBTBW4G1065355','LZYTBTBW4G1064562','LZYTBTBW2G1064561','LZYTBGBWXG1072219','LZYTBGBWXG1067912','LZYTBGBW9G1072213','LZYTBGBW9G1068209','LZYTDGAWXG1072510','LZYTDGAWXG1072507','LZYTDGAWXG1072491','LZYTDGAWXG1072488','LZYTDGAWXG1072474','LZYTDGAWXG1042522','LZYTDGAWXG1042519','LZYTDGAW9G1072515','LZYTDGAW9G1072501','LZYTDGAW9G1072496','LZYTDGAW0G1064917','LZYTBTBW8G1066931','LZYTBTBW6G1066930','LZYTBGBW8G1072218','LZYTBGBW8G1067911','LZYTBGBW7G1072212','LZYTBGBW7G1068208','LZYTBGBW6G1072217','LZYTBGBW5G1072211','LZYTBGBW5G1068210','LZYTBGBW5G1068207','LZYTBGBW4G1072216','LZYTBGBW3G1072210','LZYTBGBW3G1068206','LZYTBGBW3G1067914','LZYTBGBWXG1073872','LZYTBGBWXG1073869','LZYTBGBW9G1065715','LZYTBGBW8G1073871','LZYTBGBW8G1073868','LZYTBGBW7G1073876','LZYTBGBW7G1065714','LZYTBGBW6G1073870','LZYTDGAWXG1071602','LZYTDGAWXG1071597','LZYTDGAW8G1071601','LZYTDGAW5G1071605','LZYTDGAW3G1071604','LZYTDGAW3G1071599','LZYTDGAW1G1071603','LZYTDGAW1G1071598','LZYTBGBW6G1073867','LZYTBGBW5G1073875','LZYTBGBW4G1073866','LZYTBGBW3G1073874','LZYTBGBW2G1073865','LZYTBGBW1G1073873','LZYTBGBW0G1073864','LZYTBGBW0G1065716','LZYTBTBWXG1074254','LZYTBTBW9G1074262','LZYTBTBWXG1072410','LZYTBTBW9G1072415','LZYTBTBW7G1072414','LZYTBTBW5G1072413','LZYTBTBW1G1072411','LZYTDGAWXG1066660','LZYTDGAW9G1066665','LZYTDGAW7G1066664','LZYTBTBW9G1074259','LZYTBTBW9G1074245','LZYTBTBW8G1074253','LZYTBTBW7G1074261','LZYTBTBW7G1074258','LZYTBTBW6G1074252','LZYTBTBW6G1074249','LZYTBTBW5G1074260','LZYTBTBW5G1074257','LZYTBTBW4G1074251','LZYTBTBW4G1074248','LZYTBTBW3G1074256','LZYTBTBW2G1074250','LZYTBTBW2G1074247','LZYTBTBW1G1074255','LZYTBTBW0G1074263','LZYTBTBW0G1074246','LZYTDGAW9G1047727','LZYTDGAW6G1066669','LZYTDGAW5G1066663','LZYTDGAW4G1066668','LZYTDGAW3G1066662','LZYTDGAW2G1066667','LZYTDGAW1G1066661','LZYTDGAW0G1066666','LZYTDGAW7G1047726','LZYTDGAW5G1047725','LZYTDGAW3G1047724','LA81F1HTXFA101876','LA81F1HT9GA100168','LA81F1HT0GA100169','LZYTBTBW4F1059859','LA81F1HT5FA101283','LA81F1HT3FA101296','LA81F1HT1FA101314','LVCB3L4D3GM002017','LVCB3L4D5GM001984','LVCB3L4D7GM001985','LVCB3L4D2GM001988','LVCB3L4D2GM002011','LVCB3L4D0GM001987','LVCB3L4D3GM001997','LVCB3L4D5GM001998','LVCB3L4D7GM002151','LVCB3L4D5GM002150','LVCB3L4D6GM002142','LVCB3L4D8GM002112','LVCB3L4D5GM002116','LVCB3L4D1GM002145','LVCB3L4D9GM002118','LVCB3L4D1GM002114','LVCB3L4D3GM002115','LA81F1HT6GA100550','LVCB3L4D7GM001615','LVCB3L4D5GM001628','LVCB3L4D3GM001613','LVCB3L4D6GM001623','LVCB3L4DXGM001625','LVCB3L4D6GM001637','LVCB3L4D9GM001633','LVCB3L4DXGM001639','LVCB3L4D4GM001622','LVCB3L4D8GM001624','LVCB3L4D7GM002005','LVCB3L4D8GM001977','LVCB3L4D1GM001979','LVCB3L4D6GM001993','LVCB3L4D8GM001994','LVCB3L4D1GM001996','LVCB3L4D3GM001983','LZYTBTBW6G1018361','LZYTBTBW8G1018362','LZYTBTBWXG1018363','LZYTBTBW3G1018365','LZYTBTBW5G1018366','LZYTBTBW7G1018367','LZYTBTBW0G1018369','LZYTBTBW7G1018370','LVCB3L4DXGM002113','LVCB3L4D9GM002121','LVCB3L4D1GM002159','LVCB3L4D0GM002122','LVCB3L4D2GM002140','LVCB3L4DXGM002158','LVCB3L4D3GM002129','LVCB3L4D7GM002134','LVCB3L4D3GM002132','LVCB3L4D6GM002125','LVCB3L4D7GM002148','LVCB3L4D4GM002138','LVCB3L4D9GM002135','LVCB3L4D4GM002141','LVCB3L4D6GM002156','LVCB3L4D0GM002153','LVCB3L4D2GM002154','LVCB3L4D7GM001632','LVCB3L4D1GM001612','LA81F1HT4FA101212','LA81F1HT6GA100094','LA81F1HT1FA101300','LA81F1HT0FA101403','LA81F1HT4FA101436','LA81F1HT1FA101216','LA81F1HT3FA101217','LA81F1HT7FA101219','LA81F1HT5FA101221','LVCB4L4D1GM001820','LVCB4L4D4GM001794','LVCB4L4D3GM001849','LA81F1HT6FA101227','LA81F1HT8FA101231','LA81F1HT7FA101236','LA81F1HT8FA101245','LA81F1HT5FA101249','LA81F1HT1FA101250','LA81F1HT3FA101251','LA81F1HT9FA101254','LA81F1HT9FA101450','LA81F1HT5FA101638','LA81F1HTXFA101635','LA81F1HT3FA101623','LA81F1HT4FA101596','LA81F1HTXFA101554','LA81F1HT2FA101015','LA81F1HT6FA101051','LVCB4L4D6GM001893','LA81F1HT0FA101045','LA81F1HTXFA101053','LA81F1HT4FA101016','LA81F1HT2FA101032','LA81F1HT4FA101050','LA81F1HT1FA101040','LA81F1HT3FA101301','LA81F1HT9FA101304','LA81F1HT4FA101307','LA81F1HT8FA101309','LA81F1HT4FA101310','LA81F1HT6FA101311','LA81F1HTXFA101313','LA81F1HT9FA101321','LA81F1HT2FA101323','LA81F1HT8FA101326','LVCB4L4D9GM001838','LVCB4L4D1GM001865','LVCB4L4D2GM001888','LVCB4L4D8GM001877','LVCB4L4D7GM001899','LVCB4L4D2GM001907','LVCB4L4D1GM001798','LVCB4L4D3GM001852','LVCB4L4D1GM001817','LVCB4L4D7GM001823','LVCB4L4D7GM001952','LVCB4L4D0GM001906','LVCB4L4D6GM001909','LVCB4L4D8GM001927','LVCB4L4D0GM001968','LVCB4L4D2GM001969','LVCB4L4D2GM001941','LA81F1HT8FA101049','LA81F1HT3FA101007','LA81F1HT3FA101041','LA81F1HT0FA101143','LA81F1HT2FA101144','LA81F1HT4FA101145','LA81F1HT6FA101146','LA81F1HT8FA101147','LA81F1HTXFA101148','LA81F1HT1FA101149','LA81F1HTXFA101151','LA81F1HT1FA101152','LA81F1HT5FA101154','LA81F1HT2FA101158','LA81F1HT4FA101159','LVCB4L4D7FG016772','LVCB4L4DXFG016782','LVCB4L4D2FG016789','LZYTBTBWXG1016905','LZYTBTBW6G1018389','LA81F1HT2FA101161','LA81F1HT4FA101162','LA81F1HT8FA101164','LA81F1HT1FA101166','LA81F1HT3FA101167','LA81F1HT5FA101168','LA81F1HT5FA101171','LVCB4L4D8GM001863','LZYTBTBW2G1018390','LZYTBTBW4G1018391','LZYTBTBW8G1018393','LZYTBTBWXG1018394','LZYTBTBW3G1018396','LZYTBTBW5G1018397','LZYTBTBW9G1018399','LVCB4L4D3GM001236','LVCB4L4D0GM001937','LVCB4L4D2GM001261','LVCB4L4D3GM001303','LVCB4L4D2GM001275','LVCB4L4DXFG016796','LVCB4L4D3FG016798','LVCB4L4D2GM001955','LVCB4L4D0GM001940','LVCB4L4D5FG016799','LZYTBTBW1G1018400','LZYTBTBW7G1018403','LZYTBTBW8G1020810','LZYTBTBW1G1020812','LA81F1HT7FA101043','LA81F1HTXFA101005','LA81F1HT8FA101052','LA81F1HT2FA101029','LA81F1HT4FA101047','LA81F1HT0FA101014','LA81F1HT9FA101013','LA81F1HT4FA101002','LVCB4L4D4GM001763','LA81F1HT3FA101881','LA81F1HT8FA101889','LA81F1HT4FA101887','LA81F1HT8GA100095','LA81F1HT1FA101328','LA81F1HT1FA101331','LA81F1HT7FA101334','LA81F1HT9FA101335','LA81F1HT3FA101184','LA81F1HT0FA101191','LA81F1HT2FA101192','LA81F1HT6FA101194','LA81F1HTXFA101196','LA81F1HT5FA101199','LA81F1HT8FA101200','LA81F1HTXFA101201','LA81F1HT1FA101202','LA81F1HT7FA101205','LA81F1HT9FA101206','LA81F1HT0FA101207','LA81F1HT5GA100104','LA81F1HT9GA100123','LA81F1HT9GA100106','LA81F1HTXGA100146','LA81F1HT4GA100126','LA81F1HT2GA100142','LA81F1HT2GA100111','LVCB4L4D1GM001770','LVCB4L4DXGM001766','LA81F1HT1GA100164','LA81F1HT3GA100165','LVCB4L4D8GM001667','LVCB4L4DXGM001671','LVCB4L4D5GM001674','LVCB4L4D0GG010801','LVCB4L4D4GM001200','LVCB4L4D0GM001212','LA81F1HT9GA100087','LVCB4L4D1GM001204','LVCB4L4D4GM001245','LVCB4L4D9GM001239','LVCB4L4D0GM001243','LA81F1HT7FA101138','LVCB4L4D5FG016804','LVCB4L4D3FG016817','LVCB4L4D3GM001205','LVCB4L4D1GM001669','LVCB4L4D2GM001664','LVCB4L4D4GM001665','LVCB4L4D1GM001672','LVCB4L4D3GM001673','LVCB4L4D2GM001678','LVCB4L4D6GM001666','LVCB4L4D2GM001227','LVCB4L4D1GM001719','LVCB4L4D2GM001695','LZYTBTBW9F1059856','LZYTBTBW0F1059857','LA81F1HT3GA100568','LA81F1HT1GA100567','LA81F1HT8FA101021','LA81F1HT9GA100557','LA81F1HT8GA100565','LVCB4L4D3FG016834','LVCB4L4D2FG016839','LVCB4L4D2FG016842','LA81F1HT0GA100544','LVCB4L4DXGG010840','LA81F1HT3GA100540','LA81F1HT5GA100538','LA81F1HT6GA100502','LA81F1HT7GA100508','LVCB4L4D5GM001299','LVCB4L4DXGM001704','LVCB4L4DXGM001721','LA81F1HT4GA100532','LA81F1HT5GA100510','LA81F1HT2GA100545','LA81F1HT0GA100530','LA81F1HT8GA100520','LA81F1HT4GA100160','LVCB4L4D4GG010879','LVCB4L4D4GG010882','LVCB4L4D5GG010910','LVCB4L4D9GM001712','LVCB4L4D4GM001696','LA81F1HT1GA100505','LA81F1HT1GA100536','LA81F1HTXGA100521','LVCB4L4D8GM001703','LA81F1HT1GA100133','LA81F1HT3GA100134','LA81F1HT8GA100159','LA81F1HT7GA100587','LA81F1HT4FA101579','LA81F1HT0FA101580','LA81F1HT6FA101583','LA81F1HT8FA101584','LA81F1HTXFA101585','LA81F1HT1FA101586','LA81F1HT3FA101587','LA81F1HT9FA101593','LA81F1HT0FA101594','LA81F1HT2FA101595','LA81F1HT8FA101598','LA81F1HT9FA101660','LA81F1HT1GA100813','LA81F1HT3GA100814','LA81F1HT5GA100815','LVCB4L4D3GG011599','LZYTBTBW6G1039789','LZYTBTBW2G1039790','LZYTBTBW8G1031743','LZYTBTBW6G1039792','LZYTBTBW8G1040023','LVCB4L4D1GG011603','LZYTBTBWXG1031744','LZYTBTBWXG1031761','LZYTBTBW3G1031763','LZYTBTBW7G1031765','LZYTBTBWXG1031775','LZYTBTBW5G1031781','LZYTBTBW7G1031782','LZYTBTBW2G1031785','LZYTBTBW6G1031790','LZYTBTBW7G1039767','LZYTBTBW0G1039769','LZYTBTBW7G1039770','LZYTBTBWXG1039777','LZYTBTBW1G1039778','LZYTBTBW1G1039781','LZYTBTBW3G1039782','LA81F1HT4FA101470','LA81F1HT8FA101472','LA81F1HT1FA101474','LA81F1HT2GA100786','LA81F1HT8GA100789','LA81F1HT6GA100791','LA81F1HT6GA101083','LA81F1HTXGA101085','LA81F1HT1GA101086','LA81F1HT3GA101087','LA81F1HT5GA101088','LA81F1HT3GA101090','LA81F1HT5GA101091','LA81F1HT7GA101092','LA81F1HT9GA101093','LA81F1HT0GA101094','LA81F1HT4GA101096','LA81F1HT6GA101097','LA81F1HTXGA101099','LA81F1HT2GA101100','LA81F1HT6GA101102','LA81F1HT8GA101103','LA81F1HTXGA101104','LZYTBTBW3G1039765','LZYTBTBW5G1039766','LZYTBTBW4G1039774','LZYTBTBW6G1039775','LZYTBTBWXG1039780','LZYTBTBW7G1031734','LZYTBTBW4G1031738','LA81F1HT2GA101081','LA81F1HT1GA101105','LA81F1HT3GA101106','LZYTBTBW4G1031741','LZYTBTBW5G1031750','LZYTBTBW2G1031754','LA81F1HTXFA101117','LA81F1HT1FA101118','LA81F1HT3FA101119','LA81F1HT1FA101121','LA81F1HT3FA101122','LA81F1HT5FA101123','LA81F1HT9FA101125','LA81F1HT0FA101126','LA81F1HTXGA100793','LVCB4L4DXGG010790','LA81F1HT5GA100796','LA81F1HT0GA100799','LA81F1HT5GA100801','LA81F1HT9GA100803','LA81F1HT2GA100805','LA81F1HT4GA100806','LZYTBTBW1G1031728','LZYTBTBW3G1031732','LZYTBTBW9G1031735','LZYTBTBW0G1031736','LZYTBTBW2G1031737','LZYTBTBW6G1031739','LZYTBTBW2G1031740','LZYTBTBW6G1031742','LZYTBTBW9G1039785','LZYTBTBW3G1031794','LZYTBTBW9G1031797','LA81F1HTXGA100809','LA81F1HT6GA100810','LA81F1HT5GA101107','LA81F1HT7GA101108','LA81F1HT9GA101109','LA81F1HT7GA101111','LA81F1HT0GA101113','LA81F1HT6GA101116','LA81F1HTXGA101118','LA81F1HT6GA100032','LZYTBTBW3G1031777','LZYTBTBW0G1031784','LZYTBTBW6G1031787','LZYTBTBW8G1031788','LZYTBTBWXG1031792','LVCB4L4D2GG011528','LVCB4L4DXGG011535','LVCB4L4D7GG011542','LZYTBTBW2G1039787','LZYTBTBW4G1039788','LZYTBTBW0G1031770','LZYTBTBW5G1031795','LZYTBTBW7G1031796','LZYTBTBW5G1031733','LA81F1HT4FA101565','LA81F1HT8FA101567','LA81F1HT4FA101405','LA81F1HT6FA101406','LZYTBTBW1G1031745','LZYTBTBW3G1031746','LZYTBTBW5G1031747','LZYTBTBW7G1031748','LZYTBTBW9G1031749','LZYTBTBW7G1031751','LZYTBTBW0G1031753','LZYTBTBW6G1031756','LZYTBTBW8G1031760','LZYTBTBW1G1031762','LZYTBTBW6G1031773','LZYTBTBW8G1031774','LA81F1HT8GA101120','LA81F1HT1GA101122','LA81F1HT0GA101127','LA81F1HT4GA101079','LVCB4L4D1GG011567','LVCB4L4D8GG011582','LA81F1HT9FA101416','LA81F1HT2FA101421','LA81F1HT6FA101423','LVCB4L4D1GM001882','LVCB4L4D2GM001860','LVCB4L4D6GG011547','LA81F1HT2FA101256','LA81F1HT8FA101259','LA81F1HT4FA101260','LA81F1HT6FA101261','LVCB4L4D7GM001854','LVCB4L4DXGM001900','LVCB4L4D0GM001890','LVCB4L4D1GM001901','LA81F1HT5FA101266','LA81F1HT0FA101269','LA81F1HT6FA101275','LA81F1HT8FA101276','LA81F1HT3FA101279','LVCB4L4D0GM001730','LVCB4L4DXGM001735','LA81F1HTXFA101280','LA81F1HT0FA101286','LA81F1HT6FA101289','LA81F1HT6FA101292','LA81F1HT8FA101293','LA81F1HT1FA101295','LA81F1HT7FA101298','LVCB4L4D3GM001740','LVCB4L4D5GM001755','LVCB4L4D9GM001743','LVCB4L4DXGM001816','LVCB4L4D5GM001870','LVCB4L4D1GG011536','LVCB4L4D5GM001822','LVCB4L4D1GM001834','LVCB4L4D5GM001836','LVCB4L4D1GM001848','LA81F1HT6FA101003','LA81F1HT6FA101017','LA81F1HT6FA101034','LA81F1HT3FA101055','LA81F1HT5FA101011','LA81F1HT7FA101009','LA81F1HT3FA101038','LA81F1HT8FA101035','LA81F1HT5FA101042','LA81F1HT3FA101024','LA81F1HT9FA101030','LA81F1HT2AF101001','LA81F1HTXFA101019','LA81F1HTXFA101022','LA81F1HT1FA101006','LA81F1HT1FA101054','LA81F1HT1FA101037','LA81F1HT6FA101048','LVCB4L4D6FG016746','LVCB4L4D1FG016749','LVCB4L4D6GG011578','LVCB4L4D5GG011281','LVCB4L4D7GG011282','LVCB4L4D2GG011299','LVCB4L4D9GG011302','LVCB4L4D8GG011310','LVCB4L4D4GG011322','LVCB4L4D1GG011326','LVCB4L4DXGM003744','LVCB4L4D5GM003747','LVCB4L4D0GM003784','LVCB4L4D8GM003676','LVCB4L4D4GM004078','LVCB4L4D6GM004079','LVCB4L4D3GM004086','LVCB4L4D5GM004123','LVCB4L4D2GM004130','LVCB4L4D6GM004132','LVCB4L4D8GM004133','LVCB4L4D9GM004139','LVCB4L4D7GM004141','LVCB4L4D6GM004146','LVCB4L4D3GM004153','LVCB4L4D9GM004156','LVCB4L4D4GM004162','LVCB4L4D8GM004164','LVCB4L4D0GM004417','LVCB4L4D1GG012072','LVCB4L4D6GG012052','LVCB4L4D2GM003740','LVCB4L4D3GG060172','LVCB4L4D3GM004413','LVCB4L4D2GM004418','LVCB4L4D4GM004419','LVCB4L4D2GM004421','LVCB4L4D4GM004422','LVCB4L4D6GM004423','LVCB4L4D8GM004424','LVCB4L4D4GM004176','LA81F1HT7FA101477','LA81F1HT7FA101480','LA81F1HT1FA101491','LA81F1HT5FA101493','LA81F1HT7FA101494','LA81F1HT9FA101495','LA81F1HT0FA101496','LA81F1HT6FA101499','LA81F1HT0FA101501','LA81F1HT6FA101552','LA81F1HT1FA101555','LA81F1HT9FA101559','LA81F1HT3FA101654','LA81F1HT5FA101655','LA81F1HT7FA101656','LA81F1HT9FA101657','LA81F1HT0FA101658','LA81F1HT2FA101659','LA81F1HT0FA101661','LA81F1HT2FA101662','LA81F1HT4FA101663','LA81F1HT6FA101664','LA81F1HTXFA101666','LA81F1HT1FA101667','LA81F1HT1FA101670','LA81F1HT5FA101672','LA81F1HT0GA100768','LA81F1HT2GA100769','LA81F1HT6GA100774','LA81F1HTXGA100776','LA81F1HT3GA100778','LA81F1HT1GA100780','LA81F1HT3GA100781','LA81F1HT7GA100783','LA81F1HT9GA100784','LVCB4L4DXGG011325','LVCB4L4D6GM001277','LVCB4L4D2GM001289','LVCB4L4D8GM004407','LVCB4L4DXGM004408','LVCB4L4D5GM004414','LVCB4L4D9GM004416','LA81F1HTXFA101425','LA81F1HT1FA101426','LA81F1HT5FA101428','LVCB4L4D7GG011556','LVCB4L4D0GG011611','LVCB4L4D9GG011557','LVCB4L4D4GG011563','LVCB4L4D8GG011520','LA81F1HTXFA101599','LA81F1HT6FA101602','LA81F1HTXFA101604','LA81F1HT7FA101608','LA81F1HT9FA101609','LVCB4L4D4GG012079','LVCB4L4D2GG011612','LVCB4L4D5GG011524','LA81F1HT3FA101430','LA81F1HT7FA101432','LA81F1HT2FA101435','LVCB4L4DXGM004411','LVCB4L4D7GM003748','LVCB4L4D9GM003752','LVCB4L4D2GM003754','LA81F1HTXFA101439','LA81F1HT6FA101440','LA81F1HT1FA101443','LA81F1HT3FA101444','LA81F1HT7FA101446','LA81F1HT9FA101447','LA81F1HT4FA101453','LA81F1HT6FA101454','LA81F1HT1FA101457','LA81F1HT3FA101458','LA81F1HT3FA101461','LVCB4L4D3GM003777','LA81F1HT8FA101620','LVCB4L4D6GG012004','LA81F1HT7FA101639','LA81F1HT6FA101647','LA81F1HTXFA101618','LA81F1HT1FA101619','LVCB4L4D8GG011288','LA81F1HT4FA101467','LVCB4L4D2GG011285','LVCB4L4D5GM003778','LVCB4L4D6GM001294','LVCB4L4D8GM004410','LA9CA8N09GBBFC483','LA9CA8N06GBBFC487','LA9CA8N08GBBFC488','LA9CA8N06GBBFC490','LA9CA8N01GBBFC493','LA9CA8N05GBBFC495','LA9CA8N07GBBFC496','LA9CA8N07GBBFC420','LA9CA8N09GBBFC421','LVCB4L4D8GG011291','LA9CA8N08GBBFC460','LVCB4L4DXGG012071','LVCB4L4D1GG012086','LVCB4L4D6GG011287','LA9CA8N01GBBFC476','LA9CA8N07GBBFC479','LA9CA8N03GBBFC480','LA9CA8N07GBBFC482','LVCB4L4D1GM004149','LA9CA8N0XGBBFC427','LA9CA8N03GBBFC429','LA9CA8N00GBBFC436','LA9CA8N02GBBFC437','LA9CA8N06GBBFC442','LA9CA8N08GBBFC443','LA9CA8N01GBBFC445','LA9CA8N05GBBFC450','LA9CA8N07GBBFC451','LVCB4L4D7GM004155','LVCB4L4DXGM004165','LA9CA8N04GBBFC469','LA9CA8N00GBBFC470','LA9CA8N02GBBFC471','LA9CA8N04GBBFC472','LA9CA8N06GBBFC473','LVCB4L4D0GG011608','LA9CA8N0XGBBFC475','LVCB4L4D5GG012012','LVCB4L4D6GG012049','LVCB4L4D7GG012058','LVCB4L4D7GM004110','LVCB4L4D2GM004144','LVCB4L4DXGG013060','LVCB4L4D1GG013061','LVCB4L4D1GM004412','LA9CA8N05GBBFC402','LA9CA8N06GBBFC425','LA9CA8N08GBBFC426','LA9CA8N0XGBBFC430','LA9CA8N01GBBFC431','LA9CA8N03GBBFC432','LA9CA8N05GBBFC433','LA9CA8N09GBBFC435','LA9CA8N06GBBFC439','LA9CA8N02GBBFC440','LA9CA8N04GBBFC441','LA9CA8N03GBBFC446','LA9CA8N05GBBFC447','LA9CA8N03GBBFC401','LA9CA8N07GBBFC403','LA9CA8N09GBBFC404','LA9CA8N00GBBFC405','LA9CA8N02GBBFC406','LA9CA8N06GBBFC408','LA9CA8N08GBBFC409','LA9CA8N04GBBFC410','LA9CA8N01GBBFC462','LA9CA8N05GBBFC464','LA9CA8N09GBBFC497','LA9CA8N00GBBFC498','LA9CA8N06GBBFC411','LA9CA8N08GBBFC412','LA9CA8N01GBBFC414','LA9CA8N03GBBFC415','LA9CA8N07GBBFC417','LA9CA8N09GBBFC418','LA9CA8N00GBBFC419','LA9CA8N07GBBFC448','LA9CA8N09GBBFC449','LA9CA8N08GBBFC457','LA9CA8N00GBBFC484','LA9CA8N02GBBFC485','LA9CA8N00GBBFC453','LA9CA8N02GBBFC454','LA9CA8N04GBBFC455','LA9CA8N0XGBBFC458','LA9CA8N0XGBBFC461','LA9CA8N03GBBFC463','LA9CA8N07GBBFC465','LA9CA8N09GBBFC466','LA9CA8N00GBBFC467','LVCB4L4D8GM003628','LVCB4L4D4GG011286','LVCB4L4D2GM004080','LA9CA8N02GBBFC423','LA9CA8N04GBBFC424','LZYTDGAWXG1074306','LZYTDGAWXG1074290','LZYTDGAWXG1074287','LZYTDGAWXG1072913','LZYTDGAWXG1072894','LZYTDGAW9G1074300','LZYTDGAW9G1074295','LZYTDGAW9G1074281','LZYTDGAW9G1072904','LZYTDGAW9G1072899','LZYTDGAW8G1074305','LZYTDGAWXG1068781','LZYTDGAWXG1068778','LZYTDGAWXG1068764','LZYTDGAWXG1068750','LZYTDGAWXG1068747','LZYTDGAW9G1069520','LZYTDGAW9G1069517','LZYTDGAW9G1069503','LZYTDGAW8G1074286','LZYTDGAW8G1072912','LZYTDGAW8G1072909','LZYTDGAW7G1074294','LZYTDGAW7G1074280','LZYTDGAW7G1072903','LZYTDGAW7G1072898','LZYTDGAW9G1069498','LZYTDGAW9G1069484','LZYTDGAW9G1069470','LZYTDGAW9G1069467','LZYTDGAW9G1069453','LZYTDGAW9G1069436','LZYTDGAW9G1069422','LZYTDGAW9G1069419','LZYTDGAW9G1069405','LZYTDGAW9G1069372','LZYTDGAW9G1069369','LZYTBGBWXG1069918','LZYTBGBW9G1069294','LZYTBGBW8G1069920','LZYTBGBW8G1069299','LZYTBGBW6G1069298','LZYTBGBW4G1069297','LZYTBGBW2G1069301','LZYTBGBW2G1069296','LZYTBGBW1G1069919','LZYTBGBW0G1069295','LZYTDGAW6G1074299','LZYTDGAW6G1074285','LZYTDGAW6G1072911','LZYTDGAW6G1072908','LZYTDGAW5G1074309','LZYTDGAW5G1074293','LZYTDGAW5G1072902','LZYTDGAW0G1064335','LZYTDGAW0G1064321','LZYTDGAW5G1072897','LZYTDGAW4G1074303','LZYTDGAW4G1074284','LZYTDGAW4G1072910','LZYTDGAW4G1072907','LZYTDGAW3G1074308','LZYTDGAW3G1074292','LZYTDGAW3G1074289','LZYTDGAW3G1072901','LZYTDGAW3G1072896','LZYTDGAW2G1074302','LZYTDGAW2G1074297','LZYTBGBWXG1064363','LZYTBGBW9G1064368','LZYTBGBW8G1064362','LZYTBGBW8G1064359','LZYTBGBW7G1064367','LZYTBGBW6G1064361','LZYTBGBW6G1064358','LZYTBGBW5G1064366','LZYTBGBW4G1064360','LZYTBGBW4G1064357','LZYTBGBW3G1064365','LZYTBGBW1G1064364','LZYTDGAWXG1064326','LZYTDGAW9G1064334','LZYTDGAW8G1064339','LZYTDGAW8G1064325','LZYTDGAW7G1064333','LZYTDGAW6G1064338','LZYTDGAW6G1064324','LZYTDGAW5G1064332','LZYTDGAW5G1064329','LZYTDGAW4G1064340','LZYTDGAW4G1064323','LZYTDGAW3G1064331','LZYTDGAW3G1064328','LZYTDGAW2G1064336','LZYTDGAW2G1064322','LZYTDGAW1G1064330','LZYTDGAW1G1064327','LZYTDGAW9G1068786','LZYTDGAW9G1068772','LZYTDGAW9G1068769','LZYTDGAW9G1068755','LZYTDGAW9G1068741','LZYTDGAW9G1068738','LZYTDGAW8G1069525','LZYTDGAW2G1074283','LZYTDGAW2G1072906','LZYTDGAW1G1074307','LZYTDGAW1G1074288','LZYTDGAW1G1072900','LZYTDGAW1G1072895','LZYTDGAW0G1074301','LZYTDGAW0G1074296','LZYTDGAW0G1074282','LZYTDGAW8G1069511','LZYTDGAW8G1069508','LZYTDGAW8G1069489','LZYTDGAW8G1069475','LZYTDGAW8G1069461','LZYTDGAW8G1069458','LZYTDGAW8G1069444','LZYTDGAW8G1069430','LZYTDGAW8G1069427','LZYTDGAW8G1069413','LZYTDGAW0G1072905','LZYTDGAWXG1066674','LZYTDGAW9G1072921','LZYTDGAW9G1072918','LZYTDGAW9G1066679','LZYTDGAW8G1066673','LZYTDGAW7G1072920','LZYTDGAW7G1072917','LZYTDGAW6G1072925','LZYTDGAWXG1069526','LZYTDGAWXG1069512','LZYTDGAWXG1069509','LZYTDGAWXG1069493','LZYTDGAWXG1069476','LZYTDGAWXG1069462','LZYTDGAWXG1069459','LZYTDGAWXG1069445','LZYTDGAWXG1069431','LZYTDGAWXG1069414','LZYTDGAWXG1069400','LZYTDGAWXG1069395','LZYTDGAW6G1066672','LZYTDGAW5G1072916','LZYTDGAW5G1066677','LZYTDGAW4G1066671','LZYTDGAW3G1072915','LZYTDGAW3G1066676','LZYTDGAW2G1072923','LZYTDGAW2G1066670','LZYTDGAW1G1066675','LZYTDGAW0G1072922','LZYTDGAW0G1072919','LZYTDGAWXG1069381','LZYTDGAWXG1069378','LZYTDGAW5G1066680','LZYTDGAW8G1069380','LVCB3L4D2GM002137','LVCB3L4D5GM002147','LVCB3L4D9GM002149','LVCB3L4D9GM002152','LVCB3L4D5GM001614','LVCB3L4D1GM001626','LVCB3L4D3GM001627','LVCB3L4D5GM001631','LVCB3L4D0GM001634','LVCB3L4D2GM001635','LVCB3L4D8GM001638','LVCB3L4DXGM001978','LVCB3L4D1GM001982','LVCB3L4D4GM001989','LVCB3L4D0GM001990','LVCB3L4D4GM001992','LVCB3L4DXGM001995','LVCB3L4D0GM002136','LVCB4L4D0GG011933','LVCB3L4D7GM002117','LVCB3L4D7GM002120','LVCB3L4D2GM002123','LVCB3L4DXGM002127','LVCB3L4D1GM002128','LVCB3L4D6GM002139','LVCB3L4D3GM001630','LVCB3L4DXGM001981','LVCB3L4D9GM001986','LVCB3L4D4GM002110','LVCB3L4D0GM002119','LVCB3L4D1GM002131','LVCB3L4DXGM002144','LVCB3L4D4GM002155','LVCB4L4D0GG011897','LVCB4L4D9GG011915','LVCB3L4D1GM002016','LVCB3L4D6GM002111','LVCB4L4D4GG011949','LVCB3L4D2GM001991','LVCB3L4D7GM001999','LVCB4L4DXGG011891','LVCB4L4D6GG011905','LVCB4L4D9GM004013','LVCB4L4D4GG012082','LA9ACAABXFALYX055','LA9ACAAB7FALYX062','LA9ACAAB0FALYX100','LA9ACAAB8FALYX068','LA9ACAAB8FALYX054','LA9ACAAB5FALYX089','LA9ACAAB0FALYX114','LA9ACAAB3FALYX043','LA9ACAAB4FALYX052','LA9ACAAB1FALYX090','LVCB4L4D7GG011914','LVCB4L4D6GG011922','LVCB4L4D5GG011927','LVCB4L4D7GG011928','LVCB3L4D8GM002000','LVCB4L4D1GG011956','LVCB4L4D3GM004539','LVCB4L4D8GG011937','LVCB3L4D8GM002126','LA9ACAAB2FALYX101','LVCB4L4D2GG011934','LVCB4L4D4GG012017','LVCB4L4D3GG012025','LVCB4L4D9GG012028','LVCB4L4D9GG030495','LVCB4L4D5GM001772','LVCB4L4D7GM001756','LVCB4L4D9GM001757','LVCB4L4D4GM001777','LVCB4L4D4GM003898','LVCB4L4DXGM003890','LVCB4L4D5GM003909','LVCB4L4D0GG010751','LVCB4L4D9GM003749','LVCB4L4D8GM003757','LVCB4L4D6GM003739','LVCB4L4D3GM003746','LVCB4L4D6FG016763','LVCB4L4D8GM001782','LVCB4L4DXGM001797','LVCB4L4D0GM001839','LVCB4L4D4FG016793','LVCB4L4D1GM001879','LVCB4L4D6GM001876','LVCB4L4D4GG010929','LVCB3L4D8GM002143','LVCB3L4D8GM002160','LVCB4L4D9GG010912','LVCB4L4DXGM003761','LA9ACAABXFALYX069','LA9ACAAB5FALYX044','LVCB3L4D8GM002157','LVCB4L4D1FG016802','LVCB4L4D5FG016818','LVCB4L4D6FG016827','LVCB4L4DXFG016829','LVCB4L4D8FG016831','LA9ACAAB1FALYX039','LA9ACAAB0FALYX033','LA9ACAAB1FALYX025','LA9ACAAB8FALYX023','LVCB4L4D9GM001824','LVCB3L4D1GM003943','LVCB3L4D3GM003944','LVCB3L4D5GM003945','LVCB3L4D7GM003946','LVCB3L4D0GM003948','LVCB3L4D2GM003949','LVCB3L4D9GM003950','LVCB4L4D0GG011575','LVCB4L4D6GG011550','LVCB4L4D1GG011553','LVCB4L4D6GG011564','LVCB4L4DXGG011602','LVCB4L4D5GG011605','LVCB4L4D0FG016838','LVCB4L4D9FG016840','LVCB3L4D8GM003969','LVCB3L4D4GM003970','LVCB3L4D6GM003971','LVCB3L4D8GM003972','LVCB3L4DXGM003973','LVCB3L4D1GM003974','LVCB4L4D0FG016516','LVCB3L4D3GG012819','LVCB3L4DXGG012820','LVCB3L4D1GG012821','LVCB3L4D0GM003951','LVCB3L4D2GM003952','LVCB4L4D4GM001732','LA9ACAAB7FALYX126','LA9ACAAB6FALYX120','LVCB3L4D1GM003960','LVCB3L4D3GM003961','LA9ACAABXFALYX170','LA9ACAAB8FALYX118','LVCB4L4D2GM001714','LVCB4L4D2GM001728','LVCB4L4D7GM001725','LVCB4L4D8GM001734','LVCB4L4DXGM001718','LA9ACAAB1FALYX087','LA9ACAAB3FALYX088','LA9ACAAB3FALYX091','LVCB3L4D5GM003962','LVCB3L4D7GM003963','LVCB3L4D9GM003964','LVCB3L4D0GM003965','LVCB3L4D2GM003966','LVCB3L4D4GM003967','LVCB3L4D6GM003968','LA9ACAAB9FALYX161','LA9ACAAB0FALYX162','LA9ACAAB3FALYX110','LVCB4L4D4GG011529','LVCB4L4D0GG011530','LVCB4L4D3GG012932','LA9ACAAB7FALYX031','LA9ACAAB0FALYX064','LA9ACAAB5FALYX125','LA9ACAAB7FALYX045','LA9ACAAB1FALYX042','LA9ACAAB3FALYX169','LA9ACAAB3FALYX026','LA9ACAAB8FALYX166','LVCB4L4D3GM001933','LVCB4L4D4GM001942','LVCB4L4DXGM003792','LVCB4L4D4GG012907','LVCB4L4D6GG012908','LVCB4L4D8GG012909','LVCB3L4D7GG012810','LVCB3L4D9GG012811','LVCB3L4D0GG012812','LVCB3L4D2GG012813','LVCB3L4D4GG012814','LVCB3L4D6GG012815','LVCB3L4D8GG012816','LVCB3L4DXGG012817','LA9ACAAB7FALYX160','LA9ACAAB9FALYX029','LA9ACAAB7FALYX112','LA9ACAAB0FALYX159','LA9ACAAB2FALYX163','LA9ACAAB5FALYX075','LA9ACAABXFALYX122','LA9ACAAB6FALYX148','LA9ACAAB4FALYX150','LA9ACAABXFALYX136','LA9ACAAB7FALYX143','LA9ACAAB8FALYX149','LA9ACAAB5FALYX013','LVCB3L4D1GG012818','LA9ACAAB8FALYX099','LA9ACAAB5FALYX139','LA9ACAAB8FALYX006','LA9ACAAB6FALYX005','LA9ACAAB4FALYX097','LA9ACAAB1FALYX011','LA9ACAAB4FALYX147','LA9ACAAB5FALYX142','LA9ACAAB1FALYX140','LA9ACAAB2FALYX132','LA9ACAAB3FALYX155','LA9ACAABXFALYX105','LA9ACAABXFALYX153','LA9ACAAB6FALYX165','LA9ACAAB9FALYX158','LVCB4L4D7GM003815','LA9ACAAB6FALYX117','LA9ACAAB1FALYX154','LA9ACAAB7FALYX157','LA9ACAAB7FALYX109','LA9ACAAB8FALYX152','LA9ACAAB4FALYX116','LA9ACAAB1FALYX123','LA9ACAAB4FALYX133','LA9ACAAB3FALYX012','LVCB4L4D7GM003829','LVCB4L4DXGM003789','LVCB4L4D3GM001883','LVCB4L4DXGM001198','LVCB4L4D6GM001201','LVCB4L4D5GM001271','LA9ACAAB4FALYX004','LVCB4L4D4GM001908','LVCB4L4DXGM001928','LVCB4L4D8GM001913','LVCB4L4D6GM001957','LA9ACAAB9FALYX001','LVCB4L4D6GM003806','LVCB4L4D7GM001241','LVCB3L4D4GG012859','LVCB3L4D2GM003935','LVCB3L4D4GM003936','LVCB3L4D6GM003937','LVCB3L4D8GM003938','LVCB3L4DXGM003939','LVCB3L4D6GM003940','LVCB3L4D8GM003941','LVCB3L4DXGM003942','LVCB3L4D4GM003953','LVCB3L4D6GM003954','LVCB3L4D8GM003955','LVCB3L4DXGM003956','LVCB3L4D1GM003957','LVCB3L4D3GM003958','LVCB3L4D5GM003959','LVCB4L4D7GG011279','LVCB4L4D8GM003841','LVCB4L4DXGG010854','LVCB4L4D3GG011554','LVCB4L4D0GG010880','LVCB4L4D4GG011515','LVCB4L4D3GG011523','LVCB4L4D5GG011538','LVCB4L4D9GM001287','LVCB4L4D9GM001662','LVCB3L4DXGG012848','LVCB3L4D1GG012849','LVCB3L4D8GG012850','LVCB3L4DXGG012851','LVCB3L4D1GG012852','LVCB3L4D5GG012854','LVCB3L4D7GG012855','LVCB3L4D9GG012856','LVCB3L4D0GG012857','LA9ACAABXFALYX007','LA9ACAAB2FALYX146','LA9ACAAB2FALYX003','LVCB4L4D3FG016767','LVCB4L4D1FG016783','LVCB4L4D5FG016785','LVCB4L4D9FG016790','LVCB4L4D1FG016797','LA9ACAAB8FALYX135','LA9ACAAB6FALYX151','LA9ACAAB0FALYX145','LA9ACAAB0FALYX016','LVCB4L4D1GM004166','LVCB4L4D5GM004168','LVCB4L4D5GM004171','LVCB4L4D1GM004118','LVCB4L4D3GM004136','LVCB4L4D3GM004167','LVCB4L4D4GM004145','LVCB4L4D5GM004090','LVCB4L4D1GG030491','LA9ACAAB3FALYX138','LA9ACAAB2FALYX129','LA9ACAABXFALYX010','LA9ACAAB2FALYX020','LA9ACAAB3FALYX107','LA9ACAAB1FALYX008','LA9ACAAB4FALYX018','LA9ACAAB9FALYX127','LVCB4L4D2GG011142','LVCB4L4D3GG030492','LVCB4L4D5GG030493','LVCB4L4D7GG030494','LVCB4L4D0GG010863','LVCB4L4D7GG011539','LVCB4L4D5GG011541','LVCB4L4D0GG011544','LVCB4L4D3GG011568','LVCB4L4D3GG011571','LVCB4L4D9GG011574','LVCB4L4DXGG011583','LVCB4L4D2GG011531','LVCB4L4D7GG011587','LVCB4L4D0GG011589','LVCB4L4D9GG011591','LVCB4L4D6GG011595','LVCB4L4D1GG011598','LVCB4L4D5GG010793','LVCB4L4D6GG011600','LVCB4L4D7GG011606','LVCB3L4D6GG012846','LVCB3L4D8GG012847','LVCB4L4D8FG016800','LVCB4L4D4GG011613','LVCB4L4D6GG011614','LVCB4L4DXGG010806','LVCB4L4D3GM003830','LVCB4L4D6GM003885','LVCB4L4D5GM001898','LVCB4L4D3GM001950','LZYTDGAW0G1048717','LZYTDGAW0G1048703','LZYTDGAW0G1048698','LZYTDGAW0G1048684','LZYTDGAWXG1073303','LZYTDGAWXG1073298','LZYTDGAWXG1072605','LZYTDGAWXG1072586','LZYTDGAWXG1072572','LZYTDGAWXG1072569','LZYTDGAWXG1072555','LZYTDGAWXG1072541','LZYTDGAWXG1072538','LZYTDGAWXG1072524','LZYTDGAW9G1073308','LZYTDGAW9G1073292','LZYTDGAW9G1073289','LZYTDGAW9G1072613','LZYTDGAW9G1072594','LZYTDGAW9G1072580','LZYTBTBW0G1062792','LZYTBTBW4G1031786','LZYTBTBWXG1031789','LZYTBTBW4G1039791','LZYTBTBW9G1049491','LZYTBTBW2G1049493','LZYTBTBW5G1072671','LZYTBTBW7G1072672','LZYTDGAW6G1066168','LZYTBTBW9G1072673','LZYTBTBW0G1072674','LZYTBTBW1G1018395','LZYTBTBW7G1018398','LZYTBTBW4G1019511','LZYTBTBW4G1049494','LZYTBTBW6G1049495','LZYTBTBWXG1049497','LZYTDGAW5G1066193','LZYTDGAW5G1066176','LZYTDGAW5G1066162','LZYTDGAW5G1061656','LZYTDGAW5G1048728','LZYTDGAW5G1048714','LZYTDGAW5G1048700','LZYTDGAW5G1048695','LZYTDGAW5G1048678','LZYTDGAW4G1066203','LZYTDGAW4G1066198','LZYTDGAW4G1066184','LZYTDGAW4G1066170','LZYTDGAW4G1066167','LZYTDGAW4G1061650','LZYTDGAW4G1061647','LZYTDGAW4G1048722','LZYTDGAW4G1048719','LZYTDGAW4G1048705','LZYTDGAW4G1048686','LZYTDGAW4G1048672','LZYTDGAW3G1066208','LZYTDGAW3G1066192','LZYTDGAW3G1066189','LZYTDGAW3G1066175','LZYTDGAW3G1066161','LZYTDGAW3G1061655','LZYTDGAW3G1048730','LZYTDGAW3G1048727','LZYTDGAW3G1048694','LZYTDGAW3G1048680','LZYTDGAW3G1048677','LZYTDGAW2G1066202','LZYTDGAW2G1066197','LZYTDGAW2G1066183','LZYTDGAW2G1066166','LZYTDGAW2G1061663','LZYTDGAW2G1061646','LZYTDGAW2G1048721','LZYTDGAW2G1048718','LZYTDGAW2G1048704','LZYTDGAW2G1048699','LZYTDGAW2G1048685','LZYTDGAW2G1048671','LZYTDGAW1G1066207','LZYTDGAW1G1066191','LZYTDGAW1G1066188','LZYTDGAW1G1066174','LZYTDGAW1G1066160','LZYTDGAW1G1061654','LZYTDGAW1G1073741','LZYTDGAW0G1073746','LZYTDGAW1G1048726','LZYTDGAW1G1048712','LZYTDGAW1G1048693','LZYTDGAW1G1048676','LZYTDGAW0G1066201','LZYTDGAW0G1066196','LZYTDGAW0G1066182','LZYTDGAW0G1066179','LZYTDGAW0G1066165','LZYTDGAW0G1061662','LZYTDGAW0G1061659','LZYTDGAW0G1061645','LZYTDGAW0G1048720','LVCB4L4D7GG012867','LVCB4L4D9GG012868','LVCB4L4D0GG012869','LVCB4L4D7GG012870','LVCB4L4D9GG012871','LA9ACAAB9FALYX094','LA9ACAAB6FALYX067','LZYTDGAW8G1066169','LZYTDGAW8G1061652','LZYTDGAW3G1072588','LZYTDGAWXG1048692','LZYTDGAW8G1048707','LZYTDGAW8G1048691','LZYTDGAW8G1048688','LZYTDGAW8G1048674','LZYTDGAW7G1066194','LZYTDGAW7G1066180','LZYTDGAW7G1066177','LZYTDGAW7G1066163','LZYTDGAW7G1061660','LZYTDGAW7G1061657','LZYTDGAW7G1048729','LZYTDGAW7G1048715','LZYTDGAW7G1048701','LZYTDGAW7G1048696','LZYTDGAW7G1048682','LA9ACAAB9FALYX077','LA9ACAAB2FALYX079','LA9ACAAB1FALYX073','LA9ACAAB4FALYX083','LA9ACAAB8FALYX085','LA9ACAABXFALYX086','LZYTBTBW9G1018368','LZYTBTBW6G1018392','LVCB4L4D2FG016517','LVCB4L4D4FG016518','LZYTBTBWXG1040024','LVCB4L4D9GG012899','LVCB4L4D1GG012900','LVCB4L4D3GG012901','LVCB4L4D5GG012902','LVCB4L4D7GG012903','LVCB4L4D9GG012904','LZYTDGAW7G1048679','LZYTDGAW6G1066204','LZYTDGAW6G1066199','LZYTDGAW6G1066185','LZYTDGAW6G1066171','LZYTDGAW1G1072556','LZYTDGAW8G1048710','LZYTDGAWXG1048708','LVCB4L4D2GG011304','LVCB4L4D0GG011303','LVCB4L4D7GG011301','LVCB4L4D5GG011300','LVCB4L4D0GG011298','LVCB4L4D9GG011297','LZYTBTBW2G1062793','LZYTBTBW4G1062794','LZYTBTBW6G1062795','LZYTBTBW8G1062796','LZYTBTBWXG1062797','LVCB4L4D2GG012906','LVCB4L4D2FG016520','LVCB4L4D4FG016521','LVCB4L4D6FG016522','LVCB4L4D8FG016523','LVCB4L4DXFG016524','LVCB4L4D1FG016525','LVCB4L4D6GG011290','LVCB4L4DXGG011289','LVCB4L4D0GG011284','LVCB4L4D9GG011283','LVCB4L4D3GG011280','LVCB4L4D7GG011296','LVCB4L4D5GG011295','LVCB4L4D3GG011294','LVCB4L4DXGG011292','LVCB4L4D5GM003876','LVCB4L4D9GM001435','LA9ACAAB5FALYX092','LA9ACAAB8FALYX121','LA9ACAAB7FALYX093','LA9ACAAB5FALYX156','LA9ACAAB4FALYX164','LZYTBTBW1G1062798','LZYTBTBW3G1062799','LZYTBTBW6G1062800','LZYTBTBWXG1049483','LVCB4L4D5GG011328','LA9ACAABXFALYX041','LA9ACAAB6FALYX036','LA9ACAAB2FALYX034','LZYTDGAW6G1061651','LZYTDGAW6G1061648','LZYTDGAW6G1048723','LZYTDGAW6G1048706','LZYTDGAW6G1048690','LZYTDGAW6G1048687','LZYTDGAW6G1048673','LZYTDGAW5G1066209','LZYTDGAW8G1048724','LVCB4L4D3GG012879','LVCB4L4D1GG012881','LVCB4L4D3GG012882','LVCB4L4D5GG012883','LVCB4L4D7GG012884','LVCB4L4D9GG012885','LVCB4L4D0GG012886','LVCB4L4D2GG012887','LVCB4L4D4GG012888','LVCB3L4D5GG012823','LA9ACAAB7FALYX028','LA9ACAAB1FALYX168','LA9ACAABXFALYX167','LA9ACAABXFALYX038','LA9ACAAB2FALYX051','LA9ACAAB9FALYX063','LVCB4L4D5GG011572','LVCB4L4D6GG011581','LVCB4L4D7GG011590','LVCB4L4DXFG016779','LZYTBTBW3G1049499','LZYTBTBW6G1049500','LZYTBTBW6G1064370','LZYTBTBW3G1072667','LZYTBTBW5G1072668','LZYTBTBW7G1072669','LZYTBTBW3G1072670','LVCB4L4D4FG016809','LVCB4L4D8GG011324','LVCB4L4D6GG011323','LVCB4L4D2GG011321','LVCB4L4D4GG011319','LVCB4L4D2GG011318','LVCB4L4D0GG011317','LVCB4L4D9GG011316','LVCB4L4D7GG011315','LVCB4L4D5GG011314','LVCB4L4D3GG011313','LVCB4L4DXGG011311','LVCB3L4D9GG012839','LVCB3L4D5GG012840','LVCB3L4D7GG012841','LVCB3L4D9GG012842','LVCB3L4D0GG012843','LVCB3L4D2GG012844','LVCB4L4D3FG016770','LVCB4L4D7GM003877','LZYTDGAW9G1073745','LVCB4L4D1GG011309','LVCB4L4DXGG011308','LVCB4L4D8GG011307','LVCB4L4D6GG011306','LVCB4L4D0GG012872','LVCB4L4D2GG012873','LVCB4L4D4GG012874','LVCB4L4D6GG012875','LVCB4L4D8GG012876','LVCB4L4DXGG012877','LVCB4L4D1GG012878','LVCB4L4D2GM003219','LVCB4L4D6GM003790','LVCB4L4D6GG012889','LVCB4L4D2GG012890','LVCB4L4D4GG012891','LVCB4L4D6GG012892','LVCB4L4D9GM003864','LVCB4L4D4GM003884','LVCB4L4D0GM003882','LVCB4L4D1GM003888','LZYTDGAW7G1073744','LZYTDGAW6G1073752','LZYTDGAW6G1073749','LZYTDGAW5G1073743','LZYTDGAW4G1073751','LZYTDGAW4G1073748','LZYTDGAW3G1073742','LZYTDGAW2G1073750','LZYTDGAW9G1072546','LZYTDGAW9G1072532','LZYTDGAW9G1072529','LZYTDGAW8G1073302','LZYTDGAW8G1073297','LZYTDGAW8G1072604','LZYTDGAW8G1072599','LZYTDGAW8G1072585','LZYTDGAW8G1072568','LZYTDGAW8G1072554','LVCB4L4D8GG012893','LVCB4L4DXGG012894','LVCB4L4D1GG012895','LVCB4L4D3GG012896','LVCB4L4D5GG012897','LVCB4L4D7GG012898','LZYTBTBW7G1049487','LZYTBTBW9G1049488','LZYTDGAW8G1072540','LZYTDGAW8G1072537','LZYTDGAW8G1072523','LZYTDGAW7G1073310','LZYTDGAW7G1073307','LZYTDGAW7G1073291','LZYTDGAW7G1073288','LZYTDGAW7G1072612','LZYTDGAW7G1072609','LZYTDGAW9G1072577','LZYTDGAW9G1072563','LZYTDGAW7G1072576','LZYTDGAW7G1072562','LZYTDGAW7G1072559','LZYTDGAW7G1072545','LZYTDGAW7G1072531','LZYTDGAW7G1072528','LZYTDGAW6G1073301','LZYTDGAW6G1073296','LZYTBTBW5G1018402','LZYTBTBW4G1031772','LZYTBTBW5G1031778','LZYTBTBW3G1031780','LZYTBTBW9G1031783','LZYTBTBWXG1031758','LZYTBTBW0G1062789','LZYTBTBW7G1062790','LZYTBTBW9G1062791','LA9ACAAB5FALYX027','LA9ACAAB5FALYX030','LZYTDGAW6G1072617','LZYTDGAW6G1072603','LZYTDGAW6G1072598','LZYTDGAW6G1072584','LZYTDGAW6G1072570','LZYTDGAW6G1072567','LZYTDGAW6G1072553','LZYTDGAW6G1072536','LZYTDGAW6G1072522','LZYTDGAW5G1073306','LZYTDGAW5G1073290','LZYTDGAW5G1073287','LA9ACAAB9FALYX032','LA9ACAAB0FALYX050','LA9ACAAB2FALYX115','LA9ACAAB1FALYX056','LVCB4L4D6GG011113','LVCB4L4D1GG011147','LZYTDGAW2G1073747','LZYTDGAW5G1072592','LZYTDGAW5G1072589','LZYTDGAW5G1072575','LZYTDGAW5G1072561','LVCB4L4D5FG016771','LZYTDGAW5G1072558','LZYTDGAW5G1072544','LZYTDGAW5G1072530','LZYTDGAW4G1073300','LZYTDGAW4G1073295','LZYTDGAW4G1072616','LZYTDGAW4G1072602','LZYTDGAW4G1072597','LZYTDGAW4G1072583','LZYTDGAW4G1072566','LZYTDGAW4G1072552','LZYTDGAW4G1072549','LZYTDGAW4G1072535','LZYTDGAW3G1072610','LZYTDGAW3G1072607','LZYTDGAW3G1072591','LZYTDGAW3G1072560','LZYTDGAW3G1072557','LZYTDGAW3G1072543','LVCB4L4D2FG016775','LVCB4L4D3GG011540','LA9ACAAB6FALYX070','LA9ACAAB0FALYX078','LA9ACAAB6FALYX053','LA9ACAAB4FALYX066','LA9ACAAB5FALYX061','LA9ACAAB2FALYX065','LVCB3L4D7GG012824','LVCB3L4D9GG012825','LVCB4L4D5GM001948','LVCB4L4D5GM003814','LVCB4L4D5GM004137','LVCB4L4D5GM004154','LVCB4L4D6GM004096','LZYTDGAW3G1072526','LZYTDGAW2G1073294','LZYTDGAW2G1072615','LZYTDGAW2G1072601','LZYTDGAW2G1072596','LZYTDGAW2G1072582','LZYTDGAW2G1072579','LZYTDGAW2G1072565','LZYTDGAW2G1072551','LZYTDGAW2G1072548','LZYTDGAW2G1072534','LZYTDGAW1G1073304','LZYTDGAW1G1073299','LZYTDGAW1G1072606','LZYTDGAW1G1072590','LZYTDGAW1G1072587','LZYTDGAW1G1072573','LZYTDGAW7G1072593','LZYTDGAW3G1072574','LZYTDGAW1G1072539','LA9ACAAB0FALYX047','LA9ACAAB2FALYX048','LA9ACAAB0FALYX081','LA9ACAAB2FALYX082','LZYTBTBW1G1049484','LZYTBTBW3G1049485','LZYTBTBW5G1049486','LVCB4L4DXGG011115','LVCB3L4D0GG012826','LVCB3L4D4GG012828','LVCB3L4D6GG012829','LVCB3L4D4GG012831','LVCB3L4D6GG012832','LVCB3L4D8GG012833','LVCB3L4DXGG012834','LVCB3L4D1GG012835','LVCB3L4D3GG012836','LVCB3L4D5GG012837','LVCB3L4D7GG012838','LVCB4L4D6GM004163','LZYTDGAW0G1073309','LZYTDGAW0G1073293','LZYTDGAW0G1072614','LZYTDGAW0G1072600','LZYTDGAW0G1072595','LZYTDGAW0G1072581','LZYTDGAW0G1072578','LZYTDGAW0G1072564','LZYTDGAW0G1072550','LZYTDGAW0G1072547','LZYTDGAW0G1072533','LZYTDGAWXG1066206','LZYTDGAWXG1066190','LZYTDGAWXG1066187','LZYTDGAWXG1066173','LZYTDGAWXG1061653','LZYTDGAWXG1048711','LZYTDGAW5G1072608','LZYTBTBW8G1031757','LZYTBTBW1G1031759','LZYTBTBW5G1031764','LZYTBTBW9G1031766','LZYTBTBW1G1031793','LZYTBTBW0G1039772','LZYTBTBW2G1039773','LZYTBTBW8G1039776','LZYTBTBW3G1039779','LZYTBTBW5G1039783','LZYTBTBW0G1039786','LA9ACAAB9FALYX113','LZYTDGAW1G1072542','LVCB4L4D4GG011305','LVCB4L4D9GM004125','LZYTDGAWXG1048689','LZYTDGAWXG1048675','LZYTDGAW9G1066200','LZYTDGAW9G1066195','LZYTDGAW9G1066181','LZYTDGAW9G1066178','LZYTDGAW9G1061661','LZYTDGAW9G1061658','LZYTDGAW9G1061644','LZYTDGAW9G1048716','LZYTDGAW9G1048702','LZYTDGAW9G1048683','LZYTDGAW8G1066205','LZYTDGAW8G1066186','LZYTDGAW8G1066172','LVCB4L4DXGM004084','LVCB4L4DXGM004151','LVCB4L4D0GM004420','LVCB4L4D1GM004409','LVCB4L4D1GM004426','LVCB4L4D7GM004415','LVCB4L4DXGM004425','LVCB4L4D8GG130751','LVCB4L4D3GG011327','LVCB4L4D6GM004177','LVCB4L4D9GM004092','LVCB4L4D4GG012860','LVCB4L4D6GG012861','LVCB4L4D8GG012862','LVCB4L4DXGG012863','LVCB4L4D1GG012864','LVCB4L4D3GG012865','LVCB4L4D5GG012866','LZYTBTBW7G1068086','LZYTBTBWXG1068132','LZYTBTBWXG1068129','LZYTBTBWXG1068115','LZYTBTBWXG1068101','LZYTBTBWXG1068096','LZYTBTBWXG1068082','LZYTBTBWXG1068079','LZYTBTBW9G1068140','LZYTBTBW9G1068137','LZYTBTBW9G1068123','LZYTBTBW9G1068090','LZYTBTBW9G1068087','LZYTBTBW8G1068114','LZYTBTBW8G1068100','LZYTBTBW8G1068095','LZYTBTBW8G1068081','LZYTBTBW8G1068078','LZYTBTBW7G1068198','LZYTBTBW7G1068122','LVCB3L4D9FM009293','LVCB3L4D0FM009294','LVCB3L4D4FM009296','LVCB3L4D6FM009297','LVCB3L4D8FM009298','LVCB3L4D4FM009301','LVCB3L4D6FM009302','LVCB3L4DXFM009304','LVCB3L4D3FM009306','LVCB3L4D5FM009310','LVCB3L4D0FM009313','LVCB3L4D1FM009241','LVCB3L4D6FM009252','LVCB3L4D9FM009262','LVCB3L4D2FM009264','LVCB3L4D7FM009275','LVCB3L4D6FM009283','LVCB3L4D5FM009288','LVCB3L4D3FM009290','LVCB3L4D5FM009291','LVCB3L4D7FM009292','LVCB3L4D4FM009315','LZYTDGAWXG1058462','LZYTDGAWXG1058459','LZYTDGAW8G1058461','LZYTDGAW8G1058458','LZYTDGAW6G1058460','LZYTDGAW8G1069377','LZYTDGAW8G1068777','LZYTDGAW8G1068763','LZYTDGAW8G1068746','LZYTDGAW7G1069516','LZYTDGAW7G1069502','LZYTDGAW7G1069497','LZYTDGAW7G1069483','LZYTDGAW7G1069466','LZYTDGAW7G1069449','LZYTDGAW7G1069435','LZYTDGAW7G1069421','LZYTDGAW7G1069418','LZYTDGAW7G1069404','LZYTDGAW7G1069399','LZYTDGAW7G1069385','LZYTDGAW7G1069371','LZYTDGAW7G1069368','LZYTDGAW4G1068789','LZYTDGAW4G1068775','LZYTDGAW4G1068761','LZYTDGAW4G1068758','LZYTDGAW4G1068744','LZYTDGAW3G1069528','LZYTDGAW3G1069514','LZYTDGAW3G1069500','LZYTDGAW3G1069481','LZYTDGAW3G1069478','LZYTDGAW3G1069464','LZYTDGAW3G1069450','LZYTDGAW3G1069447','LZYTDGAW3G1069433','LZYTDGAW3G1069416','LZYTDGAW3G1069402','LZYTDGAW3G1069397','LZYTDGAW3G1069383','LZYTDGAW3G1069366','LZYTDGAW0G1068742','LZYTDGAW0G1068739','LZYTDGAWXG1064830','LZYTDGAWXG1064827','LZYTDGAWXG1064813','LZYTDGAWXG1064780','LZYTDGAWXG1064777','LZYTDGAWXG1064441','LZYTDGAWXG1064438','LZYTDGAWXG1064424','LZYTDGAWXG1064410','LZYTDGAWXG1064407','LZYTDGAW9G1064821','LZYTDGAW9G1064818','LZYTDGAW9G1064804','LZYTDGAW9G1064799','LZYTDGAW9G1064785','LZYTDGAW9G1064446','LZYTDGAW0G1068756','LZYTDGAW3G1064409','LZYTDGAW2G1064823','LZYTDGAW2G1064790','LZYTDGAW2G1064787','LZYTDGAW2G1064451','LZYTDGAW2G1064434','LZYTDGAW2G1064420','LZYTDGAW2G1064417','LZYTDGAW2G1064403','LZYTDGAW1G1064831','LZYTDGAW1G1064828','LZYTDGAW1G1064814','LZYTDGAW1G1064800','LZYTDGAW1G1064795','LZYTDGAW1G1064781','LZYTDGAW1G1064778','LZYTDGAW1G1064442','LZYTDGAW1G1064439','LZYTDGAW7G1068785','LZYTDGAW7G1068771','LZYTDGAW7G1068768','LZYTDGAW7G1068754','LZYTDGAW7G1068737','LZYTDGAW6G1069524','LZYTDGAW6G1069510','LZYTDGAW6G1069507','LZYTDGAW6G1069491','LZYTDGAW6G1069488','LZYTDGAW6G1069474','LZYTDGAW6G1069460','LZYTDGAW6G1069457','LZYTDGAW6G1069443','LZYTDGAW6G1069426','LZYTDGAW6G1069409','LZYTDGAW6G1069393','LZYTDGAW6G1069376','LZYTDGAW6G1058457','LZYTDGAW5G1058465','LZYTDGAW4G1058456','LZYTDGAW3G1058464','LZYTDGAW1G1058463','LZYTDGAW3G1068783','LZYTDGAW3G1068766','LZYTDGAW3G1068752','LZYTDGAW3G1068749','LZYTDGAW2G1069522','LZYTDGAW2G1069519','LZYTDGAW2G1069505','LZYTDGAW2G1069486','LZYTDGAW2G1069472','LZYTDGAW2G1069469','LZYTDGAW2G1069455','LZYTDGAW2G1069441','LZYTDGAW2G1069438','LZYTDGAW2G1069424','LZYTDGAW2G1069410','LZYTDGAW2G1069407','LZYTDGAW2G1069391','LZYTDGAW2G1069374','LZYTDGAW2G1068791','LZYTDGAW2G1068788','LZYTDGAW9G1064432','LZYTDGAW9G1064429','LZYTDGAW9G1064415','LZYTDGAW9G1064401','LZYTDGAW8G1064826','LZYTDGAW8G1064812','LZYTDGAW8G1064809','LZYTDGAW8G1064793','LZYTDGAW8G1064454','LZYTDGAW8G1064440','LZYTDGAW8G1064437','LZYTDGAW8G1064423','LZYTDGAW8G1064406','LZYTDGAW7G1064820','LZYTDGAW7G1064817','LZYTDGAW7G1064803','LZYTDGAW7G1064784','LZYTDGAW7G1064445','LZYTDGAW7G1064431','LZYTDGAW1G1064408','LZYTDGAW0G1064822','LZYTDGAW0G1064819','LZYTDGAW0G1064805','LZYTDGAW0G1064786','LZYTDGAW0G1064450','LZYTDGAW0G1064447','LZYTDGAW0G1064416','LZYTDGAW0G1064402','LZYTDGAW1G1064425','LZYTDGAW1G1064411','LZYTDGAW6G1068793','LZYTDGAW6G1068776','LZYTDGAW6G1068762','LZYTDGAW6G1068745','LZYTDGAW5G1069529','LZYTDGAW5G1069515','LZYTDGAW5G1069496','LZYTDGAW5G1069482','LZYTDGAW5G1069479','LZYTDGAW5G1069465','LZYTDGAW5G1069451','LZYTDGAW5G1069448','LZYTDGAW5G1069434','LZYTDGAW5G1069420','LZYTDGAW5G1069403','LZYTDGAW5G1069398','LZYTDGAW5G1069384','LZYTDGAW2G1068774','LZYTDGAW2G1068760','LZYTDGAW2G1068757','LZYTDGAW2G1068743','LZYTDGAW1G1069527','LZYTDGAW1G1069480','LZYTDGAW1G1069477','LZYTDGAW1G1069463','LZYTDGAW1G1069446','LZYTDGAW1G1069432','LZYTDGAW1G1069429','LZYTDGAW1G1069415','LZYTDGAW1G1069401','LZYTDGAW1G1069396','LZYTDGAW1G1069382','LZYTDGAW1G1069379','LZYTDGAW1G1069365','LZYTDGAW1G1068782','LZYTDGAW1G1068779','LZYTDGAW7G1064428','LZYTDGAW7G1064414','LZYTDGAW7G1064400','LZYTDGAW6G1064811','LZYTDGAW6G1064808','LZYTDGAW6G1064792','LZYTDGAW6G1064789','LZYTDGAW6G1064453','LZYTDGAW6G1064436','LZYTDGAW6G1064422','LZYTDGAW6G1064419','LZYTDGAW6G1064405','LZYTDGAW5G1064816','LZYTDGAW5G1064802','LZYTDGAW5G1064797','LZYTDGAW5G1064783','LZYTDGAW5G1064444','LZYTDGAW5G1064430','LZYTDGAW5G1064427','LZYTBTBW9G1061091','LZYTBTBW7G1061235','LZYTBTBW7G1061090','LZYTBTBW4G1061094','LZYTBTBW2G1061224','LZYTBTBW2G1061093','LZYTBTBW0G1061223','LZYTBTBW0G1061092','LZYTBTBW0G1061089','LZYTDGAW4G1068792','LZYTDGAW0G1068773','LZYTBTBW0G1073856','LZYTDGAWXG1069252','LZYTDGAWXG1069249','LZYTDGAW9G1069260','LZYTDGAW9G1069257','LZYTDGAW9G1069243','LZYTDGAW8G1069251','LZYTDGAW8G1069248','LZYTDGAW7G1069256','LZYTDGAW7G1069242','LZYTDGAW7G1069239','LZYTDGAW6G1069250','LZYTDGAW6G1069247','LZYTDGAW5G1069255','LZYTDGAW5G1069241','LZYTDGAW5G1069238','LZYTDGAW4G1069263','LZYTDGAW3G1064412','LZYTDGAW4G1069246','LZYTDGAW3G1069254','LZYTDGAW3G1069240','LZYTDGAW3G1069237','LZYTDGAW2G1069262','LZYTDGAW2G1069259','LZYTDGAW2G1069245','LZYTDGAW1G1069253','LZYTDGAW1G1069236','LZYTDGAW0G1069261','LZYTDGAW0G1069258','LZYTDGAW0G1069244','LZYTBTBW6G1068130','LZYTBTBW6G1068113','LZYTBTBW6G1068094','LZYTBTBW6G1068080','LZYTBTBW5G1068135','LZYTBTBW5G1068118','LZYTBTBW5G1068104','LZYTBTBW5G1068085','LZYTBTBW4G1068112','LZYTBTBW4G1068109','LZYTBTBW4G1068093','LZYTBTBW3G1071793','LZYTBTBW7G1068105','LZYTBTBW3G1068117','LZYTBTBW3G1068103','LZYTBTBW2G1068142','LZYTBTBW2G1068139','LZYTBTBW2G1068125','LZYTBTBW2G1068111','LZYTBTBW2G1068108','LZYTBTBW2G1068092','LZYTBTBW1G1068102','LZYTBTBW1G1068097','LZYTBTBW1G1068083','LZYTBTBW0G1068141','LZYTBTBW0G1068138','LZYTBTBW0G1068124','LZYTBTBW0G1068091','LZYTBTBW0G1068088','LZYTBTBW3G1068098','LZYTBTBW3G1068084','LZYTDGAWXG1070708','LZYTDGAW9G1070716','LZYTDGAW8G1070710','LZYTDGAW8G1070707','LZYTDGAW7G1070715','LZYTDGAW5G1070714','LZYTDGAW4G1070722','LZYTBTBW6G1062232','LZYTBTBW4G1062231','LZYTBTBWXG1074657','LZYTBTBW1G1074658','LZYTDGAW5G1069370','LZYTDGAW5G1069367','LZYTDGAW5G1068784','LZYTDGAW5G1068767','LZYTDGAW5G1068753','LZYTDGAW4G1069523','LZYTDGAW4G1069490','LZYTDGAW4G1069487','LZYTDGAW4G1069473','LZYTDGAW4G1069456','LZYTDGAW4G1069442','LZYTDGAW4G1069439','LZYTDGAW4G1069425','LZYTDGAW4G1069411','LZYTDGAW4G1069408','LZYTDGAW4G1069392','LZYTDGAW4G1069389','LZYTDGAW4G1069375','LZYTDGAW1G1068765','LZYTBTBW9G1075153','LZYTBTBW0G1075154','LZYTDGAW4G1070719','LZYTDGAW4G1070705','LZYTDGAW1G1068751','LZYTDGAW1G1068748','LZYTDGAW0G1069521','LZYTDGAW0G1069518','LZYTDGAW0G1069504','LZYTDGAW0G1069499','LZYTDGAW0G1069485','LZYTDGAW0G1069471','LZYTDGAW0G1069468','LZYTDGAW0G1069454','LZYTDGAW0G1069440','LZYTDGAW0G1069437','LZYTDGAW0G1069423','LZYTDGAW0G1069390','LZYTDGAW0G1069387','LZYTDGAW0G1069373','LZYTDGAW0G1068790','LZYTDGAW0G1068787','LZYTDGAW2G1070721','LZYTDGAW2G1070718','LZYTDGAW1G1070712','LZYTDGAW1G1070709','LZYTDGAW0G1070720','LZYTDGAW0G1070717','LZYTDGAW0G1070703','LZYTBTBWXG1073850','LZYTBTBW7G1073854','LZYTBTBW5G1073853','LZYTBTBW4G1073858','LZYTBTBW3G1073852','LZYTBTBW3G1073849','LZYTBTBW2G1073857','LZYTBTBW1G1073851','LZYTDGAW5G1064413','LZYTDGAW4G1064824','LZYTDGAW4G1064810','LZYTDGAW4G1064807','LZYTDGAW4G1064791','LZYTDGAW4G1064788','LZYTDGAW4G1064452','LZYTDGAW4G1064449','LZYTDGAW4G1064435','LZYTDGAW4G1064421','LZYTDGAW4G1064418','LZYTDGAW4G1064404','LZYTDGAW3G1064829','LZYTDGAW3G1064815','LZYTDGAW3G1064801','LZYTDGAW3G1064796','LZYTDGAW3G1064782','LZYTDGAW3G1064779','LZYTDGAW3G1064443','LZYTBTBW2G1071512','LZYTDGAW3G1064426','LZYTBTBW9G1061222','LVCB3L4D8FM009317','LVCB3L4D1FM009319','LVCB3L4DXFM009321','LVCB3L4D7FM009325','LVCB3L4D9FM009326','LVCB3L4D0FM009327','LVCB3L4D2FM009328','LVCB3L4D4FM009329','LVCB3L4D0FM009330','LVCB3L4D0GM001097','LZYTBTBW9G1069966','LZYTBTBW4G1069969','LZYTBTBW0G1069967','LZYTBTBW1G1068326','LZYTBTBW4G1069308','LZYTBTBW2G1069307','LZYTBTBW0G1069306','LZYTBTBW8G1069960','LZYTBTBWXG1074240','LZYTBTBW7G1074244','LZYTBTBWXG1069958','LZYTBTBW8G1069957','LZYTBTBW1G1069959','LZYTBTBW3G1074242','LZYTBTBW3G1069963','LZYTBTBW6G1070959','LZYTBTBW4G1070958','LZYTBTBW2G1070960','LZYTBTBW4G1073620','LZYTBTBW7G1069304','LZYTBTBW9G1071930','LZYTBTBW9G1071927','LZYTBTBW8G1071935','LZYTBTBW7G1071926','LZYTBTBW5G1071942','LZYTBTBW5G1071939','LZYTBTBW5G1071925','LZYTBTBW4G1071933','LZYTBTBW3G1071941','LZYTBTBW3G1071924','LZYTBTBW2G1071929','LZYTBTBW1G1071940','LZYTBTBW1G1071937','LZYTBTBW0G1071931','LZYTBGBW9G1072230','LZYTBGBW9G1072227','LZYTBTBWXG1076215','LZYTBTBW8G1076214','LZYTBTBW7G1076222','LZYTBTBW7G1076219','LZYTBGBW7G1072226','LZYTBGBW2G1072229','LZYTBGBW0G1072231','LZYTBGBW0G1072228','LZYTBTBW6G1076213','LZYTBTBW5G1076221','LZYTBTBW5G1076218','LZYTBTBW4G1076212','LZYTBTBW4G1076209','LZYTBTBW3G1076217','LZYTBTBW3G1076198','LZYTBTBW2G1076225','LZYTBTBW2G1076211','LZYTBTBW2G1076208','LZYTBTBW0G1076210','LZYTBTBW9G1069305','LZYTBTBW0G1076207','LZYTBTBW5G1067910','LZYTBTBW8G1064631','LZYTBTBW3G1064634','LZYTBTBW1G1064633','LZYTBTBW5G1075151','LZYTBTBW3G1066108','LZYTBTBW8G1076195','LZYTBTBW3G1066304','LZYTBTBW1G1066303','LZYTBTBWXG1069023','LZYTBTBW8G1069022','LZYTBTBW6G1069021','LZYTBTBW5G1069026','LZYTBTBW1G1069024','LZYTBTBW4G1069020','LZYTBTBW4G1069017','LZYTBTBW0G1069015','LZYTBTBW3G1069302','LZYTBTBW7G1072249','LZYTBTBW5G1068328','LZYTBTBW3G1072250','LZYTBTBWXG1072407','LZYTBTBW8G1072406','LZYTBTBW6G1072405','LZYTBTBW4G1072404','LZYTBTBW1G1072408','LZYTBTBWXG1073895','LZYTBTBW9G1073614','LZYTBTBW8G1073894','LZYTBTBW9G1074665','LZYTBTBW7G1074664','LZYTBTBW4G1074668','LZYTBTBW8G1073619','LZYTBTBW7G1073613','LZYTBTBW6G1073893','LZYTBTBW6G1073618','LZYTBTBW5G1073612','LZYTBTBW4G1073892','LZYTBTBW4G1073617','LZYTBTBW3G1073611','LZYTBTBW1G1073896','LZYTBTBW0G1073615','LZYTBTBW9G1072723','LZYTBTBW7G1072722','LZYTBTBW7G1072719','LZYTBTBW5G1072721','LZYTBTBW5G1072718','LZYTBTBW4G1072726','LZYTBTBW3G1072720','LZYTBTBW3G1072717','LZYTBTBW2G1072725','LZYTBTBW0G1072724','LZYTDGAW5G1066291','LZYTBTBW9G1072074','LZYTBTBW5G1072072','LZYTBTBW0G1072075','LZYTBTBW5G1069950','LZYTBTBW3G1069025','LZYTBTBWXG1074271','LZYTBTBWXG1074268','LZYTBTBW8G1074270','LZYTBTBW8G1074267','LZYTBTBW1G1074272','LZYTBTBW0G1074666','LZYTBTBW1G1074269','LZYTBTBW9G1073905','LZYTBTBW6G1069956','LZYTBTBW4G1069955','LZYTBTBWXG1069054','LZYTBTBW8G1069053','LZYTBTBW6G1069052','LZYTBTBW4G1069051','LZYTBTBW2G1069050','LZYTBTBW1G1069055','LZYTBGBW9G1072647','LZYTBGBW9G1072650','LZYTBGBW4G1072653','LA9ACAAB4FALYX035','LZYTBGBW3G1072658','LZYTBGBW5G1072659','LZYTBGBW9G1072664','LZYTDGAW1G1058091','LZYTDGAW1G1058043','LZYTDGAW9G1067377','LZYTDGAW9G1067363','LZYTDGAW8G1067371','LZYTDGAW8G1067368','LZYTDGAW8G1067354','LZYTDGAW7G1067376','LZYTDGAW7G1067362','LZYTDGAW7G1067359','LZYTDGAW6G1067384','LZYTDGAW6G1067370','LZYTDGAW6G1067367','LZYTDGAW6G1067353','LZYTDGAW5G1067375','LZYTDGAW5G1067361','LZYTDGAW5G1067358','LZYTDGAW4G1067383','LZYTDGAW0G1058034','LZYTDGAW0G1058020','LZYTDGAW4G1067352','LZYTDGAW3G1067374','LZYTDGAW9G1067380','LZYTDGAW3G1067360','LZYTDGAW3G1067357','LZYTDGAW2G1067382','LZYTDGAW2G1067379','LZYTDGAW2G1067365','LZYTDGAW2G1067351','LZYTDGAW1G1067373','LZYTDGAW1G1067356','LZYTDGAW0G1067381','LZYTDGAW0G1067378','LZYTDGAW0G1067364','LZYTBTBW2G1069971','LZYTBTBW9G1069952','LZYTBTBW7G1069951','LZYTBTBW2G1069954','LZYTBTBW0G1069953','LZYTBTBWXG1068339','LZYTBTBW8G1068338','LZYTBTBW6G1068340','LZYTBTBW5G1068569','LZYTBTBWXG1074318','LZYTBTBW8G1074317','LZYTBTBW6G1074316','LZYTDGAW6G1073721','LZYTDGAW5G1073726','LZYTDGAW4G1073734','LZYTDGAW4G1073720','LZYTDGAW3G1073739','LZYTDGAW3G1073725','LZYTDGAW2G1073733','LZYTDGAW1G1073738','LZYTDGAW1G1073724','LZYTDGAW0G1073729','LZYTBTBW4G1074315','LZYTBTBW2G1074314','LZYTBTBW1G1074319','LZYTDGAWXG1067369','LZYTDGAWXG1067355','LZYTDGAWXG1073740','LZYTDGAWXG1073737','LZYTDGAW9G1073731','LZYTDGAW9G1073728','LZYTDGAW8G1073736','LZYTDGAW8G1073722','LZYTDGAW8G1073719','LZYTDGAW7G1073730','LZYTDGAW7G1073727','LZYTDGAW6G1073735','LZYTBTBW0G1071928','LZYTBTBW8G1068324','LZYTBTBW4G1067929','LZYTBTBWXG1073900','LZYTBTBW7G1073899','LZYTBTBW5G1073898','LZYTBTBW1G1073901','LZYTBTBW0G1069970','LZYTBTBW6G1072047','LZYTBTBW8G1069327','LZYTBTBW1G1069329','LZYTBTBW9G1072690','LZYTBTBW9G1072687','LZYTBTBW2G1072689','LZYTBTBW0G1072691','LZYTBTBW0G1072688','LZYTBTBW3G1069882','LZYTBTBWXG1069328','LZYTBTBW7G1069948','LZYTBTBW5G1069947','LZYTBTBW3G1069946','LZYTBTBW2G1069338','LZYTBTBW0G1069337','LZYTBTBWXG1072679','LZYTBTBW8G1072681','LZYTBTBW8G1072678','LZYTBTBW6G1072680','LZYTBTBW6G1072677','LZYTBTBW4G1072676','LZYTBTBW3G1072684','LZYTBTBW2G1072675','LZYTBTBW1G1072683','LZYTBTBW9G1067621','LZYTBTBWXG1072682','LZYTBTBWXG1073606','LZYTBTBW8G1073605','LZYTBTBW6G1073604','LZYTBTBW5G1073609','LZYTBTBW4G1073603','LZYTBTBW3G1073608','LZYTBTBW1G1073610','LZYTBTBW1G1073607','LZYTBTBW0G1073601','LZYTBTBW8G1072728','LZYTBTBWXG1063612','LZYTBTBW8G1063611','LZYTBTBW6G1063610','LZYTBTBW5G1063615','LZYTBTBW3G1063614','LZYTBTBW1G1063613','LZYTBTBW9G1073600','LZYTBTBW0G1072027','LZYTBTBW9G1070941','LZYTBTBW6G1070945','LZYTBTBW4G1070944','LZYTBTBW2G1070943','LZYTBTBWXG1071774','LZYTBTBW9G1071779','LZYTBTBW7G1071778','LZYTBTBW6G1071772','LZYTBTBW5G1072685','LZYTBTBW5G1071777','LZYTBTBW4G1071771','LZYTBTBW3G1071776','LZYTBTBW1G1071775','LZYTBTBW9G1069949','LZYTBGBW8G1071506','LZYTBGBW6G1071505','LZYTBGBW2G1071503','LZYTBGBW4G1071504','LZYTDGAWXG1064486','LZYTDGAWXG1064472','LZYTDGAWXG1064469','LZYTDGAW9G1064494','LZYTDGAW9G1064480','LZYTDGAW9G1064477','LZYTDGAW9G1064463','LZYTDGAW8G1064485','LZYTDGAW8G1064471','LZYTDGAW8G1064468','LZYTDGAW7G1064493','LZYTDGAW7G1064462','LZYTDGAW7G1064459','LZYTDGAW6G1064470','LZYTDGAW6G1064467','LZYTDGAW5G1064492','LZYTDGAW5G1064489','LZYTDGAW5G1064475','LZYTDGAW5G1064461','LZYTDGAW5G1064458','LZYTDGAW4G1064483','LZYTDGAW4G1064466','LZYTDGAW3G1064491','LZYTDGAW3G1064488','LZYTDGAW3G1064474','LZYTDGAW3G1064460','LZYTDGAW3G1064457','LZYTDGAW2G1064482','LZYTDGAW2G1064479','LZYTDGAW2G1064465','LZYTDGAW1G1064490','LZYTDGAW1G1064487','LZYTDGAW1G1064456','LZYTDGAW0G1064495','LZYTDGAW0G1064481','LZYTDGAW0G1064478','LZYTDGAW0G1064464','LZYTDGAWXG1058042','LZYTDGAWXG1058039','LZYTDGAWXG1058025','LZYTDGAW6G1064484','LZYTDGAW1G1064473','LZYTDGAW9G1058033','LZYTDGAW8G1058041','LZYTDGAW8G1058038','LZYTDGAW8G1058024','LZYTDGAW7G1058046','LZYTDGAW7G1058032','LZYTDGAW7G1058029','LZYTDGAW6G1058040','LZYTDGAW6G1058037','LZYTDGAW6G1058023','LZYTDGAW5G1058045','LZYTDGAW5G1058031','LZYTDGAW5G1058028','LZYTDGAW4G1058036','LZYTDGAW4G1058022','LZYTDGAW3G1058044','LZYTDGAW3G1058030','LZYTDGAW3G1058027','LZYTDGAW2G1058035','LZYTDGAW2G1058021','LVCB3L4D2GG013105','LVCB3L4D4GG013106','LVCB3L4D6GG013107','LVCB3L4D8GG013108','LVCB3L4DXGG013109','LVCB3L4D6GG013110','LVCB3L4DXGG013112','LVCB3L4D1GG013113','LVCB3L4D3GG013114','LZYTBTBWXG1064646','LZYTBTBW8G1064645','LZYTBTBW6G1064644','LZYTBTBW3G1064648','LZYTBTBW1G1064647','LZYTBTBW9G1067926','LZYTBTBW8G1067920','LZYTBTBW7G1067925','LZYTBTBW3G1067923','LZYTBTBW2G1067928','LZYTBTBW1G1067922','LZYTBTBW1G1067919','LZYTBTBW0G1067927','LZYTBTBW8G1072048','LZYTBTBW6G1072050','LZYTBTBW7G1063454','LVCB4L4D6GG031085','LVCB4L4D9GG031100','LVCB4L4D1GG031088','LVCB4L4D1GG031091','LVCB4L4D1GG031110','LVCB4L4D2GG031083','LVCB3L4D2FM009300','LA81F1HT3FA101203','LA81F1HT3GA100098','LA81F1HT8GA100792','LA81F1HT4GA101101','LZYTBTBW7G1039784','LZYTDGAW6G1074304','LZYTBGBW0G1069300','LZYTDGAW8G1069492','LZYTBTBW9G1039768','LA81F1HT7GA100797','LA81F1HT7GA100802','LA81F1HT4GA100496','LZYTDGAW7G1066678','LZYTDGAWXG1069428','LZYTBTBW7G1031779','LA81F1HT5FA101008','LVCB3L4D3GM002146','LA81F1HT3GA101123','LA81F1HT2GA101128','LVCB4L4D7GM001871','LA81F1HT8FA101665','LA81F1HT3FA101671','LA81F1HT0GA100771','LVCB3L4D7GM001629','LVCB4L4DXGM001282','LA81F1HT6GA100161','LA81F1HT6FA101437','LA81F1HT8FA101441','LA81F1HT2FA101614','LA81F1HT6FA101650','LVCB3L4DXGM002130','LVCB4L4D9GM001242','LA81F1HT8GA100128','LVCB4L4D8GM001670','LA9CA8N0XGBBFC489','LA9CA8N05GBBFC478','LVCB3L4D4GM001636','LVCB4L4D8GM001233','LA81F1HT3GA100571','LA81F1HT0FA101224','LA81F1HT7GA100041','LVCB4L4D0GM001887','LVCB4L4D7GM001840','LVCB4L4D3GM001267','LA81F1HT8GA100551','LZYTDGAW3G1064913','LZYTDGAW0G1042531','LZYTBTBW7G1063843','LA81F1HT8FA101004','LA81F1HT0FA101157','LVCB4L4D4FG016762','LZYTBTBW9G1018371','LVCB4L4D0GM001713','LA81F1HT5GA100572','LVCB4L4D8GM001717','LZYTBTBW3G1072412','LA81F1HT9GA100817','LZYTBTBW8G1031791','LZYTBTBW2G1074264','LA81F1HT0FA101031','LA81F1HT4FA101338','LA81F1HT4FA101193','LZYTBTBW5G1067924','LZYTBTBW5G1072055','LZYTDGAW7G1069452','LZYTDGAW3G1069495','LZYTDGAWXG1064794','LZYTDGAW2G1064806','LZYTDGAW6G1069412','LZYTDGAW1G1074291','LZYTDGAW0G1064433','LZYTDGAW1G1069513','LZYTDGAW6G1064825','LZYTBGBW5G1067915','LZYTBTBW3G1068120','LA81F1HT8FA101312','LVCB4L4D2GM001938','LA81F1HT7GA100167','LA81F1HTXGA100129','LZYTDGAW3G1070713','LA81F1HT9FA101044','LA81F1HT4GA101115','LVCB4L4D1GM003793','LA9ACAAB9FALYX080','LVCB3L4D4GM002124','LVCB4L4D0GG012905','LZYTBTBW2G1071932','LZYTDGAW3G1073305','LVCB3L4D2GG012830','LZYTDGAW9G1066164','LZYTDGAW9G1048697','LVCB4L4D0GM001663','LVCB3L4D3GG012822','LZYTBTBW2G1073616','LZYTBTBW2G1074667','LZYTBTBW2G1068089','LVCB4L4D6GM001697','LA9ACAAB5FALYX058','LA9ACAAB0FALYX002','LVCB3L4D6FM009316','LZYTBTBW7G1069965','LZYTBTBW3G1068568','LZYTDGAW0G1073732','LZYTBTBW5G1072346','LZYTBGBW1G1072660','LZYTBTBW5G1073593','LZYTDGAW5G1048681','LZYTDGAW3G1048713','LZYTDGAW1G1048709','LZYTDGAW8G1061649','LVCB4L4D6FG016519','LZYTDGAWXG1073723','LZYTBTBW2G1073602','LZYTDGAW8G1072571','LZYTBTBW4G1031769','LZYTDGAW5G1072611','LZYTBTBW0G1067622','LZYTBTBWXG1067921','LVCB3L4D2GG012827','LZYTDGAW1G1072525','LZYTDGAWXG1048725','LZYTDGAW7G1064476','LZYTBTBW3G1068134','LA9CA8N05GBBFC416','LA9CA8N04GBBFC438','LA9CA8N01GBBFC428','LA9CA8N07GBBFC434','LA9CA8N01GBBFC400','LA9CA8N02GBBFC499','LA9CA8N06GBBFC456','LA9CA8N00GBBFC422','LVCB3L4D8GM001980','LZYTDGAW0G1072502','LA9CA8N08GBBFC491','LA9CA8N0XGBBFC492','LA9CA8N05GBBFC481','LA9CA8N08GBBFC474','LA9CA8N0XGBBFC444','LA9CA8N0XGBBFC413','LA9CA8N09GBBFC452','LA9CA8N03GBBFC477','LA9CA8N01GBBFC459','LA9CA8N02GBBFC468','LZYTDGAWXG1067372','LVCB4L4D0GG011320','LZYTDGAW7G1064798','LVCB4L4D7GM001675','LZYTDGAW0G1069406','LVCB3L4D3GG012853','LVCB4L4D4FG016826','LVCB4L4D4GM001830','LZYTDGAW8G1068780','LZYTBTBW3G1068327','LVCB4L4D7GM001661','LZYTDGAW4G1064337','LZYTDGAW7G1071606','LVCB3L4D1FM009322','LVCB4L4DXGM001668','LVCB4L4D1GG011570','LVCB4L4DXGG012880','LVCB4L4D0FG016810','LZYTBTBWXG1064632','LZYTDGAW2G1064448','LZYTBTBW9G1031752','LZYTBTBW3G1018401','LZYTDGAW8G1069394','LZYTDGAW7G1072478'
            )
          """.stripMargin).as[String].rdd.saveAsTextFile("/tmp/haha");


    sparkContext.stop()


  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.MatrixFactorizationModel
import org.apache.spark.mllib.recommendation.Rating
// $example off$

object RecommendationExample {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("CollaborativeFilteringExample")
    val sc = new SparkContext(conf)
    // $example on$
    // Load and parse the data
    val data = sc.textFile("data/mllib/als/test.data")
    val ratings = data.map(_.split(',') match { case Array(user, item, rate) =>
      Rating(user.toInt, item.toInt, rate.toDouble)
    })

    // Build the recommendation model using ALS
    val rank = 10
    val numIterations = 10
    val model = ALS.train(ratings, rank, numIterations, 0.01)

    // Evaluate the model on rating data
    val usersProducts = ratings.map { case Rating(user, product, rate) =>
      (user, product)
    }
    val predictions =
      model.predict(usersProducts).map { case Rating(user, product, rate) =>
        ((user, product), rate)
      }
    val ratesAndPreds = ratings.map { case Rating(user, product, rate) =>
      ((user, product), rate)
    }.join(predictions)
    val MSE = ratesAndPreds.map { case ((user, product), (r1, r2)) =>
      val err = (r1 - r2)
      err * err
    }.mean()
    println("Mean Squared Error = " + MSE)

    // Save and load model
    model.save(sc, "target/tmp/myCollaborativeFilter")
    val sameModel = MatrixFactorizationModel.load(sc, "target/tmp/myCollaborativeFilter")
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import java.io.File
import java.nio.charset.Charset

import com.google.common.io.Files

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.{Seconds, StreamingContext, Time}
import org.apache.spark.util.{IntParam, LongAccumulator}

/**
 * Use this singleton to get or register a Broadcast variable.
 */
object WordBlacklist {

  @volatile private var instance: Broadcast[Seq[String]] = null

  def getInstance(sc: SparkContext): Broadcast[Seq[String]] = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          val wordBlacklist = Seq("a", "b", "c")
          instance = sc.broadcast(wordBlacklist)
        }
      }
    }
    instance
  }
}

/**
 * Use this singleton to get or register an Accumulator.
 */
object DroppedWordsCounter {

  @volatile private var instance: LongAccumulator = null

  def getInstance(sc: SparkContext): LongAccumulator = {
    if (instance == null) {
      synchronized {
        if (instance == null) {
          instance = sc.longAccumulator("WordsInBlacklistCounter")
        }
      }
    }
    instance
  }
}

/**
 * Counts words in text encoded with UTF8 received from the network every second. This example also
 * shows how to use lazily instantiated singleton instances for Accumulator and Broadcast so that
 * they can be registered on driver failures.
 *
 * Usage: RecoverableNetworkWordCount <hostname> <port> <checkpoint-directory> <output-file>
 *   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
 *   data. <checkpoint-directory> directory to HDFS-compatible file system which checkpoint data
 *   <output-file> file to which the word counts will be appended
 *
 * <checkpoint-directory> and <output-file> must be absolute paths
 *
 * To run this on your local machine, you need to first run a Netcat server
 *
 *      `$ nc -lk 9999`
 *
 * and run the example as
 *
 *      `$ ./bin/run-example org.apache.spark.examples.streaming.RecoverableNetworkWordCount \
 *              localhost 9999 ~/checkpoint/ ~/out`
 *
 * If the directory ~/checkpoint/ does not exist (e.g. running for the first time), it will create
 * a new StreamingContext (will print "Creating new context" to the console). Otherwise, if
 * checkpoint data exists in ~/checkpoint/, then it will create StreamingContext from
 * the checkpoint data.
 *
 * Refer to the online documentation for more details.
 */
object RecoverableNetworkWordCount {

  def createContext(ip: String, port: Int, outputPath: String, checkpointDirectory: String)
    : StreamingContext = {

    // If you do not see this printed, that means the StreamingContext has been loaded
    // from the new checkpoint
    println("Creating new context")
    val outputFile = new File(outputPath)
    if (outputFile.exists()) outputFile.delete()
    val sparkConf = new SparkConf().setAppName("RecoverableNetworkWordCount")
    // Create the context with a 1 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    ssc.checkpoint(checkpointDirectory)

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    val lines = ssc.socketTextStream(ip, port)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map((_, 1)).reduceByKey(_ + _)
    wordCounts.foreachRDD { (rdd: RDD[(String, Int)], time: Time) =>
      // Get or register the blacklist Broadcast
      val blacklist = WordBlacklist.getInstance(rdd.sparkContext)
      // Get or register the droppedWordsCounter Accumulator
      val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext)
      // Use blacklist to drop words and use droppedWordsCounter to count them
      val counts = rdd.filter { case (word, count) =>
        if (blacklist.value.contains(word)) {
          droppedWordsCounter.add(count)
          false
        } else {
          true
        }
      }.collect().mkString("[", ", ", "]")
      val output = "Counts at time " + time + " " + counts
      println(output)
      println("Dropped " + droppedWordsCounter.value + " word(s) totally")
      println("Appending to " + outputFile.getAbsolutePath)
      Files.append(output + "\n", outputFile, Charset.defaultCharset())
    }
    ssc
  }

  def main(args: Array[String]) {
    if (args.length != 4) {
      System.err.println("Your arguments were " + args.mkString("[", ", ", "]"))
      System.err.println(
        """
          |Usage: RecoverableNetworkWordCount <hostname> <port> <checkpoint-directory>
          |     <output-file>. <hostname> and <port> describe the TCP server that Spark
          |     Streaming would connect to receive data. <checkpoint-directory> directory to
          |     HDFS-compatible file system which checkpoint data <output-file> file to which the
          |     word counts will be appended
          |
          |In local mode, <master> should be 'local[n]' with n > 1
          |Both <checkpoint-directory> and <output-file> must be absolute paths
        """.stripMargin
      )
      System.exit(1)
    }
    val Array(ip, IntParam(port), checkpointDirectory, outputPath) = args
    val ssc = StreamingContext.getOrCreate(checkpointDirectory,
      () => createContext(ip, port, outputPath, checkpointDirectory))
    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
package com.bitnei.report.util

/**
  *
  * @author zhangyongtian
  * @define
  * create 2018-02-10 11:10
  */

import com.esotericsoftware.kryo.io.{Input, Output}
import com.esotericsoftware.kryo.{Kryo, KryoSerializable}
import org.apache.commons.pool2.impl.GenericObjectPoolConfig
import org.apache.spark.serializer.KryoSerializer
import redis.clients.jedis.Jedis
import redis.clients.jedis.JedisPool

import org.apache.commons.pool2.impl.GenericObjectPoolConfig
import redis.clients.jedis.Jedis
import redis.clients.jedis.JedisPool

object RedisClient {
  private var jedisPool: JedisPool = null

  class CleanWorkThread extends Thread {
    override def run(): Unit = {
      System.out.println("Destroy jedis pool")
      if (null != jedisPool) {
        jedisPool.destroy()
        jedisPool = null
      }
    }
  }

}

class RedisClient() extends KryoSerializable {

  Runtime.getRuntime.addShutdownHook(new RedisClient.CleanWorkThread)
  var jedisPool: JedisPool = null
  var host: String = null

  def this(host: String) {
    this()
    this.host = host
    Runtime.getRuntime.addShutdownHook(new RedisClient.CleanWorkThread)
    RedisClient.jedisPool = new JedisPool(new GenericObjectPoolConfig, host)
  }

  def getResource: Jedis = RedisClient.jedisPool.getResource

  def returnResource(jedis: Jedis): Unit = {
    RedisClient.jedisPool.returnResource(jedis)
  }

  def write(kryo: Kryo, output: Output): Unit = {
    kryo.writeObject(output, host)
  }

  def read(kryo: Kryo, input: Input): Unit = {
    host = kryo.readObject(input, classOf[String])
    this.jedisPool = new JedisPool(new GenericObjectPoolConfig, host)
  }
}package com.bitnei.report.util.redis

import java.util.concurrent.locks.ReentrantLock

import org.apache.log4j.Logger
import redis.clients.jedis.{Jedis, JedisPool, JedisPoolConfig}

class RedisPool(config: JedisPoolConfig, host: String, port: Int) extends Serializable {

  protected var lockPool = new ReentrantLock
  protected var lockJedis = new ReentrantLock

  protected var logger: Logger = Logger.getLogger(classOf[JedisUtil])

  private var jedisPool: JedisPool = null

  /**
    * redis过期时间,以秒为单位
    */
  val EXRP_HOUR: Int = 60 * 60 //一小时

  val EXRP_DAY: Int = 60 * 60 * 24 //一天

  val EXRP_MONTH: Int = 60 * 60 * 24 * 30 //一个月


  private def  initialPool(): Unit = {
    this.synchronized {
      try {
        jedisPool = new JedisPool(config, host, port)
      } catch {
        case e: Exception =>
          logger.error("First create JedisPool error : " + e)
      }
    }
  }

  private def poolInit(): Unit = {
    if (jedisPool == null) initialPool()
  }


  def getJedis: Jedis = {
    if (jedisPool == null) poolInit()
    var jedis: Jedis = null
    try
        if (jedisPool != null) jedis = jedisPool.getResource
    catch {
      case e: Exception =>
        logger.error("Get jedis error : " + e)
    } finally returnResource(jedis)
    jedis
  }

  def returnResource(jedis: Jedis): Unit = {
    if (jedis != null && jedisPool != null) jedisPool.close()
  }
}
package com.bitnei.report.common.utils

import com.bitnei.redisclient.RedisClient
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging


/**
  * Created by wangbaosheng on 2017/5/26.
  */
class  RedisUtil(stateConf:StateConf) extends  Logging {

  val client = new RedisClient
  val redisAddress=stateConf.getString("redis.address")
  logInfo(s"the redis address is $redisAddress")
  if (!client.init(redisAddress)) {
    logError("tht redis client init failed!!!")
    throw new RuntimeException("the redis client init failed!!!")
  }

  def get(vid: String, fields: String*): Array[String] = {
    client.get(vid, fields.toArray)
  }

  def get(vid: String, field: String): Option[String] = {
    val v = client.get(vid, Array(field)).head
    if (v == null||v.trim.isEmpty) None else Some(v)
  }

  def close(): Unit = {
    logInfo("begining close redis connection")
    client.close()
  }

  def main(args: Array[String]): Unit = {
    val client = new RedisClient

    val address="192.168.2.79" + ":" + "6379"
    if (!client.init("192.168.1.54:6379")) {
      logError("tht redis client init failed!!!")
      throw new RuntimeException("the redis client init failed!!!")
    }

    println(client.get("27E2ADD691D87EB1E0533C02A8C0B094", Array("10002", "10005")))
  }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
// scalastyle:off println

package org.apache.spark.examples.mllib

// $example on$
import org.apache.spark.mllib.evaluation.RegressionMetrics
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}
// $example off$
import org.apache.spark.sql.SparkSession

@deprecated("Use ml.regression.LinearRegression and the resulting model summary for metrics",
  "2.0.0")
object RegressionMetricsExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("RegressionMetricsExample")
      .getOrCreate()
    // $example on$
    // Load the data
    val data = spark
      .read.format("libsvm").load("data/mllib/sample_linear_regression_data.txt")
      .rdd.map(row => LabeledPoint(row.getDouble(0), row.get(1).asInstanceOf[Vector]))
      .cache()

    // Build the model
    val numIterations = 100
    val model = LinearRegressionWithSGD.train(data, numIterations)

    // Get predictions
    val valuesAndPreds = data.map{ point =>
      val prediction = model.predict(point.features)
      (prediction, point.label)
    }

    // Instantiate metrics object
    val metrics = new RegressionMetrics(valuesAndPreds)

    // Squared error
    println(s"MSE = ${metrics.meanSquaredError}")
    println(s"RMSE = ${metrics.rootMeanSquaredError}")

    // R-squared
    println(s"R-squared = ${metrics.r2}")

    // Mean absolute error
    println(s"MAE = ${metrics.meanAbsoluteError}")

    // Explained variance
    println(s"Explained variance = ${metrics.explainedVariance}")
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println

package com.bitnei.report.detail

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.TimeParser
import com.bitnei.report.stateGenerate.{ChargeChangeGenerator, StateGeneratorBase}
/**
  * Created by wangbaosheng on 2017/5/9.
  */
trait ReportDetailCompute{
  def compute(stateConf: StateConf,sortedRows:Seq[RealinfoModel]):List[DetailModel]
}


//明细计算基础类
class DetailComputeBase extends ReportDetailCompute {
  override def compute(stateConf: StateConf, sortedRows: Seq[RealinfoModel]): List[DetailModel] = {
    //状态划分
    val windows = stateConf.getString("chargeChange.enable") match {
      case "true" =>
        val stateGenerator = new ChargeChangeGenerator(stateConf) {
          override type T = RealinfoModel

          override def getVid = (row: RealinfoModel) => row.vid

          override def getTime = row => row.time

          override def getCharge = row => row.charge.getOrElse(0)

          override def getSoc = row => row.soc.getOrElse(0)

          override def getSpeed = row => row.speed.getOrElse(0)

          override def getMileage = row => row.mileage.getOrElse(0)
        }


        val windows = stateGenerator.handle(sortedRows)

        windows

      case _ =>
        val stateGenerator = new StateGeneratorBase(stateConf) {
          override type T = RealinfoModel

          override def getVid = (row: RealinfoModel) => row.vid

          override def getTime = row => row.time

          override def getCharge = row => row.charge.getOrElse(0)

          override def getSoc = row => row.soc.getOrElse(0)

          override def getSpeed = row => row.speed.getOrElse(0)

          override def getMileage = row => row.mileage.getOrElse(0)
        }


        val windows = stateGenerator.handle(sortedRows.filter(_.soc.nonEmpty))
        windows
    }


    //明细计算
    DetailCompute.computeAllStates(stateConf, windows)
  }
}

package com.bitnei.report

import com.bitnei.report.common.configuration.StateConf
import org.apache.spark.sql.{DataFrame, SparkSession}

import scala.collection.mutable.ArrayBuffer

trait ReportOutput {
  //type RESULT

  def output():Unit
}

//class HdfsOutput(sparkSession: SparkSession,stateConf: StateConf,result:DataFrame) extends ReportOutput {
//  override def output(): Unit = {
//
//  }
//}
//
//class RelationDbOutput(sparkSession: SparkSession,stateConf: StateConf,result:DataFrame) extends ReportOutput{
//  override def output(): Unit = {
//    result.foreachPartition(partition=>{
//      doOutput(partition)
//    })
//  }
//
//  def doOutput
//}
class ReportOutputManager(stateConf: StateConf) {
  val outputList = new ArrayBuffer[ReportOutput]()

  def registe(output: ReportOutput): Unit = {
    outputList.append(output)
  }


  def output(): Unit = {
    outputList.foreach(output => {
      output.output
    })
  }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.RFormula
// $example off$
import org.apache.spark.sql.SparkSession

object RFormulaExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("RFormulaExample")
      .getOrCreate()

    // $example on$
    val dataset = spark.createDataFrame(Seq(
      (7, "US", 18, 1.0),
      (8, "CA", 12, 0.0),
      (9, "NZ", 15, 0.0)
    )).toDF("id", "country", "hour", "clicked")

    val formula = new RFormula()
      .setFormula("clicked ~ country + hour")
      .setFeaturesCol("features")
      .setLabelCol("label")

    val output = formula.fit(dataset).transform(dataset)
    output.select("features", "label").show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.detail

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql.{Dataset, SparkSession}

/*
* created by wangbaosheng on 2017/11/2
* 充电行驶输出作业，将充电行驶数据输出到数据库
*/
class RunChargeOutputJob(stateConf: StateConf, sparkSession: SparkSession) extends Serializable with Logging with Job {
  override type R = ChargeRun
  private val sqlContext = sparkSession.sqlContext

  import sqlContext.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("chargeRun")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("detail")

  override def registerIfNeed() = SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)

  override def unRegister() = sparkSession.catalog.dropTempView(inputTableName)


  override def doCompute[Product <: ChargeRun]() = sqlContext.sql(s"SELECT * FROM $inputTableName ${SqlHelper.buildWhere(stateConf)}").as[ChargeRun]


  override def write[Product <: ChargeRun](result: Dataset[ChargeRun]) = {
    val outputPaths = stateConf.getString("report.output").split(',')
    outputPaths.foreach({
      case e if e == "oracle" || e == "mysql" =>
        //输出到数据库
        stateConf.set("database", e)
        result.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(10)).foreachPartition(par => {
          val output = new DetailRunChargeOutput(stateConf)
          output.output(par.toIterable)
        })
      case e =>
    })
  }
}

object RunChargeOutputJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    new RunChargeOutputJob(stateConf, SparkHelper.getSparkSession(sparkMaster = None)).compute()
  }
}package com.bitnei.alarm

import java.text.SimpleDateFormat

import com.bitnei.alarm.One2manyTerminal.RunStateInput
import com.bitnei.alarm.generator.{ParkInfoGenerator, RunStateOrbitSplitGenerator}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.util.{NumberUtils, TimeUtils}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.collection.mutable.ArrayBuffer

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-01-31 11:43
  *
  */
object RunstateGeneratorTest extends Serializable with Logging {

  case class Input(vid: String, vin: String, time: String, speed: Double, mileage: Double, soc: String, isstart: String, lon: Double, lat: Double)

  case class Output(vin: String, date: String, startTime: String, stopTime: String, lon: Double, lat: Double)

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    var date = "20171102"

    //参数校验
    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)


    //输出参数
    var outputTargets = stateConf.getOption("output").getOrElse("console")
    var outFormat = stateConf.getOption("output.format").getOrElse("#")
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"



    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
    ////////////////////////////////////////////////

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.parquet("data/realinfo/*.parquet").createOrReplaceTempView("realinfo")
        sparkSession.read.json("data/realinfo/mock.txt").createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")
      }

    }

    ////////////////////////////////////////业务逻辑//////////////////////////////


    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'

    val sql =
      s"""
        SELECT
        VID,
        VIN,
        `2000` AS TIME,
        `2201` AS speed,
        `2202` As mileage,
        `2615` As soc,
        `3201` As isstart,
        cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,
        cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat
        FROM realinfo
        where VID is not null and `2000` like '${date}%'  and `2201` is not null and `2202` is not null  and `2615` is not null  and `3201` is not null  and `2502` is not null and `2503` is not null
      """.stripMargin


    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String, String, String, String)]

    //    initDS.show(false)
    //    if (env.equals("prd")) {
    //      initDS = sparkSession.sql(s"SELECT VID,VIN,`2000` AS TIME,cast(`2502`/1000000 AS DECIMAL(9, 6)) AS lon,cast(`2503`/1000000 AS DECIMAL(9, 6)) AS lat FROM realinfo where VID is not null  and `2000` is not null and `2502` is not null and `2503` is not null and year='${year}' and month='${month}' and day='${day}'").as[(String, String, String, String, String)]
    //    }

    //    logInfo("源数据量：" + initDS.count())

    //    year='${year}' and month='${month}' and day='${day}'

    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        val lon = x._8
        val lat = x._9
        val cond01 = (lon != null && lon.nonEmpty && lat != null && lat.nonEmpty)
        var cond02 = false
        if (cond01) {
          val lonD = lon.toDouble
          val latD = lat.toDouble
          //在中国境内
          cond02 = (lonD <= 136 && lonD >= 73 && latD <= 54 && latD >= 3)
        }
        val speed = x._4.toString.toDouble
        val mileage = x._5.toString.toDouble
        val soc = x._6
        cond02 && speed > 0 && mileage > 0
      })


    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val vin = x._2
          val time = x._3
          val speed = x._4.toString.toDouble / 10
          val mileage = x._5.toString.toDouble / 10
          val soc = x._6
          val isstart = x._7 //车辆状态 1.启动 2.熄火
          val lon = x._8.toDouble
          val lat = x._9.toDouble

          Input(vid, vin, time, speed, mileage, soc, isstart, lon, lat)
        })


    val result = mappedDS
      .groupByKey(_.vid)
      .flatMapGroups {
        case (vid, inputs) => {
          val arr = inputs.toArray.sortBy(_.time)
          val vin = arr.head.vin

          //TODO:轨迹切分
          val runstateInputs = arr.map(x => RunStateInput(x.vid, x.vin, x.time, x.speed, x.mileage, x.soc.toDouble, x.isstart.toInt, x.lon, x.lat))
          val runPoints = RunStateOrbitSplitGenerator.handle(runstateInputs)

          runPoints.foreach(x=>x.foreach(y=>{
            println(y.speed)
          }
          ))
          runPoints
        }
      }




    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
    }

    if (env.equals("prd")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      if (outputTargets.contains("hdfs")) {

        result.write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }


    }

    sparkSession.stop()
  }


}
package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.dayreport.Model.{RunStateModel, RunStateModelManager}
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/10/22.
  */
class RunStateModelTest extends FunSuite{
  val stateConf=new StateConf

  test("insert to travel state model test"){

    val values=Array(
      new RunStateModel(
        id = 0,
        vid = "728a83e0-d13f-4264-811b-2b4d7e6ba744",
        start ="2016-10-22 11:11:11",
        end ="2016-10-22 11:11:11",
        time_dif = 1,
        mileage_upper =1,
        mileage_lower =1,
        max_speed = 1,
        max_total_voltage = 1,
        min_total_voltage = 1,
        max_total_current = 1,
        min_total_current = 1,
        max_secondary_volatage = 1,
        min_secondary_volatage = 1,
        max_acquisition_point_temp = 1,
        min_acquisition_point_temp = 1,
        max_engine_temp =1,
        min_engine_temp =1,
        max_soc = 1,
        min_soc = 1
      ),
      new RunStateModel(
        id = 0,
        vid = "728a83e0-d13f-4264-811b-2b4d7e6ba745",
        start ="2016-10-22 11:11:11",
        end ="2016-10-22 11:11:11",
        time_dif = 1,
        mileage_upper =1,
        mileage_lower =1,
        max_speed = 1,
        max_total_voltage = 1,
        min_total_voltage = 1,
        max_total_current = 1,
        min_total_current = 1,
        max_secondary_volatage = 1,
        min_secondary_volatage = 1,
        max_acquisition_point_temp = 1,
        min_acquisition_point_temp = 1,
        max_engine_temp =1,
        min_engine_temp =1,
        max_soc = 1,
        min_soc = 1
      )
    )


    new RunStateModelManager(stateConf).output(values)
   // new TravelStateModelManager(stateConf).delete(values(0).vid)
    //new TravelStateModelManager(stateConf).delete(values(1).vid)
  }
}
package com.bitnei.alarm.generator

import com.bitnei.common.constants.Constants

import scala.annotation.tailrec
import scala.collection.mutable.{ArrayBuffer, ListBuffer}

/**
  *
  * @author zhangyongtian
  * @define 车辆轨迹切分算法 -行驶状态
  *
  *                  行驶状态开始判断：车辆上线连续10帧不为充电状态的数据为车辆行驶。
  *
  *                  行驶状态结束判断：判断是否车辆下线或车辆转换为充电时为一次行驶结束。
  *
  *                  3201 车辆状态 启动状态  0x01：车辆启动状态；0x02：熄火；0x03：其他状态；“0xFE”表示异常，“0xFF”表示无效 （只有国标的有）
  *                  速度>0
  *
  *                  开始：连续10帧
  *                  车辆为启动状态 车速>0
  *                  结束：
  *                  create 2018-03-21 13:32
  */

case class RunStateInput(vid: String, vin: String, time: String, speed: Double, mileage: Double, soc: Double, isstart: Int)


object RunStateOrbitSplitGenerator extends Serializable{

  //TODO:按行驶状态切分
  /**
    * 核心处理方法
    * @param realinfos 单车在一定时间段内的实时报文数据
    * @return 多个行驶片段
    */
  def handle(realinfos: Array[RunStateInput]): List[List[RunStateInput]] = {

    val res = new ListBuffer[List[RunStateInput]]()

    if (realinfos.isEmpty) {
      println("该车辆轨迹为空")
      return res.toList
    }

    val vin = realinfos.head.vin

    //按时间排序
    val sortedArr = realinfos.sortBy(_.time)

    //连续帧数
    var count = 0
    var startIndex = 0
    var stopIndex = 0

    @tailrec
    def handleTailRec(curIndex: Int): Unit = {

      if (curIndex < sortedArr.length) {
        //车辆启动状态
        //判断 sortedArr(curIndex).isstart == 1 && sortedArr(curIndex).speed > 0   0.5
        if (sortedArr(curIndex).isstart == Constants.Vehicle_State_RUNNING) {
          count = count + 1
        } else {
          //不符合
          if (count >= 10 && curIndex > startIndex && ((sortedArr(curIndex).isstart == Constants.Vehicle_State_MISFIRE || (curIndex == (sortedArr.length - 1))))) {

            val output = sortedArr.slice(startIndex, curIndex)

            //            val output = RunStateOutput(vin, startTime, stopTime, startLon, startLat, stopLon, startLat, startMileage, stopMileage, startSoc, stopSoc)
            res.append(output.toList)
          }
          //从新开始
          count = 0
          startIndex = curIndex
        }

        handleTailRec(curIndex + 1)

      } else {
        return
      }
    }

    handleTailRec(0)
    res.toList
  }


}



//  case class RunStateOutput(vin: String,
//                            startTime: String, stopTime: String,
//                            startLon: Double, startLat: Double, stopLon: Double, stopLat: Double,
//                            startMileage: Double, stopMileage: Double,
//                            startSoc: Double, stopSoc: Double)


//            val a = sortedArr(startIndex)
//            val b = sortedArr(stopIndex)

//            val startTime: String = a.time
//            val stopTime: String = b.time
//
//            val startLon: Double = a.lon
//            val startLat: Double = a.lat
//
//            val stopLon: Double = b.lon
//            val stopLat: Double = b.lat
//
//            val startMileage: Double = a.mileage
//            val stopMileage: Double = b.mileage
//
//            val startSoc: Double = a.soc
//            val stopSoc: Double = b.socpackage com.bitnei.report.detail.distribution



class RunTimeLengthDistribution{
  private val distributed=Array.fill(12)(0)

  def add(timeRange:Int): Unit = {
    val travelTimeLength:Double= timeRange.toDouble/(1000*3600)


    if(0<=travelTimeLength&&travelTimeLength<=1.00D){
      distributed(0)+=1
    }else if(1.00D<travelTimeLength&&travelTimeLength<=2.00D){
      distributed(1)+=1
    }else if(2.00D<travelTimeLength&&travelTimeLength<=3.00D){
      distributed(2)+=1
    }else if(3.00D<travelTimeLength&&travelTimeLength<=4.00D){
      distributed(3)+=1
    }else if(4.00D<travelTimeLength&&travelTimeLength<=5.00D){
      distributed(4)+=1
    }else if(5.00D<travelTimeLength&&travelTimeLength<=6.00D){
      distributed(5)+=1
    }else if(6.00D<travelTimeLength&&travelTimeLength<=7.00D){
      distributed(6)+=1
    }else if(7.00D<travelTimeLength&&travelTimeLength<=8.00D){
      distributed(7)+=1
    }else if(8.00D<travelTimeLength&&travelTimeLength<=9.00D){
      distributed(8)+=1
    }else if(9.00D<travelTimeLength&&travelTimeLength<=10.00D){
      distributed(9)+=1
    }else if(10.00D<travelTimeLength) {
      distributed(11) += 1
    }
  }


  def getDistribution: Array[Int] = distributed
}
object RunTimeLengthDistribution{
  def default: Array[Int] = Array.fill(12)(0)
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.util.MLUtils

/**
 * An example app for randomly generated and sampled RDDs. Run with
 * {{{
 * bin/run-example org.apache.spark.examples.mllib.SampledRDDs
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object SampledRDDs {

  case class Params(input: String = "data/mllib/sample_binary_classification_data.txt")
    extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("SampledRDDs") {
      head("SampledRDDs: an example app for randomly generated and sampled RDDs.")
      opt[String]("input")
        .text(s"Input path to labeled examples in LIBSVM format, default: ${defaultParams.input}")
        .action((x, c) => c.copy(input = x))
      note(
        """
        |For example, the following command runs this app:
        |
        | bin/spark-submit --class org.apache.spark.examples.mllib.SampledRDDs \
        |  examples/target/scala-*/spark-examples-*.jar
        """.stripMargin)
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"SampledRDDs with $params")
    val sc = new SparkContext(conf)

    val fraction = 0.1 // fraction of data to sample

    val examples = MLUtils.loadLibSVMFile(sc, params.input)
    val numExamples = examples.count()
    if (numExamples == 0) {
      throw new RuntimeException("Error: Data file had no samples to load.")
    }
    println(s"Loaded data with $numExamples examples from file: ${params.input}")

    // Example: RDD.sample() and RDD.takeSample()
    val expectedSampleSize = (numExamples * fraction).toInt
    println(s"Sampling RDD using fraction $fraction.  Expected sample size = $expectedSampleSize.")
    val sampledRDD = examples.sample(withReplacement = true, fraction = fraction)
    println(s"  RDD.sample(): sample has ${sampledRDD.count()} examples")
    val sampledArray = examples.takeSample(withReplacement = true, num = expectedSampleSize)
    println(s"  RDD.takeSample(): sample has ${sampledArray.length} examples")

    println()

    // Example: RDD.sampleByKey() and RDD.sampleByKeyExact()
    val keyedRDD = examples.map { lp => (lp.label.toInt, lp.features) }
    println(s"  Keyed data using label (Int) as key ==> Orig")
    //  Count examples per label in original data.
    val keyCounts = keyedRDD.countByKey()

    //  Subsample, and count examples per label in sampled data. (approximate)
    val fractions = keyCounts.keys.map((_, fraction)).toMap
    val sampledByKeyRDD = keyedRDD.sampleByKey(withReplacement = true, fractions = fractions)
    val keyCountsB = sampledByKeyRDD.countByKey()
    val sizeB = keyCountsB.values.sum
    println(s"  Sampled $sizeB examples using approximate stratified sampling (by label)." +
      " ==> Approx Sample")

    //  Subsample, and count examples per label in sampled data. (approximate)
    val sampledByKeyRDDExact =
      keyedRDD.sampleByKeyExact(withReplacement = true, fractions = fractions)
    val keyCountsBExact = sampledByKeyRDDExact.countByKey()
    val sizeBExact = keyCountsBExact.values.sum
    println(s"  Sampled $sizeBExact examples using exact stratified sampling (by label)." +
      " ==> Exact Sample")

    //  Compare samples
    println(s"   \tFractions of examples with key")
    println(s"Key\tOrig\tApprox Sample\tExact Sample")
    keyCounts.keys.toSeq.sorted.foreach { key =>
      val origFrac = keyCounts(key) / numExamples.toDouble
      val approxFrac = if (sizeB != 0) {
        keyCountsB.getOrElse(key, 0L) / sizeB.toDouble
      } else {
        0
      }
      val exactFrac = if (sizeBExact != 0) {
        keyCountsBExact.getOrElse(key, 0L) / sizeBExact.toDouble
      } else {
        0
      }
      println(s"$key\t$origFrac\t$approxFrac\t$exactFrac")
    }

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report

import scala.util.control.Breaks.{break, breakable}

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-11-24 9:59
  *
  */
object ScalaTest {

  def main(args: Array[String]): Unit = {
    val arr = Array(1, 2, 3, 4, 5)

    var flag = 0

    for (k <- Range(0, 100)) {
      breakable(
        for (i <- 0 until arr.length if i >= flag) {
          println(arr(i))
          flag = 2
          break()
        }
      )
    }
  }

}
package com.bitnei.samples.sca.typ

/**
  *
  * @author zhangyongtian
  * @define 类型参数化
  *
  * create 2017-12-04 14:37
  *
  */
object ScalaType0 {

  //  def position[A](xs: List[A], value: A): Int = {
  //    xs.indexOf(value)
  //  }
  //
  //  def main(args: Array[String]): Unit = {
  //
  //    val xs = List("one", "two", "three")
  //
  //    position(xs, "two")
  //
  //    val ys = List(20, 30, 40)
  //
  //    position(ys, 40)
  //
  //    position[Int](ys, 300)
  //
  //  }

//  def getStringLength(evidence =:= String) = 1

  def main(args: Array[String]): Unit = {
//    new ScalaType0().getStringLength

    val buf = new StringBuilder
    buf += 'a'
    buf ++= "bcdef"                  //++=
//    println(buf)

    val a = 'hello
    val b = 'hello

    a.name


    println(
      """|hello
           |world
      """.stripMargin)


    scala.Symbol

  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.fpm.FPGrowth
import org.apache.spark.rdd.RDD
// $example off$

object SimpleFPGrowth {

  def main(args: Array[String]) {

    val conf = new SparkConf().setAppName("SimpleFPGrowth")
    val sc = new SparkContext(conf)

    // $example on$
    val data = sc.textFile("data/mllib/sample_fpgrowth.txt")

    val transactions: RDD[Array[String]] = data.map(s => s.trim.split(' '))

    val fpg = new FPGrowth()
      .setMinSupport(0.2)
      .setNumPartitions(10)
    val model = fpg.run(transactions)

    model.freqItemsets.collect().foreach { itemset =>
      println(itemset.items.mkString("[", ",", "]") + ", " + itemset.freq)
    }

    val minConfidence = 0.8
    model.generateAssociationRules(minConfidence).collect().foreach { rule =>
      println(
        rule.antecedent.mkString("[", ",", "]")
          + " => " + rule.consequent .mkString("[", ",", "]")
          + ", " + rule.confidence)
    }
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import java.util.Random

import org.apache.spark.sql.SparkSession

/**
 * Usage: SimpleSkewedGroupByTest [numMappers] [numKVPairs] [valSize] [numReducers] [ratio]
 */
object SimpleSkewedGroupByTest {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("SimpleSkewedGroupByTest")
      .getOrCreate()

    val numMappers = if (args.length > 0) args(0).toInt else 2
    val numKVPairs = if (args.length > 1) args(1).toInt else 1000
    val valSize = if (args.length > 2) args(2).toInt else 1000
    val numReducers = if (args.length > 3) args(3).toInt else numMappers
    val ratio = if (args.length > 4) args(4).toInt else 5.0

    val pairs1 = spark.sparkContext.parallelize(0 until numMappers, numMappers).flatMap { p =>
      val ranGen = new Random
      val result = new Array[(Int, Array[Byte])](numKVPairs)
      for (i <- 0 until numKVPairs) {
        val byteArr = new Array[Byte](valSize)
        ranGen.nextBytes(byteArr)
        val offset = ranGen.nextInt(1000) * numReducers
        if (ranGen.nextDouble < ratio / (numReducers + ratio - 1)) {
          // give ratio times higher chance of generating key 0 (for reducer 0)
          result(i) = (offset, byteArr)
        } else {
          // generate a key for one of the other reducers
          val key = 1 + ranGen.nextInt(numReducers-1) + offset
          result(i) = (key, byteArr)
        }
      }
      result
    }.cache
    // Enforce that everything has been calculated and in cache
    pairs1.count

    println("RESULT: " + pairs1.groupByKey(numReducers).count)
    // Print how many keys each reducer got (for debugging)
    // println("RESULT: " + pairs1.groupByKey(numReducers)
    //                           .map{case (k,v) => (k, v.size)}
    //                           .collectAsMap)

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.common.utils

import java.util.regex.Pattern

/**
  * Created by wangbaosheng on 2017/9/21.
  */
object SizeParser {
  /**
    * 将字节，KB，MB，GB解析为字节。
    * 输入1Kb，返回1*1024 字节
    * 输入1Mb，返回1*1024*1024 字节数
    * @param sizeStr
    * @return 如果解析成功，返回Some(字节数),否则返回None.
    * */
  def parseAsByte(sizeStr: String): Option[Long] = {
    val sizeMatch = "^([1-9][0-9]*)([BbKkMmGg][Bb]?)$"
    val pattern = Pattern.compile(sizeMatch)
    val matcher = pattern.matcher(sizeStr)
    if (matcher.find()) {
      val size = matcher.group(1)
      val unit = matcher.group(2).toUpperCase()
      if (unit == "BB"||unit=="B") {
        Some(size.toLong)
      } else if (unit == "KB"||unit=="K") {
        Some(size.toLong * 1024)
      } else if (unit == "MB"||unit=="M") {
        Some(size.toLong * 1024 * 1024)
      } else if(unit == "GB"||unit=="G")  {
        Some(size.toLong * 1024 * 1024 * 1024)
      }else None
    } else {
      None
    }
  }


  def main(args: Array[String]): Unit = {
    println(parseAsByte("12B"))
    println(parseAsByte("12KB"))
    println(parseAsByte("12kb"))

    println(parseAsByte("12M"))
    println(parseAsByte("12Gb"))
    println(parseAsByte("12Mb"))

  }
}




// scalastyle:off println
package org.apache.spark.examples

import java.util.Random

import org.apache.spark.sql.SparkSession

/**
 * Usage: GroupByTest [numMappers] [numKVPairs] [KeySize] [numReducers]
 */
object SkewedGroupByTest {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("GroupBy Test")
      .getOrCreate()

    val numMappers = if (args.length > 0) args(0).toInt else 2
    var numKVPairs = if (args.length > 1) args(1).toInt else 1000
    val valSize = if (args.length > 2) args(2).toInt else 1000
    val numReducers = if (args.length > 3) args(3).toInt else numMappers

    val pairs1 = spark.sparkContext.parallelize(0 until numMappers, numMappers).flatMap { p =>
      val ranGen = new Random

      // map output sizes linearly increase from the 1st to the last
      numKVPairs = (1.0 * (p + 1) / numMappers * numKVPairs).toInt

      val arr1 = new Array[(Int, Array[Byte])](numKVPairs)
      for (i <- 0 until numKVPairs) {
        val byteArr = new Array[Byte](valSize)
        ranGen.nextBytes(byteArr)
        arr1(i) = (ranGen.nextInt(Int.MaxValue), byteArr)
      }
      arr1
    }.cache()
    // Enforce that everything has been calculated and in cache
    pairs1.count()

    println(pairs1.groupByKey(numReducers).count())

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.dayreport.distribution

class SocDistribution {
  private val  distributed = Array.fill(10)(0)
  def add(soc:Int) {
    if (0 <= soc && soc <= 10) {
      distributed(0) += 1
    } else if (10 < soc && soc <= 20) {
      distributed(1) += 1
    } else if (20 < soc && soc <= 30) {
      distributed(2) += 1
    } else if (30 < soc && soc <= 40) {
      distributed(3) += 1
    } else if (40 < soc && soc <= 50) {
      distributed(4) += 1
    } else if (50 < soc && soc <= 60) {
      distributed(5) += 1
    } else if (60 < soc && soc <= 70) {
      distributed(6) += 1
    } else if (70 < soc && soc <= 80) {
      distributed(7) += 1
    } else if (80 < soc && soc <= 90) {
      distributed(8) += 1
    } else if (90 < soc && soc <= 100) {
      distributed(9) += 1
    }
  }

  def getDistribution=distributed
}


object SocDistribution{
  def default: Array[Int] = Array.fill(10)(0)
}

// scalastyle:off println
package org.apache.spark.examples

import org.apache.commons.math3.linear._

import org.apache.spark.sql.SparkSession

/**
 * Alternating least squares matrix factorization.
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.ml.recommendation.ALS.
 */
object SparkALS {

  // Parameters set through command line arguments
  var M = 0 // Number of movies
  var U = 0 // Number of users
  var F = 0 // Number of features
  var ITERATIONS = 0
  val LAMBDA = 0.01 // Regularization coefficient

  def generateR(): RealMatrix = {
    val mh = randomMatrix(M, F)
    val uh = randomMatrix(U, F)
    mh.multiply(uh.transpose())
  }

  def rmse(targetR: RealMatrix, ms: Array[RealVector], us: Array[RealVector]): Double = {
    val r = new Array2DRowRealMatrix(M, U)
    for (i <- 0 until M; j <- 0 until U) {
      r.setEntry(i, j, ms(i).dotProduct(us(j)))
    }
    val diffs = r.subtract(targetR)
    var sumSqs = 0.0
    for (i <- 0 until M; j <- 0 until U) {
      val diff = diffs.getEntry(i, j)
      sumSqs += diff * diff
    }
    math.sqrt(sumSqs / (M.toDouble * U.toDouble))
  }

  def update(i: Int, m: RealVector, us: Array[RealVector], R: RealMatrix) : RealVector = {
    val U = us.length
    val F = us(0).getDimension
    var XtX: RealMatrix = new Array2DRowRealMatrix(F, F)
    var Xty: RealVector = new ArrayRealVector(F)
    // For each user that rated the movie
    for (j <- 0 until U) {
      val u = us(j)
      // Add u * u^t to XtX
      XtX = XtX.add(u.outerProduct(u))
      // Add u * rating to Xty
      Xty = Xty.add(u.mapMultiply(R.getEntry(i, j)))
    }
    // Add regularization coefs to diagonal terms
    for (d <- 0 until F) {
      XtX.addToEntry(d, d, LAMBDA * U)
    }
    // Solve it with Cholesky
    new CholeskyDecomposition(XtX).getSolver.solve(Xty)
  }

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of ALS and is given as an example!
        |Please use org.apache.spark.ml.recommendation.ALS
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {

    var slices = 0

    val options = (0 to 4).map(i => if (i < args.length) Some(args(i)) else None)

    options.toArray match {
      case Array(m, u, f, iters, slices_) =>
        M = m.getOrElse("100").toInt
        U = u.getOrElse("500").toInt
        F = f.getOrElse("10").toInt
        ITERATIONS = iters.getOrElse("5").toInt
        slices = slices_.getOrElse("2").toInt
      case _ =>
        System.err.println("Usage: SparkALS [M] [U] [F] [iters] [slices]")
        System.exit(1)
    }

    showWarning()

    println(s"Running with M=$M, U=$U, F=$F, iters=$ITERATIONS")

    val spark = SparkSession
      .builder
      .appName("SparkALS")
      .getOrCreate()

    val sc = spark.sparkContext

    val R = generateR()

    // Initialize m and u randomly
    var ms = Array.fill(M)(randomVector(F))
    var us = Array.fill(U)(randomVector(F))

    // Iteratively update movies then users
    val Rc = sc.broadcast(R)
    var msb = sc.broadcast(ms)
    var usb = sc.broadcast(us)
    for (iter <- 1 to ITERATIONS) {
      println(s"Iteration $iter:")
      ms = sc.parallelize(0 until M, slices)
                .map(i => update(i, msb.value(i), usb.value, Rc.value))
                .collect()
      msb = sc.broadcast(ms) // Re-broadcast ms because it was updated
      us = sc.parallelize(0 until U, slices)
                .map(i => update(i, usb.value(i), msb.value, Rc.value.transpose()))
                .collect()
      usb = sc.broadcast(us) // Re-broadcast us because it was updated
      println("RMSE = " + rmse(R, ms, us))
      println()
    }

    spark.stop()
  }

  private def randomVector(n: Int): RealVector =
    new ArrayRealVector(Array.fill(n)(math.random))

  private def randomMatrix(rows: Int, cols: Int): RealMatrix =
    new Array2DRowRealMatrix(Array.fill(rows, cols)(math.random))

}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import java.util.Random

import scala.math.exp

import breeze.linalg.{DenseVector, Vector}

import org.apache.spark.sql.SparkSession

/**
 * Logistic regression based classification.
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.ml.classification.LogisticRegression.
 */
object SparkHdfsLR {
  val D = 10   // Number of dimensions
  val rand = new Random(42)

  case class DataPoint(x: Vector[Double], y: Double)

  def parsePoint(line: String): DataPoint = {
    val tok = new java.util.StringTokenizer(line, " ")
    var y = tok.nextToken.toDouble
    var x = new Array[Double](D)
    var i = 0
    while (i < D) {
      x(i) = tok.nextToken.toDouble; i += 1
    }
    DataPoint(new DenseVector(x), y)
  }

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of Logistic Regression and is given as an example!
        |Please use org.apache.spark.ml.classification.LogisticRegression
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {

    if (args.length < 2) {
      System.err.println("Usage: SparkHdfsLR <file> <iters>")
      System.exit(1)
    }

    showWarning()

    val spark = SparkSession
      .builder
      .appName("SparkHdfsLR")
      .getOrCreate()

    val inputPath = args(0)
    val lines = spark.read.textFile(inputPath).rdd

    val points = lines.map(parsePoint).cache()
    val ITERATIONS = args(1).toInt

    // Initialize w to a random value
    var w = DenseVector.fill(D) {2 * rand.nextDouble - 1}
    println("Initial w: " + w)

    for (i <- 1 to ITERATIONS) {
      println("On iteration " + i)
      val gradient = points.map { p =>
        p.x * (1 / (1 + exp(-p.y * (w.dot(p.x)))) - 1) * p.y
      }.reduce(_ + _)
      w -= gradient
    }

    println("Final w: " + w)
    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.sparkhelper

import java.text.SimpleDateFormat
import java.util.UUID

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.hadoop.fs.{FileSystem, FileUtil, Path}
import org.apache.hadoop.hbase.client.{Result, Scan}
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.protobuf.ProtobufUtil
import org.apache.hadoop.hbase.util.Base64
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql._
import org.apache.spark.sql.catalog.Table
import org.apache.spark.sql.types._

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer
import scala.io.Source
import scala.reflect.runtime.universe._
import scala.reflect.runtime.{universe => ru}

case class FieldSchema(
                        srcColName:String,
                        srcDataType:String,
                        dstColName:String,
                        dstColDataType:String,
                        nullable:Boolean,
                        defaultValue:Option[String],
                        srcFormat:Option[String],
                        dstFormat:Option[String],
                        index:Int=0
                      )
/**
  * Created by francis on 2017/2/21.
  *
  * 1.read text(lzo,lzo+index),hbase,jdbc to rdd or dataframe
  * 2.convert between rdd and dataframe
  * 3.common api:union dataframe,registe jdbc as table
  * 4.table api
  * */
object SparkHelper extends  Logging {
  def main(args: Array[String]): Unit = {
    val x=SparkHelper.parseLine("EDLATITU:30052018,EDLONGIT:103111306,EDTIME:20171012114112,STLATITU:30051831,STLONGIT:103111438,STTIME:20171012114108,TYPE:3,VID:6139bd62-763d-426f-a43a-ed9d1eefad1")
    x.foreach(println)
  }

  /**
    * 读取文本文件或者parquet文件为一个DataFrame
    *
    * @param path          输入文件路径
    * @param parquetOrText 如果输入文件是文本文件，那么该参数为text，如果是parquet文件，那么该参数是parquet
    * @param schema        数据的架构,格式如下：
    *                      name:VID,type:StringType,nullable:false,alias:VID,default:
    *                      name:TIME,type:StringType,nullable:false,alias:TIME,default:
    **/
  def parquetOrText(sqlContext: SQLContext, path: String, parquetOrText: String, schema: Array[String]): DataFrame = {
    SparkHelper.parquetOrText(sqlContext, Array(path), parquetOrText, schema)
  }

  /**
    * 读取文本文件或者parquet文件并转换为DataFrame
    * 文本文件支持lzo压缩，以及lzo index。
    **/
  def parquetOrText(sqlContext: SQLContext, path: Array[String], parquetOrText: String, schema: Array[String]): DataFrame = {
    if (parquetOrText == "parquet") {
      val source = sqlContext.read.parquet(path: _*)
      val nameTable = parseSchema(schema).map(field => (field.srcColName, field.dstColName)).toMap
      val r = renameColumn(source, nameTable)
      r
    } else if (parquetOrText == "text.lzo.index") {
      val srcRdd = readHadoopTextFile(sqlContext.sparkContext, path.reduce((a, b) => s"$a,$b"), Some("text.lzo.index"),None)
      rddToDataFrame(sqlContext, srcRdd, schema)
    } else if (parquetOrText == "text") {
      val srcRdd = readHadoopTextFile(sqlContext.sparkContext, path.reduce((a, b) => s"$a,$b"), None,Some(200))
      rddToDataFrame(sqlContext, srcRdd, schema)
    } else throw new Exception(s"$parquetOrText has been not supported,path=$path")
  }

  def parquetOrText[T: ru.TypeTag](sqlContext: SQLContext, path: String, parquetOrText: String): DataFrame = {
    SparkHelper.parquetOrText[T](sqlContext, Array(path), parquetOrText)
  }

  def parquetOrText[T: ru.TypeTag](sqlContext: SQLContext, path: Array[String], parquetOrText: String): DataFrame = {
    if (parquetOrText == "parquet") {
      sqlContext.read.parquet(path: _*)
    } else if (parquetOrText == "text.lzo.index") {
      val srcRdd = readHadoopTextFile(sqlContext.sparkContext, path.reduce((a, b) => s"$a,$b"), Some("text.lzo.index"),None)
      rddToDataFreme(sqlContext, srcRdd, parseLine)
    } else if (parquetOrText == "text") {
      val srcRdd = readHadoopTextFile(sqlContext.sparkContext, path.reduce((a, b) => s"$a,$b"), None,None)
      rddToDataFreme(sqlContext, srcRdd, parseLine)
    } else throw new Exception("错误的数据源")
  }


  def parseLine(line: String): Map[String, String] = {
    try {
      val map = new scala.collection.mutable.HashMap[String, String]()
      line.split(',').foreach(tuple => {
        val keyValuePair = tuple.split(':')
        if (keyValuePair.length == 2) {
          map.put(keyValuePair(0), keyValuePair(1))
        }
      })
      map.toMap
    } catch {
      case e: Exception =>
        Map()
    }
  }

  def unionDataFrames(dataFrames: Array[Dataset[Row]]): Dataset[Row] = {
    dataFrames.reduce((a, b) => a.union(b))
  }

  def unionDatasets[T](dataSets: Array[Dataset[T]]): Dataset[T] = {
    dataSets.reduce((a, b) => a.union(b))
  }


  /**
    * rdd to dataframe or dataframe to rdd
    */

  def caseClassFields[T: ru.TypeTag]: Array[String] = {
    typeOf[T].members.filter(m => !m.isMethod).map(m => {
      val fieldName = m.name.toString.trim
      val (fullTypeName, nullable, default) = m.typeSignature.toString match {
        case "String" => ("StringType", false, "")
        case "Option[String]" => ("StringType", true, "")
        case "Int" => ("IntegerType", false, 0)
        case "Option[Int]" => ("IntegerType", true, 0)
        case "Long" => ("LongType", false, 0)
        case "Option[Long]" => ("LongType", true, 0)
        case "Double" => ("DoubleType", false, 0)
        case "Option[Double]" => ("DoubleType", true, 0)
        case "Float" => ("FloatType", false, 0)
        case "Option[Float]" => ("FloatType", true, 0)
        case _ => throw new Exception(s"tht type in case class was not supported:" + m.typeSignature)
      }
      s"name:$fieldName,type:$fullTypeName,nullable:$nullable,default:$default"
    }).toArray
  }


  def renameColumn(df: DataFrame, nameTable: Map[String, String]): DataFrame = {
    nameTable.foldLeft(df)((acc, namePair) => acc.withColumnRenamed(namePair._1, namePair._2))
  }


  /**
    * srcRdd:被转化的rdd，每一行格式为key:value,key:value...
    * schemaString，要生成的DataFrame架构，格式为key:value,key:value,
    * field=vid,type=int,nullable=true,alias=vid
    **/
  val typeTable: Map[String, DataType] = Map(
    "StringType" -> StringType,
    "ByteType" -> ByteType,
    "ShortType" -> ShortType,
    "IntegerType" -> IntegerType,
    "LongType" -> LongType,
    "FloatType" -> FloatType,
    "DoubleType" -> DoubleType,
    "BooleanType" -> BooleanType,
    "DateStringType" -> StringType
  )


  def getSparkSession(sparkMaster: Option[String], sparkAppName: Option[String] = Some("defaultSparkAppName")): SparkSession = {
    val sparkSession = (sparkMaster, sparkAppName) match {
      case ((Some(master), Some(appName))) => SparkSession.builder().master(master).appName(appName).getOrCreate()
      case _ => SparkSession.builder().enableHiveSupport().getOrCreate()

    }
    sparkSession
  }

//  def rddToDataFrame(sqlContext: SQLContext, srcRdd: RDD[String], fieldsString: Array[String], parserLine: (String) => Map[String, String]): DataFrame = {
//    val fieldSchemaList = parseSchema(fieldsString)
//    logInfo(s"***************************************${fieldsString.length}")
//    fieldsString.foreach(x => logInfo(x))
//
//    val dataFrameSchema=StructType(fieldSchemaList.map(s => {
//      StructField(s.dstColName, typeTable(s.dstColDataType), nullable = s.nullable)
//    }))
//
//    val b=sqlContext.sparkContext.broadcast[Array[FieldSchema]](fieldSchemaList)
//
//    val dstRdd = srcRdd
//      .map(line => {
//        val fields = parserLine(line)
//        val values = b.value.map(fieldSchema => {
//          try {
//            getValue(fieldSchema, fields.get(fieldSchema.srcColName)).orNull
//          } catch {
//            case e: java.lang.NumberFormatException =>
//              logWarning(s"src field:${fieldSchema.srcColName} dst field:${fieldSchema.dstColName} record:$line,value:${fields.get(fieldSchema.srcColName)}", e)
//              null
//            case e: Exception =>
//              logWarning(s"src field:$fieldSchema dst field:${fieldSchema.dstColName} record:$line,value:${fields.get(fieldSchema.srcColName)}", e)
//              null
//          }
//        })
//
//        Row(values.toArray: _*)
//      })
//
//
//    sqlContext.createDataFrame(dstRdd,dataFrameSchema)
//  }

  def rddToDataFrame(sqlContext: SQLContext, srcRdd: RDD[String], fieldsString: Array[String]): DataFrame = {
    val fieldSchemaList = parseSchema(fieldsString)
    logInfo(s"***************************************${fieldsString.length}")
    fieldsString.foreach(x => logInfo(x))

    val dataFrameSchema = StructType(fieldSchemaList.map(s => {
      StructField(s.dstColName, typeTable(s.dstColDataType), nullable = s.nullable)
    }))

    val b = {
      //初始化index
      sqlContext.sparkContext.broadcast[Map[String, FieldSchema]](fieldSchemaList.zipWithIndex.map(v => (v._1.srcColName, v._1.copy(index = v._2))).toMap)
    }

    val dstRdd = srcRdd
      .map(line => {
        val map = b.value
        val rowFields = mutable.ArrayBuffer.fill[Any](map.size)(null)

        parseLine(line, (key, value) => {
          map.get(key).foreach(fieldSchema => {
            try {
              val parsedValue = getValue(fieldSchema, Some(value)).orNull
              rowFields(fieldSchema.index) = parsedValue
            } catch {
              case e: java.lang.NumberFormatException =>
                logWarning(s"src field:${fieldSchema.srcColName} dst field:${fieldSchema.dstColName} record:$line,value:${value}", e)
                null
              case e: Exception =>
                logWarning(s"src field:$fieldSchema dst field:${fieldSchema.dstColName} record:$line,value:${value}", e)
                null
            }
          })
        })

        Row(rowFields.toArray: _*)
      })


    sqlContext.createDataFrame(dstRdd, dataFrameSchema)
  }

  def parseLine(line: String,processField:(String,String)=>Unit): Unit= {
    try {
      val map = new scala.collection.mutable.HashMap[String, String]()
      line.split(',').foreach(tuple => {
        val keyValuePair = tuple.split(':')
        if (keyValuePair.length == 2) {
          val key=keyValuePair(0)
          val value=keyValuePair(1)
          processField(key,value)
        }
      })
      map.toMap
    } catch {
      case e: Exception =>
        Map()
    }
  }

  def getValue(fieldSchema: FieldSchema, value: Option[String]): Option[Any] = {
    val default = fieldSchema.defaultValue

    logInfo(fieldSchema.srcDataType)

    if (fieldSchema.srcDataType == "StringType") {
      value.orElse(default)
    } else if (fieldSchema.srcDataType == "IntegerType") {
      value.orElse(default).map(_.toInt)
    } else if (fieldSchema.srcDataType == "ByteType") {
      value.orElse(default).map(_.toByte)
    } else if (fieldSchema.srcDataType == "LongType") {
      value.orElse(default).map(_.toLong)
    } else if (fieldSchema.srcDataType == "FloatType") {
      value.orElse(default).map(_.toFloat)
    } else if (fieldSchema.srcDataType == "DoubleType") {
      value.orElse(default).map(_.toDouble)
    } else if (fieldSchema.srcDataType == "BooleanType") {
      value.orElse(default).map(_.toBoolean)
    } else if (fieldSchema.srcDataType == "DateStringType") {
      if (fieldSchema.srcFormat.isEmpty) throw new RuntimeException("the srcFormat error")
      else if (fieldSchema.dstFormat.isEmpty) throw new RuntimeException("the srcFormat error")
      else
        Some(formatDate(fieldSchema.srcFormat.get, fieldSchema.dstFormat.get, value.get))
    } else {
      throw new Exception(s"不支持该类型:${fieldSchema.srcDataType}")
    }
  }

  def formatDate(srcFormat: String, dstFormat: String, date: String): String = {
    val format = new SimpleDateFormat(srcFormat)
    new SimpleDateFormat(dstFormat).format(format.parse(date))
  }


  def parseSchema(schema: Array[String]): Array[FieldSchema] = {
    schema.map(field => {
      val fieldMap = parseLine(field)
      val name = fieldMap("name").trim
      val fieldType = fieldMap("type").trim
      val nullable = fieldMap("nullable").trim
      //如果没有指定别名，那么输出列明就是原始列名
      val alias = fieldMap.getOrElse("alias", name).trim

      val default = fieldMap.get("detault")


      val srcFormat = fieldMap.get("srcFormat")
      val dstFormat = fieldMap.get("dstFormat")

      FieldSchema(name, fieldType, alias, fieldType, if (nullable == "true") true else false, default, srcFormat, dstFormat)
    })
  }


  def rddToDataFreme[T: ru.TypeTag](sqlContext: SQLContext, srcRdd: RDD[String], parserLine: (String) => Map[String, String]): DataFrame = {
    rddToDataFrame(sqlContext, srcRdd, caseClassFields[T])
  }


  /**
    * read from source to rdd or dataframe api
    *  1.read text
    *  2.read hbase
    *  3.jdbc
    */

  def readHadoopTextFile(sparkContext: SparkContext, path: String, compression: Option[String],minPartition:Option[Int]): RDD[String] = {
    compression match {
      case Some("text.lzo.index") =>
        readHadoopTextLzoFile(sparkContext, path)
      case _ => minPartition.map(sparkContext.textFile(path, _)).getOrElse(sparkContext.textFile(path))
    }
  }


  def readHadoopTextLzoFile(sparkContext: SparkContext, path: String): RDD[String] = {
    //import com.hadoop.mapreduce.LzoTextInputFormat
    import org.apache.hadoop.io.LongWritable
    import org.apache.hadoop.io.Text

    Thread.currentThread().getContextClassLoader.loadClass("com.hadoop.mapreduce.LzoTextInputFormat") match {
      case lzoTextInputFormat: Class[TextInputFormat] =>
        sparkContext.newAPIHadoopFile(
          path,
          lzoTextInputFormat,
          classOf[LongWritable],
          classOf[Text]).map(_._2.toString)
      case _ => throw new Exception("not found  com.hadoop.mapreduce.LzoTextInputFormat")
    }
  }


  def readFromHbase(sparkSession: SparkSession, zkQuorum: String, zkPort: String, tableName: String, setFilter: () => Scan): RDD[(ImmutableBytesWritable, Result)] = {
    val con = HbaseHelper.getConnection(zkQuorum, zkPort)
    val conf = con.getConfiguration

    val scan = setFilter()

    conf.set(TableInputFormat.SCAN, convertScanToString(scan))
    conf.set(TableInputFormat.INPUT_TABLE, tableName)

    sparkSession.sparkContext.newAPIHadoopRDD(conf, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result])
  }


  def readFromHbase(sparkSession: SparkSession,
                    zkQuorum: String,
                    zkPort: String,
                    tableName: String,
                    setFilter: () => Scan,
                    f:(RDD[(ImmutableBytesWritable,Result)])=>Unit) = {
    val con = HbaseHelper.getConnection(zkQuorum, zkPort)
    val conf = con.getConfiguration

    val scan = setFilter()

    conf.set(TableInputFormat.SCAN, convertScanToString(scan))
    conf.set(TableInputFormat.INPUT_TABLE, tableName)

    val sourceRdd=sparkSession.sparkContext.newAPIHadoopRDD(conf, classOf[TableInputFormat],
      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],
      classOf[org.apache.hadoop.hbase.client.Result])

    f(sourceRdd)
    con.close()
  }

  def convertScanToString(scan: Scan): String = {
    val proto = ProtobufUtil.toScan(scan)
    Base64.encodeBytes(proto.toByteArray)
  }


  def jdbc(sqlContext: SQLContext, url: String, driver: String, dbtable: String, user: String, password: String, numPartitions: Int): DataFrame = {
    sqlContext.read.format("jdbc").options(Map(
      "url" -> url,
      "driver" -> driver,
      "dbtable" -> s"(SELECT * FROM $dbtable) $dbtable",
      "user" -> user,
      "password" -> password,
      "numPartitions" -> numPartitions.toString
    )).load
  }

  def jdbc(sqlContext: SQLContext, url: String, driver: String, dbtable: String, user: String, password: String): DataFrame = jdbc(sqlContext, url, driver, dbtable, user, password, 1)

  def registerJdbcAsTempView(sqlContext: SQLContext, url: String, driver: String, dbtable: String, user: String, password: String, numPartitions: Int): Unit = {
    registerJdbcAsTempView(sqlContext, url, driver, user, password, dbtable, s"SELECT * FROM $dbtable", numPartitions)
  }

  def registerJdbcAsTempView(sqlContext: SQLContext, url: String, driver: String, dbtable: String, user: String, password: String): Unit =
    registerJdbcAsTempView(sqlContext, url, driver, user, password, dbtable, s"SELECT * FROM $dbtable", 1)


  def registerJdbcAsTempView(sqlContext: SQLContext, url: String, driver: String, user: String, password: String, tableName: String, sqlText: String, numPartitions: Int) {
    sqlContext.read.format("jdbc").options(Map(
      "url" -> url,
      "driver" -> driver,
      "dbtable" -> s"($sqlText) $tableName",
      "user" -> user,
      "password" -> password,
      "numPartitions" -> numPartitions.toString
    )).load.createOrReplaceTempView(tableName)
  }


  /**
    *
    * table api
    *
    *
    */

  def existsTable(sparkSession: SparkSession, stateConf: StateConf, tableName: String): Unit = {
    sparkSession.catalog
      .listTables()
      .collect()
      .contains((table: Table) => table.name == tableName)
  }



  def loadTable(sparkSession: SparkSession, stateConf: StateConf, tableName: String): DataFrame = {
    val tableInfo = getTableInfo(stateConf, tableName)
    logInfo(s"reading data from ${tableInfo.directory}")
    sparkSession.sqlContext
      .read
      .format(tableInfo.format)
      .load(tableInfo.directory)
  }

  def createOrReplaceTempView(sparkSession: SparkSession, stateConf: StateConf, tableName: String): Unit = {
    loadTable(sparkSession, stateConf, tableName).createOrReplaceTempView(tableName)
  }



  def saveTable(sparkSession: SparkSession, stateConf: StateConf, table: DataFrame, tableName: String): Unit = {
    saveTable(sparkSession, stateConf, table, tableName, Map[String, String]())
  }

  def saveTable(sparkSession: SparkSession, stateConf: StateConf, table: DataFrame, tableName: String, partitionColumn: scala.collection.immutable.Map[String, String]): Unit = {
    val tableInfo = getTableInfo(stateConf, tableName)

    logInfo("save table:" + tableInfo.toString)

    val repartitiond = partitionNum(stateConf, tableName).map(table.repartition(_))
      .getOrElse(table)

    val writerMode = repartitiond.write.option("path", directory(stateConf, tableName))
      .mode(saveMode(stateConf, tableName))
      .format(tableInfo.format)

    val partitionDirectory = if (partitionColumn.nonEmpty) Some(buildPartitionDirectory(partitionColumn))
    else if (tableInfo.partitionDirectory.nonEmpty) tableInfo.partitionDirectory
    else None

    partitionDirectory match {
      case Some(pd) =>
        val bucketedWriter = tableInfo.bucketColumn.foldLeft(writerMode)((acc, unit) => acc.bucketBy(tableInfo.bucketNum, unit))
        logInfo(s"partition directory:$pd")
        bucketedWriter.save(s"${tableInfo.directory}/$pd")
      case None =>
        if (tableInfo.overwriteWholeTable) {
          val partitionedWriter = if (tableInfo.partitionColumn.nonEmpty) writerMode.partitionBy(tableInfo.partitionColumn: _*) else writerMode
          val bucketedWriter = tableInfo.bucketColumn.foldLeft(partitionedWriter)((acc, unit) => acc.bucketBy(tableInfo.bucketNum, unit))
          bucketedWriter.saveAsTable(tableName)
        } else throw new Exception("no partition directory or overwriteWholeTable")
    }
  }

  /**
    *
    * 保存数据到指定分区
    * */
  def saveToPartition(sparkSession: SparkSession, stateConf: StateConf, table: DataFrame, tableName: String) {
    saveToPartition(sparkSession, stateConf, table, tableName, Map[String, String]())
  }

  def saveToPartition(sparkSession: SparkSession, stateConf: StateConf, table: DataFrame, tableName: String, partitionColumn: scala.collection.immutable.Map[String, String]) {
    val tableInfo = getTableInfo(stateConf, tableName)
    logInfo("save table:" + tableInfo.toString)

    def saveToTempPath(): Unit = {
      val repartitiond = tableInfo.partitionNum.map(table.repartition)
        .getOrElse(table)

      val writerMode = repartitiond.write
        .mode(SaveMode.Overwrite)
        .format(tableInfo.format)
     // val partitioned = if (tableInfo.partitionColumn.nonEmpty) writerMode.partitionBy(tableInfo.partitionColumn: _*) else writerMode
      val bucketedWriter = tableInfo.bucketColumn.foldLeft(writerMode)((acc, unit) => acc.bucketBy(tableInfo.bucketNum, unit))

      if (partitionColumn.nonEmpty) bucketedWriter.save(s"${tableInfo.directory}/${buildPartitionDirectory(partitionColumn)}")
      else if (tableInfo.partitionDirectory.nonEmpty) {
        val outputPath=s"${tableInfo.directory}/${tableInfo.partitionDirectory.get}"
        logInfo(s"output path:$outputPath")
        bucketedWriter.save(outputPath)
      }
      else logWarning(s"partition directition dot not exists,table name:$tableName,table info:$tableInfo")
    }


    saveToTempPath()
    //    val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
    //    val fs = FileSystem.get(hadoopConfiguration)
    //
    //    val fromPath = s"$tempPath/${tableInfo.partitionDirectory.get}"
    //    val toPath = s"${tableInfo.directory}/${tableInfo.partitionDirectory.get}"
    //    if(!fs.exists(new Path(toPath)))fs.create(new Path(toPath))
    //
    //
    //    logInfo(s"mv $fromPath to $toPath")
    //
    //    mv(fs, fromPath, toPath)
  }

  def mv(fs:FileSystem,from: String, to: String): Unit = {
    import org.apache.hadoop.fs.Path
    //  fs.rename(new Path(from), new Path(to))

    val fromPath = new Path(from)
    if (fs.exists(fromPath) && fs.listStatus(fromPath).length > 0) {
      fs.listStatus(fromPath).foreach(status => {
        FileUtil.copy(fs, status.getPath, fs, new Path(to), true, true, fs.getConf)
      })
    }
  }

  private def buildPartitionDirectory(partitionColumn: scala.collection.immutable.Map[String, String]): String = {
    partitionColumn.foldLeft("")((acc, unit) => {
      val partitionColumnName = unit._1
      val partitionColumnValue = unit._2
      if (acc.isEmpty) {
        s"$partitionColumnName=$partitionColumnValue"
      } else {
        s"$acc,$partitionColumnName=$partitionColumnValue"
      }
    })
  }

  def allTable(stateConf: StateConf): Array[TableInfo] = {
    def createTable(tableName: String): TableInfo = {
      TableInfo(
        directory =
          if (tableName == "srcRealinfo" || tableName == "srcForward" || tableName == "srcLogin" || tableName == "srcAlarm" || tableName == "srcTermstatus") "/vehicle/data"
          else if (tableName == "realinfo" || tableName == "forward" || tableName == "login" || tableName == "alarm" || tableName == "termstatus") "/spark/vehicle/data"
          else "/spark/vehicle/result",

        tableName = tableName,
        partitionColumn = Array("year-month-day"),
        partitionValue = Array.empty[String],
        partitionNum = None,
        bucketColumn = Some("vid"),
        bucketNum = 10,
        saveMode = SaveMode.ErrorIfExists,
        schema = Array.empty[String],
        format = "parquet",
        overwriteWholeTable = false
      )
    }

    def default(): mutable.HashMap[String, TableInfo] = {
      val tables = new mutable.HashMap[String, TableInfo]()
      tables.put("srcRealinfo", createTable("srcRealinfo"))
      tables.put("srcForward", createTable("srcForward"))
      tables.put("srcLogin", createTable("srcLogin"))
      tables.put("srcAlarm", createTable("srcAlarm"))
      tables.put("srcTermstatus", createTable("srcTermstatus"))

      tables.put("realinfo", createTable("realinfo"))
      tables.put("forward", createTable("forward"))
      tables.put("login", createTable("login"))
      tables.put("alarm", createTable("alarm"))
      tables.put("termstatus", createTable("termstatus"))

      tables.put("detail", createTable("detail"))
      tables.put("dayreport", createTable("dayreport"))
      tables
    }


    val tables = default()


    stateConf.getOption("table") match {
      case Some(tableList) =>
        tableList.split(',').map(_.trim).map(tableName => {
          if (tables.contains(tableName)) {
            tables.update(tableName, getTableInfo(stateConf, tableName))
          } else {
            tables.put(tableName, getTableInfo(stateConf, tableName))
          }
        })
      case None =>
    }

    tables.values.toArray
  }

  def getTableInfo(stateConf: StateConf, tableName: String): TableInfo = {
    TableInfo(
      directory = directory(stateConf, tableName),
      tableName = tableName,
      partitionColumn = partitionColumn(stateConf, tableName),
      partitionValue = partitionValue(stateConf, tableName),
      partitionNum = partitionNum(stateConf, tableName),
      bucketColumn = bucketColumn(stateConf, tableName),
      bucketNum = bucketNum(stateConf, tableName),
      saveMode = saveMode(stateConf, tableName),
      schema = schema(stateConf, tableName),
      format = format(stateConf, tableName),
      overwriteWholeTable = overwriteWholeTable(stateConf, tableName)
    )
  }

  private def schema(stateConf: StateConf, tableName: String): Array[String] = {
    stateConf.getOption(s"$tableName.table.schemaFile") match {
      case Some(schemaFile) if schemaFile.trim.nonEmpty => Source.fromFile(schemaFile.trim).getLines().toArray
      case None => Array.empty[String]
    }
  }

  private def saveMode(stateConf: StateConf, tableName: String): SaveMode = {
    stateConf.getOption(s"$tableName.table.saveMode") match {
      case Some("append") => SaveMode.Append
      case Some("errorIfExists") => SaveMode.ErrorIfExists
      case Some("ignore") => SaveMode.Ignore
      case Some("overwrite") => SaveMode.Overwrite
      case _ => SaveMode.ErrorIfExists
    }
  }

  private def directory(stateConf: StateConf, tableName: String): String = {
    stateConf.getString(s"$tableName.table.directory")
  }

  private def tableName(stateConf: StateConf, tableName: String): String = {
    stateConf.getString(s"$tableName.table.name")
  }

  private def partitionColumn(stateConf: StateConf, tableName: String): Array[String] = {
    stateConf.getOption(s"$tableName.table.partitionColumn") match {
      case Some(partitionColumn) => partitionColumn.split('-').map(_.trim)
      case None => Array.empty[String]
    }
  }

  def partitionValue(stateConf: StateConf, tableName: String): Array[String] = {
    stateConf.getOption(s"$tableName.table.partitionValue") match {
      case Some(partitionValue) => partitionValue.split('-').map(_.trim)
      case None => Array.empty[String]
    }
  }

  def setPartitionValue(stateConf: StateConf, tableName: String,partitionValues:Array[String]):Unit = {
    val partitionValue = partitionValues.reduce((a, b) => s"$a-$b")
    logInfo(s"partition value:$partitionValue")
    stateConf.set(s"$tableName.table.partitionValue", partitionValue)
  }

  private def partitionNum(stateConf: StateConf, tableName: String): Option[Int] = {
    stateConf.getOption(s"$tableName.table.partitionNum").map(_.toInt)
  }


  private def bucketColumn(stateConf: StateConf, tableName: String): Option[String] = {
    stateConf.getOption(s"$tableName.table.bucketColumn")
  }

  private def bucketNum(stateConf: StateConf, tableName: String): Int = {
    stateConf.getInt(s"$tableName.table.bucketNum", 10)
  }

  private def format(stateConf: StateConf, tableName: String): String = {
    stateConf.getOption(s"$tableName.table.format").getOrElse("parquet")
  }

  private def overwriteWholeTable(stateConf: StateConf, tableName: String): Boolean = {
    stateConf.getOption(s"$tableName.table.overwriteWholeTable").contains("true")
  }

  def saveRddAsTextFile(rdd: RDD[String], compressionClassName: Option[String], outputPath: String) {
    import org.apache.hadoop.io.compress.CompressionCodec
    //   spark.sparkContext.getConf.set("io.compression.codecs", "com.hadoop.compression.lzo.LzopCodec")
    compressionClassName match {
      case Some(compression) =>
        Thread.currentThread().getContextClassLoader.loadClass(compression) match {
          case lzo: Class[CompressionCodec] => rdd.saveAsTextFile(outputPath, lzo)
          case _ => throw new Exception(s" can not cast $compressionClassName to CompressionCodec")
        }
      case None =>
        rdd.saveAsTextFile(outputPath)
    }
  }
}


//realinfo.table.partition=
case class TableInfo(
                      directory:String,
                      tableName:String,
                      partitionColumn:Array[String],
                      partitionValue:Array[String],
                      partitionNum:Option[Int],
                      bucketColumn:Option[String],
                      bucketNum:Int,
                      saveMode: SaveMode,
                      schema:Array[String],
                      format:String,
                      overwriteWholeTable:Boolean) {
  override def toString: String = s"$directory,$tableName," +
    s"${if (partitionColumn.nonEmpty) partitionColumn.reduce((a, b) => s"$a,$b") else ""}," +
    s"$partitionNum,$bucketColumn,$bucketNum,$saveMode,$format"

  val partitionDirectory: Option[String] = {
    val stringBuilder = new StringBuilder
    var break = false

    for ((partitionColumnName, partitionColumnValue) <- partitionColumn.zip(partitionValue) if !break) {
      if (partitionColumnName.trim.nonEmpty && partitionColumnValue.trim.nonEmpty)
        stringBuilder.append(s"$partitionColumnName=$partitionColumnValue/")
      else {
        break = true
      }
    }
    if (stringBuilder.nonEmpty) Some(stringBuilder.toString()) else None
  }
}

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.examples.sql.hive

// $example on:spark_hive$
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
// $example off:spark_hive$

object SparkHiveExample {

  // $example on:spark_hive$
  case class Record(key: Int, value: String)
  // $example off:spark_hive$

  def main(args: Array[String]) {
    // When working with Hive, one must instantiate `SparkSession` with Hive support, including
    // connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined
    // functions. Users who do not have an existing Hive deployment can still enable Hive support.
    // When not configured by the hive-site.xml, the context automatically creates `metastore_db`
    // in the current directory and creates a directory configured by `spark.sql.warehouse.dir`,
    // which defaults to the directory `spark-warehouse` in the current directory that the spark
    // application is started.

    // $example on:spark_hive$
    // warehouseLocation points to the default location for managed databases and tables
    val warehouseLocation = "spark-warehouse"

    val spark = SparkSession
      .builder()
      .appName("Spark Hive Example")
      .config("spark.sql.warehouse.dir", warehouseLocation)
      .enableHiveSupport()
      .getOrCreate()

    import spark.implicits._
    import spark.sql

    sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING)")
    sql("LOAD DATA LOCAL INPATH 'examples/src/main/resources/kv1.txt' INTO TABLE src")

    // Queries are expressed in HiveQL
    sql("SELECT * FROM src").show()
    // +---+-------+
    // |key|  value|
    // +---+-------+
    // |238|val_238|
    // | 86| val_86|
    // |311|val_311|
    // ...

    // Aggregation queries are also supported.
    sql("SELECT COUNT(*) FROM src").show()
    // +--------+
    // |count(1)|
    // +--------+
    // |    500 |
    // +--------+

    // The results of SQL queries are themselves DataFrames and support all normal functions.
    val sqlDF = sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")

    // The items in DaraFrames are of type Row, which allows you to access each column by ordinal.
    val stringsDS = sqlDF.map {
      case Row(key: Int, value: String) => s"Key: $key, Value: $value"
    }
    stringsDS.show()
    // +--------------------+
    // |               value|
    // +--------------------+
    // |Key: 0, Value: val_0|
    // |Key: 0, Value: val_0|
    // |Key: 0, Value: val_0|
    // ...

    // You can also use DataFrames to create temporary views within a SparkSession.
    val recordsDF = spark.createDataFrame((1 to 100).map(i => Record(i, s"val_$i")))
    recordsDF.createOrReplaceTempView("records")

    // Queries can then join DataFrame data with data stored in Hive.
    sql("SELECT * FROM records r JOIN src s ON r.key = s.key").show()
    // +---+------+---+------+
    // |key| value|key| value|
    // +---+------+---+------+
    // |  2| val_2|  2| val_2|
    // |  4| val_4|  4| val_4|
    // |  5| val_5|  5| val_5|
    // ...
    // $example off:spark_hive$

    spark.stop()
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import breeze.linalg.{squaredDistance, DenseVector, Vector}

import org.apache.spark.sql.SparkSession

/**
 * K-means clustering.
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.ml.clustering.KMeans.
 */
object SparkKMeans {

  def parseVector(line: String): Vector[Double] = {
    DenseVector(line.split(' ').map(_.toDouble))
  }

  def closestPoint(p: Vector[Double], centers: Array[Vector[Double]]): Int = {
    var bestIndex = 0
    var closest = Double.PositiveInfinity

    for (i <- 0 until centers.length) {
      val tempDist = squaredDistance(p, centers(i))
      if (tempDist < closest) {
        closest = tempDist
        bestIndex = i
      }
    }

    bestIndex
  }

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of KMeans Clustering and is given as an example!
        |Please use org.apache.spark.ml.clustering.KMeans
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {

    if (args.length < 3) {
      System.err.println("Usage: SparkKMeans <file> <k> <convergeDist>")
      System.exit(1)
    }

    showWarning()

    val spark = SparkSession
      .builder
      .appName("SparkKMeans")
      .getOrCreate()

    val lines = spark.read.textFile(args(0)).rdd
    val data = lines.map(parseVector _).cache()
    val K = args(1).toInt
    val convergeDist = args(2).toDouble

    val kPoints = data.takeSample(withReplacement = false, K, 42)
    var tempDist = 1.0

    while(tempDist > convergeDist) {
      val closest = data.map (p => (closestPoint(p, kPoints), (p, 1)))

      val pointStats = closest.reduceByKey{case ((p1, c1), (p2, c2)) => (p1 + p2, c1 + c2)}

      val newPoints = pointStats.map {pair =>
        (pair._1, pair._2._1 * (1.0 / pair._2._2))}.collectAsMap()

      tempDist = 0.0
      for (i <- 0 until K) {
        tempDist += squaredDistance(kPoints(i), newPoints(i))
      }

      for (newP <- newPoints) {
        kPoints(newP._1) = newP._2
      }
      println("Finished iteration (delta = " + tempDist + ")")
    }

    println("Final centers:")
    kPoints.foreach(println)
    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import java.util.Random

import scala.math.exp

import breeze.linalg.{DenseVector, Vector}

import org.apache.spark.sql.SparkSession

/**
 * Logistic regression based classification.
 * Usage: SparkLR [slices]
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.ml.classification.LogisticRegression.
 */
object SparkLR {
  val N = 10000  // Number of data points
  val D = 10   // Number of dimensions
  val R = 0.7  // Scaling factor
  val ITERATIONS = 5
  val rand = new Random(42)

  case class DataPoint(x: Vector[Double], y: Double)

  def generateData: Array[DataPoint] = {
    def generatePoint(i: Int): DataPoint = {
      val y = if (i % 2 == 0) -1 else 1
      val x = DenseVector.fill(D) {rand.nextGaussian + y * R}
      DataPoint(x, y)
    }
    Array.tabulate(N)(generatePoint)
  }

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of Logistic Regression and is given as an example!
        |Please use org.apache.spark.ml.classification.LogisticRegression
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {

    showWarning()

    val spark = SparkSession
      .builder
      .appName("SparkLR")
      .getOrCreate()

    val numSlices = if (args.length > 0) args(0).toInt else 2
    val points = spark.sparkContext.parallelize(generateData, numSlices).cache()

    // Initialize w to a random value
    var w = DenseVector.fill(D) {2 * rand.nextDouble - 1}
    println("Initial w: " + w)

    for (i <- 1 to ITERATIONS) {
      println("On iteration " + i)
      val gradient = points.map { p =>
        p.x * (1 / (1 + exp(-p.y * (w.dot(p.x)))) - 1) * p.y
      }.reduce(_ + _)
      w -= gradient
    }

    println("Final w: " + w)

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import org.apache.spark.sql.SparkSession

/**
 * Computes the PageRank of URLs from an input file. Input file should
 * be in format of:
 * URL         neighbor URL
 * URL         neighbor URL
 * URL         neighbor URL
 * ...
 * where URL and their neighbors are separated by space(s).
 *
 * This is an example implementation for learning how to use Spark. For more conventional use,
 * please refer to org.apache.spark.graphx.lib.PageRank
 *
 * Example Usage:
 * {{{
 * bin/run-example SparkPageRank data/mllib/pagerank_data.txt 10
 * }}}
 */
object SparkPageRank {

  def showWarning() {
    System.err.println(
      """WARN: This is a naive implementation of PageRank and is given as an example!
        |Please use the PageRank implementation found in org.apache.spark.graphx.lib.PageRank
        |for more conventional use.
      """.stripMargin)
  }

  def main(args: Array[String]) {
    if (args.length < 1) {
      System.err.println("Usage: SparkPageRank <file> <iter>")
      System.exit(1)
    }

    showWarning()

    val spark = SparkSession
      .builder
      .appName("SparkPageRank")
      .getOrCreate()

    val iters = if (args.length > 1) args(1).toInt else 10
    val lines = spark.read.textFile(args(0)).rdd
    val links = lines.map{ s =>
      val parts = s.split("\\s+")
      (parts(0), parts(1))
    }.distinct().groupByKey().cache()
    var ranks = links.mapValues(v => 1.0)

    for (i <- 1 to iters) {
      val contribs = links.join(ranks).values.flatMap{ case (urls, rank) =>
        val size = urls.size
        urls.map(url => (url, rank / size))
      }
      ranks = contribs.reduceByKey(_ + _).mapValues(0.15 + 0.85 * _)
    }

    val output = ranks.collect()
    output.foreach(tup => println(tup._1 + " has rank: " + tup._2 + "."))

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import scala.math.random

import org.apache.spark.sql.SparkSession

/** Computes an approximation to pi */
object SparkPi {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("Spark Pi")
      .getOrCreate()
    val slices = if (args.length > 0) args(0).toInt else 2
    val n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow
    val count = spark.sparkContext.parallelize(1 until n, slices).map { i =>
      val x = random * 2 - 1
      val y = random * 2 - 1
      if (x*x + y*y < 1) 1 else 0
    }.reduce(_ + _)
    println("Pi is roughly " + 4.0 * count / (n - 1))
    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.examples.sql

// $example on:schema_inferring$
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.Encoder
// $example off:schema_inferring$
import org.apache.spark.sql.Row
// $example on:init_session$
import org.apache.spark.sql.SparkSession
// $example off:init_session$
// $example on:programmatic_schema$
// $example on:data_types$
import org.apache.spark.sql.types._
// $example off:data_types$
// $example off:programmatic_schema$

object SparkSQLExample {

  // $example on:create_ds$
  // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,
  // you can use custom classes that implement the Product interface
  case class Person(name: String, age: Long)
  // $example off:create_ds$

  def main(args: Array[String]) {
    // $example on:init_session$
    val spark = SparkSession
      .builder()
      .appName("Spark SQL basic example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    // For implicit conversions like converting RDDs to DataFrames
    import spark.implicits._
    // $example off:init_session$

    runBasicDataFrameExample(spark)
    runDatasetCreationExample(spark)
    runInferSchemaExample(spark)
    runProgrammaticSchemaExample(spark)

    spark.stop()
  }

  private def runBasicDataFrameExample(spark: SparkSession): Unit = {
    // $example on:create_df$
    val df = spark.read.json("examples/src/main/resources/people.json")

    // Displays the content of the DataFrame to stdout
    df.show()
    // +----+-------+
    // | age|   name|
    // +----+-------+
    // |null|Michael|
    // |  30|   Andy|
    // |  19| Justin|
    // +----+-------+
    // $example off:create_df$

    // $example on:untyped_ops$
    // This import is needed to use the $-notation
    import spark.implicits._
    // Print the schema in a tree format
    df.printSchema()
    // root
    // |-- age: long (nullable = true)
    // |-- name: string (nullable = true)

    // Select only the "name" column
    df.select("name").show()
    // +-------+
    // |   name|
    // +-------+
    // |Michael|
    // |   Andy|
    // | Justin|
    // +-------+

    // Select everybody, but increment the age by 1
    df.select($"name", $"age" + 1).show()
    // +-------+---------+
    // |   name|(age + 1)|
    // +-------+---------+
    // |Michael|     null|
    // |   Andy|       31|
    // | Justin|       20|
    // +-------+---------+

    // Select people older than 21
    df.filter($"age" > 21).show()
    // +---+----+
    // |age|name|
    // +---+----+
    // | 30|Andy|
    // +---+----+

    // Count people by age
    df.groupBy("age").count().show()
    // +----+-----+
    // | age|count|
    // +----+-----+
    // |  19|    1|
    // |null|    1|
    // |  30|    1|
    // +----+-----+
    // $example off:untyped_ops$

    // $example on:run_sql$
    // Register the DataFrame as a SQL temporary view
    df.createOrReplaceTempView("people")

    val sqlDF = spark.sql("SELECT * FROM people")
    sqlDF.show()
    // +----+-------+
    // | age|   name|
    // +----+-------+
    // |null|Michael|
    // |  30|   Andy|
    // |  19| Justin|
    // +----+-------+
    // $example off:run_sql$

    // $example on:global_temp_view$
    // Register the DataFrame as a global temporary view
    df.createGlobalTempView("people")

    // Global temporary view is tied to a system preserved database `global_temp`
    spark.sql("SELECT * FROM global_temp.people").show()
    // +----+-------+
    // | age|   name|
    // +----+-------+
    // |null|Michael|
    // |  30|   Andy|
    // |  19| Justin|
    // +----+-------+

    // Global temporary view is cross-session
    spark.newSession().sql("SELECT * FROM global_temp.people").show()
    // +----+-------+
    // | age|   name|
    // +----+-------+
    // |null|Michael|
    // |  30|   Andy|
    // |  19| Justin|
    // +----+-------+
    // $example off:global_temp_view$
  }

  private def runDatasetCreationExample(spark: SparkSession): Unit = {
    import spark.implicits._
    // $example on:create_ds$
    // Encoders are created for case classes
    val caseClassDS = Seq(Person("Andy", 32)).toDS()
    caseClassDS.show()
    // +----+---+
    // |name|age|
    // +----+---+
    // |Andy| 32|
    // +----+---+

    // Encoders for most common types are automatically provided by importing spark.implicits._
    val primitiveDS = Seq(1, 2, 3).toDS()
    primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)

    // DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name
    val path = "examples/src/main/resources/people.json"
    val peopleDS = spark.read.json(path).as[Person]
    peopleDS.show()
    // +----+-------+
    // | age|   name|
    // +----+-------+
    // |null|Michael|
    // |  30|   Andy|
    // |  19| Justin|
    // +----+-------+
    // $example off:create_ds$
  }

  private def runInferSchemaExample(spark: SparkSession): Unit = {
    // $example on:schema_inferring$
    // For implicit conversions from RDDs to DataFrames
    import spark.implicits._

    // Create an RDD of Person objects from a text file, convert it to a Dataframe
    val peopleDF = spark.sparkContext
      .textFile("examples/src/main/resources/people.txt")
      .map(_.split(","))
      .map(attributes => Person(attributes(0), attributes(1).trim.toInt))
      .toDF()
    // Register the DataFrame as a temporary view
    peopleDF.createOrReplaceTempView("people")

    // SQL statements can be run by using the sql methods provided by Spark
    val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")

    // The columns of a row in the result can be accessed by field index
    teenagersDF.map(teenager => "Name: " + teenager(0)).show()
    // +------------+
    // |       value|
    // +------------+
    // |Name: Justin|
    // +------------+

    // or by field name
    teenagersDF.map(teenager => "Name: " + teenager.getAs[String]("name")).show()
    // +------------+
    // |       value|
    // +------------+
    // |Name: Justin|
    // +------------+

    // No pre-defined encoders for Dataset[Map[K,V]], define explicitly
    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
    // Primitive types and case classes can be also defined as
    // implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()

    // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]
    teenagersDF.map(teenager => teenager.getValuesMap[Any](List("name", "age"))).collect()
    // Array(Map("name" -> "Justin", "age" -> 19))
    // $example off:schema_inferring$
  }

  private def runProgrammaticSchemaExample(spark: SparkSession): Unit = {
    import spark.implicits._
    // $example on:programmatic_schema$
    // Create an RDD
    val peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")

    // The schema is encoded in a string
    val schemaString = "name age"

    // Generate the schema based on the string of schema
    val fields = schemaString.split(" ")
      .map(fieldName => StructField(fieldName, StringType, nullable = true))
    val schema = StructType(fields)

    // Convert records of the RDD (people) to Rows
    val rowRDD = peopleRDD
      .map(_.split(","))
      .map(attributes => Row(attributes(0), attributes(1).trim))

    // Apply the schema to the RDD
    val peopleDF = spark.createDataFrame(rowRDD, schema)

    // Creates a temporary view using the DataFrame
    peopleDF.createOrReplaceTempView("people")

    // SQL can be run over a temporary view created using DataFrames
    val results = spark.sql("SELECT name FROM people")

    // The results of SQL queries are DataFrames and support all the normal RDD operations
    // The columns of a row in the result can be accessed by field index or by field name
    results.map(attributes => "Name: " + attributes(0)).show()
    // +-------------+
    // |        value|
    // +-------------+
    // |Name: Michael|
    // |   Name: Andy|
    // | Name: Justin|
    // +-------------+
    // $example off:programmatic_schema$
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples

import scala.collection.mutable
import scala.util.Random

import org.apache.spark.sql.SparkSession

/**
 * Transitive closure on a graph.
 */
object SparkTC {
  val numEdges = 200
  val numVertices = 100
  val rand = new Random(42)

  def generateGraph: Seq[(Int, Int)] = {
    val edges: mutable.Set[(Int, Int)] = mutable.Set.empty
    while (edges.size < numEdges) {
      val from = rand.nextInt(numVertices)
      val to = rand.nextInt(numVertices)
      if (from != to) edges.+=((from, to))
    }
    edges.toSeq
  }

  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("SparkTC")
      .getOrCreate()
    val slices = if (args.length > 0) args(0).toInt else 2
    var tc = spark.sparkContext.parallelize(generateGraph, slices).cache()

    // Linear transitive closure: each round grows paths by one edge,
    // by joining the graph's edges with the already-discovered paths.
    // e.g. join the path (y, z) from the TC with the edge (x, y) from
    // the graph to obtain the path (x, z).

    // Because join() joins on keys, the edges are stored in reversed order.
    val edges = tc.map(x => (x._2, x._1))

    // This join is iterated until a fixed point is reached.
    var oldCount = 0L
    var nextCount = tc.count()
    do {
      oldCount = nextCount
      // Perform the join, obtaining an RDD of (y, (z, x)) pairs,
      // then project the result to obtain the new (x, z) paths.
      tc = tc.union(tc.join(edges).map(x => (x._2._2, x._2._1))).distinct().cache()
      nextCount = tc.count()
    } while (nextCount != oldCount)

    println("TC has " + tc.count() + " edges.")
    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.alarm

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-01-31 11:43
  *
  */
object SparkTest extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._

    val sc = sparkSession.sparkContext
    ////////////////////////////////////////////////

    //    sparkSession.read.parquet("data/realinfo/*.parquet")
    //
    //      .filter(_.getAs[String]("VIN").equals("#"))
    //      .show(false)


    //    val a = 546D
    //
    //    val b = 345D
    //
    //    val res1 = (a/1000).toInt
    //    val res2 = (b/1000).toInt
    //
    //    println(res1)
    //    println(res2)


    val initRDD = sc.textFile("data/one2manyterminal/*.json").filter(x => {

      //      //运通125
      //      val arr = Array(
      //        "LZYTBGBW4F1052790",
      //        "LZYTBGBW3F1052781",
      //        "LZYTBGBW5F1052782",
      //        "LZYTBGBW0F1052785",
      //        "LZYTBGBW4F1052787",
      //        "LZYTBGBW3F1038041",
      //        "LZYTBGBW6F1052788",
      //        "LZYTBGBW9F1052784",
      //        "LZYTBGBW2F1052786",
      //        "LZYTBGBW5F1052779",
      //        "LZYTBGBW7F1052783",
      //        "LZYTBGBW1F1052780",
      //        "LZYTBGBW8F1052789",
      //        "LZYTBGBW9E1023896",
      //        "LZYTBGBW0E1023897"
      //
      //      )
      //
      //
      //      //"51路
      //      val arr = Array(
      //        "LVCB4L4D2HM002833",
      //        "LVCB4L4D4HM002803",
      //        "LVCB4L4D9HM002862",
      //        "LVCB4L4D3HM002839",
      //        "LVCB4L4DXHM002840",
      //        "LVCB4L4D0HM002832",
      //        "LVCB4L4D7HM002830",
      //        "LVCB4L4D7HM002858",
      //        "LVCB4L4D1HM002841",
      //        "LVCB4L4D0HM002829",
      //        "LVCB4L4D6HM002835",
      //        "LC06S24K8H1990449"
      //      )
      //
      //      //"652路
      //val arr = Array(
      //  "L9GCBF7D3G2005879",
      //  "L9GCBF7DXG2005894",
      //  "L9GCBF7D6G2005889",
      //  "L9GCBF7D6G2005892",
      //  "L9GCBF7D2G2005887",
      //  "L9GCBF7D1G2005900",
      //  "L9GCBF7D3G2005896",
      //  "L9GCBF7D8G2005876"
      //)

      //      //665路
      //val arr = Array(
      //  "LVCB4L4D5HM003880",
      //  "LVCB4L4D1HM003892",
      //  "LVCB4L4D7HM003895",
      //  "LVCB4L4D5HM002485",
      //  "LVCB4L4D5HM003894",
      //  "LVCB4L4D6HM003886",
      //  "LVCB4L4D0HM003897",
      //  "LVCB4L4D3HM003893"
      //)
      //
      //      //511路
      //      val arr = Array(
      //        "LVCB4L4D2HM004243",
      //        "LVCB4L4D8HM004229",
      //        "LVCB4L4D8HM004263",
      //        "LVCB4L4D4HM004230",
      //        "LVCB4L4D1HM004265",
      //        "LVCB4L4D6HM004228",
      //        "LVCB4L4D0HM004256",
      //        "LVCB4L4D8HM004246",
      //        "LVCB4L4D9HM004255",
      //        "LVCB4L4D5HM004253",
      //        "LVCB4L4DXHM004264",
      //        "LVCB4L4D4HM004258"
      //      )
      //
      //      //133路
      val arr = Array(
        "LVCB4L4DXHM004409",
        "LVCB4L4D0HM004399",
        "LVCB4L4D5HM004379",
        "LVCB4L4D4HM004406",
        "LVCB4L4D0HM004385",
        "LVCB4L4D8HM004411",
        "LVCB4L4D8HM004392",
        "LVCB4L4D6HM004374"
      )


      var flag = false
      arr.foreach(vin => {
        if (x.contains(vin) && !flag) {
          flag = true
        }
      })

      flag
    })

    sparkSession.read.json(initRDD).rdd.map(row => row.getAs[String]("vin")).distinct.foreach(println)

    sparkSession.stop()
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.log4j.{Level, Logger}
import scopt.OptionParser

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.classification.NaiveBayes
import org.apache.spark.mllib.util.MLUtils

/**
 * An example naive Bayes app. Run with
 * {{{
 * ./bin/run-example org.apache.spark.examples.mllib.SparseNaiveBayes [options] <input>
 * }}}
 * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
 */
object SparseNaiveBayes {

  case class Params(
      input: String = null,
      minPartitions: Int = 0,
      numFeatures: Int = -1,
      lambda: Double = 1.0) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    val parser = new OptionParser[Params]("SparseNaiveBayes") {
      head("SparseNaiveBayes: an example naive Bayes app for LIBSVM data.")
      opt[Int]("numPartitions")
        .text("min number of partitions")
        .action((x, c) => c.copy(minPartitions = x))
      opt[Int]("numFeatures")
        .text("number of features")
        .action((x, c) => c.copy(numFeatures = x))
      opt[Double]("lambda")
        .text(s"lambda (smoothing constant), default: ${defaultParams.lambda}")
        .action((x, c) => c.copy(lambda = x))
      arg[String]("<input>")
        .text("input paths to labeled examples in LIBSVM format")
        .required()
        .action((x, c) => c.copy(input = x))
    }

    parser.parse(args, defaultParams) match {
      case Some(params) => run(params)
      case _ => sys.exit(1)
    }
  }

  def run(params: Params): Unit = {
    val conf = new SparkConf().setAppName(s"SparseNaiveBayes with $params")
    val sc = new SparkContext(conf)

    Logger.getRootLogger.setLevel(Level.WARN)

    val minPartitions =
      if (params.minPartitions > 0) params.minPartitions else sc.defaultMinPartitions

    val examples =
      MLUtils.loadLibSVMFile(sc, params.input, params.numFeatures, minPartitions)
    // Cache examples because it will be used in both training and evaluation.
    examples.cache()

    val splits = examples.randomSplit(Array(0.8, 0.2))
    val training = splits(0)
    val test = splits(1)

    val numTraining = training.count()
    val numTest = test.count()

    println(s"numTraining = $numTraining, numTest = $numTest.")

    val model = new NaiveBayes().setLambda(params.lambda).run(training)

    val prediction = model.predict(test.map(_.features))
    val predictionAndLabel = prediction.zip(test.map(_.label))
    val accuracy = predictionAndLabel.filter(x => x._1 == x._2).count().toDouble / numTest

    println(s"Test accuracy = $accuracy.")

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.alarm

import java.text.SimpleDateFormat

import com.bitnei.alarm.generator.{RunStateInput, RunStateOrbitSplitGenerator}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.util.GeoUtils
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

import scala.annotation.tailrec
import scala.collection.mutable
import scala.collection.mutable.{ArrayBuffer, ListBuffer}

/**
  *
  * @author zhangyongtian
  * @define 纯电动乘用车不同车速下的电耗明细
  *
  *
  * *
  * 年月	省	市
  * *
  * 类型 私家车 出租车 共享汽车等
  * *
  * 驱动类型 纯电动
  * *
  * 第一部分分类内容 各个分类 各属性
  * *
  * =============================
  * *
  * #统计方法：
  * 删除为0的数据；
  * 只统计连续有效数据不间断的；
  * 连续数据取值范围是指最少具备连续起止两条有效数据值；
  * 将有效值累计
  * *
  *
  * 车速30km/h以下累计行驶时间
  * *
  * 车速30km/h以下累计行驶里程
  * *
  * 车速30km/h以下累计消耗SOC数值
  * *
  * 车速31-50km/h累计行驶时间
  * *
  * 车速31-50km/h累计行驶里程
  * *
  * 车速31-50km/h累计消耗SOC数值
  * *
  * 车速51-80km/h累计行驶时间
  * *
  * 车速51-80km/h累计行驶里程
  * *
  * 车速51-80km/h累计消耗SOC数值
  * *
  * 车速80km/h以上累计行驶时间
  * *
  * 车速80km/h以上累计行驶里程
  * *
  * 车速80km/h以上累计消耗SOC数值
  * *
  * 车速80km/h以下累计行驶时间
  * *
  * 车速80km/h以下累计行驶里程
  * *
  * 车速80km/h以下累计消耗SOC数值
  * *
  * 月度百公里耗电量(Kw.h)
  * *
  * 输出
  * *
  *
  * 车速区间
  * 行驶时间
  * 行驶里程
  * 消耗SOC数值
  * *
  *
  *
  * 月度百公里耗电量(Kw.h)
  * 按月度统计平均值
  *
  *
  *
  */


object SpeedPowerConsumer extends Serializable with Logging {

  def getSpeedInterval(SPEED_INTERVALS: Map[String, Array[Double]], speed: Double): String = {

    var res = "#"

    SPEED_INTERVALS.foreach(x => {
      val arr = x._2
      if (speed >= arr(0) && speed <= arr(1)) {
        res = x._1
      }
    })

    res

  }

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //输入参数
    //时间参数 20170111
    var date = stateConf.getOption("input.date").getOrElse("20180216")

    //参数校验
    if (date.length != 8) {
      throw new Exception("input.date error")
    }
    val year = date.substring(0, 4)
    val month = date.substring(4, 6)
    val day = date.substring(6)

    val geoHashLen = stateConf.getOption("input.geohash.len").getOrElse("8").toInt

    val sampleDiffMileageRatio = stateConf.getOption("input.sample.mileage.ratio").getOrElse("10").toInt

    val sampleTimeThreshold = stateConf.getOption("input.sample.time.ratio").getOrElse("30").toInt

    //输出参数
    var outputTargets = stateConf.getOption("output").getOrElse("console")
    var outFormat = stateConf.getOption("output.format").getOrElse("#")
    var hdfsPath = s"${stateConf.getString("output.hdfs.path")}/year=${year}/month=${month}/day=${day}"


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
    ////////////////////////////////////////////////

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        //        sparkSession.read.json("data/realinfo/mock.txt").createOrReplaceTempView("realinfo")
        sparkSession.read.json("data/realinfo/20180216-2-one-car.json").createOrReplaceTempView("realinfo")
        //                sparkSession.read.json("data/realinfo/20171102/*.parquet").createOrReplaceTempView("realinfo")

      }

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}").createOrReplaceTempView("realinfo")

        //        val initDs = sparkSession
        //          .read
        //          .format("parquet")
        //          .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}/day=${day}")
        //
        //        initDs.filter(x=>{
        //          val arr= Array("LVCB4L4D8HM004392","LVCB4L4D5HM004379")
        //          val vin = x.getAs[String]("VIN")
        //          arr.contains(vin)
        //        }).toJSON.repartition(1).write.json("/tmp/similar_realinfo")

      }

    }


    ////////////////////////////////////////业务逻辑//////////////////////////////
    //时间格式化
    val sdf = new SimpleDateFormat("yyyyMMddHHmmss")

    // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'


    val sql =
      s"""
        SELECT
        VID,
        VIN,
        `2000` AS TIME,
        `2201` AS speed,
        `2202` As mileage,
        `2615` As soc,
        `3201` As isstart
        FROM realinfo
        where VID is not null and `2000` like '${date}%'  and `2201` is not null and `2202` is not null  and `2615` is not null  and `3201` is not null
      """.stripMargin

    var initDS = sparkSession.sql(sql).as[(String, String, String, String, String, String, String)]

    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {
        true
      })


    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val vin = x._2
          val time = x._3
          val speed = x._4.toString.toDouble / 10
          val mileage = x._5.toString.toDouble / 10

          val soc = x._6
          val isstart = x._7 //车辆状态 1.启动 2.熄火

          Input(vid, vin, time, speed, mileage, soc, isstart)
        })


    val result = mappedDS
      .groupByKey(_.vid)
      .flatMapGroups {
        case (vid, inputs) => {
          val arr = inputs.toArray.sortBy(_.time)

          //          arr.foreach(x => println(x.soc))

          val vin = arr.head.vin

          //TODO:轨迹切分（行驶状态）
          val runstateInputs = arr.map(x => RunStateInput(x.vid, x.vin, x.time, x.speed, x.mileage, x.soc.toDouble, x.isstart.toInt))

          val runPoints =
            RunStateOrbitSplitGenerator.handle(runstateInputs)

          //          val SPEED_INTERVALS = Array(
          //            SPEED_INTERVALS(0)
          //            SPEED_INTERVALS(0)
          //            SPEED_INTERVALS(0)
          //          )


          val SPEED_INTERVALS_MAP = Map[String, Array[Double]](
            "0-30" -> Array(0.001D, 30D),
            "31-50" -> Array(31D, 50D),
            "51-80" -> Array(51D, 80D),
            "81-" -> Array(81D, Integer.MAX_VALUE)
          )

          val SPEED_INTERVALS = SPEED_INTERVALS_MAP.keySet.toArray


          //          val res = scala.collection.mutable.Map[String, Output]()


          val res = new ArrayBuffer[Output]()

          //          SPEED_INTERVALS.foreach(x => {
          //            val runDuration = 0D
          //            val runMileage = 0D
          //            val consumSoc = 0D
          //
          //            val output = Output(vid, vin, x._1, runDuration, runMileage, consumSoc)
          //            res += (x._1 -> output)
          //          })


          var runDuration0T30 = 0D
          var runMileage0T30 = 0D
          var consumSoc0T30 = 0D

          var runDuration31T50 = 0D
          var runMileage31T50 = 0D
          var consumSoc31T50 = 0D

          var runDuration51T80 = 0D
          var runMileage51T80 = 0D
          var consumSoc51T80 = 0D

          var runDuration81 = 0D
          var runMileage81 = 0D
          var consumSoc81 = 0D


          runPoints.foreach(orbit => {

            //如果跨区间，算后一帧的区间
            orbit.reduce((pre, post) => {

              val speedInterval: String = getSpeedInterval(SPEED_INTERVALS_MAP, post.speed)

              var diffMileage = post.mileage - pre.mileage

              val lastTimesatmp = sdf.parse(pre.time).getTime
              val curTimesatmp = sdf.parse(post.time).getTime
              val diffTimes = (curTimesatmp - lastTimesatmp).toDouble / 1000


              val diffSoc = pre.soc - post.soc

              if (diffMileage > 0 && diffSoc >= 0) {

                if (speedInterval.equals(SPEED_INTERVALS(0))) {
                  runDuration0T30 = runDuration0T30 + diffTimes
                  runMileage0T30 = runMileage0T30 + diffMileage
                  consumSoc0T30 = consumSoc0T30 + diffSoc
                }

                if (speedInterval.equals(SPEED_INTERVALS(1))) {
                  runDuration31T50 += diffTimes
                  runMileage31T50 += diffMileage
                  consumSoc31T50 += +diffSoc
                }

                if (speedInterval.equals(SPEED_INTERVALS(2))) {
                  runDuration51T80 += diffTimes
                  runMileage51T80 += diffMileage
                  consumSoc51T80 += diffSoc
                }

                if (speedInterval.equals(SPEED_INTERVALS(3))) {
                  runDuration81 += diffTimes
                  runMileage81 += diffMileage
                  consumSoc81 += diffSoc
                }


              }

              post
            })

          })


          val formatter = "%1.3f"

          res += Output(vid, vin, date, SPEED_INTERVALS(0), runDuration0T30, formatter.format(runMileage0T30).toDouble, consumSoc0T30)
          res += Output(vid, vin, date, SPEED_INTERVALS(1), runDuration31T50, formatter.format(runMileage31T50).toDouble, consumSoc31T50)
          res += Output(vid, vin, date, SPEED_INTERVALS(2), runDuration51T80, formatter.format(runMileage51T80).toDouble, consumSoc51T80)
          res += Output(vid, vin, date, SPEED_INTERVALS(3), runDuration81, formatter.format(runMileage81).toDouble, consumSoc81)

          //          * 车速区间
          //            * 行驶时间
          //            * 行驶里程
          //            * 消耗SOC数值
          res.toList
        }
      }


    //    runPointDS.groupByKey(_.head.vin)
    //      .mapGroups {
    //        case (vin, runPoints) => {
    //
    //
    //        }
    //      }


    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      //      parkingInfoDS.show(false)
      result.show(false)
      //      result.count()
    }

    if (env.equals("prd")) {
      //TODO: 输出到HDFS
      logInfo("输出到HDFS　start....")

      if (outputTargets.contains("hdfs")) {

        result.repartition(1).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)

        //      result.repartition(8).write.format(outFormat).mode(SaveMode.Overwrite).save(hdfsPath)
        //        parkingInfoDS.toJSON.write.format(outFormat).mode(SaveMode.Overwrite).save(s"/spark/vehicle/result/parkingheat/year=${year}/month=${month}/day=${day}")
      }

    }

    sparkSession.stop()
  }


  case class Input(vid: String, vin: String, time: String, speed: Double, mileage: Double, soc: String, isstart: String)


  case class Output(vid: String, vin: String, date: String, speedInterval: String, runDuration: Double, runMileage: Double, consumSoc: Double)

}package com.bitnei.report.tempjob


import com.bitnei.report.common.configuration.StateConf
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.mapreduce.lib.input.{FileInputFormat, TextInputFormat}
import org.apache.hadoop.mapreduce.task.{JobContextImpl, TaskAttemptContextImpl}
import org.apache.hadoop.mapreduce._

/*
* created by wangbaosheng on 2017/12/26
*/
object SplitFile {
  def main(args: Array[String]): Unit = {
    val stateConf=new StateConf
    stateConf.add(args)

    val inputFormat = new TextInputFormat()

    val jobConf = new Configuration(true)
    jobConf.addResource(new Path("file:///etc/hadoop/conf/core-site.xml"))
    jobConf.addResource(new Path("file:///etc/hadoop/conf/hdfs-site.xml"))

    val fs=FileSystem.get(jobConf)
    //fs.listStatus(new Path(stateConf.getString("input.file")))

    //获取一个job
    val job = Job.getInstance(jobConf)
    //创建job上下文
    FileInputFormat.setInputPaths(job,new Path(stateConf.getString("input.file")))
    val jobContext = new JobContextImpl(jobConf, job.getJobID)
    //获取splits
    val inputSplits = inputFormat.getSplits(jobContext)

    val iter = inputSplits.iterator()

    var taskId=0
    while (iter.hasNext) {
      val inputSplit = iter.next()
      //创建任务id
      val attemptId = new TaskAttemptID("my first job", job.getJobID.getId, TaskType.MAP, taskId, 0)
      //创建任务上下文
      val hadoopAttemptContext = new TaskAttemptContextImpl(jobConf, attemptId)
      //获取split对应的reader
      val reader = inputFormat.createRecordReader(inputSplit, hadoopAttemptContext)
      println(s"the $taskId th input split")
      inputSplit.getLocationInfo.foreach(location=>{
        println(s"${location.getLocation},${location.isOnDisk},${location.isInMemory}")
      })

      while (reader.nextKeyValue()) {
        val key = reader.getCurrentKey
        val value = reader.getCurrentValue

        println(key, value)
      }

      taskId+=1
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.examples.sql

import java.util.Properties

import org.apache.spark.sql.SparkSession

object SQLDataSourceExample {

  case class Person(name: String, age: Long)

  def main(args: Array[String]) {
    val spark = SparkSession
      .builder()
      .appName("Spark SQL data sources example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    runBasicDataSourceExample(spark)
    runBasicParquetExample(spark)
    runParquetSchemaMergingExample(spark)
    runJsonDatasetExample(spark)
    runJdbcDatasetExample(spark)

    spark.stop()
  }

  private def runBasicDataSourceExample(spark: SparkSession): Unit = {
    // $example on:generic_load_save_functions$
    val usersDF = spark.read.load("examples/src/main/resources/users.parquet")
    usersDF.select("name", "favorite_color").write.save("namesAndFavColors.parquet")
    // $example off:generic_load_save_functions$
    // $example on:manual_load_options$
    val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")
    peopleDF.select("name", "age").write.format("parquet").save("namesAndAges.parquet")
    // $example off:manual_load_options$
    // $example on:direct_sql$
    val sqlDF = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`")
    // $example off:direct_sql$
  }

  private def runBasicParquetExample(spark: SparkSession): Unit = {
    // $example on:basic_parquet_example$
    // Encoders for most common types are automatically provided by importing spark.implicits._
    import spark.implicits._

    val peopleDF = spark.read.json("examples/src/main/resources/people.json")

    // DataFrames can be saved as Parquet files, maintaining the schema information
    peopleDF.write.parquet("people.parquet")

    // Read in the parquet file created above
    // Parquet files are self-describing so the schema is preserved
    // The result of loading a Parquet file is also a DataFrame
    val parquetFileDF = spark.read.parquet("people.parquet")

    // Parquet files can also be used to create a temporary view and then used in SQL statements
    parquetFileDF.createOrReplaceTempView("parquetFile")
    val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")
    namesDF.map(attributes => "Name: " + attributes(0)).show()
    // +------------+
    // |       value|
    // +------------+
    // |Name: Justin|
    // +------------+
    // $example off:basic_parquet_example$
  }

  private def runParquetSchemaMergingExample(spark: SparkSession): Unit = {
    // $example on:schema_merging$
    // This is used to implicitly convert an RDD to a DataFrame.
    import spark.implicits._

    // Create a simple DataFrame, store into a partition directory
    val squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i => (i, i * i)).toDF("value", "square")
    squaresDF.write.parquet("data/test_table/key=1")

    // Create another DataFrame in a new partition directory,
    // adding a new column and dropping an existing column
    val cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i => (i, i * i * i)).toDF("value", "cube")
    cubesDF.write.parquet("data/test_table/key=2")

    // Read the partitioned table
    val mergedDF = spark.read.option("mergeSchema", "true").parquet("data/test_table")
    mergedDF.printSchema()

    // The final schema consists of all 3 columns in the Parquet files together
    // with the partitioning column appeared in the partition directory paths
    // root
    //  |-- value: int (nullable = true)
    //  |-- square: int (nullable = true)
    //  |-- cube: int (nullable = true)
    //  |-- key: int (nullable = true)
    // $example off:schema_merging$
  }

  private def runJsonDatasetExample(spark: SparkSession): Unit = {
    // $example on:json_dataset$
    // A JSON dataset is pointed to by path.
    // The path can be either a single text file or a directory storing text files
    val path = "examples/src/main/resources/people.json"
    val peopleDF = spark.read.json(path)

    // The inferred schema can be visualized using the printSchema() method
    peopleDF.printSchema()
    // root
    //  |-- age: long (nullable = true)
    //  |-- name: string (nullable = true)

    // Creates a temporary view using the DataFrame
    peopleDF.createOrReplaceTempView("people")

    // SQL statements can be run by using the sql methods provided by spark
    val teenagerNamesDF = spark.sql("SELECT name FROM people WHERE age BETWEEN 13 AND 19")
    teenagerNamesDF.show()
    // +------+
    // |  name|
    // +------+
    // |Justin|
    // +------+

    // Alternatively, a DataFrame can be created for a JSON dataset represented by
    // an RDD[String] storing one JSON object per string
    val otherPeopleRDD = spark.sparkContext.makeRDD(
      """{"name":"Yin","address":{"city":"Columbus","state":"Ohio"}}""" :: Nil)
    val otherPeople = spark.read.json(otherPeopleRDD)
    otherPeople.show()
    // +---------------+----+
    // |        address|name|
    // +---------------+----+
    // |[Columbus,Ohio]| Yin|
    // +---------------+----+
    // $example off:json_dataset$
  }

  private def runJdbcDatasetExample(spark: SparkSession): Unit = {
    // $example on:jdbc_dataset$
    // Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods
    // Loading data from a JDBC source
    val jdbcDF = spark.read
      .format("jdbc")
      .option("url", "jdbc:postgresql:dbserver")
      .option("dbtable", "schema.tablename")
      .option("user", "username")
      .option("password", "password")
      .load()

    val connectionProperties = new Properties()
    connectionProperties.put("user", "username")
    connectionProperties.put("password", "password")
    val jdbcDF2 = spark.read
      .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)

    // Saving data to a JDBC source
    jdbcDF.write
      .format("jdbc")
      .option("url", "jdbc:postgresql:dbserver")
      .option("dbtable", "schema.tablename")
      .option("user", "username")
      .option("password", "password")
      .save()

    jdbcDF2.write
      .jdbc("jdbc:postgresql:dbserver", "schema.tablename", connectionProperties)
    // $example off:jdbc_dataset$
  }
}
package com.bitnei.samples.sql

import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define
  *
  * date: ${date}
  *
  */
object SQLFunctionsTest {

  def main(args: Array[String]): Unit = {


    // TODO: 日志级别设置
    Logger.getLogger("org").setLevel(Level.ERROR)


    val spark = SparkSession
      .builder()
      .appName(this.getClass.getSimpleName)
      .config("spark.some.config.option", "some-value")
      .master("local[*]")
      .getOrCreate()


    spark.read.json("data/date.json").createTempView("tmp")

    //date_format(from_unixtime(time), 'yyyyMM'))
    spark.sql("select date_format(from_unixtime(time/1000), 'yyyyMMdd')  from tmp").show(false)


    spark.sql("select CAST(COUNT(distinct date_format(from_unixtime(time/1000), 'yyyyMMdd')) AS int)  from tmp").show(false)


    spark.stop()

  }
}
package com.bitnei.sparkhelper

import com.bitnei.report.common.configuration.StateConf

/**
  * Created by wangbaosheng on 2017/10/19.
  */
object SqlHelper {
   def buildSqlText(tableInfo: TableInfo):String={
    val where = buildWhereConditionBasePartitionColumn(tableInfo) match {
      case Some(whereCondition) => s"where $whereCondition"
      case None => ""
    }
    val sql = s"select * from ${tableInfo.tableName} $where"
    sql
  }



  def buildWhere(stateConf:StateConf):String= {
    val filter = buildWhereCondition(stateConf)
    val whereFilter = if (filter.trim.nonEmpty) s"WHERE ($filter)" else ""
    whereFilter
  }

  def buildWhereCondition(stateConf:StateConf):String={
    val dateList = stateConf.getOption("report.date") match {
      case Some(reportDateList) => reportDateList.split('-')
      case None => throw new Exception("report.date error")
    }


    val reportType=stateConf.getOption("reportType").getOrElse("day")
    val filter=dateList.map(date=>{
      if(reportType=="day") {
        val year = date.substring(0, 4)
        val month = date.substring(4, 6)
        val day = date.substring(6, 8)
        s"(year=$year AND month=$month AND day=$day)"
      }else if(reportType=="month") {
        val year = date.substring(0, 4)
        val month = date.substring(4, 6)
        s"(year=$year AND month=$month)"
      }else {
        val year = date.substring(0, 4)
        s"(year=$year)"
      }
    }).reduce((a,b)=>s"$a OR $b")


    filter
  }

  def buildWhereConditionBasePartitionColumn(tableInfo:TableInfo):Option[String]={
//     val stringBuilder = new StringBuilder
//     var break = false
//
//     for ((partitionColumnName, partitionColumnValue) <- tableInfo.partitionColumn.zip(tableInfo.partitionValue) if !break) {
//       if (partitionColumnName.trim.nonEmpty && partitionColumnValue.trim.nonEmpty)
//         stringBuilder.append(s"$partitionColumnName=$partitionColumnValue")
//       else {
//         break = true
//       }
//     }
//     if (stringBuilder.nonEmpty) Some(stringBuilder.toString()) else None
//

     buildWhereConditionBasePartitionColumn(tableInfo,(partitionColumnName,partitionColumnValue)=>{
       s"$partitionColumnName=$partitionColumnValue"
     })
  }

  def buildWhereConditionBasePartitionColumn(tableInfo:TableInfo,f:(String,String)=>String):Option[String]={
    tableInfo.partitionColumn.zip(tableInfo.partitionValue).map(partitionInfo => {
      val partitionColumnName = partitionInfo._1
      val partitionColumnValue = partitionInfo._2
      f(partitionColumnName,partitionColumnValue)
    }).reduceOption((a, b) => s"$a AND $b")
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext, Time}

/**
 * Use DataFrames and SQL to count words in UTF8 encoded, '\n' delimited text received from the
 * network every second.
 *
 * Usage: SqlNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example org.apache.spark.examples.streaming.SqlNetworkWordCount localhost 9999`
 */

object SqlNetworkWordCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: NetworkWordCount <hostname> <port>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    // Create the context with a 2 second batch size
    val sparkConf = new SparkConf().setAppName("SqlNetworkWordCount")
    val ssc = new StreamingContext(sparkConf, Seconds(2))

    // Create a socket stream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    // Note that no duplication in storage level only for running locally.
    // Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream(args(0), args(1).toInt, StorageLevel.MEMORY_AND_DISK_SER)
    val words = lines.flatMap(_.split(" "))

    // Convert RDDs of the words DStream to DataFrame and run SQL query
    words.foreachRDD { (rdd: RDD[String], time: Time) =>
      // Get the singleton instance of SparkSession
      val spark = SparkSessionSingleton.getInstance(rdd.sparkContext.getConf)
      import spark.implicits._

      // Convert RDD[String] to RDD[case class] to DataFrame
      val wordsDataFrame = rdd.map(w => Record(w)).toDF()

      // Creates a temporary view using the DataFrame
      wordsDataFrame.createOrReplaceTempView("words")

      // Do word count on table using SQL and print it
      val wordCountsDataFrame =
        spark.sql("select word, count(*) as total from words group by word")
      println(s"========= $time =========")
      wordCountsDataFrame.show()
    }

    ssc.start()
    ssc.awaitTermination()
  }
}


/** Case class for converting RDD to DataFrame */
case class Record(word: String)


/** Lazily instantiated singleton instance of SparkSession */
object SparkSessionSingleton {

  @transient  private var instance: SparkSession = _

  def getInstance(sparkConf: SparkConf): SparkSession = {
    if (instance == null) {
      instance = SparkSession
        .builder
        .config(sparkConf)
        .getOrCreate()
    }
    instance
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.SQLTransformer
// $example off$
import org.apache.spark.sql.SparkSession

object SQLTransformerExample {
  def main(args: Array[String]) {
    val spark = SparkSession
      .builder
      .appName("SQLTransformerExample")
      .getOrCreate()

    // $example on$
    val df = spark.createDataFrame(
      Seq((0, 1.0, 3.0), (2, 2.0, 5.0))).toDF("id", "v1", "v2")

    val sqlTrans = new SQLTransformer().setStatement(
      "SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__")

    sqlTrans.transform(df).show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

// $example on$
import org.apache.spark.graphx.{Graph, VertexId}
import org.apache.spark.graphx.util.GraphGenerators
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * An example use the Pregel operator to express computation
 * such as single source shortest path
 * Run with
 * {{{
 * bin/run-example graphx.SSSPExample
 * }}}
 */
object SSSPExample {
  def main(args: Array[String]): Unit = {
    // Creates a SparkSession.
    val spark = SparkSession
      .builder
      .appName(s"${this.getClass.getSimpleName}")
      .getOrCreate()
    val sc = spark.sparkContext

    // $example on$
    // A graph with edge attributes containing distances
    val graph: Graph[Long, Double] =
      GraphGenerators.logNormalGraph(sc, numVertices = 100).mapEdges(e => e.attr.toDouble)
    val sourceId: VertexId = 42 // The ultimate source
    // Initialize the graph such that all vertices except the root have distance infinity.
    val initialGraph = graph.mapVertices((id, _) =>
        if (id == sourceId) 0.0 else Double.PositiveInfinity)
    val sssp = initialGraph.pregel(Double.PositiveInfinity)(
      (id, dist, newDist) => math.min(dist, newDist), // Vertex Program
      triplet => {  // Send Message
        if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {
          Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))
        } else {
          Iterator.empty
        }
      },
      (a, b) => math.min(a, b) // Merge Message
    )
    println(sssp.vertices.collect.mkString("\n"))
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.{StandardScaler, StandardScalerModel}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.util.MLUtils
// $example off$

object StandardScalerExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("StandardScalerExample")
    val sc = new SparkContext(conf)

    // $example on$
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")

    val scaler1 = new StandardScaler().fit(data.map(x => x.features))
    val scaler2 = new StandardScaler(withMean = true, withStd = true).fit(data.map(x => x.features))
    // scaler3 is an identical model to scaler2, and will produce identical transformations
    val scaler3 = new StandardScalerModel(scaler2.std, scaler2.mean)

    // data1 will be unit variance.
    val data1 = data.map(x => (x.label, scaler1.transform(x.features)))

    // data2 will be unit variance and zero mean.
    val data2 = data.map(x => (x.label, scaler2.transform(Vectors.dense(x.features.toArray))))
    // $example off$

    println("data1: ")
    data1.foreach(x => println(x))

    println("data2: ")
    data2.foreach(x => println(x))

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.tools.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import com.bitnei.tools.util.{MockDataProvider, ValidateUtils}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.{SaveMode, SparkSession}

/**
  *
  * @author zhangyongtian
  * @define 将开始和结束里程补写到Oracle列 ,生成SQL
  *
  *                            create 2017-11-27 11:27
  *
  */


object StartStopMileage2Oracle extends Serializable with Logging {

  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var date = "20171102"

    val tableName = "veh_dayreport_runstate"

    //参数处理
    if (!env.equals("local")) {
      date = stateConf.getOption("input.date").get
    }

    var dataTimeRange = ""

    date.length match {
      case 4 => dataTimeRange = s"year=${date.substring(0, 4)}"
      case 6 => dataTimeRange = s"year=${date.substring(0, 4)}/month=${date.substring(4, 6)}"
      case 8 => dataTimeRange = s"year=${date.substring(0, 4)}/month=${date.substring(4, 6)}/day=${date.substring(6)}"
    }


    //输出参数
    var sqlFilePath = s"/tmp/zyt/${app}-${System.currentTimeMillis()}-sql"
    if (env.equals("prd")) {
      sqlFilePath = stateConf.getOption("output.sql.path").getOrElse(sqlFilePath)
    }
    logInfo("输出路径：" + sqlFilePath)

    // TODO: 加载上下文
    logInfo("加载上下文")

    //////////////////////////test//////////////////////////

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + date).getOrCreate()
    import sparkSession.implicits._


    // TODO: 将数据注册成表
    logInfo("将数据注册成表")


    var inputPath = ""
    env match {
      case "local" => {
        MockDataProvider.realInfo(sparkSession)
      }

      case "dev" => {
        inputPath = s"/spark/vehicle/data/realinfo/$dataTimeRange"
        //研发环境
        sparkSession
          .read
          .format("parquet")
          .load(inputPath).createOrReplaceTempView("realinfo")
      }

      case "prd" => {
        inputPath = s"/spark/vehicle/data/realinfo/$dataTimeRange"
        //生产环境
        sparkSession
          .read
          .format("parquet")
          .load(inputPath).createOrReplaceTempView("realinfo")
      }

    }

    logInfo("输入路径：" + inputPath)

    // TODO: 筛选停车信息（ 停车坐标 停车时长）
    //and year='${year}' and month='${month}' and day='${day}'

    val sql =
      s"""
        SELECT VID,`2000` AS TIME,`2202` AS Mileage FROM realinfo
        where VID is not null and `2000` like '${date}%'
        and `2000` is not null and `2202` is not null and `2202` not like '0x%'
        order by `2000`
      """.stripMargin
    println(sql)
    val initDS = sparkSession.sql(sql).as[(String, String, String)]


    // TODO: 过滤
    val filteredDS = initDS
      .filter(x => {

        val mileage = x._3.toDouble / 10

        ValidateUtils.isNumber(x._3) && mileage>0 && mileage <=999999.9
      })

    // TODO: 提取
    val mappedDS =
      filteredDS
        .map(x => {
          val vid = x._1
          val time = x._2
          val mileage = x._3.toDouble / 10
          Input(vid, time, mileage)
        })

    val rs = mappedDS
      .groupByKey(_.vid)
      .mapGroups {
        case (vid, inputs: Iterator[Input]) => {

          val arr = inputs.toArray[Input].sortBy(_.time)

          val startMileage = arr.head.mileage

          val stopMileage = arr.last.mileage

          Output(vid, date, startMileage, stopMileage)
        }
      }.filter(x=>x.stopMileage>=x.startMileage)


    //TODO: 拼接SQL
    val result =
      rs
        .map(x => {
          val sql = s"update $tableName set START_MILEAGE=" + x.startMileage + ",STOP_MILEAGE=" + x.stopMileage + " where VID='" + x.vid + "' and REPORT_TIME=to_date('" + x.date + "', 'yyyymmdd') ;"
          sql
        })


    ////////////////////////////////删除临时表#############################################
    sparkSession.catalog.dropTempView("realinfo")


    // TODO: 输出
    if (env.equals("local")) {
      result.show(false)
      stateConf.set(Constant.JdbcUserName, "ev")
      stateConf.set(Constant.JdbcPasswd, "ev")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.6.146:1521:evmsc1")
      stateConf.set("host", "192.168.6.146:1521")
      stateConf.set("database", "evmsc1")
    }


    logInfo("开始导出SQL................")

    if (env.equals("dev")) {
      stateConf.set(Constant.JdbcUserName, "ev")
      stateConf.set(Constant.JdbcPasswd, "ev")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.6.146:1521:evmsc1")
      stateConf.set("host", "192.168.6.146:1521")
      stateConf.set("database", "evmsc1")

      log.info("输出到HDFS====================================")
      result.repartition(1).write.text(sqlFilePath)
    }


    if (env.equals("prd")) {
      stateConf.set(Constant.JdbcUserName, "ev")
      stateConf.set(Constant.JdbcPasswd, "ev")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.2.51:1521:evmsc1")
      stateConf.set("database", "oracle")

      log.info("输出到HDFS====================================")
      result.repartition(1).write.text(sqlFilePath)

    }



    //验证
    //    SELECT
    //    START_MILEAGE,
    //    STOP_MILEAGE
    //    FROM
    //    VEH_DAYREPORT_RUNSTATE
    //    WHERE
    //    VID = '186e024c-28b6-4634-86f3-82f50afe8c1f'
    //    AND REPORT_TIME = TO_DATE (
    //    '2017-09-02 00:00:00',
    //    'yy-mm-dd hh24:mi:ss'
    //    )

    logInfo("结束导出................")

    logInfo("任务完成...")

    sparkSession.stop()
  }


  case class Input(vid: String, time: String, mileage: Double)

  case class Output(vid: String, date: String, startMileage: Double, stopMileage: Double)

}
package com.bitnei.report.common.configuration

import java.util.concurrent.ConcurrentHashMap

import com.bitnei.report.common.log.Logging


/**
  * Created by franciswang on 2016/10/8.
  */
class StateConf extends Logging with Serializable{
  private  val settings= new ConcurrentHashMap[String,String]()

  {

    val iter = Configuration.getKeys()
    while (iter.hasNext) {
      val key = iter.next()
      val value = Configuration.getAsString(key, "")
      settings.put(key, value)
    }
  }

  def add(args:Array[String]): Unit = {
    args.foreach(arg => {
      val param = arg.split('=')

      logInfo(arg)
      println(arg)
      if(param.length==2) {
        set(param(0), param(1))
      }else{
        throw  new IllegalArgumentException(s"the format of $arg is wrong")
      }
    })
  }


  def getOption(key:String):Option[String]={
    Option(settings.get(key))
  }

  def getString(key:String):String={
    settings.get(key)
  }

  def getLong(key:String,defaultValue:Long):Long={
    Option(settings.get(key)).map(_.toLong).getOrElse(defaultValue)
  }

  def getInt(key:String,defaultValue:Int):Int={
    Option(settings.get(key)).map(_.toInt).getOrElse(defaultValue)
  }

  def getBoolean(key:String,defaultValue:Boolean):Boolean={
    Option(settings.get(key)).map(_.toBoolean).getOrElse(defaultValue)
  }

  def set(key:String,value:String):StateConf={
    if (key == null) {
      throw new NullPointerException("null key")
    }
    if (value == null) {
      throw new NullPointerException("null value for " + key)
    }

    settings.put(key,value)
    this
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.streaming

import org.apache.spark.SparkConf
import org.apache.spark.streaming._

/**
 * Counts words cumulatively in UTF8 encoded, '\n' delimited text received from the network every
 * second starting with initial value of word count.
 * Usage: StatefulNetworkWordCount <hostname> <port>
 *   <hostname> and <port> describe the TCP server that Spark Streaming would connect to receive
 *   data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example
 *      org.apache.spark.examples.streaming.StatefulNetworkWordCount localhost 9999`
 */
object StatefulNetworkWordCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: StatefulNetworkWordCount <hostname> <port>")
      System.exit(1)
    }

    StreamingExamples.setStreamingLogLevels()

    val sparkConf = new SparkConf().setAppName("StatefulNetworkWordCount")
    // Create the context with a 1 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    ssc.checkpoint(".")

    // Initial state RDD for mapWithState operation
    val initialRDD = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))

    // Create a ReceiverInputDStream on target ip:port and count the
    // words in input stream of \n delimited test (eg. generated by 'nc')
    val lines = ssc.socketTextStream(args(0), args(1).toInt)
    val words = lines.flatMap(_.split(" "))
    val wordDstream = words.map(x => (x, 1))

    // Update the cumulative count using mapWithState
    // This will give a DStream made of state (which is the cumulative count of the words)
    val mappingFunc = (word: String, one: Option[Int], state: State[Int]) => {
      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)
      val output = (word, sum)
      state.update(sum)
      output
    }

    val stateDstream = wordDstream.mapWithState(
      StateSpec.function(mappingFunc).initialState(initialRDD))
    stateDstream.print()
    ssc.start()
    ssc.awaitTermination()
  }
}
// scalastyle:on println
package com.bitnei.report.handler

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import com.bitnei.report.model._
import com.bitnei.report.stateGenerate.Window

import scala.annotation.tailrec
import scala.collection.mutable.ListBuffer

/**
  *
  * @author zhangyongtian
  * @define 充电状态判断
  *
  * 状态生成器
  *
  * 给定一个序列source，生成满足如下约束的状态序列：
  * 1）如果source(i) 满足c ,那么 source(i) 属于某个state
  * 2）如果source(i)属于某个state，并且j是i的后继，那么source(j)属于state
  *
  * source(j)是source(i)的后继当且仅当
  * 1)j=i+1
  * 2)source(j)满足c
  * 3)source（i）和source(j)满足r
  *
  * c(i)是一个返回值为true和false的一个函数，如果source(i)满足某个条件，那么结果为true，否则为false
  * r(i,j)是一个返回值为true和false的函数，如果source(i)和source(j)满足某个相关条件，返回true，否则false
  *
  * create 2018-03-14 14:38
  *
  */
abstract class StateGenerator(stateConf: StateConf) extends Logging {

  type T

  private val chargeBeginLength = stateConf.getInt(Constant.StateWindowChargeBeginLength, 10)

  private val chargeEndLength = stateConf.getInt(Constant.StateWindowChargeEndLength, 25)

  private val fullChargeEndLength = stateConf.getInt(Constant.StateWindowFullChargeEndLength, 10)

  private val maxSoc = stateConf.getInt(Constant.MaxSoc, 100)

  var startMielage = 0

  var endMileage = 0

  protected def getVid: (T) => String

  protected def getTime: (T) => String

  protected def getSpeed: (T) => Int

  protected def getCharge: (T) => Int

  protected def getSoc: (T) => Int

  protected def getMileage: (T) => Int


  var onlineTime: Long = 0

  private val onlineBound = stateConf.getLong(Constant.StateWindowLength, 180)


  var lastValidateData: Option[T] = None

  val chargeChangeChecker: ChrgeChangeStrategy =

    stateConf.getString("ChargeChange") match {

      case "Simple" => new ChargeChangeCheckBaseSimple()

      case _ => new ChargeChangeCheckBaseTimeSerial
    }


  def beginFullCharge(source: Seq[T], curIndex: Int): Boolean = ???

  def inFullCharge(source: Seq[T]) = ???

  def beginCharge(source: Seq[T], curIndex: Int): Boolean = ???

  def inCharge(source: Seq[T]) = ???

  def validate(t: T): Boolean = ???


  def handle(source: Seq[T]): List[Window[T]] = {
    val windows = new ListBuffer[Window[T]]()

    @tailrec
    def splitWindow(curIndex: Int, curState: String): List[Window[T]] = {
      def beginChargeStrict(i: Int): Boolean = {
        val speed = getSpeed(source(i))
        val charge = getCharge(source(i))
        charge < 0 && speed <= 50
      }

      if (curIndex < source.length) {
        //计算当日开始里程和结束里程
        startMielage = if (startMielage == 0) getMileage(source(curIndex)) else startMielage
        if (getMileage(source(curIndex)) != 0) endMileage = getMileage(source(curIndex))

        //如果当前状态是满电，那么上一个状态一定是充电。
        val (endIndex, state) = if (curState == Constant.ChargeState && beginFullCharge(source, curIndex)) {
          def append(v: T): FullChargeWindow[T] = {
            val fullChargeWindow = FullChargeWindow[T]()
            fullChargeWindow.append(v)
            windows.append(fullChargeWindow)
            fullChargeWindow
          }

          val fullChargeWindow = append(source(curIndex))

          (inFullCharge(source)(curIndex + 1, v => fullChargeWindow.append(v)), Constant.FullChargeState)
        } else if (beginChargeStrict(curIndex) && beginCharge(source, curIndex)) {
          def append(v: T): ChargeWindow[T] = {
            val chargeWindow = ChargeWindow[T]()
            windows.append(chargeWindow)
            chargeWindow.append(v)
            chargeWindow
          }

          val chargeWindow = append(source(curIndex))

          (inCharge(source)(curIndex + 1, v => chargeWindow.append(v)), Constant.ChargeState)
        } else {

          if (validate(source(curIndex))) lastValidateData = Some(source(curIndex))

          chargeChangeChecker.check(source, curIndex) match {
            //如果检测到了换电行为
            case (true, corrent) =>
              windows.lastOption match {
                case Some(chargeChangeState: ChargeChangeWindow[T]) =>
                  chargeChangeState.append(source(curIndex))
                //  println("charge change"+source(curIndex),corrent)
                case _ =>
                  val chargeChangeState = new ChargeChangeWindow[T](corrent)

                  //追加开始有效数据
                  if (!validate(source(curIndex)) && lastValidateData.nonEmpty) chargeChangeState.append(lastValidateData.get)

                  chargeChangeState.append(source(curIndex))
                  windows.append(chargeChangeState)
                // println("charge change"+source(curIndex),corrent)
              }
              (curIndex + 1, Constant.ChargeChangeState)
            case _ =>
              windows.lastOption match {
                case Some(chargeChange: ChargeChangeWindow[T]) =>
                  if (!validate(chargeChange.last) && validate(source(curIndex))) chargeChange.append(source(curIndex))
                case _ =>
              }


              val curMap = source(curIndex)
              val prevMap = if (curIndex >= 1) source(curIndex - 1) else curMap
              if (!online(curMap, prevMap)) {
                windows.append(TravelWindow().append(source(curIndex)))
              } else {
                onlineTime += getOnline(source, curIndex)
                windows.lastOption match {
                  case Some(travelWindow: TravelWindow[T]) =>
                    travelWindow.append(source(curIndex))
                  case _ =>
                    windows.append(TravelWindow().append(source(curIndex)))
                }
              }
              (curIndex + 1, Constant.TravelState)
          }
        }
        setWindow(windows.last, state)
        splitWindow(endIndex, state)
      } else windows.toList
    }

    splitWindow(0, Constant.NoneState).filter(window => {
      window match {
        case travelWindow: TravelWindow[T] =>
          setWindow(travelWindow, Constant.TravelState)
          travelWindow.length >= 10
        case _ => true
      }
    })
  }


  var chargeTime: Long = 0
  var fullChargeTime: Long = 0

  def setWindow(window: Window[T], state: String): Unit = {

    def generateWindowId(head: T, last: T, state: String): String = s"${getVid(head)}|${getTime(head)}|${getTime(last)}|$state"


    val (startTime: Long, startH: Int) = Utils.parsetDate(getTime(window.head)) match {
      case Some(startDate) =>
        if (startDate.getMinutes == 0) (startDate.getTime, startDate.getHours)
        else (startDate.getTime, startDate.getHours + 1)
      case None =>
        (0, -1)
    }
    val (endTime: Long, endH: Int) = Utils.parsetDate(getTime(window.last)) match {
      case Some(endDate) =>
        if (endDate.getMinutes == 0) (endDate.getTime, endDate.getHours)
        else (endDate.getTime, endDate.getHours + 1)
      case None =>
        (0, -1)
    }

    window.onLineTime = onlineTime
    window.startMileage = startMielage
    window.endMileage = endMileage

    if (state == Constant.ChargeState) {
      chargeTime += (endTime - startTime)
    } else if (state == Constant.FullChargeState) {
      fullChargeTime += (endTime - startTime)
    } else if (state == Constant.TravelState) {
      window.chargeTime = chargeTime
      window.fullChargeTime = fullChargeTime
    } else if (state == Constant.ChargeChangeState) {
    }
  }


}
package com.bitnei.report.stateGenerate

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant

import scala.annotation.tailrec
import scala.collection.mutable.ListBuffer


/**
* 将给定的，按照时间排序数据，划分为充电，行驶，满点状态集合。
* */
abstract class StateGeneratorBase(stateConf:StateConf) extends Logging with StateGenerator{
  private val chargeBeginLength = stateConf.getInt(Constant.StateWindowChargeBeginLength, 10)
  private val chargeEndLength = stateConf.getInt(Constant.StateWindowChargeEndLength, 25)
  private val fullChargeEndLength = stateConf.getInt(Constant.StateWindowFullChargeEndLength, 10)
  private val maxSoc=stateConf.getInt(Constant.MaxSoc,100)
  var startMielage=0
  var endMileage=0

  protected def  getVid:(T)=>String
  protected def getTime:(T)=>String
  protected def getSpeed:(T)=>Int
  protected def getCharge:(T)=>Int
  protected def getSoc:(T)=>Int
  protected def getMileage:(T)=>Int
 // override type Source =  Seq[T]
  //override type States = List[Window[T]]
  //在线时间

  var onlineTime: Long = 0

  private  val onlineBound=stateConf.getLong(Constant.StateWindowLength, 180)


  /**
  * 为了减少bug和便于后续维护，比如添加新的判断条件，这里全部采用函数式实现(尾递归)，最大化的减少变量的修改，便于程序推断。
  * */
  def handle(source: Seq[T]):List[Window[T]] = {
    val windows = new ListBuffer[Window[T]]()

    @tailrec
    def splitWindow(curIndex: Int, curState: String): List[Window[T]] = {
      def beginChargeStrict(i: Int): Boolean = {
        val speed =getSpeed(source(i))
        val charge = getCharge(source(i))
        charge < 0 && speed <= 50
      }

      if (curIndex < source.length) {
        //计算当日开始里程和结束里程
        startMielage=if(startMielage==0) getMileage(source(curIndex)) else startMielage
        if(getMileage(source(curIndex))!=0) endMileage=getMileage(source(curIndex))

        //如果当前状态是满电，那么上一个状态一定是充电。
        val (endIndex, state) = if (curState == Constant.ChargeState && beginFullCharge(source, curIndex)) {
          def append(v: T): FullChargeWindow[T] = {
            val fullChargeWindow = FullChargeWindow[T]()
            fullChargeWindow.append(v)
            windows.append(fullChargeWindow)
            fullChargeWindow
          }

          val fullChargeWindow = append(source(curIndex))

          (inFullCharge(source)(curIndex + 1, v => fullChargeWindow.append(v)), Constant.FullChargeState)
        } else if (beginChargeStrict(curIndex) && beginCharge(source, curIndex)) {
          def append(v: T): ChargeWindow[T] = {
            val chargeWindow = ChargeWindow[T]()
            windows.append(chargeWindow)
            chargeWindow.append(v)
            chargeWindow
          }


          val chargeWindow=append(source(curIndex))

          (inCharge(source)(curIndex + 1,v=>chargeWindow.append(v)), Constant.ChargeState)
        } else {
          val curMap =source(curIndex)
          val prevMap = if (curIndex>= 1) source(curIndex - 1) else curMap
          if (!online(curMap, prevMap)) {
            windows.append(TravelWindow().append(source(curIndex)))
          } else {
            onlineTime += getOnline(source, curIndex)
            windows.lastOption match {
              case Some(travelWindow: TravelWindow[T]) =>
                travelWindow.append(source(curIndex))
              case _ =>
                windows.append(TravelWindow().append(source(curIndex)))
            }
          }
          (curIndex + 1, Constant.TravelState)
        }

        setWindow(windows.last, state)
        splitWindow(endIndex, state)
      } else windows.toList
    }

    splitWindow(0, Constant.NoneState).filter(window => {
      window match {
        case travelWindow: TravelWindow[T] =>
          setWindow(travelWindow, Constant.TravelState)
          travelWindow.length >= 10
        case _ => true
      }
    })
  }


  def online(cur: T, prev: T): Boolean = {
    if (prev == null) true
    else {
      val curTime = getTime(cur)
      val prevTime = getTime(prev)
      Utils.lessEq(curTime, prevTime,onlineBound)
    }
  }


  @tailrec
  private def inFullCharge(source:Seq[T])(cur: Int,f:(T)=>Unit): Int = {
    @tailrec
    def inFullChargeTail(i: Int, curMap: T, prevMap: T, continuedInEndFullCharge: Int): Boolean = {
      if (i < source.length) {
        if (!online(curMap, prevMap)) {
          /**
            * 如果是第一条记录离线，肯定进行状态切换了，否则满10条才进行状态切换,考虑如下序列
            * time=20160913101010,soc=100,charge=0,speed=0
            * time=20160913101020,soc=99,charge=0,spped=0  该条数据属于满电状态
            * time=20160913101520,soc=99,charge=0,spped=0  离线
            */
          if (continuedInEndFullCharge == 0) false else continuedInEndFullCharge < fullChargeEndLength
        } else if (continuedInEndFullCharge < fullChargeEndLength) {
          val soc = getSoc(curMap)
          val charge = getCharge(curMap)
          val speed = getSpeed(curMap)
          if (full(soc,charge,speed)) true
          else {
            val nextMap = if ((i + 1) < source.length) source(i + 1) else curMap
            inFullChargeTail(i + 1, nextMap, curMap, continuedInEndFullCharge + 1)
          }
        } else false

      } else {
        continuedInEndFullCharge<chargeEndLength
      }
    }

    if (cur < source.length && inFullChargeTail(cur, source(cur), source(cur - 1), 0)) {
      f(source(cur))
      onlineTime += getOnline(source, cur)
      inFullCharge(source)(cur + 1,f)
    } else {
      cur
    }
  }


  @tailrec
  private def inCharge(source:Seq[T])(cur: Int,f:(T)=>Unit): Int = {
    @tailrec
    def inChargeTail(i: Int, curMap: T, prevMap: T, continuedInEndCharge: Int): Boolean = {
      if (i < source.length) {
        if (!online(curMap, prevMap)) {
          if (continuedInEndCharge == 0) false else continuedInEndCharge < chargeEndLength
        } else if (beginFullCharge(source, i)) false
        else if (continuedInEndCharge < chargeEndLength) {
          val speed = getSpeed(curMap)
          val charge = getCharge(curMap)
          if (charge < 0 && speed <= 50) true
          else {
            val nextMap = if ((i + 1) < source.length) source(i + 1) else curMap
            inChargeTail(i + 1, nextMap, curMap, continuedInEndCharge + 1)
          }
        } else false
      } else {
        continuedInEndCharge < chargeEndLength
      }
    }

    if (cur < source.length && inChargeTail(cur, source(cur), source(cur - 1), 0)) {
      f(source(cur))
      onlineTime += getOnline(source, cur)
      inCharge(source)(cur + 1, f)
    } else {
      cur
    }
  }


  private def beginCharge(source: Seq[T], curIndex: Int): Boolean = {
    @tailrec
    def beginChargeTail(i: Int, validLength: Int): Boolean = {
      if (validLength == 0) {
        if (i >= source.length) false
        else if (beginFullCharge(source, i)) false
        else {
          val map = source(i)
          val speed = getSpeed(map)
          val charge = getCharge(map)
          if (charge < 0 && speed <= 50) beginChargeTail(i + 1, validLength + 1)
          else false
        }
      } else {
        if (i < source.length) {
          val curMap = source(i)
          val prevMap = if ((i - 1) >= 0)source(i - 1) else curMap
          if (!online(curMap, prevMap) && validLength < chargeBeginLength) false
          else if (beginFullCharge(source, i)) false
          else if (validLength < chargeBeginLength) {
            val map = source(i)
            val speed = getSpeed(map)
            val charge = getCharge(map)
            if (charge < 0 && speed <= 50) beginChargeTail(i + 1, validLength + 1)
            else false
          } else {
            true
          }
        } else {
          false
        }
      }
    }

    require(curIndex < source.length)
    beginChargeTail(curIndex, 0)
  }

  def beginFullCharge(source: Seq[T], curIndex: Int): Boolean = {
    val map = source(curIndex)
    val soc = getSoc(map)
    val speed = getSpeed(map)
    val charge = getCharge(map)
    full(soc,charge,speed)
  }

  val full=(soc:Int,charge:Int,speed:Int)=> soc == maxSoc && charge == 0 && speed <= 50

  def getOnline(source: Seq[T], curIndex: Int): Long = {
    if (curIndex == 0 || curIndex >= source.length) 0
    else {
      (Utils.getTime(getTime(source(curIndex))),
        Utils.getTime(getTime(source(curIndex-1)))) match {
        case (Some(curTime), Some(prevTime)) =>
          val timedif = curTime - prevTime
          if (timedif <= 3 * 60 * 1000) timedif
          else 0
        case _ => 0
      }
    }
  }

  var chargeTime: Long = 0
  var fullChargeTime: Long = 0

  def setWindow(window: Window[T], state: String): Unit = {
    val (startTime: Long, startH: Int) = Utils.parsetDate(getTime(window.head)) match {
      case Some(startDate) =>
        if (startDate.getMinutes == 0) (startDate.getTime, startDate.getHours)
        else (startDate.getTime, startDate.getHours + 1)
      case None =>
        (0, -1)
    }
    val (endTime: Long, endH: Int) = Utils.parsetDate(getTime(window.last)) match {
      case Some(endDate) =>
        if (endDate.getMinutes == 0) (endDate.getTime, endDate.getHours)
        else (endDate.getTime, endDate.getHours + 1)
      case None =>
        (0, -1)
    }

    window.onLineTime = onlineTime
    window.startMileage=startMielage
    window.endMileage=endMileage

    if (state == Constant.ChargeState) {
      chargeTime += (endTime - startTime)
    } else if (state == Constant.FullChargeState) {
      fullChargeTime += (endTime - startTime)
    } else if (state == Constant.TravelState) {
      window.chargeTime = chargeTime
      window.fullChargeTime = fullChargeTime
    }
  }
}






/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.StopWordsRemover
// $example off$
import org.apache.spark.sql.SparkSession

object StopWordsRemoverExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("StopWordsRemoverExample")
      .getOrCreate()

    // $example on$
    val remover = new StopWordsRemover()
      .setInputCol("raw")
      .setOutputCol("filtered")

    val dataSet = spark.createDataFrame(Seq(
      (0, Seq("I", "saw", "the", "red", "balloon")),
      (1, Seq("Mary", "had", "a", "little", "lamb"))
    )).toDF("id", "raw")

    remover.transform(dataSet).show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}

object StratifiedSamplingExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("StratifiedSamplingExample")
    val sc = new SparkContext(conf)

    // $example on$
    // an RDD[(K, V)] of any key value pairs
    val data = sc.parallelize(
      Seq((1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')))

    // specify the exact fraction desired from each key
    val fractions = Map(1 -> 0.1, 2 -> 0.6, 3 -> 0.3)

    // Get an approximate sample from each stratum
    val approxSample = data.sampleByKey(withReplacement = false, fractions = fractions)
    // Get an exact sample from each stratum
    val exactSample = data.sampleByKeyExact(withReplacement = false, fractions = fractions)
    // $example off$

    println("approxSample size is " + approxSample.collect().size.toString)
    approxSample.collect().foreach(println)

    println("exactSample its size is " + exactSample.collect().size.toString)
    exactSample.collect().foreach(println)

    sc.stop()
  }
}
// scalastyle:on println
//import kafka.serializer.StringDecoder
//import org.apache.spark.sql.SparkSession
//import org.apache.spark.streaming.kafka.KafkaUtils
//import org.apache.spark.streaming.{Duration, StreamingContext}
//import kafka.serializer.StringDecoder
//
//import org.apache.spark.streaming._
//import org.apache.spark.streaming.kafka._
//import org.apache.spark.SparkConf
//
//
///*
//* created by wangbaosheng on 2017/12/20
//*/
//class Streaming {
//
//  def run(): Unit = {
//    val spark = SparkSession.builder().getOrCreate();
//    val sc = spark.sparkContext
//    val streamingContext = new StreamingContext(sc, Duration(1000))
//
//    val kafkaParams = Map[String,String]("" -> "")
//
//
//    /**
//      * 1.at-least-once:checkpoint stream or set auto.offset.reset=smallest
//      * 2.extact-once:idemponet write or transcation udpate.
//      *
//      * 对于transcation update，一定要注意，再createDirectStream之前，要先从offset_table读取最后一次成功更新的offset
//      * 然后从unitoffset开始读取数据。
//      *
//      * offset_table存储我这个业务，最后一次消费成功的kafka topic partition的offset。
//      *
//      * offset_table
//      * stream(业务id)  topci  partition   offset
//      *  id1             t1     0           1000
//      *  id1             t1     1           2000
//      *  id1             t1     2           2000
//      *  id1             t2     0           500
//      *  ... ....           ...
//      */
//
//    //如果使用事务更新来保证精确一次执行予以，那么每次重新启动stream时，需要从上次消费地方之后消费。
//    val wordStream=KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](
//      streamingContext,
//      kafkaParams,
//      Set("operation_query"))
//
//    //direct stream
//    // batch->rdd->OffsetRanges,每一个OffsetRange存储了每一个partition的fromOffset,untilOffset
//   //每一个stream都有一个compute方法，该方法每个一定时间生成并返回一个rdd，我们可以对这个rdd执行计算。
//    //再directly kafka stream中，directoly stream运行再driver端，他再初始化时，会返回topic中每一个partition的startoffset来生成RDD partition.
//    //然后生成kafka rdd.
//    //kafka rdd会从partition来取数据执行计算。kafka rdd partition内部会创建simple consumer，然后根绝startoffset来获取message
//    //如果获取失败，kafka rdd partition简单的抛出一个异常，然后spark会自动重试提交失败的partition从而容错。
//
//
//
//    //这里我们可以拿到kafka rdd，kafka rdd运行再executor上。
//    wordStream.foreachRDD(rdd=>{
//      val offsetRanges=rdd.asInstanceOf[HasOffsetRanges].offsetRanges
//
//      //如何确保不会重复执行。思路，使用幂等更新或者事物操作。
//      //幂等更新，为计算结果生成全局唯一key。
//
//      rdd.mapPartitionsWithIndex({
//        case (partitionId:Int,values:Iterator[(String,String)])=>{
//          val topic=offsetRanges(partitionId).topic
//          val startOffset=offsetRanges(partitionId).fromOffset
//          val key=s"${topic}_${partitionId}_${startOffset}"
//
//          //将计算结果写入到hbase,这里是insert，不会update，因此没有就插入，有就覆盖。
//          //或者 insert into table values(unione key)
//          //saveOffset(key)
//
//          //1.set up connection
//          //2.process value
//          values.foreach(v=>{
//
//          })
//          //3.close the connection
//          //4.return nothing
//          Iterator.empty
//
//
//          //begin transaction
//          //write result to database
//          //write offset to database
//          //  update txn_offsets set offset=unitlOffset
//          //  where topic=$topic and partition=$partition and offset=fromOffset
//          //end transaction
//        }
//      }).foreach(v=>())
//
//    })
//  }
//}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.streaming

import org.apache.log4j.{Level, Logger}

import org.apache.spark.internal.Logging

/** Utility functions for Spark Streaming examples. */
object StreamingExamples extends Logging {

  /** Set reasonable logging levels for streaming if the user has not configured log4j. */
  def setStreamingLogLevels() {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
      // We first log something to initialize Spark's default logging, then we override the
      // logging level.
      logInfo("Setting log level to [WARN] for streaming example." +
        " To override add a custom log4j.properties to the classpath.")
      Logger.getRootLogger.setLevel(Level.WARN)
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
// $example on$
import org.apache.spark.mllib.clustering.StreamingKMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.streaming.{Seconds, StreamingContext}
// $example off$

/**
 * Estimate clusters on one stream of data and make predictions
 * on another stream, where the data streams arrive as text files
 * into two different directories.
 *
 * The rows of the training text files must be vector data in the form
 * `[x1,x2,x3,...,xn]`
 * Where n is the number of dimensions.
 *
 * The rows of the test text files must be labeled data in the form
 * `(y,[x1,x2,x3,...,xn])`
 * Where y is some identifier. n must be the same for train and test.
 *
 * Usage:
 *   StreamingKMeansExample <trainingDir> <testDir> <batchDuration> <numClusters> <numDimensions>
 *
 * To run on your local machine using the two directories `trainingDir` and `testDir`,
 * with updates every 5 seconds, 2 dimensions per data point, and 3 clusters, call:
 *    $ bin/run-example mllib.StreamingKMeansExample trainingDir testDir 5 3 2
 *
 * As you add text files to `trainingDir` the clusters will continuously update.
 * Anytime you add text files to `testDir`, you'll see predicted labels using the current model.
 *
 */
object StreamingKMeansExample {

  def main(args: Array[String]) {
    if (args.length != 5) {
      System.err.println(
        "Usage: StreamingKMeansExample " +
          "<trainingDir> <testDir> <batchDuration> <numClusters> <numDimensions>")
      System.exit(1)
    }

    // $example on$
    val conf = new SparkConf().setAppName("StreamingKMeansExample")
    val ssc = new StreamingContext(conf, Seconds(args(2).toLong))

    val trainingData = ssc.textFileStream(args(0)).map(Vectors.parse)
    val testData = ssc.textFileStream(args(1)).map(LabeledPoint.parse)

    val model = new StreamingKMeans()
      .setK(args(3).toInt)
      .setDecayFactor(1.0)
      .setRandomCenters(args(4).toInt, 0.0)

    model.trainOn(trainingData)
    model.predictOnValues(testData.map(lp => (lp.label, lp.features))).print()

    ssc.start()
    ssc.awaitTermination()
    // $example off$
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
// $example on$
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD
// $example off$
import org.apache.spark.streaming._

/**
 * Train a linear regression model on one stream of data and make predictions
 * on another stream, where the data streams arrive as text files
 * into two different directories.
 *
 * The rows of the text files must be labeled data points in the form
 * `(y,[x1,x2,x3,...,xn])`
 * Where n is the number of features. n must be the same for train and test.
 *
 * Usage: StreamingLinearRegressionExample <trainingDir> <testDir>
 *
 * To run on your local machine using the two directories `trainingDir` and `testDir`,
 * with updates every 5 seconds, and 2 features per data point, call:
 *    $ bin/run-example mllib.StreamingLinearRegressionExample trainingDir testDir
 *
 * As you add text files to `trainingDir` the model will continuously update.
 * Anytime you add text files to `testDir`, you'll see predictions from the current model.
 *
 */
object StreamingLinearRegressionExample {

  def main(args: Array[String]): Unit = {
    if (args.length != 2) {
      System.err.println("Usage: StreamingLinearRegressionExample <trainingDir> <testDir>")
      System.exit(1)
    }

    val conf = new SparkConf().setAppName("StreamingLinearRegressionExample")
    val ssc = new StreamingContext(conf, Seconds(1))

    // $example on$
    val trainingData = ssc.textFileStream(args(0)).map(LabeledPoint.parse).cache()
    val testData = ssc.textFileStream(args(1)).map(LabeledPoint.parse)

    val numFeatures = 3
    val model = new StreamingLinearRegressionWithSGD()
      .setInitialWeights(Vectors.zeros(numFeatures))

    model.trainOn(trainingData)
    model.predictOnValues(testData.map(lp => (lp.label, lp.features))).print()

    ssc.start()
    ssc.awaitTermination()
    // $example off$

    ssc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
 * Train a logistic regression model on one stream of data and make predictions
 * on another stream, where the data streams arrive as text files
 * into two different directories.
 *
 * The rows of the text files must be labeled data points in the form
 * `(y,[x1,x2,x3,...,xn])`
 * Where n is the number of features, y is a binary label, and
 * n must be the same for train and test.
 *
 * Usage: StreamingLogisticRegression <trainingDir> <testDir> <batchDuration> <numFeatures>
 *
 * To run on your local machine using the two directories `trainingDir` and `testDir`,
 * with updates every 5 seconds, and 2 features per data point, call:
 *    $ bin/run-example mllib.StreamingLogisticRegression trainingDir testDir 5 2
 *
 * As you add text files to `trainingDir` the model will continuously update.
 * Anytime you add text files to `testDir`, you'll see predictions from the current model.
 *
 */
object StreamingLogisticRegression {

  def main(args: Array[String]) {

    if (args.length != 4) {
      System.err.println(
        "Usage: StreamingLogisticRegression <trainingDir> <testDir> <batchDuration> <numFeatures>")
      System.exit(1)
    }

    val conf = new SparkConf().setMaster("local").setAppName("StreamingLogisticRegression")
    val ssc = new StreamingContext(conf, Seconds(args(2).toLong))

    val trainingData = ssc.textFileStream(args(0)).map(LabeledPoint.parse)
    val testData = ssc.textFileStream(args(1)).map(LabeledPoint.parse)

    val model = new StreamingLogisticRegressionWithSGD()
      .setInitialWeights(Vectors.zeros(args(3).toInt))

    model.trainOn(trainingData)
    model.predictOnValues(testData.map(lp => (lp.label, lp.features))).print()

    ssc.start()
    ssc.awaitTermination()

  }

}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.mllib.stat.test.{BinarySample, StreamingTest}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.util.Utils

/**
 * Perform streaming testing using Welch's 2-sample t-test on a stream of data, where the data
 * stream arrives as text files in a directory. Stops when the two groups are statistically
 * significant (p-value < 0.05) or after a user-specified timeout in number of batches is exceeded.
 *
 * The rows of the text files must be in the form `Boolean, Double`. For example:
 *   false, -3.92
 *   true, 99.32
 *
 * Usage:
 *   StreamingTestExample <dataDir> <batchDuration> <numBatchesTimeout>
 *
 * To run on your local machine using the directory `dataDir` with 5 seconds between each batch and
 * a timeout after 100 insignificant batches, call:
 *    $ bin/run-example mllib.StreamingTestExample dataDir 5 100
 *
 * As you add text files to `dataDir` the significance test wil continually update every
 * `batchDuration` seconds until the test becomes significant (p-value < 0.05) or the number of
 * batches processed exceeds `numBatchesTimeout`.
 */
object StreamingTestExample {

  def main(args: Array[String]) {
    if (args.length != 3) {
      // scalastyle:off println
      System.err.println(
        "Usage: StreamingTestExample " +
          "<dataDir> <batchDuration> <numBatchesTimeout>")
      // scalastyle:on println
      System.exit(1)
    }
    val dataDir = args(0)
    val batchDuration = Seconds(args(1).toLong)
    val numBatchesTimeout = args(2).toInt

    val conf = new SparkConf().setMaster("local").setAppName("StreamingTestExample")
    val ssc = new StreamingContext(conf, batchDuration)
    ssc.checkpoint {
      val dir = Utils.createTempDir()
      dir.toString
    }

    // $example on$
    val data = ssc.textFileStream(dataDir).map(line => line.split(",") match {
      case Array(label, value) => BinarySample(label.toBoolean, value.toDouble)
    })

    val streamingTest = new StreamingTest()
      .setPeacePeriod(0)
      .setWindowSize(0)
      .setTestMethod("welch")

    val out = streamingTest.registerStream(data)
    out.print()
    // $example off$

    // Stop processing if test becomes significant or we time out
    var timeoutCounter = numBatchesTimeout
    out.foreachRDD { rdd =>
      timeoutCounter -= 1
      val anySignificant = rdd.map(_.pValue < 0.05).fold(false)(_ || _)
      if (timeoutCounter == 0 || anySignificant) rdd.context.stop()
    }

    ssc.start()
    ssc.awaitTermination()
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.StringIndexer
// $example off$
import org.apache.spark.sql.SparkSession

object StringIndexerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("StringIndexerExample")
      .getOrCreate()

    // $example on$
    val df = spark.createDataFrame(
      Seq((0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c"))
    ).toDF("id", "category")

    val indexer = new StringIndexer()
      .setInputCol("category")
      .setOutputCol("categoryIndex")

    val indexed = indexer.fit(df).transform(df)
    indexed.show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.common.utils

/**
  * Created by franciswang on 2016/9/29.
  */

/*
*
* */
object StringParser {


  def toMap(v:String):Map[String,String]={
    try {
      val map = new scala.collection.mutable.HashMap[String, String]()
      getTuples(v).foreach(tuple => {
        val keyValuePair=tuple.split(':')
        if(keyValuePair.length==2) {
          map.put(keyValuePair(0), keyValuePair(1))
        }
      })
      map.toMap
    }catch {
      case e:Exception=>
        Map()
    }
  }

  def toCsv(v:String):String= {
    v.split(',').map(field => {
      val keyvalue = field.split(':')
      if (keyvalue.length == 2) {
        keyvalue(1)
      } else " "
    }).reduce((a, b) => s"$a,$b")
  }


  def toMap(v:String,split:String,tupleSplit:String):Map[String,String]={
    try {
      val map = new scala.collection.mutable.HashMap[String, String]()
      v.split(split).foreach(tuple => {
        val keyValuePair=tuple.split(tupleSplit)
        if(keyValuePair.length==2) {
          map.put(keyValuePair(0), keyValuePair(1))
        }
      })
      map.toMap
    }catch {
      case e:Exception=>
        Map()
    }
  }

  /*
  * text format:
  *    element|element|element
  * element:
  *    tuple,tuple,tuple
  * tuple:
  *    key:value
  * */



  def getTuples(element: String): Array[String] = element.split(',')





  def get(element: String, key: String): Option[String] = {
    try {
     element.split(',').find(tuple => {
        val keyValuePair=tuple.split(':')
        if(keyValuePair.length==2){
          keyValuePair(0) == key
        }else false
      }).map(_.split(':')).map(x => x(1))

    } catch {
     // case e: IndexOutOfBoundsException => throw new IndexOutOfBoundsException(element+"column:"+key)
      case e:Throwable=>None
    }
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.sql.streaming

import org.apache.spark.sql.SparkSession

/**
 * Consumes messages from one or more topics in Kafka and does wordcount.
 * Usage: StructuredKafkaWordCount <bootstrap-servers> <subscribe-type> <topics>
 *   <bootstrap-servers> The Kafka "bootstrap.servers" configuration. A
 *   comma-separated list of host:port.
 *   <subscribe-type> There are three kinds of type, i.e. 'assign', 'subscribe',
 *   'subscribePattern'.
 *   |- <assign> Specific TopicPartitions to consume. Json string
 *   |  {"topicA":[0,1],"topicB":[2,4]}.
 *   |- <subscribe> The topic list to subscribe. A comma-separated list of
 *   |  topics.
 *   |- <subscribePattern> The pattern used to subscribe to topic(s).
 *   |  Java regex string.
 *   |- Only one of "assign, "subscribe" or "subscribePattern" options can be
 *   |  specified for Kafka source.
 *   <topics> Different value format depends on the value of 'subscribe-type'.
 *
 * Example:
 *    `$ bin/run-example \
 *      sql.streaming.StructuredKafkaWordCount host1:port1,host2:port2 \
 *      subscribe topic1,topic2`
 */
object StructuredKafkaWordCount {
  def main(args: Array[String]): Unit = {
    if (args.length < 3) {
      System.err.println("Usage: StructuredKafkaWordCount <bootstrap-servers> " +
        "<subscribe-type> <topics>")
      System.exit(1)
    }

    val Array(bootstrapServers, subscribeType, topics) = args

    val spark = SparkSession
      .builder
      .appName("StructuredKafkaWordCount")
      .getOrCreate()

    import spark.implicits._

    // Create DataSet representing the stream of input lines from kafka
    val lines = spark
      .readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", bootstrapServers)
      .option(subscribeType, topics)
      .load()
      .selectExpr("CAST(value AS STRING)")
      .as[String]

    // Generate running word count
    val wordCounts = lines.flatMap(_.split(" ")).groupBy("value").count()

    // Start running the query that prints the running counts to the console
    val query = wordCounts.writeStream
      .outputMode("complete")
      .format("console")
      .start()

    query.awaitTermination()
  }

}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.sql.streaming

import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession

/**
 * Counts words in UTF8 encoded, '\n' delimited text received from the network.
 *
 * Usage: StructuredNetworkWordCount <hostname> <port>
 * <hostname> and <port> describe the TCP server that Structured Streaming
 * would connect to receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example sql.streaming.StructuredNetworkWordCount
 *    localhost 9999`
 */
object StructuredNetworkWordCount {
  def main(args: Array[String]) {
    if (args.length < 2) {
      System.err.println("Usage: StructuredNetworkWordCount <hostname> <port>")
      System.exit(1)
    }

    val host = args(0)
    val port = args(1).toInt

    val spark = SparkSession
      .builder
      .appName("StructuredNetworkWordCount")
      .getOrCreate()

    import spark.implicits._

    // Create DataFrame representing the stream of input lines from connection to host:port
    val lines = spark.readStream
      .format("socket")
      .option("host", host)
      .option("port", port)
      .load()

    // Split the lines into words
    val words = lines.as[String].flatMap(_.split(" "))

    // Generate running word count
    val wordCounts = words.groupBy("value").count()

    // Start running the query that prints the running counts to the console
    val query = wordCounts.writeStream
      .outputMode("complete")
      .format("console")
      .start()

    query.awaitTermination()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.sql.streaming

import java.sql.Timestamp

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

/**
 * Counts words in UTF8 encoded, '\n' delimited text received from the network over a
 * sliding window of configurable duration. Each line from the network is tagged
 * with a timestamp that is used to determine the windows into which it falls.
 *
 * Usage: StructuredNetworkWordCountWindowed <hostname> <port> <window duration>
 *   [<slide duration>]
 * <hostname> and <port> describe the TCP server that Structured Streaming
 * would connect to receive data.
 * <window duration> gives the size of window, specified as integer number of seconds
 * <slide duration> gives the amount of time successive windows are offset from one another,
 * given in the same units as above. <slide duration> should be less than or equal to
 * <window duration>. If the two are equal, successive windows have no overlap. If
 * <slide duration> is not provided, it defaults to <window duration>.
 *
 * To run this on your local machine, you need to first run a Netcat server
 *    `$ nc -lk 9999`
 * and then run the example
 *    `$ bin/run-example sql.streaming.StructuredNetworkWordCountWindowed
 *    localhost 9999 <window duration in seconds> [<slide duration in seconds>]`
 *
 * One recommended <window duration>, <slide duration> pair is 10, 5
 */
object StructuredNetworkWordCountWindowed {

  def main(args: Array[String]) {
    if (args.length < 3) {
      System.err.println("Usage: StructuredNetworkWordCountWindowed <hostname> <port>" +
        " <window duration in seconds> [<slide duration in seconds>]")
      System.exit(1)
    }

    val host = args(0)
    val port = args(1).toInt
    val windowSize = args(2).toInt
    val slideSize = if (args.length == 3) windowSize else args(3).toInt
    if (slideSize > windowSize) {
      System.err.println("<slide duration> must be less than or equal to <window duration>")
    }
    val windowDuration = s"$windowSize seconds"
    val slideDuration = s"$slideSize seconds"

    val spark = SparkSession
      .builder
      .appName("StructuredNetworkWordCountWindowed")
      .getOrCreate()

    import spark.implicits._

    // Create DataFrame representing the stream of input lines from connection to host:port
    val lines = spark.readStream
      .format("socket")
      .option("host", host)
      .option("port", port)
      .option("includeTimestamp", true)
      .load()

    // Split the lines into words, retaining timestamps
    val words = lines.as[(String, Timestamp)].flatMap(line =>
      line._1.split(" ").map(word => (word, line._2))
    ).toDF("word", "timestamp")

    // Group the data by window and word and compute the count of each group
    val windowedCounts = words.groupBy(
      window($"timestamp", windowDuration, slideDuration), $"word"
    ).count().orderBy("window")

    // Start running the query that prints the windowed word counts to the console
    val query = windowedCounts.writeStream
      .outputMode("complete")
      .format("console")
      .option("truncate", "false")
      .start()

    query.awaitTermination()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}
// $example off$

object SummaryStatisticsExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("SummaryStatisticsExample")
    val sc = new SparkContext(conf)

    // $example on$
    val observations = sc.parallelize(
      Seq(
        Vectors.dense(1.0, 10.0, 100.0),
        Vectors.dense(2.0, 20.0, 200.0),
        Vectors.dense(3.0, 30.0, 300.0)
      )
    )

    // Compute column summary statistics.
    val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)
    println(summary.mean)  // a dense vector containing the mean value for each column
    println(summary.variance)  // column-wise variance
    println(summary.numNonzeros)  // number of nonzeros in each column
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.SingularValueDecomposition
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.linalg.distributed.RowMatrix
// $example off$

object SVDExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("SVDExample")
    val sc = new SparkContext(conf)

    // $example on$
    val data = Array(
      Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),
      Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),
      Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0))

    val dataRDD = sc.parallelize(data, 2)

    val mat: RowMatrix = new RowMatrix(dataRDD)

    // Compute the top 5 singular values and corresponding singular vectors.
    val svd: SingularValueDecomposition[RowMatrix, Matrix] = mat.computeSVD(5, computeU = true)
    val U: RowMatrix = svd.U  // The U factor is a RowMatrix.
    val s: Vector = svd.s  // The singular values are stored in a local dense vector.
    val V: Matrix = svd.V  // The V factor is a local dense matrix.
    // $example off$
    val collect = U.rows.collect()
    println("U factor is:")
    collect.foreach { vector => println(vector) }
    println(s"Singular values are: $s")
    println(s"V factor is:\n$V")
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
// $example on$
import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.util.MLUtils
// $example off$

object SVMWithSGDExample {

  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("SVMWithSGDExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load training data in LIBSVM format.
    val data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample_libsvm_data.txt")

    // Split data into training (60%) and test (40%).
    val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)
    val training = splits(0).cache()
    val test = splits(1)

    // Run training algorithm to build the model
    val numIterations = 100
    val model = SVMWithSGD.train(training, numIterations)

    // Clear the default threshold.
    model.clearThreshold()

    // Compute raw scores on the test set.
    val scoreAndLabels = test.map { point =>
      val score = model.predict(point.features)
      (score, point.label)
    }

    // Get evaluation metrics.
    val metrics = new BinaryClassificationMetrics(scoreAndLabels)
    val auROC = metrics.areaUnderROC()

    println("Area under ROC = " + auROC)

    // Save and load model
    model.save(sc, "target/tmp/scalaSVMWithSGDModel")
    val sameModel = SVMModel.load(sc, "target/tmp/scalaSVMWithSGDModel")
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

import java.io.{FileOutputStream, PrintWriter}

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.graphx.{GraphXUtils, PartitionStrategy}
import org.apache.spark.graphx.util.GraphGenerators

/**
 * The SynthBenchmark application can be used to run various GraphX algorithms on
 * synthetic log-normal graphs.  The intent of this code is to enable users to
 * profile the GraphX system without access to large graph datasets.
 */
object SynthBenchmark {

  /**
   * To run this program use the following:
   *
   * MASTER=spark://foobar bin/run-example graphx.SynthBenchmark -app=pagerank
   *
   * Options:
   *   -app "pagerank" or "cc" for pagerank or connected components. (Default: pagerank)
   *   -niters the number of iterations of pagerank to use (Default: 10)
   *   -nverts the number of vertices in the graph (Default: 1000000)
   *   -numEPart the number of edge partitions in the graph (Default: number of cores)
   *   -partStrategy the graph partitioning strategy to use
   *   -mu the mean parameter for the log-normal graph (Default: 4.0)
   *   -sigma the stdev parameter for the log-normal graph (Default: 1.3)
   *   -degFile the local file to save the degree information (Default: Empty)
   *   -seed seed to use for RNGs (Default: -1, picks seed randomly)
   */
  def main(args: Array[String]) {
    val options = args.map {
      arg =>
        arg.dropWhile(_ == '-').split('=') match {
          case Array(opt, v) => (opt -> v)
          case _ => throw new IllegalArgumentException("Invalid argument: " + arg)
        }
    }

    var app = "pagerank"
    var niter = 10
    var numVertices = 100000
    var numEPart: Option[Int] = None
    var partitionStrategy: Option[PartitionStrategy] = None
    var mu: Double = 4.0
    var sigma: Double = 1.3
    var degFile: String = ""
    var seed: Int = -1

    options.foreach {
      case ("app", v) => app = v
      case ("niters", v) => niter = v.toInt
      case ("nverts", v) => numVertices = v.toInt
      case ("numEPart", v) => numEPart = Some(v.toInt)
      case ("partStrategy", v) => partitionStrategy = Some(PartitionStrategy.fromString(v))
      case ("mu", v) => mu = v.toDouble
      case ("sigma", v) => sigma = v.toDouble
      case ("degFile", v) => degFile = v
      case ("seed", v) => seed = v.toInt
      case (opt, _) => throw new IllegalArgumentException("Invalid option: " + opt)
    }

    val conf = new SparkConf()
      .setAppName(s"GraphX Synth Benchmark (nverts = $numVertices, app = $app)")
    GraphXUtils.registerKryoClasses(conf)

    val sc = new SparkContext(conf)

    // Create the graph
    println(s"Creating graph...")
    val unpartitionedGraph = GraphGenerators.logNormalGraph(sc, numVertices,
      numEPart.getOrElse(sc.defaultParallelism), mu, sigma, seed)
    // Repartition the graph
    val graph = partitionStrategy.foldLeft(unpartitionedGraph)(_.partitionBy(_)).cache()

    var startTime = System.currentTimeMillis()
    val numEdges = graph.edges.count()
    println(s"Done creating graph. Num Vertices = $numVertices, Num Edges = $numEdges")
    val loadTime = System.currentTimeMillis() - startTime

    // Collect the degree distribution (if desired)
    if (!degFile.isEmpty) {
      val fos = new FileOutputStream(degFile)
      val pos = new PrintWriter(fos)
      val hist = graph.vertices.leftJoin(graph.degrees)((id, _, optDeg) => optDeg.getOrElse(0))
        .map(p => p._2).countByValue()
      hist.foreach {
        case (deg, count) => pos.println(s"$deg \t $count")
      }
    }

    // Run PageRank
    startTime = System.currentTimeMillis()
    if (app == "pagerank") {
      println("Running PageRank")
      val totalPR = graph.staticPageRank(niter).vertices.map(_._2).sum()
      println(s"Total PageRank = $totalPR")
    } else if (app == "cc") {
      println("Running Connected Components")
      val numComponents = graph.connectedComponents.vertices.map(_._2).distinct().count()
      println(s"Number of components = $numComponents")
    }
    val runTime = System.currentTimeMillis() - startTime

    println(s"Num Vertices = $numVertices")
    println(s"Num Edges = $numEdges")
    println(s"Creation time = ${loadTime/1000.0} seconds")
    println(s"Run time = ${runTime/1000.0} seconds")

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.mllib.linalg.Vectors

/**
 * Compute the principal components of a tall-and-skinny matrix, whose rows are observations.
 *
 * The input matrix must be stored in row-oriented dense format, one line per row with its entries
 * separated by space. For example,
 * {{{
 * 0.5 1.0
 * 2.0 3.0
 * 4.0 5.0
 * }}}
 * represents a 3-by-2 matrix, whose first row is (0.5, 1.0).
 */
object TallSkinnyPCA {
  def main(args: Array[String]) {
    if (args.length != 1) {
      System.err.println("Usage: TallSkinnyPCA <input>")
      System.exit(1)
    }

    val conf = new SparkConf().setAppName("TallSkinnyPCA")
    val sc = new SparkContext(conf)

    // Load and parse the data file.
    val rows = sc.textFile(args(0)).map { line =>
      val values = line.split(' ').map(_.toDouble)
      Vectors.dense(values)
    }
    val mat = new RowMatrix(rows)

    // Compute principal components.
    val pc = mat.computePrincipalComponents(mat.numCols().toInt)

    println("Principal components are:\n" + pc)

    sc.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.linalg.distributed.RowMatrix
import org.apache.spark.mllib.linalg.Vectors

/**
 * Compute the singular value decomposition (SVD) of a tall-and-skinny matrix.
 *
 * The input matrix must be stored in row-oriented dense format, one line per row with its entries
 * separated by space. For example,
 * {{{
 * 0.5 1.0
 * 2.0 3.0
 * 4.0 5.0
 * }}}
 * represents a 3-by-2 matrix, whose first row is (0.5, 1.0).
 */
object TallSkinnySVD {
  def main(args: Array[String]) {
    if (args.length != 1) {
      System.err.println("Usage: TallSkinnySVD <input>")
      System.exit(1)
    }

    val conf = new SparkConf().setAppName("TallSkinnySVD")
    val sc = new SparkContext(conf)

    // Load and parse the data file.
    val rows = sc.textFile(args(0)).map { line =>
      val values = line.split(' ').map(_.toDouble)
      Vectors.dense(values)
    }
    val mat = new RowMatrix(rows)

    // Compute SVD.
    val svd = mat.computeSVD(mat.numCols().toInt)

    println("Singular values are " + svd.s)

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.taxis

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging

/**
  * Created by wangbaosheng on 2017/8/14.
  */
class TaxisModelManager(stateConf:StateConf) extends Serializable with Logging {
  //val vehType = "电动出租车"
 // val vehModel = Array("E150EV电动出租车", "E30电动出租车", "E200EV电动出租车", "首望500e电动出租车", "迷迪电动出租车")
 // val areaName = Array("密云区", "顺义区", "大兴区", "市区", "平谷区", "通州区", "怀柔区", "昌平区", "房山区", "延庆区")

  //SYS_VEH_MODEL 车型表
  //SYS_AREA 区域表
  //SYS_DICT 车辆类别表,电动出租车，环卫了等等
  def query(vehType:String,vehModel:Array[String],areaName:Array[String]): Iterable[String] = {
    val sql = new StringBuffer()
    //<uuid,车牌，类别，类型，行政区域>
    sql.append("SELECT UUID, SYS_VEHICLE.LICENSE_PLATE, SYS_DICT.NAME, SYS_VEH_MODEL.VEH_MODEL_NAME, SYS_AREA.NAME")
    sql.append(" FROM SYS_VEHICLE, SYS_DICT, SYS_AREA, SYS_VEH_MODEL")
    sql.append(" WHERE SYS_VEHICLE.VEH_TYPE_ID = SYS_DICT.ID")
    sql.append(" AND SYS_DICT.NAME = '" + vehType + "'")
    sql.append(" AND SYS_VEHICLE.VEH_MODEL_ID = SYS_VEH_MODEL.ID")

    vehModel.indices.foreach(i => {
      sql.append(
        if(i==0)" AND (SYS_VEH_MODEL.VEH_MODEL_NAME = '" + vehModel(i) + "'"
        else " OR SYS_VEH_MODEL.VEH_MODEL_NAME = '" + vehModel(i) + "'")
    })

    sql.append(")")

    sql.append(" AND SYS_VEHICLE.SYS_DIVISION_ID = SYS_AREA.ID")

    areaName.indices.foreach(i => {
      sql.append(
        if(i==0)" AND (SYS_AREA.NAME = '" + areaName(0) + "'"
        else  " OR SYS_AREA.NAME = '" + areaName(i) + "'")
    })

    sql.append(")")

    val result=new scala.collection.mutable.ListBuffer[String]()

    JdbcPoolHelper.getJdbcPoolHelper(stateConf).executeQuery(sql.toString, resultSet => {
      while (resultSet.next()) {
        result.append(s"${resultSet.getString("UUID")},${resultSet.getString("LICENSE_PLATE")}")
      }
    })

    result
  }
}
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import com.bitnei.report.taxis.Compute
import com.bitnei.sparkhelper.SparkHelper
import org.scalatest.FunSuite

import scala.io.Source

case class Point(x:Int,y:Int)

case class X(x:Int)
case class Y(y:Int)

case class PointComposite(a:X,b:Y)
/**
  * Created by francis on 2017/2/16.
  */
class TaxisModelTest extends  FunSuite {
//  def createDetailModel(startTime: String, endTime: String, category: String): DetailModel = {
//    val startDate = Utils.parsetDate(startTime, "yyyy-MM-dd HH:mm:ss").get.getTime
//    val endDate = Utils.parsetDate(endTime, "yyyy-MM-dd HH:mm:ss").get.getTime
//
//    DetailModel("vid1", "vin",category, 0,
//      startDate, endDate, endDate - startDate,
//      10, 10000, 20000, 12000, 13000, 0, 0, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10D, 10, 10, 10, 10, ChargeDistributed()
//    )
//  }
//
//  test("doMerge") {
//    val stateConf = new StateConf
//    val sortedValeus = Array(
//      createDetailModel("2017-01-01 23:23:23", "2017-01-01 23:28:23", Constant.ChargeState),
//      createDetailModel("2017-01-01 23:30:23", "2017-01-01 23:40:23", Constant.ChargeState),
//      createDetailModel("2017-01-01 23:50:23", "2017-01-01 23:55:23", Constant.ChargeState),
//      createDetailModel("2017-01-02 00:01:10", "2017-01-02 00:15:10", Constant.ChargeState),
//      //createDetailModel("2017-01-02 00:20:10","2017-01-02 00:30:10",Constant.ChargeState),
//      createDetailModel("2017-01-03 00:20:10", "2017-01-03 00:30:10", Constant.ChargeState)
//    )
//
//    val cmp = new Compute(stateConf).compute(sortedValeus).toArray
//    assert(cmp.size == 4)
//
//    assert(cmp(2).startTime == sortedValeus(2).startTime && cmp(2).endTime == sortedValeus(3).endTime &&
//      cmp(2).startSoc == sortedValeus(2).startSoc && cmp(2).endSoc == sortedValeus(3).endSoc &&
//      cmp(2).startLatitude == sortedValeus(2).startLatitude && cmp(2).stopLatitude == sortedValeus(3).endLatitude &&
//      cmp(2).startLongitude == sortedValeus(2).startLongitude && cmp(2).stopLongitude == sortedValeus(3).endLongitude &&
//      cmp(2).startMileage== sortedValeus(2).startMileage && cmp(2).endMileage == sortedValeus(3).stopMileage &&
//      cmp(2).maxCurent == Math.max(sortedValeus(2).maxTotalCurrent,sortedValeus(3).maxTotalCurrent)&&
//      cmp(2).mileage == sortedValeus(3).stopMileage-sortedValeus(2).startMileage
//      //  cmp(2).startMileage == Math.max(sortedValeus(2).maxTotalCurrent,sortedValeus(3).maxTotalCurrent) &&
//     // cmp(2).startTime == sortedValeus(2).startTime && cmp(2).endTime == sortedValeus(3).endTime
//    )
//    //cmp.foreach(x=>println(x))
//  }
//
//
//  test("sss"){
//    val stateConf=new StateConf
//    val sparkSession = SparkHelper.getSparkSession(sparkMaster = Some("local[2]"))
//
//    import sparkSession.implicits._
//    sparkSession.createDataset(Array(Point(1,2),Point(2,3))).as[PointComposite].show(false)
//  }
  //  test("query") {
  //    //    val stateConf=new StateConf
  //    //    stateConf.set("temp.taxis.report.conf","./src/main/resources/tempreport/temp_report_conf")
  //    //
  //    //    val manger=new TaxisModelManager(stateConf)
  //    //    val lines=Source.fromFile("./src/main/resources/temp_report_conf.txt").getLines()
  //    //
  //    //
  //    //    val values=lines.map(line=>{
  //    //      val keyvalue=line.split('=')
  //    //      (keyvalue(0),keyvalue(1))
  //    //    }).toMap
  //    //
  //    //    val vehType = values("vehicle.type")
  //    //    val vehModel=values("vehicle.model").split(',')
  //    //    val areaName=values("vehicle.area").split(',')
  //    //
  //    //    manger.query(vehType,vehModel,areaName).foreach(println)
  //
  //    val sparkSession = SparkHelper.getSparkSession(sparkMaster = Some("local[2]"))
  //    SparkHelper.jdbc(sparkSession.sqlContext,
  //      "jdbc:oracle:thin:@192.168.2.51:1521:evmsc1",
  //      "oracle.jdbc.driver.OracleDriver",
  //      "sys_vehicle", "ev", "ev",
  //      Array.empty[String]).show(10)
  //  }
}
package com.bitnei.report.taxis

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql.{Dataset, SparkSession}

/*
* created by wangbaosheng on 2017/11/2
*/
class TaxisOutputJob(stateConf: StateConf, sparkSession: SparkSession) extends Serializable with Logging with Job {
  override type R = TaxisResult
  private val sqlContext = sparkSession.sqlContext

  import sqlContext.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("taxis")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("detail")

  override def registerIfNeed() = SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)

  override def unRegister() = sparkSession.catalog.dropTempView(inputTableName)


  override def doCompute[Product <: TaxisResult]() = sqlContext.sql(s"SELECT * FROM $inputTableName ${SqlHelper.buildWhere(stateConf)}").as[TaxisResult]


  override def write[Product <: TaxisResult](result: Dataset[TaxisResult]) = {
    val outputPaths = stateConf.getString("report.output").split(',')
    outputPaths.foreach({
      case e if e == "oracle" || e == "mysql" =>
        result.foreachPartition(values=>{
          new TaxisOutputManager(stateConf,stateConf.getString("reportType")).output(values.toIterable)
        })
      case e => throw new Exception(s"report.output=$e")
    })
  }
}

object TaxisOutputJob {
  def main(args: Array[String]): Unit = {
    val stateConf = new StateConf
    stateConf.add(args)

    new TaxisOutputJob(stateConf, SparkHelper.getSparkSession(sparkMaster = None)).compute()
  }
}package com.bitnei.tempReport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.constants.Constant
import com.bitnei.report.taxis.{TaxisChargeOutputManager, TaxisResult, TaxisRunOutputManager}
import org.scalatest.FunSuite

class TaxisOutputManagerTest extends  FunSuite  {
  test("charge output"){
    val stateConf=new StateConf
    def configOracle(): Unit = {
      stateConf.set(Constant.JdbcUserName, "ev")
      stateConf.set(Constant.JdbcPasswd, "ev")
      stateConf.set(Constant.JdbcDriver, "oracle.jdbc.driver.OracleDriver")
      stateConf.set(Constant.JdbcUrl, "jdbc:oracle:thin:@192.168.2.51:1521:evmsc1")
      stateConf.set("database", "oracle")
    }

    configOracle()
    val output=new TaxisChargeOutputManager(stateConf,"month")
    val startTime=Utils.parsetDate("20171111000002").get.getTime
    val endTime=Utils.parsetDate("20171111010001").get.getTime
    val values=Array(
      TaxisResult("vid1",Constant.ChargeState,startTime,endTime,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0D,Some(0),Some(0),Some(false))
    )
    output.output(values)
  }
}
package com.bitnei.report.taxis

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{SystemClock, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.report.{Job, JobRunner}
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.SparkConf
import org.apache.spark.sql.{Dataset, SaveMode, SparkSession}
import org.apache.spark.storage.StorageLevel


class TaxisReportJob(@transient sparkSession: SparkSession, stateConf:StateConf) extends Serializable with Logging with Job {
  private val clock = new SystemClock
  @transient private val conf = stateConf.getOption(Constant.SPARK_DEPLOY_LOCAL) match {
    case Some(master) if master.startsWith("local") => new SparkConf().setMaster(master)
    case _ => new SparkConf()
  }

  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)
  @transient private val sqlContext = sparkSession.sqlContext

  import sqlContext.implicits._

  override type R = TaxisResult

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("detail")
  private val outputTableName = stateConf.getOption("output.table.name").getOrElse("taxisWeek")

  override def registerIfNeed(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession, stateConf, inputTableName)
  }

  override def doCompute[Product <: TaxisResult](): Dataset[TaxisResult] = {
    val isQuickChargeColumnExists = sparkSession.catalog.listColumns(inputTableName).filter(_.name == "isQuickCharge").count() > 0
    val sqlText =
      s"""
       SELECT
         VID,
         category,
         startTime,
         endTime,
         timeLeng,
         accRunTime,
         startMileage,
         stopMileage,
         stopMileage-startMileage AS mileage,
         maxSpeed,
         startSoc,
         endSoc,
         startLongitude,
         startLatitude,
         endLongitude,
         endLatitude,
         totalCharge,
         minTotalCurrent,
         ${if (isQuickChargeColumnExists) "isQuickCharge" else "false AS isQuickCharge"}
       FROM  $inputTableName ${SqlHelper.buildWhere(stateConf)}
    """.stripMargin


    val realinfoDs = sqlContext.sql(sqlText).as[DetailModel]

    val taxisResult = realinfoDs.groupByKey(_.vid)
      .flatMapGroups((vid: String, rows: Iterator[DetailModel]) => {
        //按照时间排序并去重
        val sortedRows = Utils.strict[DetailModel, Long](
          Utils.sortByDate2(rows.toArray, row => Some(row.startTime)),
          row => Some(row.startTime)
        )
        new Compute(stateConf).compute(sortedRows)
      })
    taxisResult.repartition(stateConf.getOption("finalPartitionNum").map(_.toInt).getOrElse(5))
  }

  override def write[Product <: TaxisResult](result: Dataset[TaxisResult]): Unit = {
    stateConf.getOption("report.output") match {
      case Some(model) =>
        result.persist(StorageLevel.MEMORY_ONLY_SER)
        model.split(',').foreach(m => {
          if (m == "hdfs") {
            SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), outputTableName)
          }
          else if (m == "oracle" || m == "mysql") {
            logInfo(s"output to $m")
            result.foreachPartition(values => {
              new TaxisOutputManager(stateConf, stateConf.getString("reportType")).output(values.toIterable)
            })

          }
        })
      case None => throw new IllegalArgumentException("the output is empty")
    }
  }
}


object  TaxisReportJob extends Logging {
  def main(args: Array[String]): Unit = {
    val sparkSession = SparkHelper.getSparkSession(None)
    val stateConf = new StateConf
    stateConf.add(args)

    new TaxisReportJob(sparkSession, stateConf).compute()
  }
}package com.bitnei.report.taxis

import java.util.Date

import com.bitnei.report.Job
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{SystemClock, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.SparkConf
import org.apache.spark.sql.{Dataset, SparkSession}

case class Result(areaId:String,value:String) {
  override def toString: String = value
}

/**
  * Created by wangbaosheng on 2017/9/18.
  * 根据查询条件，导出符合格式的计算结果
  */
class TaxisReportQueryUtils(@transient sparkSession: SparkSession, stateConf:StateConf) extends Serializable with Logging with Job {
  override type R = String

  private val clock = new SystemClock
  @transient private val conf = stateConf.getOption(Constant.SPARK_DEPLOY_LOCAL) match {
    case Some(master) if master.startsWith("local") => new SparkConf().setMaster(master)
    case _ => new SparkConf()
  }
  @transient private val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)
  @transient private val sqlContext = sparkSession.sqlContext

  import sqlContext.implicits._

  private val inputTableName = stateConf.getOption("input.table.name").getOrElse("taxisweek")
  override def registerIfNeed() = {
    def registeSysInfo() {
      val url = stateConf.getString(Constant.JdbcUrl)
      val driver = stateConf.getString(Constant.JdbcDriver)
      val user = stateConf.getString(Constant.JdbcUserName)
      val password = stateConf.getString(Constant.JdbcPasswd)

      //默认跑全国数据
      val sql = stateConf.getOption("lease.sys_sql").getOrElse(
        """SELECT
            uuid AS vid,
            veh.LICENSE_PLATE AS licensePlate,
            dict.NAME AS dictName,
            sysModel.VEH_MODEL_NAME AS modelName,
            area.ID AS areaId,
            sysRule.ID as ruleID
         FROM SYS_VEHICLE veh INNER JOIN SYS_RULE sysRule ON veh.RULE_ID=sysRule.ID
         INNER JOIN  SYS_DICT dict ON veh.VEH_TYPE_ID = dict.ID
         INNER JOIN  SYS_VEH_MODEL sysModel ON  veh.VEH_MODEL_ID = sysModel.ID
         INNER JOIN  SYS_AREA area ON veh.SYS_DIVISION_ID = area.ID""".stripMargin
      )
      logInfo(sql)
      SparkHelper.registerJdbcAsTempView(sqlContext, url, driver, user, password, "sys_info", sql, 5)
    }

    def registeTaxis(): Unit = {
      SparkHelper.createOrReplaceTempView(sparkSession,stateConf,inputTableName)
    }

    def registeArea() {
      val areaUtil = new AreaUtil(stateConf, sparkSession)
      areaUtil.areaTable.createOrReplaceTempView("area")
    }

    registeTaxis()
    registeSysInfo()
    registeArea()
  }



  override def doCompute[Product <: String]() = {
    val sysTable = sparkSession.sql("SELECT  vid,licensePlate,dictName,modelName,areaId,CAST(ruleId AS int) FROM sys_info").as[TaxisSysModel]
    val taxisTable = sparkSession.sql(
      s"""SELECT vid,
             category,
             startTime,
             endTime,
             timeLength,
             accRunTime,
             startMileage,
             endMileage,
             mileage,
             startSoc,
             endSoc,
             startLongitude,
             startLatitude,
             stopLongitude,
             stopLatitude,
             timeBetweenCharge,
             charge,
             prevChargeEndMileage,
             maxCurent,
             avgSpeed,
             prevChargeEndTime,
             prevChargeMaxCurrent,
             isQuickCharge
        FROM $inputTableName
        ${SqlHelper.buildWhere(stateConf)} """.stripMargin).as[TaxisResult]


    val areaTable = sparkSession.sql("SELECT * FROM area").as[Area]

    val values = sysTable.joinWith(taxisTable, sysTable.col("vid") === taxisTable.col("vid"))
      .map({ case (model: TaxisSysModel, taxis: TaxisResult) =>
        Result(model.areaId, s"${model.toString},${taxis.simpleString(showVid = false)}")
      })
    values.joinWith(areaTable, values.col("areaId") === areaTable.col("id"))
      .map({ case (r: Result, area: Area) =>
        s"${r.toString},${area.toString}"
      })
  }


  override def write[Product <: String](result: Dataset[String]) = {
    result.repartition(stateConf.getOption("output.file.count").map(_.toInt).getOrElse(3))
      .write.text(stateConf.getString("output.path"))
  }
}


object TaxisReportQueryUtils{
  def main(args: Array[String]): Unit = {
    val stateConf=new StateConf
    stateConf.add(args)

    new TaxisReportQueryUtils(SparkHelper.getSparkSession(sparkMaster = None),stateConf).compute()
  }
}package com.bitnei.report.taxis

import java.util.Date

import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant

/**
  * Created by wangbaosheng on 2017/9/21.
  */

case class TaxisResult(
                        vid:String,
                        category:String,
//                        year:String,
//                        month:String,
//                        day:String,
                        startTime:Long,
                        endTime:Long,
                        timeLength:Int,
                        accRunTime:Int,//剔除无效行驶后的实际行驶时间，
                        startMileage:Int,
                        endMileage:Int,
                        mileage:Int,
                        startSoc:Int,
                        endSoc:Int,
                        startLongitude:Long,
                        startLatitude:Long,
                        stopLongitude:Long,
                        stopLatitude:Long,

                        //充电专有的
                        timeBetweenCharge:Long,
                        charge:Double,
                        prevChargeEndMileage:Int,
                        maxCurent:Int,

                        //行驶的
                        avgSpeed:Double,
                        prevChargeEndTime:Option[Long],
                        prevChargeMaxCurrent:Option[Int],
                        isQuickCharge:Option[Boolean]) {
  override def toString: String = simpleString(showVid = true)


  def mileageBetweenCharge:Option[Double]= if(startMileage!=0&&prevChargeEndMileage!=0)
    Some(DataPrecision.mileage(startMileage-prevChargeEndMileage)) else None


  def simpleString(showVid: Boolean): String = {
    if (category == Constant.ChargeState || category == Constant.ChargeChangeState) {
      f"${if (showVid) vid + "," else ""}${if (category == Constant.ChargeState) Constant.ChargeState else Constant.ChargeChangeState}," +
        f"${Utils.formatDate(new Date(startTime), "yyyy-MM-dd HH:mm:ss")}," +
        f"${Utils.formatDate(new Date(endTime), "yyyy-MM-dd HH:mm:ss")}," +
        f"${DataPrecision.toHour(timeLength)}%1.2f,${DataPrecision.soc(startSoc)}%%,${DataPrecision.soc(endSoc)}%%,${DataPrecision.toHour(timeBetweenCharge)}," +
        f"$charge%1.1f,${if (prevChargeEndMileage == 0) " " else DataPrecision.mileage(prevChargeEndMileage)}," +
        f"${if (startMileage == 0) " " else DataPrecision.mileage(startMileage)},"+
        f"${mileageBetweenCharge.getOrElse("")}," +
        f"${DataPrecision.latitude(startLongitude)},${DataPrecision.latitude(startLatitude)},${DataPrecision.latitude(stopLongitude)},${DataPrecision.latitude(stopLatitude)}," +
        f"${DataPrecision.totalCurrent(maxCurent)}," +
        f"${if (isQuickCharge.contains(true)) "快充" else if (isQuickCharge.contains(false)) "慢充" else "未知"}"
    } else if (category == Constant.TravelState) {
      f"${if (showVid) vid + "," else ""}${Constant.TravelState}," +
        f"${Utils.formatDate(new Date(startTime), "yyyy-MM-dd HH:mm:ss")}," +
        f"${Utils.formatDate(new Date(endTime), "yyyy-MM-dd HH:mm:ss")}," +
        f"${DataPrecision.toHour(timeLength)}%1.2f,${DataPrecision.toHour(accRunTime)}%1.2f,"+
        f"${DataPrecision.mileage(startMileage)},${DataPrecision.mileage(endMileage)},${DataPrecision.mileage(mileage)}," +
        f"${DataPrecision.soc(startSoc)}%%,${DataPrecision.soc(endSoc)}%%,${avgSpeed}%1.1f," +
        f"${if (prevChargeEndTime.nonEmpty && prevChargeEndTime.exists(_ > 0)) Utils.formatDate(new Date(prevChargeEndTime.get), "yyyy-MM-dd HH:mm:ss") else " "}," +
        f"${if (prevChargeMaxCurrent.nonEmpty) DataPrecision.totalCurrent(prevChargeMaxCurrent.get) else " "}," +
        f"${DataPrecision.latitude(startLongitude)},${DataPrecision.latitude(startLatitude)},${DataPrecision.latitude(stopLongitude)},${DataPrecision.latitude(stopLatitude)}"
    } else {
      ""
    }
  }
}package com.bitnei.report.taxis

/**
  * Created by wangbaosheng on 2017/8/14.
  */
case class TaxisSysModel(vid:String, licensePlate:String, dictName:String, modelName:String, areaId:String, ruleId:Option[Int]) {
  override def toString: String = s"$licensePlate,$dictName,$modelName,${if (ruleId.contains(1)) "DB" else "GB"}"
}
//package com.bitnei.test.model
//
//import java.text.SimpleDateFormat
//
//import com.bitnei.report.common.configuration.StateConf
//import com.bitnei.report.common.utils.{SparkHelper, Utils}
//import com.bitnei.report.constants.Constant
//import com.bitnei.report.stateGenerate.Window
//import com.bitnei.tempReport.tempReport.TaxisTempDayReport
//import com.bitnei.tempReport.tempReport.schema.TaxisAndRealInfo
//import org.scalatest.FunSuite
//
///**
//  * Created by francis on 2017/2/17.
//  */
//class TaxisTempDayReportTest extends FunSuite{
//  val stateConf = new StateConf
//  stateConf.set("debug","true")
//  stateConf.set("taxistable","file")
//  stateConf.set("temp.taxis.report.conf","./src/main/resources/temp_report_conf.txt")
//  stateConf.set("temp.taxis.report.uuidtable","./src/main/sources/taxistable.txt")
//  val taxisComputer = new TaxisTempDayReport(SparkHelper.getSparkSession(sparkMaster = Some("local")),stateConf,Array(""))
//
//  test("getStateResult") {
//    def generateWindow(state: String, startTime: String,mileage:Int): Window[TaxisAndRealInfo] = {
//      val window = new Window[TaxisAndRealInfo]()
//      val date = Utils.parsetDate(startTime).get
//
//      val vid="27E2ADD690217EB1E0533C02A8C0B094"
//      window.append(TaxisAndRealInfo(vid, startTime, Some(0), Some(0), Some(0), Some(0), Some(0), Some(0),
//        Some(0), Some(0), Some(0), Some(0), Some(mileage), Some(0),"0","0","0","0","21"))
//
//      date.setSeconds(date.getSeconds + 1)
//
//
//      window.append(TaxisAndRealInfo(vid, s"${new SimpleDateFormat("yyyyMMddHHmmss").format(date)}", Some(0), Some(0), Some(0), Some(0), Some(0), Some(0),
//        Some(0), Some(0), Some(0), Some(0), Some((mileage+1)), Some(0),"0","0","0","0","21"))
//      window.windowId = s"$vid|${window.head.time}|${window.last.time}|$state"
//      window
//    }
//
//
//    val states1 = Array(
//      /**
//        * 1.充电，行驶，行驶，充电
//        * 2.行驶，充电，充电，行驶
//        **/
//      generateWindow(Constant.ChargeState, "20170217100000",1992),
//      generateWindow(Constant.TravelState, "20170217100004",1995),
//      generateWindow(Constant.TravelState, "20170217100007",1998),
//      generateWindow(Constant.ChargeState, "20170217100009",2001)
//    )
//
//    val states1Result=taxisComputer.getStateResult(states1).toArray
//
//    //主要测试上次充电开始里程
//   // assert(states1Result(0).split(',')(10)==0)
//   // assert()
//
//    val states2 = Array(
//      /**
//        * 2.行驶，充电，充电，行驶
//        **/
//      generateWindow(Constant.TravelState, "20170217100004",1992),
//      generateWindow(Constant.ChargeState, "20170217100000",1995),
//      generateWindow(Constant.ChargeState, "20170217100009",1998),
//      generateWindow(Constant.TravelState, "20170217100007",2001)
//
//    )
//
//    taxisComputer.getStateResult(states2).foreach(println)
//  }
//
//
//  test("taxisTable"){
//    stateConf.set("debug","false")
//    stateConf.set(Constant.JdbcOracleUrl,"jdbc:oracle:thin:@192.168.2.51:1521:evmsc1")
//    stateConf.set("temp.taxis.report.conf","./src/main/resources/temp_report_conf.txt")
//    taxisComputer.registerIfNeed()
//
////    taxisComputer.getTaxisTable()//.map(_.toString).write.text("./leasetable")
//   // result.foreach(x=>println(x))
//  }
//
//
//  test("reainfo") {
//    stateConf.set(Constant.RealinfoPath, "./src/main/resources/01c3d73e-a3c5-4b85-aa2c-4d330c8c6015")
//    taxisComputer.registerIfNeed()
//
// //   taxisComputer.getRealinfoTable().show(2)
//  }
//
//  test("taxi join realinfo"){
//    taxisComputer.doCompute().foreach(x=>println(x))
//  }
//
//
//  test("compute"){
//    stateConf.set("debug","false")
//    stateConf.set(Constant.JdbcOracleUrl,"jdbc:oracle:thin:@192.168.2.51:1521:evmsc1")
//    stateConf.set("temp.taxis.report.conf","./src/main/resources/temp_report_conf.txt")
//    val r= taxisComputer.doCompute()
//    r.write.text("./aa")
//  }
//
//
//}
package com.bitnei.report

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-01-31 11:43
  *
  */
object DayReport extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
////////////////////////////////////////////////












    sparkSession.stop()
  }

}
package com.bitnei.report.tmp

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.constants.Constant
import com.bitnei.sparkhelper.SparkHelper
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.log4j.{Level, Logger}


/**
  * 需求：
  * "vid":"01e2868d-e308-4209-ac9f-9bec6314ac0c"
  * "vin":"LA9CA8N02GBBFC423"
  * "date":"201711"
  * "mielage":277.1
  * "hours":15.45
  * "days":4
  * "coords":[[0.0] [0.0] [0.0] [0.0] [0.0] [0.0]]
  *
  */
object TestDataExporter {


  def main(args: Array[String]): Unit = {

    // TODO: 参数集合
    val stateConf = new StateConf
    stateConf.add(args)

    // TODO: 验证参数
//    val yearMonth = stateConf.getOption("input.month").get
//
//    if (yearMonth.length != 6) {
//      throw new Exception("input.month error")
//    }
//
//    val year = yearMonth.substring(0, 4)
//
//    val month = yearMonth.substring(4)

    // TODO: 日志级别设置
    Logger.getLogger("org").setLevel(Level.ERROR)

    // TODO: 上下文
    val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)

    val sparkConf = sparkSession.conf
    sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");

    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //    sparkConf.set("spark.kryo.registrationRequired", "true")


    // TODO: HDFS api
    val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
    val fs = FileSystem.get(hadoopConfiguration)



    ////////////////////////////////////////////////
    // TODO: 将数据注册成表
//    sparkSession.sqlContext
//      .read
//      .format("parquet")
//      .load(s"/spark/vehicle/data/realinfo/year=${year}/month=${month}").createOrReplaceTempView("realinfo")
//
//
//    sparkSession.sqlContext
//      .read
//      .format("parquet")
//      .load(s"/spark/vehicle/result/dayreport/year=${year}/month=${month}").createOrReplaceTempView("dayreport")

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

    // TODO: 实时数据计算

//    val hdfsPath = "/tmp/zyt/days"

//    sparkSession.read.textFile(hdfsPath).as[String]
//      .map(x => x.substring(x.indexOf("days"), 10))
//      .repartition(1).write.text("/tmp/zyt/days")

    //    sparkSession.sql("SELECT CAST(COUNT(distinct date_format(from_unixtime(reportDate/1000), 'yyyyMMdd')) AS int)  AS dayNum  FROM dayreport where VID = '1c2092bb-a9f2-4419-b347-4c4e254a8529'").show(1000)

    //      .repartition(1).write.json(s"/tmp/zyt/dayreport-${System.currentTimeMillis()}")

//    if (fs.exists(new Path("/tmp/spark/mileagecheck/coor_201710/_SUCCESS"))) println("!!!!!!!!!!!!!!!!!!!!!!/tmp/spark/mileagecheck/coor_201710/_SUCCESS 存在！")
//
//
//    sparkSession.sql(s"SELECT *, date_format(from_unixtime(reportDate/1000), 'yyyyMMdd') as dateStr FROM dayreport where category = '${Constant.TravelState}' and  VID = '27E2ADD6870B7EB1E0533C02A8C0B094'").repartition(1).write.json(s"/tmp/zyt/dayreport2-${System.currentTimeMillis()}")


    //    val realinfoDF = sparkSession.sql(s"SELECT VID,VIN,TIME,`2502` AS longitude,`2503` AS latitude FROM realinfo ")
    //
    //    val realinfoRDD = realinfoDF
    //      .filter(x => {
    //        x.getAs[String]("longitude").matches("^(\\d+)$") && x.getAs[String]("latitude").matches("^(\\d+)$")
    //      })
    //
    //      .map(x => {
    //        val vid = x.getAs[String]("VID")
    //        val vin = x.getAs[String]("VIN")
    //        val time = x.getAs[String]("TIME")
    //
    //        val longitude = x.getAs[String]("longitude").toLong
    //
    //        val latitude = x.getAs[String]("latitude").toLong
    //
    //        RealinfoInput(vid, vin, time, longitude, latitude)
    //      })
    //
    //    realinfoRDD.count()

    //    // TODO: 分组
    //    val realinfoGroupRDD = realinfoRDD.groupBy(x => {
    //      x.vid + "\t" + x.vin
    //    })
    //
    //    val realinfoResult = realinfoGroupRDD
    //      .map(x => {
    //        val cols = x._1.toString.split("\t")
    //        val vid = cols(0)
    //        val vin = cols(1)
    //
    //        // TODO: 按时间排序
    //        //      val realinfoIter: Iterable[MileageCheckJobTest.RealinfoInput] = x._2
    //        val sortedRealinfoArr = x._2.toArray.sortBy(x => x.time)
    //
    //        // TODO: 验证经纬度的正确性
    //        var prevLontitude = 0L
    //        var prevLatitude = 0L
    //
    //        val hightThreshold = 10000
    //        val lowThreshold = 100
    //
    //        val filterRealInfoArr = sortedRealinfoArr.filter(realinfo => {
    //
    //          val longitude = realinfo.longitude
    //          //                .getOrElse(0L)
    //          val latitude = realinfo.latitude
    //          //                .getOrElse(0L)
    //
    //          //          val ruler01 = (prevLatitude == 0 || prevLontitude == 0)
    //
    //          val longitudeDiff = Math.abs(longitude - prevLontitude)
    //          val latitudeDiff = Math.abs(latitude - prevLatitude)
    //
    //          val ruler02 = longitudeDiff >= lowThreshold && latitudeDiff >= lowThreshold
    //
    //          val ruler03 = longitudeDiff <= hightThreshold && latitudeDiff <= hightThreshold
    //
    //          prevLontitude = longitude
    //          prevLatitude = latitude
    //
    //          //          ruler01 &&
    //          ruler02 && ruler03
    //        })
    //        println(vid + "----filtered row num : " + (sortedRealinfoArr.length - filterRealInfoArr.length))
    //
    //        (vid, vin, filterRealInfoArr)
    //
    //      })
    //
    //      .map(x => {
    //        val vid = x._1
    //        val vin = x._2
    //        // TODO: 转换经纬度类型
    //        val coords = x._3
    //          .map(row => {
    //            Array[Double](DataPrecision.latitude(row.longitude), DataPrecision.latitude(row.latitude))
    //          })
    //        (vid.toString, (vin, coords))
    //      })
    //
    //
    //    // TODO: 日报表数据计算
    //    val dayReportDF = sparkSession.sql(
    //      s"""
    //         |SELECT
    //         | VID,
    //         | CAST(SUM(totalMileage) AS int) AS totalMileage,
    //         | CAST(SUM(timeLeng) AS int) AS runtime,
    //         | CAST(COUNT(*) AS int) AS dayNum
    //         | FROM dayreport where category = '${Constant.TravelState}'  GROUP BY VID
    //          """.stripMargin)
    //
    //    val dayReportResult =
    //      dayReportDF.rdd.map(x => {
    //        val vid = x.getAs[String]("VID")
    //        val totalMileage = x.getAs[Int]("totalMileage")
    //        val runtime = x.getAs[Int]("runtime")
    //        val dayNum = x.getAs[Int]("dayNum")
    //
    //        (vid.toString, (totalMileage, runtime, dayNum))
    //      })
    //
    //    // TODO: 关联
    //    val result = realinfoResult.leftOuterJoin(dayReportResult).map(x => {
    //      val vid = x._1
    //
    //      val left = x._2._1
    //      val vin = left._1
    //      val coords = left._2
    //
    //      val right = x._2._2
    //
    //      var totalMileage = 0D
    //      var runtime = 0D
    //      var dayNum = 0
    //
    //
    //      if (!right.isEmpty) {
    //        totalMileage = right.get._1.toString.toDouble
    //        runtime = right.get._2.toString.toDouble
    //        dayNum = right.get._3.toString.toInt
    //      }
    //
    //
    //      runtime = DataPrecision.toHour(runtime)
    //
    //      totalMileage = DataPrecision.mileage(totalMileage)
    //
    //      MonthCoord(
    //        vid,
    //        vin,
    //        yearMonth,
    //        totalMileage,
    //        runtime,
    //        dayNum,
    //        coords
    //      )
    //    }).distinct
    //
    //    result.count()

    ////////////////////////////////////////////////////////////////////////////////////////////////////////////////
    //    // TODO: 输出到HBase
    //    val quorum = stateConf.getOption("hbase.quorum").get
    //    val zkport = stateConf.getOption("hbase.zkport").get
    //    val htableName = "mileage_check_coords"
    //
    //    result.repartition(80).foreachPartition(monthCoords => {
    //      HbaseHelper.bulkPut(quorum, zkport, htableName, (table) => {
    //        val mapper = new ObjectMapper()
    //        mapper.registerModule(DefaultScalaModule)
    //        monthCoords.foreach(monthCoord => {
    //          val rowKey = s"${monthCoord.vid}_${monthCoord.date}"
    //          table.put(HbaseHelper.createRow(Bytes.toBytes(rowKey), "df", "coord", toJson(mapper, monthCoord)))
    //        })
    //      })
    //    })
    //


    //    // TODO: 输出到HDFS

    ///spark/vehicle/result/mileagecheck
    //    result.mapPartitions(monthCoords => {
    //      toJson(monthCoords.toArray).toIterator
    //    }).toDF().write.format("text").mode(SaveMode.Overwrite).save(s"${stateConf.getString("output.hdfs.path")}/${yearMonth}")

    sparkSession.stop()
  }


  /////////////////////////////////////////////////////////////////////////////////////////////////////



}

package com.bitnei.report.operationIndex

import java.io.{File, PrintWriter}

import com.bitnei.report.common.GpsDistance
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.mileage.{MileageComputer, MileageResult, Realinfo}
import com.bitnei.report.common.utils.{DataPrecision, FileWriter, StringParser, Utils}
import com.bitnei.report.operationIndex.TestUtil.{logError, logInfo}

import scala.io.Source

/*
* created by wangbaosheng on 2017/11/23
* 测试工具
*
*/


class TestUtil(stateConf: StateConf)extends Logging {
  val uuidTableFile = stateConf.getString("uuid_number_path")

  private val uuidTable = readUuidTable(uuidTableFile)

  val debugOutput = new FileWriter(stateConf.getString("output.directory")+".debug")
  val dataOutput = new FileWriter(stateConf.getString("output.directory")+".data")
  def run(): Unit = {
    //输入一批车，一辆车一个文件。然后计算
    processEachVehicle(oneVehicleRealinfos => {
      logInfo("computing......")
      println("computing......")
      val result = doCompute(oneVehicleRealinfos)

      logInfo("writeing......")
      write(result)

      (0 until oneVehicleRealinfos.length).foreach(i=>oneVehicleRealinfos(i)=null)
    },
      fileName=>true
    )

    debugOutput.close()
  }

  def processEachVehicle(f: (Array[Realinfo]) => Unit,filter:(String)=>Boolean): Unit = {
    def doProcess(inDirectory: File): Unit = {
      inDirectory.listFiles().foreach(file => {
        if (file.isDirectory) {
          doProcess(file)
        } else {
          try {
            val vid = file.getName
            val number = uuidTable.getOrElse(vid, "")
            if(filter(vid)){
              if (number.isEmpty && stateConf.getOption("skipEmptyVehicle").contains("true")) {
                logInfo(s"because number of $vid  is empty,so skip it .....")
              } else {
                logInfo(s"parseing $vid,$number ......")
                println(s"parseing $vid,$number ......")

                val parsedValues = TestUtil.parse(vid, number, Utils.readAllLines(file.getAbsolutePath))

                f(parsedValues)
                logInfo(s"end ${file.getAbsoluteFile}")
              }
            }
          } catch {
            case e: Exception =>
              logError(e.getMessage)
              println(e.getMessage)
              logError(s"${file.getAbsoluteFile},${inDirectory.getName}")
          }

        }
      })
    }

    val inDirectory = new File(stateConf.getString("input.directory"))

    doProcess(inDirectory)
  }



  def readUuidTable(uuidFile: String): Map[String, String] = {
    val lines = Source.fromFile(uuidFile).getLines()
    val uuidTable = scala.collection.mutable.Map[String, String]()
    lines.foreach(line => {
      val fields = StringParser.toMap(line)
      logInfo(line)
      uuidTable.put(fields("uuid"), fields("number"))
    })

    uuidTable.toMap
  }


  def doCompute(realinfoes: Array[Realinfo]): MileageResult = {
    new MileageComputer(stateConf).compute(realinfoes)
  }

  def write(result: MileageResult): Unit = {
    debugOutput.write(result.toString)
    dataOutput.write(s"${result.vid},${result.onlineMileage},${result.gpsMileage},${result.validateMileage},${result.stepPercent},${result.stepPercent}")
    debugOutput.flush()
    dataOutput.flush()
  }
}


object TestUtil extends Logging {
  def main(args: Array[String]): Unit = {
   println( Utils.parsetDate("20161121081348").get.getTime)

    val stateConf=new StateConf
    stateConf.add(args)

    stateConf.set("output.directory","C:\\D\\report\\statistics\\report\\report-bitnei\\report-operation-index\\report-operation-index\\result")
    stateConf.set("input.directory","C:\\D\\report\\statistics\\report\\report-bitnei\\report-operation-index\\report-operation-index\\output")
    stateConf.set("uuid_number_path","C:\\D\\report\\statistics\\report-master\\dayreport\\uuid_to_number")
    stateConf.set("continueCurrentWindowThresholdA","10")

    stateConf.set("continueCurrentWindowMinLength","3")


    new TestUtil(stateConf).run()
    //println(GpsDistance.getDistanceKm(117.104531,40.148974,117.104554,40.148334))

  }

  def parse(vid: String, number:String,lines: Array[String]): Array[Realinfo] = {
    def doParse(line: String): Option[Realinfo] = {
      try {
        val fields = new scala.collection.mutable.HashMap[String, String]()
        line.split(' ').zipWithIndex.foreach(pair => {
          val (field, i) = pair
          if (i == 0) {
            fields.put("time", Utils.parsetDate(field, "yyyyMMddHHmmss").map(_.getTime).getOrElse(0).toString)
          } else {
            val keyValue = field.split(':')
            if (keyValue.length == 2)
              fields.put(keyValue(0), keyValue(1))
          }
        })

        Some(Realinfo(
          vid = vid,
          vin = "",
          time = fields("time").toLong,
          mileage = fields.get("2202").map(v => {
            DataPrecision.mileage(v.toInt)
          }),
          totalVoltage = fields.get("2613").map(v => {
            DataPrecision.totalVoltage(v.toInt)
          }),
          totalCharge = fields.get("2614").map(v => {
            DataPrecision.totalCurrent(v.toInt - 10000)
          }),
          longitude = fields.get("2502").map(v => {
            DataPrecision.latitude(v.toLong)
          }),
          latitude = fields.get("2503").map(v => {
            DataPrecision.latitude(v.toLong)
          }),
          soc = fields.get("2615").map(v => {
            v.toInt
          }),
          speed = fields.get("2201").map(v => {
            DataPrecision.speed(v.toInt)
          }),
          state = fields.get("3201").map(_.toInt)
        ))
      } catch {
        case e: Exception =>
          logInfo(s"parse faield,$line")
          None
      }
    }

    lines.map(doParse).filter(_.nonEmpty).map(_.get)
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.{HashingTF, IDF}
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.rdd.RDD
// $example off$

object TFIDFExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("TFIDFExample")
    val sc = new SparkContext(conf)

    // $example on$
    // Load documents (one per line).
    val documents: RDD[Seq[String]] = sc.textFile("data/mllib/kmeans_data.txt")
      .map(_.split(" ").toSeq)

    val hashingTF = new HashingTF()
    val tf: RDD[Vector] = hashingTF.transform(documents)

    // While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:
    // First to compute the IDF vector and second to scale the term frequencies by IDF.
    tf.cache()
    val idf = new IDF().fit(tf)
    val tfidf: RDD[Vector] = idf.transform(tf)

    // spark.mllib IDF implementation provides an option for ignoring terms which occur in less than
    // a minimum number of documents. In such cases, the IDF for these terms is set to 0.
    // This feature can be used by passing the minDocFreq value to the IDF constructor.
    val idfIgnore = new IDF(minDocFreq = 2).fit(tf)
    val tfidfIgnore: RDD[Vector] = idfIgnore.transform(tf)
    // $example off$

    println("tfidf: ")
    tfidf.foreach(x => println(x))

    println("tfidfIgnore: ")
    tfidfIgnore.foreach(x => println(x))

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.samples.sca.thread

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-04 10:43
  *
  */
class ThreadExample extends Runnable {
  override def run() {
    println("Thread is running...")
  }
}

object Demo {
  def main(args: Array[String]) {
    var e = new ThreadExample()
    var t = new Thread(e)
    t.start()
  }
}package samples

import java.text.SimpleDateFormat
import java.util.concurrent.atomic.AtomicInteger

import org.apache.hadoop.hbase.client.{ConnectionFactory, Get, Scan}
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.{HBaseConfiguration, HConstants, TableName}

import scala.collection.mutable
import scala.language.postfixOps

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2017-12-15 9:57
  *
  */
object ThreadsTest {


  val zkquorum = "192.168.2.70,192.168.2.71,192.168.2.89"
  val zkport = "2181"

  val schema = Array[String](
    "key",
    "stime",
    "type",
    "verify",
    "data"
  )


  val conf = HBaseConfiguration.create()
  conf.set("hbase.zookeeper.quorum", zkquorum)
  conf.set("hbase.zookeeper.property.clientPort", zkport)
  conf.setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 300000)

  val htableName = "packet"

  val vid = "0000d218-44aa-4e15-be39-8f66c602218f"

  val deadLineDate = "20161130"


  def main(args: Array[String]): Unit = {

    //
    //    for (i <- Range(0, 4)) {
    //      new Thread(new Runnable {
    //        override def run(): Unit = {
    //          println("-------------------")
    //        }
    //      }).start()
    //
    //      println(i)
    //    }


    val arr = Range(0, 4)

    //    val counter = new AtomicInteger(0);
    var counter = 0

    for (i <- arr) {
      new Thread(new Runnable {
        override def run(): Unit = {
          println(arr(i))
          println("===" + arr(i))
          //          var out = new java.io.FileWriter("./" + arr(i) + ".txt", true)
          //          out.write(arr(i) + "\n")
          //          out close
        }
      }).start()

    }


  }

}
package com.bitnei.common.utils

import java.util.regex.Pattern

/**
  * Created by wangbaosheng on 2017/9/21.
  */
object  TimeParser {
  def parserAsMs(timeStr: String): Option[Int] = {
    val patternStr = "^([1-9][0-9]*)([Ss]|[Mm][Ss]|[Mm][Ii][Nn]|[Hh]|[Dd][Aa][Yy])"
    val pattern = Pattern.compile(patternStr)
    val matcher = pattern.matcher(timeStr)
    if (matcher.find()) {
      val time = matcher.group(1)
      val unit = matcher.group(2).toUpperCase()
      if (unit == "S") {
        Some(time.toInt * 1000)
      } else if (unit == "MS") {
        Some(time.toInt)
      } else if (unit == "MIN") {
        Some(time.toInt * 60 * 1000)
      } else if (unit == "H") {
        Some(time.toInt * 60 * 60 * 1000)
      } else if (unit == "DAY") {
        Some(time.toInt * 24*60 * 60 * 1000)
      } else None
    } else {
      None
    }
  }

  def main(args: Array[String]): Unit = {
    println(parserAsMs("1S"))
    println(parserAsMs("1Min"))
    println(parserAsMs("1Ms"))

    println(parserAsMs("1H"))
    println(parserAsMs("1Day"))
    println(parserAsMs("12Month"))
  }
}package com.bitnei.report.detail.distribution

import java.util.Date

import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.distribute.Distribution


/**
* 该算法精度为分钟。
* */
class TimeRangeDistribution extends Logging with Distribution {
  private val distributed = Array.fill(48)(0)

  override def interval = 30

  //0,30,60,90.120,
  def add(startValue: Long, endValue: Long): Unit = {
    val startDate = new Date(startValue)
    val startM = startDate.getHours * 60 + startDate.getMinutes + (if (startDate.getMinutes >30) 1 else 0)

    val endDate = new Date(endValue)
    val endM = endDate.getHours * 60 + endDate.getMinutes + (if (endDate.getMinutes >30) 1 else 0)
    doAdd(startM, endM)
  }

  def doAdd(startM: Long, endM: Long): Unit = {
    val startIndex = super.index(startM)
    val endIndex = super.index(endM)


    (startIndex to endIndex).foreach(i => {
      if (0 <= i && i < distributed.length) {
        //获取增量
        val x =  if(startIndex==endIndex) endM-startM else
        if (i == startIndex) getStartOf(i)+interval -startM
        else if (i == endIndex) endM - getStartOf(i)
        else interval

        distributed(i) += x.toInt
      }
    })
    logDebug(
      s"""
         |indexRange:$startIndex - $endIndex
         |valueRange:$startM - $endM
         |distrition:${distributed.foldLeft("")((a, b) => s"$a,$b")}
       """.stripMargin)
  }

  //获取第i个区间的开始值
  def getStartOf(i: Int): Int = i * interval

  //获取第i个区间的结束值
  def getEndOf(i: Int): Int = (i + 1) * interval


  override def getDistribution: Array[Int] = distributed
}


object TimeRangeDistribution {
  def default: Array[Int] = Array.fill(48)(0)

  def main(args: Array[String]): Unit = {
    val counter = new TimeRangeDistribution


    val startDate = Utils.parsetDate("2017-11-25 13:28:08", "yyyy-MM-dd HH:mm:ss").get
    val endDate = Utils.parsetDate("2017-11-25 14:31:48", "yyyy-MM-dd HH:mm:ss").get
    counter.add(startDate.getTime, endDate.getTime)
    counter.add(
      Utils.parsetDate("2017-11-25 11:20:08", "yyyy-MM-dd HH:mm:ss").get.getTime ,
      Utils.parsetDate("2017-11-25 13:28:07", "yyyy-MM-dd HH:mm:ss").get.getTime
    )
    counter.getDistribution.foreach(println)
  }
}
package com.bitnei.util

import java.text.SimpleDateFormat
import java.util.{Calendar, Date, GregorianCalendar}

import scala.collection.mutable.ArrayBuffer


/**
  *
  * @author zhangyongtian
  * @define 时间工具类
  *
  * create 2017-12-29 15:41
  *
  */
object TimeUtils {


  def getDurationOfSecond(pattern: String, startTime: String, stopTime: String): Long = {

    val sdf = new SimpleDateFormat(pattern)

    (sdf.parse(stopTime).getTime - sdf.parse(startTime).getTime) / (1000)

  }


  /**
    * 日期加减
    *
    * @param dateStr 指定日期
    * @param num     天数  负数为减
    * @return
    */
  def addDays(dateStr: String, pattern: String, num: Int): String = {
    val cal = Calendar.getInstance
    val sdf = new SimpleDateFormat(pattern)
    cal.setTime(sdf.parse(dateStr))
    cal.add(Calendar.DAY_OF_MONTH, num)
    sdf.format(cal.getTime)
  }


  def getFirstDayOfWeek(dateStr: String, pattern: String): String = {

    val c = new GregorianCalendar();

    c.setFirstDayOfWeek(Calendar.MONDAY)

    val sdf = new SimpleDateFormat(pattern)
    c.setTime(sdf.parse(dateStr))

    c.set(Calendar.DAY_OF_WEEK, c.getFirstDayOfWeek) // Monday

    sdf.format(c.getTime)
  }

  def getLastDayOfWeek(dateStr: String, pattern: String): String = {

    val c = new GregorianCalendar();

    c.setFirstDayOfWeek(Calendar.MONDAY)

    val sdf = new SimpleDateFormat(pattern)
    c.setTime(sdf.parse(dateStr))

    c.set(Calendar.DAY_OF_WEEK, c.getFirstDayOfWeek + 6) // Sunday

    sdf.format(c.getTime)
  }


  def getAllDaysOfWeek(dateStr: String, pattern: String): Array[String] = {

    val res = new ArrayBuffer[String]()

    val c = new GregorianCalendar();

    c.setFirstDayOfWeek(Calendar.MONDAY)

    val sdf = new SimpleDateFormat(pattern)
    c.setTime(sdf.parse(dateStr))

    c.set(Calendar.DAY_OF_WEEK, c.getFirstDayOfWeek) // Monday

    val fisrtDay = sdf.format(c.getTime)

    res.append(fisrtDay)
    ////////////////////////////////////
    var otherDay = ""
    for (i <- 1 to 5) {
      c.add(Calendar.DATE, 1)
      otherDay = sdf.format(c.getTime)

      res.append(otherDay)
    }

    /////////////////////////////////////
    c.setFirstDayOfWeek(Calendar.MONDAY)

    c.setTime(sdf.parse(dateStr))

    c.set(Calendar.DAY_OF_WEEK, c.getFirstDayOfWeek + 6) // Sunday

    val lastDay = sdf.format(c.getTime)
    res.append(lastDay)

    res.toArray
  }


  /**
    * 周五开始到下周四
    *
    * @param dateStr
    * @param pattern
    * @return
    */
  def getMyAllDaysOfWeek(dateStr: String, pattern: String): Array[String] = {

    val res = new ArrayBuffer[String]()

    val c = new GregorianCalendar();

    c.setFirstDayOfWeek(Calendar.FRIDAY)

    val sdf = new SimpleDateFormat(pattern)
    c.setTime(sdf.parse(dateStr))

    c.set(Calendar.DAY_OF_WEEK, c.getFirstDayOfWeek)

    val fisrtDay = sdf.format(c.getTime)

    res.append(fisrtDay)
    ////////////////////////////////////
    var otherDay = ""
    for (i <- 1 to 6) {
      c.add(Calendar.DATE, 1)
      otherDay = sdf.format(c.getTime)

      res.append(otherDay)
    }

    /////////////////////////////////////
    //    c.setFirstDayOfWeek(Calendar.FRIDAY)
    //
    //    c.setTime(sdf.parse(dateStr))
    //
    //    c.set(Calendar.DAY_OF_WEEK, c.getFirstDayOfWeek + 6) // Sunday
    //
    //    val lastDay = sdf.format(c.getTime)
    //    res.append(lastDay)

    res.toArray
  }


  def getTomorrow(dateStr: String, pattern: String): String = {

    val c = new GregorianCalendar();

    val sdf = new SimpleDateFormat(pattern)
    c.setTime(sdf.parse(dateStr))

    c.add(Calendar.DATE, 1)

    sdf.format(c.getTime)

  }

  def getYesterDay(dateStr: String, pattern: String): String = {

    val c = new GregorianCalendar();

    val sdf = new SimpleDateFormat(pattern)
    c.setTime(sdf.parse(dateStr))

    c.add(Calendar.DATE, -1)

    sdf.format(c.getTime)

  }


  def getWeekOfYear(timeStr: String, pattern: String): Int = {
    var cal: Calendar = Calendar.getInstance();

    val sdf = new SimpleDateFormat(pattern)
    cal.setTime(sdf.parse(timeStr))

    //    cal.setFirstDayOfWeek(Calendar.MONDAY); //

    cal.get(Calendar.WEEK_OF_YEAR)
  }


  def getDaysToNow(timeStr: String, pattern: String): Int = {

    val sdf = new SimpleDateFormat(pattern)
    val date = sdf.parse(timeStr)
    val s1 = date.getTime
    val s2 = System.currentTimeMillis
    val dayNum = (s2 - s1) / 1000 / 60 / 60 / 24

    dayNum.toInt
  }

  /**
    * time app
    *
    * @param time yyyyMMdd格式
    * @return
    */
  def isMorningPeak(time: String): Boolean = {
    val clock = time.substring(8, 10).toInt
    clock >= 7 && clock <= 9
  }


  /**
    * time app
    *
    * @param time yyyyMMdd格式
    * @return
    */
  def isnightPeak(time: String): Boolean = {
    val clock = time.substring(8, 10).toInt
    clock >= 17 && clock <= 19
  }


  def main(args: Array[String]): Unit = {
    //      println(getFirstDayOfWeek("20171229","yyyyMMdd"))
    //      println(getLastDayOfWeek("20171229","yyyyMMdd"))
    //    println(getTomorrow("20171229","yyyyMMdd"))

    //    getMyAllDaysOfWeek("20180119", "yyyyMMdd").foreach(println)

    //    println(addDays("20180119", "yyyyMMdd", 7))

    //    val duration = getDurationOfSecond("yyyyMMddHHmmss", "20180319160400", "20180319160500")
    //    println(duration)

    val sdf = new SimpleDateFormat("yyyyMMdd")

    val cal = Calendar.getInstance()

    val dateStr = "20170106"

    cal.setTime(sdf.parse(dateStr))


    for (i <- Range(0, 52)) {


      cal.add(Calendar.DATE, 7)

      val date = cal.getTime

      val res = sdf.format(date)

      println(res)


    }


  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}
import org.apache.spark.sql.functions._
// $example off$
import org.apache.spark.sql.SparkSession

object TokenizerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("TokenizerExample")
      .getOrCreate()

    // $example on$
    val sentenceDataFrame = spark.createDataFrame(Seq(
      (0, "Hi I heard about Spark"),
      (1, "I wish Java could use case classes"),
      (2, "Logistic,regression,models,are,neat")
    )).toDF("id", "sentence")

    val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")
    val regexTokenizer = new RegexTokenizer()
      .setInputCol("sentence")
      .setOutputCol("words")
      .setPattern("\\W") // alternatively .setPattern("\\w+").setGaps(false)

    val countTokens = udf { (words: Seq[String]) => words.length }

    val tokenized = tokenizer.transform(sentenceDataFrame)
    tokenized.select("sentence", "words")
        .withColumn("tokens", countTokens(col("words"))).show(false)

    val regexTokenized = regexTokenizer.transform(sentenceDataFrame)
    regexTokenized.select("sentence", "words")
        .withColumn("tokens", countTokens(col("words"))).show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.noticecode

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-01-31 11:43
  *
  */
object DayReport extends Serializable with Logging {

  def main(args: Array[String]): Unit = {

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local").trim
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")


    ///////////////////////////

    // TODO: 加载上下文
    logInfo("加载上下文")


    val sparkConf = new SparkConf()

    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[*]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app).getOrCreate()
    import sparkSession.implicits._
////////////////////////////////////////////////

    val initDS = sparkSession.read.json("data/one2manyterminal/one2manyterminal_res.txt")

    val result= initDS.toDF()
      .rdd
      .map(_.getAs[String]("groupId"))


//      .map(x=>(x,1))
//      .reduceByKey(_+_)
//

    println(result.distinct().count())


    sparkSession.stop()
  }

}
package com.bitnei.report.parquet

import java.util.Date
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.sparkhelper.{SparkHelper, TableInfo}
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.spark.SparkContext
import org.apache.spark.sql._

import scala.io.Source

/**
  * Created by franciswang on 2016/12/22.
  */


object ToParquet extends  Logging {
  val sparkSession = SparkHelper.getSparkSession(sparkMaster = None)
  @transient private val sc: SparkContext = sparkSession.sparkContext
  @transient private val sqlContext: SQLContext = sparkSession.sqlContext
  @transient private val hadoopConfiguration = sc.hadoopConfiguration
  @transient private val fs = FileSystem.get(hadoopConfiguration)

  val stateConf = new StateConf

  var startDate: Date = _
  var endDate: Date = _

  def main(args: Array[String]): Unit = {
    stateConf.add(args)
    startDate = stateConf.getOption("input.path.startDate") match {
      case Some(sd) =>
        Utils.parsetDate(sd, stateConf.getOption("input.path.startDate.format").getOrElse("yyyyMMdd")) match {
          case Some(sd) => sd
          case _ => throw new IllegalArgumentException("input.path.startDate.format dot match input.path.startDate")
        }
      case None =>
        val nowDate = new Date
        nowDate.setDate(nowDate.getDate - 1)
        nowDate
    }

    endDate = stateConf.getOption("input.path.endDate") match {
      case Some(sd) =>
        Utils.parsetDate(sd, stateConf.getOption("input.path.endDate.format").getOrElse("yyyyMMdd")) match {
          case Some(sd) => sd
          case _ => throw new IllegalArgumentException("input.path.endDate.format dot match input.path.endDate")
        }
      case None => startDate
    }

    logInfo("spark session has been created")
    tableMap(stateConf.getOption("tablemap")).foreach({
      case (inputTableName: String, outputTableName: String) =>
        doCompute(inputTableName, outputTableName)
    })
  }

  def tableMap(inputouput: Option[String]): Map[String, String] = inputouput match {
    case Some(tableMap) =>
      tableMap.split(',').map(pair => {
        val cells = pair.split(':')
        (cells(0), cells(1))
      }).toMap
    case None => Map.empty[String, String]
  }


  def doCompute(inputTableName: String, outputTableName: String) {
    val inputTableInfo = SparkHelper.getTableInfo(stateConf, inputTableName)
    logInfo(s"input table:$inputTableName output table :$outputTableName")
    getPath(inputTableInfo, startDate, endDate).foreach(path => {
      logInfo("input path:" + path)

      val curDate=getDateFromPath(path,"yyyy/MM/dd")
      val year = Utils.formatDate("yyyy/MM/dd", "yyyy", curDate)
      val month = Utils.formatDate("yyyy/MM/dd", "MM", curDate)
      val day = Utils.formatDate("yyyy/MM/dd", "dd",curDate)

      SparkHelper.setPartitionValue(stateConf, outputTableName, Array(year, month, day))

      val outputTableInfo = SparkHelper.getTableInfo(stateConf, outputTableName)
      processEachPath(inputTableInfo, outputTableInfo, path)
    })
  }


  def getDateFromPath(path:String,formatDate:String):String={
    path.substring(path.length - formatDate.length)
  }
  def processEachPath(inputTableInfo: TableInfo, outputTableInfo: TableInfo, path: String): Unit = {
    val curDate = path.substring(path.length - 10)
    val outputDirectory = outputTableInfo.directory + "/" + curDate

    logInfo(s"output directory is $outputDirectory")
    val table = SparkHelper.parquetOrText(sqlContext, Array(path), inputTableInfo.format, inputTableInfo.schema)
    SparkHelper.saveToPartition(sparkSession, stateConf, table, outputTableInfo.tableName)
  }


  def getPath(tableInfo: TableInfo, startDate: Date, endDate: Date): Array[String] = {
    logInfo("input directory:" + tableInfo.directory)
    logInfo("input start date:" + Utils.formatDate(startDate, "yyyy-MM-dd"))
    logInfo("input end date:" + Utils.formatDate(endDate, "yyyy-MM-dd"))
    val inputPaths = Utils.getValidPath(fs, tableInfo.directory, startDate, endDate, "yyyy/MM/dd")

    inputPaths
  }
}package com.bitnei.report.detail.FaultDetail

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.Utils
import com.bitnei.report.detail.DetailModel
import com.bitnei.sparkhelper.SparkHelper
import org.apache.hadoop.fs.{FileSystem, Path}

/**
  * Created by wangbaosheng on 2017/10/23.
  */
object ToPartitionJob extends Logging {



  def main(args: Array[String]): Unit = {
    val sparkSession = SparkHelper.getSparkSession(None)
    val stateConf = new StateConf
    stateConf.add(args)

    val startDate = Utils.parsetDate(stateConf.getString("startDate"), "yyyyMMdd").get
    val endDate = Utils.parsetDate(stateConf.getString("endDate"), "yyyyMMdd").get

    val outputTableName = stateConf.getString("output.table.name")
    val outputTableInfo = SparkHelper.getTableInfo(stateConf, outputTableName)
    val inputTableName = stateConf.getString("input.table.name")
    val inputTableInfo = SparkHelper.getTableInfo(stateConf, inputTableName)
    val hadoopConfiguration = sparkSession.sparkContext.hadoopConfiguration
    val fs = FileSystem.get(hadoopConfiguration)


    while (startDate.getTime <= endDate.getTime) {
      val year = Utils.formatDate(startDate, "yyyy")
      val month = Utils.formatDate(startDate, "MM")
      val day = Utils.formatDate(startDate, "dd")

      val inputPath = s"${inputTableInfo.directory}/year=$year/month=$month/day=$day"
      if (fs.exists(new Path(inputPath))) {
        try {
          logInfo(s"begin $inputPath")
          sparkSession.read.parquet(inputPath).createOrReplaceTempView(inputTableName)

          val result = sparkSession.sql(if (inputTableName == "realinfo") realinfoSql else detailSql)
          stateConf.set(s"$outputTableName.table.partitionColumn", "year-month-day")
          stateConf.set(s"$outputTableName.table.partitionValue", s"$year-$month-$day")

          SparkHelper.saveToPartition(sparkSession, stateConf, result, outputTableName)
        }catch {
          case e:Exception=>
            logError(inputPath,e)
        }
      }
      startDate.setDate(startDate.getDate + 1)
    }
  }

  val realinfoSql=          s"""
               SELECT
               VID,
               TIME,
               `2201`,
               `2614`,
               `2615`,
               `2613`,
               `2603`,
               `2606`,
               `2304`,
               `2609`,
               `2612`,
               `2202`,
               `2502`,
               `2503`,
               VIN,
               MESSAGETYPE,
               ISFILTER,
               `2210`,
               `2912`,
               `2608`,
               VTYPE,
               `2607`,
               `2002`,
               `2604`,
               `2003`,
               `2605`,
               `2602`,
               `2601`,
               `2910`,
               `2911`,
               `2505`,
               `2209`,
               `2208`,
               `2103`,
               `2101`,
               `2102`,
               `2203`,
               `2504`,
               `2501`,
               `2901`,
               `2902`,
               `2903`,
               `2904`,
               `2801`,
               `2905`,
               `2906`,
               `2616`,
               `2907`,
               `2617`,
               `2908`,
               `2611`,
               `2808`,
               `2802`,
               `2804`,
               `2610`,
               `10002`,
               `10005`,
               `10003`,
               `10004`,
               `2909`,
               `2303`,
               `2305`,
               `2306`,
               `2301`,
               `2302`,
               `2001`,
               `2000`,
               `2004`,
               `2005`,
               `2110`,
               `2111`,
               `2112`,
               `2113`,
               `2114`,
               `2115`,
               `2116`,
               `2117`,
               `2118`,
               `2119`,
               `2120`,
               `2121`,
               `2204`,
               `2205`,
               `2206`,
               `2207`,
               `2211`,
               `2212`,
               `3201`,
               `2213`,
               `2214`,
               `2307`,
               `2308`,
               `2309`,
               `2310`,
               `2311`,
               `2401`,
               `2402`,
               `2403`,
               `2404`,
               `2405`,
               `2406`,
               `2407`,
               `2408`,
               `2409`,
               `2410`,
               `2411`,
               `2412`,
               `2413`,
               `2506`,
               `7615`,
               `2618`,
               `2701`,
               `2702`,
               `3801`,
               `2803`,
               `2805`,
               `2806`,
               `2807`,
               `2809`,
               `2913`,
               `2914`,
               `2915`,
               `2916`,
               `2917`,
               `2918`,
               `2919`,
               `2930`,
               `2920`,
               `2921`,
               `2922`,
               `2923`,
               `2924`
               FROM realinfo
          """.stripMargin


  val detailSql=
    """
      SELECT
      vid,
      vin,
      category,
      onlineTime,
      startTime,
      endTime,
      timeLeng,
      accRunTime,
      startMileageOfCurrentDay,
      endMileageOfCurrentDay,
      startMileage,
      stopMileage,
      gpsMileage,
      avgSpeed,
      maxSpeed,
      maxTotalVoltage,
      minTotalVoltage,
      maxTotalCurrent,
      minTotalCurrent,
      maxSecondaryVolatage,
      minSecondaryVolatage,
      maxAcquisitionPointTemp,
      minAcquisitionPointTemp,
      maxEngineTemp,
      minEngineTemp,
      maxSoc,
      minSoc,
      startSoc,
      endSoc,
      startLongitude,
      startLatitude,
      endLongitude,
      endLatitude,
      totalCharge,
      timeBetweenCharge,
      stopMileageOfPrevCharge,
      prevChargeStopTime,
      maxCurrentOfPrevCharge,
      chargeDistributed
      FROM detail
    """.stripMargin
}
package com.bitnei.report.dayreport

import com.bitnei.report.dayreport.Model.TravelStateDayReportModel
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/10/22.
  */
class TravelDayReportTest extends  FunSuite{
  test("insert to travvel day report table"){
    val values= Array(new TravelStateDayReportModel(
      id = 0,
      report_time ="2016-10-22",
      vid = "728a83e0-d13f-4264-811b-2b4d7e6ba747",
      online_time_sum = 0,
      run_time_sum = 0,
      run_times=0,
      run_km = 0,
      run_time_avg = 0,
      run_km_avg =0,
      run_speed_avg=0,
      run_time_max =0,
      run_time_min =0,
      run_km_max =0,
      run_speed_max=0,
      run_vol_max = 1,
      run_vol_min = 1,
      run_cur_max = 1,
      run_cur_min = 1,
      run_soc_max = 1,
      run_soc_min = 1,
      run_svol_max =1,
      run_svol_min =1,
      run_cptemp_max = 1,
      run_cptemp_min = 1,
      run_engtemp_max =1,
      run_engtemp_min =1
    ))


    //new RunStateDayReportManager(new StateConf).insert(values)
   // new RunStateDayReportManager(new StateConf).delete(values(0).vid)
  }
}
package com.bitnei.report.dayreport.Model

import java.sql.Date

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.common.utils.{DataPrecision, Utils}
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.realinfo.DayReportResult

/**
  * Created by franciswang on 2016/10/20.
  */
class TravelStateDayReportModel(
                                 val id:Int ,
                                 val report_time:String,
                                 val vid:String,
                                 val online_time_sum:Double,
                                 val run_time_sum:Double,
                                 val run_times:Int,
                                 val run_km:Double,
                                 val run_time_avg: Double,
                                 val run_km_avg:Double,
                                 val run_speed_avg:Double,
                                 val run_time_max:Double,
                                 val run_time_min:Double,
                                 val run_km_max:Double,
                                 val run_speed_max:Double,
                                 val run_vol_max:Double,
                                 val run_vol_min:Double,
                                 val run_cur_max:Double,
                                 val run_cur_min:Double,
                                 val run_soc_max:Double,
                                 val run_soc_min:Double,
                                 val run_svol_max:Double,
                                 val run_svol_min:Double,
                                 val run_cptemp_max:Int,
                                 val run_cptemp_min:Int,
                                 val run_engtemp_max:Int,
                                 val run_engtemp_min:Int
                               )extends Serializable {
  def this(vid:String,reportDate:String) = this(0, reportDate, vid, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)

  override def toString: String =
    s"$report_time,$vid,$online_time_sum,$run_time_sum,$run_times,$run_km,$run_time_avg," +
      s"$run_km_avg,$run_speed_avg,$run_time_max,$run_time_min,$run_km_max,$run_speed_max," +
      s"$run_vol_max,$run_vol_min,$run_cur_max,$run_cur_min,$run_soc_max,$run_soc_min," +
      s"$run_svol_max,$run_svol_min,$run_cptemp_max,$run_cptemp_min,$run_engtemp_max,$run_engtemp_min"

}


class RunStateDayReportManager(stateConf:StateConf) extends  Serializable with OutputManager with Logging {
  private val tableName = stateConf.getOption(Constant.TravelStateDayReportTable).getOrElse("veh_dayreport_runstate")


  override type T = DayReportResult

  override def output(vs: Iterable[ DayReportResult]):Unit = {
    val sql = stateConf.getString("database") match {
      case "oracle" => s"INSERT INTO $tableName (" +
        "id, report_time,vid,online_time_sum,run_time_sum,run_times,run_km,run_time_avg,run_km_avg,run_speed_avg," +
        "run_time_max,run_time_min,run_km_max,run_speed_max,run_vol_max,run_vol_min,run_cur_max," +
        "run_cur_min,run_soc_max,run_soc_min,run_svol_max,run_svol_min,run_cptemp_max,run_cptemp_min," +
        "run_engtemp_max,run_engtemp_min) VALUES(SEQ_VEH_REPORT.Nextval,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
      case "mysql" =>
        s"INSERT INTO $tableName (" +
          "report_time,vid,online_time_sum,run_time_sum,run_times,run_km,run_time_avg,run_km_avg,run_speed_avg," +
          "run_time_max,run_time_min,run_km_max,run_speed_max,run_vol_max,run_vol_min,run_cur_max," +
          "run_cur_min,run_soc_max,run_soc_min,run_svol_max,run_svol_min,run_cptemp_max,run_cptemp_min," +
          "run_engtemp_max,run_engtemp_min) VALUES(?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
    }

    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setDate(1, new Date(v.reportDate))
          stmt.setString(2, v.vid)
          stmt.setDouble(3, DataPrecision.toHour(v.onlineTime))
          stmt.setDouble(4, DataPrecision.toHour(v.travelTime))
          stmt.setDouble(5, v.times)
          stmt.setDouble(6, DataPrecision.mileage(v.totalMileage))
          stmt.setDouble(7, DataPrecision.toHour(v.avgTravelTime))
          stmt.setDouble(8, v.avgtravelMileage)
         // logInfo(v.totalMileage + ":" + v.travelTime + ":" + v.avgSpeed)
          stmt.setDouble(9, DataPrecision.speed(v.avgSpeed))
          stmt.setDouble(10, DataPrecision.toHour(v.maxTime))
          stmt.setDouble(11, DataPrecision.toHour(v.minTime))
          stmt.setDouble(12, DataPrecision.mileage(v.maxMileage))
          stmt.setDouble(13, DataPrecision.speed(v.maxSpeed))
          stmt.setDouble(14, DataPrecision.totalVoltage(v.maxTotalVoltage))
          stmt.setDouble(15, DataPrecision.totalVoltage(v.minTotalVoltage))
          stmt.setDouble(16, DataPrecision.totalCurrent(v.maxTotalEctriccurrent))
          stmt.setDouble(17, DataPrecision.totalCurrent(v.minTotalEctriccurrent))
          stmt.setDouble(18, DataPrecision.soc(v.maxSoc))
          stmt.setDouble(19, DataPrecision.soc(v.minSoc))
          stmt.setDouble(20, DataPrecision.secondaryVoltage(v.maxSecondaryVolatage))
          stmt.setDouble(21, DataPrecision.secondaryVoltage(v.minSecondaryVolatage))
          stmt.setInt(22, DataPrecision.temp(v.maxAcquisitionPointTemp))
          stmt.setInt(23, DataPrecision.temp(v.minAcquisitionPointTemp))
          stmt.setInt(24, DataPrecision.temp(v.maxEngineTemp))
          stmt.setInt(25, DataPrecision.temp(v.minEngineTemp))
          stmt.addBatch()
        })
      })
    } catch {
      case e: Exception =>
        logError(s"数据在写入到${tableName}中出现异常")
        vs.foreach(v => logInfo(v.toString))
        throw new Exception(s"throw en exception when writting $tableName", e)
    }
  }

  def delete(reportDate: String): Int = {
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"DELETE FROM ${getTable()} WHERE report_time=to_date(?,'yyyy-mm-dd')", stmt => {
        stmt.setString(1, reportDate)
        stmt.addBatch()
      })(0)
  }

  def getTable(): String = stateConf.getString(Constant.TravelStateDayReportTable)
}package com.bitnei.report.dayreport.Model

import com.bitnei.report.OutputManager
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant

/**
  * Created by franciswang on 2016/10/19.
  */
class RunStateModel(val id: Int,
                    val start:String,
                    val end:String,
                    val time_dif:Double,
                    val vid: String,
                    val mileage_upper:Double,
                    val mileage_lower:Double=0,
                    val max_speed:Double,
                    val max_total_voltage:Double,
                    val min_total_voltage:Double=0,
                    val max_total_current:Double,
                    val min_total_current:Double=0,
                    val max_secondary_volatage:Double,
                    val min_secondary_volatage:Double=0,
                    val max_acquisition_point_temp:Int,
                    val min_acquisition_point_temp:Int=0,
                    val max_engine_temp:Int,
                    val min_engine_temp:Int=0,
                    val max_soc:Double,
                    val min_soc:Double=0
                       ) extends  Serializable{

}


class RunStateModelManager(stateConf:StateConf) extends  Serializable with OutputManager with Logging{
  private val tableName=stateConf.getString(Constant.TravelStateTable)
  override type T=RunStateModel

  override def output(vs:Iterable[RunStateModel]):Unit= {
    val sql: String =s"INSERT INTO $tableName " +
      "(id,start_time,end_time,vid,time_dif,mileage_upper,mileage_lower,max_speed,max_total_voltage,min_total_voltage,max_total_current,min_total_current,"+
      "max_secondary_volatage,min_secondary_volatage,max_acquisition_point_temp,min_acquisition_point_temp,"+
      "max_engine_temp,min_engine_temp,max_soc,min_soc)"+"" +
      "VALUES(SEQ_VEH_REPORT.Nextval,to_date(:start_time,'yyyy-mm-dd hh24:mi:ss'),to_date(:end_time,'yyyy-mm-dd hh24:mi:ss'),?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"
    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setString(1, v.start)
          stmt.setString(2, v.end)
          stmt.setString(3, v.vid)
          stmt.setDouble(4, v.time_dif)
          stmt.setDouble(5, v.mileage_upper)
          stmt.setDouble(6, v.mileage_lower)
          stmt.setDouble(7, v.max_speed)
          stmt.setDouble(8, v.max_total_voltage)
          stmt.setDouble(9, v.min_total_voltage)
          stmt.setDouble(10, v.max_total_current)
          stmt.setDouble(11, v.min_total_current)
          stmt.setDouble(12, v.max_secondary_volatage)
          stmt.setDouble(13, v.min_secondary_volatage)
          stmt.setInt(14, v.max_acquisition_point_temp)
          stmt.setInt(15, v.min_acquisition_point_temp)
          stmt.setInt(16, v.max_engine_temp)
          stmt.setInt(17, v.min_engine_temp)
          stmt.setDouble(18, v.max_soc)
          stmt.setDouble(19, v.min_soc)

          stmt.addBatch()
        })
      })
    } catch {
      case e: Exception =>
        logError(s"数据在写入到${tableName}中出现异常，${e.toString}")
        Array()
    }
  }

  def delete(reportDate:String):Int={
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"DELETE FROM ${getTable()} WHERE start_time>=to_date(?,'yyyy-mm-dd') and end_time<to_date(?,'yyyy-mm-dd')+1",stmt=>{
        stmt.setString(1,reportDate)
        stmt.setString(2,reportDate)
        stmt.addBatch()
      })(0)
  }

  def getTable():String= stateConf.getString(Constant.TravelStateTable)
}/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.graphx

// $example on$
import org.apache.spark.graphx.{GraphLoader, PartitionStrategy}
// $example off$
import org.apache.spark.sql.SparkSession

/**
 * A vertex is part of a triangle when it has two adjacent vertices with an edge between them.
 * GraphX implements a triangle counting algorithm in the [`TriangleCount` object][TriangleCount]
 * that determines the number of triangles passing through each vertex,
 * providing a measure of clustering.
 * We compute the triangle count of the social network dataset.
 *
 * Note that `TriangleCount` requires the edges to be in canonical orientation (`srcId < dstId`)
 * and the graph to be partitioned using [`Graph.partitionBy`][Graph.partitionBy].
 *
 * Run with
 * {{{
 * bin/run-example graphx.TriangleCountingExample
 * }}}
 */
object TriangleCountingExample {
  def main(args: Array[String]): Unit = {
    // Creates a SparkSession.
    val spark = SparkSession
      .builder
      .appName(s"${this.getClass.getSimpleName}")
      .getOrCreate()
    val sc = spark.sparkContext

    // $example on$
    // Load the edges in canonical order and partition the graph for triangle count
    val graph = GraphLoader.edgeListFile(sc, "data/graphx/followers.txt", true)
      .partitionBy(PartitionStrategy.RandomVertexCut)
    // Find the triangle count for each vertex
    val triCounts = graph.triangleCount().vertices
    // Join the triangle counts with the usernames
    val users = sc.textFile("data/graphx/users.txt").map { line =>
      val fields = line.split(",")
      (fields(0).toLong, fields(1))
    }
    val triCountByUsername = users.join(triCounts).map { case (id, (username, tc)) =>
      (username, tc)
    }
    // Print the result
    println(triCountByUsername.collect().mkString("\n"))
    // $example off$
    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.UnaryTransformer
import org.apache.spark.ml.param.DoubleParam
import org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}
import org.apache.spark.sql.functions.col
// $example off$
import org.apache.spark.sql.SparkSession
// $example on$
import org.apache.spark.sql.types.{DataType, DataTypes}
import org.apache.spark.util.Utils
// $example off$

/**
 * An example demonstrating creating a custom [[org.apache.spark.ml.Transformer]] using
 * the [[UnaryTransformer]] abstraction.
 *
 * Run with
 * {{{
 * bin/run-example ml.UnaryTransformerExample
 * }}}
 */
object UnaryTransformerExample {

  // $example on$
  /**
   * Simple Transformer which adds a constant value to input Doubles.
   *
   * [[UnaryTransformer]] can be used to create a stage usable within Pipelines.
   * It defines parameters for specifying input and output columns:
   * [[UnaryTransformer.inputCol]] and [[UnaryTransformer.outputCol]].
   * It can optionally handle schema validation.
   *
   * [[DefaultParamsWritable]] provides a default implementation for persisting instances
   * of this Transformer.
   */
  class MyTransformer(override val uid: String)
    extends UnaryTransformer[Double, Double, MyTransformer] with DefaultParamsWritable {

    final val shift: DoubleParam = new DoubleParam(this, "shift", "Value added to input")

    def getShift: Double = $(shift)

    def setShift(value: Double): this.type = set(shift, value)

    def this() = this(Identifiable.randomUID("myT"))

    override protected def createTransformFunc: Double => Double = (input: Double) => {
      input + $(shift)
    }

    override protected def outputDataType: DataType = DataTypes.DoubleType

    override protected def validateInputType(inputType: DataType): Unit = {
      require(inputType == DataTypes.DoubleType, s"Bad input type: $inputType. Requires Double.")
    }
  }

  /**
   * Companion object for our simple Transformer.
   *
   * [[DefaultParamsReadable]] provides a default implementation for loading instances
   * of this Transformer which were persisted using [[DefaultParamsWritable]].
   */
  object MyTransformer extends DefaultParamsReadable[MyTransformer]
  // $example off$

  def main(args: Array[String]) {
    val spark = SparkSession
      .builder()
      .appName("UnaryTransformerExample")
      .getOrCreate()

    // $example on$
    val myTransformer = new MyTransformer()
      .setShift(0.5)
      .setInputCol("input")
      .setOutputCol("output")

    // Create data, transform, and display it.
    val data = spark.range(0, 5).toDF("input")
      .select(col("input").cast("double").as("input"))
    val result = myTransformer.transform(data)
    println("Transformed by adding constant value")
    result.show()

    // Save and load the Transformer.
    val tmpDir = Utils.createTempDir()
    val dirName = tmpDir.getCanonicalPath
    myTransformer.write.overwrite().save(dirName)
    val sameTransformer = MyTransformer.load(dirName)

    // Transform the data to show the results are identical.
    println("Same transform applied from loaded model")
    val sameResult = sameTransformer.transform(data)
    sameResult.show()

    Utils.deleteRecursively(tmpDir)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.examples.sql

// $example on:typed_custom_aggregation$
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.Encoder
import org.apache.spark.sql.Encoders
import org.apache.spark.sql.SparkSession
// $example off:typed_custom_aggregation$

object UserDefinedTypedAggregation {

  // $example on:typed_custom_aggregation$
  case class Employee(name: String, salary: Long)
  case class Average(var sum: Long, var count: Long)

  object MyAverage extends Aggregator[Employee, Average, Double] {
    // A zero value for this aggregation. Should satisfy the property that any b + zero = b
    def zero: Average = Average(0L, 0L)
    // Combine two values to produce a new value. For performance, the function may modify `buffer`
    // and return it instead of constructing a new object
    def reduce(buffer: Average, employee: Employee): Average = {
      buffer.sum += employee.salary
      buffer.count += 1
      buffer
    }
    // Merge two intermediate values
    def merge(b1: Average, b2: Average): Average = {
      b1.sum += b2.sum
      b1.count += b2.count
      b1
    }
    // Transform the output of the reduction
    def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
    // Specifies the Encoder for the intermediate value type
    def bufferEncoder: Encoder[Average] = Encoders.product
    // Specifies the Encoder for the final output value type
    def outputEncoder: Encoder[Double] = Encoders.scalaDouble
  }
  // $example off:typed_custom_aggregation$

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Spark SQL user-defined Datasets aggregation example")
      .getOrCreate()

    import spark.implicits._

    // $example on:typed_custom_aggregation$
    val ds = spark.read.json("examples/src/main/resources/employees.json").as[Employee]
    ds.show()
    // +-------+------+
    // |   name|salary|
    // +-------+------+
    // |Michael|  3000|
    // |   Andy|  4500|
    // | Justin|  3500|
    // |  Berta|  4000|
    // +-------+------+

    // Convert the function to a `TypedColumn` and give it a name
    val averageSalary = MyAverage.toColumn.name("average_salary")
    val result = ds.select(averageSalary)
    result.show()
    // +--------------+
    // |average_salary|
    // +--------------+
    // |        3750.0|
    // +--------------+
    // $example off:typed_custom_aggregation$

    spark.stop()
  }

}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.examples.sql

// $example on:untyped_custom_aggregation$
import org.apache.spark.sql.expressions.MutableAggregationBuffer
import org.apache.spark.sql.expressions.UserDefinedAggregateFunction
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession
// $example off:untyped_custom_aggregation$

object UserDefinedUntypedAggregation {

  // $example on:untyped_custom_aggregation$
  object MyAverage extends UserDefinedAggregateFunction {
    // Data types of input arguments of this aggregate function
    def inputSchema: StructType = StructType(StructField("inputColumn", LongType) :: Nil)
    // Data types of values in the aggregation buffer
    def bufferSchema: StructType = {
      StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)
    }
    // The data type of the returned value
    def dataType: DataType = DoubleType
    // Whether this function always returns the same output on the identical input
    def deterministic: Boolean = true
    // Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to
    // standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides
    // the opportunity to update its values. Note that arrays and maps inside the buffer are still
    // immutable.
    def initialize(buffer: MutableAggregationBuffer): Unit = {
      buffer(0) = 0L
      buffer(1) = 0L
    }
    // Updates the given aggregation buffer `buffer` with new input data from `input`
    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
      if (!input.isNullAt(0)) {
        buffer(0) = buffer.getLong(0) + input.getLong(0)
        buffer(1) = buffer.getLong(1) + 1
      }
    }
    // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`
    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
      buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
      buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
    }
    // Calculates the final result
    def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)
  }
  // $example off:untyped_custom_aggregation$

  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("Spark SQL user-defined DataFrames aggregation example")
      .getOrCreate()

    // $example on:untyped_custom_aggregation$
    // Register the function to access it
    spark.udf.register("myAverage", MyAverage)

    val df = spark.read.json("examples/src/main/resources/employees.json")
    df.createOrReplaceTempView("employees")
    df.show()
    // +-------+------+
    // |   name|salary|
    // +-------+------+
    // |Michael|  3000|
    // |   Andy|  4500|
    // | Justin|  3500|
    // |  Berta|  4000|
    // +-------+------+

    val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")
    result.show()
    // +--------------+
    // |average_salary|
    // +--------------+
    // |        3750.0|
    // +--------------+
    // $example off:untyped_custom_aggregation$

    spark.stop()
  }

}
package com.bitnei.common.utils

import java.io._
import java.text.SimpleDateFormat
import java.util.zip.InflaterInputStream
import java.util.{Date, Locale}

import com.bitnei.common.constants.Constant
import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}

import scala.annotation.tailrec
import scala.collection.mutable.ArrayBuffer
import scala.io.Source
import scala.reflect.ClassTag

/**
  * Created by franciswang on 2016/10/8.
  */
object Utils extends  Logging {
  private val MAX_DIR_CREATION_ATTEMPTS: Int = 10

  def getSparkClassLoader: ClassLoader = getClass.getClassLoader

  def getContextOrSparkClassLoader: ClassLoader =
    Option(Thread.currentThread().getContextClassLoader).getOrElse(getSparkClassLoader)

  // scalastyle:off classforname
  /** Preferred alternative to Class.forName(className) */
  def classForName(className: String): Class[_] = {
    Class.forName(className, true, getContextOrSparkClassLoader)
    // scalastyle:on classforname
  }


  lazy val isInInterpreter: Boolean = {
    try {
      val interpClass = classForName("org.apache.spark.repl.Main")
      interpClass.getMethod("interp").invoke(null) != null
    } catch {
      case _: ClassNotFoundException => false
    }
  }

  def compareDate(dateA: String, dateB: String): Boolean = {
    val dateFormat = new java.text.SimpleDateFormat("yyyyMMddHHmmss")
    val timeA = dateFormat.parse(dateA).getTime
    val timeB = dateFormat.parse(dateB).getTime

    timeA < timeB
  }

  def getTime(date: String, format: String = "yyyyMMddHHmmss"): Option[Long] = {
    try {
      val dateFormat = new java.text.SimpleDateFormat(format)
      Some(dateFormat.parse(date).getTime)
    } catch {
      case e: Throwable => None
    }
  }

  def parsetDate(date: String, format: String = "yyyyMMddHHmmss"): Option[Date] = {
    try {
      val dateFormat = new java.text.SimpleDateFormat(format)
      Some(dateFormat.parse(date))
    } catch {
      case e: Throwable => None
    }
  }


  def timeToString(time: Long, format: String = "yyyyMMddHHmmss"): String = {
    new SimpleDateFormat(format).format(new Date(time))
  }


  def timeToH(time: Long): Double = {
    time.toDouble / (1000 * 3600).toDouble
  }

  def timeToDay(time: Long): Int = {
    val d = (time / (1000 * 3600 * 24)).toInt
    if (time % (1000 * 3600 * 24) != 0) d + 1 else d
  }


  def getSecond(dateText: String, format: String = "yyyyMMddHHmmss"): Option[Long] = {
    try {
      val dateFormat = new java.text.SimpleDateFormat(format)
      val date = dateFormat.parse(dateText)
      val t = date.getTime
      Some(t / 1000)
    } catch {
      case e: Throwable => None
    }
  }

  def ceil(v: Double): Int = Math.ceil(v).toInt

  def greate(dateTextA: String, dateTextB: String, sec: Long, format: String = "yyyyMMddHHmmss"): Boolean = {
    val dateFormat = new java.text.SimpleDateFormat(format)
    val dateA = dateFormat.parse(dateTextA)
    val dateB = dateFormat.parse(dateTextB)
    val dif = Math.abs(dateA.getTime - dateB.getTime)
    val s = dif / 1000
    val m = dif - s * 1000
    if (m == 0) {
      s > sec
    } else {
      (s + 1) > sec
    }
  }

  def lessEq(dateTextA: String, dateTextB: String, sec: Long, format: String = "yyyyMMddHHmmss"): Boolean = {
    val dateFormat = new java.text.SimpleDateFormat(format)
    val dateA = dateFormat.parse(dateTextA)
    val dateB = dateFormat.parse(dateTextB)
    val dif = Math.abs(dateA.getTime - dateB.getTime)
    val s = dif / 1000
    val m = dif - s * 1000
    if (m == 0) {
      s <= sec
    } else {
      (s + 1) <= sec
    }
  }

  def greateEq(dateTextA: String, dateTextB: String, format: String = "yyyyMMddHHmmss"): Boolean = {
    val dateFormat = new java.text.SimpleDateFormat(format)
    val dateA = dateFormat.parse(dateTextA)
    val dateB = dateFormat.parse(dateTextB)

    dateA.getTime >= dateB.getTime
  }

  def lessEq(dateTextA: String, dateTextB: String): Boolean = !greateEq(dateTextA, dateTextB)


  def timeDiff(dateTextA: String, dateTextB: String): Long = {
    (getTime(dateTextA), getTime(dateTextB)) match {
      case (Some(a), Some(b)) => a - b
      case _ => 0
    }
  }


  def formatDate(srcFormat: String, dstFormat: String, date: String): String = {
    val format = new SimpleDateFormat(srcFormat)
    new SimpleDateFormat(dstFormat).format(format.parse(date))
  }


  def formatDate(date: Date, format: String = "yyyyMMddHHmmss"): String = {
    val f = new SimpleDateFormat(format)
    f.format(date)
  }


  /**
    * @param vecileInfo :待排序的数据集合
    * @param getField   :待排序的字段
    **/
  def sortByDate[T](vecileInfo: Array[T], getField: (T) => Option[String]): Array[T] = {
    vecileInfo.sortWith((x, y) => {
      (getField(x), getField(y)) match {
        case (Some(datax), Some(datey)) => compareDate(datax, datey)
        case _ => false
      }
    })
  }

  /**
    * @param vecileInfo :待排序的数据集合
    * @param getField   :待排序的字段
    **/
  def sortByDate2[T](vecileInfo: Array[T], getField: (T) => Option[Long]): Array[T] = {
    vecileInfo.sortWith((x, y) => {
      (getField(x), getField(y)) match {
        case (Some(datax), Some(datey)) => datax < datey
        case _ => false
      }
    })
  }


  def generateWindowId(head: String, last: String, state: String): String = {
    "%s|%s|%s|%s".format(
      StringParser.get(head, Constant.VehicleIdName).getOrElse(""),
      StringParser.get(head, Constant.VehicleTimeName).getOrElse(""),
      StringParser.get(last, Constant.VehicleTimeName).getOrElse(""),
      state)
  }

  def windowIdToVId(windowId: String): Option[String] = {
    val v = windowId.split('|')
    if (v.length > 0) Some(v(0)) else None
  }


  def windowIdToState(windowId: String): Option[String] = {
    val v = windowId.split('|')
    if (v.length > 3) Some(v(3)) else None
  }

  def roundMin(v: Int): Int = if (v == Int.MaxValue||v==Int.MinValue) 0 else v

  def max(a:Option[Int],b:Option[Int]):Option[Int]=if(a.nonEmpty&&b.nonEmpty) Some(Math.max(a.get,b.get)) else if(a.nonEmpty) a else b
  def min(a:Option[Int],b:Option[Int]):Option[Int]=if(a.nonEmpty&&b.nonEmpty) Some(Math.min(a.get,b.get)) else if(a.nonEmpty) a else b


  /**
    * 将具有相同时间的数据去重
    *
    * @param sortedData 按照时间排序后的数据
    * @return 去重后的结果
    **/
  def strict[T: ClassTag, U](sortedData: Array[T], getField: (T) => Option[U]): Array[T] = {
    val strictData = new ArrayBuffer[T]()

    @tailrec
    def strictTailRec(curIndex: Int, prev: Option[T]): ArrayBuffer[T] = {
      if (curIndex < sortedData.length) {
        if (prev.isEmpty || getField(sortedData(curIndex)) != getField(prev.get)) {
          strictData += sortedData(curIndex)
        }
        strictTailRec(curIndex + 1, Some(sortedData(curIndex)))
      } else {
        strictData
      }
    }

    strictTailRec(0, None).toArray[T]
  }


  /**
    * 将具有相同时间的数据去重
    *
    * @param sortedData 按照时间排序后的数据
    * @return 去重后的结果
    **/
  def strict[T: ClassTag](sortedData: Array[T],equal:(T,T)=>Boolean): Array[T] = {
    val strictData = new ArrayBuffer[T]()

    @tailrec
    def strictTailRec(curIndex: Int, prev: Option[T]): ArrayBuffer[T] = {
      if (curIndex < sortedData.length) {
        if (prev.isEmpty || !equal(sortedData(curIndex),prev.get)) {
          strictData += sortedData(curIndex)
        }
        strictTailRec(curIndex + 1, Some(sortedData(curIndex)))
      } else {
        strictData
      }
    }

    strictTailRec(0, None).toArray[T]
  }

  def uncompress(message: Array[Byte], offset: Int, length: Int): String = {
    val bis = new ByteArrayInputStream(message, offset, length)
    val ii = new InflaterInputStream(bis)
    val baos = new ByteArrayOutputStream()
    val buffer = new ArrayBuffer[Byte]
    (0 until ii.available()).foreach(x => buffer.append(0))

    val buf = buffer.toArray
    var stop = false
    while (!stop) {
      val c = ii.read(buf)
      if (c != -1) baos.write(buf, 0, c)
      else stop = true
    }
    ii.close()
    baos.close()
    new String(baos.toByteArray, "UTF-8")
  }

  def createDirectory(root: String, namePrefix: String = "spark"): File = {
    var attempts = 0
    val maxAttempts = MAX_DIR_CREATION_ATTEMPTS
    var dir: File = null
    while (dir == null) {
      attempts += 1
      if (attempts > maxAttempts) {
        throw new IOException("Failed to create a temp directory (under " + root + ") after " +
          maxAttempts + " attempts!")
      }
      try {
        dir = new File(root, namePrefix)
        if (dir.exists() || !dir.mkdirs()) {
          dir = null
        }
      } catch {
        case e: SecurityException => dir = null;
      }
    }

    dir.getCanonicalFile
  }

  /**
    * JDK equivalent of `chmod 700 file`.
    *
    * @param file the file whose permissions will be modified
    * @return true if the permissions were successfully changed, false otherwise.
    */
  def chmod700(file: File): Boolean = {
    file.setReadable(false, false) &&
      file.setReadable(true, true) &&
      file.setWritable(false, false) &&
      file.setWritable(true, true) &&
      file.setExecutable(false, false) &&
      file.setExecutable(true, true)
  }

  def downHadoopFile(inputDirectory: String, outputDirectory: String): Unit = {
    Runtime.getRuntime.exec(s"hadoop fs -get $inputDirectory $outputDirectory").waitFor()
  }

  def mergeFile(inputPath: String, outputPath: String): Unit = {
    val inFile = new File(inputPath)
    val out = new File(outputPath)
    out.createNewFile()
    val outStream = new FileOutputStream(out, true)

    inFile.listFiles().foreach(childFile => {

      printf(s"开始处理${childFile.getAbsolutePath}")
      val buf = new ArrayBuffer[Byte]()

      for (i <- (0 until 4096)) buf.append(0)

      val input = new FileInputStream(childFile)
      copy(input, outStream, buf.toArray)
      input.close()
      printf(s"处理完毕${childFile.getAbsolutePath}")

    })

    printf(s"结果已经输出到$outputPath")


    /** Copy all data from an InputStream to an OutputStream. NIO way of file stream to file stream
      * copying is disabled by default unless explicitly set transferToEnabled as true,
      * the parameter transferToEnabled should be configured by spark.file.transferTo = [true|false].
      */
    def copyStream(in: InputStream,
                   out: OutputStream,
                   closeStreams: Boolean = false,
                   transferToEnabled: Boolean = false): Long = {
      var count = 0L
      tryWithSafeFinally {
        if (in.isInstanceOf[FileInputStream] && out.isInstanceOf[FileOutputStream]
          && transferToEnabled) {
          // When both streams are File stream, use transferTo to improve copy performance.
          val inChannel = in.asInstanceOf[FileInputStream].getChannel()
          val outChannel = out.asInstanceOf[FileOutputStream].getChannel()
          val initialPos = outChannel.position()
          val size = inChannel.size()

          // In case transferTo method transferred less data than we have required.
          while (count < size) {
            count += inChannel.transferTo(count, size - count, outChannel)
          }

          // Check the position after transferTo loop to see if it is in the right position and
          // give user information if not.
          // Position will not be increased to the expected length after calling transferTo in
          // kernel version 2.6.32, this issue can be seen in
          // https://bugs.openjdk.java.net/browse/JDK-7052359
          // This will lead to stream corruption issue when using sort-based shuffle (SPARK-3948).
          val finalPos = outChannel.position()
          assert(finalPos == initialPos + size,
            s"""
               |Current position $finalPos do not equal to expected position ${initialPos + size}
               |after transferTo, please check your kernel version to see if it is 2.6.32,
               |this is a kernel bug which will lead to unexpected behavior when using transferTo.
               |You can set spark.file.transferTo = false to disable this NIO feature.
           """.stripMargin)
        } else {
          val buf = new Array[Byte](8192)
          var n = 0
          while (n != -1) {
            n = in.read(buf)
            if (n != -1) {
              out.write(buf, 0, n)
              count += n
            }
          }
        }
        count
      } {
        if (closeStreams) {
          try {
            in.close()
          } finally {
            out.close()
          }
        }
      }
    }

  }

  def copy(inputStream: FileInputStream, outStream: FileOutputStream, buf: Array[Byte]): Unit = {
    val len = inputStream.read(buf, 0, buf.length)
    if (len != -1) {
      outStream.write(buf, 0, len)
      copy(inputStream, outStream, buf)
    }
  }

  /**
    * Execute a block of code, then a finally block, but if exceptions happen in
    * the finally block, do not suppress the original exception.
    *
    * This is primarily an issue with `finally { out.close() }` blocks, where
    * close needs to be called to clean up `out`, but if an exception happened
    * in `out.write`, it's likely `out` may be corrupted and `out.close` will
    * fail as well. This would then suppress the original/likely more meaningful
    * exception from the original `out.write` call.
    */
  def tryWithSafeFinally[T](block: => T)(finallyBlock: => Unit): T = {
    // It would be nice to find a method on Try that did this
    var originalThrowable: Throwable = null
    try {
      block
    } catch {
      case t: Throwable =>
        // Purposefully not using NonFatal, because even fatal exceptions
        // we don't want to have our finallyBlock suppress
        originalThrowable = t
        throw originalThrowable
    } finally {
      try {
        finallyBlock
      } catch {
        case t: Throwable =>
          if (originalThrowable != null) {
            originalThrowable.addSuppressed(t)
            logWarning(s"Suppressing exception in finally: " + t.getMessage, t)
            throw originalThrowable
          } else {
            throw t
          }
      }
    }
  }


  /**
    * 删除一个hdfs文件
    *
    * @param path 要删除的hdfs 文件目录
    **/
  def deleteHdfsFile(fs: FileSystem, path: Path): Boolean = {
    //递归删除一个文件
    fs.delete(path, true)
  }


  def getChildFiles(fs: FileSystem, directory: String): Array[(String, Long)] = {
    val d = new Path(directory)

    if (fs.exists(d)) {
      val childFiles = fs.listStatus(d)
      childFiles.map(file => {
        (file.getPath.toString, file.getLen)
      })
    } else {
      Array.empty
    }
  }


  def getFileName(path: String): String = {
    new Path(path).getName
  }

  def moveFile(fs: FileSystem, directories: Iterable[String], dstDict: String, filter: Path => Boolean): Unit = {
    directories.foreach(directory => {
      val directoryPath = new Path(directory)
      if (filter(directoryPath)) {
        val childFiles = fs.listStatus(directoryPath)
        childFiles.foreach(childFile => {
          val filePath = childFile.getPath
          val fileName = filePath.getName
          fs.rename(filePath, new Path(s"$dstDict/$fileName"))
        })
      }
    })
  }

  /**
    * Convert a quantity in bytes to a human-readable string such as "4.0 MB".
    */
  def bytesToString(size: Long): String = {
    val TB = 1L << 40
    val GB = 1L << 30
    val MB = 1L << 20
    val KB = 1L << 10

    val (value, unit) = {
      if (size >= 2 * TB) {
        (size.asInstanceOf[Double] / TB, "TB")
      } else if (size >= 2 * GB) {
        (size.asInstanceOf[Double] / GB, "GB")
      } else if (size >= 2 * MB) {
        (size.asInstanceOf[Double] / MB, "MB")
      } else if (size >= 2 * KB) {
        (size.asInstanceOf[Double] / KB, "KB")
      } else {
        (size.asInstanceOf[Double], "B")
      }
    }
    "%.1f %s".formatLocal(Locale.US, value, unit)
  }

  /**
    * Execute a command and return the process running the command.
    */
  def executeCommand(
                      command: Seq[String],
                      workingDir: File = new File("."),
                      extraEnvironment: Map[String, String] = Map.empty,
                      redirectStderr: Boolean = true): Process = {
    val builder = new ProcessBuilder(command: _*).directory(workingDir)
    val environment = builder.environment()
    for ((key, value) <- extraEnvironment) {
      environment.put(key, value)
    }
    val process = builder.start()
    if (redirectStderr) {
      val threadName = "redirect stderr for command " + command.head

      def log(s: String): Unit = logInfo(s)

      processStreamByLine(threadName, process.getErrorStream, log)
    }
    process
  }

  /**
    * Execute a command and get its output, throwing an exception if it yields a code other than 0.
    */
  def executeAndGetOutput(
                           command: Seq[String],
                           workingDir: File = new File("."),
                           extraEnvironment: Map[String, String] = Map.empty,
                           redirectStderr: Boolean = true): String = {
    val process = executeCommand(command, workingDir, extraEnvironment, redirectStderr)
    val output = new StringBuffer
    val threadName = "read stdout for " + command.head

    def appendToOutput(s: String): Unit = output.append(s)

    val stdoutThread = processStreamByLine(threadName, process.getInputStream, appendToOutput)
    val exitCode = process.waitFor()
    stdoutThread.join() // Wait for it to finish reading output
    if (exitCode != 0) {
      logError(s"Process $command exited with code $exitCode: $output")
      throw new Exception(s"Process $command exited with code $exitCode")
    }
    output.toString
  }

  /**
    * Return and start a daemon thread that processes the content of the input stream line by line.
    */
  def processStreamByLine(
                           threadName: String,
                           inputStream: InputStream,
                           processLine: String => Unit): Thread = {
    val t = new Thread(threadName) {
      override def run() {
        for (line <- Source.fromInputStream(inputStream).getLines()) {
          processLine(line)
        }
      }
    }
    t.setDaemon(true)
    t.start()
    t
  }


  def tryAllException(f: => Unit)(msg: String) {
    try {
      f
    } catch {
      case e: Exception =>
        logError(s"$msg,${e.getMessage}" + e)
    }
  }


  def tryAndThrow[T](f: => T)(msg: String): T = {
    try {
      f
    } catch {
      case e: Throwable =>
        logError(s"$msg,${e.getMessage}" + e)
        throw new Exception(msg, e)
    }
  }

  def readAllLines(path: String): Array[String] = readAllLines(path, line => line)

  def readAllLines[T: ClassTag](path: String, parse: (String) => T, encode: String = "UTF-8"): Array[T] = Source.fromFile(path, encode).getLines().map(parse).toArray

  def newLine: String = System.getProperty("line.separator")

  def getPartitionNum(prevTotalSize: Long, partitionNum: Int, threshold: Long): Int = {
    val avgSizeOfPartition = Math.ceil(prevTotalSize / partitionNum).toLong

    val newPartitionNum = if (Math.abs(avgSizeOfPartition - threshold) < 1024 * 1024) {
      Math.ceil(prevTotalSize / threshold).toInt
    } else partitionNum

    newPartitionNum
  }

  def getParquetThreshold(stateConf: StateConf, hadoopConfig: Configuration): Long = {
    val defaultThreshold = hadoopConfig.getLong("dfs.blocksize", 128 * 1024 * 1024)

    stateConf.getOption("parquet.threshold").map(_.toLong) match {
      case Some(threshold: Long) => Math.max(defaultThreshold, threshold)
      case _ => defaultThreshold
    }
  }


  /**
    * 获取最近n个文件
    **/
  def getLastNFiles(fs: FileSystem, baseDirectory: String, endDate: String, n: Int): Array[String] = {
    val date = Utils.parsetDate(endDate, format = "yyyyMMdd").get
    val pathArray = new ArrayBuffer[String]()

    @tailrec
    def doGetLastNFiles(i: Int, maxRetryCount: Int): Array[String] = {
      if (i < maxRetryCount && pathArray.size < n) {
        val d = new Date(date.getTime)
        d.setDate(date.getDate - i)

        val path = s"$baseDirectory/${Utils.formatDate(d, "yyyy/MM/dd")}"

        if (fs.exists(new Path(path))) pathArray.append(path)
        doGetLastNFiles(i + 1, maxRetryCount)
      } else pathArray.toArray
    }


    doGetLastNFiles(0, 10)
  }

  def buildPath(baseDirectory: String, startDate: Date, endDate: Date, format:String="yyyy/MM/dd",filter: (String) => Boolean = (path) => true): Array[String] = {
    @tailrec
    def doBuildPath(curDate: Date, pathArray: ArrayBuffer[String]): Array[String] = {
      if (curDate.getTime <= endDate.getTime) {
        val path = s"$baseDirectory/${Utils.formatDate(curDate, format)}"
        if (filter(path)) pathArray.append(path)
        val nextDate = new Date(curDate.getTime)
        nextDate.setDate(curDate.getDate + 1)
        doBuildPath(nextDate, pathArray)
      } else {
        pathArray.toArray
      }
    }

    doBuildPath(startDate, new ArrayBuffer[String]()).distinct
  }

  def getFileSize(fs: FileSystem, path: Path): Long = {
    def doCompute(fileStatus: FileStatus, size: Long): Long = {
      if (fileStatus.isDirectory) {
        //获取目录
        val childDirectoryList = fs.listStatus(fileStatus.getPath)
        childDirectoryList.map(child => doCompute(fs.getFileStatus(child.getPath), size)).reduceOption((a, b) => a + b).getOrElse(size)
      } else {
        size + fileStatus.getLen
      }
    }

    val fileStatus = fs.getFileStatus(path)
    doCompute(fileStatus, 0)
  }

  def getValidPath(fs: FileSystem, baseDirectory: String, startDate: Date, endDate: Date,format:String="yyyy/MM/dd"): Array[String] = {
    buildPath(baseDirectory, startDate, endDate,format, p => {
      val path = new Path(p)
      if (fs.exists(path)) {
        val size = getFileSize(fs, path)
        size > 0
      } else {
        false
      }
    })
  }


  def readAllStartWith(inDirectory: File, filter: (String) => Boolean, procesFile: (String, String) => Unit): Unit = {
    inDirectory.listFiles().foreach(file => {
      if (file.isDirectory) {
        readAllStartWith(file, filter, procesFile)
      } else {
        if (filter(file.getName)) {
          try {
            procesFile(file.getAbsolutePath, inDirectory.getName)
          } catch {
            case e: Exception =>
          }
        }
      }
    })
  }

  def write(path: String, values: Iterable[String]): Unit = {
    val file = new File(path)

    if (!file.getParentFile.exists()) file.getParentFile.mkdir()

    if (!file.exists() && file.createNewFile()) {
      logInfo(s"a new file create sucessed  $path")
    } else {
      logInfo(s"a new file create failed  $path")
    }


    val writer = new PrintWriter(path)
    values.foreach(line => writer.write(s"$line" + Utils.newLine))
    writer.flush()
    writer.close()
  }


  def z_score(simple: Array[Double]): Array[Double] = {
    val mean = getMean(simple)
    val std=getStd(simple,mean)

    simple.map(x => (x - mean) / std)
  }

  def getStd(simple: Array[Double],mean:Double): Double = {
    val quort=simple.reduce((a, b) => (a - mean) * (a - mean) + (b - mean) * (b - mean))
    val std = Math.sqrt(quort / (simple.length-1))
    std
  }

  def batchProcessOfArray[T](array:Iterable[T],batchLength:Int,f:(Int,Int)=>Unit): Unit = {
    assert(batchLength > 0)

    val batchCount = if (array.size % batchLength != 0) array.size / batchLength + 1
    else array.size / batchLength

    (0 until batchCount).foreach(i => {
      val offset = i * batchLength
      if (offset < array.size) {
        val length =if((offset+batchLength)<array.size) batchLength
        else array.size-offset
        f(offset, length)
      }
    })
  }


  def batchProcess[T](array:Iterable[T],batchLength:Int,f:(Int,Int)=>Unit): Unit = {
    assert(batchLength > 0)

    val batchCount = if (array.size % batchLength != 0) array.size / batchLength + 1
    else array.size / batchLength

    (0 until batchCount).foreach(i => {
      val offset = i * batchLength
      if (offset < array.size) {
        val length =if((offset+batchLength)<array.size) batchLength
        else array.size-offset
        val i=offset
        while(i<(offset+length)){

        }
        f(offset, length)
      }
    })
  }

  def getMean(simple: Array[Double]): Double =   simple.sum / simple.length
}

private[bitnei] trait Clock {
  def getTimeMillis: Long
  def waitTillTime(targetTime: Long): Long
}


private[bitnei] class SystemClock extends Serializable with Clock {

  val minPollTime = 25L

  /**
    * @return the same time (milliseconds since the epoch)
    *         as is reported by `System.currentTimeMillis()`
    */
  def getTimeMillis: Long = System.currentTimeMillis()
  def getWaitTimeMs(startTime:Long):Long={
    System.currentTimeMillis()-startTime
  }
  /**
    * @param targetTime block until the current time is at least this value
    * @return current system time when wait has completed
    */
  def waitTillTime(targetTime: Long): Long = {
    var currentTime = 0L
    currentTime = System.currentTimeMillis()

    var waitTime = targetTime - currentTime
    if (waitTime <= 0) {
      return currentTime
    }

    val pollTime = math.max(waitTime / 10.0, minPollTime).toLong

    while (true) {
      currentTime = System.currentTimeMillis()
      waitTime = targetTime - currentTime
      if (waitTime <= 0) {
        return currentTime
      }
      val sleepTime = math.min(waitTime, pollTime)
      Thread.sleep(sleepTime)
    }
    -1
  }
}
package com.bitnei.report.dayreport.validate

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.sparkhelper.SparkHelper
import org.apache.spark.sql.{Dataset, SparkSession}

import scala.collection.mutable.ArrayBuffer


class ValidateJob(
                    @transient sparkSession:SparkSession,
                    stateConf:StateConf)  extends Serializable with Logging {
  def compute(): Unit = {
    val validatyResult = new ComputeValidate(stateConf, sparkSession).computeValidate()
    output(validatyResult)
  }


  def output(result: Dataset[String]): Unit = {
    val ouputModel = stateConf.getString("report.output").split(',')
    if (ouputModel.contains("hdfs")) {
      SparkHelper.saveToPartition(sparkSession, stateConf, result.toDF(), "validate")
    } else {
      result.show(10)
      logWarning("未开启hdfs输出")
    }

    //输出到数据库
    ouputModel.find(x => x == "oracle" || x == "mysql") match {
      case Some(e) if e == "oracle" || e == "mysql" =>
        logInfo("开启数据库输出")
        stateConf.set("database", e)
        //        logWarning(s"开始删除${reportDate}数据有效性表的数据")
        //        new ValidateModelManager(stateConf).delete(reportDate)
        outputDatabase(result)
      case _ =>
        logWarning("没有开启数据库输出功能")
    }
  }

  def outputDatabase(r: Dataset[String]): Unit = {
    r.foreachPartition(resultPar => {
      val validityResult = new ArrayBuffer[String]()
      resultPar.foreach(result => {
        val tuples = result.split(',')
        tuples(0) match {
          case "validityDay" => validityResult.append(result)
          case _ =>
        }
      })

      new ValidateModelManager(stateConf).insert(ValidityDayReportFormat.unBuilde(validityResult))
    })
  }
}

object ValidateJob {
  private  val stateConf = new StateConf

  def main(args: Array[String]): Unit = {
    stateConf.add(args)

    val sparkSession = SparkSession.builder().getOrCreate()
    //removeDataIfNeed(stateConf)
    new ValidateJob(sparkSession, stateConf).compute()
  }
}
package com.bitnei.report.dayreport.validate

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.jdbc.JdbcPoolHelper
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
class ValidityModel(
                   val vid:String,
                   val reportDate:String,
  /**
    * 数据接收有效条数（realinfo, termstatus）
    */
  val data_receive_validsum: Int,
  /**
    * 数据接收无效条数（realinfo, termstatus）
    */
  val data_receive_invalidsum: Int,

  /**
    * 报警数据无效条数
    */
  val data_alarm_invalidsum: Int,
  /**
    * 报警数据总条数
    */
  val data_alarm_sum: Int,
  /**
    * 报警数据有效条数
    */
  val data_alarm_validsum: Int,
  /**
    * 单体蓄电池电压无效条数
    */
  val data_batt_vol_invalidsum: Int,

  /**
    * 单体蓄电池电压数据条数 2003字段
    */
  val data_batt_vol_sum: Int,
  /**
    * 单体蓄电池电压数据有效条数
    */
  val data_batt_vol_validsum: Int,
  /**
    * 动力蓄电池包温度数据无效条数
    */
  val data_battgroup_tempinvalidsum: Int,
  /**
    * 动力蓄电池包温度数据总条数2103字段
    */
  val data_battgroup_tempsum: Int,
  /**
    * 动力蓄电池包温度数据有效条数
    */
  val data_battgroup_tempvalidsum: Int,
  /**
    * 数据转发无效率
    */
  val data_forward_invalidpercent: Float,
  /**
    * 数据转发无效条数
    */
  val data_forward_invalidsum: Int,
  /**
    * 数据转发平台数据
    */
  val data_forward_platcount: Int,
  /**
    * 数据转发平台失败数
    */
  val data_forward_platfail: Int,
  /**
    * 数据转发平台失败率
    */
  val data_forward_platfail_percent: Float,
  /**
    * 数据转发平台成功率
    */
  val data_forward_platsucc_percent: Float,
  /**
    * 数据转发平台成功数
    */
  val data_forward_platsuccess:Int,
  /**
    * 数据转发条数
    */
  val data_forward_sum: Int,
  /**
    * 数据转发有效率
    */
  val data_forward_validpercent: Float,
  /**
    * 数据转发有效条数
    */
  val data_forward_validsum: Int,
  /**
    * 定位数据无效条数
    */
  val data_gps_invalidsum: Int,
  /**
    * 卫星定位系统条数  2501  定位状态   2502 经度   2503  纬度   2504 速度   2505 方向
    */
  val data_gps_sum: Int,

  /**
    * 定位数据有效条数
    */
  val data_gps_validsum: Int,

  /**
    * 数据无效率
    */
  val data_invalidpercent: Float,

  /**
    * 数据无效条数
    */
  val data_invalidsum: Int,

  /**
    * 极值数据无效条数
    */
  val data_limitation_invalidsum: Int,

  /**
    * 极值数据总条数
    */
  val data_limitation_sum: Int,

  /**
    * 极值数据有效条数
    */
  val data_limitation_validsum: Int,

  /**
    * 平台交换协议数据无效条数
    */
  val data_platform_invalidsum: Int,

  /**
    * 平台交换协议数据总条数
    */
  val data_platform_sum: Int,

  /**
    * 平台交换协议数据有效条数
    */
  val data_platform_validsum: Int,

  /**
    * 注册失败100%
    */
  val data_reg_failpercent: Float,
  /**
    * 注册无效条数
    */
  val data_reg_invalidsum: Int,
  /**
    * 注册成功100%
    */
  val data_reg_succpercent: Float,
  /**
    * 注册总条数
    */
  val data_reg_sum: Int,
  /**
    * 注册有效条数
    */
  val data_reg_validsum: Int,

  /**
    * 数据接收总条数 不包含注册信息
    */
  val data_sum_noreg: Int,

  /**
    * 数据有效率
    */
  val data_validpercent: Float,

  /**
    * 数据有效条数
    */
  val data_validsum: Int,

  /**
    * 整车数据无效条数
    */
  val data_vehdata_invalidsum: Int,

  /**
    * 车速 2201
    * 里程  2201
    * 档位   2203
    * 加速踏板行程值 2209
    * 制动踏板行程值 2208
    * 空调设定温度 2210
    * 预留
    * 整车数据条数包含以上任意一字段即可
    */
  val data_vehdata_sum: Int,

  /**
    * 整车数据有效条数
    */
  val data_vehdata_validsum: Int) extends Serializable{

}
/**
  * Created by franciswang on 2016/11/7.
  */
class ValidateModelManager(stateConf:StateConf) extends  Serializable with Logging{
  private val tableName=stateConf.getOption(Constant.StateValidateTable).getOrElse("veh_dayreport_datastat")
  private  val sql= stateConf.getString("database") match {
    case "oracle" => s"insert into $tableName(" +
      " id,report_time ,vid,data_reg_sum ,data_reg_validsum, data_reg_invalidsum," +
      " data_reg_succpercent, data_reg_failpercent,data_sum_noreg," +
      " data_validsum, data_invalidsum,data_validpercent,data_invalidpercent," +
      " data_forward_sum,data_forward_validsum, data_forward_invalidsum," +
      " data_forward_validpercent,data_forward_invalidpercent," +
      " data_forward_platcount,data_forward_platsuccess,data_forward_platfail, " +
      " data_forward_platsucc_percent,data_forward_platfail_percent," +
      " data_batt_vol_sum," +
      " data_batt_vol_validsum,data_batt_vol_ivalidsum, data_battgroup_tempsum," +
      " data_battgroup_tempvalidsum,data_battgroup_tempinvalidsum,data_vehdata_sum," +
      " data_vehdata_validsum,data_vehdata_invalidsum,data_gps_sum,data_gps_validsum," +
      " data_gps_invalidsum,data_limitation_sum,data_limitation_validsum,data_limitation_invalidsum," +
      " data_alarm_sum,data_alarm_validsum,data_alarm_invalidsum,data_platform_sum," +
      " data_platform_validsum,data_platform_invalidsum ) " +
      " values(seq_veh_report.nextval,to_date(:report_time,'yyyy-mm-dd'),?,?,?,?,?,?,?,?,?," +
      " ?,?,?,?,?,?,?,?,?,?," +
      " ?,?,?,?,?,?,?,?,?,?," +
      " ?,?,?,?,?,?,?,?,?,?," +
      " ?,?,?)"
    case "mysql" =>
      s"insert into $tableName(" +
        " report_time ,vid,data_reg_sum ,data_reg_validsum, data_reg_invalidsum," +
        " data_reg_succpercent, data_reg_failpercent,data_sum_noreg," +
        " data_validsum, data_invalidsum,data_validpercent,data_invalidpercent," +
        " data_forward_sum,data_forward_validsum, data_forward_invalidsum," +
        " data_forward_validpercent,data_forward_invalidpercent," +
        " data_forward_platcount,data_forward_platsuccess,data_forward_platfail, " +
        " data_forward_platsucc_percent,data_forward_platfail_percent," +
        " data_batt_vol_sum," +
        " data_batt_vol_validsum,data_batt_vol_ivalidsum, data_battgroup_tempsum," +
        " data_battgroup_tempvalidsum,data_battgroup_tempinvalidsum,data_vehdata_sum," +
        " data_vehdata_validsum,data_vehdata_invalidsum,data_gps_sum,data_gps_validsum," +
        " data_gps_invalidsum,data_limitation_sum,data_limitation_validsum,data_limitation_invalidsum," +
        " data_alarm_sum,data_alarm_validsum,data_alarm_invalidsum,data_platform_sum," +
        " data_platform_validsum,data_platform_invalidsum ) " +
        " values(?,?,?,?,?,?,?,?,?,?," +
        " ?,?,?,?,?,?,?,?,?,?," +
        " ?,?,?,?,?,?,?,?,?,?," +
        " ?,?,?,?,?,?,?,?,?,?," +
        " ?,?,?)"
  }

//  def insert(vs:Iterator[String]):Array[Int]={
//    try {
//      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
//        vs.foreach(v => {
//          v.split(',').zipWithIndex.foreach(field => {
//            if (field._2 == 1) stmt.setDate(field._2, java.sql.Date.valueOf(field._1))
//            else stmt.setObject(field._2, field)
//            stmt.addBatch()
//          })
//        })
//      })
//    }catch {
//      case e:Exception=>
//        logError(s"数据在写入到${tableName}中出现异常，${e.toString}")
//        Array()
//    }
//  }



  def insert(vs:Iterable[ValidityModel]):Array[Int]={
    try {
      JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(sql, stmt => {
        vs.foreach(v => {
          stmt.setString(1,v.reportDate)
          stmt.setString(2,v.vid)
          stmt.setInt(3,v.data_reg_sum)
          stmt.setInt(4,v.data_reg_validsum)
          stmt.setInt(5,v.data_reg_invalidsum)
          stmt.setFloat(6,v.data_reg_succpercent)
          stmt.setFloat(7,v.data_reg_failpercent)
          stmt.setInt(8,v.data_sum_noreg)
          stmt.setInt(9,v.data_validsum)
          stmt.setInt(10,v.data_invalidsum)
          stmt.setFloat(11,v.data_validpercent)
          stmt.setFloat(12,v.data_invalidpercent)
          stmt.setInt(13,v.data_forward_sum)
          stmt.setInt(14,v.data_forward_validsum)
          stmt.setInt(15, v.data_forward_invalidsum)
          stmt.setFloat(16,v.data_forward_validpercent)
          stmt.setFloat(17,v.data_forward_invalidpercent)
          stmt.setInt(18,v.data_forward_platcount)
          stmt.setInt(19,v.data_forward_platsuccess)
          stmt.setInt(20,v.data_forward_platfail)
          stmt.setFloat(21,v.data_forward_platsucc_percent)
          stmt.setFloat(22,v.data_forward_platfail_percent)
          stmt.setInt(23,v.data_batt_vol_sum)
          stmt.setInt(24,v.data_batt_vol_validsum)
          stmt.setInt(25,v.data_batt_vol_invalidsum)
          stmt.setInt(26,v.data_battgroup_tempsum)
          stmt.setInt(27,v.data_battgroup_tempvalidsum)
          stmt.setInt(28,v.data_battgroup_tempinvalidsum)
          stmt.setInt(29,v.data_vehdata_sum)
          stmt.setInt(30,v.data_vehdata_validsum)
          stmt.setInt(31,v.data_vehdata_invalidsum)
          stmt.setInt(32,v.data_gps_sum)
          stmt.setInt(33,v.data_gps_validsum)
          stmt.setInt(34,v.data_gps_invalidsum)
          stmt.setInt(35,v.data_limitation_sum)
          stmt.setInt(36,v.data_limitation_validsum)
          stmt.setInt(37,v.data_limitation_invalidsum)
          stmt.setInt(38,v.data_alarm_sum)
          stmt.setInt(39,v.data_alarm_validsum)
          stmt.setInt(40,v.data_alarm_invalidsum)
          stmt.setInt(41,v.data_platform_sum)
          stmt.setInt(42,v.data_platform_validsum)
          stmt.setInt(43,v.data_platform_invalidsum)
          stmt.addBatch()
        })
      })
    }catch {
      case e:Exception=>
        logError(s"数据在写入到$tableName 中出现异常，${e.toString}")
        e.printStackTrace()
        Array()
    }
  }

  def delete(reportDate:String): Int ={
    JdbcPoolHelper.getJdbcPoolHelper(stateConf).execute(
      s"DELETE FROM $tableName WHERE report_time=to_date(?,'yyyy-mm-dd')", stmt => {
        stmt.setString(1, reportDate)
        stmt.addBatch()
      })(0)
  }
}
package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.constants.Constant
import com.bitnei.report.dayreport.validate.ComputeValidate
import org.apache.spark.sql.SparkSession
import org.scalatest.FunSuite



/**
  * Created by franciswang on 2016/11/7.
  */
class ValidateTest extends FunSuite {
  val stateConf=new StateConf()
  stateConf.set("state.database.remove.enable","false")
  stateConf.set("useparquet","false")
  stateConf.set(Constant.OutputDatabase,"false")
  stateConf.set(Constant.CheckPointPath,"")


  stateConf.set(Constant.TermInputPath,"./src/main/resources/termstatus/*")
  stateConf.set(Constant.LoginInputPath,"./src/main/resources/login/part-00000")
  stateConf.set(Constant.ForwardInputPath,"./src/main/resources/forward/part-00000")
  stateConf.set(Constant.RealinfoPath,"./src/main/resources/realinfo1/part-00001.2")
  val sparkSession=SparkSession.builder().appName("spark单元测试").master("local").getOrCreate()

  val validate=new ComputeValidate(stateConf,sparkSession)
  test("test termstatus"){
    validate.registeTerm()
    validate.computeTerm(sparkSession.sql("select VID from term")).show()
  }

  test("test login"){
    validate.registeLogin()
    validate.computeLogin(sparkSession.sql("select VID,RESULT from login")).show()
  }

  test("test forward"){
    validate.registeForward()
  // sparkSession.sql("select VID,RESULT from forward").show(10000000)
    validate.computeForward(sparkSession.sql("select VID,RESULT from forward")).show()
  }

  test("test realinfo"){
    validate.registeRealinfoValidate()
    validate.computeRealinfoValidate(sparkSession.sql("select * from realinfo_validate")).show(truncate = false)
  }

  test("test validate"){
    validate.computeValidate().show(truncate = false)
  }
}package com.bitnei.tools.util

import java.util.regex.Pattern


/**
  * @author zhangyongtian
  * @define 验证工具类
  */
object ValidateUtils {



  /**
    * 判断是否是数字
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\d+)$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 是否包含乱码
    * @param str
    * @return
    */
  def isContainsMessyCode(str: String): Boolean = {

    //    汉字：[0x4e00,0x9fa5]（或十进制[19968,40869]）
    //    数字：[0x30,0x39]（或十进制[48, 57]）
    //    小写字母：[0x61,0x7a]（或十进制[97, 122]）
    //    大写字母：[0x41,0x5a]（或十进制[65, 90]）
    val res = str.replaceAll("[\u4e00-\u9fa5]", "").replaceAll("\\d|\\w", "")
//    println("res:" + res)
    !res.isEmpty
  }


  /**
    * 判断是否包含中文
    * @param str
    * @return
    */
  def isContainsCN(str: String): Boolean = {
    val p = Pattern.compile("[\u4e00-\u9fa5]")
    val m = p.matcher(str)

    m.find()
  }

  /**
    * 判断是否包含字母
    * @param str
    * @return
    */
  def isContainsLetter(str: String): Boolean = {
    val p = Pattern.compile("[a-zA-Z]")
    val m = p.matcher(str)

    m.find()
  }



  /**
    * 判断是否包含特殊字符
    * @param str
    * @return
    */
  def isContainsSpeciChar(str: String): Boolean = {
    val regEx = "[`~!@#$%^&*()+=|{}':;',\\\\[\\\\].<>/?~！@#￥%……&*（）——+|{}【】‘；：”“’。，、？]";
    val p = Pattern.compile(regEx);
    val m = p.matcher(str);

    m.find()
  }

  /**
    * 判断是否包含指定的关键词
    * @param str
    * @param keywordArr
    * @return
    */
  def isContainsSpecWords(str: String, keywordArr: Array[String]): Boolean = {
    var result = false
    for (ele <- keywordArr if !result) {
      result = str.contains(ele)
    }
    result
  }


  def regxpTest(str: String): Boolean = {
    val regex = """(\d{8})(.+)[[ ]*|_*|\d|-第*集|第\d集|(第\d集)|大结局|先导集]{1}""".r
    println(!regex.findFirstMatchIn(str).isEmpty)
    !regex.findAllIn(str).isEmpty
  }

  def getLoggerInfo(str: String, key: String, ignore: Boolean): String = {
    var igStr = ""
    if (ignore) {
      igStr = "(?i)"
    }
    val regex = igStr + "[\\s\\S]*[<\\[]\\s*" + key + "\\s*[>\\:\\]]\\s*(\\-\\s*\\[)?\\s*([^\\[\\]<]*)[\\s<\\]]+[\\s\\S]*"
    println("regex:" + regex)
    str.replaceAll(regex, "$2")

  }


  def main(args: Array[String]): Unit = {
    println(isContainsLetter("haha"))
    println(isContainsLetter("123"))
  }


}
package com.bitnei.report.dayreport

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.dayreport.validate.{ValidateModelManager, ValidityModel}
import org.scalatest.FunSuite

/**
  * Created by franciswang on 2016/11/8.
  */
class ValidityModelTest extends  FunSuite{
  test("insert validity model to table"){
    new ValidateModelManager(new StateConf).insert(
      Array(
        new ValidityModel("","2016-10-22",0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
      )
    )
  }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vectors
// $example off$
import org.apache.spark.sql.SparkSession

object VectorAssemblerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("VectorAssemblerExample")
      .getOrCreate()

    // $example on$
    val dataset = spark.createDataFrame(
      Seq((0, 18, 1.0, Vectors.dense(0.0, 10.0, 0.5), 1.0))
    ).toDF("id", "hour", "mobile", "userFeatures", "clicked")

    val assembler = new VectorAssembler()
      .setInputCols(Array("hour", "mobile", "userFeatures"))
      .setOutputCol("features")

    val output = assembler.transform(dataset)
    println("Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'")
    output.select("features", "clicked").show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import org.apache.spark.ml.feature.VectorIndexer
// $example off$
import org.apache.spark.sql.SparkSession

object VectorIndexerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("VectorIndexerExample")
      .getOrCreate()

    // $example on$
    val data = spark.read.format("libsvm").load("data/mllib/sample_libsvm_data.txt")

    val indexer = new VectorIndexer()
      .setInputCol("features")
      .setOutputCol("indexed")
      .setMaxCategories(10)

    val indexerModel = indexer.fit(data)

    val categoricalFeatures: Set[Int] = indexerModel.categoryMaps.keys.toSet
    println(s"Chose ${categoricalFeatures.size} categorical features: " +
      categoricalFeatures.mkString(", "))

    // Create new column "indexed" with categorical values transformed to indices
    val indexedData = indexerModel.transform(data)
    indexedData.show()
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.ml

// $example on$
import java.util.Arrays

import org.apache.spark.ml.attribute.{Attribute, AttributeGroup, NumericAttribute}
import org.apache.spark.ml.feature.VectorSlicer
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.StructType
// $example off$
import org.apache.spark.sql.SparkSession

object VectorSlicerExample {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder
      .appName("VectorSlicerExample")
      .getOrCreate()

    // $example on$
    val data = Arrays.asList(
      Row(Vectors.sparse(3, Seq((0, -2.0), (1, 2.3)))),
      Row(Vectors.dense(-2.0, 2.3, 0.0))
    )

    val defaultAttr = NumericAttribute.defaultAttr
    val attrs = Array("f1", "f2", "f3").map(defaultAttr.withName)
    val attrGroup = new AttributeGroup("userFeatures", attrs.asInstanceOf[Array[Attribute]])

    val dataset = spark.createDataFrame(data, StructType(Array(attrGroup.toStructField())))

    val slicer = new VectorSlicer().setInputCol("userFeatures").setOutputCol("features")

    slicer.setIndices(Array(1)).setNames(Array("f3"))
    // or slicer.setIndices(Array(1, 2)), or slicer.setNames(Array("f2", "f3"))

    val output = slicer.transform(dataset)
    output.show(false)
    // $example off$

    spark.stop()
  }
}
// scalastyle:on println
package com.bitnei.util.linear

import java.util
import breeze.numerics.{abs, cbrt, pow, sqrt}
import scala.collection.mutable.ArrayBuffer


object VectorUtil {

  /**
    * 二维向量叉乘
    *
    * @param A
    * @param B
    * @return
    */
  def cross(A: Array[Double], B: Array[Double]): Double = {

    var cross = 0D

    if (A.length == 2 && B.length == 2) {
      cross = A(0) * B(1) - B(0) * A(1);
    } else {
      throw new RuntimeException("vector length must be 2 !")
    }
    cross
  }


  //Compute the cross product AB x AC
  def cross(A: Array[Int], B: Array[Int], C: Array[Int]): Int = {
    val AB = new Array[Int](2)
    val AC = new Array[Int](2)
    AB(0) = B(0) - A(0)
    AB(1) = B(1) - A(1)
    AC(0) = C(0) - A(0)
    AC(1) = C(1) - A(1)
    val cross = AB(0) * AC(1) - AB(1) * AC(0)
    cross
  }

  //  //Compute the dot product AB   BC
  //  def dot(A: Array[Int], B: Array[Int], C: Array[Int]): Int = {
  //    val AB = new Array[Int](2)
  //    val BC = new Array[Int](2)
  //    AB(0) = B(0) - A(0)
  //    AB(1) = B(1) - A(1)
  //    BC(0) = C(0) - B(0)
  //    BC(1) = C(1) - B(1)
  //    val dot = AB(0) * BC(0) + AB(1) * BC(1)
  //    dot
  //  }
  //
  //  //Compute the distance from A to B
  //  def distance(A: Array[Int], B: Array[Int]): Double = {
  //    val d1 = A(0) - B(0)
  //    val d2 = A(1) - B(1)
  //    sqrt(d1 * d1 + d2 * d2)
  //  }
  //
  //  //Compute the distance from AB to C
  //  //if isSegment is true, AB is a segment, not a line.
  //  def linePointDist(A: Array[Int], B: Array[Int], C: Array[Int], isSegment: Boolean): Double = {
  //    val dist = cross(A, B, C) / distance(A, B)
  //    if (isSegment) {
  //      val dot1 = dot(A, B, C)
  //      if (dot1 > 0) return distance(B, C)
  //      val dot2 = dot(B, A, C)
  //      if (dot2 > 0) return distance(A, C)
  //    }
  //    abs(dist)
  //  }


  /**
    * 向量单位化处理
    *
    * @param arr 输入数据
    * @return
    */
  def normalize(arr: Array[Double]): Array[Double] = {

    val res = new Array[Double](arr.length)

    var sum = 0D

    arr.foreach(x => {
      sum = sum + pow(x, 2)
    })

    sum = sqrt(sum)
    for (i <- 0 until arr.length) {
      //      doubleArr(i) = (doubleArr(i) / sum)
      //      doubleArr(i) = String.format("%.2f", (doubleArr(i) / sum)).toDouble
      //      new java.text.DecimalFormat("#.00").format(3.1415926)
      //      new BigDecimal((doubleArr(i) / sum)).setScale(2, BigDecimal.ROUND_HALF_UP).doubleValue()

      var num = 0.0
      if (sum != 0) {
        num = (arr(i) / sum)
      }

      //保留小数点位数
      res(i) = f"$num%1.3f".toDouble
    }

    res
  }

  //  def normalize(arr: Array[Int]): Array[Double] = {
  //
  //    val res = new Array[Double](arr.length)
  //
  //    val sum = arr.reduce((x, y) => {
  //      sqrt(x) + sqrt(y)
  //    })
  //    for (i <- 0 until arr.length) {
  //      //      doubleArr(i) = (doubleArr(i) / sum)
  //      //      doubleArr(i) = String.format("%.2f", (doubleArr(i) / sum)).toDouble
  //      //      new java.text.DecimalFormat("#.00").format(3.1415926)
  //      //      new BigDecimal((doubleArr(i) / sum)).setScale(2, BigDecimal.ROUND_HALF_UP).doubleValue()
  //      var num = 0.0
  //      if (sum != 0) {
  //        num = (arr(i).toDouble / sum)
  //      }
  //
  //      //保留小数点位数
  //      res(i) = f"$num%1.3f".toDouble
  //    }
  //
  //    res
  //  }

  def main(args: Array[String]): Unit = {

    //    println(pow(2, 3))
    //
    val arr = Array(1D, 2D, 3D)
    //    normalize(arr).foreach(println(_))
    //    breeze.linalg.normalize(arr)

    val a = Array(1D, 3D)

    val b = Array(1D, 2D)


    println(cross(a, b))
  }
}
package com.bitnei.report.dayreport.validate

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.constants.Constant
import com.bitnei.report.stateGenerate.Window
import com.bitnei.sparkhelper.{SparkHelper, SqlHelper}
import org.apache.spark.sql._

import scala.annotation.tailrec




class ComputeValidate(stateConf: StateConf,sparkSession:SparkSession) extends  Logging with Serializable {
  private val sqlContext = sparkSession.sqlContext

  import sqlContext.implicits._

  def computeValidate(): Dataset[String] = {
    registeLogin()
    computeLogin(sqlContext.sql(s"select VID,RESULT from login ${SqlHelper.buildWhere(stateConf)}")).createOrReplaceTempView("login_result")
    sparkSession.catalog.dropTempView("login")

    registeTerm()
    computeTerm(sqlContext.sql(s"select VID from termstatus ${SqlHelper.buildWhere(stateConf)}")).createOrReplaceTempView("term_result")
    sparkSession.catalog.dropTempView("termstatus")

    registeRealinfoValidate()
    computeRealinfoValidate(
      sqlContext.sql(s"""SELECT
                  VID,
                  `2001`,
                  `2002`,
                  `2003`,
                  `2101`,
                  `2102`,
                  `2103`,
                  `2201`,
                  `2202`,
                  `2203`,
                  `2208`,
                  `2209`,
                  `2210`,
                  `2301`,
                  `2302`,
                  `2303`,
                  `2304`,
                  `2305`,
                  `2306`,
                  `2501`,
                  `2502`,
                  `2503`,
                  `2504`,
                  `2505`,
                  `2601`,
                  `2602`,
                  `2603`,
                  `2604`,
                  `2605`,
                  `2606`,
                  `2607`,
                  `2608`,
                  `2609`,
                  `2610`,
                  `2611`,
                  `2612`,
                  `2613`,
                  `2614`,
                  `2615`,
                  `2616`,
                  `2617`,
                  `2901`,
                  `2902`,
                  `2903`,
                  `2904`,
                  `2905`,
                  `2909`,
                  `2910`,
                  `2911`,
                  `2912`
                FROM realinfo ${SqlHelper.buildWhere(stateConf)}""".stripMargin)).createOrReplaceTempView("realinfo_result")

    sparkSession.catalog.dropTempView("realinfo")

    registeForward()
    computeForward(sqlContext.sql(s"select VID,RESULT from forward ${SqlHelper.buildWhere(stateConf)}")).createOrReplaceTempView("forward_result")
    sparkSession.catalog.dropTempView("forward")


    val result = sparkSession.sql("SELECT login_result._1 VID,login_result._2 as loginResult ,forward_result._2 as forwardResult,term_result._2 as termResult,"+
      "realinfo_result.*  "+
      " FROM login_result JOIN forward_result ON login_result._1=forward_result._1 "+
      " JOIN term_result ON term_result._1=forward_result._1"+
      " JOIN realinfo_result ON realinfo_result.vid=term_result._1")

    result.map(row => {
      val vid = row.getAs[String]("vid")
      val loginResult = row.getAs[String]("loginResult")
      val forwardResult = row.getAs[String]("forwardResult")
      val termResult = row.getAs[Int]("termResult")

      val realCount = row.getAs[Int]("realCount")
      val realInvalidCount = row.getAs[Int]("realValidityCount")
      val realValidCount = row.getAs[Int]("realInvalidityCount")
      val acceptTotalCount = termResult + realCount
      val acceptValidityCount = termResult + realValidCount
      val acceptValidityPercent = if (acceptTotalCount > 0) acceptValidityCount.toFloat / acceptTotalCount.toFloat else 0f
      val acceptInvalidityPercent = if (acceptTotalCount > 0) realInvalidCount.toFloat / acceptTotalCount.toFloat else 0f
      val reportDate = stateConf.getString(Constant.ReportDate)

      f"validityDay,$reportDate,$vid,$loginResult," +
        f"$acceptTotalCount,$acceptValidityCount,$realInvalidCount,$acceptValidityPercent%1.2f,$acceptInvalidityPercent%1.2f," +
        s"$forwardResult" +
        s"${row.getAs("secondaryVoltageCount")},${row.getAs("secondaryVoltageValidateCount")},${row.getAs("secondaryVoltageInvalidateCount")}," +
        s"${row.getAs("pabTempCount")},${row.getAs("pabTempValidCount")},${row.getAs("pabTempInvalidCount")}," +
        s"${row.getAs("vehicleCount")},${row.getAs("vehicleValidCount")},${row.getAs("vehicleInvalidCount")}," +
        s"${row.getAs("gpsCount")},${row.getAs("gpsValidCount")},${row.getAs("gpsInvalidCount")}," +
        s"${row.getAs("limitationCount")},${row.getAs("limitationValidCount")},${row.getAs("limitationInvalidCount")}," +
        s"${row.getAs("alarmCount")},${row.getAs("alarmValidCount")},${row.getAs("alarmInvalidCount")}," +
        s"${row.getAs("default_1")},${row.getAs("default_2")},${row.getAs("default_3")}" //平台转发暂时不实现
    })
  }


  def registeLogin(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,"login")
  }

  case class ForwardModel(vid:String,RESULT:String)

  def registeForward(){
    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,"forward")
  }

  case class TermModel(vid:String)

  def registeTerm() {
    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,"termstatus")
  }

  def registeRealinfoValidate(): Unit = {
    SparkHelper.createOrReplaceTempView(sparkSession,stateConf,"realinfo")
  }


  def computeRealinfoValidate(realinfo: DataFrame): DataFrame = {
    realinfo.groupByKey(_.getAs[String]("VID"))
      .mapGroups((vid, group) => {
        new ComputeRealinfoValidate().compute(group.toIterable)
      }).toDF()
  }


  def computeForward(forward: DataFrame): DataFrame = {
    forward.groupByKey(row => {
      try {
        val vid = row.getAs[String]("VID")
        if (vid == null) " " else vid
      } catch {
        case e: Exception => " "
      }
    })
      .mapGroups((vid, group) => {
        (vid, new ForwardCompute(stateConf, group).toString)
      }
      ).toDF()
  }

  def computeTerm(forward: DataFrame): DataFrame = {
    forward.groupByKey(_.getAs[String]("VID"))
      .mapGroups((vid, group) => {
        (vid, group.length)
      }).toDF()
  }

  def computeLogin(login: DataFrame): DataFrame = {
    def compute(rows: Iterator[Row]): String = {
      var loginCount = 0
      var invalidLogin = 0
      var sucessLogin = 0
      var failedLogin = 0
      var errorLogin = 0

      rows.foreach(row => {
        loginCount += 1
        try {
          val a = row.getAs[String]("RESULT")
          if (a == Constant.LogginInvaid) invalidLogin += 1
          else if (a == Constant.LoginSucess) sucessLogin += 1
          else if (a == Constant.LogginFailed) failedLogin += 1
          else errorLogin += 1
        } catch {
          case e: Exception => errorLogin += 1
        }
      })

      s"$loginCount,$invalidLogin,$sucessLogin,$failedLogin,$errorLogin"
    }


    login.groupByKey(_.getAs[String]("VID"))
      .mapGroups((vid, group) => {
        (vid, compute(group))
      }).toDF()
  }
}



object ValidityDayReportFormat{
  def unBuilde(vs:Iterable[String]):Iterable[ValidityModel]= {
    vs.map(v=> {
      val elements = v.split(',')

      new ValidityModel(reportDate = elements(1), vid = elements(2),
        data_reg_sum = elements(3).toInt, data_reg_validsum = elements(4).toInt, data_reg_invalidsum = elements(5).toInt, data_reg_succpercent = elements(6).toFloat, data_reg_failpercent = elements(7).toFloat,
        data_sum_noreg = elements(8).toInt, data_validsum = elements(9).toInt, data_invalidsum = elements(10).toInt,
        data_validpercent = elements(11).toFloat, data_invalidpercent = elements(12).toFloat,
        data_forward_sum = elements(13).toInt, data_forward_validsum = elements(14).toInt, data_forward_invalidsum = elements(15).toInt, data_forward_validpercent = elements(16).toFloat,
        data_forward_invalidpercent = elements(17).toFloat, data_forward_platcount = elements(18).toInt, data_forward_platsuccess = elements(19).toInt, data_forward_platfail = elements(20).toInt,
        data_forward_platsucc_percent = elements(21).toFloat, data_forward_platfail_percent = elements(22).toFloat,
        data_batt_vol_sum = elements(23).toInt, data_batt_vol_validsum = elements(24).toInt,
        data_batt_vol_invalidsum = elements(25).toInt,
        data_battgroup_tempsum = elements(26).toInt,
        data_battgroup_tempvalidsum = elements(27).toInt,
        data_battgroup_tempinvalidsum = elements(28).toInt,
        data_vehdata_sum = elements(29).toInt,
        data_vehdata_validsum = elements(30).toInt,
        data_vehdata_invalidsum = elements(31).toInt,
        data_gps_sum = elements(32).toInt,
        data_gps_validsum = elements(33).toInt,
        data_gps_invalidsum = elements(24).toInt,
        data_limitation_sum = elements(24).toInt,
        data_limitation_validsum = elements(24).toInt,
        data_limitation_invalidsum = elements(24).toInt,
        data_alarm_sum = elements(24).toInt,
        data_alarm_validsum = elements(24).toInt,
        data_alarm_invalidsum = elements(24).toInt,
        data_platform_sum = elements(24).toInt,
        data_platform_validsum = elements(24).toInt,
        data_platform_invalidsum = elements(24).toInt,
        data_receive_invalidsum=0,
        data_receive_validsum=0 )
    })
  }
}







package com.bitnei.report.model


/**
  *
  * @author zhangyongtian
  * @define
  *
  * create 2018-03-14 14:47
  *
  */

class Window[T] extends scala.collection.mutable.ArrayBuffer[T] {

  var onLineTime: Long = 0
  var chargeTime: Long = 0
  var fullChargeTime: Long = 0
  var startMileage: Int = 0
  var endMileage: Int = 0

  var beginIndexInSouce: Int = 0
  var endIndexInSource: Int = 0


  def getState: String = "none"

  def append(v: T): Window[T] = {
    super.append(v)
    this
  }

}

case class NoneWindow[T]() extends Window[T] {
  override def getState = "none"
}

case class TravelWindow[T]() extends Window[T] {
  override def getState = "run"
}

case class ChargeWindow[T]() extends Window[T] {
  override def getState = "charge"
}

case class FullChargeWindow[T]() extends Window[T] {
  override def getState = "fullcharge"
}

case class ForwardWindow[T]() extends Window[T] {
  override def getState = "forward"
}

case class AlarmWindow[T]() extends Window[T] {
  override def getState = "alarm"
}

case class LoginWindow[T]() extends Window[T] {
  override def getState = "login"
}

case class ChargeChangeWindow[T](correct: Double) extends Window[T] {
  override def getState = "chargechange"
}
package com.bitnei.report.stateGenerate

/**
  * 状态生成器
  * 给定一个序列source，生成满足如下约束的状态序列：
  * 1）如果source(i) 满足c ,那么 source(i) 属于某个state
  * 2）如果source(i)属于某个state，并且j是i的后继，那么source(j)属于state
  *
  * source(j)是source(i)的后继当且仅当
  * 1)j=i+1
  * 2)source(j)满足c
  * 3)source（i）和source(j)满足r
  *
  * c(i)是一个返回值为true和false的一个函数，如果source(i)满足某个条件，那么结果为true，否则为false
  * r(i,j)是一个返回值为true和false的函数，如果source(i)和source(j)满足某个相关条件，返回true，否则false
  */

//object WindowGenerator extends Serializable with  Logging {
//  /*
// * 根据多个条件生成窗口
// * */
//  def generate[T](source: Array[T],windowFilter: List[((Window[T],T) => Boolean, (Window[T],T, T) => Boolean, (Window[T]) => String)]): ListBuffer[Window[T]] = {
//    def addSuccessor(i: Int, c: (Window[T],T) => Boolean, r: (Window[T],T, T) => Boolean, window: Window[T]): Int = {
//      var hasNext:Boolean=true
//      var aa:Int=0
//
//      for(cur<-source.indices if hasNext){
//        val next=cur+1
//        if(next < source.length && c(window,source(next)) && r(window,source(cur), source(next))){
//          window.append(source(next))
//        }else {
//          hasNext=false
//          aa=cur
//        }
//      }
//
//      aa
//      //      val next = i + 1
//      //      if (next < source.length && c(source(next)) && r(source(i), source(next))) {
//      //        bitnei.state.window.append(source(next))
//      //        addSuccessor(next, c, r, bitnei.state.window)
//      //      } else
//      //        i
//    }
//
//    val windows = ListBuffer[Window[T]]()
//    var i: Int = 0
//
//    while (i < source.length) {
//      var one = true
//      val window=new Window[T]()
//
//
//      for ((c, r, windowId) <- windowFilter if c(window,source(i)) && one) {
//        window.append(source(i))
//        i = addSuccessor(i, c, r, window)
//        window.windowId=windowId(window)
//        windows+=window
//
//        //windows += new Window[T](buffer, windowId(buffer))
//        //logDebug(f"生成窗口${windowId(buffer)}length=${buffer.length}")
//
//        one = false
//
//      }
//      i = i + 1
//    }
//    windows
//  }
//
//
//  /*
//  * 根据多个条件生成窗口
//  * */
//  def generate1[T](source: Array[T], windowFilter: List[((T) => Boolean, (T, T) => Boolean, (ArrayBuffer[T]) => String)]): ListBuffer[Window[T]] = {
//    def addSuccessor(i: Int, c: (T) => Boolean, r: (T, T) => Boolean, window: ArrayBuffer[T]): Int = {
//      var hasNext:Boolean=true
//      var aa:Int=0
//
//      for(cur<-source.indices if hasNext){
//        val next=cur+1
//        if(next < source.length && c(source(next)) && r(source(cur), source(next))){
//          window.append(source(next))
//        }else {
//          hasNext=false
//          aa=cur
//        }
//      }
//
//      aa
////      val next = i + 1
////      if (next < source.length && c(source(next)) && r(source(i), source(next))) {
////        bitnei.state.window.append(source(next))
////        addSuccessor(next, c, r, bitnei.state.window)
////      } else
////        i
//    }
//
//    val windows = ListBuffer[Window[T]]()
//    var i: Int = 0
//
//    while (i < source.length) {
//      var one = true
//      for ((c, r, windowId) <- windowFilter if c(source(i)) && one) {
//        val buffer = new ArrayBuffer[T]()
//        buffer.append(source(i))
//        i = addSuccessor(i, c, r, buffer)
//
//        windows += new Window[T](buffer, windowId(buffer))
//        logDebug(f"生成窗口${windowId(buffer)}length=${buffer.length}")
//
//        one = false
//
//      }
//      i = i + 1
//    }
//    windows
//  }
//
//  def merge[T](windows: Seq[Window[T]], r: (Window[T], Window[T]) => Boolean):Iterator[Window[T]] = {
//    def addNext(i: Int, w: ArrayBuffer[Window[T]]): Int = {
//      val j = i + 1
//      if (j < windows.length && r(windows(i), windows(j))) {
//        w.append(windows(j))
//        addNext(j, w)
//      } else j
//    }
//
//    var i: Int = 0
//    val aggregatedWindow = new ArrayBuffer[ArrayBuffer[Window[T]]]()
//
//    while (i < windows.length) {
//      val w = new ArrayBuffer[Window[T]]()
//      w.append(windows(i))
//      aggregatedWindow.append(w)
//
//      i = addNext(i, w)
//     // i += 1
//    }
//
//    aggregatedWindow.map(windows => {
//      val newWindow = new Window[T]()
//      windows.foreach(window => {
//        newWindow.append(window)
//      })
//      newWindow
//    }).toIterator
//  }
//}


abstract class Reason()


class Window[T] extends scala.collection.mutable.ArrayBuffer[T]{
  var onLineTime:Long=0
  var chargeTime:Long=0
  var fullChargeTime:Long=0
  var startMileage:Int=0
  var endMileage:Int=0

  var beginIndexInSouce:Int=0
  var endIndexInSource:Int=0

  var reason:Reason=_


  def getState:String="none"

  def append(v:T):Window[T]={
    super.append(v)
    this
  }
}

case class NoneWindow[T]() extends Window[T]{
  override def getState = "none"
}
case class TravelWindow[T]() extends Window[T]{
  override def getState = "run"
}
case class ChargeWindow[T]()extends  Window[T]{
  override def getState = "charge"
}

case class FullChargeWindow[T]()extends  Window[T]{
  override def getState = "fullcharge"
}

case class ForwardWindow[T]()extends Window[T]{
  override def getState = "forward"
}

case class AlarmWindow[T]()extends  Window[T]{
  override def getState = "alarm"
}
case class LoginWindow[T]()extends Window[T]
{
  override def getState = "login"
}

case class ChargeChangeWindow[T](correct:Double)extends Window[T]{
  override def getState = "chargechange"
}


//class WindowGeneratorTest() {
//  def testSingleWindowFilter(): Unit = {
//    val data = Buffer[Vecile]()
//    data ++= generateCi(0, 4)
//    data ++= generateNotCi(5, 5)
//    data ++= generateCi(6, 8)
//    data ++= generateNotCi(9, 9)
//    data ++= generateCi(12, 14)
//    data ++= generateNotCi(15, 15)
//    data ++= generateCi(16, 16)
//
//
//    val states = WindowGenerator.generate(
//      data.toArray,
//      (current) => {
//        current.value - 0.5 <= 0.001
//      },
//      (current, next) => current.time + 1 == next.time
//    )
//
//    data.foreach(x => println(x.time + ":" + x.value))
//    states.foreach(state => {
//      println("******************")
//      state.foreach(vecile => println(vecile.time + ":" + vecile.value))
//    })
//  }
//
//  def testMultiWindowFilter(): Unit = {
//    val data = Buffer[Vecile]()
//    data ++= generateCi(0, 4)
//    data ++= generateNotCi(5, 5)
//    data ++= generateCi(6, 8)
//    data ++= generateNotCi(9, 9)
//    data ++= generateCi(12, 14)
//    data ++= generateNotCi(15, 18)
//    data ++= generateCi(19, 20)
//
//    val stateGenerator = new WindowGenerator[Vecile](data.toArray)
//    val windowFilters = List[((Vecile) => Boolean, (Vecile, Vecile) => Boolean,(ArrayBuffer[Vecile])=>String)](
//      (
//        (current) => current.value - 0.5 <= 0.0001,
//        (current, next) => current.time + 1 == next.time,
//        (bitnei.state.window)=>""
//        ),
//
//      (
//        (current) => current.value >=0.6,
//        (current, next) => current.time + 1 == next.time,
//        (bitnei.state.window)=>""
//        )
//    )
//    val windows = stateGenerator.generate(windowFilters)
//
//    data.foreach(x => println(x.time + ":" + x.value))
//    windows.foreach(bitnei.state.window => {
//      println("******************")
//      bitnei.state.window.foreach(vecile=>println(vecile.time + ":" + vecile.value))
//    })
//  }
//
//  def generateCi(start: Int, end: Int): Buffer[Vecile] = {
//    (start to end).map(x => Vecile(x, 0.5)).toBuffer
//  }
//
//  def generateNotCi(start: Int, end: Int): Buffer[Vecile] = {
//    (start to end).map(x => Vecile(x, 0.6)).toBuffer
//  }
//
//  def generateNotR(start: Int, end: Int): Buffer[Vecile] = {
//    (start to end).map(x => Vecile(x + 2, 0.5)).toBuffer
//  }
//
//  def generateNotCiAndNotR(start: Int, end: Int): Buffer[Vecile] = {
//    (start to end).map(x => Vecile(x + 2, 0.6)).toBuffer
//  }
//
//  case class Vecile(time:Int,value:Double)
//}


/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

// scalastyle:off println
package org.apache.spark.examples.mllib

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
// $example on$
import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}
// $example off$

object Word2VecExample {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("Word2VecExample")
    val sc = new SparkContext(conf)

    // $example on$
    val input = sc.textFile("data/mllib/sample_lda_data.txt").map(line => line.split(" ").toSeq)

    val word2vec = new Word2Vec()

    val model = word2vec.fit(input)

    val synonyms = model.findSynonyms("1", 5)

    for((synonym, cosineSimilarity) <- synonyms) {
      println(s"$synonym $cosineSimilarity")
    }

    // Save and load model
    model.save(sc, "myModelPath")
    val sameModel = Word2VecModel.load(sc, "myModelPath")
    // $example off$

    sc.stop()
  }
}
// scalastyle:on println
package com.bitnei.report.parquet

import java.io.File

import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.utils.{StringParser, Utils}

import scala.io
import scala.io.Source

/*
* created by wangbaosheng on 2017/11/8
*/

class YangxiaofeiToOraginal(stateConf: StateConf) {
  def run(): Unit = {
    val inputDirectory = stateConf.getString("inputDirectory")
    readAllStartWith(new File(inputDirectory))
  }


  //提取所有以part开头的文件
  def readAllStartWith(inDirectory: File): Unit = {
    inDirectory.listFiles().foreach(file => {
      if (file.isDirectory) {
        readAllStartWith(file)
      } else {
          try {
            procesFile(file.getAbsolutePath, inDirectory.getName)
          } catch {
            case e: Exception =>
          }

      }
    })
  }



  def procesFile(inFile: String, uuid: String): Unit ={
    val values=io.Source.fromFile(new File(inFile)).getLines().toArray.map(line=>{
      val fields=line.split(',')
      s"${fields(0)} 3201:${fields(1)} 2201:${fields(3)} 2202:${fields(4).toInt+10000} 2613:${fields(5)} 2614:${fields(6)} " +
        s"2615:${fields(7)} 2502:${fields(8)} 2503:${fields(9)}"
    })

    Utils.write(s"${stateConf.getString("output.directory")}/${uuid}", values)
  }
}


object YangxiaofeiToOraginal{
  def main(args: Array[String]): Unit = {
    val stateConf=new StateConf
    stateConf.add(args)

    new YangxiaofeiToOraginal(stateConf).run()
  }
}package com.bitnei.report.tempjob

import org.apache.hadoop.yarn.api.records.{ApplicationSubmissionContext, ContainerLaunchContext, Priority, Resource}
import org.apache.hadoop.yarn.client.api.YarnClient
import org.apache.hadoop.yarn.server.api.ResourceManagerConstants

/**
  * Created by wangbaosheng on 2017/10/12.
  */
class YarnApplicationMaster {

}


//Client ResourceManager
//ApplicationMaster ResourceManager
//ApplicationMaster NodeManager

class YarnClassClient{
//
//  //1.创建YarnClient，创建yarn客户端，用于创建和提交application
//  //2.客户端启动之后，就 可以创建application了。.初始化资源(内存，cpu)，设置队列，优先级。
//  //3.创建Container上下文
//
//  val yarnClient=YarnClient.createYarnClient()
//  yarnClient.init(conf)
//  yarnClient.start()
//
//  val application=createApplication(yarnClient)
//
//  val containerContext=createContainerContext()
//
//  submitApplication(application)
//
//  def createApplication(yarnClient:YarnClient):ApplicationSubmissionContext={
//    def initResource(application:ApplicationSubmissionContext,mem:Int,vcores:Int) {
//      val memG = 4
//      val vcores = 8
//      val capability = Resource.newInstance(memG, vcores)
//      application.setResource(capability)
//      application.setPriority(Priority.newInstance(4))
//      application.setQueue("spark")
//    }
//
//    val app=yarnClient.createApplication()
//    val appContext=app.getApplicationSubmissionContext
//    appContext.setKeepContainersAcrossApplicationAttempts(true)
//    appContext.setApplicationName("hello wolrd")
//
//    initResource(appContext,4,4)
//    appContext
//  }
//
//
//
//  def submitApplication(application:ApplicationSubmissionContext): Unit ={
//    yarnClient.submitApplication(application)
//  }
}
package com.bitnei.report.year


import com.bitnei.report.common.configuration.StateConf
import com.bitnei.report.common.log.Logging
import com.bitnei.report.year.DayReport.Output
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession

/**
  *
  * @author zhangyongtian
  * @define 根据日车辆统计数据 计算车辆年统计数据
  *
  *                   车牌  	总行驶里程	总行驶时长	总充电时长	总上线天数	平均车速	总充电量	早高峰平均车速	晚高峰平均车速
  *
  *                   create 2018-01-05 9:26
  *
  */
object YearReport extends Serializable with Logging {

  def main(args: Array[String]): Unit = {
    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    logInfo("appName:" + app)

    // TODO: 初始化参数集合
    logInfo("参数集合....")
    val stateConf = new StateConf
    stateConf.add(args)


    // TODO: 日志级别设置
    val logLevel = stateConf.getOption("log.level").getOrElse("error")
    logInfo("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = stateConf.getOption("env").getOrElse("local")
    logInfo("运行模式：" + env)

    // TODO: 验证参数
    logInfo("验证参数....")

    //时间参数 20170111
    var year = "2017"

    //参数处理
    if (!env.equals("local")) {
      year = stateConf.getOption("input.date").get
    }


    if (year.length != 4) {
      throw new Exception("input.date error")
    }
    //    val year = date.substring(0, 4)
    //    val month = date.substring(4, 6)
    //    val day = date.substring(6)

    ///////////////////////////////////////////////////////////////
    // TODO: 加载上下文
    logInfo("加载上下文")

    val sparkConf = new SparkConf()
    //      .set("spark.rdd.compress", "true")
    //      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    //

    //      .set("spark.kryo.registrator", classOf[MyRegistrator].getName)
    ///如果一个要序列化的类没有进行Kryo注册，则强制Spark报错
    //      .set("spark.kryo.registrationRequired", "true")

    //      .set("spark.kryo.unsafe", "false")
    //      .set("spark.kryoserializer.buffer.max", "128")

    var sparkMaster = "yarn"

    if (env.equals("local")) {
      sparkMaster = "local[1]"
    }

    val sparkSession = SparkSession.builder().config(sparkConf).master(sparkMaster).appName(app + "_" + year).getOrCreate()
    val sc = sparkSession.sparkContext
    import sparkSession.implicits._

    /////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // TODO: 数据源
    logInfo("数据源:将parquet数据注册成表")

    env match {
      case "local" => {
        sparkSession.read.json("data/YearReport/day/*.json").createOrReplaceTempView("dayrep")

      }

      case "dev" => {
        //研发环境
        sparkSession
          .read
          .format("json")
          .load(s"/spark/vehicle/result/yearReport/day/year=${year}").createOrReplaceTempView("dayrep")
      }
      ///tmp/zyt/data/dayrep

      case "prd" => {
        //生产环境
        sparkSession
          .read
          .format("json")
          .load(s"/spark/vehicle/result/yearReport/day/year=${year}").createOrReplaceTempView("dayrep")
      }

    }
    ////////////////////////////////////////////////////

    //    avg_speed: string (nullable = true)
    //    |-- charge_duration: string (nullable = true)
    //    |-- morningAvgSpeed: string (nullable = true)
    //    |-- morningPeakMileage: string (nullable = true)
    //    |-- morningPeakRunDuration: string (nullable = true)
    //    |-- nightAvgSpeed: string (nullable = true)
    //    |-- nightPeakMileage: string (nullable = true)
    //    |-- nightPeakRunDuration: string (nullable = true)
    //    |-- run_duration: string (nullable = true)
    //    |-- totalMilage: string (nullable = true)
    //    |-- total_charge_num: string (nullable = true)
    //    |-- vid: string (nullable = true)

    val sql =
      """
        select
        vid,
        date,
        totalMilage,
        run_duration,
        charge_duration,
        total_charge_num,
        morningPeakMileage,
        morningPeakRunDuration,
        nightPeakMileage,
        nightPeakRunDuration
        FROM dayrep
      """.stripMargin


    val initDS =
      sparkSession.sql(sql).as[(String, String, String, String, String, String, String, String, String, String)]

    val filteredDS = initDS
      .filter(x => {

        val vid = x._1
        val date = x._2.toDouble
        val totalMilage = x._3.toDouble
        val run_duration = x._4.toDouble
        val charge_duration = x._5.toDouble
        val total_charge_num = x._6.toDouble
        val morningPeakMileage = x._7.toDouble
        val morningPeakRunDuration = x._8.toDouble
        val nightPeakMileage = x._9.toDouble
        val nightPeakRunDuration = x._10.toDouble


        true
      })

    // TODO: 提取
    val mappedDS = filteredDS
      .map(x => {
        val vid = x._1
        val date = x._2.toDouble
        val totalMilage = x._3.toDouble
        val run_duration = x._4.toDouble
        val charge_duration = x._5.toDouble
        val total_charge_num = x._6.toDouble
        val morningPeakMileage = x._7.toDouble
        val morningPeakRunDuration = x._8.toDouble
        val nightPeakMileage = x._9.toDouble
        val nightPeakRunDuration = x._10.toDouble

        Input(vid, date, totalMilage, run_duration, charge_duration, total_charge_num, morningPeakMileage, morningPeakRunDuration, nightPeakMileage, nightPeakRunDuration)
      })

    val result = mappedDS
      .groupByKey(_.vid)
      .mapGroups {
        case (vid, inputs: Iterator[Input]) => {
          val arr = inputs.toArray
          val online_days = arr.length

          var mileages = 0D;
          var run_durations = 0D;
          var avg_speed = 0D;

          var charge_durations = 0D;
          var charge_nums = 0D;

          var morningPeakMileages = 0D;
          var morningPeakRunDurations = 0D

          var nightPeakMileages = 0D;
          var nightPeakRunDurations = 0D;

          arr.foreach(x => {
            mileages = mileages + x.totalMilage
            run_durations = run_durations + x.run_duration
            charge_durations = charge_durations + x.charge_duration
            charge_nums = charge_nums + x.total_charge_num
            morningPeakMileages = morningPeakMileages + x.morningPeakMileage
            morningPeakRunDurations = morningPeakRunDurations + x.morningPeakRunDuration
            nightPeakMileages = nightPeakMileages + x.nightPeakMileage
            nightPeakRunDurations = nightPeakRunDurations + x.nightPeakRunDuration
          })


          if (mileages == 0D) run_durations = 0D;

          if (morningPeakMileages == 0D) morningPeakRunDurations = 0D;

          if (nightPeakMileages == 0D) nightPeakRunDurations = 0D;


          if (run_durations != 0) {
            avg_speed = mileages / run_durations
          }

          var morningAvgSpeeds = 0D

          if (morningPeakRunDurations != 0) {
            morningAvgSpeeds = morningPeakMileages / morningPeakRunDurations
          }

          var nightAvgSpeeds = 0D

          if (nightPeakRunDurations != 0) {
            nightAvgSpeeds = nightPeakMileages / nightPeakRunDurations
          }

          //车牌  年份 	总行驶里程	总行驶时长	总充电时长	总上线天数	平均车速	总充电量	早高峰平均车速	晚高峰平均车速
          Output(vid, year, mileages, run_durations, charge_durations, online_days, avg_speed, charge_nums, morningAvgSpeeds, nightAvgSpeeds)
        }


      }














    ///////////////////////////////
    sparkSession.catalog.dropTempView("dayrep")

    // TODO: 输出
    if (env.equals("local")) {
      result.show(false)
      //      result.show(100, false)

      //      result.count()

      //      result.count()
      //    result.printSchema()
      //    result.show(false)
      //


    }

    // TODO: 输出到
    if (env.equals("dev")) {
      result.repartition(1).write.json(s"/spark/vehicle/result/yearReport/year/year=${year}")
    }

    if (env.equals("prd")) {
    }

    logInfo("任务完成...")
  }

  case class Input(vid: String, date: Double, totalMilage: Double, run_duration: Double, charge_duration: Double, total_charge_num: Double, morningPeakMileage: Double, morningPeakRunDuration: Double, nightPeakMileage: Double, nightPeakRunDuration: Double)

  case class Output(vid: String, year: String, mileages: Double, run_durations: Double, charge_durations: Double, online_days: Int, avg_speed: Double, charge_nums: Double, morningAvgSpeeds: Double, nightAvgSpeeds: Double)

}
