import scala.collection.mutable
import scala.io.Source

object Apriori{

  def main(args: Array[String]) {

    val minSup = 2											//设置最小支持度
    val list = new mutable.LinkedHashSet[String]()					//设置可变列表
Source.fromFile("c://apriori.txt").getLines()						//读取数据集并存储
.foreach(str => list.add(str))								//将数据存储
    var map = mutable.Map[String,Int]()							//设置map进行计数
    list.foreach(strss => {										//计算开始
      val strs = strss.split("、")									//分割数据
      strs.foreach(str => {									//开始计算程序
        if(map.contains(str)){									//判断是否存在
          map.update(str,map(str) + 1)							//对已有数据+1
        } else map += (str -> 1)								//将未存储的数据加入
      })
    })

    val tmpMap = map.filter(_._2 > minSup)						//判断最小支持度

    val mapKeys = tmpMap.keySet								//提取清单内容
    val tempList = new mutable.LinkedHashSet[String]()				//创先辅助List
    val conList = new mutable.LinkedHashSet[String]()				//创建连接List
    mapKeys.foreach(str => tempList.add(str))						//进行连接准备
    tempList.foreach(str => {									//开始连接
      tempList.foreach(str2 =>{								//读取辅助List
        if(str != str2){										//判断
          val result = str + "、" + str2							//创建连接字符
          conList.add(result)									//添加
        }
      })
    })

    conList.foreach(strss => {								  //开始对原始列表进行比对
      val strs = strss.split("、")									//切分数据
      strs.foreach(str => {									//开始计数
        if(map.contains(str)){									//判断是否包含
          map.update(str,map(str) + 1)							//对已有数据+1
        } else map += (str -> 1)								//将未存储的数据加入
      })
    })
  }
}

-----

import org.apache.spark.mllib.util.MLUtils
import org.apache.spark.{SparkContext, SparkConf}
import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

object Bayes {
  def main(args: Array[String]) {

val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("Bayes ")                              			//设定名称
    val sc = new SparkContext(conf)                                 //创建环境变量实例
    val data = MLUtils.loadLabeledPoints(sc,"c://bayes.txt")			//读取数据集
    val model = NaiveBayes.train(data, 1.0)						//训练贝叶斯模型
    model.labels.foreach(println)								//打印label值
model.pi.foreach(println)									//打印先验概率
}
}

-------------

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.mllib.classification.NaiveBayes
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

object Bayes {

  def main(args: Array[String]) {
val conf = new SparkConf()                                     //创建环境变量
.setMaster("local")                                             //设置本地化处理
.setAppName("BayesTest ")                              			//设定名称
    val sc = new SparkContext(conf)                                 //创建环境变量实例
    val data = MLUtils.loadLabeledPoints(sc,"c://data.txt")			//读取数据集
    val data = file.map { line =>								//处理数据
      val parts = line.split(',')									//分割数据
      LabeledPoint(parts(0).toDouble, 							//标签数据转换
Vectors.dense(parts(1).split(' ').map(_.toDouble)))				//向量数据转换
    }

    val splits = data.randomSplit(Array(0.7, 0.3), seed = 11L)			//对数据进行分配
    val trainingData = splits(0)									//设置训练数据
    val testData = splits(1)									//设置测试数据
    val model = NaiveBayes.train(trainingData, lambda = 1.0)			//训练贝叶斯模型
    val predictionAndLabel = testData.map(p => (model.predict(p.features), p.label)) //验证模型
    val accuracy = 1.0 * predictionAndLabel.filter(					//计算准确度
      label => label._1 == label._2).count()						//比较结果
    println(accuracy)										//打印准确度
  }
}

-------------

import org.apache.spark.{SparkContext, SparkConf}

object CacheTest {
  def main(args: Array[String]) {
    val conf = new SparkConf()                                     //创建环境变量
      .setMaster("local")                                           //设置本地化处理
  .setAppName("CacheTest")								//设定名称
    val sc = new SparkContext(conf)							//创建环境变量实例
    val arr = sc.parallelize(Array("abc","b","c","d","e","f"))				//设定数据集
    println(arr)												//打印结果
    println("----------------")										//分隔符
    println(arr.cache())										//打印结果
  }
}

------------

import org.apache.spark.{SparkContext, SparkConf}

object CacheTest2 {
  def main(args: Array[String]) {
    val conf = new SparkConf()                                     //创建环境变量
      .setMaster("local")                                           //设置本地化处理
  .setAppName("CacheTest2")								//设定名称
    val sc = new SparkContext(conf)							//创建环境变量实例
    val arr = sc.parallelize(Array("abc","b","c","d","e","f"))				//设定数据集
    arr.foreach(println)										//打印结果
  }
}

------------



















