spark-submit \
--class com.an.lcloud.lrs.iap.getpolicy.job.NewPolicyJob \
--jars /home/chenjia868/dom4j-1.6.1.jar  \
--master yarn-client \
--queue queue_1508_01 \
--num-executors 2 \
--executor-memory 512M \
--executor-cores 1 \
--driver-memory 1G \
--conf spark.default.parallelism=27 \
--conf spark.storage.memoryFraction=0.5 \
--conf spark.shuffle.memoryFraction=0.3 \
--conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:MaxPermSize=64m" \
/home/chenjia868/lcloud-lrs-iap-1.0.jar 2017-01-01 2017-02-01 1120

-----------------------------------------------
spark-submit \
--class com.an.lcloud.odin.iap.job.impl.RbsJob \
--jars /home/zhouqinxiong938/dom4j-1.6.1.jar,/home/zhouqinxiong938/bson-3.4.3.jar,/home/zhouqinxiong938/mongodb-driver-3.4.3.jar,/home/zhouqinxiong938/mongodb-driver-core-3.4.3.jar  \
--jars /home/zhouqinxiong938/dom4j-1.6.1.jar,/home/zhouqinxiong938/commons-jexl-2.1.1.jar,/home/zhouqinxiong938/shield-spark-common-1.0.0-SNAPSHOT.jar \
--master yarn-client \
--queue queue_1508_01 \
--num-executors 2 \
--executor-memory 512M \
--executor-cores 1 \
--driver-memory 1G \
--conf spark.default.parallelism=27 \
--conf spark.storage.memoryFraction=0.5 \
--conf spark.shuffle.memoryFraction=0.3 \
--conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:MaxPermSize=64m" \
/home/zhouqinxiong938/lcloud-odin-iap.jar /xml/hive_sql.xml


spark-submit \
--class com.an.lcloud.shield.spark.job.impl.EffectiveGuaranteeCalculateJob \
--jars /home/zhouqinxiong938/dom4j-1.6.1.jar,/home/zhouqinxiong938/commons-jexl-2.1.1.jar,/home/zhouqinxiong938/shield-spark-common-1.0.0-SNAPSHOT.jar \
--master yarn-client \
--queue queue_1508_01 \
--num-executors 5 \
--executor-memory 512M \
--executor-cores 2 \
--driver-memory 1G \
--conf spark.default.parallelism=27 \
--conf spark.storage.memoryFraction=0.5 \
--conf spark.shuffle.memoryFraction=0.3 \
--conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:MaxPermSize=64m" \
/home/zhouqinxiong938/shield-spark-cash-value.jar /xml/benefitSql.xml


spark-submit \
--class com.an.lcloud.shield.spark.job.impl.DataPrepareJob \
--jars /home/zhouqinxiong938/dom4j-1.6.1.jar,/home/zhouqinxiong938/commons-jexl-2.1.1.jar,/home/zhouqinxiong938/shield-spark-common-1.0.0-SNAPSHOT.jar \
--master yarn-client \
--queue queue_1508_01 \
--num-executors 5 \
--executor-memory 512M \
--executor-cores 2 \
--driver-memory 1G \
--conf spark.default.parallelism=27 \
--conf spark.storage.memoryFraction=0.5 \
--conf spark.shuffle.memoryFraction=0.3 \
--conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:MaxPermSize=64m" \
/home/zhouqinxiong938/shield-spark-cash-value.jar /xml/benefitSql.xml


spark-submit \
--class com.an.lcloud.odin.iap.job.impl.SynchronizedClientTagJob \
--jars /home/zhouqinxiong938/dom4j-1.6.1.jar,/home/zhouqinxiong938/bson-3.4.3.jar,/home/zhouqinxiong938/mongo-hadoop-core-1.4.2.jar,/home/zhouqinxiong938/mongodb-driver-3.4.3.jar,/home/zhouqinxiong938/mongodb-driver-core-3.4.3.jar  \
--master yarn-client \
--queue queue_1508_01 \
--num-executors 5 \
--executor-memory 512M \
--executor-cores 2 \
--driver-memory 1G \
--conf spark.default.parallelism=200 \
--conf spark.storage.memoryFraction=0.5 \
--conf spark.shuffle.memoryFraction=0.3 \
--conf spark.shuffle.partitionsw=200 \
--conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:MaxPermSize=64m" \
/home/zhouqinxiong938/lcloud-odin-iap.jar /xml/insureTagSql.xml


spark-submit \
--class com.an.lcloud.odin.iap.job.impl.DataFromOdsToBaseJob \
--jars /home/zhouqinxiong938/dom4j-1.6.1.jar,/home/zhouqinxiong938/commons-jexl-2.1.1.jar,/home/zhouqinxiong938/shield-spark-common-1.0.0-SNAPSHOT.jar \
--master yarn-client \
--queue queue_1508_01 \
--num-executors 5 \
--executor-memory 512M \
--executor-cores 2 \
--driver-memory 1G \
--conf spark.default.parallelism=27 \
--conf spark.storage.memoryFraction=0.5 \
--conf spark.shuffle.memoryFraction=0.3 \
--conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:MaxPermSize=64m" \
/home/zhouqinxiong938/lcloud-odin-iap.jar /xml/odsTobase.xml


package com.an.lcloud.act.core.framework.job

import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{Logging, SparkContext}

/**
  * Created by EX-ZHANGYONGTIAN001 on 2018-11-1.
  */
abstract class BaseJob(@transient sc: SparkContext, @transient hiveContext: HiveContext, serviceConfMap: scala.collection.mutable.HashMap[String, String]) extends JobCommon with Logging with Serializable {

  val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

  def readXml(path: String) = {
    //读取xml获取参数
    //    val document = parse(s"/mp/${app.filter(_.isDigit)}.xml")
    val document = parse(path)
    serviceConfMap.++=(loadServiceConf(document))
  }

  //运行环境 （研发、测试、生产）
  val env = serviceConfMap.getOrElse("env", "local")

  def initPrd(): Unit = {
    val hive_db = serviceConfMap.getOrElse("dbName", "sx_core_safe")
    hiveContext.sql("use " + hive_db)
    //    hiveContext.sql("set hive.exec.dynamic.partition = true")
    //    hiveContext.sql("set hive.exec.dynamic.partition.mode = nonstrict")
  }

  def init(): Unit = {
    //初始化数据
    if (env == "prd") {
      initPrd()
    }
  }

  def run(): Unit


}
package com.an.lcloud.act.consistency.job

import com.an.lcloud.act.core.framework.job.BaseJob
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{Logging, SparkContext}
import scala.collection._

/**
  * Created by EX-LIANGHANCHANG001 on 2019-4-29.
  *
  * @param sc
  * @param hiveContext
  * @param serviceConfMap
  */
class DataConsistency(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: mutable.HashMap[String, String]) extends BaseJob(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: mutable.HashMap[String, String]) with Logging {
  override def run(): Unit = {
    val proc_date = serviceConfMap.getOrElse("p_proc_date", "") //评估日期
    val res_version_num = serviceConfMap.getOrElse("res_version_num", "") //版本号

    readXml("/consistency/consistency.xml") //加载xml数据
    initPrd() //初始化

    logInfo("==========》开始加载基表数据《=======")
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("pol_ben_consistency_rules", ""), serviceConfMap))
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("las_value_added_tax_plan", ""), serviceConfMap))
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("cas_acct_vou_dtl_count", ""), serviceConfMap))
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("pol_ben_base", ""), serviceConfMap))
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("count_base", ""), serviceConfMap))
    logInfo("==========》基表数据加载完成《=======")

    /**
      * rule001~002
      **/
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule001", ""), serviceConfMap))
    val rule001 =
      s"insert overwrite table bas_act_pol_ben_actuary_consistency partition(proc_date='$proc_date',version_num='$res_version_num')" +
        s"select * from rule001"
    hiveContext.sql(rule001)
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule002", ""), serviceConfMap))
    hiveContext.sql("uncache table rule001")

    /**
      * rule003~004
      **/
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule003", ""), serviceConfMap))
//    TODO:rule003不写表，测试完后需删除
    val rule003 =
      s"insert into table bas_act_pol_ben_actuary_consistency partition(proc_date='$proc_date',version_num='$res_version_num')" +
        s"select * from rule003"
    hiveContext.sql(rule003)
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule004", ""), serviceConfMap))
    hiveContext.sql("uncache table rule003")

    /**
      * rule005~008
      **/
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule005_007", ""), serviceConfMap))
    val rule005_007 =
      s"insert into table bas_act_pol_ben_actuary_consistency partition(proc_date='$proc_date',version_num='$res_version_num')" +
//        s"select * from rule005_007 where ck_rule_no = '005'" TODO:rule007不写表，测试完后需删除
        s"select * from rule005_007"
    hiveContext.sql(rule005_007)
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule006_008", ""), serviceConfMap))
    hiveContext.sql("uncache table rule005_007")

    /**
      * rule009~015
      **/
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule009_011_013_015", ""), serviceConfMap))
    val rule009_011_013_015 =
      s"insert into table bas_act_pol_ben_actuary_consistency partition(proc_date='$proc_date',version_num='$res_version_num')" +
        s"select * from rule009_011_013_015"
    hiveContext.sql(rule009_011_013_015)
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule010_012_014_016", ""), serviceConfMap))
    hiveContext.sql("uncache table rule009_011_013_015")


    /**
      * rule017~018
      **/
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule017", ""), serviceConfMap))
    val rule017 =
      s"insert into table bas_act_pol_ben_actuary_consistency partition(proc_date='$proc_date',version_num='$res_version_num')" +
        s"select * from rule017"
    hiveContext.sql(rule017)
    hiveContext.sql(selectMapping(serviceConfMap.getOrElse("rule018", ""), serviceConfMap))
    hiveContext.sql("uncache table rule017")


    /*释放缓存*/
    hiveContext.sql("uncache table pol_ben_consistency_rules")
    hiveContext.sql("uncache table las_value_added_tax_plan")
    hiveContext.sql("uncache table cas_acct_vou_dtl_count")
    hiveContext.sql("uncache table pol_ben_base")
    hiveContext.sql("uncache table count_base")

  }

}
package com.an.lcloud.act.consistency.launcher

import com.an.lcloud.act.consistency.job.DataConsistency
import com.an.lcloud.act.core.framework.job.JobCommon
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{Logging, SparkConf, SparkContext}

/**
  * Created by EX-LIANGHANCHANG001 on 2019-4-29.
  *
  * 数据一致性入口程序
  */
object DataConsistencyExecutor extends JobCommon with Logging {
  def main(args: Array[String]): Unit = {

    val startTime = System.currentTimeMillis()
    args.foreach(arg => {
      logInfo(arg) //打印参数到控制台
      val param: Array[String] = arg.split("=")

      if (param.length == 2) {
        serviceConfMap.put(param(0), param(1))
      } else (
        throw new IllegalArgumentException(s"the format of $arg is wrong")
        )
    })

    //TODO:设置日志级别
    val logLevel = serviceConfMap.getOrElse("log.level","info")
    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    //TODO:参数校验
    if(args.length < 5) {
      logError("Usage: 未指定参数列表" +
        "[p_proc_date][p_end_date][src_version_num][res_version_num][res_hive_db]")
      System.exit(1)
    }

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    val sparkConf = new SparkConf()
      .setAppName(app)
      .set("spark.rdd.compress", "true")
      .set("spark.sql.codegen", "true") /*spark.sql.codegen 是否预编译sql成java字节码，长时间或频繁的sql有优化效果*/
      .set("spark.sql.inMemoryColumnarStorage.batchSize", "5000") /*spark.sql.inMemoryColumnarStorage.batchSize 一次处理的row数量，小心oom*/
      .set("spark.sql.inMemoryColumnarStorage.compressed", "true") /*spark.sql.inMemoryColumnarStorage.compressed 设置内存中的列存储是否需要压缩*/
      .set("spark.sql.autoBroadcastJoinThreshold", "1073741824") /*spark.sql.autoBroadcastJoinThreshold,解决数据倾斜*/
      .set("hive.execution.engine", "spark") /* hive执行引擎 */
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") /*Kryo序列化*/

    val sc = new SparkContext(sparkConf)
    val hiveContext = new HiveContext(sc)

    new DataConsistency(sc, hiveContext, serviceConfMap).run()

    logInfo("数据一致性规则校验总共花费时间：" + (System.currentTimeMillis() - startTime) / 1000 + " 秒")

    sc.stop()
  }
}
package com.an.lcloud.act.core.framework.comm

/**
  * Created by ZOUBO162 on 2018-11-1.
  * DCS常量类
  */
object DCSConstants {
  /** 个银传统分红批处理编码 */
  val CONST_GROUPING_PROC_CODE_IND = "0430"
  val CONST_NB_IF_PATH = "NBIF"
  val CONST_NB_PATH = "NB"

  /** dcs uv 批处理码 */
  val CONST_GROUPING_PROC_CODE_UV = "1030"

  /** dcs guar 批处理码 */
  val CONST_GROUPING_PROC_CODE_GUAR = "1130"

  /** dcs wb 批处理码 */
  val CONST_GROUPING_PROC_CODE_WB = "1330"

  /** dcs short 批处理码 */
  val CONST_GROUPING_PROC_CODE_SHORT = "0930"

  /** DCS PUA 批处理代码0630 */
  val CONST_GROUPING_PROC_CODE_PUA = "0630"

  /** DCS ANN 批处理代码0130 */
  val CONST_GROUPING_PROC_CODE_ANN = "0130"

  /** DCS ANN 批处理代码0530 */
  val CONST_GROUPING_PROC_CODE_TRA = "0530"

  /** dcs xml文件路径 */
  val CONST_XML_FILE_PATH_IND = "/dcs/ind.xml"
  val CONST_XML_FILE_PATH_UV = "/dcs/uv.xml"
  val CONST_XML_FILE_PATH_GUAR = "/dcs/guar.xml"
  val CONST_XML_FILE_PATH_WB = "/dcs/wb.xml"
  val CONST_XML_FILE_PATH_SHORT = "/dcs/short.xml"
  val CONST_XML_FILE_PATH_PUA = "/dcs/pua.xml"
  val CONST_XML_FILE_PATH_TOP = "/dcs/top.xml"
  val CONST_XML_FILE_PATH_ANN = "/dcs/ann.xml"
  val CONST_XML_FILE_PATH_TRA = "/dcs/tra.xml"
  val CONST_XML_FILE_PATH_BASE = "/dcs/base.xml"

  val FILE_HDFS_PATH_IND = "/apps-data/hduser1508/sx_hx_safe/act_dcs_result_ind"
  val FILE_HDFS_PATH_GUAR = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_guar"
  val FILE_HDFS_PATH_SHORT = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_short"
  val FILE_HDFS_PATH_UV = "/apps-data/hduser1508/sx_hx_safe/act_dcs_result_uv"
  val FILE_HDFS_PATH_WB = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_wb"
  val FILE_HDFS_PATH_PUA = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_pua"
  val FILE_HDFS_PATH_TOP = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_top"
  val FILE_HDFS_PATH_ANN = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_ann"
  val FILE_HDFS_PATH_TRA = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_tra"
  val FILE_HDFS_PATH_INVALID = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_invalid_log"

  val CONST_GROUPING_PROC_CODE_UL: String = "0730"
  val CONST_XML_FILE_PATH_UL: String = "/dcs/ul.xml"
  val FILE_HDFS_PATH_UL: String = "/apps-data/hduser1508/sx_hx_safe/act_dcs_result_ul"

  val FILE_HDFS_PATH_TOP888: String = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_top"
  val CONST_GROUPING_PROC_CODE_TOP888: String = "0760"
  val CONST_XML_FILE_PATH_TOP888: String = "/dcs/top.xml"


  //651类别dcs的配置
  val FILE_HDFS_PATH_651: String = "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_result_651"
  val CONST_GROUPING_PROC_CODE_651: String = "0330"
  val CONST_XML_FILE_PATH_651: String = "/dcs/651.xml"


  val CONST_DCS_CATEGORY_TOP = "top"
  val CONST_DCS_TOP_TASK = "uv_top"
  val CONST_DCS_CATEGORY_ANN = "pol_ben_ann"
  val CONST_DCS_ANN_TASK = "ann"
  val CONST_DCS_CATEGORY_UV = "universal"
  val CONST_DCS_UV_TASK = "uv"
  val CONST_DCS_CATEGORY_IND = "pol_ben_main"
  val CONST_DCS_IND_TASK = "pol_ben_ind"


  val CONST_DCS_CATEGORY_TOP888 = "top_ul888"
  val CONST_DCS_CATEGORY_TOP888_5CH = "top_ul888_KAOHE_5CH"
  val CONST_DCS_CATEGORY_UL = "ulink"
  val CONST_DCS_CATEGORY_651 = "651"
  val CONST_DCS_ULINK_TASK = "ulink"
  val CONST_DCS_651_TASK = "651"
  val CONST_DCS_TOP888_TASK = "top888"

  val CONST_DCS_CATEGORY_GUAR = "guar"
  val CONST_DCS_GUAR_TASK = "guar"
  val CONST_DCS_CATEGORY_TRA = "tra"
  val CONST_DCS_TRA_TASK = "tra"
  val CONST_DCS_CATEGORY_SHORT = "short"
  val CONST_DCS_SHORT_TASK = "short"

  /** WORK_SPACE */

  val CONST_DCS_WORK_SPACE_IND = "IND"
  val CONST_DCS_WORK_SPACE_651 = "651"
  val CONST_DCS_WORK_SPACE_ANN = "ANN"
  val CONST_DCS_WORK_SPACE_GUAR = "GUAR"
  val CONST_DCS_WORK_SPACE_TRA = "GTRA"
  val CONST_DCS_WORK_SPACE_TOP = "UV_TOP"
  val CONST_DCS_WORK_SPACE_TOP_888 = "ULINK"
  val CONST_DCS_WORK_SPACE_WB = "WB"
  val CONST_DCS_WORK_SPACE_SHORT = "SH"
  val CONST_DCS_WORK_SPACE_UV = "UV"
  val CONST_DCS_WORK_SPACE_UL = "ULINK"

  /** folder_in */
  val CONST_DCS_FOLDER_IN_IND = "POL_BEN_MAIN"
  val CONST_DCS_FOLDER_IN_651 = "651"
  val CONST_DCS_FOLDER_IN_ANN = "ANN"
  val CONST_DCS_FOLDER_IN_GUAR = "GUAR"
  val CONST_DCS_FOLDER_IN_TRA = "TRA"
  val CONST_DCS_FOLDER_IN_UV_TOP = "UV_TOP"
  val CONST_DCS_FOLDER_IN_TOP_888 = "TOP"
  val CONST_DCS_FOLDER_IN_WB = "WB"
  val CONST_DCS_FOLDER_IN_SHORT = "SHORT"
  val CONST_DCS_FOLDER_IN_UV = "UV"
  val CONST_DCS_FOLDER_IN_UL = "UL"

  /** proc_type */
  val CONST_DCS_PROC_TYPE_GAAP = "GAAP"
  val CONST_DCS_PROC_TYPE_I17 = "I17"


  /** nb or if */
  val CONST_DCS_POL_BEN_NB = "NB"
  val CONST_DCS_POL_BEN_IF = "IF"

  /** BAS_ACT_DCS_INVALID_LOG表字段 */
  val DCS_RESULT_COLUMN_INVALID = "DCS_CATEGORY,OUTPUT_PATH,PLAN_CODE,POLNO,FAILED_VAR,FAILED_VALUE,INVALID_TYPE,REMARK,CREATED_BY,CREATED_DATE,UPDATED_BY,UPDATED_DATE"

  /** BAS_ACT_DCS_RESULT_IND表字段 */
  val DCS_RESULT_COLUMN_IND = "OUTPUT_PATH,PLAN_CODE,SPCODE,AGE_AT_ENTRY,SEX,ENTRY_YEAR,ENTRY_MONTH,AGE2_ATENTRY,SEX2,POL_STATUS,DIST_CHANNEL,POL_TERM_Y,PREM_PAYBL_Y,PREM_FREQ,SUM_ASSURED,ANNUAL_PREM,UNITS,AGE_PAYMENT,DURATIONIF_M,INIT_POLS_IF,PAYMENT_CODE,POLNO,VAL_CODE,INDEX1,SUM_ASSD_ORG,COMM_INDEX,CI_RIDER,SUM_ASSD_M,SUM_ASSD_R,SUM_ASSD_R2,PLANCODE_M,INDEX_M,ANN_PREM_PKG,COMM_PAY,ANN_PREM_M,PLAN_CODE_R,ANN_PREM_R,PREM_TM_ORG,PREM_FREQ_M,EXPENSE_PC,DUR_MM,PAY_OUT_TYPE,INS_NUMBER,ANNUITY_FREQ,GTEE_PER_Y,PROC_DATE,PLAN_CODE_G,PASTYEAR,PX_ROUND,ANNUITY_TYPE,PLAN_CODE_S,PROD_NAME,SUBPARTITION_ID,CREATED_BY,CREATED_DATE,UPDATED_BY,UPDATED_DATE,GROUP_ID,INSURED_STS,REGION_CODE,CHANNEL_MODE,DNO,SNO,DEPTNO,BEN_STS,BRNO,TG_METHOD,TG_RISK_TYPE,TG_PL,TG_ENTRYYEAR,TG_INS_RISK,LOAN_IDX,LOAN_ISS_AMT,LOAN_INT_ACC,APL_IDX,APL_ISS_AMT,APL_INT_ACC,SURV_OPTION,SURV_ACC_PRC,SURV_ACC_INT,DIV_OPTION,DIV_ACC_PRP,DIV_ACC_INT,UNDWRT_DATE"

}
package com.an.lcloud.act.core.framework.comm

/**
  * Created by ZOUBO162 on 2018-11-1.
  * MP常量类
  */
object MPConstants {
  val IND_773_FILEPATH = "/mp/ind2/773.xml"
  val IND_905_FILEAPTH = "/mp/ind2/905.xml"
  val IND_BASE_XML_FILEPATH = "/mp/ind2/base.xml"
  val IND_HW_FILEPATH ="/mp/ind2/hw.xml"
  val IND_HI_FILEPATH ="/mp/ind2/hi.xml"
  val IND_ALL_FILEPATH ="/mp/ind2/all.xml"
  val IND_1142_FILEPATH ="/mp/ind2/1142.xml"
  val IND_3052_FILEPATH ="/mp/ind2/3052.xml"
  val IND_783_FILEPATH = "/mp/ind/783.xml"
  val ANN_FILEPATH = "/mp/ann/ann.xml"
  val GBS_FILEPATH = "/mp/gbs/gbs.xml"
  val PUA_FILEPATH = "/mp/pua/pua.xml"
  val IRR_FILEPATH = "/mp/irr/irr.xml"
  val GUAR_FILEPATH="/mp/guar/guar.xml"
  val SHORT_FILEPATH="/mp/short/short.xml"
  val TOP_FILEPATH = "/mp/top/top.xml"
  val UV_FILEPATH = "/mp/uv/uv.xml"
  val UV_TEMP_FILEPATH = "/mp/uv/uv_temp.xml"
  val UV_844_FILEPATH = "/mp/uv/uv844.xml"
  val UV_2007_FILEPATH = "/mp/uv/uv2007.xml"
  val UV_892_FILEPATH = "/mp/uv/uv892.xml"
  val UV_UL_RIDER_FILEPATH = "/mp/uv/univUlinkRider.xml"
  val BASETABLE_FILEPATH="/mp/guar/baseTable.xml"
  val ELSE_BASE_XML_FILEPATH = "/mp/guar/baseTable.xml"
  val ELSE_PRELOAD_XML_FILEPATH = "/mp/preload/preLoad.xml"
  val MP_PRETREATMENT_XML_PATH="/pretreatment/pretreatment.xml"
  val UL_FILEPATH = "/mp/ul/ul.xml"
  val ULTemp_FILEPATH = "/mp/ul/ul_temp.xml"
  val IND_921_FILEPATH="/mp/ind2/921.xml"
  val ULTemp2_FILEPATH = "/mp/ul/ul_temp_2.xml"
  val Else651_FILEPATH = "/mp/651/651.xml"
  val map = Map(

    //个银传统红利交清增额保单
    "const_ind_pol_pua" -> "PUA",
    //万能追加保险费信息
    "const_uv_pol_basic" -> "BASIC",
    //万能基本保单信息
    "const_uv_pol_top" -> "TOP",
    //机构库常量
    "const_lbs_brch_bj" -> "BJ",
    //处理过程常量
    "const_proc_pol_2005" -> "pol_ben_main-01-2005",
    "const_proc_pol_773" -> "pol_ben_main-02-773",
    "const_proc_pol_783" -> "pol_ben_main-03-783",
    "const_proc_pol_905" -> "pol_ben_main-04-905",
    "const_proc_pol_921" -> "pol_ben_main-05-921",
    "const_proc_pol_834" -> "pol_ben_main-06-834",
    "const_proc_pol_916" -> "pol_ben_main-07-916",
    "const_proc_pol_920" -> "pol_ben_main-08-920",
    "const_proc_pol_940" -> "pol_ben_main-09-940",
    "const_proc_pol_990" -> "pol_ben_main-10-990",
    "const_proc_pol_991" -> "pol_ben_main-11-991",
    "const_proc_pol_996" -> "pol_ben_main-12-996",
    "const_proc_pol_1101" -> "pol_ben_main-13-1101",
    "const_proc_pol_hw" -> "pol_ben_main-14-760-76701W",
    "const_proc_pol_hi" -> "pol_ben_main-15-760-76701",
    "const_proc_pol_all" -> "pol_ben_main-16-main_comm",
    "const_proc_pol_1142" -> "pol_ben_main-17-1142",
    "const_proc_pol_791" -> "pol_ben_main-18-791",
    "const_proc_pol_3052" -> "pol_ben_main-19-3052",
    "const_proc_pol_1253" -> "pol_ben_main-20-1253",
    "const_proc_pol_651" -> "pol_ben_main-21-651",
    "const_proc_pol_main" -> "pol_ben_main",
    "const_proc_pol_goat" -> "goat",
    "const_proc_pol_pua" -> "pua",
    "const_proc_pol_lbs" -> "tra",
    "const_proc_pol_ulink" -> "ulink",
    "const_proc_pol_universal" -> "universal",
    "const_proc_pol_top" -> "top",
    "const_proc_pol_885_878" -> "885_878",
    "const_proc_pol_guar" -> "guar",
    "const_proc_pol_irr" -> "irr",
    "const_proc_pol_ann" -> "ann",
    "const_proc_pol_short" -> "short",
    "const_proc_univ_get_pol_" -> "universal-01-univ_get_pol",
    "const_proc_univ_get_chg_pol_va" -> "universal-02-univ_get_chg_pol",
    "const_proc_univ_calc_pol_" -> "universal-03-univ_calc_last_pol",
    "const_proc_univ_get_curr_act" -> "universal-04-univ_get_curr_act",
    "const_proc_univ_acct_rider" -> "universal-05-univ_acct_rider",
    "const_proc_univ_pol_892" -> "universal-06-univ_pol_892",
    "const_proc_univ_pol_2007" -> "universal-07-univ_pol_2007",
    "const_proc_univ_pol_844" -> "universal-08-univ_pol_844",
    "const_proc_pol_actuary_synch" -> "auto_update_pol_ben_actuary",
    //处理过程名
    "const_pname_pol_2005" -> "个银传统分红2005等产品保单信息生成",
    "const_pname_pol_1142" -> "个银传统分红1142等产品保单信息生成",
    "const_pname_pol_783" -> "个银传统分红783等产品保单信息生成",
    "const_pname_pol_905" -> "个银传统分红905等产品保单信息生成",
    "const_pname_pol_921" -> "个银传统分红921等产品保单信息生成",
    "const_pname_pol_all" -> "个银传统分红主要产品保单信息生成",
    "const_pname_pol_834" -> "个银传统分红834等产品保单信息生成",
    "const_pname_pol_916" -> "个银传统分红916等产品保单信息生成",
    "const_pname_pol_920" -> "个银传统分红920等产品保单信息生成",
    "const_pname_pol_940" -> "个银传统分红940等产品保单信息生成",
    "const_pname_pol_990" -> "个银传统分红990等产品保单信息生成",
    "const_pname_pol_991" -> "个银传统分红991等产品保单信息生成",
    "const_pname_pol_996" -> "个银传统分红996等产品保单信息生成",
    "const_pname_pol_1101" -> "个银传统分红1101等产品保单信息生成",
    "const_pname_pol_773" -> "个银传统分红773等产品保单信息生成",
    "const_pname_pol_hi" -> "个银传统分红760-76701等产品普通状态保单信息生成",
    "const_pname_pol_hw" -> "个银传统分红760-76701等产品W状态保单信息生成",
    "const_pname_pol_791" -> "个银传统分红791等产品W状态保单信息生成",
    "const_pname_pol_3052" -> "个银传统分红3052等产品保单信息生成",
    "const_pname_pol_1253" -> "个银传统分红1253等产品保单信息生成",
    "const_pname_pol_651" -> "个银传统分红651等产品保单信息生成",
    "const_pname_pol_main" -> "个银传统分红产品保单信息生成",
    "const_pname_pol_goat" -> "山羊计划保单信息生成",
    "const_pname_pol_pua" -> "个银传统分红红利交清增额保单信息生成",
    "const_pname_pol_lbs" -> "GBS转LBS保单信息生成",
    "const_pname_pol_ulink" -> "投连保单信息生成（不含885和878）",
    "const_pname_pol_universal" -> "万能保单信息生成",
    "const_pname_pol_top" -> "万能追加保险费信息生成",
    "const_pname_pol_885_878" -> "885和878保单信息生成",
    "const_pname_pol_guar" -> "保证续保保单信息生成",
    "const_pname_pol_irr" -> "投资合同应收净保费信息生成",
    "const_pname_pol_ann" -> "个险年金产品保单信息生成",
    "const_pname_pol_short" -> "短险保单信息生成",
    "const_pname_univ_get_pol_" -> "抓取万能保单账户价值",
    "const_pnam_univ_get_chg_pol_va" -> "抓取万能保单变更的账户价值",
    "const_pname_univ_calc_pol_" -> "计算万能保单最终的账户价值",
    "const_pname_univ_get_curr_act" -> "抓取万能保单信息",
    "const_pname_univ_acct_rider" -> "万能主险保单信息下载",
    "const_pname_univ_pol_892" -> "汇总万能初始账户价值并将附加重疾险的信息合并到主险保单信息中",
    "const_pname_univ_pol_2007" -> "万能2007险种保单信息下载",
    "const_pname_univ_pol_844" -> "万能844险种保单信息下载",
    "const_pname_pol_actuary_synch" -> "pol_ben_actuary自动更新",
    //险种配置-文件夹
    "const_folder_goat" -> "GOAT",
    "const_folder_main" -> "POL_BEN_MAIN",
    //"const_folder_pua" ->"PUA",
    "const_folder_lbs" -> "TRA",
    "const_folder_ulink" -> "ULINK",
    "const_folder_universal" -> "UNIVERSAL",
    "const_folder_top" -> "TOP",
    "const_folder_885_878" -> "885_878",
    "const_folder_guar" -> "GUAR",
    "const_folder_irr" -> "IRR",
    "const_folder_ann" -> "ANN",
    "const_folder_short" -> "SHORT",
    "const_folder_651" -> "651",
    //险种配置-文件
    "const_file_goat1" -> "POL_BEN_GOAT1",
    "const_file_goat2" -> "POL_BEN_GOAT2",
    "const_file_main_773" -> "EXEC773",
    "const_file_main_783" -> "EXEC783",
    "const_file_main_905" -> "EXEC905",
    "const_file_main_921" -> "EXEC921",
    "const_file_main_polben" -> "POL_BEN",
    "const_file_main_834" -> "POL_BEN_834",
    "const_file_main_916" -> "POL_BEN_916",
    "const_file_main_920" -> "POL_BEN_920",
    "const_file_main_940" -> "POL_BEN_940",
    "const_file_main_990" -> "POL_BEN_990",
    "const_file_main_991" -> "POL_BEN_991",
    "const_file_main_996" -> "POL_BEN_996",
    "const_file_main_1101" -> "POL_BEN_1101",
    "const_file_main_2005" -> "POL_BEN_2005",
    "const_file_main_1142" -> "POL_BEN_1142",
    "const_file_main_hi" -> "POL_H_I",
    "const_file_main_hw" -> "POL_H_W",
    "const_file_main_791" -> "POL_BEN_791",
    "const_file_main_3052" -> "POL_BEN_3052",
    "const_file_main_1253" -> "POL_BEN_1253",
    "const_file_main_651" -> "POL_BEN_651",
    //"const_file_pua" ->"POL_BEN_PUA",
    "const_file_lbs" -> "POL_BEN_TRA(LBS)",
    "const_file_ulink" -> "POL_ULINK",
    "const_file_univ_892" -> "EXEC892",
    "const_file_univ_2007" -> "EXEC2007",
    "const_file_univ_844" -> "EXEC844",
    "const_file_top" -> "POL_BEN_TOP",
    "const_file_885_878" -> "POL_885_878",
    "const_file_guar" -> "POL_BEN_SHORT",
    "const_file_irr" -> "POL_IRR",
    "const_file_ann" -> "POL_BEN_ANN",
    "const_file_short" -> "POL_BEN_SHORT",
    //异常处理常量,
    "const_log_type_warn" -> "warn",
    "const_log_type_error" -> "error",
    "const_log_type_audit" -> "audit"
  )


  //个银传统红利交清增额保单
  val const_ind_pol_pua = "PUA"
  //万能追加保险费信息
  val const_uv_pol_basic = "BASIC"
  //万能基本保单信息
  val const_uv_pol_top = "TOP"
  //机构库常量
  val const_lbs_brch_bj = "BJ"
  //处理过程常量
  val const_proc_pol_2005 = "pol_ben_main-01-2005"
  val const_proc_pol_773 = "pol_ben_main-02-773"
  val const_proc_pol_783 = "pol_ben_main-03-783"
  val const_proc_pol_905 = "pol_ben_main-04-905"
  val const_proc_pol_921 = "pol_ben_main-05-921"
  val const_proc_pol_834 = "pol_ben_main-06-834"
  val const_proc_pol_916 = "pol_ben_main-07-916"
  val const_proc_pol_920 = "pol_ben_main-08-920"
  val const_proc_pol_940 = "pol_ben_main-09-940"
  val const_proc_pol_990 = "pol_ben_main-10-990"
  val const_proc_pol_991 = "pol_ben_main-11-991"
  val const_proc_pol_996 = "pol_ben_main-12-996"
  val const_proc_pol_1101 = "pol_ben_main-13-1101"
  val const_proc_pol_hw = "pol_ben_main-14-760-76701W"
  val const_proc_pol_hi = "pol_ben_main-15-760-76701"
  val const_proc_pol_all = "pol_ben_main-16-main_comm"
  val const_proc_pol_1142 = "pol_ben_main-17-1142"
  val const_proc_pol_791 = "pol_ben_main-18-791"
  val const_proc_pol_3052 = "pol_ben_main-19-3052"
  val const_proc_pol_1253 = "pol_ben_main-20-1253"
  val const_proc_pol_651 = "pol_ben_main-21-651"
  val const_proc_pol_main = "pol_ben_main"
  val const_proc_pol_goat = "goat"
  val const_proc_pol_pua = "pua"
  val const_proc_pol_lbs = "tra"
  val const_proc_pol_ulink = "ulink"
  val const_proc_pol_universal = "universal"
  val const_proc_pol_top = "top"
  val const_proc_pol_885_878 = "885_878"
  val const_proc_pol_guar = "guar"
  val const_proc_pol_irr = "irr"
  val const_proc_pol_ann = "ann"
  val const_proc_pol_short = "short"
  val const_proc_univ_get_pol_val = "universal-01-univ_get_polval"
  val const_proc_univ_get_chg_pol_va = "universal-02-univ_get_chg_polval"
  val const_proc_univ_calc_pol_val = "universal-03-univ_calc_last_polval"
  val const_proc_univ_get_curr_act = "universal-04-univ_get_curr_act"
  val const_proc_univ_acct_rider = "universal-05-univ_acct_rider"
  val const_proc_univ_pol_892 = "universal-06-univ_pol_892"
  val const_proc_univ_pol_2007 = "universal-07-univ_pol_2007"
  val const_proc_univ_pol_844 = "universal-08-univ_pol_844"
  val const_proc_pol_actuary_synch = "auto_update_pol_ben_actuary"
  //处理过程名
  val const_pname_pol_2005 = "个银传统分红2005等产品保单信息生成"
  val const_pname_pol_1142 = "个银传统分红1142等产品保单信息生成"
  val const_pname_pol_783 = "个银传统分红783等产品保单信息生成"
  val const_pname_pol_905 = "个银传统分红905等产品保单信息生成"
  val const_pname_pol_921 = "个银传统分红921等产品保单信息生成"
  val const_pname_pol_all = "个银传统分红主要产品保单信息生成"
  val const_pname_pol_834 = "个银传统分红834等产品保单信息生成"
  val const_pname_pol_916 = "个银传统分红916等产品保单信息生成"
  val const_pname_pol_920 = "个银传统分红920等产品保单信息生成"
  val const_pname_pol_940 = "个银传统分红940等产品保单信息生成"
  val const_pname_pol_990 = "个银传统分红990等产品保单信息生成"
  val const_pname_pol_991 = "个银传统分红991等产品保单信息生成"
  val const_pname_pol_996 = "个银传统分红996等产品保单信息生成"
  val const_pname_pol_1101 = "个银传统分红1101等产品保单信息生成"
  val const_pname_pol_773 = "个银传统分红773等产品保单信息生成"
  val const_pname_pol_hi = "个银传统分红760-76701等产品普通状态保单信息生成"
  val const_pname_pol_hw = "个银传统分红760-76701等产品W状态保单信息生成"
  val const_pname_pol_791 = "个银传统分红791等产品W状态保单信息生成"
  val const_pname_pol_3052 = "个银传统分红3052等产品保单信息生成"
  val const_pname_pol_1253 = "个银传统分红1253等产品保单信息生成"
  val const_pname_pol_651 = "个银传统分红651等产品保单信息生成"
  val const_pname_pol_main = "个银传统分红产品保单信息生成"
  val const_pname_pol_goat = "山羊计划保单信息生成"
  val const_pname_pol_pua = "个银传统分红红利交清增额保单信息生成"
  val const_pname_pol_lbs = "GBS转LBS保单信息生成"
  val const_pname_pol_ulink = "投连保单信息生成（不含885和878）"
  val const_pname_pol_universal = "万能保单信息生成"
  val const_pname_pol_top = "万能追加保险费信息生成"
  val const_pname_pol_885_878 = "885和878保单信息生成"
  val const_pname_pol_guar = "保证续保保单信息生成"
  val const_pname_pol_irr = "投资合同应收净保费信息生成"
  val const_pname_pol_ann = "个险年金产品保单信息生成"
  val const_pname_pol_short = "短险保单信息生成"
  val const_pname_univ_get_pol_val = "抓取万能保单账户价值"
  val const_pnam_univ_get_chg_pol_va = "抓取万能保单变更的账户价值"
  val const_pname_univ_calc_pol_val = "计算万能保单最终的账户价值"
  val const_pname_univ_get_curr_act = "抓取万能保单信息"
  val const_pname_univ_acct_rider = "万能主险保单信息下载"
  val const_pname_univ_pol_892 = "汇总万能初始账户价值并将附加重疾险的信息合并到主险保单信息中"
  val const_pname_univ_pol_2007 = "万能2007险种保单信息下载"
  val const_pname_univ_pol_844 = "万能844险种保单信息下载"
  val const_pname_pol_actuary_synch = "pol_ben_actuary自动更新"
  //险种配置-文件夹
  val const_folder_goat = "GOAT"
  val const_folder_main = "POL_BEN_MAIN"
  //const_folder_pua  ="PUA"
  val const_folder_lbs = "TRA"
  val const_folder_ulink = "ULINK"
  val const_folder_universal = "UNIVERSAL"
  val const_folder_top = "TOP"
  val const_folder_885_878 = "885_878"
  val const_folder_guar = "GUAR"
  val const_folder_irr = "IRR"
  val const_folder_ann = "ANN"
  val const_folder_short = "SHORT"
  val const_folder_651 = "651"
  //险种配置-文件
  val const_file_goat1 = "POL_BEN_GOAT1"
  val const_file_goat2 = "POL_BEN_GOAT2"
  val const_file_main_773 = "EXEC773"
  val const_file_main_783 = "EXEC783"
  val const_file_main_905 = "EXEC905"
  val const_file_main_921 = "EXEC921"
  val const_file_main_polben = "POL_BEN"
  val const_file_main_834 = "POL_BEN_834"
  val const_file_main_916 = "POL_BEN_916"
  val const_file_main_920 = "POL_BEN_920"
  val const_file_main_940 = "POL_BEN_940"
  val const_file_main_990 = "POL_BEN_990"
  val const_file_main_991 = "POL_BEN_991"
  val const_file_main_996 = "POL_BEN_996"
  val const_file_main_1101 = "POL_BEN_1101"
  val const_file_main_2005 = "POL_BEN_2005"
  val const_file_main_1142 = "POL_BEN_1142"
  val const_file_main_hi = "POL_H_I"
  val const_file_main_hw = "POL_H_W"
  val const_file_main_791 = "POL_BEN_791"
  val const_file_main_3052 = "POL_BEN_3052"
  val const_file_main_1253 = "POL_BEN_1253"
  val const_file_main_651 = "POL_BEN_651"
  //const_file_pua="POL_BEN_PUA"
  val const_file_lbs = "POL_BEN_TRA(LBS)"
  val const_file_ulink = "POL_ULINK"
  val const_file_univ_892 = "EXEC892"
  val const_file_univ_2007 = "EXEC2007"
  val const_file_univ_844 = "EXEC844"
  val const_file_top = "POL_BEN_TOP"
  val const_file_885_878 = "POL_885_878"
  val const_file_guar = "POL_BEN_SHORT"
  val const_file_irr = "POL_IRR"
  val const_file_ann = "POL_BEN_ANN"
  val const_file_short = "POL_BEN_SHORT"
  //异常处理常量
  val const_log_type_warn = "warn"
  val const_log_type_error = "error"
  val const_log_type_audit = "audit"

}package com.an.lcloud.act.core.framework.comm

/**
  * Created by ZOUBO162 on 2018-11-1.
  * MP常量类
  */
object MPConstants {
  val IND_773_FILEPATH = "/mp/ind2/773.xml"
  val IND_905_FILEAPTH = "/mp/ind2/905.xml"
  val IND_BASE_XML_FILEPATH = "/mp/ind2/base.xml"
  val IND_HW_FILEPATH ="/mp/ind2/hw.xml"
  val IND_HI_FILEPATH ="/mp/ind2/hi.xml"
  val IND_ALL_FILEPATH ="/mp/ind2/all.xml"
  val IND_1142_FILEPATH ="/mp/ind2/1142.xml"
  val IND_3052_FILEPATH ="/mp/ind2/3052.xml"
  val IND_783_FILEPATH = "/mp/ind/783.xml"
  val ANN_FILEPATH = "/mp/ann/ann.xml"
  val GBS_FILEPATH = "/mp/gbs/gbs.xml"
  val PUA_FILEPATH = "/mp/pua/pua.xml"
  val IRR_FILEPATH = "/mp/irr/irr.xml"
  val GUAR_FILEPATH="/mp/guar/guar.xml"
  val SHORT_FILEPATH="/mp/short/short.xml"
  val TOP_FILEPATH = "/mp/top/top.xml"
  val UV_FILEPATH = "/mp/uv/uv.xml"
  val UV_TEMP_FILEPATH = "/mp/uv/uv_temp.xml"
  val UV_844_FILEPATH = "/mp/uv/uv844.xml"
  val UV_2007_FILEPATH = "/mp/uv/uv2007.xml"
  val UV_892_FILEPATH = "/mp/uv/uv892.xml"
  val UV_UL_RIDER_FILEPATH = "/mp/uv/univUlinkRider.xml"
  val BASETABLE_FILEPATH="/mp/guar/baseTable.xml"
  val ELSE_BASE_XML_FILEPATH = "/mp/guar/baseTable.xml"
  val ELSE_PRELOAD_XML_FILEPATH = "/mp/preload/preLoad.xml"
  val MP_PRETREATMENT_XML_PATH="/pretreatment/pretreatment.xml"
  val UL_FILEPATH = "/mp/ul/ul.xml"
  val ULTemp_FILEPATH = "/mp/ul/ul_temp.xml"
  val IND_921_FILEPATH="/mp/ind2/921.xml"
  val ULTemp2_FILEPATH = "/mp/ul/ul_temp_2.xml"
  val Else651_FILEPATH = "/mp/651/651.xml"
  val map = Map(

    //个银传统红利交清增额保单
    "const_ind_pol_pua" -> "PUA",
    //万能追加保险费信息
    "const_uv_pol_basic" -> "BASIC",
    //万能基本保单信息
    "const_uv_pol_top" -> "TOP",
    //机构库常量
    "const_lbs_brch_bj" -> "BJ",
    //处理过程常量
    "const_proc_pol_2005" -> "pol_ben_main-01-2005",
    "const_proc_pol_773" -> "pol_ben_main-02-773",
    "const_proc_pol_783" -> "pol_ben_main-03-783",
    "const_proc_pol_905" -> "pol_ben_main-04-905",
    "const_proc_pol_921" -> "pol_ben_main-05-921",
    "const_proc_pol_834" -> "pol_ben_main-06-834",
    "const_proc_pol_916" -> "pol_ben_main-07-916",
    "const_proc_pol_920" -> "pol_ben_main-08-920",
    "const_proc_pol_940" -> "pol_ben_main-09-940",
    "const_proc_pol_990" -> "pol_ben_main-10-990",
    "const_proc_pol_991" -> "pol_ben_main-11-991",
    "const_proc_pol_996" -> "pol_ben_main-12-996",
    "const_proc_pol_1101" -> "pol_ben_main-13-1101",
    "const_proc_pol_hw" -> "pol_ben_main-14-760-76701W",
    "const_proc_pol_hi" -> "pol_ben_main-15-760-76701",
    "const_proc_pol_all" -> "pol_ben_main-16-main_comm",
    "const_proc_pol_1142" -> "pol_ben_main-17-1142",
    "const_proc_pol_791" -> "pol_ben_main-18-791",
    "const_proc_pol_3052" -> "pol_ben_main-19-3052",
    "const_proc_pol_1253" -> "pol_ben_main-20-1253",
    "const_proc_pol_651" -> "pol_ben_main-21-651",
    "const_proc_pol_main" -> "pol_ben_main",
    "const_proc_pol_goat" -> "goat",
    "const_proc_pol_pua" -> "pua",
    "const_proc_pol_lbs" -> "tra",
    "const_proc_pol_ulink" -> "ulink",
    "const_proc_pol_universal" -> "universal",
    "const_proc_pol_top" -> "top",
    "const_proc_pol_885_878" -> "885_878",
    "const_proc_pol_guar" -> "guar",
    "const_proc_pol_irr" -> "irr",
    "const_proc_pol_ann" -> "ann",
    "const_proc_pol_short" -> "short",
    "const_proc_univ_get_pol_" -> "universal-01-univ_get_pol",
    "const_proc_univ_get_chg_pol_va" -> "universal-02-univ_get_chg_pol",
    "const_proc_univ_calc_pol_" -> "universal-03-univ_calc_last_pol",
    "const_proc_univ_get_curr_act" -> "universal-04-univ_get_curr_act",
    "const_proc_univ_acct_rider" -> "universal-05-univ_acct_rider",
    "const_proc_univ_pol_892" -> "universal-06-univ_pol_892",
    "const_proc_univ_pol_2007" -> "universal-07-univ_pol_2007",
    "const_proc_univ_pol_844" -> "universal-08-univ_pol_844",
    "const_proc_pol_actuary_synch" -> "auto_update_pol_ben_actuary",
    //处理过程名
    "const_pname_pol_2005" -> "个银传统分红2005等产品保单信息生成",
    "const_pname_pol_1142" -> "个银传统分红1142等产品保单信息生成",
    "const_pname_pol_783" -> "个银传统分红783等产品保单信息生成",
    "const_pname_pol_905" -> "个银传统分红905等产品保单信息生成",
    "const_pname_pol_921" -> "个银传统分红921等产品保单信息生成",
    "const_pname_pol_all" -> "个银传统分红主要产品保单信息生成",
    "const_pname_pol_834" -> "个银传统分红834等产品保单信息生成",
    "const_pname_pol_916" -> "个银传统分红916等产品保单信息生成",
    "const_pname_pol_920" -> "个银传统分红920等产品保单信息生成",
    "const_pname_pol_940" -> "个银传统分红940等产品保单信息生成",
    "const_pname_pol_990" -> "个银传统分红990等产品保单信息生成",
    "const_pname_pol_991" -> "个银传统分红991等产品保单信息生成",
    "const_pname_pol_996" -> "个银传统分红996等产品保单信息生成",
    "const_pname_pol_1101" -> "个银传统分红1101等产品保单信息生成",
    "const_pname_pol_773" -> "个银传统分红773等产品保单信息生成",
    "const_pname_pol_hi" -> "个银传统分红760-76701等产品普通状态保单信息生成",
    "const_pname_pol_hw" -> "个银传统分红760-76701等产品W状态保单信息生成",
    "const_pname_pol_791" -> "个银传统分红791等产品W状态保单信息生成",
    "const_pname_pol_3052" -> "个银传统分红3052等产品保单信息生成",
    "const_pname_pol_1253" -> "个银传统分红1253等产品保单信息生成",
    "const_pname_pol_651" -> "个银传统分红651等产品保单信息生成",
    "const_pname_pol_main" -> "个银传统分红产品保单信息生成",
    "const_pname_pol_goat" -> "山羊计划保单信息生成",
    "const_pname_pol_pua" -> "个银传统分红红利交清增额保单信息生成",
    "const_pname_pol_lbs" -> "GBS转LBS保单信息生成",
    "const_pname_pol_ulink" -> "投连保单信息生成（不含885和878）",
    "const_pname_pol_universal" -> "万能保单信息生成",
    "const_pname_pol_top" -> "万能追加保险费信息生成",
    "const_pname_pol_885_878" -> "885和878保单信息生成",
    "const_pname_pol_guar" -> "保证续保保单信息生成",
    "const_pname_pol_irr" -> "投资合同应收净保费信息生成",
    "const_pname_pol_ann" -> "个险年金产品保单信息生成",
    "const_pname_pol_short" -> "短险保单信息生成",
    "const_pname_univ_get_pol_" -> "抓取万能保单账户价值",
    "const_pnam_univ_get_chg_pol_va" -> "抓取万能保单变更的账户价值",
    "const_pname_univ_calc_pol_" -> "计算万能保单最终的账户价值",
    "const_pname_univ_get_curr_act" -> "抓取万能保单信息",
    "const_pname_univ_acct_rider" -> "万能主险保单信息下载",
    "const_pname_univ_pol_892" -> "汇总万能初始账户价值并将附加重疾险的信息合并到主险保单信息中",
    "const_pname_univ_pol_2007" -> "万能2007险种保单信息下载",
    "const_pname_univ_pol_844" -> "万能844险种保单信息下载",
    "const_pname_pol_actuary_synch" -> "pol_ben_actuary自动更新",
    //险种配置-文件夹
    "const_folder_goat" -> "GOAT",
    "const_folder_main" -> "POL_BEN_MAIN",
    //"const_folder_pua" ->"PUA",
    "const_folder_lbs" -> "TRA",
    "const_folder_ulink" -> "ULINK",
    "const_folder_universal" -> "UNIVERSAL",
    "const_folder_top" -> "TOP",
    "const_folder_885_878" -> "885_878",
    "const_folder_guar" -> "GUAR",
    "const_folder_irr" -> "IRR",
    "const_folder_ann" -> "ANN",
    "const_folder_short" -> "SHORT",
    "const_folder_651" -> "651",
    //险种配置-文件
    "const_file_goat1" -> "POL_BEN_GOAT1",
    "const_file_goat2" -> "POL_BEN_GOAT2",
    "const_file_main_773" -> "EXEC773",
    "const_file_main_783" -> "EXEC783",
    "const_file_main_905" -> "EXEC905",
    "const_file_main_921" -> "EXEC921",
    "const_file_main_polben" -> "POL_BEN",
    "const_file_main_834" -> "POL_BEN_834",
    "const_file_main_916" -> "POL_BEN_916",
    "const_file_main_920" -> "POL_BEN_920",
    "const_file_main_940" -> "POL_BEN_940",
    "const_file_main_990" -> "POL_BEN_990",
    "const_file_main_991" -> "POL_BEN_991",
    "const_file_main_996" -> "POL_BEN_996",
    "const_file_main_1101" -> "POL_BEN_1101",
    "const_file_main_2005" -> "POL_BEN_2005",
    "const_file_main_1142" -> "POL_BEN_1142",
    "const_file_main_hi" -> "POL_H_I",
    "const_file_main_hw" -> "POL_H_W",
    "const_file_main_791" -> "POL_BEN_791",
    "const_file_main_3052" -> "POL_BEN_3052",
    "const_file_main_1253" -> "POL_BEN_1253",
    "const_file_main_651" -> "POL_BEN_651",
    //"const_file_pua" ->"POL_BEN_PUA",
    "const_file_lbs" -> "POL_BEN_TRA(LBS)",
    "const_file_ulink" -> "POL_ULINK",
    "const_file_univ_892" -> "EXEC892",
    "const_file_univ_2007" -> "EXEC2007",
    "const_file_univ_844" -> "EXEC844",
    "const_file_top" -> "POL_BEN_TOP",
    "const_file_885_878" -> "POL_885_878",
    "const_file_guar" -> "POL_BEN_SHORT",
    "const_file_irr" -> "POL_IRR",
    "const_file_ann" -> "POL_BEN_ANN",
    "const_file_short" -> "POL_BEN_SHORT",
    //异常处理常量,
    "const_log_type_warn" -> "warn",
    "const_log_type_error" -> "error",
    "const_log_type_audit" -> "audit"
  )


  //个银传统红利交清增额保单
  val const_ind_pol_pua = "PUA"
  //万能追加保险费信息
  val const_uv_pol_basic = "BASIC"
  //万能基本保单信息
  val const_uv_pol_top = "TOP"
  //机构库常量
  val const_lbs_brch_bj = "BJ"
  //处理过程常量
  val const_proc_pol_2005 = "pol_ben_main-01-2005"
  val const_proc_pol_773 = "pol_ben_main-02-773"
  val const_proc_pol_783 = "pol_ben_main-03-783"
  val const_proc_pol_905 = "pol_ben_main-04-905"
  val const_proc_pol_921 = "pol_ben_main-05-921"
  val const_proc_pol_834 = "pol_ben_main-06-834"
  val const_proc_pol_916 = "pol_ben_main-07-916"
  val const_proc_pol_920 = "pol_ben_main-08-920"
  val const_proc_pol_940 = "pol_ben_main-09-940"
  val const_proc_pol_990 = "pol_ben_main-10-990"
  val const_proc_pol_991 = "pol_ben_main-11-991"
  val const_proc_pol_996 = "pol_ben_main-12-996"
  val const_proc_pol_1101 = "pol_ben_main-13-1101"
  val const_proc_pol_hw = "pol_ben_main-14-760-76701W"
  val const_proc_pol_hi = "pol_ben_main-15-760-76701"
  val const_proc_pol_all = "pol_ben_main-16-main_comm"
  val const_proc_pol_1142 = "pol_ben_main-17-1142"
  val const_proc_pol_791 = "pol_ben_main-18-791"
  val const_proc_pol_3052 = "pol_ben_main-19-3052"
  val const_proc_pol_1253 = "pol_ben_main-20-1253"
  val const_proc_pol_651 = "pol_ben_main-21-651"
  val const_proc_pol_main = "pol_ben_main"
  val const_proc_pol_goat = "goat"
  val const_proc_pol_pua = "pua"
  val const_proc_pol_lbs = "tra"
  val const_proc_pol_ulink = "ulink"
  val const_proc_pol_universal = "universal"
  val const_proc_pol_top = "top"
  val const_proc_pol_885_878 = "885_878"
  val const_proc_pol_guar = "guar"
  val const_proc_pol_irr = "irr"
  val const_proc_pol_ann = "ann"
  val const_proc_pol_short = "short"
  val const_proc_univ_get_pol_val = "universal-01-univ_get_polval"
  val const_proc_univ_get_chg_pol_va = "universal-02-univ_get_chg_polval"
  val const_proc_univ_calc_pol_val = "universal-03-univ_calc_last_polval"
  val const_proc_univ_get_curr_act = "universal-04-univ_get_curr_act"
  val const_proc_univ_acct_rider = "universal-05-univ_acct_rider"
  val const_proc_univ_pol_892 = "universal-06-univ_pol_892"
  val const_proc_univ_pol_2007 = "universal-07-univ_pol_2007"
  val const_proc_univ_pol_844 = "universal-08-univ_pol_844"
  val const_proc_pol_actuary_synch = "auto_update_pol_ben_actuary"
  //处理过程名
  val const_pname_pol_2005 = "个银传统分红2005等产品保单信息生成"
  val const_pname_pol_1142 = "个银传统分红1142等产品保单信息生成"
  val const_pname_pol_783 = "个银传统分红783等产品保单信息生成"
  val const_pname_pol_905 = "个银传统分红905等产品保单信息生成"
  val const_pname_pol_921 = "个银传统分红921等产品保单信息生成"
  val const_pname_pol_all = "个银传统分红主要产品保单信息生成"
  val const_pname_pol_834 = "个银传统分红834等产品保单信息生成"
  val const_pname_pol_916 = "个银传统分红916等产品保单信息生成"
  val const_pname_pol_920 = "个银传统分红920等产品保单信息生成"
  val const_pname_pol_940 = "个银传统分红940等产品保单信息生成"
  val const_pname_pol_990 = "个银传统分红990等产品保单信息生成"
  val const_pname_pol_991 = "个银传统分红991等产品保单信息生成"
  val const_pname_pol_996 = "个银传统分红996等产品保单信息生成"
  val const_pname_pol_1101 = "个银传统分红1101等产品保单信息生成"
  val const_pname_pol_773 = "个银传统分红773等产品保单信息生成"
  val const_pname_pol_hi = "个银传统分红760-76701等产品普通状态保单信息生成"
  val const_pname_pol_hw = "个银传统分红760-76701等产品W状态保单信息生成"
  val const_pname_pol_791 = "个银传统分红791等产品W状态保单信息生成"
  val const_pname_pol_3052 = "个银传统分红3052等产品保单信息生成"
  val const_pname_pol_1253 = "个银传统分红1253等产品保单信息生成"
  val const_pname_pol_651 = "个银传统分红651等产品保单信息生成"
  val const_pname_pol_main = "个银传统分红产品保单信息生成"
  val const_pname_pol_goat = "山羊计划保单信息生成"
  val const_pname_pol_pua = "个银传统分红红利交清增额保单信息生成"
  val const_pname_pol_lbs = "GBS转LBS保单信息生成"
  val const_pname_pol_ulink = "投连保单信息生成（不含885和878）"
  val const_pname_pol_universal = "万能保单信息生成"
  val const_pname_pol_top = "万能追加保险费信息生成"
  val const_pname_pol_885_878 = "885和878保单信息生成"
  val const_pname_pol_guar = "保证续保保单信息生成"
  val const_pname_pol_irr = "投资合同应收净保费信息生成"
  val const_pname_pol_ann = "个险年金产品保单信息生成"
  val const_pname_pol_short = "短险保单信息生成"
  val const_pname_univ_get_pol_val = "抓取万能保单账户价值"
  val const_pnam_univ_get_chg_pol_va = "抓取万能保单变更的账户价值"
  val const_pname_univ_calc_pol_val = "计算万能保单最终的账户价值"
  val const_pname_univ_get_curr_act = "抓取万能保单信息"
  val const_pname_univ_acct_rider = "万能主险保单信息下载"
  val const_pname_univ_pol_892 = "汇总万能初始账户价值并将附加重疾险的信息合并到主险保单信息中"
  val const_pname_univ_pol_2007 = "万能2007险种保单信息下载"
  val const_pname_univ_pol_844 = "万能844险种保单信息下载"
  val const_pname_pol_actuary_synch = "pol_ben_actuary自动更新"
  //险种配置-文件夹
  val const_folder_goat = "GOAT"
  val const_folder_main = "POL_BEN_MAIN"
  //const_folder_pua  ="PUA"
  val const_folder_lbs = "TRA"
  val const_folder_ulink = "ULINK"
  val const_folder_universal = "UNIVERSAL"
  val const_folder_top = "TOP"
  val const_folder_885_878 = "885_878"
  val const_folder_guar = "GUAR"
  val const_folder_irr = "IRR"
  val const_folder_ann = "ANN"
  val const_folder_short = "SHORT"
  val const_folder_651 = "651"
  //险种配置-文件
  val const_file_goat1 = "POL_BEN_GOAT1"
  val const_file_goat2 = "POL_BEN_GOAT2"
  val const_file_main_773 = "EXEC773"
  val const_file_main_783 = "EXEC783"
  val const_file_main_905 = "EXEC905"
  val const_file_main_921 = "EXEC921"
  val const_file_main_polben = "POL_BEN"
  val const_file_main_834 = "POL_BEN_834"
  val const_file_main_916 = "POL_BEN_916"
  val const_file_main_920 = "POL_BEN_920"
  val const_file_main_940 = "POL_BEN_940"
  val const_file_main_990 = "POL_BEN_990"
  val const_file_main_991 = "POL_BEN_991"
  val const_file_main_996 = "POL_BEN_996"
  val const_file_main_1101 = "POL_BEN_1101"
  val const_file_main_2005 = "POL_BEN_2005"
  val const_file_main_1142 = "POL_BEN_1142"
  val const_file_main_hi = "POL_H_I"
  val const_file_main_hw = "POL_H_W"
  val const_file_main_791 = "POL_BEN_791"
  val const_file_main_3052 = "POL_BEN_3052"
  val const_file_main_1253 = "POL_BEN_1253"
  val const_file_main_651 = "POL_BEN_651"
  //const_file_pua="POL_BEN_PUA"
  val const_file_lbs = "POL_BEN_TRA(LBS)"
  val const_file_ulink = "POL_ULINK"
  val const_file_univ_892 = "EXEC892"
  val const_file_univ_2007 = "EXEC2007"
  val const_file_univ_844 = "EXEC844"
  val const_file_top = "POL_BEN_TOP"
  val const_file_885_878 = "POL_885_878"
  val const_file_guar = "POL_BEN_SHORT"
  val const_file_irr = "POL_IRR"
  val const_file_ann = "POL_BEN_ANN"
  val const_file_short = "POL_BEN_SHORT"
  //异常处理常量
  val const_log_type_warn = "warn"
  val const_log_type_error = "error"
  val const_log_type_audit = "audit"

}

package com.an.lcloud.act.core.framework.db

import java.sql.{Connection, DriverManager}
import java.util.{LinkedList, ResourceBundle}

import org.apache.spark.Logging

/**
  * 数据库连接池工具类
  * Created by EX-ZHANGYONGTIAN001 on 2018-12-18.
  */
object DBConnectionPool extends Logging{
  private val reader = ResourceBundle.getBundle("jdbc/version_manager_jdbc")
  private val max_connection = reader.getString("max_connection") //连接池总数
  private val connection_num = reader.getString("connection_num") //产生连接数
  private var current_num = 0 //当前连接池已产生的连接数
  private val pools = new LinkedList[Connection]() //连接池
  private val driver = reader.getString("driver")
  private val url = reader.getString("url")
  private val username = reader.getString("username")
  private val password = reader.getString("password")
  /**
    * 加载驱动
    */
  private def before() {
    if (current_num > max_connection.toInt && pools.isEmpty()) {
      println("busyness")
      Thread.sleep(2000)
      before()
    } else {
      Class.forName(driver)
    }
  }
  /**
    * 获得连接
    */
  private def initConn(): Connection = {
    val conn = DriverManager.getConnection(url, username, password)
//    logError(url)
    conn
  }


  /**
    * 初始化连接池
    */
  private def initConnectionPool(): LinkedList[Connection] = {
    AnyRef.synchronized({
      if (pools.isEmpty()) {
        before()
        for (i <- 1 to connection_num.toInt) {
          pools.push(initConn())
          current_num += 1
        }
      }
      pools
    })
  }
  /**
    * 获得连接
    */
  def getConn():Connection={
    initConnectionPool()
    pools.poll()
  }
  /**
    * 释放连接
    */
  def releaseCon(con:Connection){
    pools.push(con)
  }


}

package com.an.lcloud.act.core.framework.db

import java.sql.{CallableStatement, Connection, PreparedStatement, ResultSet, SQLException, Statement, Types}

import com.an.lcloud.act.core.framework.comm.JobConstants
import com.an.lcloud.act.core.framework.job.VersionManager

import scala.collection.mutable.ListBuffer

/**
  *
  * @author zhangyongtian
  * @define
  * create 2018-03-29 14:45
  */
object JdbcHelper {

  private var conn: Connection = null
  private var preparedStatement: PreparedStatement = null
  private var callableStatement: CallableStatement = null


  /**
    * 建立数据库连接
    *
    * @return
    * @throws SQLException
    */
  @throws[SQLException]
  private def getConnection: Connection = {
    conn = DBConnectionPool.getConn()
    conn
  }

  /**
    * 释放连接
    *
    * @param conn
    */
  private def freeConnection(conn: Connection) = {
    try
      DBConnectionPool.releaseCon(conn)
    catch {
      case e: SQLException =>
        e.printStackTrace()
    }
  }

  /**
    * 释放statement
    *
    * @param statement
    */
  private def freeStatement(statement: Statement) = {
    try
      statement.close()
    catch {
      case e: SQLException =>
        e.printStackTrace()
    }
  }

  /**
    * 释放resultset
    *
    * @param rs
    */
  private def freeResultSet(rs: ResultSet) = {
    try
      rs.close()
    catch {
      case e: SQLException =>
        e.printStackTrace()
    }
  }


  /**
    * 释放资源
    *
    * @param conn
    * @param statement
    * @param rs
    */
  def free(conn: Connection, statement: Statement, rs: ResultSet): Unit = {
    if (rs != null) freeResultSet(rs)
    if (statement != null) freeStatement(statement)
    if (conn != null) freeConnection(conn)
  }

  /////////////////////////////////////////////////////////

  /**
    * 获取PreparedStatement
    *
    * @param sql
    * @throws SQLException
    */
  @throws[SQLException]
  private def getPreparedStatement(sql: String) = {
    conn = getConnection
    preparedStatement = conn.prepareStatement(sql)
  }

  /**
    * 用于查询，返回结果集
    *
    * @param sql
    * sql语句
    * @return 结果集
    * @throws SQLException
    */
  @throws[SQLException]
  def query(sql: String): List[Map[String, Object]] = {
    var rs: ResultSet = null
    try {
      getPreparedStatement(sql)
      rs = preparedStatement.executeQuery
      ResultToListMap(rs)
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    } finally free(conn, callableStatement, rs)
  }


  /**
    * 用于带参数的查询，返回结果集
    *
    * @param sql
    * sql语句
    * @param paramters
    * 参数集合
    * @return 结果集
    * @throws SQLException
    */
  @throws[SQLException]
  def query(sql: String, paramters: Any*): List[Map[String, Object]] = {
    var rs: ResultSet = null
    try {
      getPreparedStatement(sql)
      var i = 0
      while ( {
        i < paramters.length
      }) {
        preparedStatement.setObject(i + 1, paramters(i))

        {
          i += 1
          i - 1
        }
      }
      rs = preparedStatement.executeQuery
      ResultToListMap(rs)
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    } finally free(conn, callableStatement, rs)
  }


  /**
    * 返回单个结果的值，如count\min\max等等
    *
    * @param sql
    * sql语句
    * @return 结果集
    * @throws SQLException
    */
  @throws[SQLException]
  def getSingle(sql: String): Any = {
    var result: Any = null
    var rs: ResultSet = null
    try {
      getPreparedStatement(sql)
      rs = preparedStatement.executeQuery
      if (rs.next) result = rs.getObject(1)
      result
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    } finally free(conn, callableStatement, rs)
  }

  /**
    * 返回单个结果值，如count\min\max等
    *
    * @param sql
    * sql语句
    * @param paramters
    * 参数列表
    * @return 结果
    * @throws SQLException
    */
  @throws[SQLException]
  def getSingle(sql: String, paramters: Any*): Any = {
    var result: Any = null
    var rs: ResultSet = null
    try {
      getPreparedStatement(sql)
      var i = 0
      while ( {
        i < paramters.length
      }) {
        preparedStatement.setObject(i + 1, paramters(i))

        {
          i += 1
          i - 1
        }
      }
      rs = preparedStatement.executeQuery
      if (rs.next) result = rs.getObject(1)
      result
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    } finally free(conn, callableStatement, rs)
  }

  /**
    * 用于增删改
    *
    * @param sql
    * sql语句
    * @return 影响行数
    * @throws SQLException
    */
  @throws[SQLException]
  def update(sql: String): Int = try {
    getPreparedStatement(sql)
    preparedStatement.executeUpdate
  } catch {
    case e: SQLException =>
      throw new SQLException(e)
  } finally free(conn, callableStatement, null)

  /**
    * 用于增删改（带参数）
    *
    * @param sql
    * sql语句
    * @param paramters
    * sql语句
    * @return 影响行数
    * @throws SQLException
    */
  @throws[SQLException]
  def update(sql: String, paramters: Any*): Int = try {
    getPreparedStatement(sql)
    var i = 0
    while ( {
      i < paramters.length
    }) {
      preparedStatement.setObject(i + 1, paramters(i))

      {
        i += 1
        i - 1
      }
    }
    preparedStatement.executeUpdate
  } catch {
    case e: SQLException =>
      throw new SQLException(e)
  } finally free(conn, callableStatement, null)

  /**
    * 插入值后返回主键值
    *
    * @param sql
    * 插入sql语句
    * @return 返回结果
    * @throws Exception
    */
  @throws[SQLException]
  def insertWithReturnPrimeKey(sql: String): Any = {
    var rs: ResultSet = null
    var result: Object = null
    try {
      conn = getConnection
      preparedStatement = conn.prepareStatement(sql, Statement.RETURN_GENERATED_KEYS)
      preparedStatement.execute
      rs = preparedStatement.getGeneratedKeys
      if (rs.next) result = rs.getObject(1)
      result
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    }
  }

  /**
    * 插入值后返回主键值
    *
    * @param sql
    * 插入sql语句
    * @param paramters
    * 参数列表
    * @return 返回结果
    * @throws SQLException
    */
  @throws[SQLException]
  def insertWithReturnPrimeKey(sql: String, paramters: Any*): Any = {
    var rs: ResultSet = null
    var result: Object = null
    try {
      conn = getConnection
      preparedStatement = conn.prepareStatement(sql, Statement.RETURN_GENERATED_KEYS)
      var i = 0
      while ( {
        i < paramters.length
      }) {
        preparedStatement.setObject(i + 1, paramters(i))

        {
          i += 1
          i - 1
        }
      }
      preparedStatement.execute
      rs = preparedStatement.getGeneratedKeys
      if (rs.next) result = rs.getObject(1)
      result
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    }
  }

  //////////////////////////////////////////////////////////////////

  /**
    * 获取CallableStatement
    *
    * @param procedureSql
    * @throws SQLException
    */
  @throws[SQLException]
  private def getCallableStatement(procedureSql: String) = {
    conn = getConnection
    conn.prepareCall(procedureSql)
  }


  /**
    * 存储过程查询
    * 注意outNames和outOracleTypes的顺序要对应 顺序按存储过程的参数顺序排列
    *
    * @param procedureSql
    * @param ins            输入参数数组
    * @param outNames       输出参数名称
    * @param outOracleTypes 输出参数类型
    * @return
    *
    */
  @throws[SQLException]
  def callableQuery(procedureSql: String, ins: Array[Object], outNames: Array[String], outOracleTypes: Array[Int]): Map[String, Object] = {

    val listBuffer = new ListBuffer[Object]

    try {
      callableStatement = getCallableStatement(procedureSql)

      var count = 0

      for (i <- 0 until ins.length) {
        count = count + 1
        callableStatement.setObject(count, ins(i))
      }

      for (j <- 0 until outOracleTypes.length) {
        count = count + 1
        callableStatement.registerOutParameter(count, outOracleTypes(j))
      }

      callableStatement.execute()

      count = count - outOracleTypes.length
      for (i <- 0 until outOracleTypes.length) {
        count = count + 1
        listBuffer.append(callableStatement.getObject(count))
      }
      outNames.zip(listBuffer.toList).toMap
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    }
    finally free(conn, callableStatement, null)
  }


  /**
    * 调用存储过程，查询单个值
    *
    * @param procedureSql
    * @return
    * @throws SQLException
    */
  @throws[SQLException]
  def callableGetSingle(procedureSql: String): Any = {
    var result: Any = null
    var rs: ResultSet = null
    try {
      rs = getCallableStatement(procedureSql).executeQuery
      while ( {
        rs.next
      }) result = rs.getObject(1)
      result
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    } finally free(conn, callableStatement, rs)
  }

  /**
    * 调用存储过程(带参数)，查询单个值
    *
    * @param procedureSql
    * @return
    * @throws SQLException
    */
  @throws[SQLException]
  def callableGetSingle(procedureSql: String, paramters: Any*): Any = {
    var result: Any = null
    var rs: ResultSet = null
    try {
      callableStatement = getCallableStatement(procedureSql)
      var i = 0
      while ( {
        i < paramters.length
      }) {
        callableStatement.setObject(i + 1, paramters(i))

        {
          i += 1
          i - 1
        }
      }
      rs = callableStatement.executeQuery
      while ( {
        rs.next
      }) result = rs.getObject(1)
      result
    } catch {
      case e: SQLException =>
        throw new SQLException(e)
    } finally free(conn, callableStatement, rs)
  }

  @throws[SQLException]
  def callableWithParamters(procedureSql: String): Any = try {
    callableStatement = getCallableStatement(procedureSql)
    callableStatement.registerOutParameter(0, Types.OTHER)
    callableStatement.execute
    callableStatement.getObject(0)
  } catch {
    case e: SQLException =>
      throw new SQLException(e)
  } finally free(conn, callableStatement, null)

  /**
    * 调用存储过程，执行增删改
    *
    * @param procedureSql
    * 存储过程
    * @return 影响行数
    * @throws SQLException
    */
  @throws[SQLException]
  def callableUpdate(procedureSql: String): Int = try {
    callableStatement = getCallableStatement(procedureSql)
    callableStatement.executeUpdate
  } catch {
    case e: SQLException =>
      throw new SQLException(e)
  } finally free(conn, callableStatement, null)

  /**
    * 调用存储过程（带参数），执行增删改
    *
    * @param procedureSql
    * 存储过程
    * @param parameters
    * @return 影响行数
    * @throws SQLException
    */
  @throws[SQLException]
  def callableUpdate(procedureSql: String, parameters: Any*): Int = try {
    callableStatement = getCallableStatement(procedureSql)
    var i = 0
    while ( {
      i < parameters.length
    }) {
      callableStatement.setObject(i + 1, parameters(i))

      {
        i += 1
        i - 1
      }
    }
    callableStatement.executeUpdate
  } catch {
    case e: SQLException =>
      throw new SQLException(e)
  } finally free(conn, callableStatement, null)


  @throws[SQLException]
  private def ResultToListMap(rs: ResultSet) = {

    val list = new ListBuffer[Map[String, Object]]

    while (rs.next) {
      val map = new scala.collection.mutable.HashMap[String, Object]
      val md = rs.getMetaData
      for (i <- 1 until md.getColumnCount) {
        map.put(md.getColumnLabel(i), rs.getObject(i))
      }
      list.append(map.toMap)
    }
    list.toList
  }


  def main(args: Array[String]): Unit = {
    //        val list = query("select * from POL_BEN_PLAN_CFG")
    //        for (e <- list) {
    //          println(e)
    //        }

//    val startVersionMap = VersionManager.verStartIf("20181106", "spark_ifrs17_mp_ind_aio")
//    println(startVersionMap.toString())
//
//    val endVersionMap = VersionManager.verEndIf(startVersionMap, JobConstants.P_RUN_FLAG_SUCCESS, "success")
//    println(endVersionMap.toString())

  }

}
package com.an.lcloud.act.core.framework.job

import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{Logging, SparkContext}

/**
  * Created by EX-ZHANGYONGTIAN001 on 2018-11-1.
  */
abstract class BaseJob(@transient sc: SparkContext, @transient hiveContext: HiveContext, serviceConfMap: scala.collection.mutable.HashMap[String, String]) extends JobCommon with Logging with Serializable {

  val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

  def readXml(path: String) = {
    //读取xml获取参数
    //    val document = parse(s"/mp/${app.filter(_.isDigit)}.xml")
    val document = parse(path)
    serviceConfMap.++=(loadServiceConf(document))
  }

  //运行环境 （研发、测试、生产）
  val env = serviceConfMap.getOrElse("env", "local")

  def initPrd(): Unit = {
    val hive_db = serviceConfMap.getOrElse("dbName", "sx_core_safe")
    hiveContext.sql("use " + hive_db)
    //    hiveContext.sql("set hive.exec.dynamic.partition = true")
    //    hiveContext.sql("set hive.exec.dynamic.partition.mode = nonstrict")
  }

  def init(): Unit = {
    //初始化数据
    if (env == "prd") {
      initPrd()
    }
  }

  def run(): Unit


}
package com.an.lcloud.act.core.framework.job

import java.io.{IOException, Serializable}
import java.util
import java.util.Properties
import org.apache.spark.{Logging, SparkConf}
import org.dom4j.{Document, DocumentException, Element}
import org.dom4j.io.SAXReader
import org.xml.sax.SAXException

import scala.collection._
import scala.util.control.Breaks._

/**
  * Created by fubaoshui687 on 2017/8/16.
  */

@SerialVersionUID(32532532532525L)
class JobCommon extends Serializable with Logging {

  protected var serviceConfMap = new scala.collection.mutable.HashMap[String, String]

  /**
    * 创建document方法
    *
    * @param XmlPath 本地xml文件地址
    * @return 返回document
    */
  protected def parse(XmlPath: String): Document = {
    val reader = new SAXReader
    var document: Document = null
    try {
      reader.setFeature("http://apache.org/xml/features/disallow-doctype-decl", true)
      reader.setFeature("http://xml.org/sax/features/external-general-entities", false)
      reader.setFeature("http://xml.org/sax/features/external-parameter-entities", false)
      val xml = this.getClass.getResourceAsStream(XmlPath)
      document = reader.read(xml)
    } catch {
      case e: DocumentException =>
        log.error("document can not be created:" + e)
      case e: SAXException =>
        log.error("SAXException:" + e)
    }
    document
  }

  /**
    * @param document      dom4j 操作对象
    * @param serviceConfId service 标签id
    * @return map对象
    */
  protected def loadServiceConf(document: Document, serviceConfId: String): mutable.HashMap[String, String] = { //获取文档的根节点.
    val root = document.getRootElement
    //遍历子节点,找到对应的jobConf
    val iter = root.elementIterator
    breakable(
      while ( {
        iter.hasNext
      }) {
        val element = iter.next.asInstanceOf[Element]
        if (element.attribute("id").getText == serviceConfId) { //遍历对应子节点的属性，加载进jobConfMap中
          findElement(element, serviceConfMap)
          break //todo: break is not supported
        }
      }
    )
    serviceConfMap
  }

  /**
    * @param document dom4j 操作对象
    * @return map对象
    */
  protected def loadServiceConf(document: Document): mutable.HashMap[String, String] = { //获取文档的根节点.
    val root = document.getRootElement
    //遍历子节点,找到对应的jobConf
    val iter = root.elementIterator
    while ( {
      iter.hasNext
    }) {
      val element = iter.next.asInstanceOf[Element]
      serviceConfMap.put(element.getName(), element.getText());
    }
    serviceConfMap
  }

  /**
    * 递归查找element的值，插入map对象
    *
    * @param element 元素
    * @param confMap map对象
    */
  protected def findElement(element: Element, confMap: mutable.Map[String, String]): Unit = {
    val iterInner = element.elementIterator
    while ( {
      iterInner.hasNext
    }) {
      val stringBuffer = new StringBuffer
      val elementInner = iterInner.next.asInstanceOf[Element]
      stringBuffer.append(elementInner.getName)
      if (elementInner.elementIterator.hasNext) findElement(elementInner, confMap)
      serviceConfMap.put(elementInner.getName(), elementInner.getText());
      val value = elementInner.getText
      if (!(value.trim == "")) confMap.put(stringBuffer.toString, value.trim)
    }
  }

  /**
    * 加载properties进map的方法
    *
    * @param properties properties地址
    * @return map集合
    */
  protected def loadServiceProperties(properties: String): util.Map[String, String] = {
    val serviceConfMap = new util.HashMap[String, String]
    try {
      val env = new Properties
      env.load(this.getClass.getResourceAsStream(properties))
      import scala.collection.JavaConversions._
      for (keyObj <- env.keySet) {
        val key = keyObj.asInstanceOf[String]
        val value = env.getProperty(key)
        serviceConfMap.put(key, value)
      }
    } catch {
      case e: IOException =>
        log.error("load properties error!" + e)
    }
    serviceConfMap
  }

  /**
    *
    * @param sql      传入的sql
    * @param mapValue 占位符的hashmap集合
    * @return 转换后的sql
    */
  def selectMapping(sql: String, mapValue: scala.collection.mutable.HashMap[String, String]): String = {
    val iter = mapValue.iterator
    var replaceSql = ""
    var stringBuffer: StringBuffer = null
    var resultSql = ""
    try {
      stringBuffer = new StringBuffer(sql)
      while ( {
        iter.hasNext
      }) {
        val entry = iter.next
        val key = entry._1.trim
        val `val` = entry._2
        replaceSql = stringBuffer.toString.replace("${" + key + "}", `val`)
        stringBuffer.delete(0, stringBuffer.length)
        stringBuffer.append(replaceSql)
        resultSql = stringBuffer.toString
      }
    } catch {
      case e: Exception =>
        log.error("mapping error!" + e)
    }
    resultSql
  }
}
package com.an.lcloud.act.core.framework.job

import com.an.lcloud.act.core.framework.db.JdbcHelper.callableQuery
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable.ArrayBuffer

/**
  * Created by EX-ZHANGYONGTIAN001 on 2018-12-18.
  *
  */
object VersionManager {

  /**
    * p_proc_date        输入   数据日期
    * p_task_name        输入   任务名称
    * *
    * v_source_tbl       输出  源表名称，多个使用逗号分隔
    * v_source_ver       输出  源版本号，多个使用逗号分隔，为空时使用#代替
    * v_target_tbl       输出  目标表名称，多个使用逗号分隔
    * v_target_ver       输出  目标版本号，为空时使用#代替
    * v_run_flag         输出  1：正常，2：不能运行，3：查询不到任务ID，4：其它问题；
    * v_user_params      输出  用户自定参数
    * v_ver_id           输出  版本号ID
    * v_msg              输出  异常信息
    *
    * @param p_proc_date
    *
    *
    */
  def verStartIf(p_proc_date: String, p_task_name: String, src_tables: Array[String], hiveContext: HiveContext): Map[String, String] = {

    val ins = Array[Object](p_proc_date, p_task_name)

    val outNames = Array[String](
      "v_source_tbl",
      "v_source_ver",
      "v_target_tbl",
      "v_target_ver",
      "v_run_flag",
      "v_user_params",
      "v_ver_id",
      "v_msg"
    )
    val outTypes = Array(
      oracle.jdbc.OracleTypes.VARCHAR,
      oracle.jdbc.OracleTypes.VARCHAR,
      oracle.jdbc.OracleTypes.VARCHAR,
      oracle.jdbc.OracleTypes.VARCHAR,
      oracle.jdbc.OracleTypes.VARCHAR,
      oracle.jdbc.OracleTypes.VARCHAR,
      oracle.jdbc.OracleTypes.VARCHAR,
      oracle.jdbc.OracleTypes.VARCHAR)

    val sql = "call i17_version_manage_pkg.ver_start_if(?,?,?,?,?,?,?,?,?,?)"

    val res = scala.collection.mutable.Map[String, Object]()

    res.++=(callableQuery(sql, ins, outNames, outTypes))

    if (res.get("v_source_ver").get == "#" || res.get("v_source_ver").get == null) {

      val v_source_tbl_arr = new ArrayBuffer[String]()

      val v_source_ver_arr = new ArrayBuffer[String]()

      for (table <- src_tables) {
        //BAS_LBS_POL_BEN_ACTUARY_I17
        v_source_tbl_arr.append(table)
        //源表版本号如果没有取最大值
        //proc_month = '$proc_month' and
        val sql = s"show partitions $table"
        println(sql)
        val max_source_ver = hiveContext.sql(sql).collect().map(x => {
          x.toString().substring(x.toString().lastIndexOf("=") + 1)
        }).sortWith(_ > _).head

        v_source_ver_arr.append(max_source_ver.replaceAll("\\[", "").replaceAll("\\]", ""))
      }

      res("v_source_tbl") = v_source_tbl_arr.mkString(",")
      res("v_source_ver") = v_source_ver_arr.mkString(",")
    }

    res.mapValues(x => {
      if (x == null) {
        "null"
      } else {
        x.toString
      }
    }).toMap

  }


  /**
    * p_ver_id           输入   版本号ID
    * p_source_tbl       输入   源表名称，多个使用逗号分隔
    * p_source_ver       输入   源版本号，多个使用逗号分隔，为空时使用#代替
    * p_target_tbl       输入   目标表名称，多个使用逗号分隔
    * p_target_ver       输入  目标版本号 ，为空时使用#代替
    * p_run_flag         输入  运行状态，1成功，0失败
    * p_run_msg          输入  运行异常信息
    * *
    * v_flag             输出  更新状态：1：成功，2：找不到版本号信息，3：找不到源表及目标版本号，4：其它错误
    * v_msg              输出  异常信息
    */
  def verEndIf(startVersionMap: Map[String, Object], p_run_flag: Int, p_run_msg: String): Map[String, String] = {

    val ins = Array[Object](
      startVersionMap.get("v_ver_id").orNull,
      startVersionMap.get("v_source_tbl").orNull,
      startVersionMap.get("v_source_ver").orNull,
      startVersionMap.get("v_target_tbl").orNull,
      startVersionMap.get("v_target_ver").orNull,
      p_run_flag.toString,
      p_run_msg)

    val outNames = Array[String](
      "v_flag",
      "v_msg"
    )
    val outTypes = Array(
      oracle.jdbc.OracleTypes.VARCHAR,
      oracle.jdbc.OracleTypes.VARCHAR
    )

    val sql = "call i17_version_manage_pkg.ver_end_if(?,?,?,?,?,?,?,?,?)"

    callableQuery(sql, ins, outNames, outTypes).mapValues(x => {
      if (x == null) {
        "null"
      } else {
        x.toString
      }
    })

  }


}
package com.an.lcloud.act.core.framework.schema

/**
  * @author : EX-HUANGTAO009
  * @since : 15:00 2019-3-15
  * @note : dcs階段的結果表的case class
  *       字段顺序与oracle中的字段顺序保持一致
  */
class ACT_DCS_RESULT_TRA(
                            var run_selection: String = "", //DCS运行类别
                            var prod_name: String = "", //建模名称
                            var group_id: String = "", //分组编号
                            var spcode: String = "", //对于准备金月结所用IF MP，第一位代表渠道：1为个险、2为团险、3为银保，第二位代表保单状态；对于PRC GAAP月中分摊用IF MP，前两位代表机构代码，最后一位代表渠道；对于预测分析室所用NB MP，为51；对于产品部所用NB MP，按交费期间区分为51至55
                            var entry_year: String = "", //保单生效年
                            var entry_month: String = "", //保单生效月
                            var age_at_entry: String = "", //被保人年龄
                            var sex: String = "", //被保人性别。0为男性，1为女性
                            var pol_status: String = "", //保单状态:1为交费有效，2为交清有效，3为豁免，4为减额交清，5为失效
                            var dist_channel: String = "", //渠道码
                            var pol_term_y: String = "", //保险期间
                            var prem_paybl_y: String = "", //缴费期间
                            var prem_freq: String = "", //缴费频率：1为年交/趸交，2为半年交，4为季交，12为月交
                            var sum_assured: String = "", //保额
                            var annual_prem: String = "", //年缴保费
                            var durationif_m: String = "", //保单生效日距评估日总月数
                            var init_pols_if: String = "", //单位
                            var payment_code: String = "", //附加险赔付状态
                            var sum_assd_org: String = "", //原始保额
                            var age_payment: String = "", //年金领取年龄
                            var comm_pay: String = "", //判断现金激励给付比例0为无激励，1为15%，2为10%
                            var units: String = "", //份数
                            var comm_index: String = "", //判断首年佣金给付比例，0为原比例，1为新比例48%
                            var gtee_per_y: String = "", //1142保证领取期限
                            var esc_rate_pc: String = "", //费用率
                            var ann_annuity: String = "", //年金
                            var annuity_freq: String = "", //1142年领取频率
                            var annuity_type: String = "", //年金领取方式
                            var prem_type: String = "", //缴费方式
                            var ci_rider: String = "", //是否有附加险标示
                            var retire_age: String = "", //最大缴费期满年龄限制
                            var plan_code: String = "", //险种代码
                            var polno: String = "", //保单
                            var output_path: String = "", //输出路径
                            var val_code: String = "", //移动分析需要输出保单号及评估月标示
                            var proc_date: String = "", //评估日期
                            var order_num: String = "", //计算并发号
                            var subpartition_id: String = "", //子分区编码
                            var created_by: String = "", //创建人
                            var created_date: String = "", //创建时间
                            var updated_by: String = "", //修改人
                            var updated_date: String = "", //修改日期
                            var region_code: String = "", //区域码
                            var channel_mode: String = "", //销售渠道
                            var dno: String = "", //二级机构代码
                            var sno: String = "", //销售系列
                            var deptno: String = "", //销售部门代码
                            var ben_sts : String = "" //保单状态
                            //                        TODO: 合同组和现金流这边的先不管, tra这边mp的需求未给 2019-4-12 10:33:29
                            //--------下面是合同组+现金流的
                            //                              var tg_method: String = "", // "分别对应 分红、传统高、传统低、投连、万能",
                            //                              var tg_risk_type: String = "", // "分别对应 健康、身故意外、年金",
                            //                              var tg_pl: String = "", // "分别对应 盈利、其他、亏损",
                            //                              var tg_entryyear: String = "", //  "保单生效年标签，entry_year – 1988",
                            //                              var tg_ins_risk: String = "", //重大风险测试结果
                            //                              var loan_idx: String = "", // "",是否有贷款",
                            //                              var loan_iss_amt: String = "", //  "保单贷款本金",
                            //                              var loan_int_acc: String = "", // "保单贷款累积利息",
                            //                              var apl_idx: String = "", //  "是否有保单自垫",
                            //                              var apl_iss_amt: String = "", //  "保单自垫计息基础本金",
                            //                              var apl_int_acc: String = "", // "保单自垫累积利息",
                            //                              var surv_option: String = "", //  "是否有累积生息生存金",
                            //                              var surv_acc_prc: String = "", // "累积生息生存金本金余额",
                            //                              var surv_acc_int: String = "", // "累积生息生存金利息余额",
                            //                              var div_option: String = "", //  "是否勾选红利累积生息",
                            //                              var div_acc_prp: String = "", //  "累积生息红利本金余额",
                            //                              var div_acc_int: String = "", // "累积生息红利利息余额",
                            // 新增字段 ??
//                            var undwrt_date: String = ""
                        ) extends Serializable {

    override def toString = s"LAR_DCS_RESULT_TRA($output_path, $plan_code, $spcode, $age_at_entry, $sex, $entry_year)"
    def policyDownloadValue : String = {
        //保险, 非空判断f
        if (output_path == null || output_path.equals("")){
            "Warning: output_path is null or void , please check the logic"
        }else{
            output_path match {
                case v if v.contains("SNO") => Array("*",spcode,sex,age_payment,dist_channel,annuity_type,polno,prem_paybl_y,payment_code,age_at_entry,prem_freq,annuity_freq,sum_assured,group_id,plan_code,annual_prem,pol_term_y,units,ann_annuity,esc_rate_pc,sum_assd_org,entry_year,ci_rider,entry_month,init_pols_if,gtee_per_y,durationif_m,prem_type,comm_pay,pol_status,val_code,comm_index).mkString(",")  + "\n"
                case v if v.contains("DNO") => Array("*",spcode,sex,age_payment,dist_channel,annuity_type,polno,prem_paybl_y,payment_code,age_at_entry,prem_freq,annuity_freq,sum_assured,group_id,plan_code,annual_prem,pol_term_y,units,ann_annuity,esc_rate_pc,sum_assd_org,entry_year,ci_rider,entry_month,init_pols_if,gtee_per_y,durationif_m,prem_type,comm_pay,pol_status,val_code,comm_index).mkString(",")  + "\n"
                case v if v.contains("KAOHE") => Array("*",spcode,sex,comm_index,entry_month,prem_freq,annuity_type,pol_status,polno,annuity_freq,prem_type,sum_assured,durationif_m,val_code,plan_code,units,payment_code,pol_term_y,annual_prem,prem_paybl_y,init_pols_if,age_at_entry,group_id,dist_channel,esc_rate_pc,ann_annuity,ci_rider,gtee_per_y,entry_year,comm_pay,age_payment,sum_assd_org).mkString(",")  + "\n"
                case v if v.contains("HIGHRATE") => Array("*",spcode,sex,comm_index,entry_month,prem_freq,annuity_type,pol_status,polno,annuity_freq,prem_type,sum_assured,durationif_m,val_code,plan_code,units,payment_code,pol_term_y,annual_prem,prem_paybl_y,init_pols_if,age_at_entry,group_id,dist_channel,esc_rate_pc,ann_annuity,ci_rider,gtee_per_y,entry_year,comm_pay,age_payment,sum_assd_org).mkString(",")  + "\n"
                case v if v.contains("SNO_I17") => ""
                case _ => ""
            }
        }
    }
}



package com.an.lcloud.act.core.framework.utils

import org.joda.time.DateTime
import org.joda.time.format.DateTimeFormat

/**
  * Created by ZOUBO162 on 2018-11-23.
  */
object DateUtils {

  val YYYY_MM_DD_HHDDSS = "yyyy-MM-dd HH:mm:ss"
  val YYYY_MM_DD = "yyyy-MM-dd"
  val YYYYMMDD = "yyyyMMdd"
  val YYYY_MM="yyyy-MM"
  val YYYYMM="yyyyMM"
  val YYYY="yyyy"


  /**
    * 时间字符串转换成日期
    *
    * @param date    日期字符串
    * @param pattern 日期格式
    * @return 返回dateTime时间
    */
  def strToDateTime(date: String, pattern: String = YYYY_MM_DD_HHDDSS): DateTime = {
    val format = DateTimeFormat.forPattern(pattern).withZoneUTC()
    DateTime.parse(date, format)
  }

  /**
    * dateTime转换成字符串
    *
    * @param dateTime dateTime
    * @param pattern  日期格式
    * @return 返回日期字符串
    */
  def dateToStr(dateTime: DateTime, pattern: String = YYYY_MM_DD): String = {
    dateTime.toString(pattern)
  }

  /**
    * 获取当前dateTime时间
    *
    * @param pattern 日期格式
    * @return 返回dateTime时间
    */
  def nowDateTime(pattern: String = YYYY_MM_DD_HHDDSS): DateTime = {
    new DateTime()
  }

  /**
    * 获取N天后的日期(N>0) 或者 N天前日期(N<0)
    *
    * @param dateTime dateTime日期
    * @param days     天数：正数指N天后，负数指N天前
    * @return
    */
  def plusNDays(dateTime: DateTime, days: Int): DateTime = {
    dateTime.plusDays(days)
  }



}

package com.an.lcloud.act.core.framework.utils

import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable.HashMap

/**
  * Created by ZOUBO162 on 2018-12-6.
  * 数据库操作类
  */
trait DBUtils {
  /**
    *
    * @param hiveContext
    * @param serviceConfMap
    * @param dataFrame     结果集
    * @param tempTable     临时表
    */
  def batchSaveToHive(hiveContext: HiveContext, serviceConfMap: HashMap[String, String], dataFrame: DataFrame, tempTable: String): Unit = {
    val table = serviceConfMap.getOrElse("res_table", "")
    val month = serviceConfMap.getOrElse("p_proc_month", "")
    val versionNum = serviceConfMap.getOrElse("res_version_num", "")

    val sql =
      s"""
         |insert into table $table partition(proc_month='$month',version_num='$versionNum') select * from $tempTable
      """.stripMargin
    hiveContext.sql(sql)
  }

  /**
    * 从数据库加载数据
    *
    * @param hiveContext    执行上下文
    * @param table          执行sql语句
    * @param serviceConfMap 参数
    * @return
    */
  def loadDataFromDB(hiveContext: HiveContext, table: String, serviceConfMap: HashMap[String, String]): DataFrame = {
    val url = serviceConfMap.getOrElse("jdbc_str", "jdbc:oracle:thin:@d0pala.dbdev.paic.com.cn:1526:d0pala")
    val user = serviceConfMap.getOrElse("db_user", "Palalarsdata")
    val password = serviceConfMap.getOrElse("db_psw", "paic1234")
    hiveContext.read.format("jdbc")
      .options(Map("driver" -> "oracle.jdbc.driver.OracleDriver",
        "url" -> url,
        "dbtable" -> table,
        "user" -> user,
        "password" -> password)).load
  }

  /**
    * 获取分区表中最大版本号
    *
    * @param hiveContext hivecontext
    * @param table       表名
    * @param procMonth   处理分区
    * @return
    */
  def getMaxVersionNumber(hiveContext: HiveContext, table: String, procMonth: String): DataFrame = {
    val sql =
      s"""
         |select max(version_num) from $table where proc_month='$procMonth'
      """.stripMargin
    hiveContext.sql(sql)
  }

  /**
    * 从hive中加载数据
    *
    * @param hiveContext hiveContext
    * @param sql         sql语句
    * @return
    */
  def loadDataFromHive(hiveContext: HiveContext, sql: String): DataFrame = {
    hiveContext.sql(sql)
  }

}


package com.an.lcloud.act.core.framework.utils

import java.security.MessageDigest

import com.an.lcloud.act.core.framework.comm.DCSConstants
import org.apache.commons.lang.StringUtils
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{DataTypeParser, Metadata, StructField, StructType}

import scala.collection.mutable.ArrayBuffer
import scala.io.Source

/**
  * Created by EX-ZHANGYONGTIAN001 on 2018-9-29.
  */
object Utils extends Serializable {

  def main(args: Array[String]): Unit = {

    //     val scalaClazzName =  POL_BEN_PLAN_CFG.getClass.getName
    //      val clazzName = scalaClazzName.substring(0,scalaClazzName.length-1)
    //
    //   val schema =   clazzName2StructType (clazzName)
    //
    //    schema.printTreeString()

    //    println(initcap("haha"))
  }


  /**
    * 将类转换成StructType
    *
    * @param clazzName
    * @return
    */
  def clazzName2StructType(clazzName: String): StructType = {

    val clazz = Class.forName(clazzName)
    val fields = clazz.getDeclaredFields

    val structFields = new ArrayBuffer[StructField]()

    fields.foreach(f => {
      f.setAccessible(true)
      structFields.append(StructField(f.getName, DataTypeParser.parse(f.getType.getSimpleName), true, Metadata.empty))
    })
    StructType(structFields)
  }


  /**
    * 将对象成员转换成map集合
    *
    * @param obj 对象实例
    * @return
    */
  def obj2MapOfCaseClass(obj: Any): Map[String, String] = {
    val args = scala.collection.mutable.Map[String, String]()
    val scalaClazzName = obj.getClass.getName
    //    val clazzName = scalaClazzName.substring(0,scalaClazzName.length-1)
    val clazz = Class.forName(scalaClazzName)
    val fields = clazz.getDeclaredFields

    fields.foreach(f => {
      f.setAccessible(true)
      val value = if (null eq f.get(obj)) "" else f.get(obj).toString
      args.put(f.getName.toUpperCase, value)
    })
    args.toMap
  }

  /**
    * 从数据集合中抽取数据创建对象
    *
    * @param map 数据集合
    * @param obj 对象实例
    * @return
    */
  def map2obj(map: Map[String, String], obj: Any) = {
    val map2 = map.map(x => (x._1.toLowerCase, x._2))
    val scalaClazzName = obj.getClass.getName
    val clazz = Class.forName(scalaClazzName)
    val fields = clazz.getDeclaredFields
    fields.foreach(f => {
      f.setAccessible(true)
      val fieldName = f.getName
      if (map2.keySet.contains(fieldName)) {
        f.set(obj, map2.get(fieldName).get)
      }
    })
    obj
  }

  /**
    * 将对象转换成Row
    *
    * @param obj
    * @param row
    * @tparam T
    */
  def setObjFiled[T](obj: Any, row: org.apache.spark.sql.Row) {
    val scalaClazzName = obj.getClass.getName
    val clazz = Class.forName(scalaClazzName)
    val fields = clazz.getDeclaredFields

    fields.foreach(f => {
      f.setAccessible(true)
      f.set(obj, row.getAs[String](f.getName))
    })

    obj.asInstanceOf[T]
  }


  /**
    * 读取classPath下的文件
    *
    * @param fileClassPath
    * @return
    */
  def readConf(fileClassPath: String): Array[String] = {
    val filePath = this.getClass().getResource(fileClassPath).getPath
    Source.fromFile(filePath).getLines().toArray
  }


  /**
    * md5加密算法，并将加密后的byte数组转换成16进制
    *
    * @param input 待加密字符串
    * @return
    */
  def md5(input: String): String = {
    val md5 = MessageDigest.getInstance("MD5")
    val md5Bytes: Array[Byte] = md5.digest(input.getBytes("utf-8"))
    var hexValue: String = ""
    for (i <- 0 until md5Bytes.length) {
      val str: Int = (md5Bytes(i).toInt) & 0xff
      if (str < 16) {
        hexValue = hexValue + "0"
      }
      hexValue = hexValue + (Integer.toHexString(str))
    }
    hexValue.toString.toUpperCase
  }


  /**
    * 为防止业务同事配置算法时出现大小写不统一，对参数进行统一处理
    * 首字母大写
    *
    * @param str
    * @return
    */
  def initcap(str: String): String = {
    if (StringUtils.isNotBlank(str)) {
      str.take(1).toUpperCase + str.substring(1).toLowerCase
    } else {
      str
    }
  }


  /**
    * 格式化字符串输出，左填充
    * 如果原始字符串长度超过指定长度，从左到右截取指定长度字符串
    *
    * @param string1    原始字符串
    * @param size       填充后字符串总长度
    * @param pad_string 填充字符串
    * @return
    */
  def lpad(string1: String, size: Int, pad_string: String): String = {
    if (StringUtils.isNotBlank(string1)) {
      if (string1.length < size) {
        StringUtils.leftPad(string1, size, pad_string)
      } else {
        string1.substring(0, size)
      }
    } else {
      StringUtils.leftPad("", size, pad_string)
    }
  }

  /**
    * 计算评估时间点
    * 处理日期如果超过当月19号，则取值为当月20号，否则取值为上月月底
    * proc_date
    *
    * @param procDate 入参处理时间
    * @return
    */
  def getAssessmentProcDate(procDate: String): String = {
    val date = DateUtils.strToDateTime(procDate, DateUtils.YYYYMMDD)
    val result = if (date.getDayOfMonth > 19) {
      date.withDayOfMonth(20)
    } else {
      date.withDayOfMonth(1).minusDays(1)
    }
    result.toString(DateUtils.YYYY_MM_DD)
  }


  /**
    * 判断是否是数字
    *
    * @param s
    * @return
    */
  def isNumber(s: String) = {
    val pattern = """^(\-|\+)?\d+(\.\d+)?$""".r
    s match {
      case pattern(_*) => true
      case _ => false
    }
  }

  /**
    * 利用反射给对象属性赋值
    */
  def setObjFields(obj: Any, row: Row) {
    val clazzName = obj.getClass.getName
    val clazz = Class.forName(clazzName)
    val fields = clazz.getDeclaredFields
    fields.foreach(field => {
      field.setAccessible(true)
      val fieldName = field.getName.toLowerCase
      field.set(obj, String.valueOf(row.getAs[String](fieldName)))
    })
  }

  /**
    * 将值为空或null的值置0
    *
    * @param str
    * @return
    */
  def null2Zero(str: String): String = {
    if (str.isEmpty || str.equals("null")) {
      "0"
    } else {
      str
    }
  }

  /**
    * 生成groupID之前，去掉后面的0
    */
  val trimZero = (str: String) => {
    var result = str
    if (result.indexOf(".") > 0) {
      result = result.replaceAll("0+?$", "")
      result = result.replaceAll("[.]$", "")
    }
    result
  }


  /**
    *
    * @param procType 处理类型 IF/NB
    * @param procDate 处理日期
    */
  def getRealPartition(procType: String, procDate: String) = {
    val partition = procType match {
      case DCSConstants.CONST_DCS_POL_BEN_NB => procDate + "_" + DCSConstants.CONST_DCS_POL_BEN_NB
      case _ => procDate + "_" + DCSConstants.CONST_DCS_POL_BEN_IF
    }
    partition
  }



}
package com.an.lcloud.act.core.framework.VersionManage

import java.sql.{Connection, PreparedStatement, ResultSet, Timestamp}
import java.util.Date

/**
  * Created by EX-HUANGTAO009 on 2019-3-1.
  * 在出现英文的异常提示语时, 请根据"in"后面的字符 定位方法.
  * 查询的返回值中 Map 的 key 是 Int 类型 -- 除了声明异常的 0 , 其他并没有特殊意义
  */
object VMDBMethods {
    private var conn: Connection = _
    private var ps: PreparedStatement = _
    private var rs: ResultSet = _

    /**
      * @author : EX-HUANGTAO009
      * @note : 专门给Orcale的date类型传入现在日期的函数, 进行了字符串的拼接, 使用的时候不需要额外再加引号
      * @since : 10:46 2019-3-14
      * @return : java.lang.String
      */
    private def getCurrentTime(): String = {
        var time = new Timestamp(new Date().getTime).toString
        if (time.indexOf(".") > 0) {
            time = time.substring(0, time.indexOf("."))
        }
        val result = "to_date('" + time + "', 'yyyy-MM-dd hh24:mi:ss')"
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : 检查任务是否存在
      * @since : 14:03 2019-3-5
      * @param  p_task_name 任务名
      * @return : int  1 : 有数据  0 : 没数据
      */
    def queryTaskIfExist(p_task_name: String): Int = {
        var result: Int = 0
        try {
            conn = JDBCUtils.init
            val sql =
                s"""SELECT a.id_act_task_info
                   | FROM act_task_info a
                   |WHERE task_name  = '$p_task_name'
                   |""".stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            if (rs.next()) {
                result = 1
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryTaskIfExist")
                e.printStackTrace()
                result = 0
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note :  仅S'tart_if => 获取版本管理表的数据, 对应 Cursor ver_manage_cur
      * @since : 9:48 2019-3-4
      * @param  p_proc_date String 数据日期
      * @param  p_task_name String 任务名称
      * @return :
      *         1. id_act_task_trace_info
      *         2. proc_date
      *         3. task_name
      *         4. start_date
      *         5. end_date
      *         6. exec_flag
      *         7. user_params
      */
    def queryVM_S(p_proc_date: String, p_task_name: String)
    : collection.mutable.Map[Int, (String, String, String, String, String, String, String)] = {
        val result = collection.mutable.Map[Int, (String, String, String, String, String, String, String)]()
        try {
            conn = JDBCUtils.init
            val sql =
                s"""select a.id_act_task_trace_info,a.proc_date,a.task_name,a.start_date,a.end_date,a.exec_flag,a.user_params
                   |from act_task_trace_info a
                   |where proc_date  = '$p_proc_date'
                   |and task_name  = '$p_task_name'
                   |and a.exec_flag IN ('N','R','W')
                   |order by created_date DESC
                   |""".stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            var i: Int = 1
            while (rs.next()) {
                val id = rs.getString("id_act_task_trace_info")
                val pd = rs.getString("proc_date")
                val tn = rs.getString("task_name")
                val sd = rs.getString("start_date")
                val ed = rs.getString("end_date")
                val ef = rs.getString("exec_flag")
                val up = rs.getString("user_params")
                result += (i -> (id, pd, tn, sd, ed, ef, up))
                i += 1
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryVM_S")
                e.printStackTrace()
                result.empty
                result += (0 -> ("", "", "", "", "", "", ""))
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : 仅End_if => 版本管理表数据, 对应 Cursor ver_manage_cur
      * @since : 16:15 2019-3-6
      * @param  vi_ver_id 任务id
      * @return     :
      *             1. id_act_task_trace_info
      *             2. proc_date
      *             3. task_name
      *             4. ID_act_task_info
      *             5. start_date
      *             6. end_date
      *             7. exec_flag
      *             8. user_params
      */
    def queryVM_E(vi_ver_id: String)
    : collection.mutable.Map[Int, (String, String, String, String, String, String, String, String)] = {
        val result = collection.mutable.Map[Int, (String, String, String, String, String, String, String, String)]()
        try {
            conn = JDBCUtils.init
            val sql =
                s"""SELECT a.id_act_task_trace_info,
                   | a.proc_date,
                   | a.task_name,
                   | a.ID_act_task_info,
                   | a.start_date,
                   | a.end_date,
                   | a.exec_flag,
                   | a.user_params
                   | FROM act_task_trace_info a
                   | WHERE a.id_act_task_trace_info = '$vi_ver_id'
                   | """.stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            var i: Int = 1
            while (rs.next()) {
                val ids = rs.getString("id_act_task_trace_info")
                val pd = rs.getString("proc_date")
                val tn = rs.getString("task_name")
                val id = rs.getString("ID_act_task_info")
                val sd = rs.getString("start_date")
                val ed = rs.getString("end_date")
                val ef = rs.getString("exec_flag")
                val up = rs.getString("user_params")
                result += (i -> (ids, pd, tn, id, sd, ed, ef, up))
                i += 1
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryVM_E")
                e.printStackTrace()
                result.empty
                result += (0 -> ("", "", "", "", "", "", "", ""))
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note :  Start_if => 版本管理相关表版本数据的查询, 对应 Cursor ver_tbl_cur
      * @since : 10:33 2019-3-4
      * @param  p_ver_id 版本id
      * @return :
      *         1. id_act_task_trace_info
      *         2. id_act_task_sou_tar_info
      *         3. task_name
      *         4. table_type
      *         5. table_name
      *         6. table_version
      *         7. seq_idx
      */
    def queryVT(p_ver_id: String)
    : collection.mutable.Map[Int, (String, String, String, String, String, String, String)] = {
        val result = collection.mutable.Map[Int, (String, String, String, String, String, String, String)]()
        try {
            conn = JDBCUtils.init
            val sql =
                s"""SELECT a.id_act_task_trace_info,
                   |a.id_act_task_sou_tar_info,
                   |a.task_name,
                   |a.table_type,
                   |a.table_name,
                   |a.table_version,
                   |a.seq_idx
                   |FROM act_ver_sou_tar_info a
                   |WHERE id_act_task_trace_info ='$p_ver_id'
                   |ORDER BY seq_idx
                   |""".stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            var i: Int = 1
            while (rs.next()) {
                val id = rs.getString("id_act_task_trace_info")
                val ids = rs.getString("id_act_task_sou_tar_info")
                val tsn = rs.getString("task_name")
                val ty = rs.getString("table_type")
                val tbn = rs.getString("table_name")
                val tv = rs.getString("table_version")
                val si = rs.getString("seq_idx")
                result += (i -> (id, ids, tsn, ty, tbn, tv, si))
                i += 1
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryVT")
                e.printStackTrace()
                result.empty
                result += (0 -> ("", "", "", "", "", "", ""))
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : Start_if => 任务表查询 , 对应 Cursor task_info_cur
      * @since : 10:45 2019-3-4
      * @param  v_task_name String 任务名
      * @return :
      *         1. ID_act_task_info
      *         2. business_id
      *         3. task_group
      *         4. task_name
      *         5. task_seq
      *         6. task_level
      *         7. pre_task_name
      *         8. next_task_name
      */
    def queryTI(v_task_name: String)
    : collection.mutable.Map[Int, (String, String, String, String, String, String, String, String)] = {
        val result = collection.mutable.Map[Int, (String, String, String, String, String, String, String, String)]()
        try {
            conn = JDBCUtils.init
            val sql =
                s"""SELECT a.ID_act_task_info,
                   |a.business_id,
                   |a.task_group,
                   |a.task_name,
                   |a.task_seq,
                   |a.task_level,
                   |a.pre_task_name,
                   |a.next_task_name
                   |FROM  act_task_info a
                   |WHERE a.task_name='$v_task_name' """.stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            var i: Int = 1
            while (rs.next()) {
                val id = rs.getString("ID_act_task_info")
                val bi = rs.getString("business_id")
                val tg = rs.getString("task_group")
                val tn = rs.getString("task_name")
                val ts = rs.getString("task_seq")
                val tl = rs.getString("task_level")
                val ptn = rs.getString("pre_task_name")
                val ntn = rs.getString("next_task_name")
                result += (i -> (id, bi, tg, tn, ts, tl, ptn, ntn))
                i += 1
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryTI")
                e.printStackTrace()
                result.empty
                result += (0 -> ("", "", "", "", "", "", "", ""))
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : Start_if & End_if => 查看任务需要用到版本的源表和目标表 , 对应 Cursor task_sou_tar_cur
      * @since : 11:19 2019-3-4
      * @param  v_task_name String 任务名
      * @return :
      *         1. id_act_task_sou_tar_info
      *         2. id_act_task_info
      *         3. task_name
      *         4. table_name
      *         5. table_type
      *         6. seq_idx
      *         7. is_ver
      *         8. owner_name
      */
    def queryTST(v_task_name: String)
    : collection.mutable.Map[Int, (String, String, String, String, String, String, String, String)] = {
        val result = collection.mutable.Map[Int, (String, String, String, String, String, String, String, String)]()
        try {
            conn = JDBCUtils.init
            val sql =
                s"""SELECT a.id_act_task_sou_tar_info,
                   |a.id_act_task_info,
                   |a.task_name,
                   |a.table_name,
                   |a.table_type,
                   |a.seq_idx,
                   |a.is_ver,
                   |a.owner_name
                   |FROM act_task_sou_tar_info a
                   |WHERE a.task_name='$v_task_name'
                   |AND a.is_ver='Y'
                   |""".stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            var i: Int = 1
            while (rs.next()) {
                val ids = rs.getString("id_act_task_sou_tar_info")
                val id = rs.getString("id_act_task_info")
                val tsn = rs.getString("task_name")
                val tbn = rs.getString("table_name")
                val ty = rs.getString("table_type")
                val si = rs.getString("seq_idx")
                val iv = rs.getString("is_ver")
                val on = rs.getString("owner_name")
                result += (i -> (ids, id, tsn, tbn, ty, si, iv, on))
                i += 1
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryTST")
                e.printStackTrace()
                result.empty
                result += (0 -> ("", "", "", "", "", "", "", ""))
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note   : Start_if => 查找版本号完成记录, 对应 Cursor ver_fin_all_cur
      *         过程说明:
      *         1. v_task_name 将会被输入前置任务名 , 即pre_task_name, 而这个字段确认会出现有多个任务名使用逗号间隔的字段值.
      *         2. 所以首先 v_task_name需要先做字符串分割处理, 分割出来的每个任务都需要走一遍sql.
      *         3. 由于sql中存在row number = 1 的情况, 所以实际取出来的就是一行数据.
      *         4. 整体都会依据字符串分割的情况进行循环.
      * @since : 8:53 2019-3-5
      * @param v_task_name  String 任务名,
      * @param v_proc_date  String 数据日期,
      * @param v_table_name String 表名
      * @return :
      *         1. id_act_task_trace_info
      *         2. proc_date
      *         3. task_name
      *         4. ID_act_task_info
      *         5. start_date
      *         6. end_date
      *         7. exec_flag
      *         8. user_params
      *         9. remark
      *         10. table_type
      *         11. table_name
      *         12. table_version
      *         13. seq_idx
      */
    def queryVFA(v_task_name: String, v_proc_date: String, v_table_name: String)
    : collection.mutable.Map[Int,
        (String, String, String, String, String,
            String, String, String, String, String,
            String, String, String)] = {

        val result = collection.mutable.Map[Int, (String, String, String, String, String,
            String, String, String, String, String,
            String, String, String)]()
        try {
            conn = JDBCUtils.init
            val str = v_task_name.split(",")
            var i: Int = 1
            str.foreach { x =>
                //获得了单独的任务名之后, 在内部跑语句
                val sql =
                    s"""SELECT
                       | *
                       |FROM
                       |   ( SELECT
                       |            a.id_act_task_trace_info,
                       |            a.proc_date,
                       |            a.task_name,
                       |            a.ID_act_task_info,
                       |            a.start_date,
                       |            a.end_date,
                       |            a.exec_flag,
                       |            a.user_params,
                       |            a.remark,
                       |            c.table_type,
                       |            c.table_name,
                       |            c.table_version,
                       |            c.seq_idx,
                       |            row_number() over(PARTITION BY a.task_name,c.table_name ORDER BY a.id_act_task_trace_info DESC) rk
                       |     FROM
                       |     (SELECT
                       |       id_act_task_trace_info,
                       |       proc_date,
                       |       task_name,
                       |       ID_act_task_info,
                       |       start_date,
                       |       end_date,
                       |       exec_flag,
                       |       user_params,
                       |       remark
                       |     FROM act_task_trace_info
                       |   WHERE task_name =  '$x'
                       |   ) a
                       |     INNER JOIN
                       |     (
                       |     SELECT
                       |       table_type,
                       |       table_name,
                       |       table_version,
                       |       seq_idx,
                       |       id_act_task_trace_info
                       |     FROM
                       |       act_ver_sou_tar_info
                       |     ) c
                       |     ON a.id_act_task_trace_info=c.id_act_task_trace_info
                       |     WHERE a.exec_flag='E'
                       |     AND a.proc_date='$v_proc_date'
                       |     AND c.table_name='$v_table_name'
                       |) WHERE  rk=1
                       | """.stripMargin
                ps = conn.prepareStatement(sql)
                rs = ps.executeQuery()
                //虽然由于row_number的存在, 肯定就是单次了, 但是写法暂时还是这么写
                //将计数的临时变量i拿出去
                while (rs.next()) {
                    val idtt = rs.getString("id_act_task_trace_info")
                    val pd = rs.getString("proc_date")
                    val tsn = rs.getString("task_name")
                    val idt = rs.getString("ID_act_task_info")
                    val sd = rs.getString("start_date")
                    val ed = rs.getString("end_date")
                    val ef = rs.getString("exec_flag")
                    val up = rs.getString("user_params")
                    val remark = rs.getString("remark")
                    val tt = rs.getString("table_type")
                    val tbn = rs.getString("table_name")
                    val tv = rs.getString("table_version")
                    val si = rs.getString("seq_idx")
                    result += (i -> (idtt, pd, tsn, idt, sd, ed, ef, up, remark, tt, tbn, tv, si))
                    i += 1
                }
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryVFA")
                e.printStackTrace()
                result.empty
                result += (0 -> ("", "", "", "", "", "", "", "", "", "", "", "", ""))
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note :  start_if / end_if =>更新act_task_trace_info表 , 由于235 / 443 / 547 其实相差微小, 因此在sql中做些改动, 方法用的就是同一个方法
      * @since : 16:27 2019-3-5
      * @param  lineNo                  行数, 这里是指 Oracle 中的行数, 行数不同, 可能会导致sql不同, 输入值固定应为 235 / 443 / 547 三个其中之一
      * @param   id_act_task_trace_info 可以是v_ver_id, 可以是p_ver_id
      * @param   p_run_flag             end_if专用, sql要求, start_if中请用null
      * @param  p_run_msg               end_if专用, sql要求, start_if中请用null
      */
    def updateATTI(lineNo: Int, id_act_task_trace_info: String, p_run_flag: String, p_run_msg: String): Unit = {
        try {
            conn = JDBCUtils.init
            var sql = ""
            var tmp: String = ""
            if (lineNo == 235) {
                tmp = "R"
                sql =
                    s"""UPDATE act_task_trace_info a
                       |SET
                       |a.start_date=${VMDBMethods.getCurrentTime()},
                       |a.exec_flag='R',
                       |a.updated_by='HDP',
                       |a.updated_date=${VMDBMethods.getCurrentTime()}
                       |WHERE a.id_act_task_trace_info= '$id_act_task_trace_info'
                       |""".stripMargin
            }
            else if (lineNo == 547) {
                //End_if接口会在这里根据传入的flag判断任务在运行过程中是否成功, 然后进行update
                if (p_run_flag.equalsIgnoreCase("1")) tmp = "E"
                else tmp = "X"
                sql =
                    s"""UPDATE act_task_trace_info a
                       |SET
                       |a.start_date=${VMDBMethods.getCurrentTime()},
                       |a.exec_flag='$tmp',
                       |a.remark=substr('$p_run_msg',1,199),
                       |a.updated_by='HDP',
                       |a.updated_date=${VMDBMethods.getCurrentTime()}
                       |WHERE a.id_act_task_trace_info= '$id_act_task_trace_info'
                       |""".stripMargin
            }
            else if (lineNo == 443) {
                sql =
                    s"""UPDATE act_task_trace_info a
                       |SET
                       |a.exec_flag='R',
                       |a.start_date=${VMDBMethods.getCurrentTime()}
                       |WHERE a.id_act_task_trace_info= '$id_act_task_trace_info'
                       |""".stripMargin
            }
            ps = conn.prepareStatement(sql)
            val verify = ps.executeUpdate()
            if (verify > 0) println("Update act_task_trace_info success!")
            else println("Update act_task_trace_info fail!")
        } catch {
            case e: Throwable =>
                e.printStackTrace()
                println("Exception happened in updateATTI!")

        } finally {
            JDBCUtils.close(conn, ps, null)
        }
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : 插入一条新的版本号数据, 对应行数275
      * @since : 9:36 2019-3-6
      * @param  p_proc_date      评估日期
      * @param  p_task_name      任务名
      * @param  id_act_task_info 任务id , 对应Orcale的id_act_task_info字段
      *                          ID_ACT_TASK_TRACE_INFO 是编号id 不要搞错了
      */
    def insertATTI(p_proc_date: String, p_task_name: String, id_act_task_info: String): Unit = {
        try {
            conn = JDBCUtils.init
            val sql =
                s""" INSERT INTO act_task_trace_info
                   |( ID_act_task_info, proc_date, task_name)
                   |VALUES
                   |(? , ? , ?)
         """.stripMargin
            ps = conn.prepareStatement(sql)
            ps.setString(1, id_act_task_info)
            ps.setString(2, p_proc_date)
            ps.setString(3, p_task_name)
            val verify = ps.executeUpdate()
            if (verify > 0) println("Insert act_task_trace_info success!")
            else println("Insert act_task_trace_info fail!")
        } catch {
            case e: Throwable =>
                e.printStackTrace()
                println("Exception happened in insertATTI")
        } finally {
            JDBCUtils.close(conn, ps, null)
        }
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : 取得最新一条对应id_act_task_trace_info, 在task_info_cur打开后使用, 对应行数 281
      * @since : 9:48 2019-3-6
      * @param  p_proc_date 评估日期
      * @param p_task_name  任务名
      * @return : 出来就是v_ver_id
      */
    def queryLatestVid(p_proc_date: String, p_task_name: String): String = {
        var res: String = ""
        try {
            conn = JDBCUtils.init
            val sql =
                s"""SELECT id_act_task_trace_info
                   | FROM (SELECT id_act_task_trace_info
                   |         FROM act_task_trace_info a
                   |        WHERE a.proc_date = '$p_proc_date'
                   |          AND a.task_name = '$p_task_name'
                   |          AND a.exec_flag = 'N'
                   |        ORDER BY created_date DESC)
                   |WHERE rownum = 1
                   |""".stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            if (rs.next()) {
                res = rs.getString("id_act_task_trace_info")
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryLatestVid")
                e.printStackTrace()
                res = ""
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        res
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : 写入源版本号数据, 对应行数331 / 367 / 586 & 634 Merge INTO 中的insert分支
      * @since : 10:37 2019-3-6
      * @param  id_act_task_trace_info  任务执行跟踪表ID外键
      * @param id_act_task_sou_tar_info 与act_task_sou_tar_tbl的外键
      * @param task_name                与act_task_info的task_name一致
      * @param table_type               表类型，source代表源表， target代表目标表
      * @param table_name               相关表名
      * @param table_version            表版本号
      * @param seq_idx                  表顺序号
      */
    def insertAVSTI(id_act_task_trace_info: String, id_act_task_sou_tar_info: String,
                    task_name: String, table_type: String, table_name: String,
                    table_version: String, seq_idx: String): Unit = {
        try {
            conn = JDBCUtils.init
            val sql =
                s""" INSERT INTO act_ver_sou_tar_info
                   | (id_act_task_trace_info,
                   | id_act_task_sou_tar_info,
                   | task_name,
                   | table_type,
                   | table_name,
                   | table_version,
                   | seq_idx)
                   | VALUES
                   |  ('$id_act_task_trace_info',
                   |   '$id_act_task_sou_tar_info',
                   |   '$task_name',
                   |   '$table_type',
                   |   '$table_name',
                   |   '$table_version',
                   |   '$seq_idx')
         """.stripMargin
            ps = conn.prepareStatement(sql)
            val verify = ps.executeUpdate()
            if (verify > 0) println("Insert act_ver_sou_tar_info success!")
            else println("Insert act_ver_sou_tar_info fail!")
        } catch {
            case e: Throwable =>
                e.printStackTrace()
                println("Exception happened in insertAVSTI")
        } finally {
            JDBCUtils.close(conn, ps, null)
        }
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : 查看最大版本号, 对应存储过程的行数 393
      * @since : 11:17 2019-3-6
      * @param  p_proc_date 评估日期
      * @param p_task_name  任务名
      * @param table_name   表名
      * @return : v_target_data
      */
    def queryLargestVid(p_proc_date: String, p_task_name: String, table_name: String): String = {
        var res: String = ""
        try {
            conn = JDBCUtils.init
            val sql =
                s"""SELECT to_char(to_number(MAX(table_version)) + 1) AS tmp
                   |  FROM act_task_trace_info a
                   | INNER JOIN act_ver_sou_tar_info b
                   | ON a.id_act_task_trace_info = b.id_act_task_trace_info
                   | WHERE a.proc_date = '$p_proc_date'
                   |   AND a.task_name = '$p_task_name'
                   |   AND b.table_name = '$table_name'
                   |   """.stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            if (rs.next()) {
                res = rs.getString("tmp")
                return res
            }
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryLargestVid")
                e.printStackTrace()
                res = "1"
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        res
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : End_if => 查询 act_task_sou_tar_info 的 限定数据有没有出现在 act_ver_sou_tar_info中
      *       涉及到后续是做 update操作 还是 insert操作
      * @since : 11:22 2019-3-7
      * @param  task_name    任务名 对应 task_sou_tar_arr(v_j).task_name
      * @param table_name    表名 对应 v_source_tbl / v_target_tbl
      * @param table_version 表版本 对应 v_source_ver / v_target_ver
      * @return :
      *         int 1/ 其他: 成功 0: 失败
      *         String:
      *         1. id_act_task_trace_info
      *         2. id_act_task_sou_tar_info
      *         3. id_act_task_info
      *         4. task_name
      *         5. table_type
      *         6. table_name
      *         7. seq_idx
      *         8. table_version
      */
    def queryRecordIfExist(task_name: String, table_name: String, table_version: String, p_ver_id: String)
    : collection.mutable.Map[Int, (String, String, String, String, String, String, String, String)] = {
        val result = collection.mutable.Map[Int, (String, String, String, String, String, String, String, String)]()
        try {
            conn = JDBCUtils.init
            val sql =
                s"""SELECT
                   | b.id_act_task_trace_info AS id_act_task_trace_info,
                   | b.id_act_task_sou_tar_info AS id_act_task_sou_tar_info,
                   |b.id_act_task_info AS id_act_task_info,
                   |b.task_name AS task_name,
                   |b.table_type AS table_type,
                   |b.table_name AS table_name,
                   |b.seq_idx AS seq_idx,
                   |b.table_version AS table_version
                   |FROM
                   |(SELECT
                   |     '$p_ver_id' id_act_task_trace_info,
                   |     tb.id_act_task_sou_tar_info,
                   |     tb.id_act_task_info,
                   |     tb.task_name,
                   |     tb.table_type,
                   |     tb.table_name,
                   |     tb.seq_idx,
                   |     '$table_version' table_version
                   |FROM act_task_sou_tar_info tb WHERE tb.task_name='$task_name'
                   |AND tb.table_name='$table_name' ) b
                   |INNER JOIN
                   | (
                   | SELECT
                   |  id_act_task_trace_info,
                   |  table_name
                   | FROM  act_ver_sou_tar_info
                   | ) a
                   |ON
                   | (a.id_act_task_trace_info = b.id_act_task_trace_info
                   | AND a.table_name = b.table_name)
                   | """.stripMargin
            ps = conn.prepareStatement(sql)
            rs = ps.executeQuery()
            var i: Int = 1
            while (rs.next()) {
                val idt = rs.getString("id_act_task_trace_info")
                val ids = rs.getString("id_act_task_sou_tar_info")
                val id = rs.getString("id_act_task_info")
                val tsn = rs.getString("task_name")
                val ty = rs.getString("table_type")
                val tbn = rs.getString("table_name")
                val si = rs.getString("seq_idx")
                val tv = rs.getString("table_version")
                result += (i -> (idt, ids, id, tsn, ty, tbn, si, tv))
                i += 1
            }
            //这边是在方法内判断了, 其他的方法会在外围有非空判断
            if (result.isEmpty) result += (0 -> ("", "", "", "", "", "", "", ""))
        } catch {
            case e: Throwable =>
                println("Exceptions happened in queryRecordIfExist")
                e.printStackTrace()
                result.empty
                result += (0 -> ("", "", "", "", "", "", "", ""))
        } finally {
            JDBCUtils.close(conn, ps, rs)
        }
        result
    }

    /**
      * @author : EX-HUANGTAO009
      * @note : end_if => merge into 的update分支
      * @since : 16:05 2019-3-7
      * @param  table_version 会是 v_source_ver / v_target_ver
      * @param id_act_task_trace_info p_ver_id, 编号id
      * @param table_name 表名
      * @return : void 直接打印反馈
      */
    def updateAVSTI(table_version: String, id_act_task_trace_info: String, table_name: String): Unit = {
        try {
            conn = JDBCUtils.init
            val sql =
                s"""UPDATE act_ver_sou_tar_info a
                   |SET
                   |a.table_version = '$table_version',
                   |a.updated_by = 'HDP',
                   |a.updated_date = ${VMDBMethods.getCurrentTime()}
                   |WHERE a.table_version IS NULL
                   |AND a.id_act_task_trace_info = '$id_act_task_trace_info'
                   |AND a.table_name = '$table_name'
                   |""".stripMargin
            //            println(sql)
            ps = conn.prepareStatement(sql)
            val verify = ps.executeUpdate()
            if (verify > 0) println("Update act_ver_sou_tar_info success!")
            else println("Update act_ver_sou_tar_info fail!")
        } catch {
            case e: Throwable =>
                e.printStackTrace()
                println("Exception happened in updateAVSTI")
        } finally {
            JDBCUtils.close(conn, ps, null)
        }
    }

}



package com.an.lcloud.act.dcs.launcher

import com.an.lcloud.act.core.framework.job.JobCommon
import com.an.lcloud.act.core.framework.schema.{ACT_DCS_FUNCS_CFG, BAS_PALA_POL_BEN_UL, LAR_DCS_BASIC_INFO_AIO}
import com.an.lcloud.act.core.framework.utils.{DateUtils, Utils}
import com.an.lcloud.act.dcs.job.DcsTop888Job
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}
import org.joda.time.DateTime

/**
  * Created by ex-shencheng001 on 2019/4/1.
  */
object DcsTop888Executor extends JobCommon{

  def main(args: Array[String]): Unit = {

    //startTime
    val startTime = System.currentTimeMillis()
    logError("<-------------------------------------------------------------DcsTOP888Executor开始执行------------------------------------------------->")
    //判断输入参数是否符合要求,并将参数放到集合中
    args.foreach(arg => {
      var param = arg.split("=")
      logError(arg)
      if(param.length == 2){
        serviceConfMap.put(param(0),param(1))
      }else{
        throw new IllegalArgumentException(s""" the $arg is wrong""")
      }
    })

    //设置日志的等级,debug,info,warn,error,
    val logLevel = serviceConfMap.getOrElse("logLevel","error")

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    //参数校验
        if (args.length < 5) {
          logError("Usage: 未指定参数列表 " +
            " [PROC_DATE] [SPCODE_SELECTION][SRC_VERSION_NUM][RES_VERSION_NUM][RES_HIVE_DB]")
          System.exit(1)
        }

    /**
      * 处理日期，上月最后一天
      */
    val procDate = Utils.getAssessmentProcDate(serviceConfMap.getOrElse("p_end_date", DateUtils.dateToStr(new DateTime, DateUtils.YYYY_MM_DD)))
    serviceConfMap += ("p_proc_date" -> procDate)
    //计算分区值
    if (serviceConfMap.getOrElse("p_proc_month", "") == "") {
      serviceConfMap += ("p_proc_month" -> DateUtils.strToDateTime(procDate, DateUtils.YYYY_MM_DD).toString(DateUtils.YYYYMM))
    }


    val SPCODE_SELECTION = serviceConfMap.getOrElse("spcode_selection", "0")


    //源表版本号如果没有取最大值
    val src_version_num = serviceConfMap.get("src_version_num").getOrElse("#")

    //目标表版本号如果没有取当前时间
    //    val res_version_num = serviceConfMap.get("res_version_num").getOrElse("#")
    val res_version_num = startTime.toString
    val res_hive_db = serviceConfMap.get("res_hive_db").getOrElse("#")

    //proc_date使用保单汇总记录中的proc_date
    //    serviceConfMap.put("PROC_DATE", procDate)
    serviceConfMap.put("SPCODE_SELECTION", SPCODE_SELECTION)
    serviceConfMap.put("src_version_num", src_version_num)
    serviceConfMap.put("res_version_num", res_version_num)
    serviceConfMap.put("res_hive_db", res_hive_db)
    serviceConfMap.put("dbName", res_hive_db)
    serviceConfMap.put("res_table", res_hive_db + ".bas_act_dcs_result_top")

    val conf:SparkConf = new SparkConf()
      .setAppName("DcsTOP888Executor")
      .set("spark.sql.codegen", "true") /*spark.sql.codegen 是否预编译sql成java字节码，长时间或频繁的sql有优化效果*/
      .set("spark.sql.inMemoryColumnarStorage.batchSize", "20000") /*spark.sql.inMemoryColumnarStorage.batchSize 一次处理的row数量，小心oom*/
      .set("spark.sql.inMemoryColumnarStorage.compressed", "true") /*spark.sql.inMemoryColumnarStorage.compressed 设置内存中的列存储是否需要压缩*/
      .set("spark.sql.autoBroadcastJoinThreshold", "20971520") /*spark.sql.autoBroadcastJoinThreshold,解决数据倾斜*/
      .set("hive.execution.engine", "spark")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") /*Kryo序列化*/
      .set("spark.kryoserializer.buffer.mb", "10") /*Kryo序列化*/
      .registerKryoClasses(Array[Class[_]](classOf[BAS_PALA_POL_BEN_UL], classOf[LAR_DCS_BASIC_INFO_AIO], classOf[ACT_DCS_FUNCS_CFG], classOf[Row]))

    //创建sc
    val sparkContext:SparkContext = new SparkContext(conf)
    logLevel match {
      case "warn" => sparkContext.setLogLevel(Level.WARN.toString)
      case "info" => sparkContext.setLogLevel(Level.INFO.toString)
      case "error" => sparkContext.setLogLevel(Level.ERROR.toString)
    }

    val hiveContex:HiveContext = new HiveContext(sparkContext)

    new DcsTop888Job(sparkContext,hiveContex,serviceConfMap).run

    sparkContext.stop()
    val endTime = System.currentTimeMillis()
    logError("<-------------------------------------------------------------DcsTop888Executor执行完成------------------------------------------------->")
    logError("DcsTOP888Executor耗时:" + (endTime - startTime) / 1000 +"秒")

  }
}


package com.an.lcloud.act.dcs.udf

import com.an.lcloud.act.core.framework.comm.DCSConstants
import com.an.lcloud.act.core.framework.job.JobCommon
import com.an.lcloud.act.core.framework.schema._
import org.apache.spark.sql.Dataset
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable
import scala.collection.mutable.HashMap

/**
  * 加载DCS公共数据类
  * Created by ZOUBO162 on 2018-11-22.
  */
object LoadDcsCommDataJob extends JobCommon {

  /**
    * grouping_rules规则配置
    * 区分GAAP数据和I17数据
    *
    * @param hiveContext
    * @param serviceConfMap
    * @param processCode 批处理编码
    */
  def getGroupingRules(hiveContext: HiveContext, serviceConfMap: HashMap[String, String], processCode: String = DCSConstants.CONST_GROUPING_PROC_CODE_IND) = {
    import hiveContext.implicits._
    val ruleMap = new mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]()
    val ruleMap17 = new mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]()
    serviceConfMap.put("p_proc_code", processCode)
    serviceConfMap.put("p_proc_type", DCSConstants.CONST_DCS_PROC_TYPE_GAAP)

    val sql = selectMapping(serviceConfMap.getOrElse("lar_grouping_rules", ""), serviceConfMap)
    serviceConfMap.put("p_proc_type", DCSConstants.CONST_DCS_PROC_TYPE_I17)
    val sql_17 = selectMapping(serviceConfMap.getOrElse("lar_grouping_rules", ""), serviceConfMap)
    logError(sql)
    logError(sql_17)
    if (sql.length > 1) {
      val ruleDF = hiveContext.sql(sql).as[ACT_GROUPING_RULES]
      ruleDF.collect.foreach(row => {
        val productName = row.product_name
        val rowNum = row.row_num
        if (ruleMap.contains(productName)) {
          ruleMap.getOrElse(productName, new HashMap[Int, ACT_GROUPING_RULES]()) += (rowNum.toInt -> row)
        } else {
          val map = new mutable.HashMap[Int, ACT_GROUPING_RULES]()
          map += (rowNum.toInt -> row)
          ruleMap += (productName -> map)
        }
      })
    }

//    if (sql_17.length > 0) {
//      val rule17DF = hiveContext.sql(sql_17).as[ACT_GROUPING_RULES]
//      rule17DF.collect.foreach(row => {
//        val productName = row.product_name
//        val rowNum = row.row_num
//        if (ruleMap17.contains(productName)) {
//          ruleMap17.getOrElse(productName, new HashMap[Int, ACT_GROUPING_RULES]()) += (rowNum.toInt -> row)
//        } else {
//          val map = new mutable.HashMap[Int, ACT_GROUPING_RULES]()
//          map += (rowNum.toInt -> row)
//          ruleMap17 += (productName -> map)
//        }
//      })
//    }
    (ruleMap, ruleMap17)
  }

  /**
    * 加载DCS基础信息表
    *
    * @param hiveContext
    * @param serviceConfMap 参数集合
    */
  def getDcsBasicInfo(hiveContext: HiveContext, serviceConfMap: HashMap[String, String], workSpace: String = "IND") = {
    import hiveContext.implicits._
    serviceConfMap.put("work_space", workSpace)
    val map = new mutable.HashMap[String, LAR_DCS_BASIC_INFO_AIO]()
    val sql = selectMapping(serviceConfMap.getOrElse("lar_dcs_basic_info_aio", ""), serviceConfMap)
    logError(sql)
    if (sql.length > 1) {
      val basicInfoDF = hiveContext.sql(sql).as[LAR_DCS_BASIC_INFO_AIO]
      basicInfoDF.collect.foreach(row => {
        val planCode = row.plan_code
        map += (planCode -> row)
      })
    }
    map
  }

  /**
    * grouping 配置
    *
    * @param hiveContext
    * @param serviceConfMap
    */
  def getGroupingConfig(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import hiveContext.implicits._
    val map = new mutable.HashMap[String, ACT_GROUPING_CONFIG]()
    val sql = selectMapping(serviceConfMap.getOrElse("lar_grouping_config", ""), serviceConfMap)
    if (sql.length > 1) {
      val configDF = hiveContext.sql(sql).as[ACT_GROUPING_CONFIG]
      configDF.collect.foreach(row => {
        map += (row.proc_code -> row)
      })
    }
    map
  }

  /**
    * Grouping算法配置表
    *
    * @param hiveContext
    * @param serviceConfMap
    */
  def getGroupingCalculation(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import hiveContext.implicits._
    val map = new mutable.HashMap[String, mutable.HashMap[String, ACT_GROUPING_CALCULATION]]()
    val sql = selectMapping(serviceConfMap.getOrElse("lar_grouping_calculation", ""), serviceConfMap)
    if (sql.length > 1) {
      val calculation = hiveContext.sql(sql).as[ACT_GROUPING_CALCULATION]
      calculation.collect.foreach(row => {
        val calcName = row.calc_name
        val columnName = row.column_name
        if (map.contains(calcName)) {
          map.getOrElse(calcName, new mutable.HashMap[String, ACT_GROUPING_CALCULATION]()) += (columnName -> row)
        } else {
          val columnMap = new mutable.HashMap[String, ACT_GROUPING_CALCULATION]()
          columnMap += (columnName -> row)
          map += (calcName -> columnMap)
        }
      })
    }
    map
  }

  /**
    * plan_code_g配置
    *
    * @param hiveContext
    * @param serviceConfMap
    */
  def getDcsPlanCodeGCfg(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import hiveContext.implicits._
    val map = new mutable.HashMap[String, ACT_DCS_PLAN_CODE_G_CFG]()
    val sql = selectMapping(serviceConfMap.getOrElse("lar_dcs_plan_code_g_cfg", ""), serviceConfMap)
    if (sql.length > 1) {
      val planCodeG = hiveContext.sql(sql).as[ACT_DCS_PLAN_CODE_G_CFG]
      planCodeG.collect.foreach(row => {
        map += (row.plan_code -> row)
      })
    }
    map
  }

  /**
    * dcs算法配置表
    * 按plan_code 分组，按field_seq升序排序
    *
    * @param hiveContext
    * @param serviceConfMap
    * @return
    */
  def getDcsFuncCfg(hiveContext: HiveContext, serviceConfMap: HashMap[String, String], folderIn: String = "") = {
    import hiveContext.implicits._
    serviceConfMap.put("folder_in", folderIn)
    val sql = selectMapping(serviceConfMap.getOrElse("lar_dcs_func_cfg", ""), serviceConfMap)
    logError(sql)
    val dcsFuncs = hiveContext.sql(sql).as[ACT_DCS_FUNCS_CFG]
    dcsFuncs.collect
      .groupBy(e => e.plan_code)
      .map(x => (x._1, x._2.sortBy(_.field_seq)))
  }

  /**
    * onetwo_region表
    *
    * @param hiveContext
    * @param serviceConfMap
    * @return
    */
  def getOneTwoRegion(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import hiveContext.implicits._
    val map = new mutable.HashMap[String, ACT_DCS_ONETWO_REGION]()
    val sql = selectMapping(serviceConfMap.getOrElse("onetwo_region_config", ""), serviceConfMap)
    logError(sql)
    if (sql.length > 1) {
      val onetwoRegion: Dataset[ACT_DCS_ONETWO_REGION] = hiveContext.sql(sql).as[ACT_DCS_ONETWO_REGION]
      onetwoRegion.collect.foreach(row => {
        map += (row.dno -> row)
      })
    }
    map
  }

  /**
    * 处理regon的值得问题
    *
    * @param regionCode pol_ben_ul结果表的reigon_code字段
    * @param regionBC   region_no和region的表的广播数据集
    * @param args       参数集
    */
  def getRegion(regionCode: String,
                regionBC: org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[String, REGION_CODE_CONFIG]],
                args: scala.collection.mutable.Map[String, String]) = {
    val region: String = regionBC.value.get(regionCode).get.region
    //将对应的region放入args中,供functions中计算使用
    args("REGION") = region
  }


  /**
    * 加载保单汇总数据统一入口
    *
    * @param hiveContext
    * @param serviceConfMap
    * @param sql
    * @return
    */
  def loadPolBenData(hiveContext: HiveContext, serviceConfMap: HashMap[String, String], sql: String) = {
    logError(sql)
    hiveContext.sql(sql)
  }


  /**
    * 加载ulink_price基础信息表
    *
    * @param hiveContext
    * @param serviceConfMap 参数集合
    */
  def getDcsUlPrice(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) :scala.collection.mutable.HashMap[String,String] = {
    import hiveContext.implicits._
    val priceMap:scala.collection.mutable.HashMap[String,String] = new scala.collection.mutable.HashMap[String,String]
    val sql = selectMapping(serviceConfMap.getOrElse("bas_lar_temp_ulink_price", ""), serviceConfMap)
    logError(sql)
    if (sql.length > 1) {
      val ulPrice: Dataset[LAR_TEMP_ULINK_PRICE] = hiveContext.sql(sql).as[LAR_TEMP_ULINK_PRICE]
      ulPrice.collect.foreach(row => {
        priceMap.put("PRICE1", row.units_final_price1 + "")
        priceMap.put("PRICE2", row.units_final_price2 + "")
        priceMap.put("PRICE3", row.units_final_price3 + "")
        priceMap.put("PRICE5", row.units_final_price5 + "")
        priceMap.put("PRICE8", row.units_final_price8 + "")
        priceMap.put("PRICE9", row.units_final_price9 + "")
        priceMap.put("PRICE13", row.units_final_price13 + "")
        priceMap.put("PRICE14", row.units_final_price14 + "")
      })
    }
    priceMap
  }

  /**
    * 加载表region_code_config的数据信息
    *
    * @param hiveContext
    * @param serviceConfMap 参数集
    */
  def getDcsRegion(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]): mutable.HashMap[String, REGION_CODE_CONFIG] = {
    import hiveContext.implicits._
    val map = new mutable.HashMap[String, REGION_CODE_CONFIG]()
    val sql: String = selectMapping(serviceConfMap.getOrElse("region_code_config", ""), serviceConfMap)
    logError(sql)
    if (sql.length > 1) {
      val regionCode: Dataset[REGION_CODE_CONFIG] = hiveContext.sql(sql).as[REGION_CODE_CONFIG]
      regionCode.collect.foreach(row => {
        map += (row.region_no -> row)
      })
    }
    map
  }


  /**
    * 加载汇率表 --lar_currency_rate (dcs wb 需要用到)
    *
    * @param hiveContext
    * @param serviceConfMap
    * USD --美元--03
    * GBP --英镑--06
    * AUD --澳元--07
    */
  def loadLarCurrencyRateData(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import hiveContext.implicits._
    val sql = selectMapping(serviceConfMap.getOrElse("bas_pala_lbs_currency_dcs", ""), serviceConfMap)
    logError("Rate-->" + sql)
    val map = new mutable.HashMap[String, String]()
    if (sql.length > 1) {
      val cRate: Dataset[LBS_CURRENCY_RATE] = hiveContext.sql(sql).as[LBS_CURRENCY_RATE]
      cRate.collect.foreach(row => {
        row.cno.toString match {
          case "06" => map.put("GBP", row.rate.toString)
          case "07" => map.put("AUD", row.rate.toString)
          case "03" => map.put("USD", row.rate.toString)
          /*case "06" => serviceConfMap.put("GBP","8.6762")
          case "07" => serviceConfMap.put("AUD","4.825")
          case "03" => serviceConfMap.put("USD","6.8632")*/
          case _ => serviceConfMap.put("OTHER", row.rate.toString)
        }
      })
    }
    map
  }
}


package com.an.lcloud.act.dcs.job

import com.an.lcloud.act.core.framework.comm.DCSConstants
import com.an.lcloud.act.core.framework.job.BaseJob
import com.an.lcloud.act.core.framework.schema._
import com.an.lcloud.act.core.framework.utils.{DateUtils, Utils}
import com.an.lcloud.act.dcs.udf.LoadDcsCommDataJob
import com.an.lcloud.act.dcs.utils.{ConResultObjUtils, DcsAlgCfgHandler, DcsInvalidUtils}
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.storage.StorageLevel
import utils.DcsUtils

import scala.collection.mutable.ListBuffer

/**
  * Created by ex-shencheng001 on 2019/4/2.
  *
  * TOP_888类别险种的dcs代码
  */
class DcsTop888Job(@transient sc:SparkContext,@transient hiveContext:HiveContext,serviceConfMap:scala.collection.mutable.HashMap[String,String])
  extends BaseJob(sc:SparkContext,hiveContext:HiveContext,serviceConfMap:scala.collection.mutable.HashMap[String,String]){

    override def run(): Unit ={
      super.init()

      val startTime:BigDecimal = System.currentTimeMillis()

      //解析p_end_date参数
      DcsUtils.setDateParams(serviceConfMap)

      //读取TOP888的dcs加载数据的xml文件
      readXml(DCSConstants.CONST_XML_FILE_PATH_BASE)
      readXml(DCSConstants.CONST_XML_FILE_PATH_TOP888)

      //从hive表中加载算法的配置参数
//      val configParams =LoadDcsCommDataJob.getDcsFuncTop888Cfg(hiveContext,serviceConfMap)
      val configParams =LoadDcsCommDataJob.getDcsFuncCfg(hiveContext,serviceConfMap,DCSConstants.CONST_DCS_FOLDER_IN_TOP_888)
      val comParams = sc.broadcast(configParams)

      //加载并广播groupingRules表相关数据
      val groupingRules = LoadDcsCommDataJob.getGroupingRules(hiveContext, serviceConfMap,DCSConstants.CONST_GROUPING_PROC_CODE_TOP888)
      val groupingRuleBC = sc.broadcast(groupingRules._1)
      val groupingRule17BC = sc.broadcast(groupingRules._2)

      //加载并广播表lar_dcs_basic_info_aio的数据
      val basicInfoBC = sc.broadcast(LoadDcsCommDataJob.getDcsBasicInfo(hiveContext,serviceConfMap,DCSConstants.CONST_DCS_WORK_SPACE_TOP_888))

      //加载ulink_price投连价格表,并将数据存入serviceConfMap中
      val priceBC:org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[String,String]] =
        sc.broadcast(LoadDcsCommDataJob.getDcsUlPrice(hiveContext,serviceConfMap))
      //加载表region_code_config到serviceConfMap中,并广播出去
      val regionBC:org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[String,REGION_CODE_CONFIG]] =
        sc.broadcast(LoadDcsCommDataJob.getDcsRegion(hiveContext,serviceConfMap))

      //加载bas_pala_pol_ben_ul数据
      val sql = selectMapping(serviceConfMap.getOrElse("bas_pala_pol_ben_ul", ""), serviceConfMap)
      val initRDD =
        LoadDcsCommDataJob.loadPolBenData(hiveContext,serviceConfMap,sql)
          .repartition(serviceConfMap.getOrElse("","480").toInt).persist(StorageLevel.MEMORY_AND_DISK)


      val filteredRDD =  initRDD.map(row => {
        //创建一个对象
        val obj: BAS_PALA_POL_BEN_UL = new BAS_PALA_POL_BEN_UL
        //通过对象获取这个class文件的全路径名
        val ulClassName: String = obj.getClass.getName
        //利用反射获取bas_pala_pol_ben_ul的类对象
        val ulClassObj: Class[_] = Class.forName(ulClassName)
        //遍历出BAS_PALA_POL_BEN_UL类中的所有属性
        val fields = ulClassObj.getDeclaredFields

        fields.foreach(field => {
          field.setAccessible(true)
          val fieldName: String = field.getName.toLowerCase
          field.set(obj, String.valueOf(row.getAs[String](fieldName)))
        })

        val x = obj
        def nullToZero(str: String): String = if (str.isEmpty || str.equals("null")) "0" else str
        //将数字类型的为""的字段出事值赋值为"0"
        x.sum_ins = nullToZero(x.sum_ins)
        x.unit = nullToZero(x.unit)
        x.tot_modal_prem = nullToZero(x.tot_modal_prem)
        x.ins_age = nullToZero(x.ins_age)
        x.period = nullToZero(x.period)
        x.prem_term = nullToZero(x.prem_term)
        x.payout_age = nullToZero(x.payout_age)
        x.units_number1 = nullToZero(x.units_number1)
        x.tup_number1 = nullToZero(x.tup_number1)
        x.units_number2 = nullToZero(x.units_number2)
        x.tup_number2 = nullToZero(x.tup_number2)
        x.units_number3 = nullToZero(x.units_number3)
        x.tup_number3 = nullToZero(x.tup_number3)
        x.units_number5 = nullToZero(x.units_number5)
        x.tup_number5 = nullToZero(x.tup_number5)
        x.units_number8 = nullToZero(x.units_number8)
        x.tup_number8 = nullToZero(x.tup_number8)
        x.units_number9 = nullToZero(x.units_number9)
        x.tup_number9 = nullToZero(x.tup_number9)
        x.units_number13 = nullToZero(x.units_number13)
        x.tup_number13 = nullToZero(x.tup_number13)
        x.units_number14 = nullToZero(x.units_number14)
        x.tup_number14 = nullToZero(x.tup_number14)
        x.ann_std_prem = nullToZero(x.ann_std_prem)
        x.std_modal_prem = nullToZero(x.std_modal_prem)
        x.sum_assd_ci = nullToZero(x.sum_assd_ci)
        x.sum_assd_523 = nullToZero(x.sum_assd_523)
        x.sum_assd_524 = nullToZero(x.sum_assd_524)
        x.sum_assd_529 = nullToZero(x.sum_assd_529)
        x.sum_assd_530 = nullToZero(x.sum_assd_530)
        x.sum_assd_552 = nullToZero(x.sum_assd_552)
        x.transfer_value = nullToZero(x.transfer_value)
        x.tot_modal_prem1 = nullToZero(x.tot_modal_prem1)
        x.alloc_prem1 = nullToZero(x.alloc_prem1)
        x.tot_modal_prem2 = nullToZero(x.tot_modal_prem2)
        x.alloc_prem2 = nullToZero(x.alloc_prem2)
        x.ins_age_834 = nullToZero(x.ins_age_834)
        x.period_834 = nullToZero(x.period_834)
        x.ins_age_835 = nullToZero(x.ins_age_835)
        x.period_835 = nullToZero(x.period_835)
        x.period_836 = nullToZero(x.period_836)
        x
      })
        .filter(x => {
          val basicInfoMap = basicInfoBC.value
          val ruler00 = basicInfoMap.keySet.contains(x.plan_code)
          var ruler01 = true
          if (ruler00) {
            val basic_info = basicInfoMap.get(x.plan_code).get
            ruler01 = basic_info.run_in_dcs != "N" && basic_info.work_space == "ULINK"
          }
          ruler00 && ruler01
        })

      logError("filter.count() = " + filteredRDD.count())
      val resultRDD =
        filteredRDD
          .mapPartitions(pol_ben_uls => {

            val list = new ListBuffer[(ListBuffer[Row],ListBuffer[Row])]
            //有效保单数据集
            val listEff = new ListBuffer[Row]
            //无效保单数据集
            val listInvalid = new ListBuffer[Row]
            pol_ben_uls.foreach(r => {

              val pol_ben_ul = r

              val now = DateUtils.nowDateTime().toString(DateUtils.YYYY_MM_DD_HHDDSS)

              val planCode = pol_ben_ul.plan_code
              val basic_info = basicInfoBC.value.get(planCode).get
              val args = scala.collection.mutable.Map[String, String]()
              val results = scala.collection.mutable.Map[String, String]()
              //无效保单数据Row集
              val invalidResults = new ListBuffer[scala.collection.mutable.Map[String, String]]

              args.++=(Utils.obj2MapOfCaseClass(basic_info))
              args.++=(Utils.obj2MapOfCaseClass(pol_ben_ul))
              results ++= (Utils.obj2MapOfCaseClass(pol_ben_ul))
              args.++=(serviceConfMap)
              //price价格表数据加入到args参数中
              args.++= (priceBC.value)
              //将该条数据对应的region值存入到args中
              LoadDcsCommDataJob.getRegion(pol_ben_ul.region_code,regionBC,args)

              args("SPCODE_SELECTION") = "0"
              args("INVALID") = "false"
              args("NEXT_RECORD") = "false"
              /** 各算法调用，获取rdd,通过java反射为对象赋值 */
              results("CREATED_BY") = ""
              results("CREATED_DATE") = now
              results("UPDATED_BY") = ""
              results("UPDATED_DATE") = now
              results("UNITS") = args("UNIT")

              //logError("invokeDcsUlAlg开始进入
              DcsAlgCfgHandler.invokeDcsTop888Alg(comParams.value,
                                                  args,
                                                  pol_ben_ul.plan_code,
                                                  groupingRuleBC.value,
                                                  results,
                                                  invalidResults)
              //            logError("invokeDcsUlAlg结束进入")

              //针对每条数据构建结果对象
              if (args("INVALID") == "false" && args("NEXT_RECORD") == "false") {
                val outPutPath1: String = args("p_proc_month")
                //output_path中的第二个值
                //val outPutPath2: String = (comParams.value.get(results("PLAN_CODE")).get) (0).folder_in
                val outPutPath2:String = "TOP"
                val outPutPath :String = outPutPath1 + "\\" + outPutPath2 + "\\"
                val spcodeSelection:String = args("SPCODE_SELECTION")
                var entryYear:Int = results("ENTRY_YEAR").toInt
                var entryMonth:Int = results("ENTRY_MONTH").toInt
                val entryDateNum:Int = entryMonth  + entryYear * 100

                //剔除初始账户价值的无效信息的数据
                if(BigDecimal(results("INIT_UNFDU_A")) != 0) {

                  //SPCODE_SELECTION=0的三批输出
                  if(spcodeSelection == "0") {
                    /**output_path中的第一个值*/
                    /**第一批写出常规SNO*/
                    ConResultObjUtils.conResObj[ACT_DCS_RESULT_TOP](outPutPath + "NORMAL",results("SPCODE"),results("PROD_NAME"),groupingRuleBC.value,results,args,listEff,new ACT_DCS_RESULT_TOP)
                    /**第二批过去一年NB保单MP--偿二代EV需求*/
                    if ((entryDateNum > results("VAL_CODE").toInt - 100) && (entryDateNum <= args("VAL_CODE").toInt) && (results("POL_STATUS") == "1")){
                      ConResultObjUtils.conResObj[ACT_DCS_RESULT_TOP](outPutPath + "SII_EV",results("SPCODE"),results("PROD_NAME"),groupingRuleBC.value,results,args,listEff,new ACT_DCS_RESULT_TOP)
                    }

                    /**第三批分产品MP的输出（区分一二元）*/
                    ConResultObjUtils.conResObj[ACT_DCS_RESULT_TOP](outPutPath + "KAOHE",results("SPCODE2"),results("PROD_NAME"),groupingRuleBC.value,results,args,listEff,new ACT_DCS_RESULT_TOP)
                  }
                }
              }
              //无效保单数据计算
              if(args("INVALID") == "true"){
                  val dcsCategory =  args("SPCODE_SELECTION") match {
                    case "0" => DCSConstants.CONST_DCS_CATEGORY_TOP888
                    case "1" => DCSConstants.CONST_DCS_CATEGORY_TOP888
                    case "2" => DCSConstants.CONST_DCS_CATEGORY_TOP888_5CH
                    case _ => "1"
                  }
                DcsInvalidUtils.InvalidDataPack(invalidResults,listInvalid,dcsCategory)
              }

            })
            list.append((listEff,listInvalid))
            list.iterator
          })
      logError("invalidCount:" + (resultRDD.map(x=> x._2).flatMap(x =>x)))

      logError("输出到hive表....")

      //输出到临时目录
      val hdfsPath = serviceConfMap.getOrElse(DCSConstants.FILE_HDFS_PATH_TOP888, "/apps-data/hduser1508/sx_hx_safe/act_dcs_result_ul") + "_" + System.currentTimeMillis()
      DcsInvalidUtils.dataOutput(sc,hiveContext,serviceConfMap,resultRDD,hdfsPath,DCSConstants.CONST_DCS_TOP888_TASK)

      initRDD.unpersist(true)


      logError("输出到FTP HDFS目录....")
      logError("整体任务耗时：" + (System.currentTimeMillis - startTime) / 1000 + " 秒")
      val endTime = System.currentTimeMillis
      logError("耗时：" + (endTime - startTime) / 1000 + " 秒")
    }
}

package com.an.lcloud.act.dcs.job

import com.an.lcloud.act.core.framework.comm.DCSConstants
import com.an.lcloud.act.core.framework.job.BaseJob
import com.an.lcloud.act.core.framework.schema._
import com.an.lcloud.act.core.framework.utils.{DateUtils, Utils}
import com.an.lcloud.act.dcs.udf.{Dcs651Functions, LoadDcsCommDataJob}
import com.an.lcloud.act.dcs.utils.{BigDecimaAccumlatorParam, ConResultObjUtils, DcsAlgCfgHandler, DcsInvalidUtils}
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.storage.StorageLevel
import org.apache.spark.{Accumulator, SparkContext}
import org.joda.time.DateTime
import org.joda.time.format.DateTimeFormat
import utils.DcsUtils

import scala.collection.mutable.ListBuffer

/**
  * Created by EX-SHENCHENG001 on 2019/4/12.
  */
class Dcs651Job(@transient sc: SparkContext,@transient hiveContext: HiveContext, serviceConfMap: scala.collection.mutable.HashMap[String,String])
  extends BaseJob(sc:SparkContext,hiveContext:HiveContext,serviceConfMap:scala.collection.mutable.HashMap[String,String]) {
  override def run(): Unit = {

    super.init()

    val startTime: BigDecimal = System.currentTimeMillis()

    //解析p_end_date参数
    DcsUtils.setDateParams(serviceConfMap)

    //读取651的dcs加载数据的xml文件
    readXml(DCSConstants.CONST_XML_FILE_PATH_BASE)
    readXml(DCSConstants.CONST_XML_FILE_PATH_651)

    //从hive表中加载算法的配置参数
    val configParams = LoadDcsCommDataJob.getDcsFuncCfg(hiveContext, serviceConfMap,DCSConstants.CONST_DCS_FOLDER_IN_651)
    val comParams = sc.broadcast(configParams)

    //加载并广播groupingRules表相关数据
    val groupingRules = LoadDcsCommDataJob.getGroupingRules(hiveContext, serviceConfMap,DCSConstants.CONST_GROUPING_PROC_CODE_651)
    val groupingRuleBC = sc.broadcast(groupingRules._1)
    val groupingRule17BC = sc.broadcast(groupingRules._2)
    //加载并广播表lar_dcs_basic_info_aio的数据
//    val basicInfoBC = sc.broadcast(LoadDcsCommDataJob.getDcsBasicInfo(hiveContext, serviceConfMap))

    //加载ulink_price投连价格表,并将数据存入serviceConfMap中
//    sc.broadcast(LoadDcsCommDataJob.getDcsUlPrice(hiveContext, serviceConfMap))
    //加载表region_code_config到serviceConfMap中,并广播出去
//    val regionBC: org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[String, REGION_CODE_CONFIG]]
//    = sc.broadcast(LoadDcsCommDataJob.getDcsRegion(hiveContext, serviceConfMap))

    //加载bas_pala_pol_ben_651数据
    val param:BigDecimaAccumlatorParam = new BigDecimaAccumlatorParam()
    val accountBalTot1:Accumulator[BigDecimal] = sc.accumulator(BigDecimal(0),"accountBalTot1")(param)
    val accountBalTot2:Accumulator[BigDecimal] = sc.accumulator(BigDecimal(0),"accountBalTot2")(param)

    val sql = selectMapping(serviceConfMap.getOrElse("bas_pala_pol_ben_651", ""), serviceConfMap)
    val initRDD =
      LoadDcsCommDataJob.loadPolBenData(hiveContext, serviceConfMap,sql)
        .repartition(serviceConfMap.getOrElse("", "480").toInt).persist(StorageLevel.MEMORY_AND_DISK)

    val filteredRDD = initRDD.map(row => {
      //创建一个对象
      val obj: BAS_PALA_POL_BEN_651 = new BAS_PALA_POL_BEN_651
      //通过对象获取这个class文件的全路径名
      val gbs651ClassName:String = obj.getClass.getName
      //利用反射获取bas_pala_pol_ben_651的类对象
      val gbs651ClassObj: Class[_] = Class.forName(gbs651ClassName)
      //遍历出BAS_PALA_POL_BEN_651类中的所有属性
      val fields = gbs651ClassObj.getDeclaredFields

      fields.foreach(field => {
        field.setAccessible(true)
        val fieldName: String = field.getName.toLowerCase
        field.set(obj, String.valueOf(row.getAs[String](fieldName)))
      })

      val x = obj

      def nullToZero(str: String): String = if (str.isEmpty || str.equals("null")) "0" else str

      //将数字类型的为""的字段出事值赋值为"0"
      x.prem_psns = nullToZero(x.prem_psns)
      x.units = nullToZero(x.units)
      x.sum_ins = nullToZero(x.sum_ins)
      x.ins_age = nullToZero(x.ins_age)
      x.period = nullToZero(x.period)
      x.ann_std_prem = nullToZero(x.ann_std_prem)
      x.prem_total = nullToZero(x.prem_total)
      x.modal_total_prem = nullToZero(x.modal_total_prem)
      x.pay_age = nullToZero(x.pay_age)
      x.legal_resv_amt = nullToZero(x.legal_resv_amt)
      x.loan_iss_amt = nullToZero(x.loan_iss_amt)
      x.loan_int_acc = nullToZero(x.loan_int_acc)
      x.apl_iss_amt = nullToZero(x.apl_iss_amt)
      x.apl_int_acc = nullToZero(x.apl_int_acc)
      x.surv_acc_prc = nullToZero(x.surv_acc_prc)
      x.surv_acc_int = nullToZero(x.surv_acc_int)
      x.div_acc_prp = nullToZero(x.div_acc_prp)
      x.div_acc_int = nullToZero(x.div_acc_int)

      x
    })
      logError("filteredRDD.count:"+filteredRDD.count())

    filteredRDD.mapPartitions(pol_ben_651s => {
      getAdjustRatio651(pol_ben_651s,accountBalTot1,accountBalTot2)
      pol_ben_651s
    }).collect()

    //计算变量args("adjustRatio651")的值
    val adjustRatio651:String = (accountBalTot1.value /accountBalTot2.value).toString

    val resultRDD = filteredRDD
      .mapPartitions(pol_ben_651s => {
        //有效保单数据存储
        val listEff:ListBuffer[Row] = new ListBuffer[Row]
        //invalid保单数据存储
        val listInvi:ListBuffer[Row] = new ListBuffer[Row]

        val list = new ListBuffer[(ListBuffer[Row],ListBuffer[Row])]

        pol_ben_651s.foreach(r => {
          val pol_ben_651 = r
          val now = DateUtils.nowDateTime().toString(DateUtils.YYYY_MM_DD_HHDDSS)
          //创建结果map及计算的参数map并将计算的数据存入其中
          val args = scala.collection.mutable.Map[String, String]()
          val results = scala.collection.mutable.Map[String, String]()
          args.++=(Utils.obj2MapOfCaseClass(pol_ben_651))
          results ++= (Utils.obj2MapOfCaseClass(pol_ben_651))
          args.++=(serviceConfMap)

          /**Invalid_log add  添加变量invalidResults存储invalid_log所需的值*/
          val invalidResults = new ListBuffer[scala.collection.mutable.Map[String, String]]
          //参数初始化
          args("INVALID") = "false"
          args("NEXT_RECORD") = "false"
          args("MAX_AGE") = "106"
          args("MIN_AGE") = "0"
          args("intRateJiujiu") = "0.021"
          args("adjustRation651") = adjustRatio651
          args("POLNO") = args("CERTNO")
          args("NOW_BALANCE") = args("LEGAL_RESV_AMT")
          //给默认的结果字段赋值
          results("CREATED_BY") = ""
          results("CREATED_DATE") = now
          results("UPDATED_BY") = ""
          results("UPDATED_DATE") = now
          results("POLNO") = results("CERTNO")
            //logError("invokeDcs651Alg开始进入")
          DcsAlgCfgHandler.invokeDcs651Alg(comParams.value,
              args,
              pol_ben_651.plan_code,
              groupingRuleBC.value,
              results,
              invalidResults)
            //logError("invokeDcs651Alg结束进入")

          //针对每条数据构建结果对象
          if (args("INVALID") == "false" && args("NEXT_RECORD") == "false") {
            val outPutPath:String = args("p_proc_month") + "\\" + "651"
            if(List("2", "3", "4", "5").contains(results("PREM_TYPE"))){
              results("PREM_TYPE") = "1"
            }
            if (List("1","6").contains(results("PREM_TYPE"))) {
              results("PREM_TYPE") = "0"
            }
            //路径一输出
            ConResultObjUtils.conResObj[ACT_DCS_RESULT_651](outPutPath + "\\" + "NORMAL",outPutPath + "\\" + "NORMAL_BF_ADJUST",results("SPCODE"),results("PROD_NAME"),groupingRuleBC.value,results,args,listEff,new ACT_DCS_RESULT_651)
            //路径二输出:以下语句为分一二元考核 MP输出
            ConResultObjUtils.conResObj[ACT_DCS_RESULT_651](outPutPath + "\\" + "KAOHE",outPutPath + "\\" + "KAOHE_BF_ADJUST",results("SPCODE2"),results("PROD_NAME"),groupingRuleBC.value,results,args,listEff,new ACT_DCS_RESULT_651)

          }
          //invalid 保单数据的存储
          if(args("INVALID") == "true"){
            DcsInvalidUtils.InvalidDataPack(invalidResults,listInvi,DCSConstants.CONST_DCS_CATEGORY_651)
          }
        })
        list.append((listEff,listInvi))

        list.iterator
      })


    logError("invalidCount:" + (resultRDD.map(x=> x._2).flatMap(x =>x)))
    logError("输出到hive表....")

    //输出到临时目录
    val hdfsPath = serviceConfMap.getOrElse(DCSConstants.FILE_HDFS_PATH_651, "/apps-data/hduser1508/sx_hx_safe/act_dcs_result_651") + "_" + System.currentTimeMillis()
    //输出数据到表中
    DcsInvalidUtils.dataOutput(sc,hiveContext,serviceConfMap,resultRDD,hdfsPath,DCSConstants.CONST_DCS_651_TASK)
    initRDD.unpersist(true)
    logError("输出到FTP HDFS目录....")
    val endTime = System.currentTimeMillis
    logError("耗时：" + (endTime - startTime) / 1000 + " 秒")
  }

  /**
    * 计算参数args("adjustRatio651")的方法
    *
    * @param pol_ben_651s
    * @return
    */
  def getAdjustRatio651(pol_ben_651s:Iterator[BAS_PALA_POL_BEN_651],
                        accountBalTot1:Accumulator[BigDecimal],
                        accountBalTot2:Accumulator[BigDecimal]) = {
    //   定义参数累加器ACCOUNT_BAL_TOT1 = accountBalTol1,ACCOUNT_BAL_TOT2 = accountBalTol2



    pol_ben_651s.foreach(r=>{
      val pol_ben_651 = r
      val planCode = pol_ben_651.plan_code
      val argsTemp = scala.collection.mutable.Map[String, String]()
      argsTemp.++=(Utils.obj2MapOfCaseClass(pol_ben_651))
      argsTemp.++=(serviceConfMap)
      //定义变量
      argsTemp("MAX_AGE") = "106"
      argsTemp("MIN_AGE") = "0"
      argsTemp("NOW_BALANCE") = argsTemp("LEGAL_RESV_AMT")
      //获取eff_date字段及val_date字段的时间戳
      val effDate: Long = DateTime.parse(argsTemp("EFF_DATE").substring(0, 10), DateTimeFormat.forPattern("yyyy-MM-dd")).getMillis
      val valDate: Long = DateTime.parse(argsTemp("VAL_DATE"), DateTimeFormat.forPattern("yyyy-MM-dd")).getMillis

      if (effDate - valDate <= 0 && !(List("N", "S", "M", "F", "D", "C", "X").contains(argsTemp("BEN_STS")))) {
        val nowBalance: BigDecimal = (BigDecimal)(argsTemp("NOW_BALANCE"))
        accountBalTot1.add(nowBalance)
        val func651 = new Dcs651Functions
        //计算中间标量的值
        argsTemp("POL_STATUS") = func651.POL_STATUSaaa(argsTemp)
        argsTemp("POL_TERM_Y") = func651.POL_TERM_Yaaa(argsTemp)
        argsTemp("PREM_PAYBL_Y") = func651.PREM_PAYBL_Yaaa(argsTemp)
        argsTemp("RETIRE_AGE") = func651.RETIRE_AGEaaa(argsTemp)

        val polTermY = BigDecimal(argsTemp("POL_TERM_Y"))
        val premPayblY = BigDecimal(argsTemp("PREM_PAYBL_Y"))
        val retireAge = BigDecimal(argsTemp("RETIRE_AGE"))
        //获取条件值
        val payDateBool: Boolean = argsTemp("PAY_DATE").isEmpty || argsTemp("PAY_DATE") == null || argsTemp("PAY_DATE") ==""  && argsTemp("PAY_DATE") =="NULL" || argsTemp("PAY_DATE") =="null"
        val benSts: Boolean = !List("I", "P", "W", "R", "L", "V").contains(argsTemp("BEN_STS"))
        val premPsns: Boolean = argsTemp("PREM_PSNS") == "0"
        val premType: Boolean = !List("1", "2", "3", "4", "5", "6").contains(argsTemp("PREM_TYPE"))
        val insAge: BigDecimal = BigDecimal(argsTemp("INS_AGE"))

        // 条件过滤 来计算累加器accountBalTot2的值
        if (!payDateBool && !benSts && !premPsns && !premType && nowBalance >= 0
          && (insAge >= 0 && insAge <= 106) && ((insAge + premPayblY <= 106)
          && premPayblY >= 1) && retireAge <= 106 && (insAge + polTermY <= 106 && polTermY > 0)) {
          accountBalTot2.add(nowBalance)
        }
      }
    })
  }

}

package com.an.lcloud.act.dcs.utils

import com.an.lcloud.act.core.framework.schema.{ACT_GROUPING_RULES, LAR_DCS_BASIC_INFO_AIO}
import com.an.lcloud.act.core.framework.utils.Utils
import com.an.lcloud.act.dcs.udf.DcsGroupIdGenerator
import org.apache.spark.sql.Row

import scala.collection.mutable

/**
  * dcs结果数据生成数据方法
  *
  * Created by EX-SHENCHENG001 on 2019/5/12.
  */
object ConResultObjUtils {
  /**
    * 处理prod_name的特殊问题
    *
    * @param prodName prod_name字段的值,有三种:字段proc_name+"A",字段proc_name+"B",字段proc_name+"C"
    * @param basicObj
    * @param args     参数集,用来存储后续functions用的参数
    */
  def getProdName(prodName: String, basicObj: Option[LAR_DCS_BASIC_INFO_AIO], args: scala.collection.mutable.Map[String, String]): Unit = {
    var prodNameTemp: String = if (basicObj.isEmpty) {
      ""
    } else {
      basicObj.get.prod_name
    }
    args(s"$prodName") = prodNameTemp
  }


  /**
    *为每条结果数据创建一个对象--针对group_id的计算路径和output_path的结果路径不同的计算方法
    *
    * @param outPutPath   output_path的值
    * @param groupIDPath   计算group_id的output_path
    * @param spcode        字段spcode的值
    * @param prod_name     字段prod_name的计算值
    * @param groupingRules grouprules相关的数据集
    * @param results       计算的结果数据集
    * @param args          计算的参数集
    * @param list          存储的输出的结果数据
    * @param resultBean    对应的结果类对象
    * @tparam T             对应的结果类的泛型
    */
  def conResObj[T](outPutPath:String,groupIDPath:String,
                spcode:String,prod_name:String,
                groupingRules: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]],
                results:scala.collection.mutable.Map[String,String],
                args:scala.collection.mutable.Map[String,String],
                list:scala.collection.mutable.ListBuffer[Row],
                   resultBean:T): Unit ={
    Utils.map2obj(results.toMap, resultBean)
    //计算groupID方法
    getGroupId(args,groupIDPath,groupingRules,results)
    val fieldSNO = Class.forName(resultBean.getClass.getName).getDeclaredFields
    val valueSNO = fieldSNO.map(f => {
      f.setAccessible(true)
      f.getName match {
        case "output_path" => f.set(resultBean,outPutPath)
        case "spcode" => f.set(resultBean,spcode)
        case "prod_name" => f.set(resultBean,prod_name)
        case "group_id" => f.set(resultBean,results("GROUP_ID"))
        case _ => 1
      }
      f.get(resultBean)

    }).toSeq
    list += (Row.fromSeq(valueSNO))
  }

  /**
    *为每条结果数据创建一个对象--针对group_id的计算路径和output_path的结果路径相同的计算方法
    *
    * @param outPutPath   output_path的值
    * @param spcode        字段spcode的值
    * @param prod_name     字段prod_name的计算值
    * @param groupingRules grouprules相关的数据集
    * @param results       计算的结果数据集
    * @param args          计算的参数集
    * @param list          存储的输出的结果数据
    * @param resultBean    对应的结果类对象
    * @tparam T             对应的结果类的泛型
    */
  def conResObj[T](outPutPath:String,
                   spcode:String,prod_name:String,
                   groupingRules: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]],
                   results:scala.collection.mutable.Map[String,String],
                   args:scala.collection.mutable.Map[String,String],
                   list:scala.collection.mutable.ListBuffer[Row],
                   resultBean:T): Unit ={
    Utils.map2obj(results.toMap, resultBean)
    //计算groupID方法
    getGroupId(args,outPutPath,groupingRules,results)
    val fieldSNO = Class.forName(resultBean.getClass.getName).getDeclaredFields
    val valueSNO = fieldSNO.map(f => {
      f.setAccessible(true)
      f.getName match {
        case "output_path" => f.set(resultBean,outPutPath)
        case "spcode" => f.set(resultBean,spcode)
        case "prod_name" => f.set(resultBean,prod_name)
        case "group_id" => f.set(resultBean,results("GROUP_ID"))
        case _ => 1
      }
      f.get(resultBean)

    }).toSeq
    list += (Row.fromSeq(valueSNO))
  }

  /**
    *group_id计算--正对group_id的计算路径和output_path的结果路径不同的计算方法
    *
    * @param args
    * @param groupIDPath
    * @param groupingRules
    * @param results
    */
  def getGroupId(args: mutable.Map[String, String],
                 groupIDPath:String,
                 groupingRules: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]],
                 results: scala.collection.mutable.Map[String, String]) = {
    //计算groupID方法
    args("OUTPUT_PATH") = groupIDPath
  //调用方法计算group_id
    args("GROUP_ID") = if (groupingRules.keySet.contains(args("PROD_NAME"))) {
      DcsGroupIdGenerator.getGroupId(groupingRules, args)
    } else {
      ""
    }
    results("GROUP_ID") = args("GROUP_ID")
  }



}

package com.an.lcloud.act.dcs.utils

import com.an.lcloud.act.core.framework.schema.{ACT_DCS_FUNCS_CFG, ACT_GROUPING_RULES, LAR_DCS_BASIC_INFO_AIO}
import com.an.lcloud.act.dcs.udf._
import org.apache.spark.Logging

import scala.collection.mutable
import scala.collection.mutable.ListBuffer

/**
  * Created by EX-ZHANGYONGTIAN001 on 2018-11-29.
  *
  * @define DCS算法配置处理
  */
object DcsAlgCfgHandler extends Serializable with Logging {


  /**
    * DCS算法配置解析动态调用
    *
    * @param functionMap 算法顺序
    * @param args        保单数据
    * @param plan_code   险种代码
    */
  def invokeDcsAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]], args: scala.collection.mutable.Map[String, String], plan_code: String, groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]
                   , basicInfo: mutable.HashMap[String, LAR_DCS_BASIC_INFO_AIO],
                   invalidResults: ListBuffer[mutable.Map[String, String]]): Unit = {


    val obj = new DcsIndFunctions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap


    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }

    val cfgArr = functionMap.get(plan_code).get

    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        //args("PROC_DATE")
        args("OUTPUT_PATH") = args("p_proc_month") + "\\" + cfg.folder_in + "\\" + cfg.output_path
        args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
        val a = cfg.arg1.trim
        val b = cfg.arg2.trim

        val method = methods.get(cfg.field_alg).get
        if (cfg.field_alg == "GROUP_IDaaa1") {

        } else if (args("PLAN_CODE") == "996" && cfg.field_alg == "SUM_ASSD_Maaa2") {
          //临时测试996险种：算法为SUM_ASSD_Maaa2时改调用算法SUM_ASSD_Maaa1
          methods.get("SUM_ASSD_Maaa1").get.invoke(obj, args, basicInfo, invalidResults)

        } else if (args("PLAN_CODE") == "1290K" && cfg.field_alg == "COMM_INDEXaaa1") {
          //临时测试1290k：算法为COMM_INDEXaaa1时改调用COMM_INDEXaaa14
          //                  methods.get("COMM_INDEXaaa14").get.invoke(obj, args, results,basicInfo)
        } else {
          if (a == "-" && b == "-") {
            method.invoke(obj, args, basicInfo, invalidResults)
          } else {
            method.invoke(obj, a, b, args, basicInfo, invalidResults)
          }
        }
      }
    }
    //临时测试1290k：算法为COMM_INDEXaaa1时改调用COMM_INDEXaaa14
    if (args("PLAN_CODE") == "1290K") {
      methods.get("COMM_INDEXaaa14").get.invoke(obj, args, basicInfo, invalidResults)
    }

    if (methods.keySet.contains("GROUP_IDaaa1")) {
      methods.get("GROUP_IDaaa1").get.invoke(obj, args, groupingRulesMap, basicInfo)
    }

    if (args("POL_STATUS") == "2") {
      args("POL_STATUS") = "1"
    }

  }

  /**
    * DCS Uv算法配置解析动态调用
    *
    * @param functionMap 算法顺序
    * @param args        保单数据
    * @param plan_code   险种代码
    */
  def invokeDcsUvAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]], args: scala.collection.mutable.Map[String, String], plan_code: String, groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]
                     , results: scala.collection.mutable.Map[String, String], invalidResults: ListBuffer[scala.collection.mutable.Map[String, String]]): Unit = {
    val obj = new DcsUvFunctions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap
    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }
    val cfgArr = functionMap.get(plan_code).get
    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        /** Invalid记录可以继续执行方法,next_record退出 */
        if (args("NEXT_RECORD") == "false") {
          args("OUTPUT_PATH") = args("p_proc_month") + "\\" + cfg.folder_in + "\\" + cfg.output_path
          args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
          results("OUTPUT_PATH") = args("OUTPUT_PATH")
          results("SUBPARTITION_ID") = args("SUBPARTITION_ID")
          val a = cfg.arg1.trim
          val b = cfg.arg2.trim
          val method = methods.get(cfg.field_alg).get
          if (cfg.field_alg == "GROUP_IDaaa1") {
            //          methods.get("SPCODEaaa1").get.invoke(obj, args)
            //          methods.get("POL_STATUSaaa1").get.invoke(obj, args)
            //          methods.get(cfg.field_alg).get.invoke(obj, args, groupingRulesMap)
          } else {
            if (a == "-" && b == "-") {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, args, results, invalidResults)
            } else {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, a, b, args, results, invalidResults)
            }
          }
        }
      }
    }
    /** Invalid记录和,next_record记录不用算group_id */
    if (args("NEXT_RECORD") == "false" && args("INVALID") == "false") {
      if (methods.keySet.contains("GROUP_IDaaa1")) {
        methods.get("GROUP_IDaaa1").get.invoke(obj, args, groupingRulesMap, results)
      }
      if (args("POL_STATUS") == "2") {
        args("POL_STATUS") = "1"
        results("POL_STATUS") = "1"
      }
    }
  }

  /**
    * DCS-guar算法配置解析动态调用
    *
    * @param functionMap 算法顺序
    * @param args        保单数据
    * @param plan_code   险种代码
    */
  def invokeDcsGuarAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]], args: scala.collection.mutable.Map[String, String], plan_code: String, groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]
                       , results: scala.collection.mutable.Map[String, String], invalidResults: ListBuffer[mutable.Map[String, String]]): Unit = {
    val obj = new DcsGuarFunctions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap
    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }
    val cfgArr = functionMap.get(plan_code).get
    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        args("OUTPUT_PATH") = args("p_proc_month") + "\\" + cfg.folder_in + "\\" + cfg.output_path
        args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
        results("OUTPUT_PATH") = args("OUTPUT_PATH")
        results("SUBPARTITION_ID") = args("SUBPARTITION_ID")
        val a = cfg.arg1.trim
        val b = cfg.arg2.trim
        /** Invalid_log add  NEXT_RECORD为true时就不执行后续方法，Invalid继续执行后续方法 */
        if (args("NEXT_RECORD") == "false") {
          val a = cfg.arg1.trim
          val b = cfg.arg2.trim
          val method = methods.get(cfg.field_alg).get
          if (cfg.field_alg == "GROUP_IDaaa1") {

          } else {
            if (a == "-" && b == "-") {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, args, results, invalidResults)
            } else {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, a, b, args, results, invalidResults)
            }
          }
        }
      }
    }

    if (methods.keySet.contains("GROUP_IDaaa1")) {
      methods.get("GROUP_IDaaa1").get.invoke(obj, args, groupingRulesMap, results, invalidResults)
    }


  }

  /**
    * UL类别险种dcs算法解析
    *
    * @param functionMap      ul类别险种dcs算法字段的计算算子
    * @param args             保单数据
    * @param plan_code        ul ul类别的险种
    * @param groupingRulesMap grouping相关的计算规则
    * @param results          dcs计算的结果数据集
    *
    */
  def invokeDcsUlAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]],
                     args: mutable.Map[String, String],
                     plan_code: String,
                     groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]],
                     results: mutable.Map[String, String],
                     invalidResults: ListBuffer[scala.collection.mutable.Map[String, String]]): Unit = {

    val obj = new DcsUlFunctions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap


    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }

    val cfgArr = functionMap.get(plan_code).get

    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        //          //args("PROC_DATE")
        //          args("OUTPUT_PATH") = args("p_proc_month") + "\\" + cfg.folder_in + "\\" + cfg.output_path
        args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
        //          results("OUTPUT_PATH") = args("OUTPUT_PATH")
        results("SUBPARTITION_ID") = args("SUBPARTITION_ID")
        val a = cfg.arg1.trim
        val b = cfg.arg2.trim

        val method = methods.get(cfg.field_alg).get
        if (cfg.field_alg == "GROUP_IDaaa1") {
          //          methods.get("SPCODEaaa1").get.invoke(obj, args)
          //          methods.get("POL_STATUSaaa1").get.invoke(obj, args)
          //          methods.get(cfg.field_alg).get.invoke(obj, args, groupingRulesMap)
        } else {
          if (a == "-" && b == "-") {
            method.invoke(obj, args, results, invalidResults)
          } else {
            method.invoke(obj, a, b, args, results, invalidResults)
          }
        }
      }
    }
  }

  /**
    * DCS WB算法配置解析动态调用
    *
    * @param functionMap 算法顺序
    * @param args        保单数据
    * @param plan_code   险种代码
    */
  def invokeDcsWbAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]], args: scala.collection.mutable.Map[String, String], plan_code: String, groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]
                     , results: scala.collection.mutable.Map[String, String]): Unit = {
    val obj = new DcsWbFunctions()
    obj.WbSuanFa(args, results)
  }

  /**
    * DCS-short算法配置解析动态调用
    *
    * @param functionMap 算法顺序
    * @param args        保单数据
    * @param plan_code   险种代码
    */
  def invokeDcsShortAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]], args: scala.collection.mutable.Map[String, String], plan_code: String, groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]
                        , results: scala.collection.mutable.Map[String, String], invalidResults: ListBuffer[mutable.Map[String, String]]): Unit = {
    val obj = new DcsShortFunctions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap
    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }
    val cfgArr = functionMap.get(plan_code).get
    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
        results("SUBPARTITION_ID") = args("SUBPARTITION_ID")
        val a = cfg.arg1.trim
        val b = cfg.arg2.trim
        val method = methods.get(cfg.field_alg).get
        /** Invalid_log add  NEXT_RECORD为true时就不执行后续方法，Invalid继续执行后续方法 */
        if (args("NEXT_RECORD") == "false") {
          val a = cfg.arg1.trim
          val b = cfg.arg2.trim
          val method = methods.get(cfg.field_alg).get
          if (cfg.field_alg == "GROUP_IDaaa1") {
          } else {
            if (a == "-" && b == "-") {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, args, results, invalidResults)
            } else {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, a, b, args, results, invalidResults)
            }
          }
        }
      }
    }

  }

  /**
    * DCS-UvTop算法配置解析动态调用
    *
    * @param functionMap 算法顺序
    * @param args        保单数据
    * @param plan_code   险种代码
    */
  def invokeDcsUvTopAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]], args: scala.collection.mutable.Map[String, String], plan_code: String, groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]
                        , results: scala.collection.mutable.Map[String, String], invalidResults: ListBuffer[mutable.Map[String, String]]): Unit = {
    val obj = new DcsUvTopFunctions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap
    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }

    val arr = Array("866", "866D", "867D")
    if (arr.contains(plan_code) && BigDecimal(args("POLICY_VALUE")) == 100) {
      args("NEXT_RECORD") = "true"
      //DcsInvalidUtils.loadArgsParm(invalidResults,args,"PLAN_CODE",args("PLAN_CODE"),"险种是888的保单","0",args("VAL_DATE"),args("p_proc_month") + "NB\\TOP\\NORMAL")
      return
    }

    val cfgArr = functionMap.get(plan_code).get
    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        /** Invalid_log add  NEXT_RECORD为true时就不执行后续方法，Invalid继续执行后续方法 */
        if (args("NEXT_RECORD") == "false") {
          val a = cfg.arg1.trim
          val b = cfg.arg2.trim
          val method = methods.get(cfg.field_alg).get
          if (cfg.field_alg == "GROUP_IDaaa1") {

          } else {
            if (a == "-" && b == "-") {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, args, results, invalidResults)
            } else {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, a, b, args, results, invalidResults)
            }
          }
        }
      }
    }
  }

  /**
    * 投连TOP 类的算子的调用
    *
    * @param functionMap
    * @param args
    * @param plan_code
    * @param groupingRulesMap
    * @param results
    */
  def invokeDcsTop888Alg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]],
                         args: scala.collection.mutable.Map[String, String],
                         plan_code: String,
                         groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]],
                         results: scala.collection.mutable.Map[String, String],
                         invalidResults: ListBuffer[scala.collection.mutable.Map[String, String]]): Unit = {

    val obj = new DcsTop888Functions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap


    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }
    val cfgArr = functionMap.get(plan_code).get

    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
        results("SUBPARTITION_ID") = args("SUBPARTITION_ID")
        val a = cfg.arg1.trim
        val b = cfg.arg2.trim

        val method = methods.get(cfg.field_alg).get
        //计算字段group_id方法
        if (cfg.field_alg == "GROUP_IDaaa1") {
        } else {
          if (a == "-" && b == "-") {
            method.invoke(obj, args, results, invalidResults)
          } else {
            method.invoke(obj, a, b, args, results, invalidResults)
          }
        }
      }
    }
  }


  /**
    * 651 类的算子的调用
    *
    * @param functionMap
    * @param args
    * @param plan_code
    * @param groupingRulesMap
    * @param results
    */
  def invokeDcs651Alg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]],
                      args: scala.collection.mutable.Map[String, String],
                      plan_code: String,
                      groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]],
                      results: scala.collection.mutable.Map[String, String],
                      invalidResults: ListBuffer[scala.collection.mutable.Map[String, String]]): Unit = {

    val obj = new Dcs651Functions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap


    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }
    val cfgArr = functionMap.get(plan_code).get

    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
        results("SUBPARTITION_ID") = args("SUBPARTITION_ID")
        val a = cfg.arg1.trim
        val b = cfg.arg2.trim

        val method = methods.get(cfg.field_alg).get
        //计算字段group_id方法
        if (cfg.field_alg == "GROUP_IDaaa1") {
        } else {
          if (a == "-" && b == "-") {
            method.invoke(obj, args, results, invalidResults)
          } else {
            method.invoke(obj, a, b, args, results, invalidResults)
          }
        }
      }
    }
  }

  /**
    * DCS Ann算法配置解析动态调用
    *
    * @param functionMap 算法顺序
    * @param args        保单数据
    * @param plan_code   险种代码
    */
  def invokeDcsAnnAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]], args: scala.collection.mutable.Map[String, String], plan_code: String, groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]
                      , results: scala.collection.mutable.Map[String, String], invalidResults: ListBuffer[mutable.Map[String, String]]): Unit = {
    val obj = new DcsAnnFunctions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap
    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }
    val cfgArr = functionMap.get(plan_code).get
    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        /*args("OUTPUT_PATH") = args("p_proc_month") + "\\" + cfg.folder_in + "\\" + cfg.output_path
        args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
        results("OUTPUT_PATH") = args("OUTPUT_PATH")
        results("SUBPARTITION_ID") = args("SUBPARTITION_ID")*/
        /** Invalid_log add  NEXT_RECORD为true时就不执行后续方法，Invalid继续执行后续方法 */
        if (args("NEXT_RECORD") == "false") {
          val a = cfg.arg1.trim
          val b = cfg.arg2.trim
          val method = methods.get(cfg.field_alg).get
          if (cfg.field_alg == "GROUP_IDaaa1") {

          } else {
            if (a == "-" && b == "-") {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, args, results, invalidResults)
            } else {
              /** Invalid_log add  添加invalidResults */
              method.invoke(obj, a, b, args, results, invalidResults)
            }
          }
        }
      }
    }
    /*if (methods.keySet.contains("GROUP_IDaaa1")) {
      methods.get("GROUP_IDaaa1").get.invoke(obj, args, groupingRulesMap, results)
    }*/
    if (List(2, 3, 4, 5).contains(args("PREM_TYPE").toInt)) {
      args("PREM_TYPE") = "1"
    }
    if (List(1, 6).contains(args("PREM_TYPE").toInt)) {
      args("PREM_TYPE") = "0"
    }
    args("AGE_PAYMENT") = args("RETIRE_AGE")
    args("AGE_AT_ENTRY") = args("RETIRE_AGE")
    results("PREM_TYPE") = args("PREM_TYPE")
    results("AGE_PAYMENT") = args("AGE_PAYMENT")
    results("AGE_AT_ENTRY") = args("AGE_AT_ENTRY")
    results("RETIRE_AGE") = args("RETIRE_AGE")


  }

  /**
    * DCS-tra算法配置解析动态调用
    *
    * @param functionMap 算法顺序
    * @param args        保单数据
    * @param plan_code   险种代码
    */
  def invokeDcsTraAlg(functionMap: Map[String, Array[ACT_DCS_FUNCS_CFG]], args: scala.collection.mutable.Map[String, String], plan_code: String, groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, ACT_GROUPING_RULES]]
                      , results: scala.collection.mutable.Map[String, String], invalidResults: ListBuffer[mutable.Map[String, String]]): Unit = {
    val obj = new DcsTraFunctions()
    val methods = obj.getClass.getDeclaredMethods
      .map(x => (x.getName, x)).toMap
    if (functionMap.get(plan_code).isEmpty) {
      args("INVALID") = "true"
      return
    }
    val cfgArr = functionMap.get(plan_code).get
    if (cfgArr.length > 0) {
      for (cfg <- cfgArr) {
        //先初始化
        args("OUTPUT_PATH") = "0"
        args("SUBPARTITION_ID") = cfg.folder_in + "\\" + cfg.output_path
        results("OUTPUT_PATH") = args("OUTPUT_PATH")
        results("SUBPARTITION_ID") = args("SUBPARTITION_ID")
        val a = cfg.arg1.trim
        val b = cfg.arg2.trim
        if (cfg.field_alg == "GROUP_IDaaa1") {
        } else {
          val method = methods.get(cfg.field_alg).get
          if (a == "-" && b == "-") {
            /** Invalid_log add  添加invalidResults */
            method.invoke(obj, args, results, invalidResults)
          } else {
            /** Invalid_log add  添加invalidResults */
            method.invoke(obj, a, b, args, results, invalidResults)
          }
        }
      }
    }
  }
}


package com.an.lcloud.act.dcs.utils

import java.text.SimpleDateFormat
import java.util.Date

import com.an.lcloud.act.core.framework.comm.DCSConstants
import com.an.lcloud.act.core.framework.schema.BAS_ACT_DCS_INVALID_LOG
import com.an.lcloud.act.core.framework.utils.Utils
import org.apache.hadoop.fs.Path
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import utils.DcsUtils

import scala.collection.mutable
import scala.collection.mutable.{HashMap, ListBuffer}

/**
  * Created by EX-LEIZHIYONG001 2019-05-08
  * DCSInvalid专用工具类
  */
object DcsInvalidUtils {

  /** Invalid_log add  loadArgsParm该方法用来存储无效保单数据 */
  /**
    * 传入的参数可参考对应的orcale中的方法的值
    *
    * @param invalidResults 存储invalid log记录的数据
    * @param args           --传递polno,plan_code
    * @param failedVar      --无效的字段名
    * @param failedValue    --无效的字段值
    * @param remark         --无效的原因
    * @param invalidType    --无效的类型，由于只传invalid,理论上都为1
    * @param calcDate       --评估日期
    * @param outputPath     --输出路径
    */
  def loadArgsParm(invalidResults: ListBuffer[mutable.Map[String, String]], args: mutable.Map[String, String], failedVar: String,
                   failedValue: String, remark: String, invalidType: String, calcDate: String, outputPath: String): Unit = {
    val arg = mutable.Map[String, String]()
    arg("FAILED_VAR") = failedVar
    arg("FAILED_VALUE") = failedValue
    arg("REMARK") = remark
    arg("INVALID_TYPE") = invalidType
    //arg("PROC_DATE") = calcDate
    arg("OUTPUT_PATH") = outputPath
    arg("POLNO") = args("POLNO")
    arg("PLAN_CODE") = args("PLAN_CODE")
    /** 相同记录多次无效只取一条 后续如果取多条可改成append */
    /* if(invalidResults.size == 0){
       invalidResults.append(arg)
     }else{
       invalidResults(0) = arg
     }*/
    /** 相同记录多次无效取多条 */
    invalidResults.append(arg)
  }

  /** Invalid_log add  dataOutput用来将表数据load到hive库中 */
  /**
    *
    * @param sc
    * @param hiveContext
    * @param serviceConfMap
    * @param resultRDD --存储了有效表和无效表的数据的RDD
    * @param hdfsPath  --有效表的存储路径
    */
  def dataOutput(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String],
                 resultRDD: RDD[(ListBuffer[Row], ListBuffer[Row])], hdfsPath: String, dcsTask: String): Unit = {
    val hdfsPath2 = serviceConfMap.getOrElse(DCSConstants.FILE_HDFS_PATH_INVALID, "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_invalid_log") + "_" + System.currentTimeMillis()
    resultRDD.map(x => x._1).flatMap(x => x).map(r => {
      r.mkString("\001")
    }).saveAsTextFile(hdfsPath)

    resultRDD.map(x => x._2).flatMap(x => x).map(r => {
      r.mkString("\001")
    }).saveAsTextFile(hdfsPath2)

    val res_table = serviceConfMap.getOrElse("res_table", "")
    val p_proc_month = serviceConfMap.getOrElse("p_proc_month", "")
    val res_version_num = serviceConfMap.getOrElse("res_version_num", "")
    val output_sql = s"LOAD DATA INPATH '${hdfsPath}' INTO TABLE $res_table PARTITION(PROC_MONTH='$p_proc_month',VERSION_NUM='$res_version_num')"
    val calcDate = serviceConfMap.getOrElse("VAL_DATE", "")
    val output_sql2 = s"LOAD DATA INPATH '${hdfsPath2}' INTO TABLE SX_HX_SAFE.BAS_ACT_DCS_INVALID_LOG PARTITION(DCS_TASK='$dcsTask',CALC_DATE='${calcDate}')"

    hiveContext.sql(output_sql)
    hiveContext.sql(output_sql2)

    val hdfs = org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
    val path = new Path(hdfsPath)
    if (hdfs.exists(path)) {
      hdfs.delete(path, true)
    }
    val path2 = new Path(hdfsPath2)
    if (hdfs.exists(path2)) {
      hdfs.delete(path2, true)
    }

  }

  def dataOutput2(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String],
                  resultRDD: RDD[(List[String], List[String])], hdfsPath: String, dcsTask: String): Unit = {
    val hdfsPath2 = serviceConfMap.getOrElse(DCSConstants.FILE_HDFS_PATH_INVALID, "/apps-data/hduser1508/sx_hx_safe/bas_act_dcs_invalid_log") + "_" + System.currentTimeMillis()
    resultRDD.map(x => x._1).flatMap(x=>x).saveAsTextFile(hdfsPath)

    resultRDD.map(x => x._2).flatMap(x=>x).saveAsTextFile(hdfsPath2)

    val res_table = serviceConfMap.getOrElse("res_table", "")
    val p_proc_month = serviceConfMap.getOrElse("p_proc_month", "")
    val res_version_num = serviceConfMap.getOrElse("res_version_num", "")
    val output_sql = s"LOAD DATA INPATH '${hdfsPath}' INTO TABLE $res_table PARTITION(PROC_MONTH='$p_proc_month',VERSION_NUM='$res_version_num')"
    val calcDate = serviceConfMap.getOrElse("VAL_DATE", "")
    val output_sql2 = s"LOAD DATA INPATH '${hdfsPath2}' INTO TABLE SX_HX_SAFE.BAS_ACT_DCS_INVALID_LOG PARTITION(DCS_TASK='$dcsTask',CALC_DATE='${calcDate}')"

    hiveContext.sql(output_sql)
    hiveContext.sql(output_sql2)

    val hdfs = org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
    val path = new Path(hdfsPath)
    if (hdfs.exists(path)) {
      hdfs.delete(path, true)
    }
    val path2 = new Path(hdfsPath2)
    if (hdfs.exists(path2)) {
      hdfs.delete(path2, true)
    }

  }

  def InvalidDataPack(invalidResults: ListBuffer[scala.collection.mutable.Map[String, String]], values2: ListBuffer[Row], dcsCategory: String): Unit = {
    val now = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date())
    invalidResults.foreach(
      invalidResult => {
        val resultBean = new BAS_ACT_DCS_INVALID_LOG
        invalidResult("DCS_CATEGORY") = dcsCategory
        invalidResult("CREATED_BY") = ""
        invalidResult("CREATED_DATE") = now
        invalidResult("UPDATED_BY") = ""
        invalidResult("UPDATED_DATE") = now
        Utils.map2obj(invalidResult.toMap, resultBean)
        val fields = Class.forName(resultBean.getClass.getName).getDeclaredFields
        val values = fields.map(f => {
          f.setAccessible(true)
          f.get(resultBean)
        }).toSeq
        values2 += (Row.fromSeq(values))
      })
  }

  def InvalidDataPack2(invalidResults: ListBuffer[scala.collection.mutable.Map[String, String]], values2: ListBuffer[String], dcsCategory: String): Unit = {
    val now = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date())
    invalidResults.foreach(
      invalidResult => {
        invalidResult("DCS_CATEGORY") = dcsCategory
        invalidResult("CREATED_BY") = ""
        invalidResult("CREATED_DATE") = now
        invalidResult("UPDATED_BY") = ""
        invalidResult("UPDATED_DATE") = now
        val result = DcsUtils.buildDcsResultString(invalidResult, DCSConstants.DCS_RESULT_COLUMN_INVALID)
        values2 += (result)
      })
  }
}

package utils

import com.an.lcloud.act.core.framework.comm.DCSConstants
import com.an.lcloud.act.core.framework.utils.DateUtils

import scala.collection.mutable.HashMap

/**
  * Created by ZOUBO162 on 2018-12-4.
  * DCS专用工具类
  */
object DcsUtils {

  /**
    * 根据p_proc_date,解析出year,month等
    *
    * @param serviceConfMap
    */
  def setDateParams(serviceConfMap: HashMap[String, String]) = {
    val p_proc_date = serviceConfMap.getOrElse("p_end_date", "")
    val procDate = DateUtils.strToDateTime(p_proc_date, DateUtils.YYYYMMDD)
    val procDay = procDate.getDayOfMonth
    val valDate = if (procDay >= 20) {
      procDate.plusDays(15).withDayOfMonth(1).minusDays(1)
    } else {
      procDate.withDayOfMonth(1).minusDays(1)
    }
    val valYear = valDate.getYear
    val valMoth = valDate.getMonthOfYear
    val valDateNum = valYear * 100 + valMoth
    val valCode = valYear.toString + valMoth.toString
    serviceConfMap += ("V_VAL_YEAR" -> valYear.toString)
    serviceConfMap += ("V_VAL_MONTH" -> valMoth.toString)
    serviceConfMap += ("V_VAL_DATE_NUM" -> valDateNum.toString)
    serviceConfMap += ("V_VAL_CODE" -> valCode.toString)
    serviceConfMap += ("V_VAL_DATE" -> DateUtils.dateToStr(valDate))
    serviceConfMap += ("VAL_DATE" -> DateUtils.dateToStr(valDate))
  }


  /**
    * args:dcs result结果集合
    * column:表字段
    *
    * @param args
    * @param column
    */
  def buildDcsResultString(args: scala.collection.mutable.Map[String, String], column: String = DCSConstants.DCS_RESULT_COLUMN_IND): String = {
    val buffer = new StringBuffer()
    val columnArr = column.split(",")
    columnArr.foreach(e => {
      buffer.append(args.getOrElse(e.trim, "")).append("\001")
    })
    val result = buffer.toString
    if (result.endsWith("\001")) {
      result.substring(0, result.length - 1)
    } else {
      result
    }
  }


}



package com.an.lcloud.act.download.dcs.job

import com.an.lcloud.act.core.framework.job.BaseJob
import com.an.lcloud.act.core.framework.schema.ACT_DCS_RESULT_UL
import com.an.lcloud.act.dcs.udf.LoadDcsCommDataJob
import com.an.lcloud.act.download.dcs.comm.DownloadConstants
import com.an.lcloud.act.download.dcs.utils.DownloadUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.FileSystem
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable.{HashMap, ListBuffer}

/**
  * Created by EX-SHENCHENG001 on 2019/5/15.
  */
class DcsUlinkDownloadJob(@transient sc: SparkContext, @transient hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) extends BaseJob(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) with java.io.Serializable {
  override def run() = {

    //环境初始化
    super.init()
    val startTime = System.currentTimeMillis
    log.info("=======================> 开始 <==========================")
    //读取Download目录下,对应dcs的xml
    readXml(DownloadConstants.DCS_ULINK_XML_PATH)
    //加载结果表, 这里是dcs结果表, 由于其他的命名应该就相似, 这里引用优化后mp保单汇总结果表下载的函数
    val loadDataSql = selectMapping(serviceConfMap.getOrElse("policy_download", ""), serviceConfMap)
    val initRDD = LoadDcsCommDataJob
      .loadPolBenData(hiveContext, serviceConfMap, loadDataSql)
//    logError("---------InitRDD loaded : " + initRDD.count())
    logError("加载数据耗时：" + (System.currentTimeMillis - startTime) / 1000 + " 秒")

    val mRDD:RDD[(String,ACT_DCS_RESULT_UL)] = initRDD.mapPartitions(it => {
      val list = new ListBuffer[(String,ACT_DCS_RESULT_UL)]
      it.foreach(row => {
        val obj = new ACT_DCS_RESULT_UL
        val scalaClazzName = obj.getClass.getName
        val clazz = Class.forName(scalaClazzName)
        val fields = clazz.getDeclaredFields
        fields.foreach(f => {
          f.setAccessible(true)
          val fieldName = f.getName.toLowerCase
          f.set(obj, String.valueOf(row.getAs[String](fieldName)))
        })
        list += ((obj.output_path + "@" + obj.prod_name,obj))
      })
      list.iterator
    })
    logError("mRDD耗时：" + (System.currentTimeMillis - startTime) / 1000 + " 秒")

    val groupRDD:RDD[(String,Iterable[ACT_DCS_RESULT_UL])] = mRDD.groupByKey()
    logError("groupRDD耗时：" + (System.currentTimeMillis - startTime) / 1000 + " 秒")

     val resultRDD:RDD[(String,List[ACT_DCS_RESULT_UL])] = groupRDD.mapValues(_.toList.sortBy(_.spcode))
    logError("resultRDD：" + (System.currentTimeMillis - startTime) / 1000 + " 秒")

    val hdfsPath:String = DownloadConstants.ULINK_DOWNLOAD_HDFS_PREFIX
    resultRDD.foreachPartition(x => {
        x.foreach(x => {
            //获取配置 并 初始化Hdfs文件系统, 做好获取不到的准备
            val fs:FileSystem = FileSystem.get(new Configuration)
            val key:String = x._1
            val output_path:String = key.split("@")(0)
            val prod_name:String = key.split("@")(1)
            //为了拼接表头, 这部分是需要每个开发自己对应自己的险种, 填进去的, 请在comm的Constants内写自己的常量
            val tableHead: String = output_path match {
              case v if v.contains("STER") => DownloadConstants.ULINK_STER_TABLE_HEAD
              case v if v.contains("SNO") => DownloadConstants.ULINK_SNO_TABLE_HEAD
              case v if v.contains("DNO") => DownloadConstants.ULINK_DNO_TABLE_HEAD
              case v if v.contains("KAOHE") => DownloadConstants.ULINK_KAOHE_TABLE_HEAD
              case v if v.contains("NORMAL") => DownloadConstants.ULINK_NB_NORMAL_TABLE_HEAD
              case v if v.contains("NBIF") => DownloadConstants.ULINK_NBIF_TABLE_HEAD
              case v if v.contains("SNO_I17") => ""
              case v if v.contains("NB_I17") => ""
              case _ => ""
            }
            //将对象转换成字符串
            val objList:List[String] = x._2.map(x => x.policyDownloadValue(x))
            //添加表头
            val resultSet:List[String] = objList.::(tableHead)
            //创建文件路径
            val directory = hdfsPath + output_path + "/"
            val filePath = hdfsPath + output_path + "/" + prod_name + ".rpt"
            //传入下载的工具类
            DownloadUtils.FileStreamDownloadToHDFS(directory, filePath, resultSet, fs)
          })
      })
    initRDD.unpersist(true)
    val endTime = System.currentTimeMillis
    logError("最终耗时：" + (endTime - startTime) / 1000 + " 秒")
  }
}




package com.an.lcloud.act.download.dcs.utils

import java.io.BufferedOutputStream

import org.apache.hadoop.fs.{FSDataOutputStream, FileSystem, Path}

/**
  * Created by EX-HUANGTAO009 on 2019-5-9.
  */
object DownloadUtils {

  def FileStreamDownloadToHDFS(directoryPath: String, filePath: String, resultSet: List[String], fs: FileSystem) = {
    var pw: BufferedOutputStream = null
    var fdos: FSDataOutputStream = null
    try {
      val directory = new Path(directoryPath)
      val file = new Path(filePath)

      if (!fs.exists(directory)) {
        fs.mkdirs(directory)
      }
      if (fs.exists(file)) {
        fs.delete(file, true)
      }
      //创建文件, 建立输出流
      fdos = fs.create(file)
      pw = new BufferedOutputStream(fdos, 1024 * 1024 * 2) //5MB

      for (line <- resultSet) {
        pw.write(line.getBytes)
      }
      //      pw.flush()
    } catch {
      case ex: Exception => {
        throw new Exception(""+ex.getLocalizedMessage)
      }
    } finally {
      //关闭数据流
      if (pw != null) {
      pw.close()
      }
      if (fdos != null) {
      fdos.close()
    }
      //关闭fs,同时关闭所有fs,因此不能关
      //      if(fs!=null){
      //        fs.close()
      //      }

  }

  }


}


package com.an.lcloud.ifrs17.grouping.comm

import com.an.lcloud.ifrs17.core.framework.utils.DateUtils
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable.HashMap

/**
  * Created by ZOUBO162 on 2018-12-6.
  * 数据库操作类
  */
object DBUtils {
  /**
    * grouping结果保存至hive
    *
    * @param hiveContext    hiveContext
    * @param serviceConfMap 参数
    * @param dataFrame      结果数据
    * @param tempTable      临时表名
    * @param table          hive表名
    */
  def batchSaveToHive(hiveContext: HiveContext, serviceConfMap: HashMap[String, String], dataFrame: DataFrame, tempTable: String, table: String): Unit = {
    val defaultDate = DateUtils.nowDateTime().withDayOfMonth(1).toString(DateUtils.YYYY_MM_DD)
    val procDate = serviceConfMap.getOrElse("p_proc_date", defaultDate)
    val month = DateUtils.strToDateTime(procDate, DateUtils.YYYY_MM_DD).toString(DateUtils.YYYYMM)
    val versionNum = serviceConfMap.get("res_version_num").get
    dataFrame.registerTempTable(tempTable)
    val sql =
      s"""
         |insert into $table partition(proc_month='$month',version_num='$versionNum') select * from $tempTable
      """.stripMargin
    hiveContext.sql(sql)
  }

  /**
    * 从数据库加载数据
    *
    * @param hiveContext    执行上下文
    * @param table          执行sql语句
    * @param serviceConfMap 参数
    * @return
    */
  def loadDataFromDB(hiveContext: HiveContext, table: String, serviceConfMap: HashMap[String, String]): DataFrame = {
    val url = serviceConfMap.getOrElse("jdbc_str", "jdbc:oracle:thin:@d0pala.dbdev.paic.com.cn:1526:d0pala")
    val user = serviceConfMap.getOrElse("db_user", "Palalarsdata")
    val password = serviceConfMap.getOrElse("db_psw", "paic1234")
    hiveContext.read.format("jdbc")
      .options(Map("driver" -> "oracle.jdbc.driver.OracleDriver",
        "url" -> url,
        "dbtable" -> table,
        "user" -> user,
        "password" -> password)).load
  }

  /**
    * 从hive中加载数据
    *
    * @param hiveContext hiveContext
    * @param sql         sql语句
    * @return
    */
  def loadDataFromHive(hiveContext: HiveContext, sql: String): DataFrame = {
    hiveContext.sql(sql)
  }

}
package com.an.lcloud.ifrs17.grouping.comm

import com.an.lcloud.ifrs17.core.framework.utils.DateUtils
import org.apache.commons.lang3.StringUtils

/**
  * Created by ZOUBO162 on 2018-12-6.
  */
object GroupingUtils {


  /**
    * 判断short分区
    *
    * @param outPutPath
    * @return
    */
  def getRunSelection(outPutPath: String): String = {
    outPutPath match {
      case "SH\\KAOHE" => "2"
      case "SH\\KAOHE_5CH" => "3"
      case _ => "1"
    }
  }

  /**
    * 校验更正outPutPath
    *
    * @param outPutPath
    * @return
    */
  def getRealOutPutPath(outPutPath: String): String = {
    if (outPutPath.length > 7) {
      if (outPutPath.substring(6, 7) == "\\") {
        outPutPath.substring(7)
      } else {
        outPutPath.substring(6)
      }
    } else {
      outPutPath
    }
  }

  /**
    * 计算评估时间点
    * 处理日期如果超过当月19号，则取值为当月20号，否则取值为上月月底
    *
    * @param procDate 入参处理时间
    * @return
    */
  def getAssessmentProcDate(procDate: String): String = {
    val date = DateUtils.strToDateTime(procDate, DateUtils.YYYYMMDD)
    val result = if (date.getDayOfMonth > 19) {
      date.withDayOfMonth(20)
    } else {
      date.withDayOfMonth(1).minusDays(1)
    }
    result.toString(DateUtils.YYYY_MM_DD)
  }


}
package com.an.lcloud.ifrs17.grouping.job

import com.an.lcloud.ifrs17.grouping.comm.GroupingUtils
import org.apache.spark.SparkContext
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable.HashMap

/**
  * Created by ZOUBO162 on 2018-12-5.
  *
  * 个银传统分红grouping主程序
  */
class IndGroupingJob(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) extends GroupingJob(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) {
  /** 个人传统分红分类常量 */
  private val P_PROC_CODE = "0430"

  def execute(): Unit = {
    initPrd()
    readXml("/grouping/ind.xml")

    //设置outputpath
    val outPutPath = serviceConfMap.getOrElse("p_output_path", "")
    serviceConfMap += ("p_output_path" -> GroupingUtils.getRealOutPutPath(outPutPath))
    serviceConfMap += ("p_proc_code" -> P_PROC_CODE)
    //grouping结果表名
    serviceConfMap += ("res_table" -> "bas_pala_lar_grouping_ind")

    logInfo("个银传统分红grouping开始")
    run
    logInfo("个银传统分红grouping结束")

  }

}

package com.an.lcloud.ifrs17.grouping.launcher

import com.an.lcloud.ifrs17.core.framework.comm.JobConstants
import com.an.lcloud.ifrs17.core.framework.job.{JobCommon, VersionManager}
import com.an.lcloud.ifrs17.core.framework.utils.{DateUtils, Utils}
import com.an.lcloud.ifrs17.grouping.job.IndGroupingJob
import com.an.lcloud.ifrs17.mp.launcher.MPIndExecutor2.{logError, serviceConfMap}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{Logging, SparkConf, SparkContext}
import org.joda.time.DateTime

/**
  * Created by ZOUBO162 on 2018-12-5.
  * 个银传统分红grouping程序
  */
object IndGroupingExecutor extends JobCommon with Logging {
  def main(args: Array[String]): Unit = {

    args.foreach(arg => {
      val param = arg.split('=')
      if (param.length == 2) {
        serviceConfMap.put(param(0), param(1))
      } else {
        logError("参数【" + param + "】错误,实际传参为【" + arg + "】,程序退出")
        System.exit(1)
      }
    })

    val logLevel = serviceConfMap.getOrElse("log.level", "info")
    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }
    //如果没有指定版本号,以prodDate为版本号
    //    val procDate = serviceConfMap.getOrElse("p_end_date", DateUtils.dateToStr(new DateTime, DateUtils.YYYYMM))
    val procDate = Utils.getAssessmentProcDate(serviceConfMap.getOrElse("p_end_date", DateUtils.dateToStr(new DateTime, DateUtils.YYYYMMDD)))

    /** 老版本方式 */
    //    val srcVersionNum = serviceConfMap.getOrElse("src_version_num", procDate)
    //    val resVersionNum = serviceConfMap.getOrElse("res_version_num", procDate)
    //    serviceConfMap += ("src_version_num" -> srcVersionNum)
    //    serviceConfMap += ("res_version_num" -> resVersionNum)
    //hive库
    serviceConfMap += ("dbName" -> serviceConfMap.getOrElse("res_hive_db", "sx_hx_safe"))

    /**
      * 对于在脚本中设置的启动参数，就不要在代码中设置
      */
    val sparkConf = new SparkConf()
      .setAppName("IndGroupingExecutor")
      .set("spark.rdd.compress", "true")
      .set("spark.sql.codegen", "true")
      .set("spark.sql.inMemoryColumnarStorage.batchSize", "10000")
      .set("spark.sql.inMemoryColumnarStorage.compressed", "true")
      .set("hive.execution.engine", "spark")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")


    val sc = new SparkContext(sparkConf)

    logLevel match {
      case "debug" => sc.setLogLevel(Level.DEBUG.toString)
      case "info" => sc.setLogLevel(Level.INFO.toString)
      case "warn" => sc.setLogLevel(Level.WARN.toString)
      case "error" => sc.setLogLevel(Level.ERROR.toString)
    }
    /** 各险种任务与基础公用数据使用同一个hiveContext,确保所有临时表或持久化表的生命周期与hivecontext一致 */
    val hiveContext = new HiveContext(sc)

    /** 版本号 */
    val v_proc_date = DateUtils.strToDateTime(procDate, DateUtils.YYYY_MM_DD).toString(DateUtils.YYYYMMDD)
    val v_p_task_name = "spark_ifrs17_grouping_ind_aio"
    val src_tables = Array("sx_hx_safe.bas_pala_lar_dcs_result_ind")
    val startVersionMap = VersionManager.verStartIf(v_proc_date, v_p_task_name, src_tables, hiveContext)

    val source_tab_Map = startVersionMap.get("v_source_tbl").get.toString.split(",").zip(startVersionMap.get("v_source_ver").get.toString.split(",")).toMap
    val target_tab_Map = startVersionMap.get("v_target_tbl").get.toString.split(",").zip(startVersionMap.get("v_target_ver").get.toString.split(",")).toMap

    serviceConfMap.++=(source_tab_Map)
    serviceConfMap.++=(target_tab_Map)

    serviceConfMap.put("src_version_num", startVersionMap.get("v_source_tbl").get.toString)
    serviceConfMap.put("res_version_num", startVersionMap.get("v_target_ver").get.toString)

    new IndGroupingJob(sc, hiveContext, serviceConfMap).execute


    sc.stop
    /** 更新版本号 */
    val endVersionMap = VersionManager.verEndIf(startVersionMap, JobConstants.P_RUN_FLAG_SUCCESS, "success")
    logError(endVersionMap.toString())

  }

}

package com.an.lcloud.ifrs17.grouping.launcher

import com.an.lcloud.ifrs17.core.framework.job.JobCommon
import com.an.lcloud.ifrs17.core.framework.utils.DateUtils
import com.an.lcloud.ifrs17.grouping.job.PuaGroupingJob
import org.apache.log4j.Level
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{Logging, SparkConf, SparkContext}
import org.joda.time.DateTime

/**
  * Created by ZOUBO162 on 2018-12-5.
  * pua grouping程序
  */
object PuaGroupingExecutor extends JobCommon with Logging {
  def main(args: Array[String]): Unit = {

    args.foreach(arg => {
      val param = arg.split('=')
      if (param.length == 2) {
        serviceConfMap.put(param(0), param(1))
      } else {
        logError("参数【" + param + "】错误,实际传参为【" + arg + "】,程序退出")
        System.exit(1)
      }
    })

    val logLevel = serviceConfMap.getOrElse("log.level", "info")

    //如果没有指定版本号,以prodDate为版本号
    val procDate = serviceConfMap.getOrElse("p_proc_date", DateUtils.dateToStr(new DateTime)).replaceAll("-", "")

    val srcVersionNum = serviceConfMap.getOrElse("src_version_num", procDate)
    val resVersionNum = serviceConfMap.getOrElse("res_version_num", procDate)
    serviceConfMap += ("src_version_num" -> srcVersionNum)
    serviceConfMap += ("res_version_num" -> resVersionNum)
    //hive库
    serviceConfMap += ("dbName" -> serviceConfMap.getOrElse("res_hive_db", "sx_hx_safe"))

    val sparkConf = new SparkConf()
      .setAppName("IndGroupingExecutor")
      .set("spark.rdd.compress", "true")
      .set("spark.sql.codegen", "true")
      .set("spark.sql.inMemoryColumnarStorage.batchSize", "20000")
      .set("spark.sql.inMemoryColumnarStorage.compressed", "true")
      .set("spark.sql.autoBroadcastJoinThreshold", "524288000")
      .set("hive.execution.engine", "spark")
      .set("spark.sql.shuffle.partitions", "2000")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("hive.auto.convert.join", "true")
      .set("hive.mapjoin.smalltable.filesize", "524288000")


    val sc = new SparkContext(sparkConf)

    logLevel match {
      case "debug" => sc.setLogLevel(Level.DEBUG.toString)
      case "info" => sc.setLogLevel(Level.INFO.toString)
      case "warn" => sc.setLogLevel(Level.WARN.toString)
      case "error" => sc.setLogLevel(Level.ERROR.toString)
    }
    /** 各险种任务与基础公用数据使用同一个hiveContext,确保所有临时表或持久化表的生命周期与hivecontext一致 */
    val hiveContext = new HiveContext(sc)
    new PuaGroupingJob(sc, hiveContext, serviceConfMap).execute


    sc.stop

  }

}

package com.an.lcloud.ifrs17.dcs.job

import java.text.SimpleDateFormat
import java.util.Date

import com.an.lcloud.ifrs17.core.framework.comm.DCSConstants
import com.an.lcloud.ifrs17.core.framework.job.BaseJob
import com.an.lcloud.ifrs17.core.framework.schema.{LAR_DCS_RESULT_IND, _}
import com.an.lcloud.ifrs17.core.framework.utils.{CommandExecutor, Utils}
import com.an.lcloud.ifrs17.dcs.udf.LoadDcsCommDataJob
import com.an.lcloud.ifrs17.dcs.utils.DcsAlgCfgHandler
import org.apache.hadoop.fs.Path
import org.apache.spark.SparkContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.storage.StorageLevel
import utils.DcsUtils

import scala.collection.mutable._
import scala.io.Source

/**
  * dcs业务处理主类,功能点：
  * 1.加载公参数据、保单数据
  * 2.过滤无需处理的保单
  * 3.保单处理
  * 4.结果保存至hive
  *
  * @param sc sparkContext
  * @param hiveContext
  * @param serviceConfMap
  */
class DcsIndJob(@transient sc: SparkContext, @transient hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) extends BaseJob(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) {

  override def run() = {

    super.init()

    val startTime = System.currentTimeMillis
    log.info("=======================> 开始 <==========================")
    readXml(DCSConstants.CONST_XML_FILE_PATH)

    DcsUtils.setDateParams(serviceConfMap)

    //TODO:加载算法配置
    val confFile = this.getClass.getResourceAsStream("/dcs/pol_ben_dcs_funcs_cfg.csv")
    val confMap = Source.fromInputStream(confFile).getLines().toArray.drop(1)
      .filter(_.split(",")(0) == "POL_BEN_MAIN")
      .map(x => {
        val arr = x.split(",")
        POL_BEN_DCS_FUNCS_CFG(arr(0), arr(1), arr(2), arr(3).toInt, arr(4), arr(5), arr(6), arr(7))
      })
      .groupBy(_.plan_code)
      .map(x => {
        (x._1, x._2.sortBy(_.field_seq))
      })

    val suanfaConfMapBC = sc.broadcast(confMap)

    /** 加载公参数据并广播 */
    val groupingRuleBC = sc.broadcast(LoadDcsCommDataJob.getGroupingRules(hiveContext, serviceConfMap))
    // val groupingConfig = sc.broadcast(LoadDcsCommDataJob.getGroupingConfig(hiveContext, serviceConfMap))
    //    val planCodeG = sc.broadcast(LoadDcsCommDataJob.getDcsPlanCodeGCfg(hiveContext, serviceConfMap))
    val basicInfoBC = sc.broadcast(LoadDcsCommDataJob.getDcsBasicInfo(hiveContext, serviceConfMap))


    //    //TODO：加载数据
    val initRDD = LoadDcsCommDataJob.loadPolBenIndData(hiveContext, serviceConfMap)

    val filteredRDD =
      initRDD
        .map(row => {
          val obj = new BAS_PALA_POL_BEN_IND
          val scalaClazzName = obj.getClass.getName
          val clazz = Class.forName(scalaClazzName)
          val fields = clazz.getDeclaredFields
          fields.foreach(f => {
            f.setAccessible(true)
            val fieldName = f.getName.toLowerCase
            f.set(obj, String.valueOf(row.getAs[String](fieldName)))
          })

          val x = obj

          def null2Zero(str: String): String = {
            if (str.isEmpty || str.equals("null")) {
              "0"
            } else {
              str
            }
          }

          x.unit = null2Zero(x.unit)
          x.policy_value = null2Zero(x.policy_value)
          x.ann_std_prem_m = null2Zero(x.ann_std_prem_m)
          x.ann_std_prem = null2Zero(x.ann_std_prem)
          x.ann_substd_prem = null2Zero(x.ann_substd_prem)
          x.ann_prof_prem = null2Zero(x.ann_prof_prem)
          x.std_modal_prem_m = null2Zero(x.std_modal_prem_m)
          x.tot_modal_prem = null2Zero(x.tot_modal_prem)

          x.std_modal_prem_r1 = null2Zero(x.std_modal_prem_r1)
          x.ann_std_prem_r2 = null2Zero(x.ann_std_prem_r2)
          x.std_modal_prem_r2 = null2Zero(x.std_modal_prem_r2)
          x.ann_std_prem_r3 = null2Zero(x.ann_std_prem_r3)
          x.std_modal_prem_r3 = null2Zero(x.std_modal_prem_r3)

          x.ann_std_prem_r1 = null2Zero(x.ann_std_prem_r1)
          x.ann_std_prem_r3 = null2Zero(x.ann_std_prem_r2)
          x.ann_std_prem_r3 = null2Zero(x.ann_std_prem_r3)
          x.ci_rider = null2Zero(x.ci_rider)
          x.jnt_age = null2Zero(x.jnt_age)
          x.ins_age = null2Zero(x.ins_age)

          x.sum_assd_org = null2Zero(x.sum_assd_org)

          x.sno = null2Zero(x.sno)
          x.dno = null2Zero(x.dno)
          x.jnt_prof = null2Zero(x.jnt_prof)

          x
        })
        .filter(x => {

          val basicInfoMap = basicInfoBC.value
          val ruler00 = basicInfoMap.keySet.contains(x.plan_code)

          var ruler01 = true
          if (ruler00) {
            val basic_info = basicInfoMap.get(x.plan_code).get
            ruler01 = basic_info.run_in_dcs != "N" && basic_info.work_space == "IND"
          }

          val arr = Array("1281", "1033", "1278", "3064", "3083", "3081", "3082")
          val ruler02 = !arr.contains(x.plan_code)

          var ruler03 = Array("M", "F").contains(x.ins_sex)

          ruler00 && ruler01 && ruler02 && x.payout_date !="" && x.first_eff_date !="" && x.eff_date !="" && x.pay_to_date !=""
        })
    //        .map(x => {
    //          val prefix = new Random().nextInt(random_size)
    //          //          (prefix + "#" + x.period, x)
    //          (prefix, x)
    //        })

    filteredRDD.persist(StorageLevel.MEMORY_ONLY_SER)
    logError("filteredRDD count:" + filteredRDD.count())
    println("加载数据耗时：" + (System.currentTimeMillis - startTime) / 1000 + " 秒")

    //按保险年期分区
    //    val partitionArr = filteredRDD.map(_._1).collect()
    //
    //    //TODO:业务逻辑
    val resultRDD =
    filteredRDD
      //      .partitionBy(new DcsIndPartitioner(partitionArr))
      .mapPartitions(pol_ben_inds => {

      val list = new ListBuffer[Row]

      pol_ben_inds.foreach(r => {

        //          val pol_ben_ind = r._2
        val pol_ben_ind = r

        val now = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date())

        val planCode = pol_ben_ind.plan_code
        //
        //          //TODO:调用算法
        val basic_info = basicInfoBC.value.get(planCode).get
        val args = scala.collection.mutable.Map[String, String]()
        args.++=(Utils.obj2MapOfCaseClass(basic_info))
        args.++=(Utils.obj2MapOfCaseClass(pol_ben_ind))
        args.++=(serviceConfMap)
        args("PLAN_CODE_S") = pol_ben_ind.plan_code
        args("INVALID") = "false"
        /** 各算法调用，获取rdd,通过java反射为对象赋值 */
        DcsAlgCfgHandler.invokeDcsAlg(suanfaConfMapBC.value, args, pol_ben_ind.plan_code, groupingRuleBC.value)
        args("CREATED_BY") = "#"
        args("CREATED_DATE") = now
        args("UPDATED_BY") = "#"
        args("UPDATED_DATE") = now

        if (args("INVALID") == "false") {
          val resultBean = new LAR_DCS_RESULT_IND
          Utils.map2obj(args.toMap, resultBean)
          resultBean.group_id = args("GROUP_ID")
          //            args.clear()
          val fields = Class.forName(resultBean.getClass.getName).getDeclaredFields
          val values = fields.map(f => {
            f.setAccessible(true)
            f.get(resultBean)
          }).toSeq
          list += (Row.fromSeq(values))
        }
      })

      list.iterator
    }, preservesPartitioning = false)

    logError("resultRDD count:" + resultRDD.count())

    filteredRDD.unpersist()

    resultRDD.persist(StorageLevel.MEMORY_ONLY_SER)

    //TODO:输出

    logError("输出到hive表....")

    //输出到临时目录
    val hdfsPath = serviceConfMap.get("out.hdfs.path").get + "_" + System.currentTimeMillis()
    resultRDD
      .map(r => {
        val arr = new ArrayBuffer[String]()
        for (i <- Range(1, 22)) {
          arr.append("")
        }
        r.mkString("\001") + "\001" + arr.mkString("\001")
      })
      .coalesce(80, shuffle = false)
      //      .repartition(160)
      .saveAsTextFile(hdfsPath)

    resultRDD.unpersist()


    val res_table = serviceConfMap.get("res_table").get
    val p_proc_month = serviceConfMap.get("p_proc_month").get
    val res_version_num = serviceConfMap.get("res_version_num").get
    val output_sql = s"LOAD DATA INPATH '${hdfsPath}' OVERWRITE INTO TABLE $res_table PARTITION(PROC_MONTH='$p_proc_month',VERSION_NUM='$res_version_num')"
    logError(output_sql)
    hiveContext.sql(output_sql)

    val hdfs = org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
    val path = new Path(hdfsPath)
    if (hdfs.exists(path)) {
      hdfs.delete(path, true)
    }

    logError("输出到FTP HDFS目录....")

    //TODO 输出到FTP HDFS目录
    val outputFtpPath = serviceConfMap.get("out.hdfs.path").get + "_dcs2ftp_" + s"${serviceConfMap.get("res_version_num").get}" + "_" + System.currentTimeMillis()
    val output_ftp_path = new Path(outputFtpPath)
    if (hdfs.exists(output_ftp_path)) {
      hdfs.delete(output_ftp_path, true)
    }

    val output_ftp_sql =
      s"""use sx_hx_safe;SET mapreduce.job.queuename=root.queue_sx80.1508_11;insert overwrite directory '${outputFtpPath}' row format delimited fields terminated by ',' select * from $res_table where PROC_MONTH='$p_proc_month' and VERSION_NUM='$res_version_num'""".stripMargin
    logError(output_ftp_sql)

    CommandExecutor.hiveCommand(output_ftp_sql)

    //    hiveContext.sql(output_ftp_sql)
    //    logError("hive -e " + output_ftp_sql)

    //    try {
    //      val exitValue = Process("nohup /usr/local/bin/hive -e " + output_ftp_sql).!
    //      if (0 != exitValue) {
    //        logError("call shell failed. error code is :" + exitValue);
    //      }
    //    } catch {
    //      case e: Exception => {
    //        logError("call shell failed. " + e);
    //      }
    //    }

    //jdbc
    //    val url = "jdbc:hive2://hdp2-hiveserver-sh-stg.app.paic.com.cn:10000"
    //    try {
    //      Class.forName("org.apache.hive.jdbc.HiveDriver")
    //    } catch {
    //      case e: ClassNotFoundException => {
    //        e.printStackTrace()
    //        System.exit(1)
    //      }
    //    }
    //    val conn = DriverManager.getConnection(url, "", "")
    //    val stmt = conn.createStatement()
    //    logError(output_ftp_sql)
    //    stmt.executeQuery(output_ftp_sql)
    //    conn.close()

    //    resultRDD
    //      .map(r => {
    //        val arr = new ArrayBuffer[String]()
    //        for (i <- Range(1, 22)) {
    //          arr.append("")
    //        }
    //        //        val resArr= Array[String](
    //        //          r.getAs[String]("polno"),
    //        //          r.getAs[String]("brno"),
    //        //          r.getAs[String]("play_type"),
    //        //          r.getAs[String]("plan_code"),
    //        //          r.getAs[String]("eff_date"),//
    //        //          r.getAs[String]("ben_sts"),
    //        //          r.getAs[String]("units"),
    //        //          r.getAs[String]("sum_ins"),//
    //        //          r.getAs[String]("matu_date"),//
    //        //          r.getAs[String]("ins_sex"),//
    //        //          r.getAs[String]("ins_age"),//
    //        //          r.getAs[String]("ins_prof"),//
    //        //          r.getAs[String]("jnt_sex"),//
    //        //          r.getAs[String]("jnt_age"),//
    //        //          r.getAs[String]("jnt_prof"),//
    //        //          r.getAs[String]("period"),//
    //        //          r.getAs[String]("prem_term"),//
    //        //          r.getAs[String]("prem_type"),//
    //        //          r.getAs[String]("ann_std_prem"),//
    //        //          r.getAs[String]("ann_substd_prem"),//
    //        //          r.getAs[String]("ann_prof_prem"),//
    //        //          r.getAs[String]("std_modal_prem"),//
    //        //          r.getAs[String]("tot_modal_prem"),//
    //        //          r.getAs[String]("pay_to_date"),//
    //        //          r.getAs[String]("payout_age"),//
    //        //          r.getAs[String]("payout_date"),//
    //        //          r.getAs[String]("ben_level"),//
    //        //          r.getAs[String]("policy_fee"),//
    //        //          r.getAs[String]("deptno"),
    //        //          r.getAs[String]("sno"),
    //        //          r.getAs[String]("dno"),
    //        //          r.getAs[String]("payout_type"),//
    //        //          r.getAs[String]("channel_mode"),
    //        //          r.getAs[String]("region_code")
    //        //        )
    //
    //        r.mkString(",") + "," + arr.mkString(",")
    //      })
    //      .coalesce(80, shuffle = false)
    //      //      .repartition(160)
    //      .saveAsTextFile(exportPath)


    //    val obj = new LAR_DCS_RESULT_IND
    //    val fields = Class.forName(obj.getClass.getName).getDeclaredFields
    //    val schema = StructType(
    //      fields.map(_.getName).map(fieldName => StructField(fieldName, StringType, true))
    //    )
    //    val result = hiveContext.createDataFrame(resultRDD, schema)
    //    result.show()

    //    result.registerTempTable("dcs_ind_result")

    //    val sql = s"INSERT OVERWRITE TABLE $res_table PARTITION(PROC_MONTH='$p_proc_month',VERSION_NUM='$p_proc_month') SELECT * from dcs_ind_result"
    //    logError(sql)
    //    hiveContext.sql(sql)


    val endTime = System.currentTimeMillis
    println("耗时：" + (endTime - startTime) / 1000 + " 秒")

  }


}


//package com.an.lcloud.ifrs17.dcs.job
//
//import com.an.lcloud.ifrs17.core.framework.job.BaseJob
//import com.an.lcloud.ifrs17.core.framework.schema._
//import com.an.lcloud.ifrs17.dcs.udf.LoadDcsCommDataJob
//import org.apache.spark.SparkContext
//import org.apache.spark.sql.hive.HiveContext
//
//import scala.collection.mutable._
//
///**
//  * Created by EX-ZHANGYONGTIAN001 on 2018-10-10.
//  * 输入参数：
//  * VAL_DATE = datetime.datetime.strptime('2018/8/31', '%Y/%m/%d')
//  * PROC_DATE = '2018/8/31'
//  * SPCODE_SELECTION = 1
//  * *
//  * 输出：
//  * 1106,
//  * PLAN_CODE,AGE_AT_ENTRY,SEX,ENTRY_YEAR,ENTRY_MONTH,AGE2_ATENTRY,SEX2,DIST_CHANNEL,POL_TERM_Y,SUM_ASSURED,UNITS,AGE_PAYMENT,INIT_POLS_IF,PAYMENT_CODE,POLNO,VAL_CODE,INDEX1,SUM_ASSD_ORG,COMM_INDEX,CI_RIDER,SUM_ASSD_M,SUM_ASSD_R,SUM_ASSD_R2,PLANCODE_M,INDEX_M,ANN_PREM_PKG,ANN_PREM_M,PLAN_CODE_R,ANN_PREM_R,PREM_FREQ_M,EXPENSE_PC,DUR_MM,PAY_OUT_TYPE,INS_NUMBER,ANNUITY_FREQ,GTEE_PER_Y,PROC_DATE,PLAN_CODE_G,PASTYEAR,PX_ROUND,ANNUITY_TYPE,PROD_NAME,GROUP_ID,INSURED_STS,REGION_CODE,CHANNEL_MODE,DNO,SNO,DEPTNO,BEN_STS,POL_STATUS,PREM_PAYBL_Y,PREM_FREQ,ANNUAL_PREM,DURATIONIF_M,COMM_PAY,PREM_TM_ORG,SPCODE
//  *
//  */
//class DcsIndJob2(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) extends BaseJob(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) {
//
//  import hiveContext.implicits._
//
//   val tableNames = Array("bas_pala_pol_ben_plan_cfg", "bas_lbs_pol_ben_actuary", "bas_lbs_pol_info")
//
//  override def run() = {
//
//    super.init()
//
//    val startTime = System.currentTimeMillis
//    println("appName:" + app)
//    log.info("=======================> 开始 <==========================")
//
//    val PROC_DATE = serviceConfMap.get("PROC_DATE").get
//
//    //TODO：加载个银传统分红产品保单信息表
//    var pol_ben_ind_table = "bas_pala_pol_ben_ind"
//    if (env == "prd") {
//      //      PROC_DATE = PROC_DATE+" 00:00:00"
//      pol_ben_ind_table = s"$pol_ben_ind_table where proc_date like '${PROC_DATE}%'"
//    }
//    val sql01 = s"select * from $pol_ben_ind_table "
//    println(sql01)
//    val initPolBenIndDF =
//      hiveContext.sql(sql01).map(row => {
//        val obj = new BAS_PALA_POL_BEN_IND
//        val scalaClazzName = obj.getClass.getName
//        val clazz = Class.forName(scalaClazzName)
//        val fields = clazz.getDeclaredFields
//
//        fields.foreach(f => {
//          f.setAccessible(true)
//          var fieldName = f.getName
//          if (env == "prd") fieldName = fieldName.toLowerCase
//          f.set(obj, String.valueOf(row.getAs[String](fieldName)))
//        })
//
//        obj
//      })
//
//    //    initPolBenIndDF.cache()
//    println(s"$pol_ben_ind_table 数据量：" + initPolBenIndDF.count())
//
//
//    //TODO 加载DCS基础信息表
//    val basic_info_table = "bas_pala_lar_dcs_basic_info_aio"
//    val sql02 = s"select * from $basic_info_table"
//    println(sql02)
//    val basicInfoDF = hiveContext.sql(sql02).as[LAR_DCS_BASIC_INFO_AIO]
//    basicInfoDF.cache()
//    println(s"$basic_info_table 数据量：" + basicInfoDF.count())
//
//    val basic_info_arr = basicInfoDF.collect()
//    val basic_info_br = sc.broadcast(basic_info_arr)
//
//    println("初始化和数据加载耗时：" + (System.currentTimeMillis() - startTime) / 1000 + " 秒")
//
//    //TODO 加载GROUP_ID 相关配置表
//
//
//    val v_grouping_rule =  sc.broadcast(LoadDcsCommDataJob.getGroupingRules(hiveContext,serviceConfMap))
//    val v_grouping_config = sc.broadcast(LoadDcsCommDataJob.getGroupingConfig(hiveContext,serviceConfMap))
//    val plan_code_g_table = sc.broadcast(LoadDcsCommDataJob.getDcsPlanCodeGCfg(hiveContext,serviceConfMap))
//
//
//    /////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
//    //TODO：业务逻辑
//    val bus_start_time = System.currentTimeMillis()
//    println("业务逻辑处理....")
//
//    // TODO:过滤
//    //跳过无需运行的险种
//    val filtedDF = initPolBenIndDF.filter(x => {
//      val arr = Array("1281", "1033", "1278", "3064", "3083", "3081", "3082")
//      val ruler01 = !arr.contains(x.plan_code)
//      val basicInfoArr = basic_info_br.value
//      var ruler02 = basicInfoArr.map(_.plan_code).contains(x.plan_code)
//      if (ruler02) {
//        basicInfoArr.foreach(y => {
//          if (y.plan_code.equals(x.plan_code)) {
//            if ("N".equals(y.run_in_dcs) || (!"IND".equals(y.work_space))) ruler02 = false
//          }
//        })
//      }
//      var ruler03 = Array("M", "F").contains(x.ins_sex)
//      ruler01 && ruler02 && ruler03
//    })
//
//    //TODO:加载配置数据
//    //按照order.csv顺序执行算法列表（按险种和字段去data.txt算法库中查找该字段的算法）
//    //   val orderArr =  Utils.readConf("/dcs/order.csv").map(x=>{
//    //      val arr = x.split(",")
//    //      arr
//    //    })
//    //
//    //   val dataArr =  Utils.readConf("/dcs/data.csv").drop(1).map(x=>{
//    ////     product,PLAN_CODE,SPCODE,AGE_AT_ENTRY,SEX,ENTRY_YEAR,ENTRY_MONTH,AGE2_ATENTRY,SEX2,POL_STATUS,DIST_CHANNEL,POL_TERM_Y,PREM_PAYBL_Y,PREM_FREQ,SUM_ASSURED,ANNUAL_PREM,UNITS,AGE_PAYMENT,DURATIONIF_M,INIT_POLS_IF,PAYMENT_CODE,POLNO,VAL_CODE,INDEX1,SUM_ASSD_ORG,COMM_INDEX,CI_RIDER,SUM_ASSD_M,SUM_ASSD_R,SUM_ASSD_R2,PLANCODE_M,INDEX_M,ANN_PREM_PKG,COMM_PAY,ANN_PREM_M,PLAN_CODE_R,ANN_PREM_R,PREM_TM_ORG,PREM_FREQ_M,EXPENSE_PC,DUR_MM,PAY_OUT_TYPE,INS_NUMBER,ANNUITY_FREQ,GTEE_PER_Y,PROC_DATE,PLAN_CODE_G,PASTYEAR,PX_ROUND,ANNUITY_TYPE,PLAN_CODE_S,PROD_NAME,GROUP_ID,INSURED_STS,REGION_CODE,CHANNEL_MODE,DNO,SNO,DEPTNO,BEN_STS
//    //     val arr = x.split(",")
//    //     arr
//    //   })
//    //
//    //   val orderBR = sc.broadcast(orderArr)
//    //    val dataBR = sc.broadcast(dataArr)
//
//
//    //TODO：算法
//    val resRDD = filtedDF
//      .map(x => {
//
//        val plan_code = x.plan_code
//
//        val basicInfoArr = basic_info_br.value
//
//        val basicInfo = basicInfoArr.find(r => r.plan_code.equals(plan_code)).get
//
//        (x, basicInfo)
//      })
//      .flatMap(x => {
//
//        val pol_ben_ind = x._1
//
//        val basic_info = x._2
//
//        //      var output = new LAR_DCS_RESULT_IND
//        //      DcsIndFunctions2.process(pol_ben_ind,basic_info,output,PROC_DATE)
//        //      output
//
//        //DcsIndFunctions.process(pol_ben_ind,basic_info,PROC_DATE)
//
//        val args = scala.collection.mutable.Map[String, String]()
//
////        GroupIdBuilder.process(serviceConfMap, args, pol_ben_ind, basic_info, v_grouping_rule.value, v_grouping_config.value, plan_code_g_table.value)
//
//        new ArrayBuffer[LAR_DCS_RESULT_IND].toList
//      })
//
//    println("业务逻辑处理耗时：" + (System.currentTimeMillis() - bus_start_time) / 1000 + " 秒")
//
//    // TODO:输出
//    val out_put_time = System.currentTimeMillis()
//    if (env == "local") {
//      resRDD.foreach(println)
//    }
//    val result_table = "LAR_DCS_RESULT_IND"
//
//    println(s"$result_table 数据量：" + resRDD.count)
//
//    if (env == "prd") {
//      resRDD.coalesce(200, false).saveAsTextFile(serviceConfMap.get("out.hdfs.path") + "_" + System.currentTimeMillis())
//    }
//
//
//    //    hiveContext.sql(s"LOAD DATA  INPATH '${hdfsPath}' OVERWRITE INTO TABLE sx_core_safe.LAR_DCS_RESULT_IND")
//
//    //    val hdfs = org.apache.hadoop.fs.FileSystem.get(sc.hadoopConfiguration)
//    //    val path = new Path(hdfsPath)
//    //    if(hdfs.exists(path)){
//    //      //为防止误删，禁止递归删除
//    //      hdfs.delete(path,false)
//    //    }
//
//    //TODO:结束
//    sc.stop()
//    val endTime = System.currentTimeMillis
//    println("输出耗时：" + (endTime - out_put_time) / 1000 + " 秒")
//
//    println("总耗时：" + (endTime - startTime) / 1000 + " 秒")
//    println("=======================> 结束<==========================")
//
//  }
//
//}


//package com.an.lcloud.ifrs17.dcs.job
//
//import java.text.SimpleDateFormat
//import java.util.Date
//
//import com.an.lcloud.ifrs17.core.framework.comm.DCSConstants
//import com.an.lcloud.ifrs17.core.framework.job.BaseJob
//import com.an.lcloud.ifrs17.core.framework.schema.{LAR_DCS_RESULT_IND, _}
//import com.an.lcloud.ifrs17.core.framework.utils.Utils
//import com.an.lcloud.ifrs17.dcs.udf.LoadDcsCommDataJob
//import com.an.lcloud.ifrs17.dcs.utils.DcsAlgCfgHandler
//import org.apache.spark.SparkContext
//import org.apache.spark.rdd.RDD
//import org.apache.spark.sql.Row
//import org.apache.spark.sql.hive.HiveContext
//import org.apache.spark.sql.types.{StringType, StructField, StructType}
//import utils.DcsUtils
//
//import scala.collection.mutable
//import scala.collection.mutable._
//import scala.io.Source
//
///**
//  * dcs业务处理主类,功能点：
//  * 1.加载公参数据、保单数据
//  * 2.过滤无需处理的保单
//  * 3.保单处理
//  * 4.结果保存至hive
//  *
//  * @param sc sparkContext
//  * @param hiveContext
//  * @param serviceConfMap
//  */
//class DcsIndJobBak(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) extends BaseJob(sc: SparkContext, hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) {
//
//  override def run() = {
//
//    super.init()
//
//    val startTime = System.currentTimeMillis
//    log.info("=======================> 开始 <==========================")
//    readXml(DCSConstants.CONST_XML_FILE_PATH)
//
//    //TODO:加载算法配置
//    val confFile = this.getClass.getResourceAsStream("/dcs/pol_ben_dcs_funcs_cfg.csv")
//    val confArr = Source.fromInputStream(confFile).getLines().toArray.drop(1).filter(_.split(",")(0) == "POL_BEN_MAIN")
//    val suanfaConfArrBC = sc.broadcast(confArr)
//
//
//    /** 加载公参数据并广播 */
//    val groupingRule = sc.broadcast(LoadDcsCommDataJob.getGroupingRules(hiveContext, serviceConfMap))
//    // val groupingConfig = sc.broadcast(LoadDcsCommDataJob.getGroupingConfig(hiveContext, serviceConfMap))
//    val planCodeG = sc.broadcast(LoadDcsCommDataJob.getDcsPlanCodeGCfg(hiveContext, serviceConfMap))
//    val basicInfo = sc.broadcast(LoadDcsCommDataJob.getDcsBasicInfo(hiveContext, serviceConfMap))
//
//
//    val initPolBenIndDF =
//      LoadDcsCommDataJob.loadPolBenIndData(hiveContext, serviceConfMap)
//        .map(row => {
//          val obj = new BAS_PALA_POL_BEN_IND
//          val scalaClazzName = obj.getClass.getName
//          val clazz = Class.forName(scalaClazzName)
//          val fields = clazz.getDeclaredFields
//          fields.foreach(f => {
//            f.setAccessible(true)
//            val fieldName = f.getName.toLowerCase
//            f.set(obj, String.valueOf(row.getAs[String](fieldName)))
//          })
//          obj
//        }).map(x => {
//
//        def null2Zero(str: String): String = {
//          if (str.isEmpty || str.equals("null")) {
//            "0"
//          } else {
//            str
//          }
//        }
//
//        x.unit = null2Zero(x.unit)
//        x.policy_value = null2Zero(x.policy_value)
//        x.ann_std_prem_m = null2Zero(x.ann_std_prem_m)
//        x.ann_std_prem = null2Zero(x.ann_std_prem)
//        x.ann_substd_prem = null2Zero(x.ann_substd_prem)
//        x.ann_prof_prem = null2Zero(x.ann_prof_prem)
//        x.std_modal_prem_m = null2Zero(x.std_modal_prem_m)
//        x.tot_modal_prem = null2Zero(x.tot_modal_prem)
//
//        x.std_modal_prem_r1 = null2Zero(x.std_modal_prem_r1)
//        x.ann_std_prem_r2 = null2Zero(x.ann_std_prem_r2)
//        x.std_modal_prem_r2 = null2Zero(x.std_modal_prem_r2)
//        x.ann_std_prem_r3 = null2Zero(x.ann_std_prem_r3)
//        x.std_modal_prem_r3 = null2Zero(x.std_modal_prem_r3)
//
//        x.ann_std_prem_r1 = null2Zero(x.ann_std_prem_r1)
//        x.ann_std_prem_r3 = null2Zero(x.ann_std_prem_r2)
//        x.ann_std_prem_r3 = null2Zero(x.ann_std_prem_r3)
//        x.ci_rider = null2Zero(x.ci_rider)
//        x.jnt_age = null2Zero(x.jnt_age)
//        x.ins_age = null2Zero(x.ins_age)
//
//        x.sum_assd_org = null2Zero(x.sum_assd_org)
//
//        x.sno = null2Zero(x.sno)
//        x.dno = null2Zero(x.dno)
//        x.jnt_prof = null2Zero(x.jnt_prof)
//
//        x
//      })
//    //    logError("initPolBenIndRDD count:" + initPolBenIndDF.count())
//    println("初始化和数据加载耗时：" + (System.currentTimeMillis() - startTime) / 1000 + " 秒")
//
//
//    /** 设置时间 */
//    DcsUtils.setDateParams(serviceConfMap)
//
//    /** 过滤无需处理的保单记录 */
//    val filteredRDD = filterRDD(initPolBenIndDF, basicInfo.value, serviceConfMap)
//    //    logError("filteredRDD count:" + filteredRDD.count())
//    /** dcs业务处理逻辑 */
//    /** 合理设置分区数 ,最大化利用集群资源,一般为分配到的总core数的2-3倍 */
//    val resultRDD =
//      processFilteredRdd(filteredRDD, basicInfo.value, planCodeG.value, groupingRule.value, serviceConfMap, suanfaConfArrBC.value)
//        .map(pol_ben_ind => {
//          val fields = Class.forName(pol_ben_ind.getClass.getName).getDeclaredFields
//          val values = fields.map(f => {
//            f.setAccessible(true)
//            f.get(pol_ben_ind)
//          }).toSeq
//          Row.fromSeq(values)
//        })
//
//    //    resultRDD.persist(StorageLevel.MEMORY_ONLY_SER)
//
//    val obj = new LAR_DCS_RESULT_IND
//    val fields = Class.forName(obj.getClass.getName).getDeclaredFields
//    val schema = StructType(
//      fields.map(_.getName).map(fieldName => StructField(fieldName, StringType, true))
//    )
//
//    logError("resultRDD count:" + resultRDD.count())
//    val result = hiveContext.createDataFrame(resultRDD, schema)
//    //    result.show()
//
//    /** 结果保存至 hive */
//    result.registerTempTable("dcs_ind_result")
//    val res_table = serviceConfMap.get("res_table").get
//    val p_proc_month = serviceConfMap.get("p_proc_month").get
//
//    val sql = s"INSERT INTO TABLE $res_table PARTITION(PROC_MONTH='$p_proc_month',VERSION_NUM='$p_proc_month') SELECT * from dcs_ind_result"
//    logError(sql)
//    hiveContext.sql(sql)
//
//    val endTime = System.currentTimeMillis
//    println("耗时：" + (endTime - startTime) / 1000 + " 秒")
//
//  }
//
//  /**
//    * 处理过滤后的RDD,DCS业务主要处理逻辑
//    *
//    * @param filteredRDD      过滤后的RDD
//    * @param basicInfoMap     DCS基础信息
//    * @param planCodeGMap     plan_Code_g
//    * @param groupingRulesMap 规则算法配置信息
//    * @param serviceConfMap   参数集合
//    * @param suanfaConfArr    DCS算法配置
//    * @return
//    */
//  def processFilteredRdd(
//                          filteredRDD: RDD[BAS_PALA_POL_BEN_IND],
//                          basicInfoMap: mutable.HashMap[String, LAR_DCS_BASIC_INFO_AIO],
//                          planCodeGMap: mutable.HashMap[String, LAR_DCS_PLAN_CODE_G_CFG],
//                          groupingRulesMap: mutable.HashMap[String, mutable.HashMap[Int, LAR_GROUPING_RULES]],
//                          serviceConfMap: HashMap[String, String],
//                          suanfaConfArr: Array[String]
//                        ) = {
//
//        val resultRDD = filteredRDD
//          .coalesce(500, shuffle = false)
//          .mapPartitions(pol_ben_inds => {
//
//          val list = new ListBuffer[LAR_DCS_RESULT_IND]
//          val now = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date())
//
//          pol_ben_inds.foreach(pol_ben_ind => {
//
//            val planCode = pol_ben_ind.plan_code
//
//            //TODO:调用算法
//            val basic_info = basicInfoMap.get(planCode).get
//            val args = scala.collection.mutable.Map[String, String]()
//            args.++=(Utils.obj2MapOfCaseClass(basic_info))
//            args.++=(Utils.obj2MapOfCaseClass(pol_ben_ind))
//            args.++=(serviceConfMap)
//            args("PLAN_CODE_S") = pol_ben_ind.plan_code
//            args("INVALID") = "false"
//            val startTime = System.currentTimeMillis
//            /** 各算法调用，获取rdd,通过java反射为对象赋值 */
//            DcsAlgCfgHandler.invokeDcsAlg(suanfaConfArr, args, pol_ben_ind.plan_code, groupingRulesMap)
//            println("耗时：" + (System.currentTimeMillis - startTime) / 1000 + " 秒")
//            args("CREATED_BY") = "#"
//            args("CREATED_DATE") = now
//            args("UPDATED_BY") = "#"
//            args("UPDATED_DATE") = now
//
//            if (args("INVALID") == "false") {
//              val resultBean = new LAR_DCS_RESULT_IND
//              Utils.map2obj(args.toMap, resultBean)
//              list += (resultBean)
//            }
//          })
//          list.iterator
//        })
//
//        resultRDD
//  }
//
//  /**
//    * 返回过滤后的rdd
//    * 基本过滤由读hive表时实现
//    *
//    * @param initPolBenIndRDD :BAS_PALA_POL_BEN_IND实例RDD
//    * @param basicInfoMap     :DCS基础信息
//    */
//  def filterRDD(initPolBenIndRDD: RDD[BAS_PALA_POL_BEN_IND], basicInfoMap: mutable.HashMap[String, LAR_DCS_BASIC_INFO_AIO],
//                serviceConfMap: HashMap[String, String]) = {
//    val filteredRDD = initPolBenIndRDD.filter(x => {
//
//      val ruler00 = basicInfoMap.keySet.contains(x.plan_code)
//
//      var ruler01 = true
//      if (ruler00) {
//        val basic_info = basicInfoMap.get(x.plan_code).get
//        ruler01 = basic_info.run_in_dcs != "N" && basic_info.work_space == "IND"
//      }
//
//      val arr = Array("1281", "1033", "1278", "3064", "3083", "3081", "3082")
//      val ruler02 = !arr.contains(x.plan_code)
//
//      var ruler03 = Array("M", "F").contains(x.ins_sex)
//
//      ruler00 && ruler01 && ruler02 && ruler03
//
//      /**
//        * 使用map代替数组,map接近O(1),数组O(n),减少数组遍历次数
//        */
//      /** run_in_dcs、word_space、dist_channel、prod_name判断 */
//      //      var ruler02: Boolean = true
//      //      val prodNameArr = Array("CG", "UB", "UI")
//      //      if (basicInfoMap.contains(x.plan_code)) {
//      //        val bean = basicInfoMap(x.plan_code)
//      //        if (bean.run_in_dcs == "N" || bean.work_space != "IND" || bean.dist_channel.toInt == 21 ||
//      //          prodNameArr.contains(bean.prod_name.substring(0, 2))) {
//      //          ruler02 = false
//      //        }
//      //      }
//
//      /**
//        * 1.过滤保单 2.过滤basicInfo 3.v_dist_channel 4. v_pol_ind(i).eff_date > v_val_date
//        * 5.substr(v_prod_name, 1, 2) IN ('CG', 'UB', 'UI')
//        */
//
//
//      /** 生效日期判断  改有读取hive表实现 */
//      //      val effDate = DateUtils.strToDateTime(x.EFF_DATE, DateUtils.YYYY_MM_DD)
//      //      val valDate = DateUtils.strToDateTime(serviceConfMap("v_val_date"), DateUtils.YYYY_MM_DD)
//      //      val ruler04: Boolean = if (effDate.isAfter(valDate)) {
//      //        false
//      //      } else {
//      //        true
//      //      }
//
//      /** 保单状态ben_sts判断  改有读hive表实现 */
//      //      val benStsArr = Array("C", "D", "E", "M", "N", "S", "T", "X", "F", "Z", "Y", "0", "1", "K", "U", "O", "V", "K")
//      //      val ruler05: Boolean = if (benStsArr.contains(x.BEN_STS)) {
//      //        false
//      //      } else {
//      //        true
//      //      }
//    })
//    filteredRDD
//  }
//
//}



package com.an.lcloud.ifrs17.dcs.udf

import com.an.lcloud.ifrs17.core.framework.comm.DCSConstants
import com.an.lcloud.ifrs17.core.framework.job.JobCommon
import com.an.lcloud.ifrs17.core.framework.schema._
import org.apache.spark.sql.hive.HiveContext

import scala.collection.mutable
import scala.collection.mutable.HashMap

/**
  * 加载DCS公共数据类
  * Created by ZOUBO162 on 2018-11-22.
  */
object LoadDcsCommDataJob extends JobCommon {


  /**
    * grouping规则配置
    *
    * @param hiveContext
    * @param serviceConfMap
    * @param processCode 个银传统分红批处理编码
    */
  def getGroupingRules(hiveContext: HiveContext, serviceConfMap: HashMap[String, String], processCode: String = DCSConstants.CONST_GROUPING_PROC_CODE_IND) = {
    import  hiveContext.implicits._
    val ruleMap = new mutable.HashMap[String, mutable.HashMap[Int, LAR_GROUPING_RULES]]()
    serviceConfMap.put("p_proc_code", processCode)
    val sql = selectMapping(serviceConfMap.getOrElse("lar_grouping_rules", ""), serviceConfMap)
    logError(sql)
    if (sql.length > 1) {
      val ruleDF = hiveContext.sql(sql).as[LAR_GROUPING_RULES]
      ruleDF.collect.foreach(row => {
        val productName = row.product_name
        val rowNum = row.row_num
        if (ruleMap.contains(productName)) {
          ruleMap.getOrElse(productName, new HashMap[Int, LAR_GROUPING_RULES]()) += (rowNum.toInt -> row)
        } else {
          val map = new mutable.HashMap[Int, LAR_GROUPING_RULES]()
          map += (rowNum.toInt -> row)
          ruleMap += (productName -> map)
        }
      })
    }
    ruleMap
  }

  /**
    * 加载DCS基础信息表
    *
    * @param hiveContext
    * @param serviceConfMap 参数集合
    */
  def getDcsBasicInfo(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import  hiveContext.implicits._
    val map = new mutable.HashMap[String, LAR_DCS_BASIC_INFO_AIO]()
    val sql = selectMapping(serviceConfMap.getOrElse("lar_dcs_basic_info_aio", ""), serviceConfMap)
    if (sql.length > 1) {
      val basicInfoDF = hiveContext.sql(sql).as[LAR_DCS_BASIC_INFO_AIO]
      basicInfoDF.collect.foreach(row => {
        val planCode = row.plan_code
        map += (planCode -> row)
      })
    }
    map
  }

  /**
    * grouping 配置
    *
    * @param hiveContext
    * @param serviceConfMap
    */
  def getGroupingConfig(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import  hiveContext.implicits._
    val map = new mutable.HashMap[String, LAR_GROUPING_CONFIG]()
    val sql = selectMapping(serviceConfMap.getOrElse("lar_grouping_config", ""), serviceConfMap)
    if (sql.length > 1) {
      val configDF = hiveContext.sql(sql).as[LAR_GROUPING_CONFIG]
      configDF.collect.foreach(row => {
        map += (row.proc_code -> row)
      })
    }
    map
  }

  /**
    * Grouping算法配置表
    *
    * @param hiveContext
    * @param serviceConfMap
    */
  def getGroupingCalculation(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import  hiveContext.implicits._
    val map = new mutable.HashMap[String, mutable.HashMap[String, LAR_GROUPING_CALCULATION]]()
    val sql = selectMapping(serviceConfMap.getOrElse("lar_grouping_calculation", ""), serviceConfMap)
    if (sql.length > 1) {
      val calculation = hiveContext.sql(sql).as[LAR_GROUPING_CALCULATION]
      calculation.collect.foreach(row => {
        val calcName = row.calc_name
        val columnName = row.column_name
        if (map.contains(calcName)) {
          map.getOrElse(calcName, new mutable.HashMap[String, LAR_GROUPING_CALCULATION]()) += (columnName -> row)
        } else {
          val columnMap = new mutable.HashMap[String, LAR_GROUPING_CALCULATION]()
          columnMap += (columnName -> row)
          map += (calcName -> columnMap)
        }
      })
    }
    map
  }

  /**
    * plan_code_g配置
    *
    * @param hiveContext
    * @param serviceConfMap
    */
  def getDcsPlanCodeGCfg(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) = {
    import  hiveContext.implicits._
    val map = new mutable.HashMap[String, LAR_DCS_PLAN_CODE_G_CFG]()
    val sql = selectMapping(serviceConfMap.getOrElse("lar_dcs_plan_code_g_cfg", ""), serviceConfMap)
    if (sql.length > 1) {
      val planCodeG = hiveContext.sql(sql).as[LAR_DCS_PLAN_CODE_G_CFG]
      planCodeG.collect.foreach(row => {
        map += (row.plan_code -> row)
      })
    }
    map
  }

  /**
    * 保单数据
    * @param hiveContext
    * @param serviceConfMap
    */
  def loadPolBenIndData(hiveContext: HiveContext, serviceConfMap: HashMap[String, String]) ={
   val sql=selectMapping(serviceConfMap.getOrElse("bas_pala_pol_ben_ind",""),serviceConfMap)
    logError(sql)
   hiveContext.sql(sql)
  }

}





import java.text.SimpleDateFormat
import java.util.Date

import com.an.lcloud.ifrs17.core.framework.job.JobCommon
import com.an.lcloud.ifrs17.core.framework.schema.{BAS_PALA_POL_BEN_IND, LAR_DCS_BASIC_INFO_AIO, POL_BEN_DCS_FUNCS_CFG}
import com.an.lcloud.ifrs17.core.framework.utils.DateWrapper
import com.an.lcloud.ifrs17.dcs.job.DcsIndJob
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{Logging, SparkConf, SparkContext}

/**
  * Created by EX-ZHANGYONGTIAN001 on 2018-10-31.
  */
object DcsIndExecutor extends JobCommon with Logging {

  def main(args: Array[String]): Unit = {

    val startTime = System.currentTimeMillis

    log.info("=======================> 开始 <==========================")

    val app = s"${this.getClass.getSimpleName}".filter(!_.equals('$'))

    // TODO: 初始化参数集合
    println("参数集合....")
    args.foreach(arg => {
      println("添加参数:" + arg)
      val param = arg.split('=')
      logInfo(arg)
      if (param.length == 2) {
        serviceConfMap.put(param(0), param(1))
      } else {
        throw new IllegalArgumentException(s"the format of $arg is wrong")
      }
    })

    // TODO: 日志级别设置
    val logLevel = serviceConfMap.getOrElse("log.level", "error")
    println("日志级别：" + logLevel)

    logLevel match {
      case "debug" => Logger.getLogger("org").setLevel(Level.DEBUG)
      case "info" => Logger.getLogger("org").setLevel(Level.INFO)
      case "warn" => Logger.getLogger("org").setLevel(Level.WARN)
      case "error" => Logger.getLogger("org").setLevel(Level.ERROR)
    }

    // TODO: 运行环境 （研发、测试、生产）
    val env = serviceConfMap.getOrElse("env", "local")
    println("运行模式：" + env)


    // TODO: 验证参数
    println("验证参数....")

    val sdf_yyyyMMdd = new SimpleDateFormat("yyyyMMdd")
    val sdf_yyyy_MM_dd = new SimpleDateFormat("yyyy-MM-dd")


    //参数校验
    if (args.length < 5) {
      logError("Usage: 未指定参数列表 " +
        " [PROC_DATE] [SPCODE_SELECTION][src_version_num][res_version_num][res_hive_db]")
      System.exit(1)
    }

    //    val PROC_DATE_PARTION = serviceConfMap.get("PROC_DATE").get
    //    val PROC_DATE = sdf_yyyy_MM_dd.format(new DateWrapper(sdf_yyyyMMdd.parse(PROC_DATE_PARTION)).getLastDayOfLastMonth().date)
    //    serviceConfMap.put("PROC_DATE_PARTION", PROC_DATE_PARTION)

    val p_proc_month = serviceConfMap.get("PROC_DATE").get.substring(0,6)

    val PROC_DATE = sdf_yyyy_MM_dd.format(sdf_yyyyMMdd.parse(serviceConfMap.get("PROC_DATE").get))

    val VAL_DATE = PROC_DATE + " 00:00:00"

    val SPCODE_SELECTION = serviceConfMap.get("SPCODE_SELECTION").get.toInt


    //源表版本号如果没有取最大值
    val src_version_num = serviceConfMap.get("src_version_num").getOrElse("#")

    //目标表版本号如果没有取当前时间
    var res_version_num = serviceConfMap.get("res_version_num").getOrElse("#")
    if (res_version_num == "#") res_version_num = sdf_yyyyMMdd.format(new Date())

    val res_hive_db = serviceConfMap.get("res_hive_db").getOrElse("#")

    serviceConfMap.put("PROC_DATE", PROC_DATE)
    serviceConfMap.put("VAL_DATE", VAL_DATE)
    serviceConfMap.put("p_proc_date", PROC_DATE)
    serviceConfMap.put("p_proc_month", p_proc_month)
    serviceConfMap.put("SPCODE_SELECTION", SPCODE_SELECTION.toString)
    serviceConfMap.put("src_version_num", src_version_num)
    serviceConfMap.put("res_version_num", res_version_num)
    serviceConfMap.put("res_hive_db", res_hive_db)
    serviceConfMap.put("res_table", res_hive_db + ".bas_pala_lar_dcs_result_ind")

    /////////////////////////////////////////////////////////////////////////////////////////////////

    // TODO：初始化环境
    println("初始化环境....")


    val sparkConf = new SparkConf()
      .setAppName(app)
      .set /*spark.sql.codegen 是否预编译sql成java字节码，长时间或频繁的sql有优化效果*/ ("spark.sql.codegen", "true")
      .set /*spark.sql.inMemoryColumnarStorage.batchSize 一次处理的row数量，小心oom*/ ("spark.sql.inMemoryColumnarStorage.batchSize", "20000")
      .set /*spark.sql.inMemoryColumnarStorage.compressed 设置内存中的列存储是否需要压缩*/ ("spark.sql.inMemoryColumnarStorage.compressed", "true")
      .set /*spark.sql.autoBroadcastJoinThreshold,解决数据倾斜*/ ("spark.sql.autoBroadcastJoinThreshold", "20971520")
      .set("hive.execution.engine", "spark")
      //      .set("spark.sql.shuffle.partitions","200")
      .set /*Kryo序列化*/ ("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set /*Kryo序列化*/ ("spark.kryoserializer.buffer.mb", "10")
      .registerKryoClasses(Array[Class[_]](classOf[BAS_PALA_POL_BEN_IND], classOf[LAR_DCS_BASIC_INFO_AIO], classOf[POL_BEN_DCS_FUNCS_CFG], classOf[Row]))

    val sc = new SparkContext(sparkConf)

    logLevel match {
      case "debug" => sc.setLogLevel(Level.DEBUG.toString)
      case "info" => sc.setLogLevel(Level.INFO.toString)
      case "warn" => sc.setLogLevel(Level.WARN.toString)
      case "error" => sc.setLogLevel(Level.ERROR.toString)
    }

    val hiveContext = new HiveContext(sc)

    println("===============启动任务=======================")

    ///////////////////////////////////////////////////////////////////////////

    println(PROC_DATE + "@DcsIndJob start...")

    new DcsIndJob(sc, hiveContext, serviceConfMap).run()

    println(PROC_DATE + "@DcsIndJob end....")


    ///////////////////////////////////////////////////////////////////////////

    //TODO:结束
    sc.stop()
    println("耗时：" + (System.currentTimeMillis - startTime) / 1000 + " 秒")
    println("=======================> 结束<==========================")
  }

}



<?xml version="1.0" encoding="UTF-8"?>
<sqlStore>
    <dbName>sx_core_safe</dbName>
    <saveMode>append</saveMode>

    <!--全量单预处理
    1.pol_ben_actuary
    2.新单表
    3.合同分组表
    4.特殊现金流表
    5.生存金明细表
    -->
    <pol_ben_ind_if>
        <!--INSERT INTO TABLE ${res_table} PARTITION(PROC_MONTH='${p_proc_month}', VERSION_NUM='${res_version_num}')-->
        select
        a.deptno,
        a.polno,
        a.brno ,
        a.plan_type,
        a.plan_code ,
        a.eff_date ,
        a.first_eff_date,
        a.ben_sts,
        a.units ,
        a.sum_ins ,
        a.matu_date ,
        a.ins_sex ,
        a.ins_age ,
        a.ins_prof ,
        a.jnt_sex ,
        a.jnt_age ,
        a.jnt_prof ,
        a.period ,
        a.prem_term ,
        a.prem_type ,
        a.ann_std_prem,
        a.ann_substd_prem ,
        a.ann_prof_prem ,
        a.std_modal_prem ,
        a.prof_prem ,
        a.substd_prem ,
        a.tot_modal_prem,
        a.pay_to_date ,
        a.payout_age ,
        a.payout_date ,
        a.ben_level ,
        a.region_code ,
        a.policy_fee ,
        a.part_to_date ,
        a.pua_sum ,
        a.payout_type ,
        a.payout_grade ,
        a.coverage_option ,
        a.pk_serial ,
        a.fcu ,
        a.date_fcd ,
        a.lcu ,
        a.date_lcd ,
        a.created_by ,
        a.created_date ,
        a.updated_by ,
        a.updated_date ,
        a.insno ,
        <!---->
        '' as confirm_method ,
        '' as fcd ,
        '' as lcd ,
        <!--合同分组-->
        b.tg_method ,
        b.tg_risk_type ,
        b.tg_pl string ,
        b.tg_entryyear ,
        b.tg_ins_risk ,
        <!--保单贷款-->
        <!--if(loan_iss_amt>0,'1','0') as loan_idx ,-->
        cashIndex('借款本金',e.cash_flow) as loan_idx,
        <!--nvl(loan_iss_amt,0) as loan_iss_amt ,-->
        specialCashFlow('借款本金',e.cash_flow) as loan_iss_amt,
        <!--nvl(loan_int_acc,0) as loan_int_acc ,-->
        specialCashFlow('借款利息',e.cash_flow) as loan_int_acc,
        <!--保单自垫-->
        <!--if(apl_iss_amt>0,'1','0') as apl_idx  ,-->
        cashIndex('自垫本金',e.cash_flow) as apl_idx,
        <!--nvl(apl_iss_am,0) as apl_iss_amt ,-->
        specialCashFlow('自垫本金',e.cash_flow) as apl_iss_amt,
        <!--nvl(apl_int_acc,0) as apl_int_acc ,-->
        specialCashFlow('自垫利息',e.cash_flow) as apl_int_acc,
        <!--生存金-->
        nvl(e.polno,'1','0') as surv_option ,
        <!--nvl(surv_acc_prc,0) as surv_acc_prc ,-->
        specialCashFlow('累计生息生存金本金',e.cash_flow) as surv_acc_prc,
        <!--nvl(surv_acc_int,0) as surv_acc_int ,-->
        specialCashFlow('累计生息生存金利息',e.cash_flow) as surv_acc_int,
        <!--保单红利-->
        if(info.div_opt1=2,'1','0') as div_option ,
        <!--nvl(div_acc_prp,0) as div_acc_prp ,-->
        specialCashFlow('累计生息红利本金',e.cash_flow) as div_acc_prp,
        <!--nvl(div_acc_int,0) as div_acc_int ,-->
        specialCashFlow('累计生息红利利息',e.cash_flow) as div_acc_int,
        a.region_sid,
        nvl(d.is_nb,'N') as is_nb，
        cg.spcode_i17 as spcode_i17
        from (select * from bas_act_pol_ben_actuary
        where proc_date='${p_proc_date}') a
        left join(select polno,div_opt1 region_sid from bas_lbs_pol_info) info
        on(a.region_sid=info.region_sid and a.polno=info.polno)
        <!--合同分组-->
        left join (
        select
        <!--保单生效年分组标签，ENTRY_YEAR – 1988-->
        (tg_entry_year - 1988) as tg_entryyear,
        <!--1-5分别对应 分红、传统高、传统低、投连、万能-->
        tg_method,
        <!--1-3,分别对应 盈利、其他、亏损-->
        tg_pl,
        <!--0-1,重大风险测试结果,1通过，0未通过-->
        tg_ins_risk,
        <!--1-3,分别对应 健康、身故意外、年金-->
        tg_risk_type,
        <!--和同组编号-->
        contract_group_code as contract_group_code,
        region_sid
        from BAS_ACT_CG_CONTRACT_GROUP
        where proc_date='${p_proc_date}'
        and cg_valid_end_date is null or cg_valid_end_date=''
        ) b
        on(a.region_sid=b.region_sid
        and a.polno=b.polno
        and a.brno=b.brno
        and a.plan_code=b.plan_code)
        <!--特殊现金流-->
        left join cp_special_cash_flow_bal c
        on(a.region_sid=c.region_sid
        and a.polno=c.polno
        and a.brno=c.brno
        and a.plan_code=c.plan_code
        )
        <!--新单表，犹豫期脱退MP需求-->
        left join cg_new_pol_init d
        on(a.region_sid=d.region_sid
        and a.polno=d.polno
        and a.brno=d.brno
        and a.plan_code=d.plan_code
        )
        <!--生存金明细表-->
        left join interest_polno e
        on(a.region_sid=e.region_sid
        and a.polno=e.polno
        and a.brno=e.brno
        and a.plan_code=e.plan_code)
        <!--合同分组编号表-->
        left join cg_contract_group_code cg
        on(b.contract_group_code=cg.contract_group_code)
    </pol_ben_ind_if>

    <!--NB拆分
    所需表：
    1.pol_ben_actuary
    2.BAS_ACT_CG_NEW_POL_INIT
    -->
    <pol_ben_ind_nb>
        select
        a.deptno,
        a.polno,
        a.brno ,
        a.plan_type,
        a.plan_code ,
        a.eff_date ,
        a.first_eff_date,
        a.ben_sts,
        a.units ,
        a.sum_ins ,
        a.matu_date ,
        a.ins_sex ,
        a.ins_age ,
        a.ins_prof ,
        a.jnt_sex ,
        a.jnt_age ,
        a.jnt_prof ,
        a.period ,
        a.prem_term ,
        a.prem_type ,
        a.ann_std_prem,
        a.ann_substd_prem ,
        a.ann_prof_prem ,
        a.std_modal_prem ,
        a.prof_prem ,
        a.substd_prem ,
        a.tot_modal_prem,
        a.pay_to_date ,
        a.payout_age ,
        a.payout_date ,
        a.ben_level ,
        a.region_code ,
        a.policy_fee ,
        a.part_to_date ,
        a.pua_sum ,
        a.payout_type ,
        a.payout_grade ,
        a.coverage_option ,
        a.pk_serial ,
        a.fcu ,
        a.date_fcd ,
        a.lcu ,
        a.date_lcd ,
        a.created_by ,
        a.created_date ,
        a.updated_by ,
        a.updated_date ,
        a.insno ,
        '' as confirm_method ,
        '' as fcd ,
        '' as lcd ,
        '0' as tg_method ,
        '0' as tg_risk_type ,
        '0' as tg_pl string ,
        '0' as tg_entryyear ,
        '0' as tg_ins_risk ,
        '0' as loan_idx ,
        0 as loan_iss_amt ,
        0 as loan_int_acc ,
        '0' as apl_idx string ,
        0 as apl_iss_amt ,
        0 as apl_int_acc ,
        '0' as surv_option ,
        0 as surv_acc_prc ,
        0 as surv_acc_int ,
        '0' as div_option ,
        0 as div_acc_prp ,
        0 as div_acc_int ,
        a.region_sid,
        b.is_nb as is_nb
        from (select * from bas_act_pol_ben_actuary
        where proc_date='${p_proc_date}') a
        join cg_new_pol_init b
        on(a.region_sid=b.region_sid
        and a.polno=b.polno
        and a.brno=b.brno
        and a.plan_code=b.plan_code
        )
    </pol_ben_ind_nb>

    <!--合同分组编号表-->
    <act_cg_contract_group_code>
        cache table cg_contract_group_code as
        select
        CONTRACT_GROUP_CODE,
        ID_ACT_CG_CONTRACT_GROUP_CODE as spcode_i17
        from BAS_ACT_CG_CONTRACT_GROUP_CODE
    </act_cg_contract_group_code>
    <!--新单表-->
    <cg_new_pol_init>
        cache table cg_new_pol_init as
        select
        region_sid,
        polno,
        brno,
        plan_code,
        'Y' as is_nb
        from bas_act_cg_new_pol_init
        where proc_date='${p_proc_date}'
        and new_pol_fst_proc_month='${p_proc_month}'
    </cg_new_pol_init>
    <!--生存金-->
    <interest_polno>
        cache table interest_polno as
        select
        distinct
        a.polno,
        a.brno,
        a.plan_code,
        a.region_sid
        from
        (select polno,brno,plan_code,region_sid from bas_lbs_pol_ben
        where ben_sts in ('I', 'B', 'A', 'P', 'J', 'W', 'R', 'L', 'H', 'G')) a
        join payment_plan_table b
        on(a.plan_code=b.plan_code and a.region_sid=b.region_sid)
        join (select polno,region_code from bas_act_beneficiary_info
        where PROC_DATE='${p_proc_date}'
        and status = 'N') c
        on(a.polno=c.polno and a.region_sid=c.region_sid)
        left join special_payment_policy d
        on(a.polno=d.polno and a.region_sid=d.region_sid)
        where d.polno is null
    </interest_polno>
    <!--给付险种定义表-->
    <payment_plan_table>
        cache table payment_plan_table as
        select plan_code,region_sid
        from bas_act_payment_plan_table
        where PROC_DATE='${p_proc_date}' and interest_flag = 'Y'
    </payment_plan_table>
    <!--特殊保单记录表-->
    <special_payment_policy>
        cache table special_payment_policy as
        select
        polno,
        region_sid
        from
        bas_act_special_payment_policy
        where special_payment_code = '07'
    </special_payment_policy>

    <!--特殊现金流-->
    <cp_special_cash_flow_bal>
        cache table cp_special_cash_flow_bal as
        select
        a.polno,
        a.brno,
        a.plan_code,
        a.region_sid,
        concat_ws(','collect_set(a.cash_flow))
        from
        (select
        polno,
        brno,
        plan_code,
        region_sid,
        concat_ws('_',project_name,end_bal) as cash_flow
        <!--project_name,-->
        <!--end_bal,-->
        from
        bas_act_cp_special_cash_flow_bal
        where proc_date='${p_proc_date}') a
        group by a.polno,brno,plan_code,region_sid
    </cp_special_cash_flow_bal>
</sqlStore>
