数据量大的时候，对数据进行采样，然后再做模型分析。作为数据仓库的必备品hive，我们如何对其进行采样呢？

当然，浪尖写本文还有另一个目的就是复习hive的四by。不知是否有印象呢？

Hive : SORT BY vs ORDER BY vs DISTRIBUTE BY vs CLUSTER BY

欢迎点击阅读原文，加入浪尖知识星球。



假设有一张包含100亿行的Hive表，希望有效地随机抽样一个固定行数的数据 - 比如10000。最明显（而且显然是错误的）的方法是：
select * from my_table
limit 10000;
如果不对表进行排序，Hive不保证数据的顺序，但在实践中，它们按照它们在文件中的顺序返回，所以这远非真正随机。那么接着可以尝试：

select * from my_table
order by rand()
limit 10000;
这确实提供了真正的随机数据，但性能并不是那么好。为了实现总排序，Hive必须将所有数据强制传输到单个reducer。该reducer将对整个数据集进行排序。这很不好。幸运的是，Hive有一个非标准SQL“sort by”子句，它只在单个reducer中排序，并且不保证数据跨多个reducers中排序：

select * from my_table
sort by rand()
limit 10000;
这要好得多，但我不相信它真的是随机的。问题是Hive的将数据拆分为多个reducer的方法是未定义的。它可能是真正随机的，它可能基于文件顺序，它可能基于数据中的某些值。Hive如何在reducers中实现limit子句也是未定义的。也许它按顺序从reducer中获取数据 - 即，reducer 0中的所有数据，然后全部来reducer1，等等。也许它通过它们循环并将所有内容混合在一起。

在最坏的情况下，假设reduce 的key是基于数据列，而limit子句是reducers的顺序。然后样品会非常倾斜。

解决方案是另一个非标准的Hive功能：“distribute by”。对于reduce key不是由查询结构确定的查询（没有“group by”，没有join），可以准确指定reduce key的内容。如果我们随机分布，并在每个reducer中随机排序，那么“limit”功能如何无关紧要。

select * from my_table
distribute by rand()
sort by rand()
limit 10000;
最后，作为最后一次优化，可以在map-side做一些过滤。如果表的总大小是已知的，轻松设置一个随机阈值条件来进行数据过滤，如下所示：

select * from my_table
where rand() <= 0.0001
distribute by rand()
sort by rand()
limit 10000;
在这种情况下，由于总大小是100亿，样本大小是一万，我可以很容易地计算出样本占总数据的0.000001。但是，如果where子句是“rand（）<0.000001”，则最终输出的行数可能少于10000行。“rand（）<0.000002”可能会起作用，但这确实依赖于rand（）有非常好的实现。最后它并不重要，因为瓶颈是全表扫描，而不是传输给reducer的这点数据。



DOUBLE

round(DOUBLE a)

Returns the rounded BIGINT value of a.

返回对a四舍五入的BIGINT值

DOUBLE

round(DOUBLE a, INT d)

Returns a rounded to d decimal places.

返回DOUBLE型d的保留n位小数的DOUBLW型的近似值

DOUBLE	bround(DOUBLE a)	Returns the rounded BIGINT value of a using HALF_EVEN rounding mode (as of Hive 1.3.0, 2.0.0). Also known as Gaussian rounding or bankers' rounding. Example: bround(2.5) = 2, bround(3.5) = 4.
银行家舍入法（1~4：舍，6~9：进，5->前位数是偶：舍，5->前位数是奇：进）
DOUBLE	bround(DOUBLE a, INT d)	Returns a rounded to d decimal places using HALF_EVEN rounding mode (as of Hive 1.3.0, 2.0.0). Example: bround(8.25, 1) = 8.2, bround(8.35, 1) = 8.4.
银行家舍入法,保留d位小数
BIGINT

floor(DOUBLE a)

Returns the maximum BIGINT value that is equal to or less than a

向下取整，最数轴上最接近要求的值的左边的值  如：6.10->6   -3.4->-4

BIGINT

ceil(DOUBLE a), ceiling(DOUBLE a)

Returns the minimum BIGINT value that is equal to or greater than a.

求其不小于小给定实数的最小整数如：ceil(6) = ceil(6.1)= ceil(6.9) = 6

DOUBLE

rand(), rand(INT seed)

Returns a random number (that changes from row to row) that is distributed uniformly from 0 to 1. Specifying the seed will make sure the generated random number sequence is deterministic.

每行返回一个DOUBLE型随机数seed是随机因子

DOUBLE

exp(DOUBLE a), exp(DECIMAL a)

Returns ea where e is the base of the natural logarithm. Decimal version added in Hive 0.13.0.

返回e的a幂次方， a可为小数

DOUBLE

ln(DOUBLE a), ln(DECIMAL a)

Returns the natural logarithm of the argument a. Decimal version added in Hive 0.13.0.

以自然数为底d的对数，a可为小数

DOUBLE

log10(DOUBLE a), log10(DECIMAL a)

Returns the base-10 logarithm of the argument a. Decimal version added in Hive 0.13.0.

以10为底d的对数，a可为小数

DOUBLE

log2(DOUBLE a), log2(DECIMAL a)

Returns the base-2 logarithm of the argument a. Decimal version added in Hive 0.13.0.

以2为底数d的对数，a可为小数

DOUBLE

log(DOUBLE base, DOUBLE a)

log(DECIMAL base, DECIMAL a)

Returns the base-base logarithm of the argument a. Decimal versions added in Hive 0.13.0.

以base为底的对数，base 与 a都是DOUBLE类型

DOUBLE

pow(DOUBLE a, DOUBLE p), power(DOUBLE a, DOUBLE p)

Returns ap.

计算a的p次幂

DOUBLE

sqrt(DOUBLE a), sqrt(DECIMAL a)

Returns the square root of a. Decimal version added in Hive 0.13.0.

计算a的平方根

STRING

bin(BIGINT a)

Returns the number in binary format (see http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_bin).

计算二进制a的STRING类型，a为BIGINT类型

STRING

hex(BIGINT a) hex(STRING a) hex(BINARY a)

If the argument is an INT or binary, hex returns the number as a STRING in hexadecimal format. Otherwise if the number is a STRING, it converts each character into its hexadecimal representation and returns the resulting STRING. (Seehttp://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_hex, BINARY version as of Hive 0.12.0.)

计算十六进制a的STRING类型，如果a为STRING类型就转换成字符相对应的十六进制

BINARY

unhex(STRING a)

Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of the number. (BINARY version as of Hive 0.12.0, used to return a string.)

hex的逆方法

STRING

conv(BIGINT num, INT from_base, INT to_base), conv(STRING num, INT from_base, INT to_base)

Converts a number from a given base to another (see http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_conv).

将GIGINT/STRING类型的num从from_base进制转换成to_base进制

DOUBLE

abs(DOUBLE a)

Returns the absolute value.

计算a的绝对值

INT or DOUBLE

pmod(INT a, INT b), pmod(DOUBLE a, DOUBLE b)

Returns the positive value of a mod b.

a对b取模

DOUBLE

sin(DOUBLE a), sin(DECIMAL a)

Returns the sine of a (a is in radians). Decimal version added in Hive 0.13.0.

求a的正弦值

DOUBLE

asin(DOUBLE a), asin(DECIMAL a)

Returns the arc sin of a if -1<=a<=1 or NULL otherwise. Decimal version added in Hive 0.13.0.

求d的反正弦值

DOUBLE

cos(DOUBLE a), cos(DECIMAL a)

Returns the cosine of a (a is in radians). Decimal version added in Hive 0.13.0.

求余弦值

DOUBLE

acos(DOUBLE a), acos(DECIMAL a)

Returns the arccosine of a if -1<=a<=1 or NULL otherwise. Decimal version added in Hive 0.13.0.

求反余弦值

DOUBLE

tan(DOUBLE a), tan(DECIMAL a)

Returns the tangent of a (a is in radians). Decimal version added in Hive 0.13.0.

求正切值

DOUBLE

atan(DOUBLE a), atan(DECIMAL a)

Returns the arctangent of a. Decimal version added in Hive 0.13.0.

求反正切值

DOUBLE

degrees(DOUBLE a), degrees(DECIMAL a)

Converts value of a from radians to degrees. Decimal version added in Hive 0.13.0.

奖弧度值转换角度值

DOUBLE

radians(DOUBLE a), radians(DOUBLE a)

Converts value of a from degrees to radians. Decimal version added in Hive 0.13.0.

将角度值转换成弧度值

INT or DOUBLE

positive(INT a), positive(DOUBLE a)

Returns a.

返回a

INT or DOUBLE

negative(INT a), negative(DOUBLE a)

Returns -a.

返回a的相反数

DOUBLE or INT

sign(DOUBLE a), sign(DECIMAL a)

Returns the sign of a as '1.0' (if a is positive) or '-1.0' (if a is negative), '0.0' otherwise. The decimal version returns INT instead of DOUBLE. Decimal version added in Hive 0.13.0.

如果a是正数则返回1.0，是负数则返回-1.0，否则返回0.0

DOUBLE

e()

Returns the value of e.

数学常数e

DOUBLE

pi()

Returns the value of pi.

数学常数pi

BIGINT	factorial(INT a)	Returns the factorial of a (as of Hive 1.2.0). Valid a is [0..20].
求a的阶乘
DOUBLE	cbrt(DOUBLE a)	Returns the cube root of a double value (as of Hive 1.2.0).
求a的立方根


INT BIGINT

shiftleft(TINYINT|SMALLINT|INT a, INT b)

shiftleft(BIGINT a, INT b)

Bitwise left shift (as of Hive 1.2.0). Shifts a b positions to the left.

Returns int for tinyint, smallint and int a. Returns bigint for bigint a.

按位左移

INT

BIGINT

shiftright(TINYINT|SMALLINT|INT a, INTb)

shiftright(BIGINT a, INT b)

Bitwise right shift (as of Hive 1.2.0). Shifts a b positions to the right.

Returns int for tinyint, smallint and int a. Returns bigint for bigint a.

按拉右移

INT

BIGINT

shiftrightunsigned(TINYINT|SMALLINT|INTa, INT b),

shiftrightunsigned(BIGINT a, INT b)

Bitwise unsigned right shift (as of Hive 1.2.0). Shifts a b positions to the right.

Returns int for tinyint, smallint and int a. Returns bigint for bigint a.

无符号按位右移（<<<）

T	greatest(T v1, T v2, ...)	Returns the greatest value of the list of values (as of Hive 1.1.0). Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with ">" operator (as of Hive 2.0.0).
求最大值
T	least(T v1, T v2, ...)	Returns the least value of the list of values (as of Hive 1.1.0). Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with "<" operator (as of Hive 2.0.0).
求最小值
2 集合函数
Return Type

Name(Signature)

Description

int

size(Map<K.V>)

Returns the number of elements in the map type.

求map的长度

int

size(Array<T>)

Returns the number of elements in the array type.

求数组的长度

array<K>

map_keys(Map<K.V>)

Returns an unordered array containing the keys of the input map.

返回map中的所有key

array<V>

map_values(Map<K.V>)

Returns an unordered array containing the values of the input map.

返回map中的所有value

boolean

array_contains(Array<T>, value)

Returns TRUE if the array contains value.

如该数组Array<T>包含value返回true。，否则返回false

array

sort_array(Array<T>)

Sorts the input array in ascending order according to the natural ordering of the array elements and returns it (as of version 0.9.0).

按自然顺序对数组进行排序并返回

3 类型转换函数
Return Type

Name(Signature)

Description

binary

binary(string|binary)

Casts the parameter into a binary.

将输入的值转换成二进制

Expected "=" to follow "type"

cast(expr as <type>)

Converts the results of the expression expr to <type>. For example, cast('1' as BIGINT) will convert the string '1' to its integral representation. A null is returned if the conversion does not succeed. If cast(expr as boolean) Hive returns true for a non-empty string.

将expr转换成type类型 如：cast("1" as BIGINT) 将字符串1转换成了BIGINT类型，如果转换失败将返回NULL

4 日期函数
Return Type

Name(Signature)

Description

string

from_unixtime(bigint unixtime[, string format])

Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the format of "1970-01-01 00:00:00".

将时间的秒值转换成format格式（format可为“yyyy-MM-dd hh:mm:ss”,“yyyy-MM-dd hh”,“yyyy-MM-dd hh:mm”等等）如from_unixtime(1250111000,"yyyy-MM-dd") 得到2009-03-12

bigint

unix_timestamp()

Gets current Unix timestamp in seconds.

获取本地时区下的时间戳

bigint

unix_timestamp(string date)

Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds), using the default timezone and the default locale, return 0 if fail: unix_timestamp('2009-03-20 11:30:01') = 1237573801

将格式为yyyy-MM-dd HH:mm:ss的时间字符串转换成时间戳  如unix_timestamp('2009-03-20 11:30:01') = 1237573801

bigint

unix_timestamp(string date, string pattern)

Convert time string with given pattern (see [http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html]) to Unix time stamp (in seconds), return 0 if fail: unix_timestamp('2009-03-20', 'yyyy-MM-dd') = 1237532400.

将指定时间字符串格式字符串转换成Unix时间戳，如果格式不对返回0 如：unix_timestamp('2009-03-20', 'yyyy-MM-dd') = 1237532400

string

to_date(string timestamp)

Returns the date part of a timestamp string: to_date("1970-01-01 00:00:00") = "1970-01-01".

返回时间字符串的日期部分

int

year(string date)

Returns the year part of a date or a timestamp string: year("1970-01-01 00:00:00") = 1970, year("1970-01-01") = 1970.

返回时间字符串的年份部分

int	quarter(date/timestamp/string)	Returns the quarter of the year for a date, timestamp, or string in the range 1 to 4 (as of Hive 1.3.0). Example: quarter('2015-04-08') = 2.
返回当前时间属性哪个季度 如quarter('2015-04-08') = 2

int

month(string date)

Returns the month part of a date or a timestamp string: month("1970-11-01 00:00:00") = 11, month("1970-11-01") = 11.

返回时间字符串的月份部分

int

day(string date) dayofmonth(date)

Returns the day part of a date or a timestamp string: day("1970-11-01 00:00:00") = 1, day("1970-11-01") = 1.

返回时间字符串的天

int

hour(string date)

Returns the hour of the timestamp: hour('2009-07-30 12:58:59') = 12, hour('12:58:59') = 12.

返回时间字符串的小时

int

minute(string date)

Returns the minute of the timestamp.

返回时间字符串的分钟

int

second(string date)

Returns the second of the timestamp.

返回时间字符串的秒

int

weekofyear(string date)

Returns the week number of a timestamp string: weekofyear("1970-11-01 00:00:00") = 44, weekofyear("1970-11-01") = 44.

返回时间字符串位于一年中的第几个周内  如weekofyear("1970-11-01 00:00:00") = 44, weekofyear("1970-11-01") = 44

int

datediff(string enddate, string startdate)

Returns the number of days from startdate to enddate: datediff('2009-03-01', '2009-02-27') = 2.

计算开始时间startdate到结束时间enddate相差的天数

string

date_add(string startdate, int days)

Adds a number of days to startdate: date_add('2008-12-31', 1) = '2009-01-01'.

从开始时间startdate加上days

string

date_sub(string startdate, int days)

Subtracts a number of days to startdate: date_sub('2008-12-31', 1) = '2008-12-30'.

从开始时间startdate减去days

timestamp

from_utc_timestamp(timestamp, string timezone)

Assumes given timestamp is UTC and converts to given timezone (as of Hive 0.8.0). For example, from_utc_timestamp('1970-01-01 08:00:00','PST') returns 1970-01-01 00:00:00.

如果给定的时间戳并非UTC，则将其转化成指定的时区下时间戳

timestamp

to_utc_timestamp(timestamp, string timezone)

Assumes given timestamp is in given timezone and converts to UTC (as of Hive 0.8.0). For example, to_utc_timestamp('1970-01-01 00:00:00','PST') returns 1970-01-01 08:00:00.

如果给定的时间戳指定的时区下时间戳，则将其转化成UTC下的时间戳

date	current_date
Returns the current date at the start of query evaluation (as of Hive 1.2.0). All calls of current_date within the same query return the same value.

返回当前时间日期

timestamp	current_timestamp
Returns the current timestamp at the start of query evaluation (as of Hive 1.2.0). All calls of current_timestamp within the same query return the same value.

返回当前时间戳

string	add_months(string start_date, int num_months)
Returns the date that is num_months after start_date (as of Hive 1.1.0). start_date is a string, date or timestamp. num_months is an integer. The time part of start_date is ignored. If start_date is the last day of the month or if the resulting month has fewer days than the day component of start_date, then the result is the last day of the resulting month. Otherwise, the result has the same day component as start_date.

返回当前时间下再增加num_months个月的日期

string	last_day(string date)	Returns the last day of the month which the date belongs to (as of Hive 1.1.0). date is a string in the format 'yyyy-MM-dd HH:mm:ss' or 'yyyy-MM-dd'. The time part of date is ignored.
返回这个月的最后一天的日期，忽略时分秒部分（HH:mm:ss）

string	next_day(string start_date, string day_of_week)	Returns the first date which is later than start_date and named as day_of_week (as of Hive1.2.0). start_date is a string/date/timestamp. day_of_week is 2 letters, 3 letters or full name of the day of the week (e.g. Mo, tue, FRIDAY). The time part of start_date is ignored. Example: next_day('2015-01-14', 'TU') = 2015-01-20.
返回当前时间的下一个星期X所对应的日期 如：next_day('2015-01-14', 'TU') = 2015-01-20  以2015-01-14为开始时间，其下一个星期二所对应的日期为2015-01-20

string	trunc(string date, string format)	Returns date truncated to the unit specified by the format (as of Hive 1.2.0). Supported formats: MONTH/MON/MM, YEAR/YYYY/YY. Example: trunc('2015-03-17', 'MM') = 2015-03-01.
返回时间的最开始年份或月份  如trunc("2016-06-26",“MM”)=2016-06-01  trunc("2016-06-26",“YY”)=2016-01-01   注意所支持的格式为MONTH/MON/MM, YEAR/YYYY/YY

double	months_between(date1, date2)	Returns number of months between dates date1 and date2 (as of Hive 1.2.0). If date1 is later than date2, then the result is positive. If date1 is earlier than date2, then the result is negative. If date1 and date2 are either the same days of the month or both last days of months, then the result is always an integer. Otherwise the UDF calculates the fractional portion of the result based on a 31-day month and considers the difference in time components date1 and date2. date1 and date2 type can be date, timestamp or string in the format 'yyyy-MM-dd' or 'yyyy-MM-dd HH:mm:ss'. The result is rounded to 8 decimal places. Example: months_between('1997-02-28 10:30:00', '1996-10-30') = 3.94959677
返回date1与date2之间相差的月份，如date1>date2，则返回正，如果date1<date2,则返回负，否则返回0.0  如：months_between('1997-02-28 10:30:00', '1996-10-30') = 3.94959677  1997-02-28 10:30:00与1996-10-30相差3.94959677个月

string	date_format(date/timestamp/string ts, string fmt)
Converts a date/timestamp/string to a value of string in the format specified by the date format fmt (as of Hive 1.2.0). Supported formats are Java SimpleDateFormat formats –https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html. The second argument fmt should be constant. Example: date_format('2015-04-08', 'y') = '2015'.

date_format can be used to implement other UDFs, e.g.:

dayname(date) is date_format(date, 'EEEE')
dayofyear(date) is date_format(date, 'D')
按指定格式返回时间date 如：date_format("2016-06-22","MM-dd")=06-22

5 条件函数
Return Type

Name(Signature)

Description

T

if(boolean testCondition, T valueTrue, T valueFalseOrNull)

Returns valueTrue when testCondition is true, returns valueFalseOrNull otherwise.

如果testCondition 为true就返回valueTrue,否则返回valueFalseOrNull ，（valueTrue，valueFalseOrNull为泛型）

T	nvl(T value, T default_value)	Returns default value if value is null else returns value (as of HIve 0.11).
如果value值为NULL就返回default_value,否则返回value

T

COALESCE(T v1, T v2, ...)

Returns the first v that is not NULL, or NULL if all v's are NULL.

返回第一非null的值，如果全部都为NULL就返回NULL  如：COALESCE (NULL,44,55)=44/strong>

T

CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END

When a = b, returns c; when a = d, returns e; else returns f.

如果a=b就返回c,a=d就返回e，否则返回f  如CASE 4 WHEN 5  THEN 5 WHEN 4 THEN 4 ELSE 3 END 将返回4

T

CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END

When a = true, returns b; when c = true, returns d; else returns e.

如果a=ture就返回b,c= ture就返回d，否则返回e  如：CASE WHEN  5>0  THEN 5 WHEN 4>0 THEN 4 ELSE 0 END 将返回5；CASE WHEN  5<0  THEN 5 WHEN 4<0 THEN 4 ELSE 0 END 将返回0

boolean	isnull( a )	Returns true if a is NULL and false otherwise.
如果a为null就返回true，否则返回false

boolean	isnotnull ( a )	Returns true if a is not NULL and false otherwise.
如果a为非null就返回true，否则返回false

6 字符函数
Return Type

Name(Signature)

Description

int

ascii(string str)

Returns the numeric value of the first  character of str.

返回str中首个ASCII字符串的整数值

string

base64(binary bin)

Converts the argument from binary to a base 64 string (as of Hive 0.12.0)..

将二进制bin转换成64位的字符串

string

concat(string|binary A, string|binary B...)

Returns the string or bytes resulting from concatenating the strings or bytes passed in as parameters in order. For example, concat('foo', 'bar') results in 'foobar'. Note that this function can take any number of input strings..

对二进制字节码或字符串按次序进行拼接

array<struct<string,double>>

context_ngrams(array<array<string>>, array<string>, int K, int pf)

Returns the top-k contextual N-grams from a set of tokenized sentences, given a string of "context". See StatisticsAndDataMining for more information..

与ngram类似，但context_ngram()允许你预算指定上下文(数组)来去查找子序列，具体看StatisticsAndDataMining(这里的解释更易懂)

string

concat_ws(string SEP, string A, string B...)

Like concat() above, but with custom separator SEP..

与concat()类似，但使用指定的分隔符喜进行分隔

string

concat_ws(string SEP, array<string>)

Like concat_ws() above, but taking an array of strings. (as of Hive 0.9.0).

拼接Array中的元素并用指定分隔符进行分隔

string

decode(binary bin, string charset)

Decodes the first argument into a String using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'). If either argument is null, the result will also be null. (As of Hive 0.12.0.).

使用指定的字符集charset将二进制值bin解码成字符串，支持的字符集有：'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'，如果任意输入参数为NULL都将返回NULL

binary

encode(string src, string charset)

Encodes the first argument into a BINARY using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'). If either argument is null, the result will also be null. (As of Hive 0.12.0.).

使用指定的字符集charset将字符串编码成二进制值，支持的字符集有：'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'，如果任一输入参数为NULL都将返回NULL

int

find_in_set(string str, string strList)

Returns the first occurance of str in strList where strList is a comma-delimited string. Returns null if either argument is null. Returns 0 if the first argument contains any commas. For example, find_in_set('ab', 'abc,b,ab,c,def') returns 3..

返回以逗号分隔的字符串中str出现的位置，如果参数str为逗号或查找失败将返回0，如果任一参数为NULL将返回NULL回

string

format_number(number x, int d)

Formats the number X to a format like '#,###,###.##', rounded to D decimal places, and returns the result as a string. If D is 0, the result has no decimal point or fractional part. (As of Hive 0.10.0; bug with float types fixed in Hive 0.14.0, decimal type support added in Hive 0.14.0).

将数值X转换成"#,###,###.##"格式字符串，并保留d位小数，如果d为0，将进行四舍五入且不保留小数

string

get_json_object(string json_string, string path)

Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid. NOTE: The json path can only have the characters [0-9a-z_], i.e., no upper-case or special characters. Also, the keys *cannot start with numbers.* This is due to restrictions on Hive column names..

从指定路径上的JSON字符串抽取出JSON对象，并返回这个对象的JSON格式，如果输入的JSON是非法的将返回NULL,注意此路径上JSON字符串只能由数字 字母 下划线组成且不能有大写字母和特殊字符，且key不能由数字开头，这是由于Hive对列名的限制

boolean

in_file(string str, string filename)

Returns true if the string str appears as an entire line in filename..

如果文件名为filename的文件中有一行数据与字符串str匹配成功就返回true

int

instr(string str, string substr)

Returns the position of the first occurrence of substr in str. Returns null if either of the arguments are null and returns 0 if substr could not be found in str. Be aware that this is not zero based. The first character in str has index 1..

查找字符串str中子字符串substr出现的位置，如果查找失败将返回0，如果任一参数为Null将返回null，注意位置为从1开始的

int

length(string A)

Returns the length of the string..

返回字符串的长度

int

locate(string substr, string str[, int pos])

Returns the position of the first occurrence of substr in str after position pos..

查找字符串str中的pos位置后字符串substr第一次出现的位置

string

lower(string A) lcase(string A)

Returns the string resulting from converting all characters of B to lower case. For example, lower('fOoBaR') results in 'foobar'..

将字符串A的所有字母转换成小写字母

string

lpad(string str, int len, string pad)

Returns str, left-padded with pad to a length of len..

从左边开始对字符串str使用字符串pad填充，最终len长度为止，如果字符串str本身长度比len大的话，将去掉多余的部分

string

ltrim(string A)

Returns the string resulting from trimming spaces from the beginning(left hand side) of A. For example, ltrim(' foobar ') results in 'foobar '..

去掉字符串A前面的空格

array<struct<string,double>>

ngrams(array<array<string>>, int N, int K, int pf)

Returns the top-k N-grams from a set of tokenized sentences, such as those returned by the sentences() UDAF. See StatisticsAndDataMining for more information..

返回出现次数TOP K的的子序列,n表示子序列的长度，具体看StatisticsAndDataMining (这里的解释更易懂)

string

parse_url(string urlString, string partToExtract [, string keyToExtract])

Returns the specified part from the URL. Valid values for partToExtract include HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO. For example, parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') returns 'facebook.com'. Also a value of a particular key in QUERY can be extracted by providing the key as the third argument, for example, parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1') returns 'v1'..

返回从URL中抽取指定部分的内容，参数url是URL字符串，而参数partToExtract是要抽取的部分，这个参数包含(HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO,例如：parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST') ='facebook.com'，如果参数partToExtract值为QUERY则必须指定第三个参数key  如：parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1') =‘v1’

string

printf(String format, Obj... args)

Returns the input formatted according do printf-style format strings (as of Hive0.9.0)..

按照printf风格格式输出字符串

string

regexp_extract(string subject, string pattern, int index)

Returns the string extracted using the pattern. For example, regexp_extract('foothebar', 'foo(.*?)(bar)', 2) returns 'bar.' Note that some care is necessary in using predefined character classes: using '\s' as the second argument will match the letter s; '\\s' is necessary to match whitespace, etc. The 'index' parameter is the Java regex Matcher group() method index. See docs/api/java/util/regex/Matcher.html for more information on the 'index' or Java regex group() method..

抽取字符串subject中符合正则表达式pattern的第index个部分的子字符串，注意些预定义字符的使用，如第二个参数如果使用'\s'将被匹配到s,'\\s'才是匹配空格

string

regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)

Returns the string resulting from replacing all substrings in INITIAL_STRING that match the java regular expression syntax defined in PATTERN with instances of REPLACEMENT. For example, regexp_replace("foobar", "oo|ar", "") returns 'fb.' Note that some care is necessary in using predefined character classes: using '\s' as the second argument will match the letter s; '\\s' is necessary to match whitespace, etc..

按照Java正则表达式PATTERN将字符串INTIAL_STRING中符合条件的部分成REPLACEMENT所指定的字符串，如里REPLACEMENT这空的话，抽符合正则的部分将被去掉  如：regexp_replace("foobar", "oo|ar", "") = 'fb.' 注意些预定义字符的使用，如第二个参数如果使用'\s'将被匹配到s,'\\s'才是匹配空格

string

repeat(string str, int n)

Repeats str n times..

重复输出n次字符串str

string

reverse(string A)

Returns the reversed string..

反转字符串

string

rpad(string str, int len, string pad)

Returns str, right-padded with pad to a length of len..

从右边开始对字符串str使用字符串pad填充，最终len长度为止，如果字符串str本身长度比len大的话，将去掉多余的部分

string

rtrim(string A)

Returns the string resulting from trimming spaces from the end(right hand side) of A. For example, rtrim(' foobar ') results in ' foobar'..

去掉字符串后面出现的空格

array<array<string>>

sentences(string str, string lang, string locale)

Tokenizes a string of natural language text into words and sentences, where each sentence is broken at the appropriate sentence boundary and returned as an array of words. The 'lang' and 'locale' are optional arguments. For example, sentences('Hello there! How are you?') returns ( ("Hello", "there"), ("How", "are", "you") )..

字符串str将被转换成单词数组，如：sentences('Hello there! How are you?') =( ("Hello", "there"), ("How", "are", "you") )

string

space(int n)

Returns a string of n spaces..

返回n个空格

array

split(string str, string pat)

Splits str around pat (pat is a regular expression)..

按照正则表达式pat来分割字符串str,并将分割后的数组字符串的形式返回

map<string,string>

str_to_map(text[, delimiter1, delimiter2])

Splits text into key-value pairs using two delimiters. Delimiter1 separates text into K-V pairs, and Delimiter2 splits each K-V pair. Default delimiters are ',' for delimiter1 and '=' for delimiter2..

将字符串str按照指定分隔符转换成Map，第一个参数是需要转换字符串，第二个参数是键值对之间的分隔符，默认为逗号;第三个参数是键值之间的分隔符，默认为"="

string

substr(string|binary A, int start) substring(string|binary A, int start)

Returns the substring or slice of the byte array of A starting from start position till the end of string A. For example, substr('foobar', 4) results in 'bar' (see [http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substr])..

对于字符串A,从start位置开始截取字符串并返回

string

substr(string|binary A, int start, int len) substring(string|binary A, int start, int len)

Returns the substring or slice of the byte array of A starting from start position with length len. For example, substr('foobar', 4, 1) results in 'b' (see [http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_substr])..

对于二进制/字符串A,从start位置开始截取长度为length的字符串并返回

string	substring_index(string A, string delim, int count)	Returns the substring from string A before count occurrences of the delimiter delim (as of Hive 1.3.0). If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. Substring_index performs a case-sensitive match when searching for delim. Example: substring_index('www.apache.org', '.', 2) = 'www.apache'..
截取第count分隔符之前的字符串，如count为正则从左边开始截取，如果为负则从右边开始截取

string

translate(string|char|varchar input, string|char|varchar from, string|char|varchar to)

Translates the input string by replacing the characters present in the from string with the corresponding characters in the to string. This is similar to the translatefunction in PostgreSQL. If any of the parameters to this UDF are NULL, the result is NULL as well. (Available as of Hive 0.10.0, for string types)

Char/varchar support added as of Hive 0.14.0..

将input出现在from中的字符串替换成to中的字符串 如：translate("MOBIN","BIN","M")="MOM"

string

trim(string A)

Returns the string resulting from trimming spaces from both ends of A. For example, trim(' foobar ') results in 'foobar'.

将字符串A前后出现的空格去掉

binary

unbase64(string str)

Converts the argument from a base 64 string to BINARY. (As of Hive 0.12.0.).

将64位的字符串转换二进制值

string

upper(string A) ucase(string A)

Returns the string resulting from converting all characters of A to upper case. For example, upper('fOoBaR') results in 'FOOBAR'..

将字符串A中的字母转换成大写字母

string	initcap(string A)	Returns string, with the first letter of each word in uppercase, all other letters in lowercase. Words are delimited by whitespace. (As of Hive 1.1.0.).
将字符串A转换第一个字母大写其余字母的字符串

int	levenshtein(string A, string B)	Returns the Levenshtein distance between two strings (as of Hive 1.2.0). For example, levenshtein('kitten', 'sitting') results in 3..
计算两个字符串之间的差异大小  如：levenshtein('kitten', 'sitting') = 3

string	soundex(string A)	Returns soundex code of the string (as of Hive 1.2.0). For example, soundex('Miller') results in M460..
将普通字符串转换成soundex字符串

7 聚合函数
Return Type

Name(Signature)

Description

BIGINT

count(*), count(expr), count(DISTINCT expr[, expr...])

count(*) - Returns the total number of retrieved rows, including rows containing NULL values.

统计总行数，包括含有NULL值的行

count(expr) - Returns the number of rows for which the supplied expression is non-NULL.

统计提供非NULL的expr表达式值的行数

count(DISTINCT expr[, expr]) - Returns the number of rows for which the supplied expression(s) are unique and non-NULL. Execution of this can be optimized with hive.optimize.distinct.rewrite.

统计提供非NULL且去重后的expr表达式值的行数

DOUBLE

sum(col), sum(DISTINCT col)

Returns the sum of the elements in the group or the sum of the distinct values of the column in the group.

sum(col),表示求指定列的和，sum(DISTINCT col)表示求去重后的列的和

DOUBLE

avg(col), avg(DISTINCT col)

Returns the average of the elements in the group or the average of the distinct values of the column in the group.

avg(col),表示求指定列的平均值，avg(DISTINCT col)表示求去重后的列的平均值

DOUBLE

min(col)

Returns the minimum of the column in the group.

求指定列的最小值

DOUBLE

max(col)

Returns the maximum value of the column in the group.

求指定列的最大值

DOUBLE

variance(col), var_pop(col)

Returns the variance of a numeric column in the group.

求指定列数值的方差

DOUBLE

var_samp(col)

Returns the unbiased sample variance of a numeric column in the group.

求指定列数值的样本方差

DOUBLE

stddev_pop(col)

Returns the standard deviation of a numeric column in the group.

求指定列数值的标准偏差

DOUBLE

stddev_samp(col)

Returns the unbiased sample standard deviation of a numeric column in the group.

求指定列数值的样本标准偏差

DOUBLE

covar_pop(col1, col2)

Returns the population covariance of a pair of numeric columns in the group.

求指定列数值的协方差

DOUBLE

covar_samp(col1, col2)

Returns the sample covariance of a pair of a numeric columns in the group.

求指定列数值的样本协方差

DOUBLE

corr(col1, col2)

Returns the Pearson coefficient of correlation of a pair of a numeric columns in the group.

返回两列数值的相关系数

DOUBLE

 percentile(BIGINT col, p)

Returns the exact pth percentile of a column in the group (does not work with floating point types). p must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral.

返回col的p%分位数

8 表生成函数
Return Type

Name(Signature)

Description

Array Type

explode(array<TYPE> a)

For each element in a, generates a row containing that element.

对于a中的每个元素，将生成一行且包含该元素

N rows

explode(ARRAY)

Returns one row for each element from the array..

每行对应数组中的一个元素

N rows

explode(MAP)

Returns one row for each key-value pair from the input map with two columns in each row: one for the key and another for the value. (As of Hive 0.8.0.).

每行对应每个map键-值，其中一个字段是map的键，另一个字段是map的值

N rows

posexplode(ARRAY)

Behaves like explode for arrays, but includes the position of items in the original array by returning a tuple of (pos, value). (As of Hive 0.13.0.).

与explode类似，不同的是还返回各元素在数组中的位置

N rows

stack(INT n, v_1, v_2, ..., v_k)

Breaks up v_1, ..., v_k into n rows. Each row will have k/n columns. n must be constant..

把M列转换成N行，每行有M/N个字段，其中n必须是个常数

tuple

json_tuple(jsonStr, k1, k2, ...)

Takes a set of names (keys) and a JSON string, and returns a tuple of values. This is a more efficient version of the get_json_object UDF because it can get multiple keys with just one call..

从一个JSON字符串中获取多个键并作为一个元组返回，与get_json_object不同的是此函数能一次获取多个键值

tuple

parse_url_tuple(url, p1, p2, ...)

This is similar to the parse_url() UDF but can extract multiple parts at once out of a URL. Valid part names are: HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:<KEY>..

返回从URL中抽取指定N部分的内容，参数url是URL字符串，而参数p1,p2,....是要抽取的部分，这个参数包含HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:<KEY>



inline(ARRAY<STRUCT[,STRUCT]>)

Explodes an array of structs into a table. (As of Hive 0.10.).

将结构体数组提取出来并插入到表中

二、复杂数据类型
1、array
 现有数据如下：

1 huangbo guangzhou,xianggang,shenzhen a1:30,a2:20,a3:100 beijing,112233,13522334455,500
2	xuzheng	xianggang	b2:50,b3:40	tianjin,223344,13644556677,600
3	wangbaoqiang	beijing,zhejinag	c1:200	chongqinjg,334455,15622334455,20

建表语句

复制代码
use class;
create table cdt(
id int,
name string,
work_location array<string>,
piaofang map<string,bigint>,
address struct<location:string,zipcode:int,phone:string,value:int>)
row format delimited
fields terminated by "\t"
collection items terminated by ","
map keys terminated by ":"
lines terminated by "\n";
复制代码


导入数据

0: jdbc:hive2://hadoop3:10000> load data local inpath "/home/hadoop/cdt.txt" into table cdt;
查询语句

select * from cdt;


select name from cdt;


select work_location from cdt;


select work_location[0] from cdt;


select work_location[1] from cdt;


2、map
建表语句、导入数据同1

查询语句

select piaofang from cdt;


select piaofang["a1"] from cdt;


3、struct
建表语句、导入数据同1

查询语句

select address from cdt;


select address.location from cdt;


4、uniontype
很少使用

三、视图
1、Hive 的视图和关系型数据库的视图区别
和关系型数据库一样，Hive 也提供了视图的功能，不过请注意，Hive 的视图和关系型数据库的数据还是有很大的区别：

　　（1）只有逻辑视图，没有物化视图；

　　（2）视图只能查询，不能 Load/Insert/Update/Delete 数据；

　　（3）视图在创建时候，只是保存了一份元数据，当查询视图的时候，才开始执行视图对应的 那些子查询

2、Hive视图的创建语句
create view view_cdt as select * from cdt;


3、Hive视图的查看语句
show views;
desc view_cdt;-- 查看某个具体视图的信息


4、Hive视图的使用语句
select * from view_cdt;


5、Hive视图的删除语句
drop view view_cdt;


四、函数
1、内置函数
（1）查看内置函数
show functions;


（2）显示函数的详细信息
desc function substr;


（3）显示函数的扩展信息
desc function extended substr;


2、自定义函数UDF
当 Hive 提供的内置函数无法满足业务处理需要时，此时就可以考虑使用用户自定义函数。

UDF（user-defined function）作用于单个数据行，产生一个数据行作为输出。（数学函数，字 符串函数）

UDAF（用户定义聚集函数 User- Defined Aggregation Funcation）：接收多个输入数据行，并产 生一个输出数据行。（count，max）

UDTF（表格生成函数 User-Defined Table Functions）：接收一行输入，输出多行（explode）

(1) 简单UDF示例
A.　导入hive需要的jar包，自定义一个java类继承UDF，重载 evaluate 方法
ToLowerCase.java

复制代码
import org.apache.hadoop.hive.ql.exec.UDF;

public class ToLowerCase extends UDF{

    // 必须是 public，并且 evaluate 方法可以重载
    public String evaluate(String field) {
    String result = field.toLowerCase();
    return result;
    }

}
复制代码
B.　打成 jar 包上传到服务器
C.　将 jar 包添加到 hive 的 classpath
add JAR /home/hadoop/udf.jar;


D.　创建临时函数与开发好的 class 关联起来
0: jdbc:hive2://hadoop3:10000> create temporary function tolowercase as 'com.study.hive.udf.ToLowerCase';


E.　至此，便可以在 hql 在使用自定义的函数
0: jdbc:hive2://hadoop3:10000> select tolowercase('HELLO');


(2) JSON数据解析UDF开发
现有原始 json 数据（rating.json）如下

{"movie":"1193","rate":"5","timeStamp":"978300760","uid":"1"}

{"movie":"661","rate":"3","timeStamp":"978302109","uid":"1"}

{"movie":"914","rate":"3","timeStamp":"978301968","uid":"1"}

{"movie":"3408","rate":"4","timeStamp":"978300275","uid":"1"}

{"movie":"2355","rate":"5","timeStamp":"978824291","uid":"1"}

{"movie":"1197","rate":"3","timeStamp":"978302268","uid":"1"}

{"movie":"1287","rate":"5","timeStamp":"978302039","uid":"1"}

{"movie":"2804","rate":"5","timeStamp":"978300719","uid":"1"}

{"movie":"594","rate":"4","timeStamp":"978302268","uid":"1"}

现在需要将数据导入到 hive 仓库中，并且最终要得到这么一个结果：



该怎么做、？？？（提示：可用内置 get_json_object 或者自定义函数完成）

A.　get_json_object(string json_string, string path)
返回值: string

说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回NULL。  这个函数每次只能返回一个数据项。

0: jdbc:hive2://hadoop3:10000> select get_json_object('{"movie":"594","rate":"4","timeStamp":"978302268","uid":"1"}','$.movie');


创建json表并将数据导入进去

0: jdbc:hive2://hadoop3:10000> create table json(data string);
No rows affected (0.983 seconds)
0: jdbc:hive2://hadoop3:10000> load data local inpath '/home/hadoop/json.txt' into table json;
No rows affected (1.046 seconds)
0: jdbc:hive2://hadoop3:10000>


0: jdbc:hive2://hadoop3:10000> select
. . . . . . . . . . . . . . .> get_json_object(data,'$.movie') as movie
. . . . . . . . . . . . . . .> from json；


B.　json_tuple(jsonStr, k1, k2, ...)
参数为一组键k1，k2……和JSON字符串，返回值的元组。该方法比 get_json_object 高效，因为可以在一次调用中输入多个键

复制代码
0: jdbc:hive2://hadoop3:10000> select
. . . . . . . . . . . . . . .>   b.b_movie,
. . . . . . . . . . . . . . .>   b.b_rate,
. . . . . . . . . . . . . . .>   b.b_timeStamp,
. . . . . . . . . . . . . . .>   b.b_uid
. . . . . . . . . . . . . . .> from json a
. . . . . . . . . . . . . . .> lateral view json_tuple(a.data,'movie','rate','timeStamp','uid') b as b_movie,b_rate,b_timeStamp,b_uid;
复制代码


(3) Transform实现
Hive 的 TRANSFORM 关键字提供了在 SQL 中调用自写脚本的功能。适合实现 Hive 中没有的 功能又不想写 UDF 的情况

具体以一个实例讲解。

Json 数据： {"movie":"1193","rate":"5","timeStamp":"978300760","uid":"1"}

需求：把 timestamp 的值转换成日期编号

1、先加载 rating.json 文件到 hive 的一个原始表 rate_json

create table rate_json(line string) row format delimited;
load data local inpath '/home/hadoop/rating.json' into table rate_json;
2、创建 rate 这张表用来存储解析 json 出来的字段：

create table rate(movie int, rate int, unixtime int, userid int) row format delimited fields
terminated by '\t';
解析 json，得到结果之后存入 rate 表：

insert into table rate select
get_json_object(line,'$.movie') as moive,
get_json_object(line,'$.rate') as rate,
get_json_object(line,'$.timeStamp') as unixtime,
get_json_object(line,'$.uid') as userid
from rate_json;
3、使用 transform+python 的方式去转换 unixtime 为 weekday

先编辑一个 python 脚本文件

复制代码
########python######代码
## vi weekday_mapper.py
#!/bin/python
import sys
import datetime
for line in sys.stdin:
 line = line.strip()
 movie,rate,unixtime,userid = line.split('\t')
 weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
 print '\t'.join([movie, rate, str(weekday),userid])
复制代码
保存文件 然后，将文件加入 hive 的 classpath：

hive>add file /home/hadoop/weekday_mapper.py;
hive> insert into table lastjsontable select transform(movie,rate,unixtime,userid)
using 'python weekday_mapper.py' as(movie,rate,weekday,userid) from rate;
创建最后的用来存储调用 python 脚本解析出来的数据的表：lastjsontable

create table lastjsontable(movie int, rate int, weekday int, userid int) row format delimited
fields terminated by '\t';
最后查询看数据是否正确

select distinct(weekday) from lastjsontable;
五、特殊分隔符处理
补充：hive 读取数据的机制：

1、 首先用 InputFormat<默认是：org.apache.hadoop.mapred.TextInputFormat >的一个具体实 现类读入文件数据，返回一条一条的记录（可以是行，或者是你逻辑中的“行”）

2、 然后利用 SerDe<默认：org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe>的一个具体 实现类，对上面返回的一条一条的记录进行字段切割

Hive 对文件中字段的分隔符默认情况下只支持单字节分隔符，如果数据文件中的分隔符是多 字符的，如下所示：

01||huangbo

02||xuzheng

03||wangbaoqiang

1、使用RegexSerDe正则表达式解析
创建表

create table t_bi_reg(id string,name string)
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe'
with serdeproperties('input.regex'='(.*)\\|\\|(.*)','output.format.string'='%1$s %2$s')
stored as textfile;


导入数据并查询

0: jdbc:hive2://hadoop3:10000> load data local inpath '/home/hadoop/data.txt' into table t_bi_reg;
No rows affected (0.747 seconds)
0: jdbc:hive2://hadoop3:10000> select a.* from t_bi_reg a;







针对上面一张学生成绩表(class)，有year-学年，class-课程，student-学生，score-分数这四个字段，请看问题：



问题1：每年每门学科排名第一的学生是？

问题2：每年总成绩都有所提升的学生是？



对于问题1来说比较简单，既可以使用聚合函数来统计，也可以使用窗口函数来统计，其中窗口函数给了两种解法：



--使用聚合函数
select a.year,a.class,b.student
from
(
select year,class,max(score) as max_score
from class
group by year,class
) a join class b
on a.year = b.year and a.class = b.class
and a.max_score = b.score
order by a.year


执行结果如下，如果有相同成绩的话都会保留。







--使用窗口函数max
select a.year,a.class,a.student
from
(
select year,class,score,student
,max(score) over
(partition by year,class) as max_score
--增加一列为聚合后的最高分
    from `class`
) a
where a.score = max_score  --保留与最高分相同的记录数




执行结果如下，同样的如果有相同记录也会保留下来。







--使用窗口函数first_value
select distinct year,class
,first_value(student) over
(partition by year,class
order by score desc) as student
from class


执行结果，需要注意的是如果有相同成绩，只会取一条记录。







对比两种写法可以发现：

• 使用窗口函数的SQL代码量少

• 避免了与原表的join



对于问题2，是一个相对复杂但是比较常见的需求，无法只使用聚合函数来统计，只能配合窗口函数来统计。



select student
from
(
  select year,student
  ,if((sum_score - lag(sum_score,1,0) over (partition by student  order by year )) > 0,1,0) as flag,
  (sum_score - lag(sum_score,1,0) over
  (partition by student order by year)) as flag1
  --按照student进行分区并进行year正序排序
  --，找到每个学生的上一条学年总成绩
  --，并与当年成绩相减，如果小于
  --，则将flag值置为1，否则置为0
  from
  (
    select year,student
    ,sum(score) as sum_score
    --按照学年和学生进行成绩汇总
    from class
    group by year,student
  ) a
) b
group by student
having avg(flag) = 1
--平均值为1则代表是每年都有增长


执行结果：









通过上面两个问题，可以对窗口函数的特征做一个简单的小结：



• 聚合函数可以作为窗口函数使用

• 具有计算和取值的功能

• 不改变记录数






理论



窗口函数也称为OLAP（Online Analytical Processing）函数，是对一组值进行操作，不需要使用Group by子句对数据进行分组，还能在同一行返回原来行的列和使用聚合函数得到的聚合列。



那为什么叫窗口函数呢？因为窗口函数将表以窗口为单位进行分割，并在其中进行各种分析操作，为了让大家快速形成直观印象，才起了这样一个容易理解的名称。








SQL语法



<窗口函数>()
OVER
(
  [PARTITION BY <列清单>]
  [ORDER BY <排序用清单列>] [ASC/DESC]
  (ROWS | RANGE) <范围条件>
)


如上代码所示，窗口函数的语法分为四个部分：



函数子句：指明具体操作，如sum-求和，first_value-取第一个值；

partition by子句：指明分区字段，如果没有，则将所有数据作为一个分区；

order by子句：指明了每个分区排序的字段和方式,也是可选的，没有就是按照表中的顺序；

窗口子句：指明相对当前记录的计算范围，可以向上（preceding），可以向下（following）,也可以使用between指明，上下边界的值，没有的话默认为当前分区。有些场景比较特殊，后文会讲到这种场景。



窗口函数分类



下面的思维导图基本包含了Hive所有的窗口函数，按照窗口函数的功能分为：计算、取值、排序、序列四种，前三种的使用场景比较常见，容易理解，最后一种(序列)的使用场景比较少。







窗口函数使用场景





介绍了这么多，那窗口函数到底可以帮我们做什么呢？



结合实际场景看看怎么用窗口函数来解决问题。下面针对不同的使用场景，将窗口函数的使用呈现给大家。所有例子的数据均来自下图这张表。








用于辅助计算



主要的用法是在原有表的基础上，增加一列聚合后的值，辅以后续的计算。

例如：统计出不同产品类型售价最高的产品。

具体代码如下：

--使用窗口函数max
select a.product_type,a.product_name
from
(
  select product_name,product_type,sale_price
  ,max(sale_price) over
  (
    partition by product_type
  ) as max_sale_price
  --增加一列为聚合后的最高售价
  from product
) a
where a.sale_price = a.max_sale_price;
--保留与最高售价相同的记录数


执行结果：







几乎所有的窗口函数都可以用于辅助计算。




累积计算



标准聚合函数作为窗口函数配合order by使用，可以实现累积计算。



例如：sum窗口函数配合order by，可以实现累积和。

具体代码如下：

SELECT product_id,product_name
  ,product_type,sale_price
  ,SUM(sale_price) OVER
  (
    ORDER BY product_id
  ) AS current_sum
FROM product;


执行结果：







相应的AVG窗口函数配合order by，可以实现累积平均，max可以实现累积最大值，min可以实现累积最小值，count则可以实现累积计数。注意，只有计算类的窗口函数可以实现累积计算。



这里提出一个问题，为什么增加了order by就可以实现累积计算呢？读者可以停顿思考一下！



答案马上揭晓：标准聚合函数作为窗口函数使用的时候，在指明order by的情况下，如果没有Window子句，则Window子句默认为：RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW(上边界不限制，下边界到当前行)。




移动计算



移动计算是在分区和排序的基础上，对计算范围进一步做出限定。



例如：按照产品ID排序，将最近3条的销售价格进行汇总平均。

具体代码如下：

SELECT product_id,product_name
   ,sale_price
   ,AVG(sale_price)
   over
   (
     ORDER BY product_id
     rows 2 preceding
   ) AS moving_avg
FROM product;


rows 2 preceding的意思就是“截止到之前2行”。也就是将作为汇总对象的记录限定为如下的最靠近的3行。



执行结果如下：





使用关键字FOLLOWING(“之后”）替换PRECEDING，就可以指定截止到之后~行。




取任一字段值



取值的窗口函数有：first_value/last_value、lag/lead，其中first_value和lag在开篇的例子中已经使用到了，这里就不举例说明了。只细化说明下他们的语法。



first_value(字段名)-取出分区中的第一条记录的任意一个字段的值，可以排序也可以不排序，此处也可以进一步指明Window子句。



lag(字段名,N,默认值)-取出当前行之上的第N条记录的任意一个字段的值，这里的N和默认值都是可选的，默认N为1，默认值为null。




排序



排序对应的四个窗口函数为：rank、dense_rank、row_number、ntitle



rank：计算排序时，如果存在相同位次的记录，则会跳过之后的位次。

e.g. 有三条记录排在第1位时：1位、1位、1位、4位......

dense_rank：计算排序时，即使存在相同位次的记录，也不会跳过之后的位次。

e.g. 有三条记录排在第1位时：1位、1位、1位、2位......

row_number：赋予唯一的连续位次。

e.g. 有三条记录排在第1位时：1位、2位、3位、4位...

ntitle：用于将分组数据按照顺序切分成n片，返回当前切片值

e.g. 对于一组数字（1，2，3，4，5，6），ntile(2)切片后为（1，1，1，2，2，2）



1）统计所有产品的售价排名

具体代码如下：

SELECT product_name,product_type
   ,sale_price,
   RANK () OVER
   (
     ORDER BY sale_price
   ) AS ranking
FROM product;


执行结果如下：







2）统计各产品类型下各产品的售价排名

具体代码如下：

SELECT product_name,product_type
   ,sale_price,
   RANK () OVER
   (
     PARTITION BY product_type
     ORDER BY sale_price
   ) AS ranking
FROM product;


执行结果如下：







对比一下dense_rank、row_number、ntile



具体代码如下：

SELECT product_name,product_type,sale_price,
    RANK () OVER (ORDER BY sale_price) AS ranking,
    DENSE_RANK () OVER (ORDER BY sale_price) AS dense_ranking,
    ROW_NUMBER () OVER (ORDER BY sale_price) AS row_num,
    ntile(3) OVER (ORDER BY sale_price) as nt1,
    ntile(30) OVER (ORDER BY sale_price) as nt2
    --切片大于总记录数
FROM product;


执行结果如下：







从结果可以发现，当ntile(30)中的切片大于了总记录数时，切片的值为记录的序号。




序列



序列中的两个窗口函数cume_dist和percent_rank，通过实例来看看它们是怎么使用的。



1）统计小于等于当前售价的产品数，所占总产品数的比例

具体代码如下：

SELECT product_type,product_name,sale_price,
CUME_DIST() OVER(ORDER BY sale_price) AS rn1,
CUME_DIST() OVER
(
  PARTITION BY product_type
  ORDER BY sale_price
) AS rn2
FROM product;


执行结果如下：







rn1: 没有partition,所有数据均为1组，总行数为8，

     第一行：小于等于100的行数为1，因此，1/8=0.125

     第二行：小于等于500的行数为3，因此，3/8=0.375

rn2: 按照产品类型分组，product_type=厨房用品的行数为4,

     第三行：小于等于500的行数为1，因此，1/4=0.25



2）统计每个产品的百分比排序

当前行的RANK值-1/分组内总行数-1

具体代码如下：

SELECT product_type,product_name,sale_price,
percent_rank() OVER (ORDER BY sale_price) AS rn1,
percent_rank() OVER
(
  PARTITION BY product_type
  ORDER BY sale_price
)  AS rn2
FROM product;


执行结果如下：







rn1: 没有partition,所有数据均为1组，总行数为8，

第一行：排序为1，因此，（1-1）/（8-1）= 0

第二行：排序为2，因此，（2-1）/（8-1）= 0.14

rn2: 按照产品类型分组，product_type=厨房用品的行数为4,

第三行：排序为1，因此，（1-1）/（4-1）= 0

第四行：排序为1，因此，（2-1）/（4-1）= 0.33



总结



以上介绍了Hive中窗口函数的几乎所有的使用场景，每种函数的用法也配合代码进行讲解，相信大家看了本文后，在实际数据工作中对于窗口函数的使用肯定会得心应手。



CREATE TABLE accumulo_ck_1(key struct<col1:string,col2:string,col3:string>, value string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
    "accumulo.table.name" = "accumulo_custom",
    "accumulo.columns.mapping" = ":rowid,cf:string",
    "accumulo.composite.rowid.factory"="org.apache.hadoop.hive.accumulo.serde.DelimitedAccumuloRowIdFactory",
    "accumulo.composite.delimiter" = "$");

CREATE EXTERNAL TABLE accumulo_ck_2(key string, value string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
    "accumulo.table.name" = "accumulo_custom",
    "accumulo.columns.mapping" = ":rowid,cf:string");

insert overwrite table accumulo_ck_1 select struct('1000','2000','3000'),'value'
from src where key = 100;

select * from accumulo_ck_1;
select * from accumulo_ck_2;

DROP TABLE accumulo_ck_1;
DROP TABLE accumulo_ck_2;
CREATE TABLE accumulo_ck_3(key struct<col1:string,col2:string,col3:string>, value string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
    "accumulo.table.name" = "accumulo_custom2",
    "accumulo.columns.mapping" = ":rowid,cf:string",
    "accumulo.composite.rowid"="org.apache.hadoop.hive.accumulo.serde.FirstCharAccumuloCompositeRowId");

insert overwrite table accumulo_ck_3 select struct('abcd','mnop','wxyz'),'value'
from src where key = 100;

select * from accumulo_ck_3;

DROP TABLE accumulo_ck_3;
DROP TABLE users;
DROP TABLE states;
DROP TABLE countries;
DROP TABLE users_level;

-- From HIVE-1257

CREATE TABLE users(key string, state string, country string, country_id int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
"accumulo.columns.mapping" = ":rowID,info:state,info:country,info:country_id"
);

CREATE TABLE states(key string, name string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
"accumulo.columns.mapping" = ":rowID,state:name"
);

CREATE TABLE countries(key string, name string, country string, country_id int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
"accumulo.columns.mapping" = ":rowID,info:name,info:country,info:country_id"
);

INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0
FROM src WHERE key=100;

INSERT OVERWRITE TABLE states SELECT 'IA', 'Iowa'
FROM src WHERE key=100;

INSERT OVERWRITE TABLE countries SELECT 'USA', 'United States', 'USA', 1
FROM src WHERE key=100;

set hive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat;

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country = c.key);

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country = c.country);

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country_id = c.country_id);

SELECT u.key, u.state, s.name FROM users u JOIN states s
ON (u.state = s.key);

set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country = c.key);

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country = c.country);

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country_id = c.country_id);

SELECT u.key, u.state, s.name FROM users u JOIN states s
ON (u.state = s.key);

DROP TABLE users;
DROP TABLE states;
DROP TABLE countries;

CREATE TABLE users(key int, userid int, username string, created int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowID,f:userid,f:nickname,f:created");

CREATE TABLE users_level(key int, userid int, level int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowID,f:userid,f:level");

-- HIVE-1903:  the problem fixed here showed up even without any data,
-- so no need to load any to test it
SELECT year(from_unixtime(users.created)) AS year, level, count(users.userid) AS num
 FROM users JOIN users_level ON (users.userid = users_level.userid)
 GROUP BY year(from_unixtime(users.created)), level;

DROP TABLE users;
DROP TABLE users_level;
CREATE TABLE accumulo_pushdown(key string, value string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowid,cf:string");

INSERT OVERWRITE TABLE accumulo_pushdown
SELECT cast(key as string), value
FROM src;

-- with full pushdown
explain select * from accumulo_pushdown where key>'90';

select * from accumulo_pushdown where key>'90';
select * from accumulo_pushdown where key<'1';
select * from accumulo_pushdown where key<='2';
select * from accumulo_pushdown where key>='90';

-- with constant expression
explain select * from accumulo_pushdown where key>=cast(40 + 50 as string);
select * from accumulo_pushdown where key>=cast(40 + 50 as string);

-- with partial pushdown

explain select * from accumulo_pushdown where key>'90' and value like '%9%';

select * from accumulo_pushdown where key>'90' and value like '%9%';

-- with two residuals

explain select * from accumulo_pushdown
where key>='90' and value like '%9%' and key=cast(value as int);

select * from accumulo_pushdown
where key>='90' and value like '%9%' and key=cast(value as int);


-- with contradictory pushdowns

explain select * from accumulo_pushdown
where key<'80' and key>'90' and value like '%90%';

select * from accumulo_pushdown
where key<'80' and key>'90' and value like '%90%';

-- with nothing to push down

explain select * from accumulo_pushdown;

-- with a predicate which is not actually part of the filter, so
-- it should be ignored by pushdown

explain select * from accumulo_pushdown
where (case when key<'90' then 2 else 4 end) > 3;

-- with a predicate which is under an OR, so it should
-- be ignored by pushdown

explain select * from accumulo_pushdown
where key<='80' or value like '%90%';

explain select * from accumulo_pushdown where key > '281'
and key < '287';

select * from accumulo_pushdown where key > '281'
and key < '287';

set hive.optimize.ppd.storage=false;

-- with pushdown disabled

explain select * from accumulo_pushdown where key<='90';
DROP TABLE accumulo_table_1;
CREATE TABLE accumulo_table_1(key int, value string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowID,cf:string")
TBLPROPERTIES ("accumulo.table.name" = "accumulo_table_0");

DESCRIBE EXTENDED accumulo_table_1;

select * from accumulo_table_1;

EXPLAIN FROM src INSERT OVERWRITE TABLE accumulo_table_1 SELECT * WHERE (key%2)=0;
FROM src INSERT OVERWRITE TABLE accumulo_table_1 SELECT * WHERE (key%2)=0;

DROP TABLE accumulo_table_2;
CREATE EXTERNAL TABLE accumulo_table_2(key int, value string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowID,cf:string")
TBLPROPERTIES ("accumulo.table.name" = "accumulo_table_0");

EXPLAIN
SELECT Y.*
FROM
(SELECT accumulo_table_1.* FROM accumulo_table_1) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
ORDER BY key, value LIMIT 20;

SELECT Y.*
FROM
(SELECT accumulo_table_1.* FROM accumulo_table_1) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
ORDER BY key, value LIMIT 20;

EXPLAIN
SELECT Y.*
FROM
(SELECT accumulo_table_1.* FROM accumulo_table_1 WHERE 100 < accumulo_table_1.key) x
JOIN
(SELECT accumulo_table_2.* FROM accumulo_table_2 WHERE accumulo_table_2.key < 120) Y
ON (x.key = Y.key)
ORDER BY key, value;

SELECT Y.*
FROM
(SELECT accumulo_table_1.* FROM accumulo_table_1 WHERE 100 < accumulo_table_1.key) x
JOIN
(SELECT accumulo_table_2.* FROM accumulo_table_2 WHERE accumulo_table_2.key < 120) Y
ON (x.key = Y.key)
ORDER BY key,value;

DROP TABLE empty_accumulo_table;
CREATE TABLE empty_accumulo_table(key int, value string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowID,cf:string");

DROP TABLE empty_normal_table;
CREATE TABLE empty_normal_table(key int, value string);

select * from (select count(1) as c from empty_normal_table union all select count(1) as c from empty_accumulo_table) x order by c;
select * from (select count(1) c from empty_normal_table union all select count(1) as c from accumulo_table_1) x order by c;
select * from (select count(1) c from src union all select count(1) as c from empty_accumulo_table) x order by c;
select * from (select count(1) c from src union all select count(1) as c from accumulo_table_1) x order by c;

CREATE TABLE accumulo_table_3(key int, value string, count int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
"accumulo.columns.mapping" = ":rowID,cf:val,cf2:count"
);

EXPLAIN
INSERT OVERWRITE TABLE accumulo_table_3
SELECT x.key, x.value, Y.count
FROM
(SELECT accumulo_table_1.* FROM accumulo_table_1) x
JOIN
(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
ON (x.key = Y.key);

INSERT OVERWRITE TABLE accumulo_table_3
SELECT x.key, x.value, Y.count
FROM
(SELECT accumulo_table_1.* FROM accumulo_table_1) x
JOIN
(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
ON (x.key = Y.key);

select count(1) from accumulo_table_3;
select * from accumulo_table_3 order by key, value limit 5;
select key, count from accumulo_table_3 order by key, count desc limit 5;

DROP TABLE accumulo_table_4;
CREATE TABLE accumulo_table_4(key int, value1 string, value2 int, value3 int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
"accumulo.columns.mapping" = ":rowID,a:b,a:c,d:e"
);

INSERT OVERWRITE TABLE accumulo_table_4 SELECT key, value, key+1, key+2
FROM src WHERE key=98 OR key=100;

SELECT * FROM accumulo_table_4 ORDER BY key;

DROP TABLE accumulo_table_5;
CREATE EXTERNAL TABLE accumulo_table_5(key int, value map<string,string>)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowID,a:*")
TBLPROPERTIES ("accumulo.table.name" = "accumulo_table_4");

SELECT * FROM accumulo_table_5 ORDER BY key;

DROP TABLE accumulo_table_6;
CREATE TABLE accumulo_table_6(key int, value map<string,string>)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
"accumulo.columns.mapping" = ":rowID,cf:*"
);
INSERT OVERWRITE TABLE accumulo_table_6 SELECT key, map(value, key) FROM src
WHERE key=98 OR key=100;

SELECT * FROM accumulo_table_6 ORDER BY key;

DROP TABLE accumulo_table_7;
CREATE TABLE accumulo_table_7(value map<string,string>, key int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
"accumulo.columns.mapping" = "cf:*,:rowID"
);
INSERT OVERWRITE TABLE accumulo_table_7
SELECT map(value, key, upper(value), key+1), key FROM src
WHERE key=98 OR key=100;

SELECT * FROM accumulo_table_7 ORDER BY key;

DROP TABLE accumulo_table_8;
CREATE TABLE accumulo_table_8(key int, value1 string, value2 int, value3 int)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES (
"accumulo.columns.mapping" = ":rowID,a:b,a:c,d:e"
);

INSERT OVERWRITE TABLE accumulo_table_8 SELECT key, value, key+1, key+2
FROM src WHERE key=98 OR key=100;

SELECT * FROM accumulo_table_8 ORDER BY key;

DROP TABLE accumulo_table_1;
DROP TABLE accumulo_table_2;
DROP TABLE accumulo_table_3;
DROP TABLE accumulo_table_4;
DROP TABLE accumulo_table_5;
DROP TABLE accumulo_table_6;
DROP TABLE accumulo_table_7;
DROP TABLE accumulo_table_8;
DROP TABLE empty_accumulo_table;
DROP TABLE empty_normal_table;
-- HIVE-4375 Single sourced multi insert consists of native and non-native table mixed throws NPE
CREATE TABLE src_x1(key string, value string);
CREATE TABLE src_x2(key string, value string)
STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler'
WITH SERDEPROPERTIES ("accumulo.columns.mapping" = ":rowid, cf:value");

explain
from src a
insert overwrite table src_x1
select key,"" where a.key > 0 AND a.key < 50
insert overwrite table src_x2
select value,"" where a.key > 50 AND a.key < 100;

from src a
insert overwrite table src_x1
select key,"" where a.key > 0 AND a.key < 50
insert overwrite table src_x2
select value,"" where a.key > 50 AND a.key < 100;

select * from src_x1 order by key;
select * from src_x2 order by key;

DROP TABLE src_x1;
DROP TABLE src_x2;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.fetch.task.conversion=none;
set hive.limit.optimize.enable=true;
set hive.mapred.mode=nonstrict;
-- Global Limit optimization does not work with ACID table. Make sure to skip it for ACID table.
CREATE TABLE acidtest1(c1 INT, c2 STRING)
CLUSTERED BY (c1) INTO 3 BUCKETS
STORED AS ORC
TBLPROPERTIES ("transactional"="true");

insert into table acidtest1 select cint, cstring1 from alltypesorc where cint is not null order by cint;

explain
select cast (c1 as string) from acidtest1 limit 10;
select cast (c1 as string) from acidtest1 limit 10;

drop table acidtest1;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


-- This test checks that a join with tables with two different buckets send the right bucket info to each table.
create table acidjoin1(name varchar(50), age int) clustered by (age) into 2 buckets stored as orc TBLPROPERTIES ("transactional"="true");
create table acidjoin2(name varchar(50), gpa decimal(3, 2)) clustered by (gpa) into 4 buckets stored as orc TBLPROPERTIES ("transactional"="true");
create table acidjoin3(name varchar(50), age int, gpa decimal(3, 2)) clustered by (gpa) into 8 buckets stored as orc TBLPROPERTIES ("transactional"="true");

insert into table acidjoin1 values ('aaa', 35), ('bbb', 32), ('ccc', 32), ('ddd', 35), ('eee', 32);
insert into table acidjoin2 values ('aaa', 3.00), ('bbb', 3.01), ('ccc', 3.02), ('ddd', 3.03), ('eee', 3.04);

insert into table acidjoin3 select a.name, age, gpa from acidjoin1 a join acidjoin2 b on (a.name = b.name);
select * from acidjoin3 order by name;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_uanp(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_uanp select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint < 0 order by cint limit 10;
insert overwrite table acid_uanp select cint, cast(cstring1 as varchar(128)) from alltypesorc;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=true;

CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10;
set hive.vectorized.execution.enabled=true;
insert into table acid_vectorized values (1, 'bar');
set hive.vectorized.execution.enabled=true;
update acid_vectorized set b = 'foo' where b = 'bar';
set hive.vectorized.execution.enabled=true;
delete from acid_vectorized where b = 'foo';
set hive.vectorized.execution.enabled=true;
select a, b from acid_vectorized order by a, b;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

set hive.exec.dynamic.partition.mode=nonstrict;

CREATE TABLE acid_vectorized_part(a INT, b STRING) partitioned by (ds string) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid_vectorized_part partition (ds = 'today') select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10;
insert into table acid_vectorized_part partition (ds = 'tomorrow') select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10;
set hive.vectorized.execution.enabled=true;
select * from acid_vectorized_part order by a, b, ds;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

set hive.exec.dynamic.partition.mode=nonstrict;

CREATE TABLE acid_vectorized(a INT, b STRING, c float) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid_vectorized select cint, cstring1, cfloat from alltypesorc where cint is not null order by cint limit 10;
set hive.vectorized.execution.enabled=true;
select a,b from acid_vectorized order by a;
select a,c from acid_vectorized order by a;
select b,c from acid_vectorized order by b;

create table addpart1 (a int) partitioned by (b string, c string);

alter table addpart1 add partition (b='f', c='s');

show partitions addpart1;

alter table addpart1 add partition (b='f', c='');

show prtitions addpart1;


dfs -copyFromLocal ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar pfile://${system:test.tmp.dir}/hive-contrib-${system:hive.version}.jar;

add jar pfile://${system:test.tmp.dir}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_add AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleAdd';

DROP TEMPORARY FUNCTION example_add;
!mkdir ${system:test.tmp.dir}/tmpjars;
!touch ${system:test.tmp.dir}/tmpjars/added1.jar;
!touch ${system:test.tmp.dir}/tmpjars/added2.jar;

select count(key) from src;

add jar ${system:test.tmp.dir}/tmpjars/added1.jar;
add jar ${system:test.tmp.dir}/tmpjars/added2.jar;

select count(key) from src;

!rm ${system:test.tmp.dir}/tmpjars/added1.jar;

select count(key) from src;

SET hive.metastore.partition.name.whitelist.pattern=;
-- Test with no partition name whitelist pattern

CREATE TABLE part_nowhitelist_test (key STRING, value STRING) PARTITIONED BY (ds STRING);
SHOW PARTITIONS part_nowhitelist_test;

ALTER TABLE part_nowhitelist_test ADD PARTITION (ds='1,2,3,4');
SET hive.metastore.partition.name.whitelist.pattern=[A-Za-z]*;
-- This pattern matches only letters.

CREATE TABLE part_whitelist_test (key STRING, value STRING) PARTITIONED BY (ds STRING);
SHOW PARTITIONS part_whitelist_test;

ALTER TABLE part_whitelist_test ADD PARTITION (ds='Part');


SET hive.metastore.partition.name.whitelist.pattern=[\\x20-\\x7E&&[^,]]* ;
-- This pattern matches all printable ASCII characters (disallow unicode) and disallows commas

CREATE TABLE part_whitelist_test (key STRING, value STRING) PARTITIONED BY (ds STRING);
SHOW PARTITIONS part_whitelist_test;

ALTER TABLE part_whitelist_test ADD PARTITION (ds='1,2,3,4');

CREATE TABLE add_part_test (key STRING, value STRING) PARTITIONED BY (ds STRING);
SHOW PARTITIONS add_part_test;

ALTER TABLE add_part_test ADD PARTITION (ds='2010-01-01');
SHOW PARTITIONS add_part_test;

ALTER TABLE add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-01');
SHOW PARTITIONS add_part_test;

ALTER TABLE add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-02');
SHOW PARTITIONS add_part_test;

ALTER TABLE add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-01') PARTITION (ds='2010-01-02') PARTITION (ds='2010-01-03');
SHOW PARTITIONS add_part_test;

DROP TABLE add_part_test;
SHOW TABLES;

-- Test ALTER TABLE ADD PARTITION in non-default Database
CREATE DATABASE add_part_test_db;

CREATE TABLE add_part_test_db.add_part_test (key STRING, value STRING) PARTITIONED BY (ds STRING);
SHOW PARTITIONS add_part_test_db.add_part_test;

ALTER TABLE add_part_test_db.add_part_test ADD PARTITION (ds='2010-01-01');
SHOW PARTITIONS add_part_test_db.add_part_test;

ALTER TABLE add_part_test_db.add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-01');
SHOW PARTITIONS add_part_test_db.add_part_test;

ALTER TABLE add_part_test_db.add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-02');
SHOW PARTITIONS add_part_test_db.add_part_test;

ALTER TABLE add_part_test_db.add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-01') PARTITION (ds='2010-01-02') PARTITION (ds='2010-01-03');
SHOW PARTITIONS add_part_test_db.add_part_test;

DROP TABLE add_part_test_db.add_part_test;
DROP DATABASE add_part_test_db;
set hive.mapred.mode=nonstrict;
-- HIVE-5122 locations for 2nd, 3rd... partition are ignored

CREATE TABLE add_part_test (key STRING, value STRING) PARTITIONED BY (ds STRING);

explain
ALTER TABLE add_part_test ADD IF NOT EXISTS
PARTITION (ds='2010-01-01') location 'A'
PARTITION (ds='2010-02-01') location 'B'
PARTITION (ds='2010-03-01')
PARTITION (ds='2010-04-01') location 'C';

ALTER TABLE add_part_test ADD IF NOT EXISTS
PARTITION (ds='2010-01-01') location 'A'
PARTITION (ds='2010-02-01') location 'B'
PARTITION (ds='2010-03-01')
PARTITION (ds='2010-04-01') location 'C';

from src TABLESAMPLE (1 ROWS)
insert into table add_part_test PARTITION (ds='2010-01-01') select 100,100
insert into table add_part_test PARTITION (ds='2010-02-01') select 200,200
insert into table add_part_test PARTITION (ds='2010-03-01') select 400,300
insert into table add_part_test PARTITION (ds='2010-04-01') select 500,400;

select * from add_part_test;
-- HIVE-2477 Use name of original expression for name of CAST output
explain select key from (select cast(key as int) from src )t;

--backward
explain select key2 from (select cast(key as int) key2 from src )t;
set hive.mapred.mode=nonstrict;
explain
select concat(*),array(*) from src where key < 100 limit 10;

select concat(*),array(*) from src where key < 100 limit 10;

-- The order of columns is decided by row schema of prev operator
-- Like join which has two or more aliases, it's from left most aias to right aliases.

explain
select stack(2, *) as (e1,e2,e3) from (
  select concat(*), concat(a.*), concat(b.*), concat(a.*, b.key), concat(a.key, b.*)
  from src a join src b on a.key+1=b.key where a.key < 100) x limit 10;

select stack(2, *) as (e1,e2,e3) from (
  select concat(*), concat(a.*), concat(b.*), concat(a.*, b.key), concat(a.key, b.*)
  from src a join src b on a.key+1=b.key where a.key < 100) x limit 10;

-- HIVE-4181 TOK_FUNCTIONSTAR for UDTF
create table allcolref as select array(key, value) from src;
explain select explode(*) as x from allcolref limit 10;
select explode(*) as x from allcolref limit 10;
create table alter1(a int, b int);
describe extended alter1;
alter table alter1 set tblproperties ('a'='1', 'c'='3');
describe extended alter1;
alter table alter1 set tblproperties ('a'='1', 'c'='4', 'd'='3');
describe extended alter1;

alter table alter1 set tblproperties ('EXTERNAL'='TRUE');
describe extended alter1;
alter table alter1 set tblproperties ('EXTERNAL'='FALSE');
describe extended alter1;

alter table alter1 set serdeproperties('s1'='9');
describe extended alter1;
alter table alter1 set serdeproperties('s1'='10', 's2' ='20');
describe extended alter1;

add jar ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
alter table alter1 set serde 'org.apache.hadoop.hive.serde2.TestSerDe' with serdeproperties('s1'='9');
describe extended alter1;

alter table alter1 set serde 'org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe';
describe extended alter1;
set hive.metastore.disallow.incompatible.col.type.changes=false;
alter table alter1 replace columns (a int, b int, c string);
reset hive.metastore.disallow.incompatible.col.type.changes;
describe alter1;

-- Cleanup
DROP TABLE alter1;
SHOW TABLES;

-- With non-default Database

CREATE DATABASE alter1_db;
SHOW TABLES alter1_db;

CREATE TABLE alter1_db.alter1(a INT, b INT);
DESCRIBE EXTENDED alter1_db.alter1;

ALTER TABLE alter1_db.alter1 SET TBLPROPERTIES ('a'='1', 'c'='3');
DESCRIBE EXTENDED alter1_db.alter1;

ALTER TABLE alter1_db.alter1 SET TBLPROPERTIES ('a'='1', 'c'='4', 'd'='3');
DESCRIBE EXTENDED alter1_db.alter1;

ALTER TABLE alter1_db.alter1 SET TBLPROPERTIES ('EXTERNAL'='TRUE');
DESCRIBE EXTENDED alter1_db.alter1;

ALTER TABLE alter1_db.alter1 SET TBLPROPERTIES ('EXTERNAL'='FALSE');
DESCRIBE EXTENDED alter1_db.alter1;

ALTER TABLE alter1_db.alter1 SET SERDEPROPERTIES('s1'='9');
DESCRIBE EXTENDED alter1_db.alter1;

ALTER TABLE alter1_db.alter1 SET SERDEPROPERTIES('s1'='10', 's2' ='20');
DESCRIBE EXTENDED alter1_db.alter1;

add jar ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
ALTER TABLE alter1_db.alter1 SET SERDE 'org.apache.hadoop.hive.serde2.TestSerDe' WITH SERDEPROPERTIES ('s1'='9');
DESCRIBE EXTENDED alter1_db.alter1;

ALTER TABLE alter1_db.alter1 SET SERDE 'org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe';
DESCRIBE EXTENDED alter1_db.alter1;
set hive.metastore.disallow.incompatible.col.type.changes=false;
ALTER TABLE alter1_db.alter1 REPLACE COLUMNS (a int, b int, c string);
reset hive.metastore.disallow.incompatible.col.type.changes;
DESCRIBE alter1_db.alter1;

DROP TABLE alter1_db.alter1;
DROP DATABASE alter1_db;
create table alter2(a int, b int) partitioned by (insertdate string);
describe extended alter2;
show partitions alter2;
alter table alter2 add partition (insertdate='2008-01-01') location '2008/01/01';
describe extended alter2;
show partitions alter2;
alter table alter2 add partition (insertdate='2008-01-02') location '2008/01/02';
describe extended alter2;
show partitions alter2;
drop table alter2;

create external table alter2(a int, b int) partitioned by (insertdate string);
describe extended alter2;
show partitions alter2;
alter table alter2 add partition (insertdate='2008-01-01') location '2008/01/01';
describe extended alter2;
show partitions alter2;
alter table alter2 add partition (insertdate='2008-01-02') location '2008/01/02';
describe extended alter2;
show partitions alter2;

-- Cleanup
DROP TABLE alter2;
SHOW TABLES;

-- Using non-default Database

CREATE DATABASE alter2_db;
USE alter2_db;
SHOW TABLES;

CREATE TABLE alter2(a int, b int) PARTITIONED BY (insertdate string);
DESCRIBE EXTENDED alter2;
SHOW PARTITIONS alter2;
ALTER TABLE alter2 ADD PARTITION (insertdate='2008-01-01') LOCATION '2008/01/01';
DESCRIBE EXTENDED alter2;
SHOW PARTITIONS alter2;
ALTER TABLE alter2 ADD PARTITION (insertdate='2008-01-02') LOCATION '2008/01/02';
DESCRIBE EXTENDED alter2;
SHOW PARTITIONS alter2;
DROP TABLE alter2;

CREATE EXTERNAL TABLE alter2(a int, b int) PARTITIONED BY (insertdate string);
DESCRIBE EXTENDED alter2;
SHOW PARTITIONS alter2;
ALTER TABLE alter2 ADD PARTITION (insertdate='2008-01-01') LOCATION '2008/01/01';
DESCRIBE EXTENDED alter2;
SHOW PARTITIONS alter2;
ALTER TABLE alter2 ADD PARTITION (insertdate='2008-01-02') LOCATION '2008/01/02';
DESCRIBE EXTENDED alter2;
SHOW PARTITIONS alter2;

DROP TABLE alter2;
USE default;
DROP DATABASE alter2_db;
create table alter3_src ( col1 string ) stored as textfile ;
load data local inpath '../../data/files/test.dat' overwrite into table alter3_src ;

create table alter3 ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;

create table alter3_like like alter3;

insert overwrite table alter3 partition (pCol1='test_part:', pcol2='test_part:') select col1 from alter3_src ;
select * from alter3 where pcol1='test_part:' and pcol2='test_part:';


alter table alter3 rename to alter3_renamed;
describe extended alter3_renamed;
describe extended alter3_renamed partition (pCol1='test_part:', pcol2='test_part:');
select * from alter3_renamed where pcol1='test_part:' and pcol2='test_part:';

insert overwrite table alter3_like
partition (pCol1='test_part:', pcol2='test_part:')
select col1 from alter3_src;
alter table alter3_like rename to alter3_like_renamed;

describe extended alter3_like_renamed;

-- Cleanup
DROP TABLE alter3_src;
DROP TABLE alter3_renamed;
DROP TABLE alter3_like_renamed;
SHOW TABLES;

-- With non-default Database

CREATE DATABASE alter3_db;
USE alter3_db;
SHOW TABLES;

CREATE TABLE alter3_src (col1 STRING) STORED AS TEXTFILE ;
LOAD DATA LOCAL INPATH '../../data/files/test.dat' OVERWRITE INTO TABLE alter3_src ;

CREATE TABLE alter3 (col1 STRING) PARTITIONED BY (pcol1 STRING, pcol2 STRING) STORED AS SEQUENCEFILE;

CREATE TABLE alter3_like LIKE alter3;

INSERT OVERWRITE TABLE alter3 PARTITION (pCol1='test_part:', pcol2='test_part:') SELECT col1 FROM alter3_src ;
SELECT * FROM alter3 WHERE pcol1='test_part:' AND pcol2='test_part:';

ALTER TABLE alter3 RENAME TO alter3_renamed;
DESCRIBE EXTENDED alter3_renamed;
DESCRIBE EXTENDED alter3_renamed PARTITION (pCol1='test_part:', pcol2='test_part:');
SELECT * FROM alter3_renamed WHERE pcol1='test_part:' AND pcol2='test_part:';

INSERT OVERWRITE TABLE alter3_like
PARTITION (pCol1='test_part:', pcol2='test_part:')
SELECT col1 FROM alter3_src;
ALTER TABLE alter3_like RENAME TO alter3_like_renamed;

DESCRIBE EXTENDED alter3_like_renamed;
CREATE TABLE set_bucketing_test (key INT, value STRING) CLUSTERED BY (key) INTO 10 BUCKETS;
DESCRIBE EXTENDED set_bucketing_test;

ALTER TABLE set_bucketing_test NOT CLUSTERED;
DESCRIBE EXTENDED set_bucketing_test;

-- Cleanup
DROP TABLE set_bucketing_test;
SHOW TABLES;

-- with non-default Database

CREATE DATABASE alter4_db;
USE alter4_db;
SHOW TABLES;

CREATE TABLE set_bucketing_test (key INT, value STRING) CLUSTERED BY (key) INTO 10 BUCKETS;
DESCRIBE EXTENDED set_bucketing_test;

ALTER TABLE set_bucketing_test NOT CLUSTERED;
DESCRIBE EXTENDED set_bucketing_test;

DROP TABLE set_bucketing_test;
USE default;
DROP DATABASE alter4_db;
SHOW DATABASES;
--
-- Added to validate the fix for HIVE-2117 - explicit partition location
--

create table alter5_src ( col1 string ) stored as textfile ;
load data local inpath '../../data/files/test.dat' overwrite into table alter5_src ;

create table alter5 ( col1 string ) partitioned by (dt string);

--
-- Here's the interesting bit for HIVE-2117 - partition subdir should be
-- named "parta".
--
alter table alter5 add partition (dt='a') location 'parta';

describe extended alter5 partition (dt='a');

insert overwrite table alter5 partition (dt='a') select col1 from alter5_src ;
select * from alter5 where dt='a';

describe extended alter5 partition (dt='a');

-- Cleanup
DROP TABLE alter5_src;
DROP TABLE alter5;
SHOW TABLES;

-- With non-default Database

CREATE DATABASE alter5_db;
USE alter5_db;
SHOW TABLES;

create table alter5_src ( col1 string ) stored as textfile ;
load data local inpath '../../data/files/test.dat' overwrite into table alter5_src ;

create table alter5 ( col1 string ) partitioned by (dt string);
alter table alter5 add partition (dt='a') location 'parta';

describe extended alter5 partition (dt='a');

insert overwrite table alter5 partition (dt='a') select col1 from alter5_src ;
select * from alter5 where dt='a';

describe extended alter5 partition (dt='a');

create table altern1(a int, b int) partitioned by (ds string);
alter table altern1 replace columns(a int, b int, ds string);

create database newDB location "/tmp/";
describe database extended newDB;
use newDB;
create table tab (name string);
alter table tab rename to newName;
-- SORT_QUERY_RESULTS

create database ac;

create table ac.alter_char_1 (key string, value string);
insert overwrite table ac.alter_char_1
  select key, value from src order by key limit 5;

select * from ac.alter_char_1;

-- change column to char
alter table ac.alter_char_1 change column value value char(20);
-- contents should still look the same
select * from ac.alter_char_1;

-- change column to smaller char
alter table ac.alter_char_1 change column value value char(3);
-- value column should be truncated now
select * from ac.alter_char_1;

-- change back to bigger char
alter table ac.alter_char_1 change column value value char(20);
-- column values should be full size again
select * from ac.alter_char_1;

-- add char column
alter table ac.alter_char_1 add columns (key2 int, value2 char(10));
select * from ac.alter_char_1;

insert overwrite table ac.alter_char_1
  select key, value, key, value from src order by key limit 5;
select * from ac.alter_char_1;

drop table ac.alter_char_1;
drop database ac;
set hive.mapred.mode=nonstrict;

-- alter column type, with partitioned table
drop table if exists alter_char2;

create table alter_char2 (
  c1 char(255)
) partitioned by (hr int);

insert overwrite table alter_char2 partition (hr=1)
  select value from src limit 1;

select c1, length(c1) from alter_char2;

alter table alter_char2 change column c1 c1 char(10);

select hr, c1, length(c1) from alter_char2 where hr = 1;

insert overwrite table alter_char2 partition (hr=2)
  select key from src limit 1;

select hr, c1, length(c1) from alter_char2 where hr = 1;
select hr, c1, length(c1) from alter_char2 where hr = 2;
set hive.mapred.mode=nonstrict;
set hive.exec.concatenate.check.index =false;
create table src_rc_concatenate_test(key int, value string) stored as rcfile;

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_concatenate_test;
load data local inpath '../../data/files/smbbucket_2.rc' into table src_rc_concatenate_test;
load data local inpath '../../data/files/smbbucket_3.rc' into table src_rc_concatenate_test;

show table extended like `src_rc_concatenate_test`;

select count(1) from src_rc_concatenate_test;
select sum(hash(key)), sum(hash(value)) from src_rc_concatenate_test;

create index src_rc_concatenate_test_index on table src_rc_concatenate_test(key) as 'compact' WITH DEFERRED REBUILD IDXPROPERTIES ("prop1"="val1", "prop2"="val2");
show indexes on src_rc_concatenate_test;

alter table src_rc_concatenate_test concatenate;

show table extended like `src_rc_concatenate_test`;

select count(1) from src_rc_concatenate_test;
select sum(hash(key)), sum(hash(value)) from src_rc_concatenate_test;

drop index src_rc_concatenate_test_index on src_rc_concatenate_test;

create table src_rc_concatenate_test_part(key int, value string) partitioned by (ds string) stored as rcfile;

alter table src_rc_concatenate_test_part add partition (ds='2011');

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_concatenate_test_part partition (ds='2011');
load data local inpath '../../data/files/smbbucket_2.rc' into table src_rc_concatenate_test_part partition (ds='2011');
load data local inpath '../../data/files/smbbucket_3.rc' into table src_rc_concatenate_test_part partition (ds='2011');

show table extended like `src_rc_concatenate_test_part` partition (ds='2011');

select count(1) from src_rc_concatenate_test_part;
select sum(hash(key)), sum(hash(value)) from src_rc_concatenate_test_part;

create index src_rc_concatenate_test_part_index on table src_rc_concatenate_test_part(key) as 'compact' WITH DEFERRED REBUILD IDXPROPERTIES ("prop1"="val1", "prop2"="val2");
show indexes on src_rc_concatenate_test_part;

alter table src_rc_concatenate_test_part partition (ds='2011') concatenate;

show table extended like `src_rc_concatenate_test_part` partition (ds='2011');

select count(1) from src_rc_concatenate_test_part;
select sum(hash(key)), sum(hash(value)) from src_rc_concatenate_test_part;

drop index src_rc_concatenate_test_part_index on src_rc_concatenate_test_part;
set hive.exec.concatenate.check.index=true;
create table src_rc_concatenate_test(key int, value string) stored as rcfile;

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_concatenate_test;
load data local inpath '../../data/files/smbbucket_2.rc' into table src_rc_concatenate_test;
load data local inpath '../../data/files/smbbucket_3.rc' into table src_rc_concatenate_test;

show table extended like `src_rc_concatenate_test`;

select count(1) from src_rc_concatenate_test;
select sum(hash(key)), sum(hash(value)) from src_rc_concatenate_test;

create index src_rc_concatenate_test_index on table src_rc_concatenate_test(key) as 'compact' WITH DEFERRED REBUILD IDXPROPERTIES ("prop1"="val1", "prop2"="val2");
show indexes on src_rc_concatenate_test;

alter table src_rc_concatenate_test concatenate;

create database db_alter_onr;
describe database db_alter_onr;

alter database db_alter_onr set owner user user1;
describe database db_alter_onr;

alter database db_alter_onr set owner role role1;
describe database db_alter_onr;
create table alter_file_format_test (key int, value string);
desc FORMATTED alter_file_format_test;

alter table alter_file_format_test set fileformat rcfile;
desc FORMATTED alter_file_format_test;

alter table alter_file_format_test set fileformat textfile;
desc FORMATTED alter_file_format_test;

alter table alter_file_format_test set fileformat rcfile;
desc FORMATTED alter_file_format_test;

alter table alter_file_format_test set fileformat sequencefile;
desc FORMATTED alter_file_format_test;

alter table alter_file_format_test set fileformat parquet;
desc FORMATTED alter_file_format_test;

ALTER TABLE alter_file_format_test SET FILEFORMAT INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat' SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';
desc FORMATTED alter_file_format_test;

drop table alter_partition_format_test;

--partitioned table
create table alter_partition_format_test (key int, value string) partitioned by (ds string);

alter table alter_partition_format_test add partition(ds='2010');
desc FORMATTED alter_partition_format_test partition(ds='2010');

alter table alter_partition_format_test partition(ds='2010') set fileformat rcfile;
desc FORMATTED alter_partition_format_test partition(ds='2010');

alter table alter_partition_format_test partition(ds='2010') set fileformat textfile;
desc FORMATTED alter_partition_format_test partition(ds='2010');

alter table alter_partition_format_test partition(ds='2010') set fileformat rcfile;
desc FORMATTED alter_partition_format_test partition(ds='2010');

alter table alter_partition_format_test partition(ds='2010') set fileformat sequencefile;
desc FORMATTED alter_partition_format_test partition(ds='2010');

alter table alter_partition_format_test partition(ds='2010') set fileformat parquet;
desc FORMATTED alter_partition_format_test partition(ds='2010');

drop table alter_partition_format_test;create table alter_file_format_test (key int, value string);
desc FORMATTED alter_file_format_test;

ALTER TABLE alter_file_format_test SET FILEFORMAT INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';

drop table alter_partition_format_test;drop index src_index_8 on src;

create index src_index_8 on table default.src(key) as 'compact' WITH DEFERRED REBUILD IDXPROPERTIES ("prop1"="val1", "prop2"="val2");
desc extended default__src_src_index_8__;

alter index src_index_8 on default.src set IDXPROPERTIES ("prop1"="val1_new", "prop3"="val3");
desc extended default__src_src_index_8__;

drop index src_index_8 on default.src;

show tables;
set hive.mapred.mode=nonstrict;
create table src_rc_merge_test(key int, value string) stored as rcfile;

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_merge_test;
load data local inpath '../../data/files/smbbucket_2.rc' into table src_rc_merge_test;
load data local inpath '../../data/files/smbbucket_3.rc' into table src_rc_merge_test;

show table extended like `src_rc_merge_test`;

select count(1) from src_rc_merge_test;
select sum(hash(key)), sum(hash(value)) from src_rc_merge_test;

alter table src_rc_merge_test concatenate;

show table extended like `src_rc_merge_test`;

select count(1) from src_rc_merge_test;
select sum(hash(key)), sum(hash(value)) from src_rc_merge_test;


create table src_rc_merge_test_part(key int, value string) partitioned by (ds string) stored as rcfile;

alter table src_rc_merge_test_part add partition (ds='2011');

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_merge_test_part partition (ds='2011');
load data local inpath '../../data/files/smbbucket_2.rc' into table src_rc_merge_test_part partition (ds='2011');
load data local inpath '../../data/files/smbbucket_3.rc' into table src_rc_merge_test_part partition (ds='2011');

show table extended like `src_rc_merge_test_part` partition (ds='2011');

select count(1) from src_rc_merge_test_part;
select sum(hash(key)), sum(hash(value)) from src_rc_merge_test_part;

alter table src_rc_merge_test_part partition (ds='2011') concatenate;

show table extended like `src_rc_merge_test_part` partition (ds='2011');

select count(1) from src_rc_merge_test_part;
select sum(hash(key)), sum(hash(value)) from src_rc_merge_test_part;

drop table src_rc_merge_test;
drop table src_rc_merge_test_part;create table src_rc_merge_test_part(key int, value string) partitioned by (ds string, ts string) stored as rcfile;

alter table src_rc_merge_test_part add partition (ds='2012-01-03', ts='2012-01-03+14:46:31');
desc extended src_rc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31');

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31');
load data local inpath '../../data/files/smbbucket_2.rc' into table src_rc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31');
load data local inpath '../../data/files/smbbucket_3.rc' into table src_rc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31');

select count(1) from src_rc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';
select sum(hash(key)), sum(hash(value)) from src_rc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';

alter table src_rc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31') concatenate;


select count(1) from src_rc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';
select sum(hash(key)), sum(hash(value)) from src_rc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';

drop table src_rc_merge_test_part;
set hive.mapred.mode=nonstrict;
create table src_orc_merge_test_part(key int, value string) partitioned by (ds string, ts string) stored as orc;

alter table src_orc_merge_test_part add partition (ds='2012-01-03', ts='2012-01-03+14:46:31');
desc extended src_orc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31');

insert overwrite table src_orc_merge_test_part partition(ds='2012-01-03', ts='2012-01-03+14:46:31') select * from src order by key, value;
insert into table src_orc_merge_test_part partition(ds='2012-01-03', ts='2012-01-03+14:46:31') select * from src order by key, value limit 100;
insert into table src_orc_merge_test_part partition(ds='2012-01-03', ts='2012-01-03+14:46:31') select * from src order by key, value limit 10;

select count(1) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';
select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';

alter table src_orc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31') concatenate;


select count(1) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';
select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';

drop table src_orc_merge_test_part;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/alter_merge_location/ds20140804;
dfs -put ../../data/files/smbbucket_1.rc ${system:test.tmp.dir}/alter_merge_location/ds20140804;
dfs -put ../../data/files/smbbucket_2.rc ${system:test.tmp.dir}/alter_merge_location/ds20140804;
dfs -put ../../data/files/smbbucket_3.rc ${system:test.tmp.dir}/alter_merge_location/ds20140804;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/alter_merge_location/ds20140805;
dfs -put ../../data/files/smbbucket_1.rc ${system:test.tmp.dir}/alter_merge_location/ds20140805;
dfs -put ../../data/files/smbbucket_2.rc ${system:test.tmp.dir}/alter_merge_location/ds20140805;
dfs -put ../../data/files/smbbucket_3.rc ${system:test.tmp.dir}/alter_merge_location/ds20140805;

create table src_rc_merge_test_part (key int, value string) partitioned by (ds string) stored as rcfile;

set fs.hdfs.impl.disable.cache=false;
alter table src_rc_merge_test_part add partition (ds = '2014-08-04') location '${system:test.tmp.dir}/alter_merge_location/ds20140804';
alter table src_rc_merge_test_part partition (ds = '2014-08-04') concatenate;
select count(1) from src_rc_merge_test_part where ds='2014-08-04';

set fs.hdfs.impl.disable.cache=true;
alter table src_rc_merge_test_part add partition (ds = '2014-08-05') location '${system:test.tmp.dir}/alter_merge_location/ds20140805';
alter table src_rc_merge_test_part partition (ds = '2014-08-05') concatenate;
select count(1) from src_rc_merge_test_part where ds='2014-08-05';

drop table src_rc_merge_test_part;set hive.mapred.mode=nonstrict;
create table src_orc_merge_test(key int, value string) stored as orc;

insert overwrite table src_orc_merge_test select * from src;
insert into table src_orc_merge_test select * from src;
insert into table src_orc_merge_test select * from src;

show table extended like `src_orc_merge_test`;

select count(1) from src_orc_merge_test;
select sum(hash(key)), sum(hash(value)) from src_orc_merge_test;

alter table src_orc_merge_test concatenate;

show table extended like `src_orc_merge_test`;

select count(1) from src_orc_merge_test;
select sum(hash(key)), sum(hash(value)) from src_orc_merge_test;


create table src_orc_merge_test_part(key int, value string) partitioned by (ds string) stored as orc;

alter table src_orc_merge_test_part add partition (ds='2011');

insert overwrite table src_orc_merge_test_part partition (ds='2011') select * from src;
insert into table src_orc_merge_test_part partition (ds='2011') select * from src;
insert into table src_orc_merge_test_part partition (ds='2011') select * from src;

show table extended like `src_orc_merge_test_part` partition (ds='2011');

select count(1) from src_orc_merge_test_part;
select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part;

alter table src_orc_merge_test_part partition (ds='2011') concatenate;

show table extended like `src_orc_merge_test_part` partition (ds='2011');

select count(1) from src_orc_merge_test_part;
select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part;

drop table src_orc_merge_test;
drop table src_orc_merge_test_part;
create table src_rc_merge_test_stat(key int, value string) stored as rcfile;

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_merge_test_stat;
load data local inpath '../../data/files/smbbucket_2.rc' into table src_rc_merge_test_stat;
load data local inpath '../../data/files/smbbucket_3.rc' into table src_rc_merge_test_stat;

show table extended like `src_rc_merge_test_stat`;
desc extended src_rc_merge_test_stat;

analyze table src_rc_merge_test_stat compute statistics;

desc extended src_rc_merge_test_stat;

alter table src_rc_merge_test_stat concatenate;

show table extended like `src_rc_merge_test_stat`;
desc extended src_rc_merge_test_stat;


create table src_rc_merge_test_part_stat(key int, value string) partitioned by (ds string) stored as rcfile;

alter table src_rc_merge_test_part_stat add partition (ds='2011');

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_merge_test_part_stat partition (ds='2011');
load data local inpath '../../data/files/smbbucket_2.rc' into table src_rc_merge_test_part_stat partition (ds='2011');
load data local inpath '../../data/files/smbbucket_3.rc' into table src_rc_merge_test_part_stat partition (ds='2011');

show table extended like `src_rc_merge_test_part_stat` partition (ds='2011');
desc extended src_rc_merge_test_part_stat;

analyze table src_rc_merge_test_part_stat partition(ds='2011') compute statistics;

desc extended src_rc_merge_test_part_stat;

alter table src_rc_merge_test_part_stat partition (ds='2011') concatenate;

show table extended like `src_rc_merge_test_part_stat` partition (ds='2011');
desc extended src_rc_merge_test_part_stat;

drop table src_rc_merge_test_stat;
drop table src_rc_merge_test_part_stat;set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;
create table src_orc_merge_test_stat(key int, value string) stored as orc;

insert overwrite table src_orc_merge_test_stat select * from src;
insert into table src_orc_merge_test_stat select * from src;
insert into table src_orc_merge_test_stat select * from src;

show table extended like `src_orc_merge_test_stat`;
desc extended src_orc_merge_test_stat;

analyze table src_orc_merge_test_stat compute statistics noscan;
desc formatted  src_orc_merge_test_stat;

alter table src_orc_merge_test_stat concatenate;
analyze table src_orc_merge_test_stat compute statistics noscan;
desc formatted src_orc_merge_test_stat;


create table src_orc_merge_test_part_stat(key int, value string) partitioned by (ds string) stored as orc;

alter table src_orc_merge_test_part_stat add partition (ds='2011');

insert overwrite table src_orc_merge_test_part_stat partition (ds='2011') select * from src;
insert into table src_orc_merge_test_part_stat partition (ds='2011') select * from src;
insert into table src_orc_merge_test_part_stat partition (ds='2011') select * from src;

show table extended like `src_orc_merge_test_part_stat` partition (ds='2011');
desc formatted src_orc_merge_test_part_stat partition (ds='2011');

analyze table src_orc_merge_test_part_stat partition(ds='2011') compute statistics noscan;
desc formatted src_orc_merge_test_part_stat partition (ds='2011');

alter table src_orc_merge_test_part_stat partition (ds='2011') concatenate;
analyze table src_orc_merge_test_part_stat partition(ds='2011') compute statistics noscan;
desc formatted src_orc_merge_test_part_stat partition (ds='2011');

drop table src_orc_merge_test_stat;
drop table src_orc_merge_test_part_stat;

CREATE TABLE non_native1(key int, value string)
STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler';

-- we do not support ALTER TABLE on non-native tables yet
ALTER TABLE non_native1 RENAME TO new_non_native;
-- Tests that when overwriting a partition in a table after altering the bucketing/sorting metadata
-- the partition metadata is updated as well.

CREATE TABLE tst1(key STRING, value STRING) PARTITIONED BY (ds STRING);

DESCRIBE FORMATTED tst1;



INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test an unbucketed partition gets converted to bucketed
ALTER TABLE tst1 CLUSTERED BY (key) INTO 8 BUCKETS;

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test an unsorted partition gets converted to sorted
ALTER TABLE tst1 CLUSTERED BY (key) SORTED BY (key DESC) INTO 8 BUCKETS;

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test changing the bucket columns
ALTER TABLE tst1 CLUSTERED BY (value) SORTED BY (key DESC) INTO 8 BUCKETS;

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test changing the number of buckets
ALTER TABLE tst1 CLUSTERED BY (value) SORTED BY (key DESC) INTO 4 BUCKETS;

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test changing the sort columns
ALTER TABLE tst1 CLUSTERED BY (value) SORTED BY (value DESC) INTO 4 BUCKETS;

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test changing the sort order
ALTER TABLE tst1 CLUSTERED BY (value) SORTED BY (value ASC) INTO 4 BUCKETS;

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test a sorted partition gets converted to unsorted
ALTER TABLE tst1 CLUSTERED BY (value) INTO 4 BUCKETS;

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test a bucketed partition gets converted to unbucketed
ALTER TABLE tst1 NOT CLUSTERED;

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)
create table tst1(key string, value string) partitioned by (ds string) clustered by (key) into 10 buckets;

alter table tst1 clustered by (key) into 8 buckets;

describe formatted tst1;


insert overwrite table tst1 partition (ds='1') select key, value from src;

describe formatted tst1 partition (ds = '1');

-- Test changing bucket number

alter table tst1 clustered by (key) into 12 buckets;

insert overwrite table tst1 partition (ds='1') select key, value from src;

describe formatted tst1 partition (ds = '1');

describe formatted tst1;

-- Test changing bucket number of (table/partition)

alter table tst1 into 4 buckets;

describe formatted tst1;

describe formatted tst1 partition (ds = '1');

alter table tst1 partition (ds = '1') into 6 buckets;

describe formatted tst1;

describe formatted tst1 partition (ds = '1');

-- Test adding sort order

alter table tst1 clustered by (key) sorted by (key asc) into 12 buckets;

describe formatted tst1;

-- Test changing sort order

alter table tst1 clustered by (key) sorted by (value desc) into 12 buckets;

describe formatted tst1;

-- Test removing test order

alter table tst1 clustered by (value) into 12 buckets;

describe formatted tst1;

-- Test removing buckets

alter table tst1 not clustered;

describe formatted tst1;
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

-- SORT_QUERY_RESULTS

create table alter_partition_change_col0 (c1 string, c2 string);
load data local inpath '../../data/files/dec.txt' overwrite into table alter_partition_change_col0;

create table alter_partition_change_col1 (c1 string, c2 string) partitioned by (p1 string comment 'Column p1', p2 string comment 'Column p2');

insert overwrite table alter_partition_change_col1 partition (p1, p2)
  select c1, c2, 'abc', '123' from alter_partition_change_col0
  union all
  select c1, c2, null, '123' from alter_partition_change_col0;

show partitions alter_partition_change_col1;
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

-- Change c2 to decimal(10,0)
alter table alter_partition_change_col1 change c2 c2 decimal(10,0);
alter table alter_partition_change_col1 partition (p1='abc', p2='123') change c2 c2 decimal(10,0);
alter table alter_partition_change_col1 partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123') change c2 c2 decimal(10,0);
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

-- Change the column type at the table level. Table-level describe shows the new type, but the existing partition does not.
alter table alter_partition_change_col1 change c2 c2 decimal(14,4);
describe alter_partition_change_col1;
describe alter_partition_change_col1 partition (p1='abc', p2='123');
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

-- change the comment on a partition column without changing type or renaming it
alter table alter_partition_change_col1 partition column (p1 string comment 'Changed comment for p1');
describe alter_partition_change_col1;

-- now change the column type of the existing partition
alter table alter_partition_change_col1 partition (p1='abc', p2='123') change c2 c2 decimal(14,4);
describe alter_partition_change_col1 partition (p1='abc', p2='123');
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

-- change column for default partition value
alter table alter_partition_change_col1 partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123') change c2 c2 decimal(14,4);
describe alter_partition_change_col1 partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

-- Try out replace columns
alter table alter_partition_change_col1 partition (p1='abc', p2='123') replace columns (c1 string);
describe alter_partition_change_col1;
describe alter_partition_change_col1 partition (p1='abc', p2='123');
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

alter table alter_partition_change_col1 replace columns (c1 string);
describe alter_partition_change_col1;
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

-- Try add columns
alter table alter_partition_change_col1 add columns (c2 decimal(14,4));
describe alter_partition_change_col1;
describe alter_partition_change_col1 partition (p1='abc', p2='123');
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

alter table alter_partition_change_col1 partition (p1='abc', p2='123') add columns (c2 decimal(14,4));
describe alter_partition_change_col1 partition (p1='abc', p2='123');
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';

-- Try changing column for all partitions at once
alter table alter_partition_change_col1 partition (p1, p2='123') change column c2 c2 decimal(10,0);
describe alter_partition_change_col1 partition (p1='abc', p2='123');
describe alter_partition_change_col1 partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_partition_change_col1 where p1='abc';
select * from alter_partition_change_col1 where p1='__HIVE_DEFAULT_PARTITION__';
create table alter_partition_change_col_dup_col (c1 string, c2 decimal(10,0)) partitioned by (p1 string);
alter table alter_partition_change_col_dup_col add partition (p1='abc');
-- should fail because of duplicate name c1
alter table alter_partition_change_col_dup_col change c2 c1 decimal(14,4);
create table alter_partition_change_col_nonexist (c1 string, c2 decimal(10,0)) partitioned by (p1 string);
alter table alter_partition_change_col_nonexist add partition (p1='abc');
-- should fail because of nonexistent column c3
alter table alter_partition_change_col_nonexist change c3 c4 decimal(14,4);

create table alter_table_partition_clusterby_sortby (a int, b int) partitioned by (c string) clustered by (a, b) sorted by (a desc, b asc) into 4 buckets;
alter table alter_table_partition_clusterby_sortby add partition(c='abc');

-- Turn off sorting for a partition

alter table alter_table_partition_clusterby_sortby partition(c='abc') not sorted;
desc formatted alter_table_partition_clusterby_sortby partition(c='abc');

-- Modify clustering for a partition

alter table alter_table_partition_clusterby_sortby partition(c='abc') clustered by (b) sorted by (b desc) into 4 buckets;
desc formatted alter_table_partition_clusterby_sortby partition(c='abc');

-- Turn off clustering for a partition

alter table alter_table_partition_clusterby_sortby partition(c='abc') not clustered;
desc formatted alter_table_partition_clusterby_sortby partition(c='abc');

-- Table properties should be unchanged

desc formatted alter_table_partition_clusterby_sortby;

drop table alter_table_partition_clusterby_sortby;
-- create testing table.
create table alter_coltype(key string, value string) partitioned by (dt string, ts string);

-- insert and create a partition.
insert overwrite table alter_coltype partition(dt='100', ts='6.30') select * from src1;

desc alter_coltype;

-- select with paritition predicate.
select count(*) from alter_coltype where dt = '100';

-- alter partition key column data type for dt column.
alter table alter_coltype partition column (dt int);

-- load a new partition using new data type.
insert overwrite table alter_coltype partition(dt=100, ts='3.0') select * from src1;

-- make sure the partition predicate still works.
select count(*) from alter_coltype where dt = '100';
explain extended select count(*) from alter_coltype where dt = '100';

-- alter partition key column data type for ts column.
alter table alter_coltype partition column (ts double);

alter table alter_coltype partition column (dt string);

-- load a new partition using new data type.
insert overwrite table alter_coltype partition(dt='100', ts=3.0) select * from src1;

--  validate partition key column predicate can still work.
select count(*) from alter_coltype where ts = '6.30';
explain extended select count(*) from alter_coltype where ts = '6.30';

--  validate partition key column predicate on two different partition column data type
--  can still work.
select count(*) from alter_coltype where ts = 3.0 and dt=100;
explain extended select count(*) from alter_coltype where ts = 3.0 and dt=100;

-- query where multiple partition values (of different datatypes) are being selected
select key, value, dt, ts from alter_coltype where dt is not null;
explain extended select key, value, dt, ts from alter_coltype where dt is not null;

select count(*) from alter_coltype where ts = 3.0;

-- make sure the partition predicate still works.
select count(*) from alter_coltype where dt = '100';

desc alter_coltype;
set hive.typecheck.on.insert=false;
desc alter_coltype partition (dt='100', ts='6.30');
desc alter_coltype partition (dt='100', ts=3.0);

drop table alter_coltype;

create database pt;

create table pt.alterdynamic_part_table(intcol string) partitioned by (partcol1 string, partcol2 string);

set hive.exec.dynamic.partition.mode=nonstrict;

insert into table pt.alterdynamic_part_table partition(partcol1, partcol2) select '1', '1', '1' from src where key=150 limit 5;

insert into table pt.alterdynamic_part_table partition(partcol1, partcol2) select '1', '2', '1' from src where key=150 limit 5;
insert into table pt.alterdynamic_part_table partition(partcol1, partcol2) select NULL, '1', '1' from src where key=150 limit 5;

alter table pt.alterdynamic_part_table partition column (partcol1 int);

explain extended select intcol from pt.alterdynamic_part_table where partcol1='1' and partcol2='1';

explain extended select intcol from pt.alterdynamic_part_table where (partcol1='2' and partcol2='1')or (partcol1='1' and partcol2='__HIVE_DEFAULT_PARTITION__');
select intcol from pt.alterdynamic_part_table where (partcol1='2' and partcol2='1')or (partcol1='1' and partcol2='__HIVE_DEFAULT_PARTITION__');

drop table pt.alterdynamic_part_table;
drop database pt;
-- create testing table
create table alter_coltype(key string, value string) partitioned by (dt string, ts string);

-- insert and create a partition
insert overwrite table alter_coltype partition(dt='100x', ts='6:30pm') select * from src1;

desc alter_coltype;

-- alter partition change multiple keys at same time
alter table alter_coltype partition column (dt int, ts int);

-- create testing table
create table alter_coltype(key string, value string) partitioned by (dt string, ts string);

-- insert and create a partition
insert overwrite table alter_coltype partition(dt='100x', ts='6:30pm') select * from src1;

desc alter_coltype;

-- alter partition key column with invalid column name
alter table alter_coltype partition column (dd int);


-- create testing table
create table alter_coltype(key string, value string) partitioned by (dt string, ts string);

-- insert and create a partition
insert overwrite table alter_coltype partition(dt='100x', ts='6:30pm') select * from src1;

desc alter_coltype;

-- alter partition key column data type for ts column to a wrong type
alter table alter_coltype partition column (ts time);

create table alter_partition_format_test (key int, value string);
desc extended alter_partition_format_test;

alter table alter_partition_format_test set fileformat rcfile;
desc extended alter_partition_format_test;

alter table alter_partition_format_test set location "file:/test/test/";
desc extended alter_partition_format_test;

drop table alter_partition_format_test;

--partitioned table
create table alter_partition_format_test (key int, value string) partitioned by (ds string);

alter table alter_partition_format_test add partition(ds='2010');
desc extended alter_partition_format_test partition(ds='2010');

alter table alter_partition_format_test partition(ds='2010') set fileformat rcfile;
desc extended alter_partition_format_test partition(ds='2010');

alter table alter_partition_format_test partition(ds='2010') set location "file:/test/test/ds=2010";
desc extended alter_partition_format_test partition(ds='2010');

desc extended alter_partition_format_test;

alter table alter_partition_format_test set fileformat rcfile;
desc extended alter_partition_format_test;

alter table alter_partition_format_test set location "file:/test/test/";
desc extended alter_partition_format_test;

drop table alter_partition_format_test;SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

create table alter_partition_partial_spec_dyndisabled0 (c1 string) partitioned by (p1 string, p2 string);

alter table alter_partition_partial_spec_dyndisabled0 add partition (p1='abc', p2='123');
alter table alter_partition_partial_spec_dyndisabled0 partition (p1, p2) change c1 c1 int;

describe alter_partition_partial_spec_dyndisabled0 partition (p1='abc', p2='123');

SET hive.exec.dynamic.partition = false;
-- Same statement should fail if dynamic partitioning disabled
alter table alter_partition_partial_spec_dyndisabled0 partition (p1, p2) change c1 c1 int;
create table src_stat_part_one(key string, value string) partitioned by (partitionId int);

insert overwrite table src_stat_part_one partition (partitionId=1)
  select * from src1;

ANALYZE TABLE src_stat_part_one PARTITION(partitionId=1) COMPUTE STATISTICS for columns;

describe formatted src_stat_part_one PARTITION(partitionId=1) key;

ALTER TABLE src_stat_part_one PARTITION(partitionId=1) UPDATE STATISTICS for column key SET ('numDVs'='11','avgColLen'='2.2');

describe formatted src_stat_part_one PARTITION(partitionId=1) key;

create table src_stat_part_two(key string, value string) partitioned by (px int, py string);

insert overwrite table src_stat_part_two partition (px=1, py='a')
  select * from src1;

ANALYZE TABLE src_stat_part_two PARTITION(px=1) COMPUTE STATISTICS for columns;

describe formatted src_stat_part_two PARTITION(px=1, py='a') key;

ALTER TABLE src_stat_part_two PARTITION(px=1, py='a') UPDATE STATISTICS for column key SET ('numDVs'='30','maxColLen'='40');

describe formatted src_stat_part_two PARTITION(px=1, py='a') key;

create database if not exists dummydb;

use dummydb;

ALTER TABLE default.src_stat_part_two PARTITION(px=1, py='a') UPDATE STATISTICS for column key SET ('numDVs'='40','maxColLen'='50');

describe formatted default.src_stat_part_two PARTITION(px=1, py='a') key;

use default;

drop database dummydb;
SET hive.metastore.partition.name.whitelist.pattern=[A-Za-z]*;
-- This pattern matches only letters.

CREATE TABLE part_whitelist_test (key STRING, value STRING) PARTITIONED BY (ds STRING);
SHOW PARTITIONS part_whitelist_test;

ALTER TABLE part_whitelist_test ADD PARTITION (ds='Part');

ALTER TABLE part_whitelist_test PARTITION (ds='Part') rename to partition (ds='Apart');
SET hive.metastore.partition.name.whitelist.pattern=[\\x20-\\x7E&&[^,]]* ;
-- This pattern matches all printable ASCII characters (disallow unicode) and disallows commas

CREATE TABLE part_whitelist_test (key STRING, value STRING) PARTITIONED BY (ds STRING);
SHOW PARTITIONS part_whitelist_test;

ALTER TABLE part_whitelist_test ADD PARTITION (ds='1');

ALTER TABLE part_whitelist_test PARTITION (ds='1') rename to partition (ds='1,2,3');
-- Cleanup
DROP TABLE alter_rename_partition_src;
DROP TABLE alter_rename_partition;
SHOW TABLES;

create table alter_rename_partition_src ( col1 string ) stored as textfile ;
load data local inpath '../../data/files/test.dat' overwrite into table alter_rename_partition_src ;

create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;

insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;
select * from alter_rename_partition where pcol1='old_part1:' and pcol2='old_part2:';

alter table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') rename to partition (pCol1='new_part1:', pcol2='new_part2:');
SHOW PARTITIONS alter_rename_partition;
select * from alter_rename_partition where pcol1='old_part1:' and pcol2='old_part2:';
select * from alter_rename_partition where pcol1='new_part1:' and pcol2='new_part2:';

-- Cleanup
DROP TABLE alter_rename_partition_src;
DROP TABLE alter_rename_partition;
SHOW TABLES;

-- With non-default Database

CREATE DATABASE alter_rename_partition_db;
USE alter_rename_partition_db;
SHOW TABLES;

CREATE TABLE alter_rename_partition_src (col1 STRING) STORED AS TEXTFILE ;
LOAD DATA LOCAL INPATH '../../data/files/test.dat' OVERWRITE INTO TABLE alter_rename_partition_src ;

CREATE TABLE alter_rename_partition (col1 STRING) PARTITIONED BY (pcol1 STRING, pcol2 STRING) STORED AS SEQUENCEFILE;

INSERT OVERWRITE TABLE alter_rename_partition PARTITION (pCol1='old_part1:', pcol2='old_part2:') SELECT col1 FROM alter_rename_partition_src ;
SELECT * FROM alter_rename_partition WHERE pcol1='old_part1:' AND pcol2='old_part2:';

ALTER TABLE alter_rename_partition PARTITION (pCol1='old_part1:', pcol2='old_part2:') RENAME TO PARTITION (pCol1='new_part1:', pcol2='new_part2:');
SHOW PARTITIONS alter_rename_partition;
SELECT * FROM alter_rename_partition WHERE pcol1='old_part1:' and pcol2='old_part2:';
SELECT * FROM alter_rename_partition WHERE pcol1='new_part1:' and pcol2='new_part2:';
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-- SORT_BEFORE_DIFF

create table src_auth_tmp as select * from src;

create table authorization_part (key int, value string) partitioned by (ds string);
ALTER TABLE authorization_part SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");
set hive.security.authorization.enabled=true;
grant select on table src_auth_tmp to user hive_test_user;

-- column grant to user
grant Create on table authorization_part to user hive_test_user;
grant Update on table authorization_part to user hive_test_user;
grant Drop on table authorization_part to user hive_test_user;

show grant user hive_test_user on table authorization_part;
grant select(key) on table authorization_part to user hive_test_user;
insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp;
show grant user hive_test_user on table authorization_part(key) partition (ds='2010');
alter table authorization_part partition (ds='2010') rename to partition (ds='2010_tmp');
show grant user hive_test_user on table authorization_part(key) partition (ds='2010_tmp');

drop table authorization_part;
create table alter_rename_partition_src ( col1 string ) stored as textfile ;
load data local inpath '../../data/files/test.dat' overwrite into table alter_rename_partition_src ;
create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
insert overwrite table alter_rename_partition partition (pCol1='old_part1', pcol2='old_part2') select col1 from alter_rename_partition_src ;

alter table alter_rename_partition partition (pCol1='nonexist_part1', pcol2='nonexist_part2') rename to partition (pCol1='new_part1', pcol2='new_part2');
create table alter_rename_partition_src ( col1 string ) stored as textfile ;
load data local inpath '../../data/files/test.dat' overwrite into table alter_rename_partition_src ;
create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;

alter table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') rename to partition (pCol1='old_part1:', pcol2='old_part2:');
create table alter_rename_partition_src ( col1 string ) stored as textfile ;
load data local inpath '../../data/files/test.dat' overwrite into table alter_rename_partition_src ;
create table alter_rename_partition ( col1 string ) partitioned by (pcol1 string , pcol2 string) stored as sequencefile;
insert overwrite table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') select col1 from alter_rename_partition_src ;

alter table alter_rename_partition partition (pCol1='old_part1:', pcol2='old_part2:') rename to partition (pCol1='old_part1:', pcol2='old_part2:', pcol3='old_part3:');set hive.mapred.mode=nonstrict;
create database source;
create database target;

create table source.src like default.src;
load data local inpath '../../data/files/kv1.txt' overwrite into table source.src;

create table source.srcpart like default.srcpart;
load data local inpath '../../data/files/kv1.txt' overwrite into table source.srcpart partition (ds='2008-04-08', hr='11');
load data local inpath '../../data/files/kv1.txt' overwrite into table source.srcpart partition (ds='2008-04-08', hr='12');
load data local inpath '../../data/files/kv1.txt' overwrite into table source.srcpart partition (ds='2008-04-09', hr='11');
load data local inpath '../../data/files/kv1.txt' overwrite into table source.srcpart partition (ds='2008-04-09', hr='12');

set hive.fetch.task.conversion=more;

select * from source.src tablesample (10 rows);
select * from source.srcpart tablesample (10 rows);

explain
ALTER TABLE source.src RENAME TO target.src;
ALTER TABLE source.src RENAME TO target.src;

select * from target.src tablesample (10 rows);

explain
ALTER TABLE source.srcpart RENAME TO target.srcpart;
ALTER TABLE source.srcpart RENAME TO target.srcpart;

select * from target.srcpart tablesample (10 rows);

create table source.src like default.src;
create table source.src1 like default.src;
load data local inpath '../../data/files/kv1.txt' overwrite into table source.src;

ALTER TABLE source.src RENAME TO target.src1;
select * from target.src1 tablesample (10 rows);create table original (key STRING, value STRING);

describe formatted original;

alter table original SKEWED BY (key) ON (1,5,6);

describe formatted original;

drop table original;

create database skew_test;

create table skew_test.original2 (key STRING, value STRING) ;

describe formatted skew_test.original2;

alter table skew_test.original2 SKEWED BY (key, value) ON ((1,1),(5,6));

describe formatted skew_test.original2;

drop table skew_test.original2;

create table skew_test.original3 (key STRING, value STRING) SKEWED BY (key, value) ON ((1,1),(5,6));

describe formatted skew_test.original3;

alter table skew_test.original3 not skewed;

describe formatted skew_test.original3;

drop table skew_test.original3;

drop database skew_test;

create table mp (a int) partitioned by (b int);

-- should fail
alter table mp add partition (b='1', c='1');

SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

-- SORT_QUERY_RESULTS

drop table if exists alter_table_src;
drop table if exists alter_table_cascade;

create table alter_table_src(c1 string, c2 string);
load data local inpath '../../data/files/dec.txt' overwrite into table alter_table_src;

create table alter_table_cascade (c1 string) partitioned by (p1 string, p2 string);

insert overwrite table alter_table_cascade partition (p1, p2)
  select c1, 'abc', '123' from alter_table_src
  union all
  select c1, null, '123' from alter_table_src;

show partitions alter_table_cascade;
describe alter_table_cascade;
describe alter_table_cascade partition (p1='abc', p2='123');
describe alter_table_cascade partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_table_cascade where p1='abc';
select * from alter_table_cascade where p1='__HIVE_DEFAULT_PARTITION__';

-- add columns c2 by replace columns (for HIVE-6131)
-- reload data to existing partition __HIVE_DEFAULT_PARTITION__
-- load data to a new partition xyz
-- querying data (form new or existing partition) should return non-NULL values for the new column
alter table alter_table_cascade replace columns (c1 string, c2 string) cascade;
load data local inpath '../../data/files/dec.txt' overwrite into table alter_table_cascade partition (p1='__HIVE_DEFAULT_PARTITION__',p2='123');
load data local inpath '../../data/files/dec.txt' overwrite into table alter_table_cascade partition (p1='xyz', p2='123');
describe alter_table_cascade;
describe alter_table_cascade partition (p1='xyz', p2='123');
describe alter_table_cascade partition (p1='abc', p2='123');
describe alter_table_cascade partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_table_cascade where p1='xyz';
select * from alter_table_cascade where p1='abc';
select * from alter_table_cascade where p1='__HIVE_DEFAULT_PARTITION__';

-- Change c2 to decimal(10,0), the change should cascaded to all partitions
-- the c2 value returned should be in decimal(10,0)
alter table alter_table_cascade change c2 c2 decimal(10,0) comment "change datatype" cascade;
describe alter_table_cascade;
describe alter_table_cascade partition (p1='xyz', p2='123');
describe alter_table_cascade partition (p1='abc', p2='123');
describe alter_table_cascade partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_table_cascade where p1='xyz';
select * from alter_table_cascade where p1='abc';
select * from alter_table_cascade where p1='__HIVE_DEFAULT_PARTITION__';

-- rename c1 to c2fromc1 and move it to after c2, the change should cascaded to all partitions
alter table alter_table_cascade change c1 c2fromc1 string comment "change position after" after c2 cascade;
describe alter_table_cascade;
describe alter_table_cascade partition (p1='xyz', p2='123');
describe alter_table_cascade partition (p1='abc', p2='123');
describe alter_table_cascade partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');

-- rename c2fromc1 back to c1 and move to first as c1, the change should cascaded to all partitions
alter table alter_table_cascade change c2fromc1 c1 string comment "change position first" first cascade;
describe alter_table_cascade;
describe alter_table_cascade partition (p1='xyz', p2='123');
describe alter_table_cascade partition (p1='abc', p2='123');
describe alter_table_cascade partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');

-- Try out replace columns, the change should cascaded to all partitions
alter table alter_table_cascade replace columns (c1 string) cascade;
describe alter_table_cascade;
describe alter_table_cascade partition (p1='xyz', p2='123');
describe alter_table_cascade partition (p1='abc', p2='123');
describe alter_table_cascade partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_table_cascade where p1='xyz';
select * from alter_table_cascade where p1='abc';
select * from alter_table_cascade where p1='__HIVE_DEFAULT_PARTITION__';

-- Try add columns, the change should cascaded to all partitions
alter table alter_table_cascade add columns (c2 decimal(14,4)) cascade;
describe alter_table_cascade;
describe alter_table_cascade partition (p1='xyz', p2='123');
describe alter_table_cascade partition (p1='abc', p2='123');
describe alter_table_cascade partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_table_cascade where p1='xyz';
select * from alter_table_cascade where p1='abc';
select * from alter_table_cascade where p1='__HIVE_DEFAULT_PARTITION__';

--

drop table if exists alter_table_restrict;

create table alter_table_restrict (c1 string) partitioned by (p1 string, p2 string);
insert overwrite table alter_table_restrict partition (p1, p2)
  select c1, 'abc', '123' from alter_table_src
  union all
  select c1, null, '123' from alter_table_src;

show partitions alter_table_restrict;
describe alter_table_restrict;
describe alter_table_restrict partition (p1='abc', p2='123');
describe alter_table_restrict partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_table_restrict where p1='abc';
select * from alter_table_restrict where p1='__HIVE_DEFAULT_PARTITION__';

-- add columns c2 by replace columns (for HIVE-6131) without cascade
-- only table column definition has changed, partitions do not
-- after replace, only new partition xyz return the value to new added columns but not existing partitions abc and __HIVE_DEFAULT_PARTITION__
alter table alter_table_restrict replace columns (c1 string, c2 string) restrict;
load data local inpath '../../data/files/dec.txt' overwrite into table alter_table_restrict partition (p1='abc', p2='123');
load data local inpath '../../data/files/dec.txt' overwrite into table alter_table_restrict partition (p1='__HIVE_DEFAULT_PARTITION__',p2='123');
load data local inpath '../../data/files/dec.txt' overwrite into table alter_table_restrict partition (p1='xyz', p2='123');
describe alter_table_restrict;
describe alter_table_restrict partition (p1='xyz', p2='123');
describe alter_table_restrict partition (p1='abc', p2='123');
describe alter_table_restrict partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
select * from alter_table_restrict where p1='xyz';
select * from alter_table_restrict where p1='abc';
select * from alter_table_restrict where p1='__HIVE_DEFAULT_PARTITION__';

-- Change c2 to decimal(10,0), only limited to table and new partition
alter table alter_table_restrict change c2 c2 decimal(10,0) restrict;
describe alter_table_restrict;
describe alter_table_restrict partition (p1='xyz', p2='123');
describe alter_table_restrict partition (p1='abc', p2='123');
describe alter_table_restrict partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');

-- Try out replace columns, only limited to table and new partition
alter table alter_table_restrict replace columns (c1 string);
describe alter_table_restrict;
describe alter_table_restrict partition (p1='xyz', p2='123');
describe alter_table_restrict partition (p1='abc', p2='123');
describe alter_table_restrict partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');

-- Try add columns, only limited to table and new partition
alter table alter_table_restrict add columns (c2 decimal(14,4));
describe alter_table_restrict;
describe alter_table_restrict partition (p1='xyz', p2='123');
describe alter_table_restrict partition (p1='abc', p2='123');
describe alter_table_restrict partition (p1='__HIVE_DEFAULT_PARTITION__', p2='123');
set hive.mapred.mode=nonstrict;
set hive.metastore.try.direct.sql=true;

drop database if exists statsdb1;
create database statsdb1;
drop database if exists statsdb2;
create database statsdb2;

create table statsdb1.testtable1 (col1 int, col2 string, col3 string);
insert into statsdb1.testtable1 select key, value, 'val3' from src limit 10;

create table statsdb1.testpart1 (col1 int, col2 string, col3 string) partitioned by (part string);
insert into statsdb1.testpart1 partition (part = 'part1') select key, value, 'val3' from src limit 10;
insert into statsdb1.testpart1 partition (part = 'part2') select key, value, 'val3' from src limit 20;

use statsdb1;

analyze table testtable1 compute statistics for columns;

describe formatted statsdb1.testtable1 col1;
describe formatted statsdb1.testtable1 col2;
describe formatted statsdb1.testtable1 col3;

alter table testtable1 replace columns (col1 int, col2 string, col4 string);
describe formatted statsdb1.testtable1 col1;
describe formatted statsdb1.testtable1 col2;
describe formatted statsdb1.testtable1 col4;

alter table testtable1 change col1 col1 string;
describe formatted statsdb1.testtable1 col1;
describe formatted statsdb1.testtable1 col2;
describe formatted statsdb1.testtable1 col4;

alter table statsdb1.testtable1 rename to statsdb2.testtable2;


analyze table testpart1 compute statistics for columns;

describe formatted statsdb1.testpart1 partition (part = 'part1') col1;
describe formatted statsdb1.testpart1 partition (part = 'part1') col2;
describe formatted statsdb1.testpart1 partition (part = 'part1') col3;
describe formatted statsdb1.testpart1 partition (part = 'part2') col1;
describe formatted statsdb1.testpart1 partition (part = 'part2') col2;
describe formatted statsdb1.testpart1 partition (part = 'part2') col3;

alter table statsdb1.testpart1 partition (part = 'part2') rename to partition (part = 'part3');
describe formatted statsdb1.testpart1 partition (part = 'part1') col1;
describe formatted statsdb1.testpart1 partition (part = 'part1') col2;
describe formatted statsdb1.testpart1 partition (part = 'part1') col3;
describe formatted statsdb1.testpart1 partition (part = 'part3') col1;
describe formatted statsdb1.testpart1 partition (part = 'part3') col2;
describe formatted statsdb1.testpart1 partition (part = 'part3') col3;

alter table statsdb1.testpart1 replace columns (col1 int, col2 string, col4 string) cascade;
describe formatted statsdb1.testpart1 partition (part = 'part1') col1;
describe formatted statsdb1.testpart1 partition (part = 'part1') col2;
describe formatted statsdb1.testpart1 partition (part = 'part1') col4;

alter table statsdb1.testpart1 change column col1 col1 string;
set hive.exec.dynamic.partition = true;
alter table statsdb1.testpart1 partition (part) change column col1 col1 string;
describe formatted statsdb1.testpart1 partition (part = 'part1') col1;
describe formatted statsdb1.testpart1 partition (part = 'part1') col2;
describe formatted statsdb1.testpart1 partition (part = 'part1') col4;

alter table statsdb1.testpart1 rename to statsdb2.testpart2;
use statsdb2;

alter table statsdb2.testpart2 drop partition (part = 'part1');
drop table statsdb2.testpart2;

drop table statsdb2.testtable2;

use default;
drop database statsdb1;
drop database statsdb2;


set hive.metastore.try.direct.sql=false;

drop database if exists statsdb1;
create database statsdb1;
drop database if exists statsdb2;
create database statsdb2;

create table statsdb1.testtable1 (col1 int, col2 string, col3 string);
insert into statsdb1.testtable1 select key, value, 'val3' from src limit 10;

create table statsdb1.testpart1 (col1 int, col2 string, col3 string) partitioned by (part string);
insert into statsdb1.testpart1 partition (part = 'part1') select key, value, 'val3' from src limit 10;
insert into statsdb1.testpart1 partition (part = 'part2') select key, value, 'val3' from src limit 20;

use statsdb1;

analyze table testtable1 compute statistics for columns;

describe formatted statsdb1.testtable1 col1;
describe formatted statsdb1.testtable1 col2;
describe formatted statsdb1.testtable1 col3;

alter table testtable1 replace columns (col1 int, col2 string, col4 string);
describe formatted statsdb1.testtable1 col1;
describe formatted statsdb1.testtable1 col2;
describe formatted statsdb1.testtable1 col4;

alter table testtable1 change col1 col1 string;
describe formatted statsdb1.testtable1 col1;
describe formatted statsdb1.testtable1 col2;
describe formatted statsdb1.testtable1 col4;

alter table statsdb1.testtable1 rename to statsdb2.testtable2;


analyze table testpart1 compute statistics for columns;

describe formatted statsdb1.testpart1 partition (part = 'part1') col1;
describe formatted statsdb1.testpart1 partition (part = 'part1') col2;
describe formatted statsdb1.testpart1 partition (part = 'part1') col3;
describe formatted statsdb1.testpart1 partition (part = 'part2') col1;
describe formatted statsdb1.testpart1 partition (part = 'part2') col2;
describe formatted statsdb1.testpart1 partition (part = 'part2') col3;

alter table statsdb1.testpart1 partition (part = 'part2') rename to partition (part = 'part3');
describe formatted statsdb1.testpart1 partition (part = 'part1') col1;
describe formatted statsdb1.testpart1 partition (part = 'part1') col2;
describe formatted statsdb1.testpart1 partition (part = 'part1') col3;
describe formatted statsdb1.testpart1 partition (part = 'part3') col1;
describe formatted statsdb1.testpart1 partition (part = 'part3') col2;
describe formatted statsdb1.testpart1 partition (part = 'part3') col3;

alter table statsdb1.testpart1 replace columns (col1 int, col2 string, col4 string) cascade;
describe formatted statsdb1.testpart1 partition (part = 'part1') col1;
describe formatted statsdb1.testpart1 partition (part = 'part1') col2;
describe formatted statsdb1.testpart1 partition (part = 'part1') col4;

alter table statsdb1.testpart1 change column col1 col1 string;
set hive.exec.dynamic.partition = true;
alter table statsdb1.testpart1 partition (part) change column col1 col1 string;
describe formatted statsdb1.testpart1 partition (part = 'part1') col1;
describe formatted statsdb1.testpart1 partition (part = 'part1') col2;
describe formatted statsdb1.testpart1 partition (part = 'part1') col4;

alter table statsdb1.testpart1 rename to statsdb2.testpart2;
use statsdb2;

alter table statsdb2.testpart2 drop partition (part = 'part1');
drop table statsdb2.testpart2;

drop table statsdb2.testtable2;

use default;
drop database statsdb1;
drop database statsdb2;
drop table if exists hcat_altertable_16;
create table hcat_altertable_16(a int, b string) stored as textfile;
show table extended like  hcat_altertable_16;
alter table hcat_altertable_16 set location 'file:${system:test.tmp.dir}/hcat_altertable_16';
show table extended like  hcat_altertable_16;
create table alter_table_not_sorted (a int, b int) clustered by (a) sorted by (a) into 4 buckets;
desc formatted alter_table_not_sorted;

alter table alter_table_not_sorted not sorted;
desc formatted alter_table_not_sorted;

drop table alter_table_not_sorted;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition.mode=nonstrict;

DROP TABLE IF EXISTS part_table PURGE;
CREATE TABLE part_table (key INT, value STRING) partitioned by (p STRING);

INSERT INTO part_table PARTITION(p)(p,key,value) values('2014-09-23', 1, 'foo'),('2014-09-24', 2, 'bar');
SELECT * FROM part_table;
ALTER TABLE part_table DROP PARTITION (p='2014-09-23');
SELECT * FROM part_table;
ALTER TABLE part_table DROP PARTITION (p='2014-09-24') PURGE;
SELECT * FROM part_table;
-- test table
create table test_table (id int, query string, name string);
describe extended test_table;

alter table test_table set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
describe extended test_table;

alter table test_table set serdeproperties ('field.delim' = ',');
describe extended test_table;

drop table test_table;

--- test partitioned table
create table test_table (id int, query string, name string) partitioned by (dt string);

alter table test_table add partition (dt = '2011');
describe extended test_table partition (dt='2011');

alter table test_table set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
describe extended test_table partition (dt='2011');

alter table test_table set serdeproperties ('field.delim' = ',');
describe extended test_table partition (dt='2011');

-- test partitions

alter table test_table partition(dt='2011') set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
describe extended test_table partition (dt='2011');

alter table test_table partition(dt='2011') set serdeproperties ('field.delim' = ',');
describe extended test_table partition (dt='2011');

drop table test_table
-- Tests that when overwriting a partition in a table after altering the serde properties
-- the partition metadata is updated as well.

CREATE TABLE tst1(key STRING, value STRING) PARTITIONED BY (ds STRING);

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');

-- Test altering the serde properties

ALTER TABLE tst1 SET SERDEPROPERTIES ('field.delim' = ',');

DESCRIBE FORMATTED tst1;

INSERT OVERWRITE TABLE tst1 PARTITION (ds = '1') SELECT key, value FROM src;

DESCRIBE FORMATTED tst1 PARTITION (ds = '1');
create table src_stat as select * from src1;

create table src_stat_int (
  key         double,
  value       string
);

LOAD DATA LOCAL INPATH '../../data/files/kv3.txt' INTO TABLE src_stat_int;

ANALYZE TABLE src_stat COMPUTE STATISTICS for columns key;

describe formatted src_stat key;

ALTER TABLE src_stat UPDATE STATISTICS for column key SET ('numDVs'='1111','avgColLen'='1.111');

describe formatted src_stat key;

ALTER TABLE src_stat UPDATE STATISTICS for column value SET ('numDVs'='121','numNulls'='122','avgColLen'='1.23','maxColLen'='124');

describe formatted src_stat value;

ANALYZE TABLE src_stat_int COMPUTE STATISTICS for columns key;

describe formatted src_stat_int key;

ALTER TABLE src_stat_int UPDATE STATISTICS for column key SET ('numDVs'='2222','lowValue'='333.22','highValue'='22.22');

describe formatted src_stat_int key;



create database if not exists dummydb;

use dummydb;

ALTER TABLE default.src_stat UPDATE STATISTICS for column key SET ('numDVs'='3333','avgColLen'='2.222');

describe formatted default.src_stat key;

ALTER TABLE default.src_stat UPDATE STATISTICS for column value SET ('numDVs'='232','numNulls'='233','avgColLen'='2.34','maxColLen'='235');

describe formatted default.src_stat value;

use default;

drop database dummydb;
create table testwrongloc(id int);

-- Assume port 12345 is not open
alter table testwrongloc set location "hdfs://localhost:12345/tmp/testwrongloc";
drop table aa;
create table aa ( test STRING )
  ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
  WITH SERDEPROPERTIES ("input.regex" = "(.*)", "output.format.string" = "$1s");

alter table aa set serdeproperties ("input.regex" = "[^\\](.*)", "output.format.string" = "$1s");

-- SORT_QUERY_RESULTS

create database avc;

create table avc.alter_varchar_1 (key string, value string);
insert overwrite table avc.alter_varchar_1
  select key, value from src order by key limit 5;

select * from avc.alter_varchar_1;

-- change column to varchar
alter table avc.alter_varchar_1 change column value value varchar(20);
-- contents should still look the same
select * from avc.alter_varchar_1;

-- change column to smaller varchar
alter table avc.alter_varchar_1 change column value value varchar(3);
-- value column should be truncated now
select * from avc.alter_varchar_1;

-- change back to bigger varchar
alter table avc.alter_varchar_1 change column value value varchar(20);
-- column values should be full size again
select * from avc.alter_varchar_1;

-- add varchar column
alter table avc.alter_varchar_1 add columns (key2 int, value2 varchar(10));
select * from avc.alter_varchar_1;

insert overwrite table avc.alter_varchar_1
  select key, value, key, value from src order by key limit 5;
select * from avc.alter_varchar_1;

drop table avc.alter_varchar_1;
drop database avc;
set hive.mapred.mode=nonstrict;

-- alter column type, with partitioned table
drop table if exists alter_varchar2;

create table alter_varchar2 (
  c1 varchar(255)
) partitioned by (hr int);

insert overwrite table alter_varchar2 partition (hr=1)
  select value from src tablesample (1 rows);

select c1, length(c1) from alter_varchar2;

alter table alter_varchar2 change column c1 c1 varchar(10);

select hr, c1, length(c1) from alter_varchar2 where hr = 1;

insert overwrite table alter_varchar2 partition (hr=2)
  select key from src tablesample (1 rows);

set hive.fetch.task.conversion=more;

select hr, c1, length(c1) from alter_varchar2 where hr = 1;
select hr, c1, length(c1) from alter_varchar2 where hr = 2;
CREATE DATABASE tv;
CREATE VIEW tv.testView as SELECT * FROM srcpart;
DESCRIBE FORMATTED tv.testView;

ALTER VIEW tv.testView AS SELECT value FROM src WHERE key=86;
DESCRIBE FORMATTED tv.testView;

ALTER VIEW tv.testView AS
SELECT * FROM src
WHERE key > 80 AND key < 100
ORDER BY key, value
LIMIT 10;
DESCRIBE FORMATTED tv.testView;

DROP VIEW tv.testView;
DROP DATABASE tv;DROP VIEW testView;

-- Cannot ALTER VIEW AS SELECT if view currently does not exist
ALTER VIEW testView AS SELECT * FROM srcpart;
CREATE VIEW testViewPart PARTITIONED ON (value)
AS
SELECT key, value
FROM src
WHERE key=86;

ALTER VIEW testViewPart
ADD PARTITION (value='val_86') PARTITION (value='val_xyz');
DESCRIBE FORMATTED testViewPart;

-- If a view has partition, could not replace it with ALTER VIEW AS SELECT
ALTER VIEW testViewPart as SELECT * FROM srcpart;
DROP VIEW xxx3;
CREATE VIEW xxx3 AS SELECT * FROM src;
ALTER TABLE xxx3 REPLACE COLUMNS (xyz int);
DROP VIEW xxx4;
CREATE VIEW xxx4
PARTITIONED ON (value)
AS
SELECT * FROM src;

-- should fail:  need to use ALTER VIEW, not ALTER TABLE
ALTER TABLE xxx4 ADD PARTITION (value='val_86');
-- should fail:  can't use ALTER VIEW on a table
ALTER VIEW srcpart ADD PARTITION (ds='2012-12-31', hr='23');
DROP VIEW xxx5;
CREATE VIEW xxx5
PARTITIONED ON (value)
AS
SELECT * FROM src;

-- should fail:  LOCATION clause is illegal
ALTER VIEW xxx5 ADD PARTITION (value='val_86') LOCATION '/foo/bar/baz';
DROP VIEW xxx6;
CREATE VIEW xxx6
PARTITIONED ON (value)
AS
SELECT * FROM src;

-- should fail:  partition column name does not match
ALTER VIEW xxx6 ADD PARTITION (v='val_86');
DROP VIEW xxx7;
CREATE VIEW xxx7
PARTITIONED ON (key)
AS
SELECT hr,key FROM srcpart;

SET hive.mapred.mode=strict;

-- strict mode should cause this to fail since view partition
-- predicate does not correspond to an underlying table partition predicate
ALTER VIEW xxx7 ADD PARTITION (key=10);
DROP VIEW xxx8;
CREATE VIEW xxx8
PARTITIONED ON (ds,hr)
AS
SELECT key,ds,hr FROM srcpart;

-- should fail:  need to fill in all partition columns
ALTER VIEW xxx8 ADD PARTITION (ds='2011-01-01');
-- should fail:  can't use ALTER VIEW on a table
CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);
ALTER VIEW invites RENAME TO invites2;
DROP VIEW xxx4;
CREATE VIEW xxx4
AS
SELECT * FROM src;

-- should fail:  need to use ALTER VIEW, not ALTER TABLE
ALTER TABLE xxx4 RENAME TO xxx4a;
set hive.mapred.mode=nonstrict;
CREATE DATABASE tv1;
CREATE DATABASE tv2;

CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);
CREATE VIEW tv1.view1 as SELECT * FROM invites;
DESCRIBE EXTENDED tv1.view1;

ALTER VIEW tv1.view1 RENAME TO tv2.view2;
DESCRIBE EXTENDED tv2.view2;
SELECT * FROM tv2.view2;

DROP TABLE invites;
DROP VIEW tv2.view2;

DROP DATABASE tv1;
DROP DATABASE tv2;
-- check cluster/distribute/partitionBy
SELECT * FROM SRC x where x.key = 20 CLUSTER BY (key,value) ;

SELECT * FROM SRC x where x.key = 20 CLUSTER BY ((key),value) ;

SELECT * FROM SRC x where x.key = 20 CLUSTER BY (key,(value)) ;

SELECT * FROM SRC x where x.key = 20 CLUSTER BY ((key),(value)) ;

SELECT * FROM SRC x where x.key = 20 CLUSTER BY ((key),(((value))));

-- HIVE-6950
SELECT tab1.key,
       tab1.value,
       SUM(1)
FROM src as tab1
GROUP BY tab1.key,
         tab1.value
GROUPING SETS ((tab1.key, tab1.value));

SELECT key,
       src.value,
       SUM(1)
FROM src
GROUP BY key,
         src.value
GROUPING SETS ((key, src.value));

explain extended select int(1.2) from src limit 1;
select int(1.2) from src limit 1;
select bigint(1.34) from src limit 1;
select binary('1') from src limit 1;
select boolean(1) from src limit 1;
select date('1') from src limit 2;
select double(1) from src limit 1;
select float(1) from src limit 1;
select smallint(0.9) from src limit 1;
select timestamp('1') from src limit 2;

explain extended desc default.src key;

desc default.src key;
set hive.support.quoted.identifiers=none;
-- TOK_ALLCOLREF
explain select * from (select a.key, a.* from (select * from src) a join (select * from src1) b on (a.key = b.key)) t;
select * from (select a.key, a.* from (select * from src) a join (select * from src1) b on (a.key = b.key)) t;
-- DOT
explain select * from (select a.key, a.`[k].*` from (select * from src) a join (select * from src1) b on (a.key = b.key)) t;
select * from (select a.key, a.`[k].*` from (select * from src) a join (select * from src1) b on (a.key = b.key)) t;
-- EXPRESSION
explain select * from (select a.key, a.key from (select * from src) a join (select * from src1) b on (a.key = b.key)) t;
select * from (select a.key, a.key from (select * from src) a join (select * from src1) b on (a.key = b.key)) t;
FROM (SELECT key, concat(value) AS key FROM src) a SELECT a.key;
FROM src src1 JOIN src src2 ON src1.key = src2.key
INSERT OVERWRITE TABLE dest1 SELECT key
analyze table srcpart compute statistics;
analyze table srcpart partition (key) compute statistics;
analyze table nonexistent compute statistics;
SET hive.exec.dynamic.partition.mode=nonstrict;

DROP TABLE IF EXISTS test1;
DROP TABLE IF EXISTS test2;

CREATE TABLE test1(name string, age int);
CREATE TABLE test2(name string) PARTITIONED by (age int);

LOAD DATA LOCAL INPATH '../../data/files/test1.txt' INTO TABLE test1;
FROM test1 INSERT OVERWRITE TABLE test2 PARTITION(age) SELECT test1.name, test1.age;

ANALYZE TABLE test2 PARTITION(age) COMPUTE STATISTICS;

-- To show stats. It doesn't show due to a bug.
DESC EXTENDED test2;

-- Another way to show stats.
EXPLAIN EXTENDED select * from test2;

DROP TABLE test1;
DROP TABLE test2;
set hive.mapred.mode=nonstrict;
create table src_stat_part(key string, value string) partitioned by (partitionId int);

insert overwrite table src_stat_part partition (partitionId=1)
select * from src1;

insert overwrite table src_stat_part partition (partitionId=2)
select * from src1;

ANALYZE TABLE src_stat_part partition (partitionId) COMPUTE STATISTICS for columns key;

describe formatted src_stat_part PARTITION(partitionId=1) key;

ANALYZE TABLE src_stat_part partition (partitionId) COMPUTE STATISTICS for columns key, value;

describe formatted src_stat_part PARTITION(partitionId=1) key;

describe formatted src_stat_part PARTITION(partitionId=2) value;

create table src_stat_string_part(key string, value string) partitioned by (partitionName string);

insert overwrite table src_stat_string_part partition (partitionName="p'1")
select * from src1;

insert overwrite table src_stat_string_part partition (partitionName="p\"1")
select * from src1;

ANALYZE TABLE src_stat_string_part partition (partitionName="p'1") COMPUTE STATISTICS for columns key, value;

ANALYZE TABLE src_stat_string_part partition (partitionName="p\"1") COMPUTE STATISTICS for columns key, value;
DROP VIEW av;

CREATE VIEW av AS SELECT * FROM src;

-- should fail:  can't analyze a view...yet
ANALYZE TABLE av COMPUTE STATISTICS;
create table over1k(
t tinyint,
si smallint,
i int,
b bigint,
f float,
d double,
bo boolean,
s string,
ts timestamp,
dec decimal(4,2),
bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

load data local inpath '../../data/files/over1k' overwrite into table over1k;
load data local inpath '../../data/files/over1k' into table over1k;

analyze table over1k compute statistics;
analyze table over1k compute statistics for columns;

set hive.stats.fetch.column.stats=true;
set hive.optimize.point.lookup=false;
explain select count(*) from over1k where (
(t=1 and si=2)
or (t=2 and si=3)
or (t=3 and si=4)
or (t=4 and si=5)
or (t=5 and si=6)
or (t=6 and si=7)
or (t=7 and si=8)
or (t=9 and si=10)
or (t=10 and si=11)
or (t=11 and si=12)
or (t=12 and si=13)
or (t=13 and si=14)
or (t=14 and si=15)
or (t=15 and si=16)
or (t=16 and si=17)
or (t=17 and si=18)
or (t=27 and si=28)
or (t=37 and si=38)
or (t=47 and si=48)
or (t=52 and si=53));

set hive.stats.fetch.column.stats=false;
explain select count(*) from over1k where (
(t=1 and si=2)
or (t=2 and si=3)
or (t=3 and si=4)
or (t=4 and si=5)
or (t=5 and si=6)
or (t=6 and si=7)
or (t=7 and si=8)
or (t=9 and si=10)
or (t=10 and si=11)
or (t=11 and si=12)
or (t=12 and si=13)
or (t=13 and si=14)
or (t=14 and si=15)
or (t=15 and si=16)
or (t=16 and si=17)
or (t=17 and si=18)
or (t=27 and si=28)
or (t=37 and si=38)
or (t=47 and si=48)
or (t=52 and si=53));
set hive.mapred.mode=nonstrict;
set hive.stats.fetch.column.stats=true;

create table if not exists loc_staging (
  state string,
  locid int,
  zip bigint,
  year int
) row format delimited fields terminated by '|' stored as textfile;

create table loc_orc like loc_staging;
alter table loc_orc set fileformat orc;

load data local inpath '../../data/files/loc.txt' overwrite into table loc_staging;

insert overwrite table loc_orc select * from loc_staging;

-- numRows: 8 rawDataSize: 796
explain select * from loc_orc;

-- column stats are not COMPLETE, so stats are not updated
-- numRows: 8 rawDataSize: 796
explain select * from loc_orc where state='OH';

analyze table loc_orc compute statistics for columns state,locid,zip,year;

-- state column has 5 distincts. numRows/countDistincts
-- numRows: 1 rawDataSize: 102
explain select * from loc_orc where state='OH';

-- not equals comparison shouldn't affect number of rows
-- numRows: 8 rawDataSize: 804
explain select * from loc_orc where state!='OH';
explain select * from loc_orc where state<>'OH';

-- nulls are treated as constant equality comparison
-- numRows: 1 rawDataSize: 102
explain select * from loc_orc where zip is null;
-- numRows: 1 rawDataSize: 102
explain select * from loc_orc where !(zip is not null);

-- not nulls are treated as inverse of nulls
-- numRows: 7 rawDataSize: 702
explain select * from loc_orc where zip is not null;
-- numRows: 7 rawDataSize: 702
explain select * from loc_orc where !(zip is null);

-- NOT evaluation. true will pass all rows, false will not pass any rows
-- numRows: 8 rawDataSize: 804
explain select * from loc_orc where !false;
-- numRows: 0 rawDataSize: 0
explain select * from loc_orc where !true;

-- Constant evaluation. true will pass all rows, false will not pass any rows
-- numRows: 8 rawDataSize: 804
explain select * from loc_orc where true;
-- numRows: 8 rawDataSize: 804
explain select * from loc_orc where 'foo';
-- numRows: 8 rawDataSize: 804
explain select * from loc_orc where true = true;
-- numRows: 0 rawDataSize: 0
explain select * from loc_orc where false = true;
-- numRows: 0 rawDataSize: 0
explain select * from loc_orc where 'foo' = 'bar';
-- numRows: 0 rawDataSize: 0
explain select * from loc_orc where false;

-- OR evaluation. 1 row for OH and 1 row for CA
-- numRows: 2 rawDataSize: 204
explain select * from loc_orc where state='OH' or state='CA';

-- AND evaluation. cascadingly apply rules. 8/2 = 4/2 = 2
-- numRows: 2 rawDataSize: 204
explain select * from loc_orc where year=2001 and year is null;
-- numRows: 1 rawDataSize: 102
explain select * from loc_orc where year=2001 and state='OH' and state='FL';

-- AND and OR together. left expr will yield 1 row and right will yield 1 row
-- numRows: 3 rawDataSize: 306
explain select * from loc_orc where (year=2001 and year is null) or (state='CA');

-- AND and OR together. left expr will yield 8 rows and right will yield 1 row
-- numRows: 1 rawDataSize: 102
explain select * from loc_orc where (year=2001 or year is null) and (state='CA');

-- all inequality conditions rows/3 is the rules
-- numRows: 2 rawDataSize: 204
explain select * from loc_orc where locid < 30;
explain select * from loc_orc where locid > 30;
explain select * from loc_orc where locid <= 30;
explain select * from loc_orc where locid >= 30;
set hive.stats.fetch.column.stats=true;
set hive.map.aggr.hash.percentmemory=0.0f;

-- hash aggregation is disabled

-- There are different cases for Group By depending on map/reduce side, hash aggregation,
-- grouping sets and column stats. If we don't have column stats, we just assume hash
-- aggregation is disabled. Following are the possible cases and rule for cardinality
-- estimation

-- MAP SIDE:
-- Case 1: NO column stats, NO hash aggregation, NO grouping sets — numRows
-- Case 2: NO column stats, NO hash aggregation, grouping sets — numRows * sizeOfGroupingSet
-- Case 3: column stats, hash aggregation, NO grouping sets — Min(numRows / 2, ndvProduct * parallelism)
-- Case 4: column stats, hash aggregation, grouping sets — Min((numRows * sizeOfGroupingSet) / 2, ndvProduct * parallelism * sizeOfGroupingSet)
-- Case 5: column stats, NO hash aggregation, NO grouping sets — numRows
-- Case 6: column stats, NO hash aggregation, grouping sets — numRows * sizeOfGroupingSet

-- REDUCE SIDE:
-- Case 7: NO column stats — numRows / 2
-- Case 8: column stats, grouping sets — Min(numRows, ndvProduct * sizeOfGroupingSet)
-- Case 9: column stats, NO grouping sets - Min(numRows, ndvProduct)

create table if not exists loc_staging (
  state string,
  locid int,
  zip bigint,
  year int
) row format delimited fields terminated by '|' stored as textfile;

create table loc_orc like loc_staging;
alter table loc_orc set fileformat orc;

load data local inpath '../../data/files/loc.txt' overwrite into table loc_staging;

insert overwrite table loc_orc select * from loc_staging;

-- numRows: 8 rawDataSize: 796
explain select * from loc_orc;

-- partial column stats
analyze table loc_orc compute statistics for columns state;

-- inner group by: map - numRows: 8 reduce - numRows: 4
-- outer group by: map - numRows: 4 reduce numRows: 2
explain select a, c, min(b)
from ( select state as a, locid as b, count(*) as c
       from loc_orc
       group by state,locid
     ) sq1
group by a,c;

analyze table loc_orc compute statistics for columns state,locid,year;

-- Case 5: column stats, NO hash aggregation, NO grouping sets - cardinality = 8
-- Case 9: column stats, NO grouping sets - caridnality = 2
explain select year from loc_orc group by year;

-- Case 5: column stats, NO hash aggregation, NO grouping sets - cardinality = 8
-- Case 9: column stats, NO grouping sets - caridnality = 8
explain select state,locid from loc_orc group by state,locid;

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 32
-- Case 8: column stats, grouping sets - cardinality = 32
explain select state,locid from loc_orc group by state,locid with cube;

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 24
-- Case 8: column stats, grouping sets - cardinality = 24
explain select state,locid from loc_orc group by state,locid with rollup;

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 8
-- Case 8: column stats, grouping sets - cardinality = 8
explain select state,locid from loc_orc group by state,locid grouping sets((state));

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 16
-- Case 8: column stats, grouping sets - cardinality = 16
explain select state,locid from loc_orc group by state,locid grouping sets((state),(locid));

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 24
-- Case 8: column stats, grouping sets - cardinality = 24
explain select state,locid from loc_orc group by state,locid grouping sets((state),(locid),());

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 32
-- Case 8: column stats, grouping sets - cardinality = 32
explain select state,locid from loc_orc group by state,locid grouping sets((state,locid),(state),(locid),());

set hive.map.aggr.hash.percentmemory=0.5f;
set mapred.max.split.size=80;
-- map-side parallelism will be 10

-- Case 3: column stats, hash aggregation, NO grouping sets - cardinality = 4
-- Case 9: column stats, NO grouping sets - caridnality = 2
explain select year from loc_orc group by year;

-- Case 4: column stats, hash aggregation, grouping sets - cardinality = 16
-- Case 8: column stats, grouping sets - cardinality = 16
explain select state,locid from loc_orc group by state,locid with cube;

-- ndvProduct becomes 0 as zip does not have column stats
-- Case 3: column stats, hash aggregation, NO grouping sets - cardinality = 4
-- Case 9: column stats, NO grouping sets - caridnality = 2
explain select state,zip from loc_orc group by state,zip;

set mapred.max.split.size=1000;
set hive.stats.fetch.column.stats=false;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 32
-- Case 7: NO column stats - cardinality = 16
explain select state,locid from loc_orc group by state,locid with cube;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 24
-- Case 7: NO column stats - cardinality = 12
explain select state,locid from loc_orc group by state,locid with rollup;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 8
-- Case 7: NO column stats - cardinality = 4
explain select state,locid from loc_orc group by state,locid grouping sets((state));

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 16
-- Case 7: NO column stats - cardinality = 8
explain select state,locid from loc_orc group by state,locid grouping sets((state),(locid));

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 24
-- Case 7: NO column stats - cardinality = 12
explain select state,locid from loc_orc group by state,locid grouping sets((state),(locid),());

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 32
-- Case 7: NO column stats - cardinality = 16
explain select state,locid from loc_orc group by state,locid grouping sets((state,locid),(state),(locid),());

set mapred.max.split.size=80;

-- Case 1: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 8
-- Case 7: NO column stats - cardinality = 4
explain select year from loc_orc group by year;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 32
-- Case 7: NO column stats - cardinality = 16
explain select state,locid from loc_orc group by state,locid with cube;

drop table location;

-- There are different cases for Group By depending on map/reduce side, hash aggregation,
-- grouping sets and column stats. If we don't have column stats, we just assume hash
-- aggregation is disabled. Following are the possible cases and rule for cardinality
-- estimation

-- MAP SIDE:
-- Case 1: NO column stats, NO hash aggregation, NO grouping sets — numRows
-- Case 2: NO column stats, NO hash aggregation, grouping sets — numRows * sizeOfGroupingSet
-- Case 3: column stats, hash aggregation, NO grouping sets — Min(numRows / 2, ndvProduct * parallelism)
-- Case 4: column stats, hash aggregation, grouping sets — Min((numRows * sizeOfGroupingSet) / 2, ndvProduct * parallelism * sizeOfGroupingSet)
-- Case 5: column stats, NO hash aggregation, NO grouping sets — numRows
-- Case 6: column stats, NO hash aggregation, grouping sets — numRows * sizeOfGroupingSet

-- REDUCE SIDE:
-- Case 7: NO column stats — numRows / 2
-- Case 8: column stats, grouping sets — Min(numRows, ndvProduct * sizeOfGroupingSet)
-- Case 9: column stats, NO grouping sets - Min(numRows, ndvProduct)

create table location (state string, country string, votes bigint);
load data local inpath "../../data/files/location.txt" overwrite into table location;

analyze table location compute statistics;
analyze table location compute statistics for columns state, country;

set mapred.max.split.size=50;
set hive.map.aggr.hash.percentmemory=0.5f;
set hive.stats.fetch.column.stats=false;

-- Case 1: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 20
-- Case 7: NO column stats - cardinality = 10
explain select state, country from location group by state, country;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 80
-- Case 7: NO column stats - cardinality = 40
explain select state, country from location group by state, country with cube;

set hive.stats.fetch.column.stats=true;
-- parallelism = 4

-- Case 3: column stats, hash aggregation, NO grouping sets - cardinality = 8
-- Case 9: column stats, NO grouping sets - caridnality = 2
explain select state, country from location group by state, country;

-- column stats for votes is missing, so ndvProduct becomes 0 and will be set to numRows / 2
-- Case 3: column stats, hash aggregation, NO grouping sets - cardinality = 10
-- Case 9: column stats, NO grouping sets - caridnality = 5
explain select state, votes from location group by state, votes;

-- Case 4: column stats, hash aggregation, grouping sets - cardinality = 32
-- Case 8: column stats, grouping sets - cardinality = 8
explain select state, country from location group by state, country with cube;

set hive.map.aggr.hash.percentmemory=0.0f;
-- Case 5: column stats, NO hash aggregation, NO grouping sets - cardinality = 20
-- Case 9: column stats, NO grouping sets - caridnality = 2
explain select state, country from location group by state, country;

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 80
-- Case 8: column stats, grouping sets - cardinality = 8
explain select state, country from location group by state, country with cube;

drop table location;
set hive.stats.fetch.column.stats=true;
set hive.stats.ndv.error=0.0;

create table if not exists emp (
  lastname string,
  deptid int,
  locid int
) row format delimited fields terminated by '|' stored as textfile;

create table if not exists dept (
  deptid int,
  deptname string
) row format delimited fields terminated by '|' stored as textfile;

create table if not exists loc (
  state string,
  locid int,
  zip bigint,
  year int
) row format delimited fields terminated by '|' stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/emp.txt' OVERWRITE INTO TABLE emp;
LOAD DATA LOCAL INPATH '../../data/files/dept.txt' OVERWRITE INTO TABLE dept;
LOAD DATA LOCAL INPATH '../../data/files/loc.txt' OVERWRITE INTO TABLE loc;

analyze table emp compute statistics;
analyze table dept compute statistics;
analyze table loc compute statistics;
analyze table emp compute statistics for columns lastname,deptid,locid;
analyze table dept compute statistics for columns deptname,deptid;
analyze table loc compute statistics for columns state,locid,zip,year;

-- number of rows
-- emp  - 48
-- dept - 6
-- loc  - 8

-- count distincts for relevant columns (since count distinct values are approximate in some cases count distint values will be greater than number of rows)
-- emp.deptid - 3
-- emp.lastname - 6
-- emp.locid - 7
-- dept.deptid - 7
-- dept.deptname - 6
-- loc.locid - 7
-- loc.state - 6

-- 2 relations, 1 attribute
-- Expected output rows: (48*6)/max(3,7) = 41
explain select * from emp e join dept d on (e.deptid = d.deptid);

-- 2 relations, 2 attributes
-- Expected output rows: (48*6)/(max(3,7) * max(6,6)) = 6
explain select * from emp,dept where emp.deptid = dept.deptid and emp.lastname = dept.deptname;
explain select * from emp e join dept d on (e.deptid = d.deptid and e.lastname = d.deptname);

-- 2 relations, 3 attributes
-- Expected output rows: (48*6)/(max(3,7) * max(6,6) * max(6,6)) = 1
explain select * from emp,dept where emp.deptid = dept.deptid and emp.lastname = dept.deptname and dept.deptname = emp.lastname;

-- 3 relations, 1 attribute
-- Expected output rows: (48*6*48)/top2largest(3,7,3) = 658
explain select * from emp e join dept d on (e.deptid = d.deptid) join emp e1 on (e.deptid = e1.deptid);

-- Expected output rows: (48*6*8)/top2largest(3,7,7) = 47
explain select * from emp e join dept d  on (e.deptid = d.deptid) join loc l on (e.deptid = l.locid);

-- 3 relations and 2 attribute
-- Expected output rows: (48*6*8)/top2largest(3,7,7)*top2largest(6,6,6) = 1
explain select * from emp e join dept d on (e.deptid = d.deptid and e.lastname = d.deptname) join loc l on (e.deptid = l.locid and e.lastname = l.state);

set hive.stats.fetch.column.stats=true;

drop table store_sales;
drop table store;
drop table customer_address;

-- s_store_sk is PK, ss_store_sk is FK
-- ca_address_sk is PK, ss_addr_sk is FK

create table store_sales
(
    ss_sold_date_sk           int,
    ss_sold_time_sk           int,
    ss_item_sk                int,
    ss_customer_sk            int,
    ss_cdemo_sk               int,
    ss_hdemo_sk               int,
    ss_addr_sk                int,
    ss_store_sk               int,
    ss_promo_sk               int,
    ss_ticket_number          int,
    ss_quantity               int,
    ss_wholesale_cost         float,
    ss_list_price             float,
    ss_sales_price            float,
    ss_ext_discount_amt       float,
    ss_ext_sales_price        float,
    ss_ext_wholesale_cost     float,
    ss_ext_list_price         float,
    ss_ext_tax                float,
    ss_coupon_amt             float,
    ss_net_paid               float,
    ss_net_paid_inc_tax       float,
    ss_net_profit             float
)
row format delimited fields terminated by '|';

create table store
(
    s_store_sk                int,
    s_store_id                string,
    s_rec_start_date          string,
    s_rec_end_date            string,
    s_closed_date_sk          int,
    s_store_name              string,
    s_number_employees        int,
    s_floor_space             int,
    s_hours                   string,
    s_manager                 string,
    s_market_id               int,
    s_geography_class         string,
    s_market_desc             string,
    s_market_manager          string,
    s_division_id             int,
    s_division_name           string,
    s_company_id              int,
    s_company_name            string,
    s_street_number           string,
    s_street_name             string,
    s_street_type             string,
    s_suite_number            string,
    s_city                    string,
    s_county                  string,
    s_state                   string,
    s_zip                     string,
    s_country                 string,
    s_gmt_offset              float,
    s_tax_precentage          float
)
row format delimited fields terminated by '|';

create table customer_address
(
    ca_address_sk             int,
    ca_address_id             string,
    ca_street_number          string,
    ca_street_name            string,
    ca_street_type            string,
    ca_suite_number           string,
    ca_city                   string,
    ca_county                 string,
    ca_state                  string,
    ca_zip                    string,
    ca_country                string,
    ca_gmt_offset             float,
    ca_location_type          string
)
row format delimited fields terminated by '|';

load data local inpath '../../data/files/store.txt' overwrite into table store;
load data local inpath '../../data/files/store_sales.txt' overwrite into table store_sales;
load data local inpath '../../data/files/customer_address.txt' overwrite into table customer_address;

analyze table store compute statistics;
analyze table store compute statistics for columns s_store_sk, s_floor_space;
analyze table store_sales compute statistics;
analyze table store_sales compute statistics for columns ss_store_sk, ss_addr_sk, ss_quantity;
analyze table customer_address compute statistics;
analyze table customer_address compute statistics for columns ca_address_sk;

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk);

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) where s.s_store_sk > 0;

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) where s.s_company_id > 0 and ss.ss_quantity > 10;

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) where s.s_floor_space > 0;

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) where ss.ss_quantity > 10;

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) join store s1 on (s1.s_store_sk = ss.ss_store_sk);

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) join store s1 on (s1.s_store_sk = ss.ss_store_sk) where s.s_store_sk > 1000;

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) join store s1 on (s1.s_store_sk = ss.ss_store_sk) where s.s_floor_space > 1000;

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) join store s1 on (s1.s_store_sk = ss.ss_store_sk) where ss.ss_quantity > 10;

explain select s.s_store_sk from store s join store_sales ss on (s.s_store_sk = ss.ss_store_sk) join customer_address ca on (ca.ca_address_sk = ss.ss_addr_sk);

drop table store_sales;
drop table store;
drop table customer_address;
set hive.stats.fetch.column.stats=true;

create table if not exists loc_staging (
  state string,
  locid int,
  zip bigint,
  year int
) row format delimited fields terminated by '|' stored as textfile;

create table loc_orc like loc_staging;
alter table loc_orc set fileformat orc;

load data local inpath '../../data/files/loc.txt' overwrite into table loc_staging;

insert overwrite table loc_orc select * from loc_staging;

analyze table loc_orc compute statistics for columns state, locid, zip, year;

-- numRows: 8 rawDataSize: 796
explain select * from loc_orc;

-- numRows: 4 rawDataSize: 396
explain select * from loc_orc limit 4;

-- greater than the available number of rows
-- numRows: 8 rawDataSize: 796
explain select * from loc_orc limit 16;

-- numRows: 0 rawDataSize: 0
explain select * from loc_orc limit 0;
set hive.mapred.mode=nonstrict;
set hive.stats.fetch.column.stats=true;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.metastore.aggregate.stats.cache.enabled=false;

create table if not exists loc_staging (
  state string,
  locid int,
  zip bigint,
  year string
) row format delimited fields terminated by '|' stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/loc.txt' OVERWRITE INTO TABLE loc_staging;

create table if not exists loc_orc (
  state string,
  locid int,
  zip bigint
) partitioned by(year string) stored as orc;

-- basicStatState: NONE colStatState: NONE
explain select * from loc_orc;

insert overwrite table loc_orc partition(year) select * from loc_staging;

-- stats are disabled. basic stats will report the file size but not raw data size. so initial statistics will be PARTIAL

-- basicStatState: PARTIAL colStatState: NONE
explain select * from loc_orc;

-- partition level analyze statistics for specific parition
analyze table loc_orc partition(year='2001') compute statistics;

-- basicStatState: PARTIAL colStatState: NONE
explain select * from loc_orc where year='__HIVE_DEFAULT_PARTITION__';

-- basicStatState: PARTIAL colStatState: NONE
explain select * from loc_orc;

-- basicStatState: COMPLETE colStatState: NONE
explain select * from loc_orc where year='2001';

-- partition level analyze statistics for all partitions
analyze table loc_orc partition(year) compute statistics;

-- basicStatState: COMPLETE colStatState: NONE
explain select * from loc_orc where year='__HIVE_DEFAULT_PARTITION__';

-- basicStatState: COMPLETE colStatState: NONE
explain select * from loc_orc;

-- basicStatState: COMPLETE colStatState: NONE
explain select * from loc_orc where year='2001' or year='__HIVE_DEFAULT_PARTITION__';

-- both partitions will be pruned
-- basicStatState: NONE colStatState: NONE
explain select * from loc_orc where year='2001' and year='__HIVE_DEFAULT_PARTITION__';

-- partition level partial column statistics
analyze table loc_orc partition(year='2001') compute statistics for columns state,locid;

-- basicStatState: COMPLETE colStatState: NONE
explain select zip from loc_orc;

-- basicStatState: COMPLETE colStatState: PARTIAL
explain select state from loc_orc;

-- basicStatState: COMPLETE colStatState: COMPLETE
explain select year from loc_orc;

-- column statistics for __HIVE_DEFAULT_PARTITION__ is not supported yet. Hence colStatState reports PARTIAL
-- basicStatState: COMPLETE colStatState: PARTIAL
explain select state,locid from loc_orc;

-- basicStatState: COMPLETE colStatState: COMPLETE
explain select state,locid from loc_orc where year='2001';

-- basicStatState: COMPLETE colStatState: NONE
explain select state,locid from loc_orc where year!='2001';

-- basicStatState: COMPLETE colStatState: PARTIAL
explain select * from loc_orc;

-- This is to test filter expression evaluation on partition column
-- numRows: 2 dataSize: 8 basicStatState: COMPLETE colStatState: COMPLETE
explain select locid from loc_orc where locid>0 and year='2001';
explain select locid,year from loc_orc where locid>0 and year='2001';
explain select * from (select locid,year from loc_orc) test where locid>0 and year='2001';
set hive.mapred.mode=nonstrict;
set hive.stats.fetch.column.stats=true;

create table if not exists alltypes (
 bo1 boolean,
 ti1 tinyint,
 si1 smallint,
 i1 int,
 bi1 bigint,
 f1 float,
 d1 double,
 de1 decimal,
 ts1 timestamp,
 da1 timestamp,
 s1 string,
 vc1 varchar(5),
 m1 map<string, string>,
 l1 array<int>,
 st1 struct<c1:int, c2:string>
) row format delimited fields terminated by '|'
collection items terminated by ','
map keys terminated by ':' stored as textfile;

create table alltypes_orc like alltypes;
alter table alltypes_orc set fileformat orc;

load data local inpath '../../data/files/alltypes.txt' overwrite into table alltypes;

insert overwrite table alltypes_orc select * from alltypes;

-- basicStatState: COMPLETE colStatState: NONE numRows: 2 rawDataSize: 1514
explain select * from alltypes_orc;

-- statistics for complex types are not supported yet
analyze table alltypes_orc compute statistics for columns bo1, ti1, si1, i1, bi1, f1, d1, s1, vc1;

-- numRows: 2 rawDataSize: 1514
explain select * from alltypes_orc;

-- numRows: 2 rawDataSize: 8
explain select bo1 from alltypes_orc;

-- col alias renaming
-- numRows: 2 rawDataSize: 8
explain select i1 as int1 from alltypes_orc;

-- numRows: 2 rawDataSize: 174
explain select s1 from alltypes_orc;

-- column statistics for complex types unsupported and so statistics will not be updated
-- numRows: 2 rawDataSize: 1514
explain select m1 from alltypes_orc;

-- numRows: 2 rawDataSize: 246
explain select bo1, ti1, si1, i1, bi1, f1, d1,s1 from alltypes_orc;

-- numRows: 2 rawDataSize: 0
explain select null from alltypes_orc;

-- numRows: 2 rawDataSize: 8
explain select 11 from alltypes_orc;

-- numRows: 2 rawDataSize: 16
explain select 11L from alltypes_orc;

-- numRows: 2 rawDataSize: 16
explain select 11.0 from alltypes_orc;

-- numRows: 2 rawDataSize: 178
explain select "hello" from alltypes_orc;
explain select cast("hello" as char(5)) from alltypes_orc;
explain select cast("hello" as varchar(5)) from alltypes_orc;

-- numRows: 2 rawDataSize: 96
explain select unbase64("0xe23") from alltypes_orc;

-- numRows: 2 rawDataSize: 16
explain select cast("1" as TINYINT), cast("20" as SMALLINT) from alltypes_orc;

-- numRows: 2 rawDataSize: 80
explain select cast("1970-12-31 15:59:58.174" as TIMESTAMP) from alltypes_orc;

-- numRows: 2 rawDataSize: 112
explain select cast("1970-12-31 15:59:58.174" as DATE) from alltypes_orc;

-- numRows: 2 rawDataSize: 224
explain select cast("58.174" as DECIMAL) from alltypes_orc;

-- numRows: 2 rawDataSize: 112
explain select array(1,2,3) from alltypes_orc;

-- numRows: 2 rawDataSize: 1508
explain select str_to_map("a=1 b=2 c=3", " ", "=") from alltypes_orc;

-- numRows: 2 rawDataSize: 112
explain select NAMED_STRUCT("a", 11, "b", 11) from alltypes_orc;

-- numRows: 2 rawDataSize: 250
explain select CREATE_UNION(0, "hello") from alltypes_orc;

-- COUNT(*) is projected as new column. It is not projected as GenericUDF and so datasize estimate will be based on number of rows
-- numRows: 1 rawDataSize: 8
explain select count(*) from alltypes_orc;

-- COUNT(1) is projected as new column. It is not projected as GenericUDF and so datasize estimate will be based on number of rows
-- numRows: 1 rawDataSize: 8
explain select count(1) from alltypes_orc;

-- column statistics for complex column types will be missing. data size will be calculated from available column statistics
-- numRows: 2 rawDataSize: 254
explain select *,11 from alltypes_orc;

-- subquery selects
-- inner select - numRows: 2 rawDataSize: 8
-- outer select - numRows: 2 rawDataSize: 8
explain select i1 from (select i1 from alltypes_orc limit 10) temp;

-- inner select - numRows: 2 rawDataSize: 16
-- outer select - numRows: 2 rawDataSize: 8
explain select i1 from (select i1,11 from alltypes_orc limit 10) temp;

-- inner select - numRows: 2 rawDataSize: 16
-- outer select - numRows: 2 rawDataSize: 186
explain select i1,"hello" from (select i1,11 from alltypes_orc limit 10) temp;

-- inner select - numRows: 2 rawDataSize: 24
-- outer select - numRows: 2 rawDataSize: 16
explain select x from (select i1,11.0 as x from alltypes_orc limit 10) temp;

-- inner select - numRows: 2 rawDataSize: 104
-- outer select - numRows: 2 rawDataSize: 186
explain select x,"hello" from (select i1 as x, unbase64("0xe23") as ub from alltypes_orc limit 10) temp;

-- inner select -  numRows: 2 rawDataSize: 186
-- middle select - numRows: 2 rawDataSize: 178
-- outer select -  numRows: 2 rawDataSize: 194
explain select h, 11.0 from (select hell as h from (select i1, "hello" as hell from alltypes_orc limit 10) in1 limit 10) in2;

-- This test is for FILTER operator where filter expression is a boolean column
-- numRows: 2 rawDataSize: 8
explain select bo1 from alltypes_orc where bo1;

-- numRows: 0 rawDataSize: 0
explain select bo1 from alltypes_orc where !bo1;
set hive.stats.fetch.column.stats=true;
set hive.stats.autogather=false;

create table if not exists emp_staging (
  lastname string,
  deptid int
) row format delimited fields terminated by '|' stored as textfile;

create table if not exists emp_orc like emp_staging;
alter table emp_orc set fileformat orc;

-- basicStatState: NONE colStatState: NONE
explain select * from emp_orc;

LOAD DATA LOCAL INPATH '../../data/files/emp.txt' OVERWRITE INTO TABLE emp_staging;

insert overwrite table emp_orc select * from emp_staging;

-- stats are disabled. basic stats will report the file size but not raw data size. so initial statistics will be PARTIAL

-- basicStatState: PARTIAL colStatState: NONE
explain select * from emp_orc;

-- table level analyze statistics
analyze table emp_orc compute statistics;

-- basicStatState: COMPLETE colStatState: NONE
explain select * from emp_orc;

-- column level partial statistics
analyze table emp_orc compute statistics for columns deptid;

-- basicStatState: COMPLETE colStatState: PARTIAL
explain select * from emp_orc;

-- all selected columns have statistics
-- basicStatState: COMPLETE colStatState: COMPLETE
explain select deptid from emp_orc;

-- column level complete statistics
analyze table emp_orc compute statistics for columns lastname,deptid;

-- basicStatState: COMPLETE colStatState: COMPLETE
explain select * from emp_orc;

-- basicStatState: COMPLETE colStatState: COMPLETE
explain select lastname from emp_orc;

-- basicStatState: COMPLETE colStatState: COMPLETE
explain select deptid from emp_orc;

-- basicStatState: COMPLETE colStatState: COMPLETE
explain select lastname,deptid from emp_orc;

create table tmp as select 1;
explain create table tmp as select 1;
set hive.stats.fetch.column.stats=true;

create table if not exists loc_staging (
  state string,
  locid int,
  zip bigint,
  year int
) row format delimited fields terminated by '|' stored as textfile;

create table loc_orc like loc_staging;
alter table loc_orc set fileformat orc;

load data local inpath '../../data/files/loc.txt' overwrite into table loc_staging;

insert overwrite table loc_orc select * from loc_staging;

analyze table loc_orc compute statistics for columns state,locid,zip,year;

-- numRows: 8 rawDataSize: 688
explain select state from loc_orc;

-- numRows: 16 rawDataSize: 1376
explain select * from (select state from loc_orc union all select state from loc_orc) tmp;

-- numRows: 8 rawDataSize: 796
explain select * from loc_orc;

-- numRows: 16 rawDataSize: 1592
explain select * from (select * from loc_orc union all select * from loc_orc) tmp;

create database test;
use test;
create table if not exists loc_staging (
  state string,
  locid int,
  zip bigint,
  year int
) row format delimited fields terminated by '|' stored as textfile;

create table loc_orc like loc_staging;
alter table loc_orc set fileformat orc;

load data local inpath '../../data/files/loc.txt' overwrite into table loc_staging;

insert overwrite table loc_orc select * from loc_staging;

analyze table loc_staging compute statistics;
analyze table loc_staging compute statistics for columns state,locid,zip,year;
analyze table loc_orc compute statistics for columns state,locid,zip,year;

-- numRows: 16 rawDataSize: 1376
explain select * from (select state from default.loc_orc union all select state from test.loc_orc) temp;

-- numRows: 16 rawDataSize: 1376
explain select * from (select state from test.loc_staging union all select state from test.loc_orc) temp;
set hive.mapred.mode=nonstrict;

set hive.compat=latest;

-- With ansi sql arithmetic enabled, int / int => exact numeric type
explain select cast(key as int) / cast(key as int) from src limit 1;
select cast(key as int) / cast(key as int) from src limit 1;


set hive.compat=0.12;

-- With ansi sql arithmetic disabled, int / int => double
explain select cast(key as int) / cast(key as int) from src limit 1;
select cast(key as int) / cast(key as int) from src limit 1;
set hive.archive.enabled = true;
-- Tests trying to archive a partition twice.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE srcpart_archived LIKE srcpart;

INSERT OVERWRITE TABLE srcpart_archived PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE srcpart_archived ARCHIVE PARTITION (ds='2008-04-08', hr='12');
ALTER TABLE srcpart_archived ARCHIVE PARTITION (ds='2008-04-08', hr='12');
set hive.archive.enabled = true;
-- Tests trying to unarchive a non-archived partition
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

drop table tstsrcpart;
create table tstsrcpart like srcpart;
insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='12')
select key, value from srcpart where ds='2008-04-08' and hr='12';

ALTER TABLE tstsrcpart UNARCHIVE PARTITION (ds='2008-04-08', hr='12');
set hive.archive.enabled = true;
-- Tests archiving a table
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

ALTER TABLE srcpart ARCHIVE;
set hive.archive.enabled = true;
-- Tests archiving multiple partitions
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

ALTER TABLE srcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12') PARTITION (ds='2008-04-08', hr='11');
set hive.archive.enabled = true;
-- Tests creating a partition where the partition value will collide with the
-- a intermediate directory

ALTER TABLE srcpart ADD PARTITION (ds='2008-04-08', hr='14_INTERMEDIATE_ORIGINAL')
USE default;

set hive.archive.enabled = true;


drop table tstsrcpart;

create table tstsrcpart like srcpart;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
-- The version of GzipCodec that is provided in Hadoop 0.20 silently ignores
-- file format errors. However, versions of Hadoop that include
-- HADOOP-6835 (e.g. 0.23 and 1.x) cause a Wrong File Format exception
-- to be thrown during the LOAD step. This former behavior is tested
-- in clientpositive/archive_corrupt.q

load data local inpath '../../data/files/archive_corrupt.rc' overwrite into table tstsrcpart partition (ds='2008-04-08', hr='11');

set hive.mapred.mode=nonstrict;
set hive.archive.enabled = true;
;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;

drop table tstsrc;
drop table tstsrcpart;

create table tstsrc like src;
insert overwrite table tstsrc select key, value from src;

create table tstsrcpart (key string, value string) partitioned by (ds string, hr string) clustered by (key) into 10 buckets;

insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='11')
select key, value from srcpart where ds='2008-04-08' and hr='11';

insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='12')
select key, value from srcpart where ds='2008-04-08' and hr='12';

insert overwrite table tstsrcpart partition (ds='2008-04-09', hr='11')
select key, value from srcpart where ds='2008-04-09' and hr='11';

insert overwrite table tstsrcpart partition (ds='2008-04-09', hr='12')
select key, value from srcpart where ds='2008-04-09' and hr='12';

SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col
FROM (SELECT * FROM tstsrcpart WHERE ds='2008-04-08') subq1) subq2;

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12');

SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col
FROM (SELECT * FROM tstsrcpart WHERE ds='2008-04-08') subq1) subq2;

SELECT key, count(1) FROM tstsrcpart WHERE ds='2008-04-08' AND hr='12' AND key='0' GROUP BY key;

SELECT * FROM tstsrcpart a JOIN tstsrc b ON a.key=b.key
WHERE a.ds='2008-04-08' AND a.hr='12' AND a.key='0';

ALTER TABLE tstsrcpart UNARCHIVE PARTITION (ds='2008-04-08', hr='12');

SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col
FROM (SELECT * FROM tstsrcpart WHERE ds='2008-04-08') subq1) subq2;

CREATE TABLE harbucket(key INT)
PARTITIONED by (ds STRING)
CLUSTERED BY (key) INTO 10 BUCKETS;

INSERT OVERWRITE TABLE harbucket PARTITION(ds='1') SELECT CAST(key AS INT) AS a FROM tstsrc WHERE key < 50;

SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key;
ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12');
SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key;
ALTER TABLE tstsrcpart UNARCHIVE PARTITION (ds='2008-04-08', hr='12');
SELECT key FROM harbucket TABLESAMPLE(BUCKET 1 OUT OF 10) SORT BY key;


CREATE TABLE old_name(key INT)
PARTITIONED by (ds STRING);

INSERT OVERWRITE TABLE old_name PARTITION(ds='1') SELECT CAST(key AS INT) AS a FROM tstsrc WHERE key < 50;
ALTER TABLE old_name ARCHIVE PARTITION (ds='1');
SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col
FROM (SELECT * FROM old_name WHERE ds='1') subq1) subq2;
ALTER TABLE old_name RENAME TO new_name;
SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col
FROM (SELECT * FROM new_name WHERE ds='1') subq1) subq2;

drop table tstsrc;
drop table tstsrcpart;
set hive.archive.enabled = true;
-- Tests trying to insert into archived partition.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12');

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';
set hive.archive.enabled = true;
-- Tests trying to insert into archived partition.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';
set hive.archive.enabled = true;
-- Tests trying to create partition inside of archived directory.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='11')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='11';
set hive.archive.enabled = true;
-- Tests trying to (possible) dynamic insert into archived partition.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12');

SET hive.exec.dynamic.partition=true;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr)
SELECT key, value, hr FROM srcpart WHERE ds='2008-04-08' AND hr='12';
set hive.mapred.mode=nonstrict;
set hive.archive.enabled = true;
;

create database ac_test;

create table ac_test.tstsrc like default.src;
insert overwrite table ac_test.tstsrc select key, value from default.src;

create table ac_test.tstsrcpart like default.srcpart;

insert overwrite table ac_test.tstsrcpart partition (ds='2008-04-08', hr='11')
select key, value from default.srcpart where ds='2008-04-08' and hr='11';

insert overwrite table ac_test.tstsrcpart partition (ds='2008-04-08', hr='12')
select key, value from default.srcpart where ds='2008-04-08' and hr='12';

insert overwrite table ac_test.tstsrcpart partition (ds='2008-04-09', hr='11')
select key, value from default.srcpart where ds='2008-04-09' and hr='11';

insert overwrite table ac_test.tstsrcpart partition (ds='2008-04-09', hr='12')
select key, value from default.srcpart where ds='2008-04-09' and hr='12';

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col
FROM (SELECT * FROM ac_test.tstsrcpart WHERE ds='2008-04-08') subq1) subq2;

ALTER TABLE ac_test.tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');

SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col
FROM (SELECT * FROM ac_test.tstsrcpart WHERE ds='2008-04-08') subq1) subq2;

SELECT key, count(1) FROM ac_test.tstsrcpart WHERE ds='2008-04-08' AND hr='12' AND key='0' GROUP BY key;

SELECT * FROM ac_test.tstsrcpart a JOIN ac_test.tstsrc b ON a.key=b.key
WHERE a.ds='2008-04-08' AND a.hr='12' AND a.key='0';

ALTER TABLE ac_test.tstsrcpart UNARCHIVE PARTITION (ds='2008-04-08');

SELECT SUM(hash(col)) FROM (SELECT transform(*) using 'tr "\t" "_"' AS col
FROM (SELECT * FROM ac_test.tstsrcpart WHERE ds='2008-04-08') subq1) subq2;
set hive.archive.enabled = true;
-- Tests trying to archive a partition twice.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='11')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='11';
INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');
ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');
set hive.archive.enabled = true;
-- Tests trying to unarchive a non-archived partition group
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

drop table tstsrcpart;
create table tstsrcpart like srcpart;
INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='11')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='11';
insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='12')
select key, value from srcpart where ds='2008-04-08' and hr='12';

ALTER TABLE tstsrcpart UNARCHIVE PARTITION (ds='2008-04-08', hr='12');
set hive.archive.enabled = true;
-- Tests trying to archive outer partition group containing other partition inside.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='11')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='11';
INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12');
ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');
set hive.archive.enabled = true;
-- Tests trying to archive inner partition contained in archived partition group.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='11')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='11';
INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');
ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12');
set hive.archive.enabled = true;
-- Tests trying to unarchive outer partition group containing other partition inside.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='11')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='11';
INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08', hr='12');
ALTER TABLE tstsrcpart UNARCHIVE PARTITION (ds='2008-04-08');
set hive.archive.enabled = true;
-- Tests trying to unarchive inner partition contained in archived partition group.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='11')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='11';
INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');
ALTER TABLE tstsrcpart UNARCHIVE PARTITION (ds='2008-04-08', hr='12');
set hive.archive.enabled = true;
-- Tests trying to archive a partition group with custom locations.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE tstsrcpart LIKE srcpart;

INSERT OVERWRITE TABLE tstsrcpart PARTITION (ds='2008-04-08', hr='11')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='11';
ALTER TABLE tstsrcpart ADD PARTITION (ds='2008-04-08', hr='12')
LOCATION "${system:test.tmp.dir}/tstsrc";

ALTER TABLE tstsrcpart ARCHIVE PARTITION (ds='2008-04-08');
set hive.archive.enabled = true;
-- Tests trying to archive a partition twice.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE srcpart_archived LIKE srcpart;

INSERT OVERWRITE TABLE srcpart_archived PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE srcpart_archived ARCHIVE PARTITION (ds='2008-04-08', nonexistingpart='12');
set hive.archive.enabled = true;
-- Tests trying to archive a partition twice.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE srcpart_archived LIKE srcpart;

INSERT OVERWRITE TABLE srcpart_archived PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE srcpart_archived ARCHIVE PARTITION (hr='12');
set hive.archive.enabled = true;
-- Tests trying to archive a partition twice.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE srcpart_archived LIKE srcpart;

INSERT OVERWRITE TABLE srcpart_archived PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE srcpart_archived ARCHIVE PARTITION ();
set hive.archive.enabled = true;
-- Tests trying to archive a partition twice.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE srcpart_archived LIKE srcpart;

INSERT OVERWRITE TABLE srcpart_archived PARTITION (ds='2008-04-08', hr='12')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE srcpart_archived ARCHIVE PARTITION (hr='12', ds='2008-04-08');set hive.archive.enabled = true;
-- Tests trying to archive a partition twice.
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

CREATE TABLE srcpart_archived (key string, value string) partitioned by (ds string, hr int, min int);

INSERT OVERWRITE TABLE srcpart_archived PARTITION (ds='2008-04-08', hr='12', min='00')
SELECT key, value FROM srcpart WHERE ds='2008-04-08' AND hr='12';

ALTER TABLE srcpart_archived ARCHIVE PARTITION (ds='2008-04-08', min='00');set hive.fetch.task.conversion=more;

create table array_table (`array` array<string>, index int );
insert into table array_table select array('first', 'second', 'third'), key%3 from src tablesample (4 rows);

explain
select index, `array`[index] from array_table;
select index, `array`[index] from array_table;

create table map_table (data map<string,string>, key int );
insert into table map_table select map('1','one','2','two','3','three'), cast((key%3+1) as int) from src tablesample (4 rows);

explain
select key, data[key] from map_table;
select key, data[key] from map_table;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

-- SORT_BEFORE_DIFF

create table src_autho_test as select * from src;

set hive.security.authorization.enabled=true;

--table grant to user

grant select on table src_autho_test to user hive_test_user;

show grant user hive_test_user on table src_autho_test;
show grant user hive_test_user on table src_autho_test(key);

select key from src_autho_test order by key limit 20;

revoke select on table src_autho_test from user hive_test_user;
show grant user hive_test_user on table src_autho_test;
show grant user hive_test_user on table src_autho_test(key);

--column grant to user

grant select(key) on table src_autho_test to user hive_test_user;

show grant user hive_test_user on table src_autho_test;
show grant user hive_test_user on table src_autho_test(key);

select key from src_autho_test order by key limit 20;

revoke select(key) on table src_autho_test from user hive_test_user;
show grant user hive_test_user on table src_autho_test;
show grant user hive_test_user on table src_autho_test(key);

--table grant to group

grant select on table src_autho_test to group hive_test_group1;

show grant group hive_test_group1 on table src_autho_test;
show grant group hive_test_group1 on table src_autho_test(key);

select key from src_autho_test order by key limit 20;

revoke select on table src_autho_test from group hive_test_group1;
show grant group hive_test_group1 on table src_autho_test;
show grant group hive_test_group1 on table src_autho_test(key);

--column grant to group

grant select(key) on table src_autho_test to group hive_test_group1;

show grant group hive_test_group1 on table src_autho_test;
show grant group hive_test_group1 on table src_autho_test(key);

select key from src_autho_test order by key limit 20;

revoke select(key) on table src_autho_test from group hive_test_group1;
show grant group hive_test_group1 on table src_autho_test;
show grant group hive_test_group1 on table src_autho_test(key);

--role
create role sRc_roLE;
grant role sRc_roLE to user hive_test_user;
show role grant user hive_test_user;

--column grant to role

grant select(key) on table src_autho_test to role sRc_roLE;

show grant role sRc_roLE on table src_autho_test;
show grant role sRc_roLE on table src_autho_test(key);

select key from src_autho_test order by key limit 20;

revoke select(key) on table src_autho_test from role sRc_roLE;

--table grant to role

grant select on table src_autho_test to role sRc_roLE;

select key from src_autho_test order by key limit 20;

show grant role sRc_roLE on table src_autho_test;
show grant role sRc_roLE on table src_autho_test(key);
revoke select on table src_autho_test from role sRc_roLE;

-- drop role
drop role sRc_roLE;

set hive.security.authorization.enabled=false;
drop table src_autho_test;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;

create table src_autho_test (key STRING, value STRING) ;

set hive.security.authorization.enabled=true;

--select dummy table
select 1;

set  role ADMIN;
--table grant to user

grant select on table src_autho_test to user user_sauth;

show grant user user_sauth on table src_autho_test;


revoke select on table src_autho_test from user user_sauth;
show grant user user_sauth on table src_autho_test;

--role
create role src_role;
grant role src_role to user user_sauth;
show role grant user user_sauth;

--table grant to role

-- also verify case insesitive behavior of role name
grant select on table src_autho_test to role Src_ROle;

show grant role src_role on table src_autho_test;
revoke select on table src_autho_test from role src_rolE;

-- drop role
drop role SRc_role;

set hive.security.authorization.enabled=false;
drop table src_autho_test;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

-- SORT_BEFORE_DIFF

create table authorization_part (key int, value string) partitioned by (ds string);
create table src_auth_tmp as select * from src;
ALTER TABLE authorization_part SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");
set hive.security.authorization.enabled=true;

-- column grant to user
grant Create on  authorization_part to user hive_test_user;
grant Update on table authorization_part to user hive_test_user;
grant Drop on table authorization_part to user hive_test_user;
grant select on table src_auth_tmp to user hive_test_user;

show grant user hive_test_user on table authorization_part;

alter table authorization_part add partition (ds='2010');
show grant user hive_test_user on table authorization_part partition (ds='2010');

grant select(key) on table authorization_part to user hive_test_user;
alter table authorization_part drop partition (ds='2010');
insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp;
show grant user hive_test_user on table authorization_part(key) partition (ds='2010');
show grant user hive_test_user on table authorization_part(key);
select key from authorization_part where ds='2010' order by key limit 20;

revoke select(key) on table authorization_part from user hive_test_user;
show grant user hive_test_user on table authorization_part(key);
show grant user hive_test_user on table authorization_part(key) partition (ds='2010');

select key from authorization_part where ds='2010' order by key limit 20;

revoke select(key) on table authorization_part partition (ds='2010') from user hive_test_user;
show grant user hive_test_user on table authorization_part(key) partition (ds='2010');

alter table authorization_part drop partition (ds='2010');

-- table grant to user
show grant user hive_test_user on table authorization_part;

alter table authorization_part add partition (ds='2010');
show grant user hive_test_user on table authorization_part partition (ds='2010');

grant select on table authorization_part to user hive_test_user;
alter table authorization_part drop partition (ds='2010');
insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp;
show grant user hive_test_user on table authorization_part partition (ds='2010');
show grant user hive_test_user on table authorization_part;
select key from authorization_part where ds='2010' order by key limit 20;

revoke select on table authorization_part from user hive_test_user;
show grant user hive_test_user on table authorization_part;
show grant user hive_test_user on table authorization_part partition (ds='2010');

select key from authorization_part where ds='2010' order by key limit 20;

revoke select on table authorization_part partition (ds='2010') from user hive_test_user;
show grant user hive_test_user on table authorization_part partition (ds='2010');

alter table authorization_part drop partition (ds='2010');

-- column grant to group

show grant group hive_test_group1 on table authorization_part;

alter table authorization_part add partition (ds='2010');
show grant group hive_test_group1 on table authorization_part partition (ds='2010');

grant select(key) on table authorization_part to group hive_test_group1;
alter table authorization_part drop partition (ds='2010');
insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp;
show grant group hive_test_group1 on table authorization_part(key) partition (ds='2010');
show grant group hive_test_group1 on table authorization_part(key);
select key from authorization_part where ds='2010' order by key limit 20;

revoke select(key) on table authorization_part from group hive_test_group1;
show grant group hive_test_group1 on table authorization_part(key);
show grant group hive_test_group1 on table authorization_part(key) partition (ds='2010');

select key from authorization_part where ds='2010' order by key limit 20;

revoke select(key) on table authorization_part partition (ds='2010') from group hive_test_group1;
show grant group hive_test_group1 on table authorization_part(key) partition (ds='2010');

alter table authorization_part drop partition (ds='2010');

-- table grant to group
show grant group hive_test_group1 on table authorization_part;

alter table authorization_part add partition (ds='2010');
show grant group hive_test_group1 on table authorization_part partition (ds='2010');

grant select on table authorization_part to group hive_test_group1;
alter table authorization_part drop partition (ds='2010');
insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp;
show grant group hive_test_group1 on table authorization_part partition (ds='2010');
show grant group hive_test_group1 on table authorization_part;
select key from authorization_part where ds='2010' order by key limit 20;

revoke select on table authorization_part from group hive_test_group1;
show grant group hive_test_group1 on table authorization_part;
show grant group hive_test_group1 on table authorization_part partition (ds='2010');

select key from authorization_part where ds='2010' order by key limit 20;

revoke select on table authorization_part partition (ds='2010') from group hive_test_group1;
show grant group hive_test_group1 on table authorization_part partition (ds='2010');


revoke select on table src_auth_tmp from user hive_test_user;
set hive.security.authorization.enabled=false;
drop table authorization_part;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

-- SORT_BEFORE_DIFF

create table src_autho_test as select * from src;

grant drop on table src_autho_test to user hive_test_user;
grant select on table src_autho_test to user hive_test_user;

show grant user hive_test_user on table src_autho_test;

revoke select on table src_autho_test from user hive_test_user;
revoke drop on table src_autho_test from user hive_test_user;

grant drop,select on table src_autho_test to user hive_test_user;
show grant user hive_test_user on table src_autho_test;
revoke drop,select on table src_autho_test from user hive_test_user;

grant drop,select(key), select(value) on table src_autho_test to user hive_test_user;
show grant user hive_test_user on table src_autho_test;
revoke drop,select(key), select(value) on table src_autho_test from user hive_test_user;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

-- SORT_BEFORE_DIFF

create table src_autho_test as select * from src;

grant All on table src_autho_test to user hive_test_user;

set hive.security.authorization.enabled=true;

show grant user hive_test_user on table src_autho_test;

select key from src_autho_test order by key limit 20;

drop table src_autho_test;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

-- SORT_BEFORE_DIFF

CREATE DATABASE IF NOT EXISTS test_db COMMENT 'Hive test database';
SHOW DATABASES;

GRANT drop ON DATABASE test_db TO USER hive_test_user;
GRANT select ON DATABASE test_db TO USER hive_test_user;

SHOW GRANT USER hive_test_user ON DATABASE test_db;

CREATE ROLE db_TEST_Role;
GRANT ROLE db_TEST_Role TO USER hive_test_user;
SHOW ROLE GRANT USER hive_test_user;

GRANT drop ON DATABASE test_db TO ROLE db_TEST_Role;
GRANT select ON DATABASE test_db TO ROLE db_TEST_Role;

SHOW GRANT ROLE db_TEST_Role ON DATABASE test_db;

DROP DATABASE IF EXISTS test_db;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

-- SORT_BEFORE_DIFF

create table src_auth_tmp as select * from src;

create table authorization_part (key int, value string) partitioned by (ds string);
ALTER TABLE authorization_part SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");
set hive.security.authorization.enabled=true;
grant select on table src_auth_tmp to user hive_test_user;

-- column grant to user
grant Create on table authorization_part to user hive_test_user;
grant Update on table authorization_part to user hive_test_user;
grant Drop on table authorization_part to user hive_test_user;

show grant user hive_test_user on table authorization_part;
grant select(key) on table authorization_part to user hive_test_user;
insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp;
insert overwrite table authorization_part partition (ds='2011') select key, value from src_auth_tmp;
show grant user hive_test_user on table authorization_part(key) partition (ds='2010');
show grant user hive_test_user on table authorization_part(key) partition (ds='2011');
show grant user hive_test_user on table authorization_part(key);
select key from authorization_part where ds>='2010' order by key limit 20;

drop table authorization_part;

set hive.security.authorization.enabled=false;
create table authorization_part (key int, value string) partitioned by (ds string);
ALTER TABLE authorization_part SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="FALSE");

set hive.security.authorization.enabled=true;
grant Create on table authorization_part to user hive_test_user;
grant Update on table authorization_part to user hive_test_user;

show grant user hive_test_user on table authorization_part;

grant select(key) on table authorization_part to user hive_test_user;
insert overwrite table authorization_part partition (ds='2010') select key, value from src_auth_tmp;
insert overwrite table authorization_part partition (ds='2011') select key, value from src_auth_tmp;
show grant user hive_test_user on table authorization_part(key) partition (ds='2010');
show grant user hive_test_user on table authorization_part(key) partition (ds='2011');
show grant user hive_test_user on table authorization_part(key);
select key from authorization_part where ds>='2010' order by key limit 20;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

GRANT ALL TO USER hive_test_user;
SET hive.security.authorization.enabled=true;
CREATE TABLE src_authorization_7 (key int, value string);
DESCRIBE src_authorization_7;
DROP TABLE  src_authorization_7;
REVOKE ALL FROM USER hive_test_user;

SET hive.security.authorization.enabled=false;

GRANT ALL TO GROUP hive_test_group1;
SET hive.security.authorization.enabled=true;
CREATE TABLE src_authorization_7 (key int, value string);
DESCRIBE src_authorization_7;
DROP TABLE  src_authorization_7;
REVOKE ALL FROM GROUP hive_test_group1;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.security.authorization.enabled=true;
GRANT ALL TO USER hive_test_user;
CREATE TABLE tbl_j5jbymsx8e (key INT, value STRING) PARTITIONED BY (ds STRING);
CREATE VIEW view_j5jbymsx8e_1 as SELECT * FROM tbl_j5jbymsx8e;
DESCRIBE view_j5jbymsx8e_1;
ALTER VIEW view_j5jbymsx8e_1 RENAME TO view_j5jbymsx8e_2;
REVOKE ALL FROM USER hive_test_user;
set hive.security.authorization.enabled=false;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-- SORT_BEFORE_DIFF

create table dummy (key string, value string);

grant select to user hive_test_user;
grant select on database default to user hive_test_user;
grant select on table dummy to user hive_test_user;
grant select (key, value) on table dummy to user hive_test_user;

show grant user hive_test_user on database default;
show grant user hive_test_user on table dummy;
show grant user hive_test_user on all;

grant select to user hive_test_user2;
grant select on database default to user hive_test_user2;
grant select on table dummy to user hive_test_user2;
grant select (key, value) on table dummy to user hive_test_user2;

show grant on all;
show grant user hive_test_user on all;
show grant user hive_test_user2 on all;

revoke select from user hive_test_user;
revoke select on database default from user hive_test_user;
revoke select on table dummy from user hive_test_user;
revoke select (key, value) on table dummy from user hive_test_user;

revoke select from user hive_test_user2;
revoke select on database default from user hive_test_user2;
revoke select on table dummy from user hive_test_user2;
revoke select (key, value) on table dummy from user hive_test_user2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.enabled=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;

add jar dummy.jar
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=user1;
-- check add partition without insert privilege
create table tpart(i int, j int) partitioned by (k string);

set user.name=user2;
alter table tpart add partition (k = 'abc');
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_test_user;

-- actions from admin should work as if admin has all privileges

create table t1(i int);
set user.name=hive_admin_user;

show current roles;
set role ADMIN;
show current roles;
select * from t1;
grant all on table t1 to user user1;
show grant user user1 on table t1;
drop table t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;

-- test commands such as dfs,add,delete,compile allowed only by admin user, after following statement
use default;

set role admin;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_admin_almighty1;
dfs -ls ${system:test.tmp.dir}/a_admin_almighty1;

create table a_table1(a int, b int);
add jar ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
alter table a_table1 set serde 'org.apache.hadoop.hive.serde2.TestSerDe' with serdeproperties('s1'='9');
drop table a_table;

delete jar ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;

compile `import org.apache.hadoop.hive.ql.exec.UDF \;
public class Pyth extends UDF {
  public double evaluate(double a, double b){
    return Math.sqrt((a*a) + (b*b)) \;
  }
} `AS GROOVY NAMED Pyth.groovy;
CREATE TEMPORARY FUNCTION Pyth as 'Pyth';

SELECT Pyth(3,4) FROM src tablesample (1 rows);

DROP TEMPORARY FUNCTION Pyth;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if alter table owner fails
-- for now, alter db owner is allowed only for admin

create database dbao;
alter database dbao set owner user user2;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if alter table owner fails
alter database default set owner user user1;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;
create role all;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;
create role default;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;
create role None;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;

create role testrole;
show roles;
drop role TESTROLE;
show roles;
create role TESTROLE;
show roles;
grant role testROLE to user hive_admin_user;
set role testrolE;
set role adMin;
show roles;
create role TESTRoLE;
set hive.users.in.admin.role=hive_admin_user;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_test_user;

-- verify that sql std auth can be set as the authorizer with hive cli
-- and that the create table/view result in correct permissions (suitable for sql std auth mode)

create table t_cli(i int);
show grant user hive_test_user on t_cli;

create view v_cli (i) as select i from t_cli;
show grant user hive_test_user on v_cli;
set hive.users.in.admin.role=hive_admin_user;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.metastore.pre.event.listeners=org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener;
set hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.MetastoreAuthzAPIDisallowAuthorizer;
set user.name=hive_test_user;

-- verify that sql std auth can be set as the authorizer with hive cli, while metastore authorization api calls are disabled (for cli)

create table t_cli(i int);

create view v_cli (i) as select i from t_cli;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;
set hive.security.authorization.enabled=false;

-- Verify that dfs,compile,add,delete commands can be run from hive cli, and no authorization checks happen when auth is diabled

use default;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_admin_almighty1;
dfs -ls ${system:test.tmp.dir}/a_admin_almighty1;

create table a_table1(a int, b int);
add jar ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
alter table a_table1 set serde 'org.apache.hadoop.hive.serde2.TestSerDe' with serdeproperties('s1'='9');
drop table a_table;

delete jar ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;

compile `import org.apache.hadoop.hive.ql.exec.UDF \;
public class Pyth extends UDF {
  public double evaluate(double a, double b){
    return Math.sqrt((a*a) + (b*b)) \;
  }
} `AS GROOVY NAMED Pyth.groovy;
CREATE TEMPORARY FUNCTION Pyth as 'Pyth';

SELECT Pyth(3,4) FROM src tablesample (1 rows);

DROP TEMPORARY FUNCTION Pyth;

set hive.users.in.admin.role=hive_admin_user;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

-- verify that SQLStdConfOnlyAuthorizerFactory as the authorizer factory with hive cli, with hive.security.authorization.enabled=true
-- authorization verification would be just no-op

create table t_cli(i int);
describe t_cli;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.enabled=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;

COMPILE `dummy code ` AS groovy NAMED something.groovy;



set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

-- check create view without select privileges
create table t1(i int);
set user.name=user1;
create view v1 as select * from t1;


set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=hive_admin_user;

-- admin required for create function
set role ADMIN;

create temporary function temp_fn as 'org.apache.hadoop.hive.ql.udf.UDFAscii';
create function perm_fn as 'org.apache.hadoop.hive.ql.udf.UDFAscii';

drop temporary function temp_fn;
drop function perm_fn;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=hive_test_user;

-- permanent function creation should fail for non-admin roles
create function perm_fn as 'org.apache.hadoop.hive.ql.udf.UDFAscii';
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=hive_test_user;

-- temp function creation should fail for non-admin roles
create temporary function temp_fn as 'org.apache.hadoop.hive.ql.udf.UDFAscii';

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
create table t1 (a int);
set user.name=user2;
create index t1_index on table t1(a) as 'COMPACT' WITH DEFERRED REBUILD;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=hive_admin_user;

-- admin required for create macro
set role ADMIN;

create temporary macro mymacro1(x double) x * x;

drop temporary macro mymacro1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=hive_test_user;

-- temp macro creation should fail for non-admin roles
create temporary macro mymacro1(x double) x * x;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
-- this test will fail because hive_test_user is not in admin role.
create role r1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;

create table create_table_creator_priv_test(i int);

-- all privileges should have been set for user

show grant user user1 on table create_table_creator_priv_test;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set hive.security.authorization.enabled=true;
set user.name=user33;
create database db23221;
use db23221;

set user.name=user44;
create table twew221(a string);
set hive.mapred.mode=nonstrict;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

create table authorization_create_temp_table_1 as select * from src limit 10;
grant select on authorization_create_temp_table_1 to user user1;

set user.name=user1;
set hive.security.authorization.enabled=true;

create temporary table tmp1(c1 string, c2 string);

insert overwrite table tmp1 select * from authorization_create_temp_table_1;

select c1, count(*) from tmp1 group by c1 order by c1;

drop table tmp1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=user3;
create database db1;
use db1;
create table tab1(i int);

set user.name=user4;
-- create view should fail as view is being created in db that it does not own
create view db1.view1(i) as select * from tab1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

-- check query without select privilege fails
create table t1(i int);

set user.name=user1;
create table t2 as select * from t1;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=user_dbowner;
-- check ctas without db ownership
create database ctas_auth;

set user.name=user_unauth;
create table t1(i int);
use ctas_auth;
show tables;
create table t2 as select * from default.t1;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.security.authorization.createtable.owner.grants=ALL;

create table default_auth_table_creator_priv_test(i int);

-- Table owner (hive_test_user) should have ALL privileges
show grant on table default_auth_table_creator_priv_test;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE t_auth_del(i int) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

-- grant update privilege to another user
GRANT DELETE ON t_auth_del TO USER userWIns;
GRANT SELECT ON t_auth_del TO USER userWIns;

set user.name=hive_admin_user;
set role admin;
SHOW GRANT ON TABLE t_auth_del;


set user.name=userWIns;
delete from t_auth_del where i > 0;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.enabled=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;

delete jar dummy.jar

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;



-- check update without update priv
create table auth_nodel(i int) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

set user.name=user1;
delete from auth_nodel where i > 0;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;



set user.name=user1;
create table auth_noupd(i int) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
delete from auth_noupd where i > 0;

set user.name=hive_admin_user;
set role admin;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if alter table fails as different user
create table t1(i int);
desc t1;

grant all on table t1 to user user2;
revoke select on table t1 from user user2;

set user.name=user2;
desc t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.enabled=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;

dfs -ls dummy_file;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authorization.enabled=true;
set hive.entity.capture.transform=true;
set role ALL;
create table t1(i int);
SELECT TRANSFORM (*) USING 'cat' AS (key, value) FROM t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/authz_drop_part_1;

-- check drop partition without delete privilege
create table tpart(i int, j int) partitioned by (k string);
alter table tpart add partition (k = 'abc') location 'file:${system:test.tmp.dir}/authz_drop_part_1' ;
set user.name=user1;
alter table tpart drop partition (k = 'abc');
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role admin;
drop role admin;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- ensure that drop database cascade works
create database dba1;
create table dba1.tab1(i int);
drop database dba1 cascade;

-- check if drop database fails if the db has a table for which user does not have permission
create database dba2;
create table dba2.tab2(i int);

set user.name=hive_admin_user;
set role ADMIN;
alter database dba2 set owner user user2;

set user.name=user2;
show current roles;
drop database dba2 cascade ;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if changing owner and dropping as other user works
create database dba1;

set user.name=hive_admin_user;
set role ADMIN;
alter database dba1 set owner user user2;

set user.name=user2;
show current roles;
drop database dba1;


set user.name=user1;
-- check if dropping db as another user fails
show current roles;
create database dba2;

set user.name=user2;
show current roles;

drop database dba2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
create table t1 (a int);
create index t1_index on table t1(a) as 'COMPACT' WITH DEFERRED REBUILD;
set user.name=user2;
drop index t1_index on t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;
show current roles;
create role r1;
set role ALL;
show current roles;
drop role r1;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.mapred.mode=nonstrict;
set hive.security.authorization.enabled=true;

-- JAVA_VERSION_SPECIFIC_OUTPUT

explain authorization select * from src join srcpart;
explain formatted authorization select * from src join srcpart;

explain authorization use default;
explain formatted authorization use default;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table authorization_fail_1 (key int, value string);
set hive.security.authorization.enabled=true;

grant Create on table authorization_fail_1 to user hive_test_user;
grant Create on table authorization_fail_1 to user hive_test_user;


set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table authorization_fail_2 (key int, value string) partitioned by (ds string);

set hive.security.authorization.enabled=true;

alter table authorization_fail_2 add partition (ds='2010');


set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-- SORT_BEFORE_DIFF

create table authorization_fail_3 (key int, value string) partitioned by (ds string);
set hive.security.authorization.enabled=true;

grant Create on table authorization_fail_3 to user hive_test_user;
alter table authorization_fail_3 add partition (ds='2010');

show grant user hive_test_user on table authorization_fail_3;
show grant user hive_test_user on table authorization_fail_3 partition (ds='2010');

select key from authorization_fail_3 where ds='2010';
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-- SORT_BEFORE_DIFF

create table authorization_fail_4 (key int, value string) partitioned by (ds string);

set hive.security.authorization.enabled=true;
grant Alter on table authorization_fail_4 to user hive_test_user;
ALTER TABLE authorization_fail_4 SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");

grant Create on table authorization_fail_4 to user hive_test_user;
alter table authorization_fail_4 add partition (ds='2010');

show grant user hive_test_user on table authorization_fail_4;
show grant user hive_test_user on table authorization_fail_4 partition (ds='2010');

select key from authorization_fail_4 where ds='2010';
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-- SORT_BEFORE_DIFF

create table authorization_fail (key int, value string) partitioned by (ds string);
set hive.security.authorization.enabled=true;

grant Alter on table authorization_fail to user hive_test_user;
ALTER TABLE authorization_fail SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");

grant Create on table authorization_fail to user hive_test_user;
grant Select on table authorization_fail to user hive_test_user;
alter table authorization_fail add partition (ds='2010');

show grant user hive_test_user on table authorization_fail;
show grant user hive_test_user on table authorization_fail partition (ds='2010');

revoke Select on table authorization_fail partition (ds='2010') from user hive_test_user;

show grant user hive_test_user on table authorization_fail partition (ds='2010');

select key from authorization_fail where ds='2010';
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-- SORT_BEFORE_DIFF

create table authorization_part_fail (key int, value string) partitioned by (ds string);
set hive.security.authorization.enabled=true;

ALTER TABLE authorization_part_fail SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-- SORT_BEFORE_DIFF

create table authorization_fail (key int, value string);

set hive.security.authorization.enabled=true;

create role hive_test_role_fail;

grant role hive_test_role_fail to user hive_test_user;
grant select on table authorization_fail to role hive_test_role_fail;
show role grant user hive_test_user;

show grant role hive_test_role_fail on table authorization_fail;

drop role hive_test_role_fail;

select key from authorization_fail;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
create table authorization_fail (key int, value string) partitioned by (ds string);
GRANT SELECT ON authorization_fail TO USER user2 WITH GRANT OPTION;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE authorization_fail;
-- user2 current has grant option, this should work
GRANT SELECT ON authorization_fail TO USER user3;
REVOKE SELECT ON authorization_fail FROM USER user3;

set user.name=user1;
REVOKE GRANT OPTION FOR SELECT ON authorization_fail FROM USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE authorization_fail;
-- Now that grant option has been revoked, granting to other users should fail
GRANT SELECT ON authorization_fail TO USER user3;

set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.security.authorization.enabled=true;

create database db_to_fail;


set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.security.authorization.enabled=false;
create database db_fail_to_drop;
set hive.security.authorization.enabled=true;

drop database db_fail_to_drop;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE table_gg(i int);

-- grant insert on group should fail
GRANT INSERT ON table_gg TO group g1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=hive_admin_user;
set role admin;
create role r1;
grant role r1 to user r1user;

set user.name=user1;
CREATE TABLE  t1(i int);

-- all privileges should have been set for user

GRANT ALL ON t1 TO ROLE r1 WITH GRANT OPTION;

set user.name=r1user;
-- check if user belong to role r1 can grant privileges to others
GRANT ALL ON t1 TO USER user3;

set user.name=hive_admin_user;
set role admin;
-- check privileges on table
show grant on table t1;

-- check if drop role removes privileges for that role
drop role r1;
show grant on table t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE  t_gpr1(i int);

-- all privileges should have been set for user

GRANT ALL ON t_gpr1 TO ROLE pubLic;

SHOW GRANT USER user1 ON TABLE t_gpr1;
SHOW GRANT ROLE pubLic ON TABLE t_gpr1;

set user.name=user2;
SHOW CURRENT ROLES;
-- user2 should be able to do a describe table, as pubic is in the current roles
DESC t_gpr1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;

-- grant insert on group should fail
GRANT ALL ON SERVER foo TO user user2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE table_priv_allf(i int);

-- grant insert to user2 WITH grant option
GRANT INSERT ON table_priv_allf TO USER user2 with grant option;

set user.name=user2;
-- try grant all to user3, without having all privileges
GRANT ALL ON table_priv_allf TO USER user3;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE  tauth_gdup(i int);

-- It should be possible to revert owners privileges
revoke SELECT ON tauth_gdup from user user1;

show grant user user1 on table tauth_gdup;

-- Owner already has all privileges granted, another grant would become duplicate
-- and result in error
GRANT INSERT ON tauth_gdup TO USER user1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE  table_priv_gfail1(i int);

set user.name=user2;
-- try grant insert to user3 as user2
GRANT INSERT ON table_priv_gfail1 TO USER user3;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE table_priv_gfail1(i int);

-- grant insert to user2 WITHOUT grant option
GRANT INSERT ON table_priv_gfail1 TO USER user2;

set user.name=user2;
-- try grant insert to user3
GRANT INSERT ON table_priv_gfail1 TO USER user3;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE  table_priv1(i int);

-- all privileges should have been set for user

-- grant insert privilege to another user
GRANT INSERT ON table_priv1 TO USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv1;
set user.name=user1;

-- grant select privilege to another user with grant
GRANT SELECT ON table_priv1 TO USER user2 with grant option;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv1;

-- changed to other user - user2
-- grant permissions to another user as user2
GRANT SELECT ON table_priv1 TO USER user3 with grant option;

set user.name=user3;
SHOW GRANT USER user3 ON TABLE table_priv1;

-- change to other user - user3
-- grant permissions to another user as user3
GRANT SELECT ON table_priv1 TO USER user4 with grant option;

set user.name=user4;
SHOW GRANT USER user4 ON TABLE table_priv1;

set user.name=user1;
-- switched back to table owner

-- grant all with grant to user22
GRANT ALL ON table_priv1 TO USER user22 with grant option;

set user.name=user22;
SHOW GRANT USER user22 ON TABLE table_priv1;

-- grant all without grant to user33
GRANT ALL ON table_priv1 TO USER user33 with grant option;

set user.name=user33;
SHOW GRANT USER user33 ON TABLE table_priv1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

-- grant insert on group should fail
GRANT ALL ON URI '/tmp' TO user user2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authorization.enabled=true;

set test.hive.authz.sstd.validator.bypassObjTypes=DFS_URI;

set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=import_auth_t1,import_auth_t2,import_auth_t3;

drop table if exists import_auth_t1;
create table import_auth_t1 ( dep_id int comment "department id") stored as textfile;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/import_auth_t1/temp;
dfs -rmr target/tmp/ql/test/data/exports/import_auth_t1;

export table import_auth_t1 to 'ql/test/data/exports/import_auth_t1';

dfs -touchz target/tmp/ql/test/data/exports/import_auth_t1/1.txt;
dfs -chmod 777 target/tmp/ql/test/data/exports/import_auth_t1/1.txt;
dfs -chmod 777 target/tmp/ql/test/data/exports/import_auth_t1;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/import_auth_t2/temp;

set user.name=hive_admin_user;
set role admin;

create database importer;
use importer;

show roles;

set user.name=hive_test_user;
set role public;

import table import_auth_t2 from 'ql/test/data/exports/import_auth_t1';

set user.name=hive_admin_user;
set role admin;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.stats.dbclass=fs;
set hive.security.authorization.enabled=true;
create table t1 (a int);
create index t1_index on table t1(a) as 'COMPACT' WITH DEFERRED REBUILD;
desc formatted default__t1_t1_index__;
alter index t1_index on t1 rebuild;

drop table t1;

set hive.security.authorization.enabled=false;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE  t_auth_ins(i int);

CREATE TABLE t_select(i int);
GRANT ALL ON TABLE t_select TO ROLE public;

-- grant insert privilege to another user
GRANT INSERT ON t_auth_ins TO USER userWIns;
GRANT INSERT,DELETE ON t_auth_ins TO USER userWInsAndDel;

set user.name=hive_admin_user;
set role admin;
SHOW GRANT ON TABLE t_auth_ins;


set user.name=userWIns;
INSERT INTO TABLE t_auth_ins SELECT * from t_select;

set user.name=userWInsAndDel;
INSERT OVERWRITE TABLE t_auth_ins SELECT * from t_select;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=hive_test_user;

-- check insert overwrite without delete priv
create table t1(i int);
grant insert on table t1 to user user1;

show grant user hive_test_user on table t1;

set user.name=user1;
show grant user user1 on table t1;

create table user1tab(i int);
insert overwrite table t1 select * from user1tab;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

-- check insert without select priv
create table t1(i int);

set user.name=user1;
create table user2tab(i int);
insert into table t1 select * from user2tab;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

-- check insert without select priv
create table t1(i int);

set user.name=user1;
create table t2(i int);
insert into table t2 select * from t1;

set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table if not exists authorization_invalid_v1 (key int, value string);
grant delete on table authorization_invalid_v1 to user hive_test_user;
drop table authorization_invalid_v1;



set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;

create table if not exists authorization_invalid_v2 (key int, value string);
grant index on table authorization_invalid_v2 to user hive_test_user;
drop table authorization_invalid_v2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;

set role ADMIN;
drop table if exists src_autho_test;
create table src_autho_test (id int);

create role src_role2;

grant role src_role2 to user bar;
grant role src_role2 to user `foo-1`;

show role grant user bar;
show role grant user `foo-1`;

grant select on table src_autho_test to user bar;
grant select on table src_autho_test to user `foo-1`;

show grant user bar on all;
show grant user `foo-1` on all;

drop table src_autho_test;
drop role src_role2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if alter table fails as different user
create table t1(i int);

set user.name=user2;
alter table t1 rename to tnew1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if alter table fails as different user
create table t1(i int);

set user.name=user2;
ALTER TABLE t1 SET SERDEPROPERTIES ('field.delim' = ',');
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if create table fails as different user
create table t1(i int);

set user.name=user2;
drop table t1;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

create database db1;
use db1;
-- check if create table fails as different user. use db.table sytax
create table t1(i int);
use default;

set user.name=user2;
drop table db1.t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if create table fails as different user
create table t1(i int);
create view vt1 as select * from t1;

set user.name=user2;
drop view vt1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- actions that require user to be table owner
create table t1(i int);

ALTER TABLE t1 SET SERDEPROPERTIES ('field.delim' = ',');
drop table t1;

create table t1(i int);
create view vt1 as select * from t1;

drop view vt1;
alter table t1 rename to tnew1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=hive_admin_user;

set role admin;
-- create role, db, make role the owner of db
create role testrole;
grant role testrole to user hrt_1;
create database testdb;
alter database testdb set owner role testrole;
desc database testdb;

-- actions that require user to be db owner
-- create table
use testdb;
create table foobar (foo string, bar string);

-- drop db
drop database testdb cascade;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
-- SORT_BEFORE_DIFF

create table authorization_part_fail (key int, value string) partitioned by (ds string);
ALTER TABLE authorization_part_fail SET TBLPROPERTIES ("PARTITION_LEVEL_PRIVILEGE"="TRUE");
create table src_auth as select * from src;
set hive.security.authorization.enabled=true;

grant Create on table authorization_part_fail to user hive_test_user;
grant Update on table authorization_part_fail to user hive_test_user;
grant Drop on table authorization_part_fail to user hive_test_user;
grant select on table src_auth to user hive_test_user;

-- column grant to group

grant select(key) on table authorization_part_fail to group hive_test_group1;
grant select on table authorization_part_fail to group hive_test_group1;

show grant group hive_test_group1 on table authorization_part_fail;

insert overwrite table authorization_part_fail partition (ds='2010') select key, value from src_auth;
show grant group hive_test_group1 on table authorization_part_fail(key) partition (ds='2010');
show grant group hive_test_group1 on table authorization_part_fail partition (ds='2010');
select key, value from authorization_part_fail where ds='2010' order by key limit 20;

insert overwrite table authorization_part_fail partition (ds='2011') select key, value from src_auth;
show grant group hive_test_group1 on table authorization_part_fail(key) partition (ds='2011');
show grant group hive_test_group1 on table authorization_part_fail partition (ds='2011');
select key, value from authorization_part_fail where ds='2011' order by key limit 20;

select key,value, ds from authorization_part_fail where ds>='2010' order by key, ds limit 20;

revoke select on table authorization_part_fail partition (ds='2010') from group hive_test_group1;

select key,value, ds from authorization_part_fail where ds>='2010' order by key, ds limit 20;

drop table authorization_part_fail;
drop table src_auth;
set hive.mapred.mode=nonstrict;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_uri_add_part1;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_uri_add_part2;




-- check add partition without insert privilege
create table tpart(i int, j int) partitioned by (k string);

alter table tpart add partition (k = '1') location '${system:test.tmp.dir}/a_uri_add_part1/';
alter table tpart add partition (k = '2') location '${system:test.tmp.dir}/a_uri_add_part2/';

select count(*) from tpart;

analyze table tpart partition (k) compute statistics;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;

-- the test verifies that authorization is happening with privileges of the current roles

-- grant privileges with grant option for table to role2
create role role2;
grant role role2 to user user2;
create table tpriv_current_role(i int);
grant all on table tpriv_current_role to role role2 with grant option;

set user.name=user2;
-- switch to user2

-- by default all roles should be in current roles, and grant to new user should work
show current roles;
grant all on table tpriv_current_role to user user3;

set role role2;
-- switch to role2, grant should work
grant all on table tpriv_current_role to user user4;

set user.name=user4;
show grant user user4 on table tpriv_current_role;
set user.name=user2;

set role PUBLIC;
-- set role to public, should fail as role2 is not one of the current roles
grant all on table tpriv_current_role to user user5;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create role public;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
drop role public;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set hive.metastore.server.min.threads=101;
set hive.metastore.server.min.threads;
reset;
set hive.metastore.server.min.threads;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE table_priv_rfail1(i int);

-- grant insert to user2
GRANT INSERT ON table_priv_rfail1 TO USER user2;

set user.name=user3;
-- try dropping the privilege as user3
REVOKE INSERT ON TABLE table_priv_rfail1 FROM USER user2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE table_priv_rfai2(i int);

-- grant insert to user2
GRANT INSERT ON table_priv_rfai2 TO USER user2;
GRANT SELECT ON table_priv_rfai2 TO USER user3 WITH GRANT OPTION;

set user.name=user3;
-- grant select as user3 to user 2
GRANT SELECT ON table_priv_rfai2 TO USER user2;

-- try dropping the privilege as user3
REVOKE INSERT ON TABLE table_priv_rfai2 FROM USER user2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE table_priv_rev(i int);

-- grant insert privilege to user2
GRANT INSERT ON table_priv_rev TO USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
SHOW GRANT USER user2 ON ALL;
set user.name=user1;

-- revoke insert privilege from user2
REVOKE INSERT ON TABLE table_priv_rev FROM USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set user.name=user1;

-- grant all privileges one at a time --
-- grant insert privilege to user2
GRANT INSERT ON table_priv_rev TO USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
SHOW GRANT USER user2 ON ALL;
set user.name=user1;

-- grant select privilege to user2, with grant option
GRANT SELECT ON table_priv_rev TO USER user2 WITH GRANT OPTION;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set user.name=user1;

-- grant update privilege to user2
GRANT UPDATE ON table_priv_rev TO USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set user.name=user1;

-- grant delete privilege to user2
GRANT DELETE ON table_priv_rev TO USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set user.name=user1;

-- start revoking --
-- revoke update privilege from user2
REVOKE UPDATE ON TABLE table_priv_rev FROM USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
SHOW GRANT USER user2 ON ALL;
set user.name=user1;

-- revoke DELETE privilege from user2
REVOKE DELETE ON TABLE table_priv_rev FROM USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set user.name=user1;

-- revoke insert privilege from user2
REVOKE INSERT ON TABLE table_priv_rev FROM USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set user.name=user1;

-- revoke grant option for select privilege from user2
REVOKE GRANT OPTION FOR SELECT ON TABLE table_priv_rev FROM USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set user.name=user1;

-- revoke select privilege from user2
REVOKE SELECT ON TABLE table_priv_rev FROM USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
SHOW GRANT USER user2 ON ALL;
set user.name=user1;

-- grant all followed by revoke all
GRANT ALL ON table_priv_rev TO USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set user.name=user1;

REVOKE ALL ON TABLE table_priv_rev FROM USER user2;

set user.name=user2;
SHOW GRANT USER user2 ON TABLE table_priv_rev;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=hive_admin_user;
show current roles;
set role ADMIN;

----------
-- create the following user, role mapping
-- user1 -> role1 -> role2 -> role3
----------

create role role1;
grant role1 to user user1;

create role role2;
grant role2 to role role1;

create role role3;
grant role3 to role role2;


create table t1(i int);
grant select on t1 to role role3;

set user.name=user1;
show current roles;
select * from t1;

set user.name=hive_admin_user;
show current roles;
grant select on t1 to role role2;


set user.name=user1;
show current roles;
select * from t1;

set user.name=hive_admin_user;
set role ADMIN;
show current roles;
revoke select on table t1 from role role2;


create role role4;
grant role4 to user user1;
grant role3 to role role4;;

set user.name=user1;
show current roles;
select * from t1;

set user.name=hive_admin_user;
show current roles;
set role ADMIN;

-- Revoke role3 from hierarchy one at a time and check permissions
-- after revoking from both, select should fail
revoke role3 from role role2;

set user.name=user1;
show current roles;
select * from t1;

set user.name=hive_admin_user;
show current roles;
set role ADMIN;
revoke role3 from role role4;

set user.name=user1;
show current roles;
select * from t1;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create role mixCaseRole1;
create role mixCaseRole2;

show roles;


create table t1(i int);
grant SELECT  on table t1 to role mixCaseRole1;
-- grant with wrong case should fail with legacy auth
grant UPDATE  on table t1 to role mixcaserole2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;
-- this is applicable to any security mode as check is in metastore
create role role1;
create role role2;
grant role role1 to role role2;

-- this will create a cycle
grant role role2 to role role1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=hive_admin_user;
set role ADMIN;
-- this is applicable to any security mode as check is in metastore

create role role1;

create role role2;
grant role role2 to role role1;

create role role3;
grant role role3 to role role2;

create role role4;
grant role role4 to role role3;

create role role5;
grant role role5 to role role4;

-- this will create a cycle in middle of the hierarchy
grant role role2 to role role4;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;

set role ADMIN;

----------------------------------------
-- role granting with admin option
-- since user2 doesn't have admin option for role_noadmin, last grant should fail
----------------------------------------

create role role_noadmin;
create role src_role_wadmin;
grant  src_role_wadmin to user user2 with admin option;
grant  role_noadmin to user user2;
show role grant user user2;


set user.name=user2;
set role role_noadmin;
grant  src_role_wadmin to user user3;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;

-- enable sql standard authorization
-- role granting without role keyword
-- also test role being treated as case insensitive
set role ADMIN;
create role src_Role2;

grant SRC_role2 to user user2 ;
show role grant user user2;
show roles;

-- revoke role without role keyword
revoke src_rolE2 from user user2;
show role grant user user2;
show roles;

----------------------------------------
-- role granting without role keyword, with admin option (syntax check)
----------------------------------------

create role src_role_wadmin;
grant src_role_wadmin to user user2 with admin option;
show role grant user user2;

-- revoke admin option
revoke admin option for src_role_wadmin from user user2;
show role grant user user2;

-- revoke role without role keyword
revoke src_role_wadmin from user user2;
show role grant user user2;

-- drop roles
show roles;
drop role Src_role2;
show roles;
drop role sRc_role_wadmin;
show roles;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set hive.cli.print.header=true;
set user.name=hive_admin_user;
set role ADMIN;

----------------------------------------
-- role granting with admin option
----------------------------------------
-- Also test case sensitivity of role name

create role srC_role_wadmin;
create role src_roLe2;
grant src_role_wadmin to user user2 with admin option;
show role grant user user2;
show principals src_role_wadmin;


set user.name=user2;
set role src_role_WadMin;
show principals src_role_wadmin;
-- grant role to another user
grant src_Role_wadmin to user user3;

set user.name=user3;
show role grant user user3;

set user.name=user2;
-- grant role to another role
grant src_role_wadmin to role sRc_role2;

set user.name=hive_admin_user;
set role ADMIn;
grant src_role2 to user user3;

set user.name=user3;
-- as user3 belings to src_role2 hierarchy, its should be able to run show grant on it
show role grant role src_Role2;

set user.name=hive_admin_user;
set role ADMIN;



show principals src_ROle_wadmin;

set user.name=user2;
set role src_role_wadmin;
-- revoke user from role
revoke src_rolE_wadmin from user user3;

set user.name=user3;
show role grant user user3;
set user.name=user2;

-- revoke role from role
revoke src_rolE_wadmin from role sRc_role2;
set user.name=hive_admin_user;
set role ADMIN;

show role grant role sRc_role2;

show principals src_role_wadmin;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;

set role ADMIN;

----------------------------------------
-- grant role with admin option, then revoke admin option
-- once the admin option has been revoked, last grant should fail
----------------------------------------

create role src_role_wadmin;
grant  src_role_wadmin to user user2 with admin option;
show role grant user user2;


set user.name=user2;
set role src_role_wadmin;
grant  src_role_wadmin to user user3;
revoke src_role_wadmin from user user3;

set user.name=hive_admin_user;
set role ADMIN;
revoke admin option for src_role_wadmin from user user2;
show role grant user user2;
set user.name=user2;
set role src_role_wadmin;
-- grant/revoke should now fail
grant  src_role_wadmin to user user3;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;

set role ADMIN;

----------------------------------------
-- granting role to a role that does not exist should fail
----------------------------------------

create role role1;
grant role1 to role nosuchrole;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=hive_admin_user;
set role ADMIN;

create role accounting;

set user.name=user1;
-- user does not belong to this role, so the show role grant should fail
show role grant role accounting;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set user.name=ruser1;
show role grant user ruser1;

set user.name=hive_admin_user;
set role ADMIN;
show role grant user ruser1;
show role grant user ruser2;

set user.name=ruser1;
--  show role grant for another user as non admin user should fail
show role grant user ruser2;
set hive.metastore.pre.event.listeners=org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener;
set hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_sba_droptab1;

create table t1(i int) location '${system:test.tmp.dir}/a_sba_droptab1';
dfs -chmod 555 ${system:test.tmp.dir}/a_sba_droptab1;
-- Attempt to drop table without having write permissions on table dir should result in error
drop table t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

-- check query without select privilege fails
create table t1(i int);

set user.name=user1;
select * from t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

-- check create view without select privileges
create table t1(i int);
create view v1 as select * from t1;
set user.name=user1;
select * from v1;


set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.enabled=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;

-- run a sql query to initialize authorization, then try setting a allowed config and then a disallowed config param
use default;
set hive.optimize.listbucketing=true;
set hive.security.authorization.enabled=true;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.enabled=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory;

-- run a sql query to initialize authorization, then try setting a non-existent config param
use default;
set hive.exec.reduce.max=1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;

-- an error should be thrown if 'set role ' is done for role that does not exist

set role nosuchroleexists;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;

-- an error should be thrown if 'set role ' is done for role that does not exist

create role rset_role_neg;
grant role rset_role_neg to user user2;

set user.name=user2;
set role rset_role_neg;
set role public;
set role nosuchroleexists;;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set user.name=hive_admin_user;
set role ADMIN;
show current roles;

create role r1;
grant role r1 to user hive_admin_user;
set role r1;
show current roles;

set role PUBLIC;
show current roles;

set role NONE;
show current roles;

set role ALL;
show current roles;

set role ADMIN;
drop role r1;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

create database db1;
use db1;
-- check query without select privilege fails
create table t1(i int);

set user.name=user1;
show columns in t1;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=hive_admin_user;
set role admin;

-- test show grant authorization

create role roleA;
create role roleB;

grant role roleA to user userA;
grant role roleB to role roleA;

set user.name=user1;

-- create table and grant privileges to a role
create table t1(i int, j int, k int);
create table t2(i int, j int, k int);

grant select on t1 to role roleA;
grant insert on t2 to role roleA;
grant insert on t2 to role roleB;

grant insert,delete on t1 to user userA;
grant select,insert on t2 to user userA;


set user.name=hive_admin_user;
set role admin;

-- as user in admin role, it should be possible to see other users grant
show grant user user1 on table t1;
show grant user user1;
show grant role roleA on table t1;
show grant role roleA;
show grant;


set user.name=userA;
-- user belonging to role should be able to see it
show grant role roleA on table t1;
show grant role roleA;

show grant role roleB on table t1;
show grant role roleB;

show grant user userA on table t1;
show grant user userA;


set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=hive_admin_user;
set role admin;
create role role1;


set user.name=user1;
show grant role role1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=user1;
show grant;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=user1;
show grant user user2;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set user.name=user1;
create table t1(i int, j int, k int);

show grant user user2 on table t1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- check if alter table fails as different user
create table t_show_parts(i int) partitioned by (j string);

set user.name=user2;
show partitions t_show_parts;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
-- This test will fail because hive_test_user is not in admin role
show roles;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
-- This test will fail because hive_test_user is not in admin role
show principals role1;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create role role1;
grant role1 to user user1 with admin option;
grant role1 to user user2 with admin option;
show role grant user user1;
show role grant user user2;
show principals role1;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

----------------------------------------
-- granting object privilege to a role that does not exist should fail
----------------------------------------
create table t1(i int);
grant ALL on t1 to role nosuchrole;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

-- check add partition without insert privilege
create table t1(i int, j int);
set user.name=user1;
truncate table t1;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


set user.name=user1;
-- current user has been set (comment line before the set cmd is resulting in parse error!!)

CREATE TABLE t_auth_up(i int, j int) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

CREATE TABLE t_select(i int);
GRANT ALL ON TABLE t_select TO ROLE public;

-- grant update privilege to another user
GRANT UPDATE ON t_auth_up TO USER userWIns;
GRANT SELECT ON t_auth_up TO USER userWIns;

set user.name=hive_admin_user;
set role admin;
SHOW GRANT ON TABLE t_auth_up;


set user.name=userWIns;
update t_auth_up set j = 0 where i > 0;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;



-- check update without update priv
create table auth_noupd(i int, j int) clustered by (j) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

set user.name=user1;
update auth_noupd set i = 0 where i > 0;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;



set user.name=user1;
create table auth_noupd(i int, j int) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
update auth_noupd set j = 0 where i > 0;

set user.name=hive_admin_user;
set role admin;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_uri_add_part;
dfs -touchz ${system:test.tmp.dir}/a_uri_add_part/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/a_uri_add_part/1.txt;

create table tpart(i int, j int) partitioned by (k string);
alter table tpart add partition (k = 'abc') location '${system:test.tmp.dir}/a_uri_add_part/';
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/az_uri_alterpart_loc_perm;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/az_uri_alterpart_loc;
dfs -touchz ${system:test.tmp.dir}/az_uri_alterpart_loc/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/az_uri_alterpart_loc/1.txt;

create table tpart(i int, j int) partitioned by (k string);
alter table tpart add partition (k = 'abc') location '${system:test.tmp.dir}/az_uri_alterpart_loc_perm/';

alter table tpart partition (k = 'abc') set location '${system:test.tmp.dir}/az_uri_alterpart_loc/';


-- Attempt to set partition to location without permissions should fail
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/az_uri_altertab_setloc;
dfs -touchz ${system:test.tmp.dir}/az_uri_altertab_setloc/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/az_uri_altertab_setloc/1.txt;

create table t1(i int);

alter table t1 set location '${system:test.tmp.dir}/az_uri_altertab_setloc/1.txt'

-- Attempt to set location of table to a location without permissions should fail
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/az_uri_createdb;
dfs -touchz ${system:test.tmp.dir}/az_uri_createdb/1.txt;
dfs -chmod 300 ${system:test.tmp.dir}/az_uri_createdb/1.txt;

create database az_test_db location '${system:test.tmp.dir}/az_uri_createdb/';

-- Attempt to create db for dir without sufficient permissions should fail

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_uri_crtab1;
dfs -touchz ${system:test.tmp.dir}/a_uri_crtab1/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/a_uri_crtab1/1.txt;

create table t1(i int) location '${system:test.tmp.dir}/a_uri_crtab1';

-- Attempt to create table with dir that does not have write permission should fail
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/a_uri_crtab_ext;
dfs -touchz ${system:test.tmp.dir}/a_uri_crtab_ext/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/a_uri_crtab_ext/1.txt;

create external table t1(i int) location '${system:test.tmp.dir}/a_uri_crtab_ext';

-- Attempt to create table with dir that does not have write permission should fail
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=export_auth_uri;


create table export_auth_uri ( dep_id int comment "department id")
	stored as textfile;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/export_auth_uri/temp;
dfs -rmr target/tmp/ql/test/data/exports/export_auth_uri;


dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/export_auth_uri/;
dfs -chmod 555 target/tmp/ql/test/data/exports/export_auth_uri;

export table export_auth_uri to 'ql/test/data/exports/export_auth_uri';

-- Attempt to export to location without sufficient permissions should fail
set hive.mapred.mode=nonstrict;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=import_auth_uri;


create table import_auth_uri ( dep_id int comment "department id")
	stored as textfile;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/import_auth_uri/temp;
dfs -rmr target/tmp/ql/test/data/exports/import_auth_uri;
export table import_auth_uri to 'ql/test/data/exports/import_auth_uri';
drop table import_auth_uri;

dfs -touchz target/tmp/ql/test/data/exports/import_auth_uri/1.txt;
dfs -chmod 555 target/tmp/ql/test/data/exports/import_auth_uri/1.txt;

create database importer;
use importer;

import from 'ql/test/data/exports/import_auth_uri';

-- Attempt to import from location without sufficient permissions should fail
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/az_uri_index;
dfs -touchz ${system:test.tmp.dir}/az_uri_index/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/az_uri_index/1.txt;


create table t1(i int);
create index idt1 on table t1 (i) as 'COMPACT' WITH DEFERRED REBUILD LOCATION '${system:test.tmp.dir}/az_uri_index/';

-- Attempt to use location for index that does not have permissions should fail
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/az_uri_insert;
dfs -touchz ${system:test.tmp.dir}/az_uri_insert/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/az_uri_insert/1.txt;

create table t1(i int, j int);

insert overwrite directory '${system:test.tmp.dir}/az_uri_insert/' select * from t1;

-- Attempt to insert into uri without permissions should fail

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/az_uri_insert_local;
dfs -touchz ${system:test.tmp.dir}/az_uri_insert_local/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/az_uri_insert_local/1.txt;

create table t1(i int, j int);

insert overwrite local directory '${system:test.tmp.dir}/az_uri_insert_local/' select * from t1;

-- Attempt to insert into uri without permissions should fail

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/authz_uri_load_data;
dfs -touchz ${system:test.tmp.dir}/authz_uri_load_data/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/authz_uri_load_data/1.txt;

create table t1(i int);
load data inpath 'pfile:${system:test.tmp.dir}/authz_uri_load_data/' overwrite into table t1;

set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;
set hive.security.authorization.enabled=true;
set user.name=user1;

-- Test view authorization , and 'show grant' variants

create table t1(i int, j int, k int);
show grant user user1 on table t1;

-- protecting certain columns
create view vt1 as select i,k from t1;

-- protecting certain rows
create view vt2 as select * from t1 where i > 1;

show grant user user1 on all;

--view grant to user
-- try with and without table keyword

grant select on vt1 to user user2;
grant insert on table vt1 to user user3;

set user.name=user2;
show grant user user2 on table vt1;
set user.name=user3;
show grant user user3 on table vt1;


set user.name=user2;

explain authorization select * from vt1;
select * from vt1;

-- verify input objects required does not include table
-- even if view is within a sub query
select * from (select * from vt1) a;

set user.name=user1;

grant all on table vt2 to user user2;

set user.name=user2;
show grant user user2 on table vt2;
show grant user user2 on all;
set user.name=user1;

revoke all on vt2 from user user2;

set user.name=user2;
show grant user user2 on table vt2;


set user.name=hive_admin_user;
set role admin;
show grant on table vt2;

set user.name=user1;
revoke select on table vt1 from user user2;

set user.name=user2;
show grant user user2 on table vt1;
show grant user user2 on all;

set user.name=user3;
-- grant privileges on roles for view, after next statement
show grant user user3 on table vt1;

set user.name=hive_admin_user;
show current roles;
set role ADMIN;
create role role_v;
grant  role_v to user user4 ;
show role grant user user4;
show roles;

grant all on table vt2 to role role_v;
show grant role role_v on table vt2;

revoke delete on table vt2 from role role_v;
show grant role role_v on table vt2;
show grant on table vt2;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
grant role public to user hive_test_user;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
revoke role public from user hive_test_user;
set hive.mapred.mode=nonstrict;
CREATE TEMPORARY FUNCTION test_max AS 'org.apache.hadoop.hive.ql.udf.UDAFTestMax';

create table dest_grouped_old1 as select 1+1, 2+2 as zz, src.key, test_max(length(src.value)), count(src.value), sin(count(src.value)), count(sin(src.value)), current_timestamp(), CAST(SUM(IF(value > 10, value, 1)) AS INT), if(src.key > 1,
1,
0)
 from src group by src.key;
describe dest_grouped_old1;

create table dest_grouped_old2 as select distinct src.key from src;
describe dest_grouped_old2;

set hive.autogen.columnalias.prefix.label=column_;
set hive.autogen.columnalias.prefix.includefuncname=true;

create table dest_grouped_new1 as select 1+1, 2+2 as zz, ((src.key % 2)+2)/2, test_max(length(src.value)), count(src.value), sin(count(src.value)), count(sin(src.value)), current_timestamp(), CAST(SUM(IF(value > 10, value, 1)) AS INT), if(src.key > 10,
	(src.key +5) % 2,
0)
from src group by src.key;
describe dest_grouped_new1;

create table dest_grouped_new2 as select distinct src.key from src;
describe dest_grouped_new2;

-- Drop the temporary function at the end till HIVE-3160 gets fixed
DROP TEMPORARY FUNCTION test_max;
set mapred.job.tracker=abracadabra;
set hive.exec.mode.local.auto.inputbytes.max=1;
set hive.exec.mode.local.auto=true;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20)
-- hadoop0.23 changes the behavior of JobClient initialization
-- in hadoop0.20, JobClient initialization tries to get JobTracker's address
-- this throws the expected IllegalArgumentException
-- in hadoop0.23, JobClient initialization only initializes cluster
-- and get user group information
-- not attempts to get JobTracker's address
-- no IllegalArgumentException thrown in JobClient Initialization
-- an exception is thrown when JobClient submitJob

SELECT key FROM src;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join = true;

-- SORT_QUERY_RESULTS

explain
select sum(hash(a.k1,a.v1,a.k2, a.v2))
from (
SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10 order by src.key, src.value) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10 order by src.key, src.value) src2
  SORT BY k1, v1, k2, v2
) a;

select sum(hash(a.k1,a.v1,a.k2, a.v2))
from (
SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10 order by src.key, src.value) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10 order by src.key, src.value) src2
  SORT BY k1, v1, k2, v2
) a;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join =true;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;

explain
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

SELECT sum(hash(dest_j1.key,dest_j1.value)) FROM dest_j1;set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

explain
FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

explain
SELECT sum(hash(src1.c1, src2.c4))
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100;

SELECT sum(hash(src1.c1, src2.c4))
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100;
set hive.mapred.mode=nonstrict;


set hive.auto.convert.join = true;


explain
SELECT sum(hash(src1.c1, src2.c4))
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 = src3.c5 AND src3.c5 < 80;

SELECT sum(hash(src1.c1, src2.c4))
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 = src3.c5 AND src3.c5 < 80;
set hive.mapred.mode=nonstrict;

set hive.auto.convert.join = true;

explain
SELECT sum(hash(src1.c1, src2.c4))
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 + src2.c3 = src3.c5 AND src3.c5 < 200;

SELECT sum(hash(src1.c1, src2.c4))
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 + src2.c3 = src3.c5 AND src3.c5 < 200;
set hive.mapred.mode=nonstrict;

set hive.auto.convert.join = true;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)

CREATE TABLE dest1(c1 INT, c2 STRING) STORED AS TEXTFILE;

set mapreduce.framework.name=yarn;
set mapreduce.jobtracker.address=localhost:58;
set hive.exec.mode.local.auto=true;

explain
FROM src JOIN srcpart ON src.key = srcpart.key AND srcpart.ds = '2008-04-08' and src.key > 100
INSERT OVERWRITE TABLE dest1 SELECT src.key, srcpart.value;

FROM src JOIN srcpart ON src.key = srcpart.key AND srcpart.ds = '2008-04-08' and src.key > 100
INSERT OVERWRITE TABLE dest1 SELECT src.key, srcpart.value;

SELECT sum(hash(dest1.c1,dest1.c2)) FROM dest1;

set hive.auto.convert.join = true;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)

CREATE TABLE dest1(c1 INT, c2 STRING) STORED AS TEXTFILE;

set mapred.job.tracker=localhost:58;
set hive.exec.mode.local.auto=true;

explain
FROM src JOIN srcpart ON src.key = srcpart.key AND srcpart.ds = '2008-04-08' and src.key > 100
INSERT OVERWRITE TABLE dest1 SELECT src.key, srcpart.value;

FROM src JOIN srcpart ON src.key = srcpart.key AND srcpart.ds = '2008-04-08' and src.key > 100
INSERT OVERWRITE TABLE dest1 SELECT src.key, srcpart.value;

SELECT sum(hash(dest1.c1,dest1.c2)) FROM dest1;

set hive.auto.convert.join = true;

explain
select sum(hash(a.k1,a.v1,a.k2, a.v2))
from (
SELECT src1.key as k1, src1.value as v1, src2.key as k2, src2.value as v2
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
SORT BY k1, v1, k2, v2
) a;


select sum(hash(a.k1,a.v1,a.k2, a.v2))
from (
SELECT src1.key as k1, src1.value as v1, src2.key as k2, src2.value as v2
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
SORT BY k1, v1, k2, v2
) a;

set hive.mapred.mode=nonstrict;

set hive.auto.convert.join = true;


explain
SELECT sum(hash(subq.key, tab.value))
FROM
(select a.key, a.value from src a where a.key > 10 ) subq
JOIN src tab
ON (subq.key = tab.key and subq.key > 20 and subq.value = tab.value)
where tab.value < 200;

SELECT sum(hash(subq.key, tab.value))
FROM
(select a.key, a.value from src a where a.key > 10 ) subq
JOIN src tab
ON (subq.key = tab.key and subq.key > 20 and subq.value = tab.value)
where tab.value < 200;set hive.mapred.mode=nonstrict;

set hive.auto.convert.join = true;

CREATE TABLE dest1(key1 INT, value1 STRING, key2 INT, value2 STRING) STORED AS TEXTFILE;

explain
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.*, src2.*;


FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.*, src2.*;

SELECT sum(hash(dest1.key1,dest1.value1,dest1.key2,dest1.value2)) FROM dest1;set hive.mapred.mode=nonstrict;

set hive.auto.convert.join = true;
explain
 SELECT sum(hash(a.key, a.value, b.key, b.value))
 FROM
  (
  SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key
  ) a
 FULL OUTER JOIN
 (
  SELECT src2.key as key, count(distinct(src2.value)) AS value
  FROM src1 src2 group by src2.key
 ) b
 ON (a.key = b.key);


 SELECT sum(hash(a.key, a.value, b.key, b.value))
 FROM
  (
  SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key
  ) a
 FULL OUTER JOIN
 (
  SELECT src2.key as key, count(distinct(src2.value)) AS value
  FROM src1 src2 group by src2.key
 ) b
 ON (a.key = b.key);
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

explain
 SELECT sum(hash(a.key, a.value, b.key, b.value1,  b.value2))
 FROM
  (
  SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key
  ) a
 FULL OUTER JOIN
 (
  SELECT src2.key as key, count(distinct(src2.value)) AS value1,
  count(distinct(src2.key)) AS value2
  FROM src1 src2 group by src2.key
 ) b
 ON (a.key = b.key);


 SELECT sum(hash(a.key, a.value, b.key, b.value1,  b.value2))
 FROM
  (
  SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key
  ) a
 FULL OUTER JOIN
 (
  SELECT src2.key as key, count(distinct(src2.value)) AS value1,
  count(distinct(src2.key)) AS value2
  FROM src1 src2 group by src2.key
 ) b
 ON (a.key = b.key);
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

explain
FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value
where (src1.ds = '2008-04-08' or src1.ds = '2008-04-09' )and (src1.hr = '12' or src1.hr = '11');


FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value
where (src1.ds = '2008-04-08' or src1.ds = '2008-04-09' )and (src1.hr = '12' or src1.hr = '11');


SELECT sum(hash(dest1.key,dest1.value)) FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest_j2(key INT, value STRING) STORED AS TEXTFILE;

explain
FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;


FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;

SELECT sum(hash(dest_j2.key,dest_j2.value)) FROM dest_j2;
set hive.auto.convert.join = true;

explain
select sum(hash(a.k1,a.v1,a.k2,a.v2,a.k3,a.v3))
from (
SELECT src1.key as k1, src1.value as v1, src2.key as k2, src2.value as v2 , src3.key as k3, src3.value as v3
FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY k1,v1,k2,v2,k3,v3
)a;

select sum(hash(a.k1,a.v1,a.k2,a.v2,a.k3,a.v3))
from (
SELECT src1.key as k1, src1.value as v1, src2.key as k2, src2.value as v2 , src3.key as k3, src3.value as v3
FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY k1,v1,k2,v2,k3,v3
)a;

explain
select sum(hash(a.k1,a.v1,a.k2,a.v2,a.k3,a.v3))
from (
SELECT src1.key as k1, src1.value as v1, src2.key as k2, src2.value as v2 , src3.key as k3, src3.value as v3
FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key < 15) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY k1,v1,k2,v2,k3,v3
)a;

select sum(hash(a.k1,a.v1,a.k2,a.v2,a.k3,a.v3))
from (
SELECT src1.key as k1, src1.value as v1, src2.key as k2, src2.value as v2 , src3.key as k3, src3.value as v3
FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key < 15) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY k1,v1,k2,v2,k3,v3
)a;
set hive.explain.user=false;
set hive.auto.convert.join = true;

-- SORT_QUERY_RESULTS

explain
SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;
explain
SELECT sum(hash(src5.src1_value)) FROM (SELECT src3.*, src4.value as src4_value, src4.key as src4_key FROM src src4 JOIN (SELECT src2.*, src1.key as src1_key, src1.value as src1_value FROM src src1 JOIN src src2 ON src1.key = src2.key) src3 ON src3.src1_key = src4.key) src5;

SELECT sum(hash(src5.src1_value)) FROM (SELECT src3.*, src4.value as src4_value, src4.key as src4_key FROM src src4 JOIN (SELECT src2.*, src1.key as src1_key, src1.value as src1_value FROM src src1 JOIN src src2 ON src1.key = src2.key) src3 ON src3.src1_key = src4.key) src5;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

-- SORT_QUERY_RESULTS

explain
SELECT  *  FROM src src1 JOIN src src2 WHERE src1.key < 10 and src2.key < 10 SORT BY src1.key, src1.value, src2.key, src2.value;

SELECT  *  FROM src src1 JOIN src src2 WHERE src1.key < 10 and src2.key < 10 SORT BY src1.key, src1.value, src2.key, src2.value;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

create table tst1(key STRING, cnt INT);

INSERT OVERWRITE TABLE tst1
SELECT a.key, count(1) FROM src a group by a.key;

explain
SELECT sum(a.cnt)  FROM tst1 a JOIN tst1 b ON a.key = b.key;

SELECT sum(a.cnt)  FROM tst1 a JOIN tst1 b ON a.key = b.key;


set hive.mapred.mode=nonstrict;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter,org.apache.hadoop.hive.ql.hooks.PrintCompletedTasksHook;

set hive.auto.convert.join = true;
set hive.mapjoin.localtask.max.memory.usage = 0.0001;
set hive.mapjoin.check.memory.rows = 2;
set hive.auto.convert.join.noconditionaltask = false;

-- This test tests the scenario when the mapper dies. So, create a conditional task for the mapjoin
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value
where (src1.ds = '2008-04-08' or src1.ds = '2008-04-09' )and (src1.hr = '12' or src1.hr = '11');

SELECT sum(hash(dest1.key,dest1.value)) FROM dest1;



CREATE TABLE dest_j2(key INT, value STRING) STORED AS TEXTFILE;

FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;

SELECT sum(hash(dest_j2.key,dest_j2.value)) FROM dest_j2;

CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;

FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

SELECT sum(hash(dest_j1.key,dest_j1.value)) FROM dest_j1;

set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, cnt INT);
set hive.auto.convert.join = true;
EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;

INSERT OVERWRITE TABLE dest_j1
SELECT  x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;

select * from dest_j1;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

explain
SELECT count(1)
FROM
(
SELECT src.key, src.value from src
UNION ALL
SELECT DISTINCT src.key, src.value from src
) src_12
JOIN
(SELECT src.key as k, src.value as v from src) src3
ON src_12.key = src3.k AND src3.k < 200;


SELECT count(1)
FROM
(
SELECT src.key, src.value from src
UNION ALL
SELECT DISTINCT src.key, src.value from src
) src_12
JOIN
(SELECT src.key as k, src.value as v from src) src3
ON src_12.key = src3.k AND src3.k < 200;
set hive.mapjoin.smalltable.filesize = 1;

set hive.auto.convert.join = true;
explain
SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) LEFT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) LEFT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;
set hive.explain.user=false;
set hive.auto.convert.join = true;

-- SORT_QUERY_RESULTS

explain
SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) LEFT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) LEFT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) LEFT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) LEFT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) LEFT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) LEFT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

explain
FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key = src3.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;

FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key = src3.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;

SELECT sum(hash(dest1.key,dest1.value)) FROM dest1;set hive.explain.user=false;
set hive.auto.convert.join = true;

explain
FROM
(SELECT src.* FROM src sort by key) x
JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT src.* FROM src sort by key) x
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT src.* FROM src sort by key) x
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT src.* FROM src sort by key) x
JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT src.* FROM src sort by key) x
JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT src.* FROM src sort by key) x
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT src.* FROM src sort by key) x
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
LEFT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT src.* FROM src sort by key) x
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));
set hive.auto.convert.join = true;

explain
FROM
(SELECT src.* FROM src sort by key) x
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT src.* FROM src sort by key) x
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (x.key = Y.key)
JOIN
(SELECT src.* FROM src sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;

-- empty tables
create table studenttab10k (name string, age int, gpa double);
create table votertab10k (name string, age int, registration string, contributions float);

explain select s.name, count(distinct registration)
from studenttab10k s join votertab10k v
on (s.name = v.name)
group by s.name;

select s.name, count(distinct registration)
from studenttab10k s join votertab10k v
on (s.name = v.name)
group by s.name;

set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;
set hive.auto.convert.sortmerge.join=true;

-- smb
create table studenttab10k_smb (name string, age int, gpa double) clustered by (name) sorted by (name) into 2 buckets;
create table votertab10k_smb (name string, age int, registration string, contributions float) clustered by (name) sorted by (name) into 2 buckets;

explain select s.name, count(distinct registration)
from studenttab10k_smb s join votertab10k_smb v
on (s.name = v.name)
group by s.name;

select s.name, count(distinct registration)
from studenttab10k_smb s join votertab10k_smb v
on (s.name = v.name)
group by s.name;

load data local inpath '../../data/files/empty1.txt' into table studenttab10k_smb;
load data local inpath '../../data/files/empty2.txt' into table studenttab10k_smb;
load data local inpath '../../data/files/empty1.txt' into table votertab10k_smb;
load data local inpath '../../data/files/empty2.txt' into table votertab10k_smb;

explain select s.name, count(distinct registration)
from studenttab10k_smb s join votertab10k_smb v
on (s.name = v.name)
group by s.name;

select s.name, count(distinct registration)
from studenttab10k_smb s join votertab10k_smb v
on (s.name = v.name)
group by s.name;

-- smb + partitions
create table studenttab10k_part (name string, age int, gpa double) partitioned by (p string) clustered by (name) sorted by (name) into 2 buckets;
create table votertab10k_part (name string, age int, registration string, contributions float) partitioned by (p string) clustered by (name) sorted by (name) into 2 buckets;

load data local inpath '../../data/files/empty1.txt' into table studenttab10k_part partition (p='foo');
load data local inpath '../../data/files/empty2.txt' into table studenttab10k_part partition (p='foo');
load data local inpath '../../data/files/empty1.txt' into table votertab10k_part partition (p='foo');
load data local inpath '../../data/files/empty2.txt' into table votertab10k_part partition (p='foo');

explain select s.name, count(distinct registration)
from studenttab10k_part s join votertab10k_part v
on (s.name = v.name)
where s.p = 'bar'
and v.p = 'bar'
group by s.name;

select s.name, count(distinct registration)
from studenttab10k_part s join votertab10k_part v
on (s.name = v.name)
where s.p = 'bar'
and v.p = 'bar'
group by s.name;

drop table studenttab10k;
drop table votertab10k;
drop table studenttab10k_smb;
drop table votertab10k_smb;
drop table studenttab10k_part;
drop table votertab10k_part;set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

-- SORT_QUERY_RESULTS

explain
SELECT * FROM
  (SELECT * FROM src WHERE key+1 < 10) a
    JOIN
  (SELECT * FROM src WHERE key+2 < 10) b
    ON a.key+1=b.key+2;

SELECT * FROM
  (SELECT * FROM src WHERE key+1 < 10) a
    JOIN
  (SELECT * FROM src WHERE key+2 < 10) b
    ON a.key+1=b.key+2;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS TEXTFILE;

explain
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

SELECT sum(hash(dest1.c1,dest1.c2,dest1.c3,dest1.c4)) FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS TEXTFILE;

explain
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 RIGHT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 RIGHT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

SELECT sum(hash(dest1.c1,dest1.c2,dest1.c3,dest1.c4)) FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS TEXTFILE;

explain
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;


SELECT sum(hash(dest1.c1,dest1.c2,dest1.c3,dest1.c4)) FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING, c5 INT, c6 STRING) STORED AS TEXTFILE;


explain
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 LEFT OUTER JOIN
 (
  FROM src src3 SELECT src3.key AS c5, src3.value AS c6 WHERE src3.key > 20 and src3.key < 25
 ) c
 ON (a.c1 = c.c5)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4, c.c5 AS c5, c.c6 AS c6
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4, c.c5, c.c6;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 LEFT OUTER JOIN
 (
  FROM src src3 SELECT src3.key AS c5, src3.value AS c6 WHERE src3.key > 20 and src3.key < 25
 ) c
 ON (a.c1 = c.c5)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4, c.c5 AS c5, c.c6 AS c6
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4, c.c5, c.c6;


SELECT sum(hash(dest1.c1,dest1.c2,dest1.c3,dest1.c4,dest1.c5,dest1.c6)) FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS TEXTFILE;

explain
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4 where c.c3 IS NULL AND c.c1 IS NOT NULL;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4 where c.c3 IS NULL AND c.c1 IS NOT NULL;

SELECT sum(hash(dest1.c1,dest1.c2,dest1.c3,dest1.c4)) FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

explain
FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value where src1.ds = '2008-04-08' and src1.hr = '12';

FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value where src1.ds = '2008-04-08' and src1.hr = '12';



SELECT sum(hash(dest1.key,dest1.value)) FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE myinput1(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in3.txt' INTO TABLE myinput1;

SELECT sum(hash(a.key,a.value,b.key,b.value))  FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value))  FROM myinput1 a LEFT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value))  FROM myinput1 a RIGHT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value))  FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key and a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key=b.key and a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.value = c.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.key = c.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;


CREATE TABLE smb_input1(key int, value int) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE smb_input2(key int, value int) CLUSTERED BY (value) SORTED BY (value) INTO 2 BUCKETS;
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' into table smb_input1;
LOAD DATA LOCAL INPATH '../../data/files/in2.txt' into table smb_input1;
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' into table smb_input2;
LOAD DATA LOCAL INPATH '../../data/files/in2.txt' into table smb_input2;

SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
SET hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

SET hive.outerjoin.supports.filters = false;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key and a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key=b.key and a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.value = c.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.key = c.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE myinput1(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' INTO TABLE myinput1;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key=b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key and a.value=b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key=b.key and a.value = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value and a.key=b.key;

SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value) LEFT OUTER JOIN myinput1 c ON (b.value=c.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.value = c.value;

set hive.mapred.mode=nonstrict;
-- HIVE-5056 RS has expression list for values, but it's ignored in MapJoinProcessor

create table testsrc ( `key` int,`val` string);
load data local inpath '../../data/files/kv1.txt' overwrite into table testsrc;
drop table if exists orderpayment_small;
create table orderpayment_small (`dealid` int,`date` string,`time` string, `cityid` int, `userid` int);
insert overwrite table orderpayment_small select 748, '2011-03-24', '2011-03-24', 55 ,5372613 from testsrc tablesample (1 rows);
drop table if exists user_small;
create table user_small( userid int);
insert overwrite table user_small select key from testsrc tablesample (100 rows);

set hive.auto.convert.join.noconditionaltask.size = 200;
explain extended SELECT
     `dim_pay_date`.`date`
    , `deal`.`dealid`
FROM `orderpayment_small` `orderpayment`
JOIN `orderpayment_small` `dim_pay_date` ON `dim_pay_date`.`date` = `orderpayment`.`date`
JOIN `orderpayment_small` `deal` ON `deal`.`dealid` = `orderpayment`.`dealid`
JOIN `orderpayment_small` `order_city` ON `order_city`.`cityid` = `orderpayment`.`cityid`
JOIN `user_small` `user` ON `user`.`userid` = `orderpayment`.`userid`
limit 5;

SELECT
     `dim_pay_date`.`date`
    , `deal`.`dealid`
FROM `orderpayment_small` `orderpayment`
JOIN `orderpayment_small` `dim_pay_date` ON `dim_pay_date`.`date` = `orderpayment`.`date`
JOIN `orderpayment_small` `deal` ON `deal`.`dealid` = `orderpayment`.`dealid`
JOIN `orderpayment_small` `order_city` ON `order_city`.`cityid` = `orderpayment`.`cityid`
JOIN `user_small` `user` ON `user`.`userid` = `orderpayment`.`userid`
limit 5;
set hive.auto.convert.join = true;
set hive.auto.convert.join.noconditionaltask.size=2660;

-- Setting HTS(src2) < threshold < HTS(src2) + HTS(smalltable).
-- This query plan should thus not try to combine the mapjoin into a single work.

create table smalltable(key string, value string) stored as textfile;
load data local inpath '../../data/files/T1.txt' into table smalltable;
analyze table smalltable compute statistics;

explain select src1.key, src2.key, smalltable.key from src src1 JOIN src src2 ON (src1.key = src2.key) JOIN smalltable ON (src1.key + src2.key = smalltable.key);
select src1.key, src2.key, smalltable.key from src src1 JOIN src src2 ON (src1.key = src2.key) JOIN smalltable ON (src1.key + src2.key = smalltable.key);

create table smalltable2(key string, value string) stored as textfile;
load data local inpath '../../data/files/T1.txt' into table smalltable2;
analyze table smalltable compute statistics;

explain select src1.key, src2.key, smalltable.key from src src1 JOIN src src2 ON (src1.key = src2.key) JOIN smalltable ON (src1.key + src2.key = smalltable.key) JOIN smalltable2 ON (src1.key + src2.key = smalltable2.key);
select src1.key, src2.key, smalltable.key from src src1 JOIN src src2 ON (src1.key = src2.key) JOIN smalltable ON (src1.key + src2.key = smalltable.key) JOIN smalltable2 ON (src1.key + src2.key = smalltable2.key);set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

-- Auto_join2 no longer tests merging the mapjoin work if big-table selection is based on stats, as src3 is smaller statistically than src1 + src2.
-- Hence forcing the third table to be smaller.

create table smalltable(key string, value string) stored as textfile;
load data local inpath '../../data/files/T1.txt' into table smalltable;

explain select src1.key, src2.key, smalltable.key from src src1 JOIN src src2 ON (src1.key = src2.key) JOIN smalltable ON (src1.key + src2.key = smalltable.key);
select src1.key, src2.key, smalltable.key from src src1 JOIN src src2 ON (src1.key = src2.key) JOIN smalltable ON (src1.key + src2.key = smalltable.key);

create table smalltable2(key string, value string) stored as textfile;
load data local inpath '../../data/files/T1.txt' into table smalltable2;
analyze table smalltable compute statistics;

explain select src1.key, src2.key, smalltable.key from src src1 JOIN src src2 ON (src1.key = src2.key) JOIN smalltable ON (src1.key + src2.key = smalltable.key) JOIN smalltable2 ON (src1.key + src2.key = smalltable2.key);
select src1.key, src2.key, smalltable.key from src src1 JOIN src src2 ON (src1.key = src2.key) JOIN smalltable ON (src1.key + src2.key = smalltable.key) JOIN smalltable2 ON (src1.key + src2.key = smalltable2.key);set hive.mapred.mode=nonstrict;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter,org.apache.hadoop.hive.ql.hooks.PrintCompletedTasksHook;
set hive.auto.convert.join=true;
set hive.auto.convert.join.use.nonstaged=true;

set hive.auto.convert.join.noconditionaltask.size=100;

explain
select a.* from src a join src b on a.key=b.key order by key, value limit 40;

select a.* from src a join src b on a.key=b.key order by key, value limit 40;

explain
select a.* from src a join src b on a.key=b.key join src c on a.value=c.value order by a.key, a.value limit 40;

select a.* from src a join src b on a.key=b.key join src c on a.value=c.value order by a.key, a.value limit 40;

set hive.auto.convert.join.noconditionaltask.size=100;

explain
select a.* from src a join src b on a.key=b.key join src c on a.value=c.value where a.key>100 order by a.key, a.value limit 40;

select a.* from src a join src b on a.key=b.key join src c on a.value=c.value where a.key>100 order by a.key, a.value limit 40;

set hive.mapjoin.localtask.max.memory.usage = 0.0001;
set hive.mapjoin.check.memory.rows = 2;

-- fallback to common join
select a.* from src a join src b on a.key=b.key join src c on a.value=c.value where a.key>100 order by a.key, a.value limit 40;

set hive.mapred.mode=nonstrict;
;

set hive.exec.reducers.max = 1;

-- SORT_QUERY_RESULTS

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1
select * from src where key < 10;

insert overwrite table tbl2
select * from src where key < 10;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

set hive.auto.convert.sortmerge.join=true;

-- The join is being performed as part of sub-query. It should be converted to a sort-merge join
explain
select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

-- The join is being performed as part of more than one sub-query. It should be converted to a sort-merge join
explain
select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

-- A join is being performed across different sub-queries, where a join is being performed in each of them.
-- Each sub-query should be converted to a sort-merge join.
explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join, although there is more than one level of sub-query
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

-- Both the tables are nested sub-queries i.e more then 1 level of sub-query.
-- The join should be converted to a sort-merge join
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters and the join key
-- is not getting modified, it should be converted to a sort-merge join. Note that the sub-query modifies one
-- item, but that is not part of the join key.
explain
select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

-- Since the join key is modified by the sub-query, neither sort-merge join not bucketized map-side
-- join should be performed
explain
select count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

-- One of the tables is a sub-query and the other is not.
-- It should be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

-- There are more than 2 inputs to the join, all of them being sub-queries.
-- It should be converted to to a sort-merge join
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on (subq1.key = subq2.key)
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

-- The join is being performed on a nested sub-query, and an aggregation is performed after that.
-- The join should be converted to a sort-merge join
explain
select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

CREATE TABLE dest1(key int, value string);
CREATE TABLE dest2(key int, val1 string, val2 string);

-- The join is followed by a multi-table insert. It should be converted to
-- a sort-merge join
explain
from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
insert overwrite table dest1 select key, val1
insert overwrite table dest2 select key, val1, val2;

from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
insert overwrite table dest1 select key, val1
insert overwrite table dest2 select key, val1, val2;

select * from dest1;
select * from dest2;

DROP TABLE dest2;
CREATE TABLE dest2(key int, cnt int);

-- The join is followed by a multi-table insert, and one of the inserts involves a reducer.
-- It should be converted to a sort-merge join
explain
from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
insert overwrite table dest1 select key, val1
insert overwrite table dest2 select key, count(*) group by key;

from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
insert overwrite table dest1 select key, val1
insert overwrite table dest2 select key, count(*) group by key;

select * from dest1;
select * from dest2;
set hive.mapred.mode=nonstrict;
-- small 1 part, 2 bucket & big 2 part, 4 bucket

CREATE TABLE bucket_small (key string, value string) partitioned by (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

-- Since size is being used to find the big table, the order of the tables in the join does not matter
explain extended select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;
explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
;

set hive.exec.reducers.max = 1;

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1
select * from src where key < 10;

insert overwrite table tbl2
select * from src where key < 10;

set hive.auto.convert.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

set hive.auto.convert.sortmerge.join=true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
-- One of the subqueries contains a union, so it should not be converted to a sort-merge join.
explain
select count(*) from
  (
  select * from
  (select a.key as key, a.value as value from tbl1 a where key < 6
     union all
   select a.key as key, a.value as value from tbl1 a where key < 6
  ) usubq1 ) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

select count(*) from
  (
  select * from
  (select a.key as key, a.value as value from tbl1 a where key < 6
     union all
   select a.key as key, a.value as value from tbl1 a where key < 6
  ) usubq1 ) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

-- One of the subqueries contains a groupby, so it should not be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, count(*) as value from tbl1 a where key < 6 group by a.key) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, count(*) as value from tbl1 a where key < 6 group by a.key) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;
set hive.mapred.mode=nonstrict;
-- small 1 part, 2 bucket & big 2 part, 4 bucket

CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.auto.convert.join=true;

explain extended select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;

-- Since size is being used to find the big table, the order of the tables in the join does not matter
-- The tables are only bucketed and not sorted, the join should not be converted
-- Currenly, a join is only converted to a sort-merge join without a hint, automatic conversion to
-- bucketized mapjoin is not done
explain extended select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

-- The join is converted to a bucketed mapjoin with a mapjoin hint
explain extended select /*+ mapjoin(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /*+ mapjoin(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

-- HIVE-7023
explain extended select /* + MAPJOIN(a,b) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key JOIN bucket_big c ON a.key = c.key;
select /* + MAPJOIN(a,b) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key JOIN bucket_big c ON a.key = c.key;
set hive.mapred.mode=nonstrict;
-- small 1 part, 2 bucket & big 2 part, 4 bucket

CREATE TABLE bucket_small (key string, value string) partitioned by (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

CREATE TABLE bucket_medium (key string, value string) partitioned by (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 3 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_medium partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_medium partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket3outof4.txt' INTO TABLE bucket_medium partition(ds='2008-04-08');

explain extended select count(*) FROM bucket_small a JOIN bucket_medium b ON a.key = b.key JOIN bucket_big c ON c.key = b.key JOIN bucket_medium d ON c.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_medium b ON a.key = b.key JOIN bucket_big c ON c.key = b.key JOIN bucket_medium d ON c.key = b.key;
set hive.explain.user=false;
;

set hive.exec.reducers.max = 1;

-- SORT_QUERY_RESULTS

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1 select * from src where key < 10;
insert overwrite table tbl2 select * from src where key < 10;

CREATE TABLE dest1(k1 int, k2 int);
CREATE TABLE dest2(k1 string, k2 string);

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.join=true;

-- A SMB join followed by a mutli-insert
explain
from (
  SELECT a.key key1, a.value value1, b.key key2, b.value value2
  FROM tbl1 a JOIN tbl2 b
  ON a.key = b.key ) subq
INSERT OVERWRITE TABLE dest1 select key1, key2
INSERT OVERWRITE TABLE dest2 select value1, value2;

from (
  SELECT a.key key1, a.value value1, b.key key2, b.value value2
  FROM tbl1 a JOIN tbl2 b
  ON a.key = b.key ) subq
INSERT OVERWRITE TABLE dest1 select key1, key2
INSERT OVERWRITE TABLE dest2 select value1, value2;

select * from dest1;
select * from dest2;

set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=200;
set hive.mapjoin.hybridgrace.minwbsize=100;
set hive.mapjoin.hybridgrace.minnumpartitions=2;

-- A SMB join followed by a mutli-insert
explain
from (
  SELECT a.key key1, a.value value1, b.key key2, b.value value2
  FROM tbl1 a JOIN tbl2 b
  ON a.key = b.key ) subq
INSERT OVERWRITE TABLE dest1 select key1, key2
INSERT OVERWRITE TABLE dest2 select value1, value2;

from (
  SELECT a.key key1, a.value value1, b.key key2, b.value value2
  FROM tbl1 a JOIN tbl2 b
  ON a.key = b.key ) subq
INSERT OVERWRITE TABLE dest1 select key1, key2
INSERT OVERWRITE TABLE dest2 select value1, value2;

select * from dest1;
select * from dest2;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;
-- A SMB join followed by a mutli-insert
explain
from (
  SELECT a.key key1, a.value value1, b.key key2, b.value value2
  FROM tbl1 a JOIN tbl2 b
  ON a.key = b.key ) subq
INSERT OVERWRITE TABLE dest1 select key1, key2
INSERT OVERWRITE TABLE dest2 select value1, value2;

from (
  SELECT a.key key1, a.value value1, b.key key2, b.value value2
  FROM tbl1 a JOIN tbl2 b
  ON a.key = b.key ) subq
INSERT OVERWRITE TABLE dest1 select key1, key2
INSERT OVERWRITE TABLE dest2 select value1, value2;

select * from dest1;
select * from dest2;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
;

set hive.exec.reducers.max = 1;

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1 select * from src where key < 20;
insert overwrite table tbl2 select * from src where key < 10;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.to.mapjoin=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.join=true;

-- Since tbl1 is the bigger table, tbl1 Left Outer Join tbl2 can be performed
explain
select count(*) FROM tbl1 a LEFT OUTER JOIN tbl2 b ON a.key = b.key;
select count(*) FROM tbl1 a LEFT OUTER JOIN tbl2 b ON a.key = b.key;

insert overwrite table tbl2 select * from src where key < 200;

-- Since tbl2 is the bigger table, tbl1 Right Outer Join tbl2 can be performed
explain
select count(*) FROM tbl1 a RIGHT OUTER JOIN tbl2 b ON a.key = b.key;
select count(*) FROM tbl1 a RIGHT OUTER JOIN tbl2 b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
;

set hive.exec.reducers.max = 1;

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1 select * from src where key < 20;
insert overwrite table tbl2 select * from src where key < 10;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.to.mapjoin=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.join=true;

explain
select count(*) FROM tbl1 a LEFT OUTER JOIN tbl2 b ON a.key = b.key;

explain
select count(*) FROM tbl1 a RIGHT OUTER JOIN tbl2 b ON a.key = b.key;
set hive.auto.convert.join=true;

set hive.exec.dynamic.partition.mode=nonstrict;



set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

-- SORT_QUERY_RESULTS

CREATE TABLE stage_bucket_big
(
key BIGINT,
value STRING
)
PARTITIONED BY (file_tag STRING);

CREATE TABLE bucket_big
(
key BIGINT,
value STRING
)
PARTITIONED BY (day STRING, pri bigint)
clustered by (key) sorted by (key) into 12 buckets
stored as RCFile;

CREATE TABLE stage_bucket_small
(
key BIGINT,
value string
)
PARTITIONED BY (file_tag STRING);

CREATE TABLE bucket_small
(
key BIGINT,
value string
)
PARTITIONED BY (pri bigint)
clustered by (key) sorted by (key) into 12 buckets
stored as RCFile;

load data local inpath '../../data/files/srcsortbucket1outof4.txt' overwrite into table stage_bucket_small partition (file_tag='1');
load data local inpath '../../data/files/srcsortbucket1outof4.txt' overwrite into table stage_bucket_small partition (file_tag='2');

insert overwrite table bucket_small partition(pri)
select
key,
value,
file_tag as pri
from
stage_bucket_small
where file_tag between 1 and 2;

load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' overwrite into table stage_bucket_big partition (file_tag='1');

insert overwrite table bucket_big partition(day,pri)
select
key,
value,
'day1' as day,
1 as pri
from
stage_bucket_big
where
file_tag='1';

select
a.key ,
a.value ,
b.value ,
'day1' as day,
1 as pri
from
(
select
key,
value
from bucket_big where day='day1'
) a
left outer join
(
select
key,
value
from bucket_small
where pri between 1 and 2
) b
on
(a.key = b.key)
;

set hive.mapred.mode=nonstrict;
-- small 1 part, 4 bucket & big 2 part, 2 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

-- Since the leftmost table is assumed as the big table, arrange the tables in the join accordingly
explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;
set hive.mapjoin.localtask.max.memory.usage = 0.0001;
set hive.mapjoin.check.memory.rows = 2;

-- The mapjoin should fail resulting in the sort-merge join
explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small 2 part, 2 bucket & big 1 part, 4 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

-- Since size is being used to find the big table, the order of the tables in the join does not matter
explain extended select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;
explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small 2 part, 4 bucket & big 1 part, 2 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/smallsrcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/smallsrcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ;

-- Since size is being used to find the big table, the order of the tables in the join does not matter
explain extended select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;
explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small no part, 4 bucket & big no part, 2 bucket

-- SORT_QUERY_RESULTS

CREATE TABLE bucket_small (key string, value string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/smallsrcsortbucket3outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/smallsrcsortbucket4outof4.txt' INTO TABLE bucket_small;

CREATE TABLE bucket_big (key string, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big;
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big;

set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ;

-- Since size is being used to find the big table, the order of the tables in the join does not matter
explain extended select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;
set hive.auto.convert.join=true;
explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
;

set hive.exec.reducers.max = 1;
set hive.explain.user=false;

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl4(key int, value string) CLUSTERED BY (value) SORTED BY (value) INTO 2 BUCKETS;

insert overwrite table tbl1 select * from src;
insert overwrite table tbl2 select * from src;
insert overwrite table tbl3 select * from src;
insert overwrite table tbl4 select * from src;

set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=200;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;

-- A SMB join is being followed by a regular join on a non-bucketed table on a different key

-- Three tests below are all the same query with different alias, which changes dispatch order of GenMapRedWalker
-- This is dependent to iteration order of HashMap, so can be meaningless in non-sun jdk
-- b = TS[0]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]-SEL[9]-FS[10]
-- c = TS[1]-RS[7]-JOIN[8]
-- a = TS[2]-MAPJOIN[11]
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.value = a.value;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.value = a.value;

-- d = TS[0]-RS[7]-JOIN[8]-SEL[9]-FS[10]
-- b = TS[1]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]
-- a = TS[2]-MAPJOIN[11]
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src d on d.value = a.value;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src d on d.value = a.value;

-- b = TS[0]-OP[13]-MAPJOIN[11]-RS[6]-JOIN[8]-SEL[9]-FS[10]
-- a = TS[1]-MAPJOIN[11]
-- h = TS[2]-RS[7]-JOIN[8]
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value;

-- A SMB join is being followed by a regular join on a non-bucketed table on the same key
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.key = a.key;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.key = a.key;

-- A SMB join is being followed by a regular join on a bucketed table on the same key
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join tbl3 c on c.key = a.key;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join tbl3 c on c.key = a.key;

-- A SMB join is being followed by a regular join on a bucketed table on a different key
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join tbl4 c on c.value = a.value;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join tbl4 c on c.value = a.value;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- A SMB join is being followed by a regular join on a non-bucketed table on a different key
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.value = a.value;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.value = a.value;

-- A SMB join is being followed by a regular join on a non-bucketed table on the same key
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.key = a.key;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src c on c.key = a.key;

-- A SMB join is being followed by a regular join on a bucketed table on the same key
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join tbl3 c on c.key = a.key;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join tbl3 c on c.key = a.key;

-- A SMB join is being followed by a regular join on a bucketed table on a different key
explain select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join tbl4 c on c.value = a.value;
select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join tbl4 c on c.value = a.value;
set hive.mapred.mode=nonstrict;
-- small 2 part, 4 bucket & big 2 part, 2 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/smallsrcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/smallsrcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ;

-- Since size is being used to find the big table, the order of the tables in the join does not matter
explain extended select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;
explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;

-- small 2 part, 2 bucket & big 2 part, 4 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

load data local inpath '../../data/files/smallsrcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/smallsrcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

-- Since size is being used to find the big table, the order of the tables in the join does not matter
explain extended select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;
set hive.mapjoin.localtask.max.memory.usage = 0.0001;
set hive.mapjoin.check.memory.rows = 2;

-- The mapjoin should fail resulting in the sort-merge join
explain extended select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
select count(*) FROM bucket_big a JOIN bucket_small b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
;

set hive.exec.reducers.max = 1;

-- SORT_QUERY_RESULTS

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1
select * from src where key < 10;

insert overwrite table tbl2
select * from src where key < 10;

set hive.auto.convert.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join=true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;
-- The join is being performed as part of sub-query. It should be converted to a sort-merge join
explain
select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

-- The join is being performed as part of sub-query. It should be converted to a sort-merge join
explain
select key, count(*) from
(
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
group by key;

select key, count(*) from
(
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
group by key;

-- The join is being performed as part of more than one sub-query. It should be converted to a sort-merge join
explain
select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

-- A join is being performed across different sub-queries, where a join is being performed in each of them.
-- Each sub-query should be converted to a sort-merge join.
explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join, although there is more than one level of sub-query
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

-- Both the tables are nested sub-queries i.e more then 1 level of sub-query.
-- The join should be converted to a sort-merge join
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters and the join key
-- is not getting modified, it should be converted to a sort-merge join. Note that the sub-query modifies one
-- item, but that is not part of the join key.
explain
select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

-- Since the join key is modified by the sub-query, neither sort-merge join not bucketized mapside
-- join should be performed
explain
select count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

-- The left table is a sub-query and the right table is not.
-- It should be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

-- The right table is a sub-query and the left table is not.
-- It should be converted to a sort-merge join.
explain
select count(*) from tbl1 a
  join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq1
  on a.key = subq1.key;

select count(*) from tbl1 a
  join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq1
  on a.key = subq1.key;

-- There are more than 2 inputs to the join, all of them being sub-queries.
-- It should be converted to to a sort-merge join
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on (subq1.key = subq2.key)
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

-- The join is being performed on a nested sub-query, and an aggregation is performed after that.
-- The join should be converted to a sort-merge join
explain
select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- The join is being performed as part of sub-query. It should be converted to a sort-merge join
explain
select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

-- The join is being performed as part of sub-query. It should be converted to a sort-merge join
explain
select key, count(*) from
(
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
group by key;

select key, count(*) from
(
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
group by key;

-- The join is being performed as part of more than one sub-query. It should be converted to a sort-merge join
explain
select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

-- A join is being performed across different sub-queries, where a join is being performed in each of them.
-- Each sub-query should be converted to a sort-merge join.
explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join, although there is more than one level of sub-query
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

-- Both the tables are nested sub-queries i.e more then 1 level of sub-query.
-- The join should be converted to a sort-merge join
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters and the join key
-- is not getting modified, it should be converted to a sort-merge join. Note that the sub-query modifies one
-- item, but that is not part of the join key.
explain
select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

-- The left table is a sub-query and the right table is not.
-- It should be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

-- The right table is a sub-query and the left table is not.
-- It should be converted to a sort-merge join.
explain
select count(*) from tbl1 a
  join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq1
  on a.key = subq1.key;

select count(*) from tbl1 a
  join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq1
  on a.key = subq1.key;

-- There are more than 2 inputs to the join, all of them being sub-queries.
-- It should be converted to to a sort-merge join
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on (subq1.key = subq2.key)
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

-- The join is being performed on a nested sub-query, and an aggregation is performed after that.
-- The join should be converted to a sort-merge join
explain
select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;
drop table if exists emptyavro;
create table emptyavro (a int) stored as avro;
select count(*) from emptyavro;
insert into emptyavro select count(*) from emptyavro;
select count(*) from emptyavro;
insert into emptyavro select key from src where key = 100 limit 1;
select * from emptyavro;

-- SORT_QUERY_RESULTS

-- verify that we can actually read avro files
CREATE TABLE doctors (
  number int,
  first_name string)
STORED AS AVRO;

DESCRIBE doctors;

ALTER TABLE doctors ADD COLUMNS (last_name string);

DESCRIBE doctors;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors;

SELECT * FROM doctors;-- SORT_QUERY_RESULTS

-- verify that we can actually read avro files
CREATE TABLE doctors (
  number int,
  first_name string,
  last_name string)
STORED AS AVRO;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors;

CREATE TABLE doctors_copy (
  number int,
  first_name string)
STORED AS AVRO;

INSERT INTO TABLE doctors_copy SELECT number, first_name FROM doctors;

ALTER TABLE doctors_copy ADD COLUMNS (last_name string);

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors_copy;

DESCRIBE doctors_copy;

SELECT * FROM doctors_copy;set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

-- verify that we can actually read avro files
CREATE TABLE doctors (
  number int,
  first_name string,
  last_name string)
STORED AS AVRO;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors;

CREATE TABLE doctors_copy (
  number int,
  first_name string)
PARTITIONED BY (part int)
STORED AS AVRO;

INSERT INTO TABLE doctors_copy PARTITION(part=1) SELECT number, first_name FROM doctors;

ALTER TABLE doctors_copy ADD COLUMNS (last_name string);

DESCRIBE doctors_copy;

SELECT * FROM doctors_copy;-- verify that we can update the table properties
CREATE TABLE avro2
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{ "namespace": "org.apache.hive",
  "name": "first_schema",
  "type": "record",
  "fields": [
    { "name":"string1", "type":"string" },
    { "name":"string2", "type":"string" }
  ] }');

DESCRIBE avro2;

ALTER TABLE avro2 SET TBLPROPERTIES ('avro.schema.literal'='{ "namespace": "org.apache.hive",
  "name": "second_schema",
  "type": "record",
  "fields": [
    { "name":"int1", "type":"int" },
    { "name":"float1", "type":"float" },
  { "name":"double1", "type":"double" }
  ] }');

DESCRIBE avro2;

DROP TABLE avro_charvarchar_staging;
DROP TABLE avro_charvarchar;

CREATE TABLE avro_charvarchar_staging (
  cchar char(5),
  cvarchar varchar(10),
  m1 map<string, char(2)>,
  l1 array<string>,
  st1 struct<c1:int, c2:varchar(4)>
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

CREATE TABLE avro_charvarchar (
  cchar char(5),
  cvarchar varchar(10),
  m1 map<string, char(2)>,
  l1 array<string>,
  st1 struct<c1:int, c2:varchar(4)>
) STORED AS AVRO;

LOAD DATA LOCAL INPATH '../../data/files/avro_charvarchar.txt' OVERWRITE INTO TABLE avro_charvarchar_staging;

INSERT OVERWRITE TABLE avro_charvarchar SELECT * FROM avro_charvarchar_staging;

SELECT * FROM avro_charvarchar;
-- verify Avro columns comments
DROP TABLE IF EXISTS testAvroComments1;

CREATE TABLE testAvroComments1
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int",
      "doc":"Order of playing the role"
    },
    {
      "name":"first_name",
      "type":"string",
      "doc":"first name of actor playing role"
    },
    {
      "name":"last_name",
      "type":"string",
      "doc":"last name of actor playing role"
    },
    {
      "name":"extra_field",
      "type":"string",
      "doc":"an extra field not in the original file",
      "default":"fishfingers and custard"
    }
  ]
}');

DESCRIBE testAvroComments1;
DROP TABLE testAvroComments1;

DROP TABLE IF EXISTS testAvroComments2;
CREATE TABLE testAvroComments2
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int",
      "doc":"Order of playing the role"
    },
    {
      "name":"first_name",
      "type":"string"
    },
    {
      "name":"last_name",
      "type":"string",
      "doc":"last name of actor playing role"
    },
    {
      "name":"extra_field",
      "type":"string",
      "default":"fishfingers and custard"
    }
  ]
}');

DESCRIBE testAvroComments2;
DROP TABLE testAvroComments2;

DROP TABLE IF EXISTS testAvroComments3;
CREATE TABLE testAvroComments3
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int"
    },
    {
      "name":"first_name",
      "type":"string"
    },
    {
      "name":"last_name",
      "type":"string"
    },
    {
      "name":"extra_field",
      "type":"string",
      "default":"fishfingers and custard"
    }
  ]
}');

DESCRIBE testAvroComments3;
DROP TABLE testAvroComments3;

DROP TABLE IF EXISTS testAvroComments4;

CREATE TABLE testAvroComments4 (
  number int COMMENT "Order of playing the role",
  first_name string COMMENT "first name of actor playing role",
  last_name string COMMENT "last name of actor playing role",
  extra_field string COMMENT "an extra field not in the original file")
STORED AS AVRO;

DESCRIBE testAvroComments4;
DROP TABLE testAvroComments4;

DROP TABLE IF EXISTS testAvroComments5;

CREATE TABLE testAvroComments5 (
  number int COMMENT "Order of playing the role",
  first_name string,
  last_name string COMMENT "last name of actor playing role",
  extra_field string)
STORED AS AVRO;

DESCRIBE testAvroComments5;
DROP TABLE testAvroComments5;

DROP TABLE IF EXISTS testAvroComments6;

CREATE TABLE testAvroComments6 (
  number int,
  first_name string,
  last_name string,
  extra_field string)
STORED AS AVRO;

DESCRIBE testAvroComments6;
DROP TABLE testAvroComments6;


-- verify that new joins bring in correct schemas (including evolved schemas)

CREATE TABLE doctors4
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int",
      "doc":"Order of playing the role"
    },
    {
      "name":"first_name",
      "type":"string",
      "doc":"first name of actor playing role"
    },
    {
      "name":"last_name",
      "type":"string",
      "doc":"last name of actor playing role"
    },
    {
      "name":"extra_field",
      "type":"string",
      "doc":"an extra field not in the original file",
      "default":"fishfingers and custard"
    }
  ]
}');

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors4;

set hive.exec.compress.output=true;

select count(*) from src;

-- verify that new joins bring in correct schemas (including evolved schemas)

CREATE TABLE doctors4 (
  number int,
  first_name string,
  last_name string,
  extra_field string)
STORED AS AVRO;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors4;

set hive.exec.compress.output=true;

SELECT count(*) FROM src;set hive.mapred.mode=nonstrict;
-- JAVA_VERSION_SPECIFIC_OUTPUT

DROP TABLE avro_date_staging;
DROP TABLE avro_date;
DROP TABLE avro_date_casts;

CREATE TABLE avro_date_staging (d date, m1 map<string, date>, l1 array<date>)
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
   COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'
   STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/avro_date.txt' OVERWRITE INTO TABLE avro_date_staging;

CREATE TABLE avro_date (d date, m1 map<string, date>, l1 array<date>)
  PARTITIONED BY (p1 int, p2 date)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
  COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'
  STORED AS AVRO;

INSERT OVERWRITE TABLE avro_date PARTITION(p1=2, p2='2014-09-26') SELECT * FROM avro_date_staging;

SELECT * FROM avro_date;
SELECT d, COUNT(d) FROM avro_date GROUP BY d;
SELECT * FROM avro_date WHERE d!='1947-02-11';
SELECT * FROM avro_date WHERE d<'2014-12-21';
SELECT * FROM avro_date WHERE d>'8000-12-01';
DROP TABLE IF EXISTS dec;

CREATE TABLE dec(name string, value decimal(8,4));

LOAD DATA LOCAL INPATH '../../data/files/dec.txt' into TABLE dec;

ANALYZE TABLE dec COMPUTE STATISTICS FOR COLUMNS value;
DESC FORMATTED dec value;

DROP TABLE IF EXISTS avro_dec;

CREATE TABLE `avro_dec`(
  `name` string COMMENT 'from deserializer',
  `value` decimal(5,2) COMMENT 'from deserializer')
COMMENT 'just drop the schema right into the HQL'
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES (
  'numFiles'='1',
  'avro.schema.literal'='{\"namespace\":\"com.howdy\",\"name\":\"some_schema\",\"type\":\"record\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"value\",\"type\":{\"type\":\"bytes\",\"logicalType\":\"decimal\",\"precision\":5,\"scale\":2}}]}'
);

DESC avro_dec;

INSERT OVERWRITE TABLE avro_dec select name, value from dec;

SELECT * FROM avro_dec;

DROP TABLE IF EXISTS avro_dec1;

CREATE TABLE `avro_dec1`(
  `name` string COMMENT 'from deserializer',
  `value` decimal(4,1) COMMENT 'from deserializer')
COMMENT 'just drop the schema right into the HQL'
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES (
  'numFiles'='1',
  'avro.schema.literal'='{\"namespace\":\"com.howdy\",\"name\":\"some_schema\",\"type\":\"record\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"value\",\"type\":{\"type\":\"bytes\",\"logicalType\":\"decimal\",\"precision\":4,\"scale\":1}}]}'
);

DESC avro_dec1;

LOAD DATA LOCAL INPATH '../../data/files/dec.avro' into TABLE avro_dec1;

select value from avro_dec1;

DROP TABLE dec;
DROP TABLE avro_dec;
DROP TABLE avro_dec1;
DROP TABLE IF EXISTS dec;

CREATE TABLE dec (
  name string,
  value decimal(8,4));

LOAD DATA LOCAL INPATH '../../data/files/dec.txt' into TABLE dec;

ANALYZE TABLE dec COMPUTE STATISTICS FOR COLUMNS value;
DESC FORMATTED dec value;

DROP TABLE IF EXISTS avro_dec;

CREATE TABLE avro_dec(
  name string,
  value decimal(5,2))
COMMENT 'just drop the schema right into the HQL'
STORED AS AVRO;

DESC avro_dec;

INSERT OVERWRITE TABLE avro_dec SELECT name, value FROM dec;

SELECT * FROM avro_dec;

DROP TABLE IF EXISTS avro_dec1;

CREATE TABLE avro_dec1(
  name string,
  value decimal(4,1))
COMMENT 'just drop the schema right into the HQL'
STORED AS AVRO;

DESC avro_dec1;

LOAD DATA LOCAL INPATH '../../data/files/dec.avro' INTO TABLE avro_dec1;

SELECT value FROM avro_dec1;

DROP TABLE dec;
DROP TABLE avro_dec;
DROP TABLE avro_dec1;
-- These test attempts to deserialize an Avro file that contains map null values, and the file schema
-- vs record schema have the null values in different positions
-- i.e.
-- fileSchema   = [{ "type" : "map", "values" : ["string","null"]}, "null"]
-- recordSchema = ["null", { "type" : "map", "values" : ["string","null"]}]

-- JAVA_VERSION_SPECIFIC_OUTPUT

DROP TABLE IF EXISTS avro_table;

CREATE TABLE avro_table (avreau_col_1 map<string,string>) STORED AS AVRO;
LOAD DATA LOCAL INPATH '../../data/files/map_null_val.avro' OVERWRITE INTO TABLE avro_table;
SELECT * FROM avro_table;

DROP TABLE avro_table;
-- SORT_QUERY_RESULTS

-- verify that new fields in schema get propagated to table scans
CREATE TABLE doctors_with_new_field
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int",
      "doc":"Order of playing the role"
    },
    {
      "name":"first_name",
      "type":"string",
      "doc":"first name of actor playing role"
    },
    {
      "name":"last_name",
      "type":"string",
      "doc":"last name of actor playing role"
    },
    {
      "name":"extra_field",
      "type":"string",
      "doc":"an extra field not in the original file",
      "default":"fishfingers and custard"
    }
  ]
}');

DESCRIBE doctors_with_new_field;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors_with_new_field;

SELECT * FROM doctors_with_new_field;

-- SORT_QUERY_RESULTS

-- verify that new joins bring in correct schemas (including evolved schemas)

CREATE TABLE doctors4
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int",
      "doc":"Order of playing the role"
    },
    {
      "name":"first_name",
      "type":"string",
      "doc":"first name of actor playing role"
    },
    {
      "name":"last_name",
      "type":"string",
      "doc":"last name of actor playing role"
    },
    {
      "name":"extra_field",
      "type":"string",
      "doc":"an extra field not in the original file",
      "default":"fishfingers and custard"
    }
  ]
}');

DESCRIBE doctors4;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors4;

CREATE TABLE episodes
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "episodes",
  "type": "record",
  "fields": [
    {
      "name":"title",
      "type":"string",
      "doc":"episode title"
    },
    {
      "name":"air_date",
      "type":"string",
      "doc":"initial date"
    },
    {
      "name":"doctor",
      "type":"int",
      "doc":"main actor playing the Doctor in episode"
    }
  ]
}');

DESCRIBE episodes;

LOAD DATA LOCAL INPATH '../../data/files/episodes.avro' INTO TABLE episodes;

SELECT e.title, e.air_date, d.first_name, d.last_name, d.extra_field, e.air_date
FROM doctors4 d JOIN episodes e ON (d.number=e.doctor);


-- SORT_QUERY_RESULTS

-- verify that new joins bring in correct schemas (including evolved schemas)

CREATE TABLE doctors4 (
  number int COMMENT "Order of playing the role",
  first_name string COMMENT "first name of actor playing role",
  last_name string COMMENT "last name of actor playing role")
STORED AS AVRO;

DESCRIBE doctors4;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors4;

CREATE TABLE episodes (
  title string COMMENT "episode title",
  air_date string COMMENT "initial date",
  doctor int COMMENT "main actor playing the Doctor in episode")
STORED AS AVRO;

DESCRIBE episodes;

LOAD DATA LOCAL INPATH '../../data/files/episodes.avro' INTO TABLE episodes;

SELECT e.title, e.air_date, d.first_name, d.last_name, e.air_date
FROM doctors4 d JOIN episodes e ON (d.number=e.doctor);-- SORT_QUERY_RESULTS

-- verify that we can actually read avro files
CREATE TABLE doctors (
  number int,
  first_name string,
  last_name string)
STORED AS AVRO;

DESCRIBE doctors;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors;

SELECT * FROM doctors;-- Verify that nullable fields properly work

-- JAVA_VERSION_SPECIFIC_OUTPUT

CREATE TABLE test_serializer(string1 STRING,
                             int1 INT,
                             tinyint1 TINYINT,
                             smallint1 SMALLINT,
                             bigint1 BIGINT,
                             boolean1 BOOLEAN,
                             float1 FLOAT,
                             double1 DOUBLE,
                             list1 ARRAY<STRING>,
                             map1 MAP<STRING,INT>,
                             struct1 STRUCT<sint:INT,sboolean:BOOLEAN,sstring:STRING>,
                             enum1 STRING,
                             nullableint INT,
                             bytes1 BINARY,
                             fixed1 BINARY)
 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY ':' MAP KEYS TERMINATED BY '#' LINES TERMINATED BY '\n'
 STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/csv.txt' INTO TABLE test_serializer;

CREATE TABLE as_avro
  ROW FORMAT
  SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
  STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
  TBLPROPERTIES (
    'avro.schema.literal'='{
      "namespace": "com.howdy",
      "name": "some_schema",
      "type": "record",
      "fields": [
        { "name": "string1", "type": ["null", "string"] },
        { "name": "int1", "type": ["null", "int"] },
        { "name": "tinyint1", "type": ["null", "int"] },
        { "name": "smallint1", "type": ["null", "int"] },
        { "name": "bigint1", "type": ["null", "long"] },
        { "name": "boolean1", "type": ["null", "boolean"] },
        { "name": "float1", "type": ["null", "float"] },
        { "name": "double1", "type": ["null", "double"] },
        { "name": "list1", "type": ["null", {"type": "array", "items": "string"}] },
        { "name": "map1", "type": ["null", {"type": "map", "values": "int"}] },
        { "name": "struct1", "type": ["null", {"type": "record", "name": "struct1_name", "fields": [
          { "name": "sInt", "type": "int" },
          { "name": "sBoolean", "type": "boolean" },
          { "name": "sString", "type": "string" }
        ]}] },
        { "name": "enum1", "type": ["null", {"type": "enum", "name": "enum1_values", "symbols": ["BLUE", "RED", "GREEN"]}] },
        { "name": "nullableint", "type": ["null", "int"] },
        { "name": "bytes1", "type": ["null", "bytes"] },
        { "name": "fixed1", "type": ["null", {"type": "fixed", "name": "threebytes", "size": 3}] }
      ]
    }'
  )
;

INSERT OVERWRITE TABLE as_avro SELECT * FROM test_serializer;
SELECT * FROM as_avro;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- Verify that table scans work with partitioned Avro tables
CREATE TABLE episodes
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "episodes",
  "type": "record",
  "fields": [
    {
      "name":"title",
      "type":"string",
      "doc":"episode title"
    },
    {
      "name":"air_date",
      "type":"string",
      "doc":"initial date"
    },
    {
      "name":"doctor",
      "type":"int",
      "doc":"main actor playing the Doctor in episode"
    }
  ]
}');

LOAD DATA LOCAL INPATH '../../data/files/episodes.avro' INTO TABLE episodes;

CREATE TABLE episodes_partitioned
PARTITIONED BY (doctor_pt INT)
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "episodes",
  "type": "record",
  "fields": [
    {
      "name":"title",
      "type":"string",
      "doc":"episode title"
    },
    {
      "name":"air_date",
      "type":"string",
      "doc":"initial date"
    },
    {
      "name":"doctor",
      "type":"int",
      "doc":"main actor playing the Doctor in episode"
    }
  ]
}');

SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE TABLE episodes_partitioned PARTITION (doctor_pt) SELECT title, air_date, doctor, doctor as doctor_pt FROM episodes;

SELECT * FROM episodes_partitioned WHERE doctor_pt > 6;

-- Verify that Fetch works in addition to Map
SELECT * FROM episodes_partitioned ORDER BY air_date LIMIT 5;
-- Fetch w/filter to specific partition
SELECT * FROM episodes_partitioned WHERE doctor_pt = 6;
-- Fetch w/non-existent partition
SELECT * FROM episodes_partitioned WHERE doctor_pt = 7 LIMIT 5;
-- Alter table add an empty partition
ALTER TABLE episodes_partitioned ADD PARTITION (doctor_pt=7);
SELECT COUNT(*) FROM episodes_partitioned;

-- Verify that reading from an Avro partition works
-- even if it has an old schema relative to the current table level schema

-- Create table and store schema in SERDEPROPERTIES
CREATE TABLE episodes_partitioned_serdeproperties
PARTITIONED BY (doctor_pt INT)
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
WITH SERDEPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "episodes",
  "type": "record",
  "fields": [
    {
      "name":"title",
      "type":"string",
      "doc":"episode title"
    },
    {
      "name":"air_date",
      "type":"string",
      "doc":"initial date"
    },
    {
      "name":"doctor",
      "type":"int",
      "doc":"main actor playing the Doctor in episode"
    }
  ]
}')
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat';

-- Insert data into a partition
INSERT INTO TABLE episodes_partitioned_serdeproperties PARTITION (doctor_pt) SELECT title, air_date, doctor, doctor as doctor_pt FROM episodes;
set hive.metastore.disallow.incompatible.col.type.changes=false;
-- Evolve the table schema by adding new array field "cast_and_crew"
ALTER TABLE episodes_partitioned_serdeproperties
SET SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
WITH SERDEPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "episodes",
  "type": "record",
  "fields": [
    {
      "name":"cast_and_crew",
      "type":{"type":"array","items":"string"},
      "default":[]
    },
    {
      "name":"title",
      "type":"string",
      "doc":"episode title"
    },
    {
      "name":"air_date",
      "type":"string",
      "doc":"initial date"
    },
    {
      "name":"doctor",
      "type":"int",
      "doc":"main actor playing the Doctor in episode"
    }
  ]
}');

-- Try selecting from the evolved table
SELECT * FROM episodes_partitioned_serdeproperties;
reset hive.metastore.disallow.incompatible.col.type.changes;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- Verify that table scans work with partitioned Avro tables
CREATE TABLE episodes (
  title string COMMENT "episode title",
  air_date string COMMENT "initial date",
  doctor int COMMENT "main actor playing the Doctor in episode")
STORED AS AVRO;

LOAD DATA LOCAL INPATH '../../data/files/episodes.avro' INTO TABLE episodes;

CREATE TABLE episodes_partitioned (
  title string COMMENT "episode title",
  air_date string COMMENT "initial date",
  doctor int COMMENT "main actor playing the Doctor in episode")
PARTITIONED BY (doctor_pt INT)
STORED AS AVRO;

SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE TABLE episodes_partitioned PARTITION (doctor_pt)
SELECT title, air_date, doctor, doctor as doctor_pt FROM episodes;

SELECT * FROM episodes_partitioned WHERE doctor_pt > 6;

-- Verify that Fetch works in addition to Map
SELECT * FROM episodes_partitioned ORDER BY air_date LIMIT 5;
-- Fetch w/filter to specific partition
SELECT * FROM episodes_partitioned WHERE doctor_pt = 6;
-- Fetch w/non-existent partition
SELECT * FROM episodes_partitioned WHERE doctor_pt = 7 LIMIT 5;-- SORT_QUERY_RESULTS

-- verify that we can actually read avro files
CREATE TABLE doctors
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int",
      "doc":"Order of playing the role"
    },
    {
      "name":"first_name",
      "type":"string",
      "doc":"first name of actor playing role"
    },
    {
      "name":"last_name",
      "type":"string",
      "doc":"last name of actor playing role"
    }
  ]
}');

DESCRIBE doctors;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors;

SELECT * FROM doctors;

set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- Verify that table scans work with partitioned Avro tables
CREATE TABLE episodes (
  title string COMMENT "episode title",
  air_date string COMMENT "initial date",
  doctor int COMMENT "main actor playing the Doctor in episode")
STORED AS AVRO;

LOAD DATA LOCAL INPATH '../../data/files/episodes.avro' INTO TABLE episodes;

CREATE TABLE episodes_partitioned (
  title string COMMENT "episode title",
  air_date string COMMENT "initial date",
  doctor int COMMENT "main actor playing the Doctor in episode")
PARTITIONED BY (doctor_pt INT)
STORED AS AVRO;

SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE TABLE episodes_partitioned PARTITION (doctor_pt)
SELECT title, air_date, doctor, doctor as doctor_pt FROM episodes;

ALTER TABLE episodes_partitioned
SET SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
WITH
SERDEPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "episodes",
  "type": "record",
  "fields": [
    {
      "name":"title",
      "type":"string",
      "doc":"episode title"
    },
    {
      "name":"air_date",
      "type":"string",
      "doc":"initial date"
    },
    {
      "name":"doctor",
      "type":"int",
      "doc":"main actor playing the Doctor in episode"
    },
     {
       "name":"value",
       "type":"int",
       "default":0,
       "doc":"default value"
     }
  ]
}');


SELECT * FROM episodes_partitioned WHERE doctor_pt > 6;

-- Verify that Fetch works in addition to Map
SELECT * FROM episodes_partitioned ORDER BY air_date LIMIT 5;
-- Fetch w/filter to specific partition
SELECT * FROM episodes_partitioned WHERE doctor_pt = 6;
-- Fetch w/non-existent partition
SELECT * FROM episodes_partitioned WHERE doctor_pt = 7 LIMIT 5;CREATE TABLE avro1
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "org.apache.hive",
  "name": "big_old_schema",
  "type": "record",
  "fields": [
    { "name":"string1", "type":"string" },
    { "name":"int1", "type":"int" },
    { "name":"tinyint1", "type":"int" },
    { "name":"smallint1", "type":"int" },
    { "name":"bigint1", "type":"long" },
    { "name":"boolean1", "type":"boolean" },
    { "name":"float1", "type":"float" },
    { "name":"double1", "type":"double" },
    { "name":"list1", "type":{"type":"array", "items":"string"} },
    { "name":"map1", "type":{"type":"map", "values":"int"} },
    { "name":"struct1", "type":{"type":"record", "name":"struct1_name", "fields": [
          { "name":"sInt", "type":"int" }, { "name":"sBoolean", "type":"boolean" }, { "name":"sString", "type":"string" } ] } },
    { "name":"union1", "type":["float", "boolean", "string"] },
    { "name":"enum1", "type":{"type":"enum", "name":"enum1_values", "symbols":["BLUE","RED", "GREEN"]} },
    { "name":"nullableint", "type":["int", "null"] },
    { "name":"bytes1", "type":"bytes" },
    { "name":"fixed1", "type":{"type":"fixed", "name":"threebytes", "size":3} },
    { "name":"dec1", "type":{"type":"bytes", "logicalType":"decimal", "precision":5, "scale":2} }
  ] }');

DESCRIBE avro1;

set hive.mapred.mode=nonstrict;
-- Exclude test on Windows due to space character being escaped in Hive paths on Windows.
-- EXCLUDE_OS_WINDOWS
-- JAVA_VERSION_SPECIFIC_OUTPUT

DROP TABLE avro_timestamp_staging;
DROP TABLE avro_timestamp;
DROP TABLE avro_timestamp_casts;

CREATE TABLE avro_timestamp_staging (d timestamp, m1 map<string, timestamp>, l1 array<timestamp>)
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
   COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'
   STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/avro_timestamp.txt' OVERWRITE INTO TABLE avro_timestamp_staging;

CREATE TABLE avro_timestamp (d timestamp, m1 map<string, timestamp>, l1 array<timestamp>)
  PARTITIONED BY (p1 int, p2 timestamp)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
  COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'
  STORED AS AVRO;

INSERT OVERWRITE TABLE avro_timestamp PARTITION(p1=2, p2='2014-09-26 07:08:09.123') SELECT * FROM avro_timestamp_staging;

SELECT * FROM avro_timestamp;
SELECT d, COUNT(d) FROM avro_timestamp GROUP BY d;
SELECT * FROM avro_timestamp WHERE d!='1947-02-11 07:08:09.123';
SELECT * FROM avro_timestamp WHERE d<'2014-12-21 07:08:09.123';
SELECT * FROM avro_timestamp WHERE d>'8000-12-01 07:08:09.123';
-- Windows-specific test due to space character being escaped in Hive paths on Windows.
-- INCLUDE_OS_WINDOWS
-- JAVA_VERSION_SPECIFIC_OUTPUT

DROP TABLE avro_timestamp_staging;
DROP TABLE avro_timestamp;
DROP TABLE avro_timestamp_casts;

CREATE TABLE avro_timestamp_staging (d timestamp, m1 map<string, timestamp>, l1 array<timestamp>)
   ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
   COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'
   STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/avro_timestamp.txt' OVERWRITE INTO TABLE avro_timestamp_staging;

CREATE TABLE avro_timestamp (d timestamp, m1 map<string, timestamp>, l1 array<timestamp>)
  PARTITIONED BY (p1 int, p2 timestamp)
  ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
  COLLECTION ITEMS TERMINATED BY ',' MAP KEYS TERMINATED BY ':'
  STORED AS AVRO;

INSERT OVERWRITE TABLE avro_timestamp PARTITION(p1=2, p2='2014-09-26 07:08:09.123') SELECT * FROM avro_timestamp_staging;

SELECT * FROM avro_timestamp;
SELECT d, COUNT(d) FROM avro_timestamp GROUP BY d;
SELECT * FROM avro_timestamp WHERE d!='1947-02-11 07:08:09.123';
SELECT * FROM avro_timestamp WHERE d<'2014-12-21 07:08:09.123';
SELECT * FROM avro_timestamp WHERE d>'8000-12-01 07:08:09.123';
-- File Schema { "name" : "val", "type" : [ "null", "int" ] }
-- Record Schema { "name" : "val", "type" : [ "long", "null" ] }

DROP TABLE IF EXISTS avro_type_evolution;

CREATE TABLE avro_type_evolution (val bigint) STORED AS AVRO
TBLPROPERTIES (
    'avro.schema.literal'='{
  "type" : "record",
  "name" : "type_evolution",
  "namespace" : "default",
  "fields" : [ {
    "name" : "val",
    "type" : [ "long", "null" ]
  } ]
}');
LOAD DATA LOCAL INPATH '../../data/files/type_evolution.avro' OVERWRITE INTO TABLE avro_type_evolution;
SELECT * FROM avro_type_evolution;

DROP TABLE avro_type_evolution;
set hive.exec.pre.hooks="org.this.is.a.bad.class";

EXPLAIN
SELECT x.* FROM SRC x LIMIT 20;

SELECT x.* FROM SRC x LIMIT 20;
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) AS 'UNKNOWN' WITH DEFERRED REBUILD;
CREATE TABLE dest1(key INT, value STRING, dt STRING, hr STRING) STORED AS TEXTFILE;

INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcpart TABLESAMPLE (BUCKET 1 OUT OF 2) s
WHERE s.ds='2008-04-08' and s.hr='11';

-- SORT_QUERY_RESULTS

drop table ba_test;

-- This query tests a) binary type works correctly in grammar b) string can be cast into binary c) binary can be stored in a table d) binary data can be loaded back again and queried d) order-by on a binary key

create table ba_test (ba_key binary, ba_val binary) ;

describe extended ba_test;

from src insert overwrite table ba_test select cast (src.key as binary), cast (src.value as binary);

select * from ba_test tablesample (10 rows);

drop table ba_test;
-- SORT_QUERY_RESULTS

drop table ba_test;

-- All the test in ba_test1.q + using LazyBinarySerde instead of LazySimpleSerde

create table ba_test (ba_key binary, ba_val binary) ;
alter table ba_test set serde 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe';

describe extended ba_test;

from src insert overwrite table ba_test select cast (src.key as binary), cast (src.value as binary);

select * from ba_test tablesample (10 rows);

drop table ba_test;


drop table ba_test;

-- All the tests of ba_table1.q + test for a group-by and aggregation on a binary key.

create table ba_test (ba_key binary, ba_val binary) ;

from src insert overwrite table ba_test select cast (src.key as binary), cast (src.value as binary);

select ba_test.ba_key, count(ba_test.ba_val) from ba_test group by ba_test.ba_key order by ba_key limit 5;

drop table ba_test;


-- SORT_QUERY_RESULTS

USE default;

CREATE TABLE dest1(bytes1 BINARY,
                   bytes2 BINARY,
                   string STRING);

FROM src INSERT OVERWRITE TABLE dest1
SELECT
  CAST(key AS BINARY),
  CAST(value AS BINARY),
  value
ORDER BY value
LIMIT 100;

--Add in a null row for good measure
INSERT INTO TABLE dest1 SELECT NULL, NULL, NULL FROM dest1 LIMIT 1;

-- this query tests all the udfs provided to work with binary types

SELECT
  bytes1,
  bytes2,
  string,
  LENGTH(bytes1),
  CONCAT(bytes1, bytes2),
  SUBSTR(bytes2, 1, 4),
  SUBSTR(bytes2, 3),
  SUBSTR(bytes2, -4, 3),
  HEX(bytes1),
  UNHEX(HEX(bytes1)),
  BASE64(bytes1),
  UNBASE64(BASE64(bytes1)),
  HEX(ENCODE(string, 'US-ASCII')),
  DECODE(ENCODE(string, 'US-ASCII'), 'US-ASCII')
FROM dest1;
drop table ba_test;

-- this query tests ba_table1.q + nested queries with multiple operations on binary data types + union on binary types
create table ba_test (ba_key binary, ba_val binary) ;

describe extended ba_test;

from src insert overwrite table ba_test select cast (src.key as binary), cast (src.value as binary);

select * from ( select key  from src where key < 50 union all select cast(ba_key as string) as key from ba_test order by key limit 50) unioned order by key limit 10;

drop table ba_test;


CREATE TABLE mytable(key STRING, value STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '9'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/string.txt' INTO TABLE mytable;

EXPLAIN
SELECT REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(key, '\001', '^A'), '\0', '^@'), '\002', '^B'), value
FROM (
        SELECT key, sum(value) as value
        FROM mytable
        GROUP BY key
) a;

SELECT REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(key, '\001', '^A'), '\0', '^@'), '\002', '^B'), value
FROM (
        SELECT key, sum(value) as value
        FROM mytable
        GROUP BY key
) a;
set hive.fetch.task.conversion=more;

select cast(cast('a' as binary) as string) from src tablesample (1 rows);
-- Create a table with binary output format
CREATE TABLE dest1(mydata STRING)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
  'serialization.last.column.takes.rest'='true'
)
STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveBinaryOutputFormat';

-- Insert into that table using transform
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest1
SELECT TRANSFORM(*)
  USING 'cat'
  AS mydata STRING
    ROW FORMAT SERDE
      'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
    WITH SERDEPROPERTIES (
      'serialization.last.column.takes.rest'='true'
    )
    RECORDREADER 'org.apache.hadoop.hive.ql.exec.BinaryRecordReader'
FROM src;

INSERT OVERWRITE TABLE dest1
SELECT TRANSFORM(*)
  USING 'cat'
  AS mydata STRING
    ROW FORMAT SERDE
      'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
    WITH SERDEPROPERTIES (
      'serialization.last.column.takes.rest'='true'
    )
    RECORDREADER 'org.apache.hadoop.hive.ql.exec.BinaryRecordReader'
FROM src;

-- Test the result
SELECT * FROM dest1;
drop table ba_test;

-- Tests everything in binary_table_colserde.q + uses LazyBinaryColumnarSerde

create table ba_test (ba_key binary, ba_val binary) stored as rcfile;
alter table ba_test set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';

describe extended ba_test;

from src insert overwrite table ba_test select cast (src.key as binary), cast (src.value as binary);

select ba_key, ba_val from ba_test order by ba_key limit 10;

drop table ba_test;


drop table ba_test;

-- Everything in ba_table1.q + columnar serde in RCFILE.

create table ba_test (ba_key binary, ba_val binary) stored as rcfile;
alter table ba_test set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';

describe extended ba_test;

from src insert overwrite table ba_test select cast (src.key as binary), cast (src.value as binary);

select ba_key, ba_val from ba_test order by ba_key limit 10;

drop table ba_test;


DROP TABLE IF EXISTS bool_literal;

CREATE TABLE bool_literal(key int, value boolean);

LOAD DATA LOCAL INPATH '../../data/files/bool_literal.txt' INTO TABLE bool_literal;

SET hive.lazysimple.extended_boolean_literal=false;

SELECT * FROM bool_literal;

SET hive.lazysimple.extended_boolean_literal=true;

SELECT * FROM bool_literal;

DROP TABLE bool_literal;;
set hive.exec.reducers.max = 200;

-- SORT_QUERY_RESULTS

CREATE TABLE bucket1_1(key int, value string) CLUSTERED BY (key) INTO 100 BUCKETS;

explain extended
insert overwrite table bucket1_1
select * from src;

insert overwrite table bucket1_1
select * from src;

select * from bucket1_1;
set hive.explain.user=false;
;
set hive.exec.reducers.max = 1;

-- SORT_QUERY_RESULTS

CREATE TABLE bucket2_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS;

explain extended
insert overwrite table bucket2_1
select * from src;

insert overwrite table bucket2_1
select * from src;

explain
select * from bucket2_1 tablesample (bucket 1 out of 2) s;

select * from bucket2_1 tablesample (bucket 1 out of 2) s;
set hive.explain.user=false;
;
set hive.exec.reducers.max = 1;

-- SORT_QUERY_RESULTS

CREATE TABLE bucket3_1(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS;

explain extended
insert overwrite table bucket3_1 partition (ds='1')
select * from src;

insert overwrite table bucket3_1 partition (ds='1')
select * from src;

insert overwrite table bucket3_1 partition (ds='2')
select * from src;

explain
select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1';

select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1';
set hive.explain.user=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
;

set hive.exec.reducers.max = 1;

CREATE TABLE bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

explain extended
insert overwrite table bucket4_1
select * from src;

insert overwrite table bucket4_1
select * from src;

explain
select * from bucket4_1 tablesample (bucket 1 out of 2) s;

select * from bucket4_1 tablesample (bucket 1 out of 2) s;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
;

set hive.exec.reducers.max = 1;
set hive.merge.mapfiles = true;
set hive.merge.mapredfiles = true;
set mapred.reduce.tasks = 2;

-- Tests that when a multi insert inserts into a bucketed table and a table which is not bucketed
-- the bucketed table is not merged and the table which is not bucketed is

CREATE TABLE bucketed_table(key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE unbucketed_table(key INT, value STRING);

EXPLAIN EXTENDED
FROM src
INSERT OVERWRITE TABLE bucketed_table SELECT key, value
INSERT OVERWRITE TABLE unbucketed_table SELECT key, value cluster by key;

FROM src
INSERT OVERWRITE TABLE bucketed_table SELECT key, value
INSERT OVERWRITE TABLE unbucketed_table SELECT key, value cluster by key;

DESC FORMATTED bucketed_table;

SELECT * FROM bucketed_table TABLESAMPLE (BUCKET 1 OUT OF 2) s LIMIT 10;
SELECT * FROM bucketed_table TABLESAMPLE (BUCKET 2 OUT OF 2) s LIMIT 10;

-- Should be 2 (not merged)
SELECT COUNT(DISTINCT INPUT__FILE__NAME) FROM bucketed_table;

-- Should be 1 (merged)
SELECT COUNT(DISTINCT INPUT__FILE__NAME) FROM unbucketed_table;
set hive.mapred.mode=nonstrict;
CREATE TABLE src_bucket(key STRING, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;


;

explain
insert into table src_bucket select key,value from srcpart;
insert into table src_bucket select key,value from srcpart;

select * from src_bucket limit 100;
set hive.mapred.mode=nonstrict;
-- small 1 part, 2 bucket & big 2 part, 4 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.optimize.bucketmapjoin = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small 1 part, 4 bucket & big 2 part, 2 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.optimize.bucketmapjoin = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small 2 part, 2 bucket & big 1 part, 4 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

set hive.optimize.bucketmapjoin = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small 2 part, 4 bucket & big 1 part, 2 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

set hive.optimize.bucketmapjoin = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
-- small no part, 4 bucket & big no part, 2 bucket
CREATE TABLE bucket_small (key string, value string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_small;

CREATE TABLE bucket_big (key string, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big;
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big;

set hive.optimize.bucketmapjoin = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small no part, 4 bucket & big 2 part, 2 bucket
CREATE TABLE bucket_small (key string, value string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_small;
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_small;

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.optimize.bucketmapjoin = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small 2 part, 4 bucket & big 2 part, 2 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.optimize.bucketmapjoin = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
-- small 2 part, 2 bucket & big 2 part, 4 bucket
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-09');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.optimize.bucketmapjoin = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
explain extended select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set mapred.max.split.size = 32000000;

CREATE TABLE T1(name STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE T1;

CREATE TABLE T2(name STRING) STORED AS SEQUENCEFILE;

INSERT OVERWRITE TABLE T2 SELECT * FROM (
SELECT tmp1.name as name FROM (
  SELECT name, 'MMM' AS n FROM T1) tmp1
  JOIN (SELECT 'MMM' AS n FROM T1) tmp2
  JOIN (SELECT 'MMM' AS n FROM T1) tmp3
  ON tmp1.n = tmp2.n AND tmp1.n = tmp3.n) ttt LIMIT 5000000;

CREATE TABLE T3(name STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE T3;
LOAD DATA LOCAL INPATH '../../data/files/kv2.txt' INTO TABLE T3;

set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter,org.apache.hadoop.hive.ql.hooks.ShowMapredStatsHook;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- 2 split by max.split.size
SELECT COUNT(1) FROM T2;

-- 1 split for two file
SELECT COUNT(1) FROM T3;

set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- 1 split
SELECT COUNT(1) FROM T2;

-- 2 split for two file
SELECT COUNT(1) FROM T3;

set hive.mapred.mode=nonstrict;
CREATE TABLE bucket_small (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_small partition(ds='2008-04-08');

CREATE TABLE bucket_big (key string, value string) partitioned by (ds string) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-08');

load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket_big partition(ds='2008-04-09');

set hive.optimize.bucketmapjoin = true;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;

set hive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat;
select /* + MAPJOIN(a) */ count(*) FROM bucket_small a JOIN bucket_big b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

set hive.optimize.bucketmapjoin = true;

-- empty partitions (HIVE-3205)
explain extended
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key where b.ds="2008-04-08";

select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key where b.ds="2008-04-08";

explain extended
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key where b.ds="2008-04-08";

select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key where b.ds="2008-04-08";

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.optimize.bucketmapjoin = true;
create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;

insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;


select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;


set hive.optimize.bucketmapjoin = true;
explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;


insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_1 CLUSTERED BY (key) INTO 3 BUCKETS;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='2');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='2');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='2');

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 3 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key) INTO 2 BUCKETS;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='2');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='2');

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key) INTO 3 BUCKETS;

set hive.optimize.bucketmapjoin=true;

-- The table bucketing metadata matches but the partition metadata does not, bucket map join should not be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part IS NOT NULL AND b.part IS NOT NULL;

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part IS NOT NULL AND b.part IS NOT NULL;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_1 CLUSTERED BY (key) INTO 4 BUCKETS;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='2');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='2');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='2');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='2');

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key) INTO 2 BUCKETS;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='2');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='2');


set hive.optimize.bucketmapjoin=true;

-- The table and partition bucketing metadata doesn't match but the bucket numbers of all partitions is
-- a power of 2 and the bucketing columns match so bucket map join should be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part IS NOT NULL AND b.part IS NOT NULL;

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part IS NOT NULL AND b.part IS NOT NULL;

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = b.part AND a.part IS NOT NULL AND b.part IS NOT NULL;

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = b.part AND a.part IS NOT NULL AND b.part IS NOT NULL;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_2 NOT CLUSTERED;

CREATE TABLE srcbucket_mapjoin_part_3 (key INT, value STRING) PARTITIONED BY (part STRING)
STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_3 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_3 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_3 CLUSTERED BY (key) INTO 2 BUCKETS;

set hive.optimize.bucketmapjoin=true;

-- The partition bucketing metadata match but one table is not bucketed, bucket map join should still be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' and b.part = '1';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' and b.part = '1';

-- The table bucketing metadata match but one partition is not bucketed, bucket map join should not be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_3 b
ON a.key = b.key AND a.part = '1' and b.part = '1';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_3 b
ON a.key = b.key AND a.part = '1' and b.part = '1';
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max=1;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (value) INTO 2 BUCKETS;

-- part=1 partition for srcbucket_mapjoin_part_1 is bucketed by 'value'
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_1 PARTITION (part='1')
SELECT * FROM src;

ALTER TABLE srcbucket_mapjoin_part_1 CLUSTERED BY (key) INTO 2 BUCKETS;

-- part=2 partition for srcbucket_mapjoin_part_1 is bucketed by 'key'
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_1 PARTITION (part='2')
SELECT * FROM src;

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 2 BUCKETS;

-- part=1 partition for srcbucket_mapjoin_part_2 is bucketed by 'key'
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_2 PARTITION (part='1')
SELECT * FROM src;

set hive.optimize.bucketmapjoin=true;

-- part=1 partition for srcbucket_mapjoin_part_1 is bucketed by 'value'
-- and it is also being joined. So, bucketed map-join cannot be performed
EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key;

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key;

-- part=2 partition for srcbucket_mapjoin_part_1 is bucketed by 'key'
-- and it is being joined. So, bucketed map-join can be performed
EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key and a.part = '2';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key and a.part = '2';

ALTER TABLE srcbucket_mapjoin_part_1 drop partition (part = '1');

-- part=2 partition for srcbucket_mapjoin_part_1 is bucketed by 'key'
-- and it is being joined. So, bucketed map-join can be performed
EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key;

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key;

ALTER TABLE srcbucket_mapjoin_part_1 CLUSTERED BY (value) INTO 2 BUCKETS;

-- part=2 partition for srcbucket_mapjoin_part_1 is bucketed by 'key'
-- and it is being joined. So, bucketed map-join can be performed
-- The fact that the table is being bucketed by 'value' does not matter
EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key;

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key;
set hive.mapred.mode=nonstrict;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.optimize.bucketmapjoin = true;
create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;


set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;

set hive.optimize.bucketmapjoin = true;
explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;


set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;

-- HIVE-3210
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-09');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-09');

set hive.optimize.bucketmapjoin = true;

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key;

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;
CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;

CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.optimize.bucketmapjoin = true;
create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part_2 a join srcbucket_mapjoin_part b
on a.key=b.key and b.ds="2008-04-08" and a.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part_2 a join srcbucket_mapjoin_part b
on a.key=b.key and b.ds="2008-04-08" and a.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part_2 a join srcbucket_mapjoin_part b
on a.key=b.key and b.ds="2008-04-08" and a.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;

set hive.optimize.bucketmapjoin = true;
explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part_2 a join srcbucket_mapjoin_part b
on a.key=b.key and b.ds="2008-04-08" and a.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part_2 a join srcbucket_mapjoin_part b
on a.key=b.key and b.ds="2008-04-08" and a.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part_2 a join srcbucket_mapjoin_part b
on a.key=b.key and b.ds="2008-04-08" and a.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;

CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.optimize.bucketmapjoin = true;
create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin b
on a.key=b.key;

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;


set hive.optimize.bucketmapjoin = true;
explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin b
on a.key=b.key;

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;
set hive.mapred.mode=nonstrict;
CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;

CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-09');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-09');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-09');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-09');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-09');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-09');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.optimize.bucketmapjoin = true;
create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key;

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;


set hive.optimize.bucketmapjoin = true;
explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part_2 b
on a.key=b.key;

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part_2 b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part_2 b
on a.key=b.key;

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- SORT_QUERY_RESULTS

create table tmp1 (a string, b string) clustered by (a) sorted by (a) into 10 buckets;

create table tmp2 (a string, b string) clustered by (a) sorted by (a) into 10 buckets;


;

set hive.exec.reducers.max=1;


insert overwrite table tmp1 select * from src where key < 50;
insert overwrite table tmp2 select * from src where key < 50;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.merge.mapfiles=false;
create table tmp3 (a string, b string, c string) clustered by (a) sorted by (a) into 10 buckets;


insert overwrite table tmp3
  select /*+ MAPJOIN(l) */ i.a, i.b, l.b
  from tmp1 i join tmp2 l ON i.a = l.a;

select * from tmp3;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (ds STRING, hr STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (ds='2008-04-08', hr='0');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (ds='2008-04-08', hr='0');

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (ds STRING, hr STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (ds='2008-04-08', hr='0');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (ds='2008-04-08', hr='0');

set hive.optimize.bucketmapjoin=true;

-- Tests that bucket map join works with a table with more than one level of partitioning

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ a.key, b.value
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.ds = '2008-04-08' AND b.ds = '2008-04-08'
ORDER BY a.key, b.value LIMIT 1;

SELECT /*+ MAPJOIN(b) */ a.key, b.value
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.ds = '2008-04-08' AND b.ds = '2008-04-08'
ORDER BY a.key, b.value LIMIT 1;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key) INTO 3 BUCKETS;

set hive.optimize.bucketmapjoin=true;

-- The partition bucketing metadata match but the tables have different numbers of buckets, bucket map join should still be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' and b.part = '1';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' and b.part = '1';

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (value) INTO 2 BUCKETS;

-- The partition bucketing metadata match but the tables are bucketed on different columns, bucket map join should still be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' and b.part = '1';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' and b.part = '1';
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_1 PARTITION (part='1');

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) INTO 3 BUCKETS STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key) INTO 2 BUCKETS;

set hive.optimize.bucketmapjoin=true;

-- The table bucketing metadata matches but the partitions have different numbers of buckets, bucket map join should not be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' and b.part = '1';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' and b.part = '1';

ALTER TABLE srcbucket_mapjoin_part_2 DROP PARTITION (part='1');
ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (value) INTO 2 BUCKETS;
LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 PARTITION (part='1');

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key) INTO 2 BUCKETS;

-- The table bucketing metadata matches but the partitions are bucketed on different columns, bucket map join should not be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' AND b.part = '1';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' AND b.part = '1';




CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;

CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 3 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketmapjoin = true;
create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";




set hive.mapred.mode=nonstrict;
CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-09');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-09');

set hive.optimize.bucketmapjoin = true;
create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part_2 b
on a.key=b.key;
drop table test1;
drop table test2;
drop table test3;
drop table test4;

create table test1 (key string, value string) clustered by (key) sorted by (key) into 3 buckets;
create table test2 (key string, value string) clustered by (value) sorted by (value) into 3 buckets;
create table test3 (key string, value string) clustered by (key, value) sorted by (key, value) into 3 buckets;
create table test4 (key string, value string) clustered by (value, key) sorted by (value, key) into 3 buckets;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE test1;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE test1;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE test1;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE test2;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE test2;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE test2;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE test3;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE test3;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE test3;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE test4;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE test4;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE test4;

set hive.optimize.bucketmapjoin = true;
-- should be allowed
explain extended select /* + MAPJOIN(R) */ * from test1 L join test1 R on L.key=R.key AND L.value=R.value;
explain extended select /* + MAPJOIN(R) */ * from test2 L join test2 R on L.key=R.key AND L.value=R.value;

-- should not apply bucket mapjoin
explain extended select /* + MAPJOIN(R) */ * from test1 L join test1 R on L.key+L.key=R.key;
explain extended select /* + MAPJOIN(R) */ * from test1 L join test2 R on L.key=R.key AND L.value=R.value;
explain extended select /* + MAPJOIN(R) */ * from test1 L join test3 R on L.key=R.key AND L.value=R.value;
explain extended select /* + MAPJOIN(R) */ * from test1 L join test4 R on L.key=R.key AND L.value=R.value;
explain extended select /* + MAPJOIN(R) */ * from test2 L join test3 R on L.key=R.key AND L.value=R.value;
explain extended select /* + MAPJOIN(R) */ * from test2 L join test4 R on L.key=R.key AND L.value=R.value;
explain extended select /* + MAPJOIN(R) */ * from test3 L join test4 R on L.key=R.key AND L.value=R.value;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.optimize.index.filter=true;
set hive.tez.bucket.pruning=true;
set hive.explain.user=false;
set hive.fetch.task.conversion=none;

CREATE TABLE srcbucket_pruned(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 16 BUCKETS STORED AS TEXTFILE;

-- cannot prune 2-key scenarios without a smarter optimizer
CREATE TABLE srcbucket_unpruned(key int, value string) partitioned by (ds string) CLUSTERED BY (key,value) INTO 16 BUCKETS STORED AS TEXTFILE;

-- good cases

explain extended
select * from srcbucket_pruned where key = 1;

explain extended
select * from srcbucket_pruned where key = 16;

explain extended
select * from srcbucket_pruned where key = 17;

explain extended
select * from srcbucket_pruned where key = 16+1;

explain extended
select * from srcbucket_pruned where key = '11';

explain extended
select * from srcbucket_pruned where key = 1 and ds='2008-04-08';

explain extended
select * from srcbucket_pruned where key = 1 and ds='2008-04-08' and value='One';

explain extended
select * from srcbucket_pruned where value='One' and key = 1 and ds='2008-04-08';

explain extended
select * from srcbucket_pruned where key in (2,3);

explain extended
select * from srcbucket_pruned where key in (2,3) and ds='2008-04-08';

explain extended
select * from srcbucket_pruned where key in (2,3) and ds='2008-04-08' and value='One';

explain extended
select * from srcbucket_pruned where value='One' and key in (2,3) and ds='2008-04-08';

explain extended
select * from srcbucket_pruned where (key=1 or key=2) and ds='2008-04-08';

explain extended
select * from srcbucket_pruned where (key=1 or key=2) and value = 'One' and ds='2008-04-08';

-- compat case (-15 = 1 & 15)

explain extended
select * from srcbucket_pruned where key = -15;

-- valid but irrelevant case (all buckets selected)

explain extended
select * from srcbucket_pruned where key in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17);

explain extended
select * from srcbucket_pruned where key in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17) and ds='2008-04-08';

explain extended
select * from srcbucket_pruned where key in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17) and ds='2008-04-08' and value='One';

explain extended
select * from srcbucket_pruned where value='One' and key in (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17) and ds='2008-04-08';

-- valid, but unimplemented cases

explain extended
select * from srcbucket_pruned where key = 1 and ds='2008-04-08' or key = 2;

explain extended
select * from srcbucket_pruned where key = 1 and ds='2008-04-08' and (value='One' or value = 'Two');

explain extended
select * from srcbucket_pruned where key = 1 or value = "One" or key = 2 and value = "Two";

-- Invalid cases

explain extended
select * from srcbucket_pruned where key = 'x11';

explain extended
select * from srcbucket_pruned where key = 1 or value = "One";

explain extended
select * from srcbucket_pruned where key = 1 or value = "One" or key = 2;

explain extended
select * from srcbucket_unpruned where key in (3, 5);

explain extended
select * from srcbucket_unpruned where key = 1;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.key, x.value from
(
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1'
)x;

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.key, x.value from
(
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1'
)x;

select count(*) from test_table2 where ds = '1';
select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1';
select count(*) from test_table2 tablesample (bucket 2 out of 2) s where ds = '1';

EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT * from
(
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1'
)x;

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT * from
(
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1'
)x;

select count(*) from test_table2 where ds = '1';
select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1';
select count(*) from test_table2 tablesample (bucket 2 out of 2) s where ds = '1';

-- it should be a map-only job
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.key, concat(x.value, x.value) from
(
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1'
)x;

-- it should be a map-reduce job
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.key+x.key, x.value from
(
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1'
)x;

-- it should be a map-only job
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.k1, concat(x.v1, x.v1) from
(
SELECT a.key as k1, a.value as v1 FROM test_table1 a WHERE a.ds = '1'
)x;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT * where key < 10;

FROM src
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT * where key < 100;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '2') SELECT * where key < 10;

FROM src
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2') SELECT * where key < 100;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- Since more than one partition of 'a' (the big table) is being selected,
-- it should be a map-reduce job
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds is not null and b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds is not null and b.ds = '1';

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- Since a single partition of the big table ('a') is being selected, it should be a map-only
-- job even though multiple partitions of 'b' are being selected
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds is not null;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds is not null;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- This should be a map-only job
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1') a
JOIN
(select key, value from test_table2 where ds = '1') b
ON a.key = b.key;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1') a
JOIN
(select key, value from test_table2 where ds = '1') b
ON a.key = b.key;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- This should be a map-only job
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.v1, b.v2)
FROM
(select key, concat(value, value) as v1 from test_table1 where ds = '1') a
JOIN
(select key, concat(value, value) as v2 from test_table2 where ds = '1') b
ON a.key = b.key;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.v1, b.v2)
FROM
(select key, concat(value, value) as v1 from test_table1 where ds = '1') a
JOIN
(select key, concat(value, value) as v2 from test_table2 where ds = '1') b
ON a.key = b.key;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- This should be a map-reduce job
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key+a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1') a
JOIN
(select key, value from test_table2 where ds = '1') b
ON a.key = b.key;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key+a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1') a
JOIN
(select key, value from test_table2 where ds = '1') b
ON a.key = b.key;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (value STRING, key INT) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *;

-- Insert data into the bucketed table by selecting from another bucketed table
-- The bucketing positions dont match - although the actual bucketing do.
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.value, x.key from
(SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1')x;

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.value, x.key from
(SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1')x;

select count(*) from test_table2 where ds = '1';
select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1';
select count(*) from test_table2 tablesample (bucket 2 out of 2) s where ds = '1';

CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (value) SORTED BY (value) INTO 2 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- The bucketing positions dont match - this should be a map-reduce operation
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.key, x.value from
(SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1')x;

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT x.key, x.value from
(SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1')x;

select count(*) from test_table2 where ds = '1';
select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1';
select count(*) from test_table2 tablesample (bucket 2 out of 2) s where ds = '1';
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table3 (key INT, key2 INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key2) SORTED BY (key2) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT * where key < 10;

FROM src
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT * where key < 100;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation, since the insert is happening on the bucketing position
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

DROP TABLE test_table3;

CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (value) SORTED BY (value) INTO 2 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation, since the insert is happening on a non-bucketing position
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, a.value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, a.value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

DROP TABLE test_table3;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key desc) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT * where key < 10;

FROM src
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT * where key < 100;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation, since the sort-order does not match
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- This should be a map-reduce job since the sort order does not match
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1') a
JOIN
(select key, value from test_table2 where ds = '1') b
ON a.key = b.key;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1') a
JOIN
(select key, value from test_table2 where ds = '1') b
ON a.key = b.key;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, key2 INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key, key2) SORTED BY (key ASC, key2 DESC) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, key2 INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key, key2) SORTED BY (key ASC, key2 DESC) INTO 2 BUCKETS;
CREATE TABLE test_table3 (key INT, key2 INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key, key2) SORTED BY (key ASC, key2 DESC) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT key, key+1, value where key < 10;

FROM src
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT key, key+1, value where key < 100;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation, since the sort-order matches
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, a.key2, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, a.key2, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1';

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation, since the sort-order matches
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT subq1.key, subq1.key2, subq1.value from
(
SELECT a.key, a.key2, concat(a.value, b.value) as value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1'
)subq1;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT subq1.key, subq1.key2, subq1.value from
(
SELECT a.key, a.key2, concat(a.value, b.value) as value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1'
)subq1;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key2, a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1';

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT subq1.key2, subq1.key, subq1.value from
(
SELECT a.key, a.key2, concat(a.value, b.value) as value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1'
)subq1;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT subq2.key, subq2.key2, subq2.value from
(
SELECT subq1.key2, subq1.key, subq1.value from
(
SELECT a.key, a.key2, concat(a.value, b.value) as value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1'
)subq1
)subq2;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT subq2.key, subq2.key2, subq2.value from
(
SELECT subq1.key2, subq1.key, subq1.value from
(
SELECT a.key, a.key2, concat(a.value, b.value) as value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1'
)subq1
)subq2;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT subq2.k2, subq2.k1, subq2.value from
(
SELECT subq1.key2 as k1, subq1.key as k2, subq1.value from
(
SELECT a.key, a.key2, concat(a.value, b.value) as value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1'
)subq1
)subq2;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT subq2.k2, subq2.k1, subq2.value from
(
SELECT subq1.key2 as k1, subq1.key  as k2, subq1.value from
(
SELECT a.key, a.key2, concat(a.value, b.value) as value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1'
)subq1
)subq2;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

CREATE TABLE test_table4 (key INT, key2 INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key, key2) SORTED BY (key DESC, key2 DESC) INTO 2 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation
EXPLAIN
INSERT OVERWRITE TABLE test_table4 PARTITION (ds = '1')
SELECT subq2.k2, subq2.k1, subq2.value from
(
SELECT subq1.key2 as k1, subq1.key  as k2, subq1.value from
(
SELECT a.key, a.key2, concat(a.value, b.value) as value
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key and a.key2 = b.key2 WHERE a.ds = '1' and b.ds = '1'
)subq1
)subq2;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT * where key < 10;

FROM src
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT * where key < 100;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1'
and (a.key = 0 or a.key = 5);

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1'
and (a.key = 0 or a.key = 5);

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- This should be a map-only job
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1' and (key = 0 or key = 5)) a
JOIN
(select key, value from test_table2 where ds = '1' and (key = 0 or key = 5)) b
ON a.key = b.key;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1' and (key = 0 or key = 5)) a
JOIN
(select key, value from test_table2 where ds = '1' and (key = 0 or key = 5)) b
ON a.key = b.key;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- This should be a map-only job
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1' and key < 8) a
JOIN
(select key, value from test_table2 where ds = '1' and key < 8) b
ON a.key = b.key
WHERE a.key = 0 or a.key = 5;

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, concat(a.value, b.value)
FROM
(select key, value from test_table1 where ds = '1' and key < 8) a
JOIN
(select key, value from test_table2 where ds = '1' and key < 8) b
ON a.key = b.key
WHERE a.key = 0 or a.key = 5;

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

set hive.auto.convert.sortmerge.join.to.mapjoin=true;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table3 (key INT, key2 INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT * where key < 10;

FROM src
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT * where key < 100;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, b.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.key, b.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT b.key, a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT b.key, a.key, concat(a.value, b.value)
FROM test_table1 a JOIN test_table2 b
ON a.key = b.key WHERE a.ds = '1' and b.ds = '1';

select * from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select * from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';
set hive.mapred.mode=nonstrict;
create table clustergroupby(key string, value string) partitioned by(ds string);
describe extended clustergroupby;
alter table clustergroupby clustered by (key) into 1 buckets;

insert overwrite table clustergroupby partition (ds='100') select key, value from src sort by key;

explain
select key, count(1) from clustergroupby where ds='100' group by key limit 10;
select key, count(1) from clustergroupby where ds='100' group by key limit 10;

describe extended clustergroupby;
insert overwrite table clustergroupby partition (ds='101') select key, value from src distribute by key;

--normal--
explain
select key, count(1) from clustergroupby  where ds='101'  group by key limit 10;
select key, count(1) from clustergroupby  where ds='101' group by key limit 10;

--function--
explain
select length(key), count(1) from clustergroupby  where ds='101'  group by length(key) limit 10;
select length(key), count(1) from clustergroupby  where ds='101' group by length(key) limit 10;
explain
select abs(length(key)), count(1) from clustergroupby  where ds='101'  group by abs(length(key)) limit 10;
select abs(length(key)), count(1) from clustergroupby  where ds='101' group by abs(length(key)) limit 10;

--constant--
explain
select key, count(1) from clustergroupby  where ds='101'  group by key,3 limit 10;
select key, count(1) from clustergroupby  where ds='101' group by key,3 limit 10;

--subquery--
explain
select key, count(1) from (select value as key, key as value from clustergroupby where ds='101')subq group by key limit 10;
select key, count(1) from (select value as key, key as value from clustergroupby where ds='101')subq group by key limit 10;

explain
select key, count(1) from clustergroupby  group by key;
select key, count(1) from clustergroupby  group by key;

explain
select key, count(1) from clustergroupby  group by key, 3;

-- number of buckets cannot be changed, so drop the table
drop table clustergroupby;
create table clustergroupby(key string, value string) partitioned by(ds string);

--sort columns--
alter table clustergroupby clustered by (value) sorted by (key, value) into 1 buckets;
describe extended clustergroupby;
insert overwrite table clustergroupby partition (ds='102') select key, value from src distribute by value sort by key, value;

explain
select key, count(1) from clustergroupby  where ds='102'  group by key limit 10;
select key, count(1) from clustergroupby  where ds='102' group by key limit 10;
explain
select value, count(1) from clustergroupby  where ds='102'  group by value limit 10;
select value, count(1) from clustergroupby  where ds='102'  group by value limit 10;
explain
select key, count(1) from clustergroupby  where ds='102'  group by key, value limit 10;
select key, count(1) from clustergroupby  where ds='102'  group by key, value limit 10;

-- number of buckets cannot be changed, so drop the table
drop table clustergroupby;
create table clustergroupby(key string, value string) partitioned by(ds string);

alter table clustergroupby clustered by (value, key) sorted by (key) into 1 buckets;
describe extended clustergroupby;
insert overwrite table clustergroupby partition (ds='103') select key, value from src distribute by value, key sort by key;
explain
select key, count(1) from clustergroupby  where ds='103'  group by key limit 10;
select key, count(1) from clustergroupby  where ds='103' group by key limit 10;
explain
select key, count(1) from clustergroupby  where ds='103'  group by value, key limit 10;
select key, count(1) from clustergroupby  where ds='103' group by  value, key limit 10;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/bmjpathfilter;

create table t1 (dt string) location '${system:test.tmp.dir}/bmjpathfilter/t1';
Create table t2 (dt string) stored as orc;
dfs -touchz ${system:test.tmp.dir}/bmjpathfilter/t1/_SUCCESS;

SET hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
SET hive.optimize.bucketmapjoin=true;

SELECT /*+ MAPJOIN(b) */ a.dt FROM t1 a JOIN t2 b ON (a.dt = b.dt);

SET hive.optimize.bucketmapjoin=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

dfs -rmr ${system:test.tmp.dir}/bmjpathfilter;
;
set mapred.reduce.tasks = 16;

create table bucket_many(key int, value string) clustered by (key) into 256 buckets;

explain extended
insert overwrite table bucket_many
select * from src;

insert overwrite table bucket_many
select * from src;

explain
select * from bucket_many tablesample (bucket 1 out of 256) s;

select * from bucket_many tablesample (bucket 1 out of 256) s;
set hive.mapred.mode=nonstrict;
CREATE TABLE srcbucket_mapjoin_part (key int, value string)
  partitioned by (ds string) CLUSTERED BY (key) INTO 3 BUCKETS
  STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt'
  INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt'
  INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt'
  INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string)
  partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS
  STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt'
  INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt'
  INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

-- The number of buckets in the 2 tables above (being joined later) dont match.
-- Throw an error if the user requested a bucketed mapjoin to be enforced.
-- In the default case (hive.enforce.bucketmapjoin=false), the query succeeds
-- even though mapjoin is not being performed

explain
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and a.ds="2008-04-08" and b.ds="2008-04-08";

set hive.optimize.bucketmapjoin = true;

explain
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and a.ds="2008-04-08" and b.ds="2008-04-08";

set hive.enforce.bucketmapjoin=true;

explain
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and a.ds="2008-04-08" and b.ds="2008-04-08";

-- Although the user has specified a bucketed map-join, the number of buckets in the table
-- do not match the number of files
drop table table1;
drop table table2;

create table table1(key string, value string) clustered by (key, value)
into 2 BUCKETS stored as textfile;
create table table2(key string, value string) clustered by (value, key)
into 2 BUCKETS stored as textfile;

load data local inpath '../../data/files/T1.txt' overwrite into table table1;

load data local inpath '../../data/files/T1.txt' overwrite into table table2;
load data local inpath '../../data/files/T2.txt' overwrite into table table2;

set hive.optimize.bucketmapjoin = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value;

-- Although the user has specified a bucketed map-join, the number of buckets in the table
-- do not match the number of files
drop table table1;
drop table table2;

create table table1(key string, value string) partitioned by (ds string) clustered by (key, value)
into 2 BUCKETS stored as textfile;
create table table2(key string, value string) clustered by (value, key)
into 2 BUCKETS stored as textfile;

load data local inpath '../../data/files/T1.txt' overwrite into table table1 partition (ds='1');
load data local inpath '../../data/files/T2.txt' overwrite into table table1 partition (ds='1');

load data local inpath '../../data/files/T1.txt' overwrite into table table1 partition (ds='2');

load data local inpath '../../data/files/T1.txt' overwrite into table table2;
load data local inpath '../../data/files/T2.txt' overwrite into table table2;

set hive.optimize.bucketmapjoin = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

select /*+ mapjoin(b) */ count(*) from table1 a join table2 b
on a.key=b.key and a.value=b.value and a.ds is not null;

drop table table1;
drop table table2;

;


create table table1(key string, value string) clustered by (key, value)
sorted by (key, value) into 1 BUCKETS stored as textfile;
create table table2(key string, value string) clustered by (value, key)
sorted by (value, key) into 1 BUCKETS stored as textfile;

load data local inpath '../../data/files/SortCol1Col2.txt' overwrite into table table1;
load data local inpath '../../data/files/SortCol2Col1.txt' overwrite into table table2;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

-- The tables are bucketed in same columns in different order,
-- but sorted in different column orders
-- Neither bucketed map-join, nor sort-merge join should be performed

explain extended
select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value;

select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value;

drop table table1;
drop table table2;

;


create table table1(key string, value string) clustered by (key, value)
sorted by (key desc, value desc) into 1 BUCKETS stored as textfile;
create table table2(key string, value string) clustered by (value, key)
sorted by (value desc, key desc) into 1 BUCKETS stored as textfile;

load data local inpath '../../data/files/SortCol1Col2.txt' overwrite into table table1;
load data local inpath '../../data/files/SortCol2Col1.txt' overwrite into table table2;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

-- The tables are bucketed in same columns in different order,
-- but sorted in different column orders
-- Neither bucketed map-join, nor sort-merge join should be performed

explain extended
select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value;

select /*+ mapjoin(b) */ count(*) from table1 a join table2 b on a.key=b.key and a.value=b.value;

set hive.mapred.mode=nonstrict;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.auto.convert.join = true;

set hive.optimize.bucketmapjoin = true;

create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;

explain extended
insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.mapred.mode=nonstrict;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.auto.convert.join = true;

set hive.optimize.bucketmapjoin = true;

create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = true;

explain extended
insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.mapred.mode=nonstrict;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.auto.convert.join = true;

set hive.optimize.bucketmapjoin = true;

create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;

explain extended
insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select a.key, a.value, b.value
from srcbucket_mapjoin_part a join srcbucket_mapjoin_part_2 b
on a.key=b.key and b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.mapred.mode=nonstrict;
;

set hive.exec.reducers.max = 1;

-- SORT_QUERY_RESULTS

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1
select * from src where key < 10;

insert overwrite table tbl2
select * from src where key < 10;

insert overwrite table tbl3
select * from src where key < 10;

;

set hive.exec.reducers.max = 100;

set hive.auto.convert.join=true;

set hive.optimize.bucketmapjoin = true;

explain extended
select a.key as key, a.value as val1, b.value as val2, c.value as val3
from tbl1 a join tbl2 b on a.key = b.key join tbl3 c on a.value = c.value;

select a.key as key, a.value as val1, b.value as val2, c.value as val3
from tbl1 a join tbl2 b on a.key = b.key join tbl3 c on a.value = c.value;

set hive.optimize.bucketmapjoin = false;

explain extended
select a.key as key, a.value as val1, b.value as val2, c.value as val3
from tbl1 a join tbl2 b on a.key = b.key join tbl3 c on a.value = c.value;

select a.key as key, a.value as val1, b.value as val2, c.value as val3
from tbl1 a join tbl2 b on a.key = b.key join tbl3 c on a.value = c.value;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin_part;

CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

set hive.convert.join.bucket.mapjoin.tez = true;
explain
select a.key, a.value, b.value
from tab a join tab_part b on a.key = b.key;

explain
select count(*)
from
(select distinct key, value from tab_part) a join tab b on a.key = b.key;

select count(*)
from
(select distinct key, value from tab_part) a join tab b on a.key = b.key;

explain
select count(*)
from
(select a.key as key, a.value as value from tab a join tab_part b on a.key = b.key) c
join
tab_part d on c.key = d.key;

select count(*)
from
(select a.key as key, a.value as value from tab a join tab_part b on a.key = b.key) c
join
tab_part d on c.key = d.key;

explain
select count(*)
from
tab_part d
join
(select a.key as key, a.value as value from tab a join tab_part b on a.key = b.key) c on c.key = d.key;

select count(*)
from
tab_part d
join
(select a.key as key, a.value as value from tab a join tab_part b on a.key = b.key) c on c.key = d.key;


-- one side is really bucketed. srcbucket_mapjoin is not really a bucketed table.
-- In this case the sub-query is chosen as the big table.
explain
select a.k1, a.v1, b.value
from (select sum(substr(srcbucket_mapjoin.value,5)) as v1, key as k1 from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
join tab b on a.k1 = b.key;

explain
select a.k1, a.v1, b.value
from (select sum(substr(tab.value,5)) as v1, key as k1 from tab_part join tab on tab_part.key = tab.key GROUP BY tab.key) a
join tab b on a.k1 = b.key;

explain
select a.k1, a.v1, b.value
from (select sum(substr(x.value,5)) as v1, x.key as k1 from tab x join tab y on x.key = y.key GROUP BY x.key) a
join tab_part b on a.k1 = b.key;

-- multi-way join
explain
select a.key, a.value, b.value
from tab_part a join tab b on a.key = b.key join tab c on a.key = c.key;

explain
select a.key, a.value, c.value
from (select x.key, x.value from tab_part x join tab y on x.key = y.key) a join tab c on a.key = c.key;

-- in this case sub-query is the small table
explain
select a.key, a.value, b.value
from (select key, sum(substr(srcbucket_mapjoin.value,5)) as value from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
join tab_part b on a.key = b.key;

set hive.map.aggr=false;
explain
select a.key, a.value, b.value
from (select key, sum(substr(srcbucket_mapjoin.value,5)) as value from srcbucket_mapjoin GROUP BY srcbucket_mapjoin.key) a
join tab_part b on a.key = b.key;

-- join on non-bucketed column results in broadcast join.
explain
select a.key, a.value, b.value
from tab a join tab_part b on a.value = b.value;

CREATE TABLE tab1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab1
select key,value from srcbucket_mapjoin;

explain
select a.key, a.value, b.value
from tab1 a join tab_part b on a.key = b.key;

explain select a.key, b.key from tab_part a join tab_part c on a.key = c.key join tab_part b on a.value = b.value;

explain
select a.key, a.value, b.value
from tab a join tab_part b on a.key = b.key and a.ds = b.ds;

set hive.mapjoin.hybridgrace.hashtable = false;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin where key = 411;

explain
select count(*)
from tab_part a join tab b on a.key = b.key;

select count(*)
from tab_part a join tab b on a.key = b.key;

set hive.mapjoin.hybridgrace.hashtable = false;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin where key = 411;

explain
select count(*)
from tab_part a join tab b on a.key = b.key;

select count(*)
from tab_part a join tab b on a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin_part;

CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

set hive.convert.join.bucket.mapjoin.tez = true;

explain select a.key, b.key from tab_part a join tab_part c on a.key = c.key join tab_part b on a.value = b.value;

CREATE TABLE tab1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab1
select key,value from srcbucket_mapjoin;

explain
select a.key, a.value, b.value
from tab1 a join src b on a.key = b.key;

explain
select a.key, b.key from (select key from tab_part where key > 1) a join (select key from tab_part where key > 2) b on a.key = b.key;

explain
select a.key, b.key from (select key from tab_part where key > 1) a left outer join (select key from tab_part where key > 2) b on a.key = b.key;

explain
select a.key, b.key from (select key from tab_part where key > 1) a right outer join (select key from tab_part where key > 2) b on a.key = b.key;

explain select a.key, b.key from (select distinct key from tab) a join tab b on b.key = a.key;

explain select a.value, b.value from (select distinct value from tab) a join tab b on b.key = a.value;
;
set hive.exec.mode.local.auto=false;
set mapred.reduce.tasks = 10;

-- This test sets number of mapred tasks to 10 for a database with 50 buckets,
-- and uses a post-hook to confirm that 10 tasks were created

CREATE TABLE bucket_nr(key int, value string) CLUSTERED BY (key) INTO 50 BUCKETS;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyNumReducersHook;
set VerifyNumReducersHook.num.reducers=10;

insert overwrite table bucket_nr
select * from src;

set hive.exec.post.hooks=;
drop table bucket_nr;
;
set hive.exec.mode.local.auto=false;
set hive.exec.reducers.max = 2;

-- This test sets the maximum number of reduce tasks to 2 for overwriting a
-- table with 3 buckets, and uses a post-hook to confirm that 1 reducer was used

CREATE TABLE test_table(key int, value string) CLUSTERED BY (key) INTO 3 BUCKETS;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyNumReducersHook;
set VerifyNumReducersHook.num.reducers=1;

insert overwrite table test_table
select * from src;
set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.VerifyCachingPrintStreamHook;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyCachingPrintStreamHook;

SELECT count(*) FROM src;
FROM src SELECT TRANSFORM (key, value) USING 'FAKE_SCRIPT_SHOULD_NOT_EXIST' AS key, value;

set hive.exec.failure.hooks=;
set hive.exec.post.hooks=;

CREATE DATABASE hbaseDB;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20, 0.20S)
-- Hadoop 0.23 changes the behavior FsShell on Exit Codes
-- In Hadoop 0.20
-- Exit Code == 0 on success
-- Exit code < 0 on any failure
-- In Hadoop 0.23
-- Exit Code == 0 on success
-- Exit Code < 0 on syntax/usage error
-- Exit Code > 0 operation failed

CREATE TABLE hbaseDB.hbase_table_0(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
TBLPROPERTIES ("hbase.table.name" = "hbase_table_0");

dfs -ls target/tmp/hbase/data/default/hbase_table_0;

DROP DATABASE IF EXISTS hbaseDB CASCADE;

dfs -ls target/tmp/hbase/data/default/hbase_table_0;







CREATE DATABASE hbaseDB;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20, 0.20S)
-- Hadoop 0.23 changes the behavior FsShell on Exit Codes
-- In Hadoop 0.20
-- Exit Code == 0 on success
-- Exit code < 0 on any failure
-- In Hadoop 0.23
-- Exit Code == 0 on success
-- Exit Code < 0 on syntax/usage error
-- Exit Code > 0 operation failed

CREATE TABLE hbaseDB.hbase_table_0(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string")
TBLPROPERTIES ("hbase.table.name" = "hbase_table_0");

dfs -ls target/tmp/hbase/data/default/hbase_table_0;

DROP DATABASE IF EXISTS hbaseDB CASCADE;

dfs -ls target/tmp/hbase/data/hbase/default/hbase_table_0;






FROM SRC_THRIFT
INSERT OVERWRITE TABLE dest1 SELECT src_Thrift.LINT[1], src_thrift.lintstring[0].MYSTRING where src_thrift.liNT[0] > 0
CREATE TABLE DEST1(Key INT, VALUE STRING) STORED AS TEXTFILE;

EXPLAIN
FROM SRC_THRIFT
INSERT OVERWRITE TABLE dest1 SELECT src_Thrift.LINT[1], src_thrift.lintstring[0].MYSTRING where src_thrift.liNT[0] > 0;

FROM SRC_THRIFT
INSERT OVERWRITE TABLE dest1 SELECT src_Thrift.LINT[1], src_thrift.lintstring[0].MYSTRING where src_thrift.liNT[0] > 0;

SELECT DEST1.* FROM Dest1;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;

drop temporary function row_sequence;

add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;
create temporary function row_sequence as
'org.apache.hadoop.hive.contrib.udf.UDFRowSequence';

-- make sure a stateful function inside of CASE throws an exception
-- since the short-circuiting requirements are contradictory
SELECT CASE WHEN 3 > 2 THEN 10 WHEN row_sequence() > 5 THEN 20 ELSE 30 END
FROM src LIMIT 1;
FROM src
SELECT 3 + 2, 3.0 + 2, 3 + 2.0, 3.0 + 2.0, 3 + CAST(2.0 AS INT), CAST(1 AS BOOLEAN), CAST(TRUE AS INT) WHERE src.key = 86
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(c1 INT, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 INT, c6 STRING, c7 INT) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT 3 + 2, 3.0 + 2, 3 + 2.0, 3.0 + 2.0, 3 + CAST(2.0 AS INT) + CAST(CAST(0 AS SMALLINT) AS INT), CAST(1 AS BOOLEAN), CAST(TRUE AS INT) WHERE src.key = 86;

FROM src INSERT OVERWRITE TABLE dest1 SELECT 3 + 2, 3.0 + 2, 3 + 2.0, 3.0 + 2.0, 3 + CAST(2.0 AS INT) + CAST(CAST(0 AS SMALLINT) AS INT), CAST(1 AS BOOLEAN), CAST(TRUE AS INT) WHERE src.key = 86;

select dest1.* FROM dest1;

select
  cast(key as decimal(10,2)) as c1,
  cast(key as char(10)) as c2,
  cast(key as varchar(10)) as c3
from src
order by c1, c2, c3
limit 1;
drop table t;
CREATE TABLE t(c tinyint);
insert overwrite table t select 10 from src limit 1;

select * from t where c = 10.0;

select * from t where c = -10.0;set hive.fetch.task.conversion=more;

-- cast string floats to integer types
select
  cast('1' as float),
  cast('1.4' as float),
  cast('1.6' as float),
  cast('1' as int),
  cast('1.4' as int),
  cast('1.6' as int),
  cast('1' as tinyint),
  cast('1.4' as tinyint),
  cast('1.6' as tinyint),
  cast('1' as smallint),
  cast('1.4' as smallint),
  cast('1.6' as smallint),
  cast('1' as bigint),
  cast('1.4' as bigint),
  cast('1.6' as bigint),
  cast (cast('1' as float) as int),
  cast(cast ('1.4' as float) as int),
  cast(cast ('1.6' as float) as int),
  cast('+1e5' as int),
  cast('2147483647' as int),
  cast('-2147483648' as int),
  cast('32767' as smallint),
  cast('-32768' as smallint),
  cast('-128' as tinyint),
  cast('127' as tinyint),
  cast('1.0a' as int),
  cast('-1.-1' as int)
from src tablesample (1 rows);
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS

-- 6. Test Select + TS + Join + Fil + GB + GB Having
select key, (c_int+1)+2 as x, sum(c_int) from cbo_t1 group by c_float, cbo_t1.c_int, key;

select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from cbo_t1 group by c_float, cbo_t1.c_int, key) R group by y, x;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0) group by c_float, cbo_t1.c_int, key order by a) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b % c asc, b desc) cbo_t1 left outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key  having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c  having cbo_t3.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0  order by cbo_t3.c_int % c asc, cbo_t3.c_int desc;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b+c, a desc) cbo_t1 right outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 2) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by c+a desc) cbo_t1 full outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by p+q desc, r asc) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c having cbo_t3.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0 order by cbo_t3.c_int;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 21. Test groupby is empty and there is no other cols in aggr
select unionsrc.key FROM (select 'tst1' as key, count(1) as value from src) unionsrc;

select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src) unionsrc;

select unionsrc.key FROM (select 'max' as key, max(c_int) as value from cbo_t3 s1
	UNION  ALL
    	select 'min' as key,  min(c_int) as value from cbo_t3 s2
    UNION ALL
        select 'avg' as key,  avg(c_int) as value from cbo_t3 s3) unionsrc order by unionsrc.key;

select unionsrc.key, unionsrc.value FROM (select 'max' as key, max(c_int) as value from cbo_t3 s1
	UNION  ALL
    	select 'min' as key,  min(c_int) as value from cbo_t3 s2
    UNION ALL
        select 'avg' as key,  avg(c_int) as value from cbo_t3 s3) unionsrc order by unionsrc.key;

select unionsrc.key, count(1) FROM (select 'max' as key, max(c_int) as value from cbo_t3 s1
    UNION  ALL
        select 'min' as key,  min(c_int) as value from cbo_t3 s2
    UNION ALL
        select 'avg' as key,  avg(c_int) as value from cbo_t3 s3) unionsrc group by unionsrc.key order by unionsrc.key;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS
-- 4. Test Select + Join + TS
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 join             cbo_t2 on cbo_t1.key=cbo_t2.key;
select cbo_t1.key from cbo_t1 join cbo_t3;
select cbo_t1.key from cbo_t1 join cbo_t3 where cbo_t1.key=cbo_t3.key and cbo_t1.key >= 1;
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 left outer join  cbo_t2 on cbo_t1.key=cbo_t2.key;
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 right outer join cbo_t2 on cbo_t1.key=cbo_t2.key;
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 full outer join  cbo_t2 on cbo_t1.key=cbo_t2.key;

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;
select a, cbo_t1.b, key, cbo_t2.c_int, cbo_t3.p from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join cbo_t2  on cbo_t1.a=key join (select key as p, c_int as q, cbo_t3.c_float as r from cbo_t3)cbo_t3 on cbo_t1.a=cbo_t3.p;
select b, cbo_t1.c, cbo_t2.c_int, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join cbo_t2 on cbo_t1.a=cbo_t2.key join cbo_t3 on cbo_t1.a=cbo_t3.key;
select cbo_t3.c_int, b, cbo_t2.c_int, cbo_t1.c from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join cbo_t2 on cbo_t1.a=cbo_t2.key join cbo_t3 on cbo_t1.a=cbo_t3.key;

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p left outer join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p right outer join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p full outer join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;

-- 5. Test Select + Join + FIL + TS
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 join cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + cbo_t2.c_int == 2) and (cbo_t1.c_int > 0 or cbo_t2.c_float >= 0);
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 left outer join  cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + cbo_t2.c_int == 2) and (cbo_t1.c_int > 0 or cbo_t2.c_float >= 0);
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 right outer join cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + cbo_t2.c_int == 2) and (cbo_t1.c_int > 0 or cbo_t2.c_float >= 0);
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 full outer join  cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + cbo_t2.c_int == 2) and (cbo_t1.c_int > 0 or cbo_t2.c_float >= 0);

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or cbo_t2.q >= 0);

select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0);

select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0);

select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 7. Test Select + TS + Join + Fil + GB + GB Having + Limit
select key, (c_int+1)+2 as x, sum(c_int) from cbo_t1 group by c_float, cbo_t1.c_int, key order by x limit 1;
select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from cbo_t1 group by c_float, cbo_t1.c_int, key) R group by y, x order by x,y limit 1;
select key from(select key from (select key from cbo_t1 limit 5)cbo_t2  limit 5)cbo_t3  limit 5;
select key, c_int from(select key, c_int from (select key, c_int from cbo_t1 order by c_int limit 5)cbo_t1  order by c_int limit 5)cbo_t2  order by c_int limit 5;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0) group by c_float, cbo_t1.c_int, key order by a limit 5) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc limit 5) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c limit 5;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b % c asc, b desc limit 5) cbo_t1 left outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key  having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 limit 5) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c  having cbo_t3.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0  order by cbo_t3.c_int % c asc, cbo_t3.c_int, c desc limit 5;
set hive.mapred.mode=nonstrict;
set hive.cbo.returnpath.hiveop=true;
set hive.stats.fetch.column.stats=true;
set hive.map.aggr.hash.percentmemory=0.0f;

-- hash aggregation is disabled

-- There are different cases for Group By depending on map/reduce side, hash aggregation,
-- grouping sets and column stats. If we don't have column stats, we just assume hash
-- aggregation is disabled. Following are the possible cases and rule for cardinality
-- estimation

-- MAP SIDE:
-- Case 1: NO column stats, NO hash aggregation, NO grouping sets — numRows
-- Case 2: NO column stats, NO hash aggregation, grouping sets — numRows * sizeOfGroupingSet
-- Case 3: column stats, hash aggregation, NO grouping sets — Min(numRows / 2, ndvProduct * parallelism)
-- Case 4: column stats, hash aggregation, grouping sets — Min((numRows * sizeOfGroupingSet) / 2, ndvProduct * parallelism * sizeOfGroupingSet)
-- Case 5: column stats, NO hash aggregation, NO grouping sets — numRows
-- Case 6: column stats, NO hash aggregation, grouping sets — numRows * sizeOfGroupingSet

-- REDUCE SIDE:
-- Case 7: NO column stats — numRows / 2
-- Case 8: column stats, grouping sets — Min(numRows, ndvProduct * sizeOfGroupingSet)
-- Case 9: column stats, NO grouping sets - Min(numRows, ndvProduct)

create table if not exists loc_staging (
  state string,
  locid int,
  zip bigint,
  year int
) row format delimited fields terminated by '|' stored as textfile;

create table loc_orc like loc_staging;
alter table loc_orc set fileformat orc;

load data local inpath '../../data/files/loc.txt' overwrite into table loc_staging;

insert overwrite table loc_orc select * from loc_staging;

-- numRows: 8 rawDataSize: 796
explain select * from loc_orc;

-- partial column stats
analyze table loc_orc compute statistics for columns state;

-- inner group by: map - numRows: 8 reduce - numRows: 4
-- outer group by: map - numRows: 4 reduce numRows: 2
explain select a, c, min(b)
from ( select state as a, locid as b, count(*) as c
       from loc_orc
       group by state,locid
     ) sq1
group by a,c;

analyze table loc_orc compute statistics for columns state,locid,year;

-- Case 5: column stats, NO hash aggregation, NO grouping sets - cardinality = 8
-- Case 9: column stats, NO grouping sets - caridnality = 2
explain select year from loc_orc group by year;

-- Case 5: column stats, NO hash aggregation, NO grouping sets - cardinality = 8
-- Case 9: column stats, NO grouping sets - caridnality = 8
explain select state,locid from loc_orc group by state,locid;

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 32
-- Case 8: column stats, grouping sets - cardinality = 32
explain select state,locid from loc_orc group by state,locid with cube;

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 24
-- Case 8: column stats, grouping sets - cardinality = 24
explain select state,locid from loc_orc group by state,locid with rollup;

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 8
-- Case 8: column stats, grouping sets - cardinality = 8
explain select state,locid from loc_orc group by state,locid grouping sets((state));

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 16
-- Case 8: column stats, grouping sets - cardinality = 16
explain select state,locid from loc_orc group by state,locid grouping sets((state),(locid));

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 24
-- Case 8: column stats, grouping sets - cardinality = 24
explain select state,locid from loc_orc group by state,locid grouping sets((state),(locid),());

-- Case 6: column stats, NO hash aggregation, grouping sets - cardinality = 32
-- Case 8: column stats, grouping sets - cardinality = 32
explain select state,locid from loc_orc group by state,locid grouping sets((state,locid),(state),(locid),());

set hive.map.aggr.hash.percentmemory=0.5f;
set mapred.max.split.size=80;
-- map-side parallelism will be 10

-- Case 3: column stats, hash aggregation, NO grouping sets - cardinality = 4
-- Case 9: column stats, NO grouping sets - caridnality = 2
explain select year from loc_orc group by year;

-- Case 4: column stats, hash aggregation, grouping sets - cardinality = 16
-- Case 8: column stats, grouping sets - cardinality = 16
explain select state,locid from loc_orc group by state,locid with cube;

-- ndvProduct becomes 0 as zip does not have column stats
-- Case 3: column stats, hash aggregation, NO grouping sets - cardinality = 4
-- Case 9: column stats, NO grouping sets - caridnality = 2
explain select state,zip from loc_orc group by state,zip;

set mapred.max.split.size=1000;
set hive.stats.fetch.column.stats=false;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 32
-- Case 7: NO column stats - cardinality = 16
explain select state,locid from loc_orc group by state,locid with cube;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 24
-- Case 7: NO column stats - cardinality = 12
explain select state,locid from loc_orc group by state,locid with rollup;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 8
-- Case 7: NO column stats - cardinality = 4
explain select state,locid from loc_orc group by state,locid grouping sets((state));

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 16
-- Case 7: NO column stats - cardinality = 8
explain select state,locid from loc_orc group by state,locid grouping sets((state),(locid));

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 24
-- Case 7: NO column stats - cardinality = 12
explain select state,locid from loc_orc group by state,locid grouping sets((state),(locid),());

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 32
-- Case 7: NO column stats - cardinality = 16
explain select state,locid from loc_orc group by state,locid grouping sets((state,locid),(state),(locid),());

set mapred.max.split.size=80;

-- Case 1: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 8
-- Case 7: NO column stats - cardinality = 4
explain select year from loc_orc group by year;

-- Case 2: NO column stats, NO hash aggregation, NO grouping sets - cardinality = 32
-- Case 7: NO column stats - cardinality = 16
explain select state,locid from loc_orc group by state,locid with cube;

set hive.mapred.mode=nonstrict;
set hive.cbo.returnpath.hiveop=true;
set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join = true;

explain
select sum(hash(a.k1,a.v1,a.k2, a.v2))
from (
SELECT cbo_t1.key as k1, cbo_t1.value as v1,
       cbo_t2.key as k2, cbo_t2.value as v2 FROM
  (SELECT * FROM cbo_t3 WHERE cbo_t3.key < 10) cbo_t1
    JOIN
  (SELECT * FROM cbo_t3 WHERE cbo_t3.key < 10) cbo_t2
  SORT BY k1, v1, k2, v2
) a;

explain
select sum(hash(a.k1,a.v1,a.k2, a.v2))
from (
SELECT cbo_t1.key as k1, cbo_t1.value as v1,
       cbo_t2.key as k2, cbo_t2.value as v2 FROM
  (SELECT * FROM cbo_t3 WHERE cbo_t3.key < 10) cbo_t1
    JOIN
  (SELECT * FROM cbo_t3 WHERE cbo_t3.key < 10) cbo_t2
  SORT BY k1, v1, k2, v2
) a;
set hive.cbo.returnpath.hiveop=true;
set hive.stats.fetch.column.stats=true;
;

set hive.exec.reducers.max = 1;
set hive.transpose.aggr.join=true;
-- SORT_QUERY_RESULTS

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1
select * from src where key < 10;

insert overwrite table tbl2
select * from src where key < 10;

analyze table tbl1 compute statistics;
analyze table tbl1 compute statistics for columns;

analyze table tbl2 compute statistics;
analyze table tbl2 compute statistics for columns;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

set hive.auto.convert.sortmerge.join=true;

-- The join is being performed as part of sub-query. It should be converted to a sort-merge join
explain
select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

-- The join is being performed as part of more than one sub-query. It should be converted to a sort-merge join
explain
select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

-- A join is being performed across different sub-queries, where a join is being performed in each of them.
-- Each sub-query should be converted to a sort-merge join.
explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join, although there is more than one level of sub-query
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

-- Both the tables are nested sub-queries i.e more then 1 level of sub-query.
-- The join should be converted to a sort-merge join
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters and the join key
-- is not getting modified, it should be converted to a sort-merge join. Note that the sub-query modifies one
-- item, but that is not part of the join key.
explain
select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

-- Since the join key is modified by the sub-query, neither sort-merge join not bucketized map-side
-- join should be performed
explain
select count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

-- One of the tables is a sub-query and the other is not.
-- It should be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

-- There are more than 2 inputs to the join, all of them being sub-queries.
-- It should be converted to to a sort-merge join
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on (subq1.key = subq2.key)
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

-- The join is being performed on a nested sub-query, and an aggregation is performed after that.
-- The join should be converted to a sort-merge join
explain
select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

-- The join is followed by a multi-table insert. It should be converted to
-- a sort-merge join
explain select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key;

select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key;

-- The join is followed by a multi-table insert, and one of the inserts involves a reducer.
-- It should be converted to a sort-merge join
explain select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key;

select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key;

set hive.mapred.mode=nonstrict;
set hive.cbo.returnpath.hiveop=true;
set hive.auto.convert.join = true;

CREATE TABLE dest1(key1 INT, value1 STRING, key2 INT, value2 STRING) STORED AS TEXTFILE;

explain
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.*, src2.*;


FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.*, src2.*;

SELECT sum(hash(dest1.key1,dest1.value1,dest1.key2,dest1.value2)) FROM dest1;set hive.mapred.mode=nonstrict;
set hive.cbo.returnpath.hiveop=true;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

create table A as
select * from src;

create table B as
select * from src order by key
limit 10;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;

explain select * from A join B;

explain select * from B d1 join B d2 on d1.key = d2.key join A;

explain select * from A join
         (select d1.key
          from B d1 join B d2 on d1.key = d2.key
          where 1 = 1 group by d1.key) od1;

explain select * from A join (select d1.key from B d1 join B d2 where 1 = 1 group by d1.key) od1;

explain select * from
(select A.key from A group by key) ss join
(select d1.key from B d1 join B d2 on d1.key = d2.key where 1 = 1 group by d1.key) od1;


set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS

-- 6. Test Select + TS + Join + Fil + GB + GB Having
select key, (c_int+1)+2 as x, sum(c_int) from cbo_t1 group by c_float, cbo_t1.c_int, key;

select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from cbo_t1 group by c_float, cbo_t1.c_int, key) R group by y, x;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0) group by c_float, cbo_t1.c_int, key order by a) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b % c asc, b desc) cbo_t1 left outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key  having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c  having cbo_t3.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0  order by cbo_t3.c_int % c asc, cbo_t3.c_int desc;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b+c, a desc) cbo_t1 right outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 2) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by c+a desc) cbo_t1 full outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by p+q desc, r asc) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c having cbo_t3.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0 order by cbo_t3.c_int;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;

set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value)
GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value)
GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;

-- HIVE-5560 when group by key is used in distinct funtion, invalid result are returned

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.key,1,1)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value)
GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.key,1,1)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value)
GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 21. Test groupby is empty and there is no other cols in aggr
select unionsrc.key FROM (select 'tst1' as key, count(1) as value from src) unionsrc;

select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src) unionsrc;

select unionsrc.key FROM (select 'max' as key, max(c_int) as value from cbo_t3 s1
	UNION  ALL
    	select 'min' as key,  min(c_int) as value from cbo_t3 s2
    UNION ALL
        select 'avg' as key,  avg(c_int) as value from cbo_t3 s3) unionsrc order by unionsrc.key;

select unionsrc.key, unionsrc.value FROM (select 'max' as key, max(c_int) as value from cbo_t3 s1
	UNION  ALL
    	select 'min' as key,  min(c_int) as value from cbo_t3 s2
    UNION ALL
        select 'avg' as key,  avg(c_int) as value from cbo_t3 s3) unionsrc order by unionsrc.key;

select unionsrc.key, count(1) FROM (select 'max' as key, max(c_int) as value from cbo_t3 s1
    UNION  ALL
        select 'min' as key,  min(c_int) as value from cbo_t3 s2
    UNION ALL
        select 'avg' as key,  avg(c_int) as value from cbo_t3 s3) unionsrc group by unionsrc.key order by unionsrc.key;

set hive.cbo.returnpath.hiveop=true;
set hive.map.aggr=false;
set hive.mapred.mode=nonstrict;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE, c10 DOUBLE, c11 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

SELECT dest1.* FROM dest1;

set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;

drop database if exists x314 cascade;
create database x314;
use x314;
create table source(s1 int, s2 int);
create table target1(x int, y int, z int);

insert into source(s2,s1) values(2,1);
-- expect source to contain 1 row (1,2)
select * from source;
insert into target1(z,x) select * from source;
-- expect target1 to contain 1 row (2,NULL,1)
select * from target1;

drop database if exists x314 cascade;set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS
-- 4. Test Select + Join + TS
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 join             cbo_t2 on cbo_t1.key=cbo_t2.key;
select cbo_t1.key from cbo_t1 join cbo_t3;
select cbo_t1.key from cbo_t1 join cbo_t3 where cbo_t1.key=cbo_t3.key and cbo_t1.key >= 1;
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 left outer join  cbo_t2 on cbo_t1.key=cbo_t2.key;
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 right outer join cbo_t2 on cbo_t1.key=cbo_t2.key;
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 full outer join  cbo_t2 on cbo_t1.key=cbo_t2.key;

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;
select a, cbo_t1.b, key, cbo_t2.c_int, cbo_t3.p from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join cbo_t2  on cbo_t1.a=key join (select key as p, c_int as q, cbo_t3.c_float as r from cbo_t3)cbo_t3 on cbo_t1.a=cbo_t3.p;
select b, cbo_t1.c, cbo_t2.c_int, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join cbo_t2 on cbo_t1.a=cbo_t2.key join cbo_t3 on cbo_t1.a=cbo_t3.key;
select cbo_t3.c_int, b, cbo_t2.c_int, cbo_t1.c from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join cbo_t2 on cbo_t1.a=cbo_t2.key join cbo_t3 on cbo_t1.a=cbo_t3.key;

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p left outer join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p right outer join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p full outer join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;

-- 5. Test Select + Join + FIL + TS
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 join cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + cbo_t2.c_int == 2) and (cbo_t1.c_int > 0 or cbo_t2.c_float >= 0);
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 left outer join  cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + cbo_t2.c_int == 2) and (cbo_t1.c_int > 0 or cbo_t2.c_float >= 0);
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 right outer join cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + cbo_t2.c_int == 2) and (cbo_t1.c_int > 0 or cbo_t2.c_float >= 0);
select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 full outer join  cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + cbo_t2.c_int == 2) and (cbo_t1.c_int > 0 or cbo_t2.c_float >= 0);

select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or cbo_t2.q >= 0);

select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0);

select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0);

select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS
-- Merge join into multijoin operator 1
explain select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join
(select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p right outer join
(select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3) cbo_t3 on cbo_t1.key=a;

select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join
(select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p right outer join
(select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3) cbo_t3 on cbo_t1.key=a;

-- Merge join into multijoin operator 2
explain select key, c_int, cbo_t2.p, cbo_t2.q, cbo_t3.x, cbo_t4.b from cbo_t1 join
(select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p right outer join
(select cbo_t3.key as x, cbo_t3.c_int as y, c_float as z from cbo_t3) cbo_t3 on cbo_t1.key=x left outer join
(select key as a, c_int as b, c_float as c from cbo_t1) cbo_t4 on cbo_t1.key=a;

select key, c_int, cbo_t2.p, cbo_t2.q, cbo_t3.x, cbo_t4.b from cbo_t1 join
(select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p right outer join
(select cbo_t3.key as x, cbo_t3.c_int as y, c_float as z from cbo_t3) cbo_t3 on cbo_t1.key=x left outer join
(select key as a, c_int as b, c_float as c from cbo_t1) cbo_t4 on cbo_t1.key=a;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;

CREATE TABLE myinput1(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in3.txt' INTO TABLE myinput1;

SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
SET hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

SET hive.outerjoin.supports.filters = false;

EXPLAIN SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key = 40 AND b.key = 40;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key = 40 AND b.key = 40;

EXPLAIN SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key = 40 AND a.value = 40 AND a.key = a.value AND b.key = 40;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key = 40 AND a.key = a.value AND b.key = 40;

EXPLAIN SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key = 40 AND a.key = b.key AND b.key = 40;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key = 40 AND a.key = b.key AND b.key = 40;

EXPLAIN SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 7. Test Select + TS + Join + Fil + GB + GB Having + Limit
select key, (c_int+1)+2 as x, sum(c_int) from cbo_t1 group by c_float, cbo_t1.c_int, key order by x limit 1;
select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from cbo_t1 group by c_float, cbo_t1.c_int, key) R group by y, x order by x,y limit 1;
select key from(select key from (select key from cbo_t1 limit 5)cbo_t2  limit 5)cbo_t3  limit 5;
select key, c_int from(select key, c_int from (select key, c_int from cbo_t1 order by c_int limit 5)cbo_t1  order by c_int limit 5)cbo_t2  order by c_int limit 5;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0) group by c_float, cbo_t1.c_int, key order by a limit 5) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc limit 5) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c limit 5;

select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b % c asc, b desc limit 5) cbo_t1 left outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key  having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 limit 5) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c  having cbo_t3.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0  order by cbo_t3.c_int % c asc, cbo_t3.c_int, c desc limit 5;
set hive.mapred.mode=nonstrict;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.LineageLogger;

drop table if exists src2;
create table src2 as select key key2, value value2 from src1;

select * from src1 where key is not null and value is not null limit 3;
select * from src1 where key > 10 and value > 'val' order by key limit 5;

drop table if exists dest1;
create table dest1 as select * from src1;
insert into table dest1 select * from src2;

select key k, dest1.value from dest1;
select key from src1 union select key2 from src2 order by key;
select key k from src1 union select key2 from src2 order by k;

select key, count(1) a from dest1 group by key;
select key k, count(*) from dest1 group by key;
select key k, count(value) from dest1 group by key;
select value, max(length(key)) from dest1 group by value;
select value, max(length(key)) from dest1 group by value order by value limit 5;

select key, length(value) from dest1;
select length(value) + 3 from dest1;
select 5 from dest1;
select 3 * 5 from dest1;

drop table if exists dest2;
create table dest2 as select * from src1 JOIN src2 ON src1.key = src2.key2;
insert overwrite table dest2 select * from src1 JOIN src2 ON src1.key = src2.key2;
insert into table dest2 select * from src1 JOIN src2 ON src1.key = src2.key2;
insert into table dest2
  select * from src1 JOIN src2 ON length(src1.value) = length(src2.value2) + 1;

select * from src1 where length(key) > 2;
select * from src1 where length(key) > 2 and value > 'a';

drop table if exists dest3;
create table dest3 as
  select * from src1 JOIN src2 ON src1.key = src2.key2 WHERE length(key) > 1;
insert overwrite table dest2
  select * from src1 JOIN src2 ON src1.key = src2.key2 WHERE length(key) > 3;

drop table if exists dest_l1;
CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE;

INSERT OVERWRITE TABLE dest_l1
SELECT j.*
FROM (SELECT t1.key, p1.value
      FROM src1 t1
      LEFT OUTER JOIN src p1
      ON (t1.key = p1.key)
      UNION ALL
      SELECT t2.key, p2.value
      FROM src1 t2
      LEFT OUTER JOIN src p2
      ON (t2.key = p2.key)) j;

drop table if exists emp;
drop table if exists dept;
drop table if exists project;
drop table if exists tgt;
create table emp(emp_id int, name string, mgr_id int, dept_id int);
create table dept(dept_id int, dept_name string);
create table project(project_id int, project_name string);
create table tgt(dept_name string, name string,
  emp_id int, mgr_id int, proj_id int, proj_name string);

INSERT INTO TABLE tgt
SELECT emd.dept_name, emd.name, emd.emp_id, emd.mgr_id, p.project_id, p.project_name
FROM (
  SELECT d.dept_name, em.name, em.emp_id, em.mgr_id, em.dept_id
  FROM (
    SELECT e.name, e.dept_id, e.emp_id emp_id, m.emp_id mgr_id
    FROM emp e JOIN emp m ON e.emp_id = m.emp_id
    ) em
  JOIN dept d ON d.dept_id = em.dept_id
  ) emd JOIN project p ON emd.dept_id = p.project_id;

drop table if exists dest_l2;
create table dest_l2 (id int, c1 tinyint, c2 int, c3 bigint) stored as textfile;
insert into dest_l2 values(0, 1, 100, 10000);

select * from (
  select c1 + c2 x from dest_l2
  union all
  select sum(c3) y from (select c3 from dest_l2) v1) v2 order by x;

drop table if exists dest_l3;
create table dest_l3 (id int, c1 string, c2 string, c3 int) stored as textfile;
insert into dest_l3 values(0, "s1", "s2", 15);

select sum(a.c1) over (partition by a.c1 order by a.id)
from dest_l2 a
where a.c2 != 10
group by a.c1, a.c2, a.id
having count(a.c2) > 0;

select sum(a.c1), count(b.c1), b.c2, b.c3
from dest_l2 a join dest_l3 b on (a.id = b.id)
where a.c2 != 10 and b.c3 > 0
group by a.c1, a.c2, a.id, b.c1, b.c2, b.c3
having count(a.c2) > 0
order by b.c3 limit 5;

drop table if exists t;
create table t as
select distinct a.c2, a.c3 from dest_l2 a
inner join dest_l3 b on (a.id = b.id)
where a.id > 0 and b.c3 = 15;

SELECT substr(src1.key,1,1), count(DISTINCT substr(src1.value,5)),
concat(substr(src1.key,1,1),sum(substr(src1.value,5)))
from src1
GROUP BY substr(src1.key,1,1);

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.optimize.ppd=true;

-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

EXPLAIN EXTENDED
 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

EXPLAIN EXTENDED
 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';

 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 12. SemiJoin
select cbo_t1.c_int           from cbo_t1 left semi join   cbo_t2 on cbo_t1.key=cbo_t2.key;
select cbo_t1.c_int           from cbo_t1 left semi join   cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0);
select * from (select c, b, a from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left semi join cbo_t3 on cbo_t1.a=key where (b + 1 == 2) and (b > 0 or c >= 0)) R where  (b + 1 = 2) and (R.b > 0 or c >= 0);
select * from (select cbo_t3.c_int, cbo_t1.c, b from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 = 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t3.c_int  == 2) and (b > 0 or c_int >= 0)) R where  (R.c_int + 1 = 2) and (R.b > 0 or c_int >= 0);
select * from (select c_int, b, cbo_t1.c from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + 1 == 2) and (b > 0 or c_int >= 0)) R where  (c + 1 = 2) and (R.b > 0 or c_int >= 0);
select * from (select c_int, b, cbo_t1.c from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + 1 == 2) and (b > 0 or c_int >= 0)) R where  (c + 1 = 2) and (R.b > 0 or c_int >= 0);
select a, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by a+b desc, c asc) cbo_t1 left semi join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by q+r/10 desc, p) cbo_t2 on cbo_t1.a=p left semi join cbo_t3 on cbo_t1.a=key where (b + 1  >= 0) and (b > 0 or a >= 0) group by a, c  having a > 0 and (a >=1 or c >= 1) and (a + c) >= 0 order by c, a;
select a, c, count(*)  from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by a+b desc, c asc limit 5) cbo_t1 left semi join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by q+r/10 desc, p limit 5) cbo_t2 on cbo_t1.a=p left semi join cbo_t3 on cbo_t1.a=key where (b + 1  >= 0) and (b > 0 or a >= 0) group by a, c  having a > 0 and (a >=1 or c >= 1) and (a + c) >= 0 order by c, a;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 1. Test Select + TS
select * from cbo_t1;
select * from cbo_t1 as cbo_t1;
select * from cbo_t1 as cbo_t2;

select cbo_t1.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1;
select * from cbo_t1 where (((key=1) and (c_float=10)) and (c_int=20));

-- 2. Test Select + TS + FIL
select * from cbo_t1 where cbo_t1.c_int >= 0;
select * from cbo_t1 as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from cbo_t1 as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

select cbo_t2.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1 as cbo_t2  where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

-- 3 Test Select + Select + TS + FIL
select * from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1;
select * from (select * from cbo_t1 as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1;
select * from (select * from cbo_t1 as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1;
select * from (select cbo_t2.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1 as cbo_t2  where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1;

select * from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1 where cbo_t1.c_int >= 0;
select * from (select * from cbo_t1 as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from (select * from cbo_t1 as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from (select cbo_t2.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1 as cbo_t2  where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1 where cbo_t1.c_int >= 0 and y+c_int >= 0 or x <= 100;

select cbo_t1.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1 where cbo_t1.c_int >= 0;
select cbo_t2.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t2 where cbo_t2.c_int >= 0;



select * from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1 where cbo_t1.c_int >= 0;
select * from (select * from cbo_t1 as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from (select * from cbo_t1 as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from (select cbo_t2.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1 as cbo_t2  where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1 where cbo_t1.c_int >= 0 and y+c_int >= 0 or x <= 100;

select cbo_t1.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1 where cbo_t1.c_int >= 0;
select cbo_t2.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t2 where cbo_t2.c_int >= 0;



-- 13. null expr in select list
select null from cbo_t3;

-- 14. unary operator
select key from cbo_t1 where c_int = -6  or c_int = +6;

-- 15. query referencing only partition columns
select count(cbo_t1.dt) from cbo_t1 join cbo_t2 on cbo_t1.dt  = cbo_t2.dt  where cbo_t1.dt = '2014' ;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 20. Test get stats with empty partition list
select cbo_t1.value from cbo_t1 join cbo_t2 on cbo_t1.key = cbo_t2.key where cbo_t1.dt = '10' and cbo_t1.c_boolean = true;

set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 18. SubQueries Not Exists
-- distinct, corr
select *
from src_cbo b
where not exists
  (select distinct a.key
  from src_cbo a
  where b.value = a.value and a.value > 'val_2'
  )
;

-- no agg, corr, having
select *
from src_cbo b
group by key, value
having not exists
  (select a.key
  from src_cbo a
  where b.value = a.value  and a.key = b.key and a.value > 'val_12'
  )
;

-- 19. SubQueries Exists
-- view test
create view cv1 as
select *
from src_cbo b
where exists
  (select a.key
  from src_cbo a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9')
;

select * from cv1
;

-- sq in from
select *
from (select *
      from src_cbo b
      where exists
          (select a.key
          from src_cbo a
          where b.value = a.value  and a.key = b.key and a.value > 'val_9')
     ) a
;

-- sq in from, having
select *
from (select b.key, count(*)
  from src_cbo b
  group by b.key
  having exists
    (select a.key
    from src_cbo a
    where a.key = b.key and a.value > 'val_9'
    )
) a
;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 17. SubQueries In
-- non agg, non corr
select *
from src_cbo
where src_cbo.key in (select key from src_cbo s1 where s1.key > '9') order by key
;

-- agg, corr
-- add back once rank issue fixed for cbo

-- distinct, corr
select *
from src_cbo b
where b.key in
        (select distinct a.key
         from src_cbo a
         where b.value = a.value and a.key > '9'
        ) order by b.key
;

-- non agg, corr, with join in Parent Query
select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR' and l_linenumber = li.l_linenumber)
 order by p.p_partkey
;

-- where and having
-- Plan is:
-- Stage 1: b semijoin sq1:src_cbo (subquery in where)
-- Stage 2: group by Stage 1 o/p
-- Stage 5: group by on sq2:src_cbo (subquery in having)
-- Stage 6: Stage 2 o/p semijoin Stage 5
select key, value, count(*)
from src_cbo b
where b.key in (select key from src_cbo where src_cbo.key > '8')
group by key, value
having count(*) in (select count(*) from src_cbo s1 where s1.key > '9' group by s1.key ) order by key
;

-- non agg, non corr, windowing
select p_mfgr, p_name, avg(p_size)
from part
group by p_mfgr, p_name
having p_name in
  (select first_value(p_name) over(partition by p_mfgr order by p_size) from part) order by p_mfgr
;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 16. SubQueries Not In
-- non agg, non corr
select *
from src_cbo
where src_cbo.key not in
  ( select key  from src_cbo s1
    where s1.key > '2'
  ) order by key
;

-- non agg, corr
select p_mfgr, b.p_name, p_size
from part b
where b.p_name not in
  (select p_name
  from (select p_mfgr, p_name, p_size as r from part) a
  where r < 10 and b.p_mfgr = a.p_mfgr
  ) order by p_mfgr,p_size
;

-- agg, non corr
select p_name, p_size
from
part where part.p_size not in
  (select avg(p_size)
  from (select p_size from part) a
  where p_size < 10
  ) order by p_name
;

-- agg, corr
select p_mfgr, p_name, p_size
from part b where b.p_size not in
  (select min(p_size)
  from (select p_mfgr, p_size from part) a
  where p_size < 10 and b.p_mfgr = a.p_mfgr
  ) order by  p_name
;

-- non agg, non corr, Group By in Parent Query
select li.l_partkey, count(*)
from lineitem li
where li.l_linenumber = 1 and
  li.l_orderkey not in (select l_orderkey from lineitem where l_shipmode = 'AIR')
group by li.l_partkey order by li.l_partkey
;

-- add null check test from sq_notin.q once HIVE-7721 resolved.

-- non agg, corr, having
select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from (select p_mfgr, min(p_retailprice) l, max(p_retailprice) r, avg(p_retailprice) a from part group by p_mfgr) a
  where min(p_retailprice) = l and r - l > 600
  )
  order by b.p_mfgr
;

-- agg, non corr, having
select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from part a
  group by p_mfgr
  having max(p_retailprice) - min(p_retailprice) > 600
  )
  order by b.p_mfgr
;

set hive.mapred.mode=nonstrict;
set hive.cbo.returnpath.hiveop=true;
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- 0.23 changed input order of data in reducer task, which affects result of percentile_approx

CREATE TABLE bucket (key double, value string) CLUSTERED BY (key) SORTED BY (key DESC)  INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket;

create table t1 (result double);
create table t2 (result double);
create table t3 (result double);
create table t4 (result double);
create table t5 (result double);
create table t6 (result double);
create table t7 (result array<double>);
create table t8 (result array<double>);
create table t9 (result array<double>);
create table t10 (result array<double>);
create table t11 (result array<double>);
create table t12 (result array<double>);

set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.map.aggr=false;
-- disable map-side aggregation
FROM bucket
insert overwrite table t1 SELECT percentile_approx(cast(key AS double), 0.5)
insert overwrite table t2 SELECT percentile_approx(cast(key AS double), 0.5, 100)
insert overwrite table t3 SELECT percentile_approx(cast(key AS double), 0.5, 1000)

insert overwrite table t4 SELECT percentile_approx(cast(key AS int), 0.5)
insert overwrite table t5 SELECT percentile_approx(cast(key AS int), 0.5, 100)
insert overwrite table t6 SELECT percentile_approx(cast(key AS int), 0.5, 1000)

insert overwrite table t7 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98))
insert overwrite table t8 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t9 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 1000)

insert overwrite table t10 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98))
insert overwrite table t11 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t12 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 1000);

select * from t1;
select * from t2;
select * from t3;
select * from t4;
select * from t5;
select * from t6;
select * from t7;
select * from t8;
select * from t9;
select * from t10;
select * from t11;
select * from t12;

set hive.map.aggr=true;
-- enable map-side aggregation
FROM bucket
insert overwrite table t1 SELECT percentile_approx(cast(key AS double), 0.5)
insert overwrite table t2 SELECT percentile_approx(cast(key AS double), 0.5, 100)
insert overwrite table t3 SELECT percentile_approx(cast(key AS double), 0.5, 1000)

insert overwrite table t4 SELECT percentile_approx(cast(key AS int), 0.5)
insert overwrite table t5 SELECT percentile_approx(cast(key AS int), 0.5, 100)
insert overwrite table t6 SELECT percentile_approx(cast(key AS int), 0.5, 1000)

insert overwrite table t7 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98))
insert overwrite table t8 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t9 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 1000)

insert overwrite table t10 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98))
insert overwrite table t11 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t12 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 1000);

select * from t1;
select * from t2;
select * from t3;
select * from t4;
select * from t5;
select * from t6;
select * from t7;
select * from t8;
select * from t9;
select * from t10;
select * from t11;
select * from t12;

-- NaN
explain
select percentile_approx(case when key < 100 then cast('NaN' as double) else key end, 0.5) from bucket;
select percentile_approx(case when key < 100 then cast('NaN' as double) else key end, 0.5) between 340.5 and 343.0 from bucket;

-- with CBO
explain
select percentile_approx(key, 0.5) from bucket;
select percentile_approx(key, 0.5) between 255.0 and 257.0 from bucket;
set hive.cbo.returnpath.hiveop=true;

DESCRIBE FUNCTION percentile;
DESCRIBE FUNCTION EXTENDED percentile;


set hive.map.aggr = false;
set hive.groupby.skewindata = false;

-- SORT_QUERY_RESULTS

SELECT CAST(key AS INT) DIV 10,
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


set hive.map.aggr = true;
set hive.groupby.skewindata = false;

SELECT CAST(key AS INT) DIV 10,
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;



set hive.map.aggr = false;
set hive.groupby.skewindata = true;

SELECT CAST(key AS INT) DIV 10,
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


set hive.map.aggr = true;
set hive.groupby.skewindata = true;

SELECT CAST(key AS INT) DIV 10,
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


set hive.map.aggr = true;
set hive.groupby.skewindata = false;

-- test null handling
SELECT CAST(key AS INT) DIV 10,
       percentile(NULL, 0.0),
       percentile(NULL, array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


-- test empty array handling
SELECT CAST(key AS INT) DIV 10,
       percentile(IF(CAST(key AS INT) DIV 10 < 5, 1, NULL), 0.5),
       percentile(IF(CAST(key AS INT) DIV 10 < 5, 1, NULL), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;

select percentile(cast(key as bigint), 0.5) from src where false;

-- test where percentile list is empty
select percentile(cast(key as bigint), array()) from src where false;
set hive.cbo.returnpath.hiveop=true;

DESCRIBE FUNCTION percentile;
DESCRIBE FUNCTION EXTENDED percentile;


set hive.map.aggr = false;
set hive.groupby.skewindata = false;

-- SORT_QUERY_RESULTS

SELECT CAST(key AS INT) DIV 10,
       count(distinct(value)),
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;

SELECT CAST(key AS INT) DIV 10,
       count(distinct(value)),
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       count(distinct(substr(value, 5))),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


SELECT CAST(key AS INT) DIV 10,
       count(distinct(value)),
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       count(distinct(substr(value, 5))),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       count(distinct(substr(value, 2))),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       count(distinct(CAST(key AS INT) DIV 10)),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS

-- 8. Test UDF/UDAF
select count(*), count(c_int), sum(c_int), avg(c_int), max(c_int), min(c_int) from cbo_t1;
select count(*), count(c_int) as a, sum(c_int), avg(c_int), max(c_int), min(c_int), case c_int when 0  then 1 when 1 then 2 else 3 end, sum(case c_int when 0  then 1 when 1 then 2 else 3 end) from cbo_t1 group by c_int order by a;
select * from (select count(*) as a, count(distinct c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1;
select * from (select count(*) as a, count(distinct c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f, case c_int when 0  then 1 when 1 then 2 else 3 end as g, sum(case c_int when 0  then 1 when 1 then 2 else 3 end) as h from cbo_t1 group by c_int) cbo_t1 order by a;
select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1;
select f,a,e,b from (select count(*) as a, count(distinct c_int) as b, sum(distinct c_int) as c, avg(distinct c_int) as d, max(distinct c_int) as e, min(distinct c_int) as f from cbo_t1) cbo_t1;
select key,count(c_int) as a, avg(c_float) from cbo_t1 group by key order by a;
select count(distinct c_int) as a, avg(c_float) from cbo_t1 group by c_float order by a;
select count(distinct c_int) as a, avg(c_float) from cbo_t1 group by c_int order by a;
select count(distinct c_int) as a, avg(c_float) from cbo_t1 group by c_float, c_int order by a;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS

-- 11. Union All
select * from (select * from cbo_t1 order by key, c_boolean, value, dt)a union all select * from (select * from cbo_t2 order by key, c_boolean, value, dt)b;
select key from (select key, c_int from (select * from cbo_t1 union all select * from cbo_t2 where cbo_t2.key >=0)r1 union all select key, c_int from cbo_t3)r2 where key >=0 order by key;
select r2.key from (select key, c_int from (select key, c_int from cbo_t1 union all select key, c_int from cbo_t3 )r1 union all select key, c_int from cbo_t3)r2 join   (select key, c_int from (select * from cbo_t1 union all select * from cbo_t2 where cbo_t2.key >=0)r1 union all select key, c_int from cbo_t3)r3 on r2.key=r3.key where r3.key >=0 order by r2.key;

set hive.cbo.returnpath.hiveop=true;
-- SORT_QUERY_RESULTS

CREATE TABLE u1 as select key, value from src order by key limit 5;

CREATE TABLE u2 as select key, value from src order by key limit 3;

CREATE TABLE u3 as select key, value from src order by key desc limit 5;

select * from u1;

select * from u2;

select * from u3;

select key, value from
(
select key, value from u1
union all
select key, value from u2
union all
select key as key, value from u3
) tab;

select key, value from
(
select key, value from u1
union
select key, value from u2
union all
select key, value from u3
) tab;

select key, value from
(
select key, value from u1
union distinct
select key, value from u2
union all
select key as key, value from u3
) tab;

select key, value from
(
select key, value from u1
union all
select key, value from u2
union
select key, value from u3
) tab;

select key, value from
(
select key, value from u1
union
select key, value from u2
union
select key as key, value from u3
) tab;

select distinct * from
(
select key, value from u1
union all
select key, value from u2
union all
select key as key, value from u3
) tab;

select distinct * from
(
select distinct * from u1
union
select key, value from u2
union all
select key as key, value from u3
) tab;

drop view if exists v;

create view v as select distinct * from
(
select distinct * from u1
union
select key, value from u2
union all
select key as key, value from u3
) tab;

describe extended v;

select * from v;

drop view if exists v;

create view v as select tab.* from
(
select distinct * from u1
union
select distinct * from u2
) tab;

describe extended v;

select * from v;

drop view if exists v;

create view v as select * from
(
select distinct u1.* from u1
union all
select distinct * from u2
) tab;

describe extended v;

select * from v;

select distinct * from
(
select key, value from u1
union all
select key, value from u2
union
select key as key, value from u3
) tab;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 10. Test views
create view v1 as select c_int, value, c_boolean, dt from cbo_t1;
create view v2 as select c_int, value from cbo_t2;

select value from v1 where c_boolean=false;
select max(c_int) from v1 group by (c_boolean);

select count(v1.c_int)  from v1 join cbo_t2 on v1.c_int = cbo_t2.c_int;
select count(v1.c_int)  from v1 join v2 on v1.c_int = v2.c_int;

select count(*) from v1 a join v1 b on a.value = b.value;

create view v3 as select v1.value val from v1 join cbo_t1 on v1.c_boolean = cbo_t1.c_boolean;

select count(val) from v3 where val != '1';
with q1 as ( select key from cbo_t1 where key = '1')
select count(*) from q1;

with q1 as ( select value from v1 where c_boolean = false)
select count(value) from q1 ;

create view v4 as
with q1 as ( select key,c_int from cbo_t1  where key = '1')
select * from q1
;

with q1 as ( select c_int from q2 where c_boolean = false),
q2 as ( select c_int,c_boolean from v1  where value = '1')
select sum(c_int) from (select c_int from q1) a;

with q1 as ( select cbo_t1.c_int c_int from q2 join cbo_t1 where q2.c_int = cbo_t1.c_int  and cbo_t1.dt='2014'),
q2 as ( select c_int,c_boolean from v1  where value = '1' or dt = '14')
select count(*) from q1 join q2 join v4 on q1.c_int = q2.c_int and v4.c_int = q2.c_int;


drop view v1;
drop view v2;
drop view v3;
drop view v4;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 9. Test Windowing Functions
-- SORT_QUERY_RESULTS

select count(c_int) over() from cbo_t1;
select count(c_int) over(partition by c_float order by key), sum(c_float) over(partition by c_float order by key), max(c_int) over(partition by c_float order by key), min(c_int) over(partition by c_float order by key), row_number() over(partition by c_float order by key) as rn, rank() over(partition by c_float order by key), dense_rank() over(partition by c_float order by key), round(percent_rank() over(partition by c_float order by key), 2), lead(c_int, 2, c_int) over(partition by c_float order by key), lag(c_float, 2, c_float) over(partition by c_float order by key) from cbo_t1 order by rn;
select * from (select count(c_int) over(partition by c_float order by key), sum(c_float) over(partition by c_float order by key), max(c_int) over(partition by c_float order by key), min(c_int) over(partition by c_float order by key), row_number() over(partition by c_float order by key) as rn, rank() over(partition by c_float order by key), dense_rank() over(partition by c_float order by key), round(percent_rank() over(partition by c_float order by key),2), lead(c_int, 2, c_int) over(partition by c_float   order by key  ), lag(c_float, 2, c_float) over(partition by c_float   order by key) from cbo_t1 order by rn) cbo_t1;
select x from (select count(c_int) over() as x, sum(c_float) over() from cbo_t1) cbo_t1;
select 1+sum(c_int) over() from cbo_t1;
select sum(c_int)+sum(sum(c_int)) over() from cbo_t1;
select * from (select max(c_int) over (partition by key order by value Rows UNBOUNDED PRECEDING), min(c_int) over (partition by key order by value rows current row), count(c_int) over(partition by key order by value ROWS 1 PRECEDING), avg(value) over (partition by key order by value Rows between unbounded preceding and unbounded following), sum(value) over (partition by key order by value rows between unbounded preceding and current row), avg(c_float) over (partition by key order by value Rows between 1 preceding and unbounded following), sum(c_float) over (partition by key order by value rows between 1 preceding and current row), max(c_float) over (partition by key order by value rows between 1 preceding and unbounded following), min(c_float) over (partition by key order by value rows between 1 preceding and 1 following) from cbo_t1) cbo_t1;
select i, a, h, b, c, d, e, f, g, a as x, a +1 as y from (select max(c_int) over (partition by key order by value range UNBOUNDED PRECEDING) a, min(c_int) over (partition by key order by value range current row) b, count(c_int) over(partition by key order by value range 1 PRECEDING) c, avg(value) over (partition by key order by value range between unbounded preceding and unbounded following) d, sum(value) over (partition by key order by value range between unbounded preceding and current row) e, avg(c_float) over (partition by key order by value range between 1 preceding and unbounded following) f, sum(c_float) over (partition by key order by value range between 1 preceding and current row) g, max(c_float) over (partition by key order by value range between 1 preceding and unbounded following) h, min(c_float) over (partition by key order by value range between 1 preceding and 1 following) i from cbo_t1) cbo_t1;
select *, rank() over(partition by key order by value) as rr from src1;
select *, rank() over(partition by key order by value) from src1;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.cbo.returnpath.hiveop=true;
set hive.exec.check.crossproducts=false;
set mapred.reduce.tasks=4;
-- SORT_QUERY_RESULTS

-- 1. testWindowing
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1
from part
;

-- 2. testGroupByWithPartitioning
select p_mfgr, p_name, p_size,
min(p_retailprice),
rank() over(distribute by p_mfgr sort by p_name)as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
group by p_mfgr, p_name, p_size
;

-- 3. testGroupByHavingWithSWQ
select p_mfgr, p_name, p_size, min(p_retailprice),
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
group by p_mfgr, p_name, p_size
having p_size > 0
;

-- 4. testCount
select p_mfgr, p_name,
count(p_size) over(distribute by p_mfgr sort by p_name) as cd
from part
;

-- 5. testCountWithWindowingUDAF
select p_mfgr, p_name,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
count(p_size) over(distribute by p_mfgr sort by p_name) as cd,
p_retailprice, sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
;

-- 6. testCountInSubQ
select sub1.r, sub1.dr, sub1.cd, sub1.s1, sub1.deltaSz
from (select p_mfgr, p_name,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
count(p_size) over(distribute by p_mfgr sort by p_name) as cd,
p_retailprice, sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
) sub1;

-- 7. testJoinWithWindowingAndPTF
select abc.p_mfgr, abc.p_name,
rank() over(distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over(distribute by abc.p_mfgr sort by abc.p_name) as dr,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over(distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noop(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey
;

-- 8. testMixedCaseAlias
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name, p_size desc) as R
from part
;

-- 9. testHavingWithWindowingNoGBY
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s1
from part
;

-- 10. testHavingWithWindowingCondRankNoGBY
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1
from part
;

-- 11. testFirstLast
select  p_mfgr,p_name, p_size,
sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2,
first_value(p_size) over w1  as f,
last_value(p_size, false) over w1  as l
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);

-- 12. testFirstLastWithWhere
select  p_mfgr,p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2,
first_value(p_size) over w1 as f,
last_value(p_size, false) over w1 as l
from part
where p_mfgr = 'Manufacturer#3'
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);

-- 13. testSumWindow
select  p_mfgr,p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over (distribute by p_mfgr  sort by p_name rows between current row and current row)  as s2
from part
window w1 as (distribute by p_mfgr  sort by p_name rows between 2 preceding and 2 following);

-- 14. testNoSortClause
select  p_mfgr,p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r, dense_rank() over(distribute by p_mfgr sort by p_name) as dr
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);

-- 15. testExpressions
select  p_mfgr,p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
cume_dist() over(distribute by p_mfgr sort by p_name) as cud,
percent_rank() over(distribute by p_mfgr sort by p_name) as pr,
ntile(3) over(distribute by p_mfgr sort by p_name) as nt,
count(p_size) over(distribute by p_mfgr sort by p_name) as ca,
avg(p_size) over(distribute by p_mfgr sort by p_name) as avg,
stddev(p_size) over(distribute by p_mfgr sort by p_name) as st,
first_value(p_size % 5) over(distribute by p_mfgr sort by p_name) as fv,
last_value(p_size) over(distribute by p_mfgr sort by p_name) as lv,
first_value(p_size) over w1  as fvW1
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 16. testMultipleWindows
select  p_mfgr,p_name, p_size,
  rank() over(distribute by p_mfgr sort by p_name) as r,
  dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
cume_dist() over(distribute by p_mfgr sort by p_name) as cud,
sum(p_size) over (distribute by p_mfgr sort by p_name range between unbounded preceding and current row) as s1,
sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row) as s2,
first_value(p_size) over w1  as fv1
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 17. testCountStar
select  p_mfgr,p_name, p_size,
count(*) over(distribute by p_mfgr sort by p_name ) as c,
count(p_size) over(distribute by p_mfgr sort by p_name) as ca,
first_value(p_size) over w1  as fvW1
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 18. testUDAFs
select  p_mfgr,p_name, p_size,
sum(p_retailprice) over w1 as s,
min(p_retailprice) over w1 as mi,
max(p_retailprice) over w1 as ma,
avg(p_retailprice) over w1 as ag
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 19. testUDAFsWithGBY
select  p_mfgr,p_name, p_size, p_retailprice,
sum(p_retailprice) over w1 as s,
min(p_retailprice) as mi ,
max(p_retailprice) as ma ,
avg(p_retailprice) over w1 as ag
from part
group by p_mfgr,p_name, p_size, p_retailprice
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 20. testSTATs
select  p_mfgr,p_name, p_size,
stddev(p_retailprice) over w1 as sdev,
stddev_pop(p_retailprice) over w1 as sdev_pop,
collect_set(p_size) over w1 as uniq_size,
variance(p_retailprice) over w1 as var,
corr(p_size, p_retailprice) over w1 as cor,
covar_pop(p_size, p_retailprice) over w1 as covarp
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 21. testDISTs
select  p_mfgr,p_name, p_size,
histogram_numeric(p_retailprice, 5) over w1 as hist,
percentile(p_partkey, 0.5) over w1 as per,
row_number() over(distribute by p_mfgr sort by p_mfgr, p_name) as rn
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 22. testViewAsTableInputWithWindowing
create view IF NOT EXISTS mfgr_price_view as
select p_mfgr, p_brand,
round(sum(p_retailprice),2) as s
from part
group by p_mfgr, p_brand;

select *
from (
select p_mfgr, p_brand, s,
round(sum(s) over w1 , 2)  as s1
from mfgr_price_view
window w1 as (distribute by p_mfgr sort by p_mfgr )
) sq
order by p_mfgr, p_brand;

select p_mfgr, p_brand, s,
round(sum(s) over w1 ,2)  as s1
from mfgr_price_view
window w1 as (distribute by p_mfgr sort by p_brand rows between 2 preceding and current row);

-- 23. testCreateViewWithWindowingQuery
create view IF NOT EXISTS mfgr_brand_price_view as
select p_mfgr, p_brand,
sum(p_retailprice) over w1  as s
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and current row);

select * from mfgr_brand_price_view;

-- 24. testLateralViews
select p_mfgr, p_name,
lv_col, p_size, sum(p_size) over w1   as s
from (select p_mfgr, p_name, p_size, array(1,2,3) arr from part) p
lateral view explode(arr) part_lv as lv_col
window w1 as (distribute by p_mfgr sort by p_size, lv_col rows between 2 preceding and current row);

-- 25. testMultipleInserts3SWQs
CREATE TABLE part_1(
p_mfgr STRING,
p_name STRING,
p_size INT,
r INT,
dr INT,
s DOUBLE);

CREATE TABLE part_2(
p_mfgr STRING,
p_name STRING,
p_size INT,
r INT,
dr INT,
cud INT,
s2 DOUBLE,
fv1 INT);

CREATE TABLE part_3(
p_mfgr STRING,
p_name STRING,
p_size INT,
c INT,
ca INT,
fv INT);

from part
INSERT OVERWRITE TABLE part_1
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name ) as r,
dense_rank() over(distribute by p_mfgr sort by p_name ) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s
INSERT OVERWRITE TABLE part_2
select  p_mfgr,p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
cume_dist() over(distribute by p_mfgr sort by p_name) as cud,
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
first_value(p_size) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following)
INSERT OVERWRITE TABLE part_3
select  p_mfgr,p_name, p_size,
count(*) over(distribute by p_mfgr sort by p_name) as c,
count(p_size) over(distribute by p_mfgr sort by p_name) as ca,
first_value(p_size) over w1  as fv
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

select * from part_1;

select * from part_2;

select * from part_3;

-- 26. testGroupByHavingWithSWQAndAlias
select p_mfgr, p_name, p_size, min(p_retailprice) as mi,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
group by p_mfgr, p_name, p_size
having p_size > 0
;

-- 27. testMultipleRangeWindows
select  p_mfgr,p_name, p_size,
sum(p_size) over (distribute by p_mfgr sort by p_size range between 10 preceding and current row) as s2,
sum(p_size) over (distribute by p_mfgr sort by p_size range between current row and 10 following )  as s1
from part
window w1 as (rows between 2 preceding and 2 following);

-- 28. testPartOrderInUDAFInvoke
select p_mfgr, p_name, p_size,
sum(p_size) over (partition by p_mfgr  order by p_name  rows between 2 preceding and 2 following) as s
from part;

-- 29. testPartOrderInWdwDef
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s
from part
window w1 as (partition by p_mfgr  order by p_name  rows between 2 preceding and 2 following);

-- 30. testDefaultPartitioningSpecRules
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s,
sum(p_size) over w2 as s2
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following),
       w2 as (partition by p_mfgr order by p_name);

-- 31. testWindowCrossReference
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over w2 as s2
from part
window w1 as (partition by p_mfgr order by p_name range between 2 preceding and 2 following),
       w2 as w1;


-- 32. testWindowInheritance
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over w2 as s2
from part
window w1 as (partition by p_mfgr order by p_name range between 2 preceding and 2 following),
       w2 as (w1 rows between unbounded preceding and current row);


-- 33. testWindowForwardReference
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over w2 as s2,
sum(p_size) over w3 as s3
from part
window w1 as (distribute by p_mfgr sort by p_name range between 2 preceding and 2 following),
       w2 as w3,
       w3 as (distribute by p_mfgr sort by p_name range between unbounded preceding and current row);


-- 34. testWindowDefinitionPropagation
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over w2 as s2,
sum(p_size) over (w3 rows between 2 preceding and 2 following)  as s3
from part
window w1 as (distribute by p_mfgr sort by p_name range between 2 preceding and 2 following),
       w2 as w3,
       w3 as (distribute by p_mfgr sort by p_name range between unbounded preceding and current row);

-- 35. testDistinctWithWindowing
select DISTINCT p_mfgr, p_name, p_size,
sum(p_size) over w1 as s
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);

-- 36. testRankWithPartitioning
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name )  as r
from part;

-- 37. testPartitioningVariousForms
select p_mfgr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_mfgr),2) as s1,
min(p_retailprice) over (partition by p_mfgr) as s2,
max(p_retailprice) over (distribute by p_mfgr sort by p_mfgr) as s3,
round(avg(p_retailprice) over (distribute by p_mfgr),2) as s4,
count(p_retailprice) over (cluster by p_mfgr ) as s5
from part;

-- 38. testPartitioningVariousForms2
select p_mfgr, p_name, p_size,
sum(p_retailprice) over (partition by p_mfgr, p_name order by p_mfgr, p_name rows between unbounded preceding and current row) as s1,
min(p_retailprice) over (distribute by p_mfgr, p_name sort by p_mfgr, p_name rows between unbounded preceding and current row) as s2,
max(p_retailprice) over (partition by p_mfgr, p_name order by p_name) as s3
from part;

-- 39. testUDFOnOrderCols
select p_mfgr, p_type, substr(p_type, 2) as short_ptype,
rank() over (partition by p_mfgr order by substr(p_type, 2))  as r
from part;

-- 40. testNoBetweenForRows
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows unbounded preceding) as s1
     from part ;

-- 41. testNoBetweenForRange
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (distribute by p_mfgr sort by p_size range unbounded preceding) as s1
     from part ;

-- 42. testUnboundedFollowingForRows
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between current row and unbounded following) as s1
    from part ;

-- 43. testUnboundedFollowingForRange
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (distribute by p_mfgr sort by p_size range between current row and unbounded following) as s1
    from part ;

-- 44. testOverNoPartitionSingleAggregate
select p_name, p_retailprice,
round(avg(p_retailprice) over(),2)
from part
order by p_name;

-- 45. empty partition test
select p_mfgr,
  sum(p_size) over (partition by p_mfgr order by p_size rows between unbounded preceding and current row)
from part
where p_mfgr = 'Manufacturer#6'
;

-- 46. window sz is same as partition sz
select p_retailprice, avg(p_retailprice) over (partition by p_mfgr order by p_name rows between current row and 6 following),
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between current row and 6 following)
from part
where p_mfgr='Manufacturer#1';

-- 47. empty partition
select sum(p_size) over (partition by p_mfgr )
from part where p_mfgr = 'm1';
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 12. SemiJoin
select cbo_t1.c_int           from cbo_t1 left semi join   cbo_t2 on cbo_t1.key=cbo_t2.key;
select cbo_t1.c_int           from cbo_t1 left semi join   cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0);
select * from (select c, b, a from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left semi join cbo_t3 on cbo_t1.a=key where (b + 1 == 2) and (b > 0 or c >= 0)) R where  (b + 1 = 2) and (R.b > 0 or c >= 0);
select * from (select cbo_t3.c_int, cbo_t1.c, b from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 = 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t3.c_int  == 2) and (b > 0 or c_int >= 0)) R where  (R.c_int + 1 = 2) and (R.b > 0 or c_int >= 0);
select * from (select c_int, b, cbo_t1.c from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + 1 == 2) and (b > 0 or c_int >= 0)) R where  (c + 1 = 2) and (R.b > 0 or c_int >= 0);
select * from (select c_int, b, cbo_t1.c from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + 1 == 2) and (b > 0 or c_int >= 0)) R where  (c + 1 = 2) and (R.b > 0 or c_int >= 0);
select a, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by a+b desc, c asc) cbo_t1 left semi join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by q+r/10 desc, p) cbo_t2 on cbo_t1.a=p left semi join cbo_t3 on cbo_t1.a=key where (b + 1  >= 0) and (b > 0 or a >= 0) group by a, c  having a > 0 and (a >=1 or c >= 1) and (a + c) >= 0 order by c, a;
select a, c, count(*)  from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by a+b desc, c asc limit 5) cbo_t1 left semi join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by q+r/10 desc, p limit 5) cbo_t2 on cbo_t1.a=p left semi join cbo_t3 on cbo_t1.a=key where (b + 1  >= 0) and (b > 0 or a >= 0) group by a, c  having a > 0 and (a >=1 or c >= 1) and (a + c) >= 0 order by c, a;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 1. Test Select + TS
select * from cbo_t1;
select * from cbo_t1 as cbo_t1;
select * from cbo_t1 as cbo_t2;

select cbo_t1.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1;
select * from cbo_t1 where (((key=1) and (c_float=10)) and (c_int=20));

-- 2. Test Select + TS + FIL
select * from cbo_t1 where cbo_t1.c_int >= 0;
select * from cbo_t1 as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from cbo_t1 as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

select cbo_t2.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1 as cbo_t2  where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

-- 3 Test Select + Select + TS + FIL
select * from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1;
select * from (select * from cbo_t1 as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1;
select * from (select * from cbo_t1 as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1;
select * from (select cbo_t2.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1 as cbo_t2  where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1;

select * from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1 where cbo_t1.c_int >= 0;
select * from (select * from cbo_t1 as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from (select * from cbo_t1 as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from (select cbo_t2.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1 as cbo_t2  where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1 where cbo_t1.c_int >= 0 and y+c_int >= 0 or x <= 100;

select cbo_t1.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1 where cbo_t1.c_int >= 0;
select cbo_t2.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t2 where cbo_t2.c_int >= 0;



select * from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1 where cbo_t1.c_int >= 0;
select * from (select * from cbo_t1 as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1  where cbo_t1.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from (select * from cbo_t1 as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t2 where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;
select * from (select cbo_t2.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1 as cbo_t2  where cbo_t2.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as cbo_t1 where cbo_t1.c_int >= 0 and y+c_int >= 0 or x <= 100;

select cbo_t1.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t1 where cbo_t1.c_int >= 0;
select cbo_t2.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from cbo_t1 where cbo_t1.c_int >= 0) as cbo_t2 where cbo_t2.c_int >= 0;



-- 13. null expr in select list
select null from cbo_t3;

-- 14. unary operator
select key from cbo_t1 where c_int = -6  or c_int = +6;

-- 15. query referencing only partition columns
select count(cbo_t1.dt) from cbo_t1 join cbo_t2 on cbo_t1.dt  = cbo_t2.dt  where cbo_t1.dt = '2014' ;
set hive.mapred.mode=nonstrict;

create table s as select * from src limit 10;

explain
select key from s a
union all
select key from s b
order by key;

explain
select key from s a
union all
select key from s b
limit 0;

explain
select key from s a
union all
select key from s b
limit 5;

explain
select key from s a
union all
select key from s b
order by key
limit 5;

explain
select * from(
select src1.key, src2.value
from src src1 left outer join src src2
on src1.key = src2.key
limit 10)subq1
union all
select * from(
select src1.key, src2.value
from src src1 left outer join src src2
on src1.key = src2.key
limit 10)subq2
limit 5;

set hive.optimize.limittranspose=true;

explain
select key from s a
union all
select key from s b
order by key;

explain
select key from s a
union all
select key from s b
limit 0;

explain
select key from s a
union all
select key from s b
limit 5;

explain
select key from s a
union all
select key from s b
order by key
limit 5;

explain
select * from(
select src1.key, src2.value
from src src1 left outer join src src2
on src1.key = src2.key
limit 10)subq1
union all
select * from(
select src1.key, src2.value
from src src1 left outer join src src2
on src1.key = src2.key
limit 10)subq2
limit 5;

set hive.optimize.limittranspose.reductionpercentage=0.1f;

explain
select key from s a
union all
select key from s b
limit 5;

set hive.optimize.limittranspose.reductionpercentage=1f;
set hive.optimize.limittranspose.reductiontuples=8;

explain
select key from s a
union all
select key from s b
limit 5;set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 20. Test get stats with empty partition list
select cbo_t1.value from cbo_t1 join cbo_t2 on cbo_t1.key = cbo_t2.key where cbo_t1.dt = '10' and cbo_t1.c_boolean = true;

set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 18. SubQueries Not Exists
-- distinct, corr
select *
from src_cbo b
where not exists
  (select distinct a.key
  from src_cbo a
  where b.value = a.value and a.value > 'val_2'
  )
;

-- no agg, corr, having
select *
from src_cbo b
group by key, value
having not exists
  (select a.key
  from src_cbo a
  where b.value = a.value  and a.key = b.key and a.value > 'val_12'
  )
;

-- 19. SubQueries Exists
-- view test
create view cv1 as
select *
from src_cbo b
where exists
  (select a.key
  from src_cbo a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9')
;

select * from cv1
;

-- sq in from
select *
from (select *
      from src_cbo b
      where exists
          (select a.key
          from src_cbo a
          where b.value = a.value  and a.key = b.key and a.value > 'val_9')
     ) a
;

-- sq in from, having
select *
from (select b.key, count(*)
  from src_cbo b
  group by b.key
  having exists
    (select a.key
    from src_cbo a
    where a.key = b.key and a.value > 'val_9'
    )
) a
;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 17. SubQueries In
-- non agg, non corr
select *
from src_cbo
where src_cbo.key in (select key from src_cbo s1 where s1.key > '9') order by key
;

-- agg, corr
-- add back once rank issue fixed for cbo

-- distinct, corr
select *
from src_cbo b
where b.key in
        (select distinct a.key
         from src_cbo a
         where b.value = a.value and a.key > '9'
        ) order by b.key
;

-- non agg, corr, with join in Parent Query
select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR' and l_linenumber = li.l_linenumber)
 order by p.p_partkey
;

-- where and having
-- Plan is:
-- Stage 1: b semijoin sq1:src_cbo (subquery in where)
-- Stage 2: group by Stage 1 o/p
-- Stage 5: group by on sq2:src_cbo (subquery in having)
-- Stage 6: Stage 2 o/p semijoin Stage 5
select key, value, count(*)
from src_cbo b
where b.key in (select key from src_cbo where src_cbo.key > '8')
group by key, value
having count(*) in (select count(*) from src_cbo s1 where s1.key > '9' group by s1.key ) order by key
;

-- non agg, non corr, windowing
select p_mfgr, p_name, avg(p_size)
from part
group by p_mfgr, p_name
having p_name in
  (select first_value(p_name) over(partition by p_mfgr order by p_size) from part) order by p_mfgr
;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 16. SubQueries Not In
-- non agg, non corr
select *
from src_cbo
where src_cbo.key not in
  ( select key  from src_cbo s1
    where s1.key > '2'
  ) order by key
;

-- non agg, corr
select p_mfgr, b.p_name, p_size
from part b
where b.p_name not in
  (select p_name
  from (select p_mfgr, p_name, p_size as r from part) a
  where r < 10 and b.p_mfgr = a.p_mfgr
  ) order by p_mfgr,p_size
;

-- agg, non corr
select p_name, p_size
from
part where part.p_size not in
  (select avg(p_size)
  from (select p_size from part) a
  where p_size < 10
  ) order by p_name
;

-- agg, corr
select p_mfgr, p_name, p_size
from part b where b.p_size not in
  (select min(p_size)
  from (select p_mfgr, p_size from part) a
  where p_size < 10 and b.p_mfgr = a.p_mfgr
  ) order by  p_name
;

-- non agg, non corr, Group By in Parent Query
select li.l_partkey, count(*)
from lineitem li
where li.l_linenumber = 1 and
  li.l_orderkey not in (select l_orderkey from lineitem where l_shipmode = 'AIR')
group by li.l_partkey order by li.l_partkey
;

-- add null check test from sq_notin.q once HIVE-7721 resolved.

-- non agg, corr, having
select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from (select p_mfgr, min(p_retailprice) l, max(p_retailprice) r, avg(p_retailprice) a from part group by p_mfgr) a
  where min(p_retailprice) = l and r - l > 600
  )
  order by b.p_mfgr
;

-- agg, non corr, having
select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from part a
  group by p_mfgr
  having max(p_retailprice) - min(p_retailprice) > 600
  )
  order by b.p_mfgr
;

set hive.mapred.mode=nonstrict;
set hive.cbo.returnpath.hiveop=true;

DESCRIBE FUNCTION max;
DESCRIBE FUNCTION EXTENDED max;


set hive.map.aggr = false;
set hive.groupby.skewindata = false;

SELECT max(struct(CAST(key as INT), value)),
       max(struct(key, value))
FROM src;


set hive.map.aggr = true;
set hive.groupby.skewindata = false;

SELECT max(struct(CAST(key as INT), value)),
       max(struct(key, value))
FROM src;


set hive.map.aggr = false;
set hive.groupby.skewindata = true;

SELECT max(struct(CAST(key as INT), value)),
       max(struct(key, value))
FROM src;


set hive.map.aggr = true;
set hive.groupby.skewindata = true;

SELECT max(struct(CAST(key as INT), value)),
       max(struct(key, value))
FROM src;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS

-- 8. Test UDF/UDAF
select count(*), count(c_int), sum(c_int), avg(c_int), max(c_int), min(c_int) from cbo_t1;
select count(*), count(c_int) as a, sum(c_int), avg(c_int), max(c_int), min(c_int), case c_int when 0  then 1 when 1 then 2 else 3 end, sum(case c_int when 0  then 1 when 1 then 2 else 3 end) from cbo_t1 group by c_int order by a;
select * from (select count(*) as a, count(distinct c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1;
select * from (select count(*) as a, count(distinct c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f, case c_int when 0  then 1 when 1 then 2 else 3 end as g, sum(case c_int when 0  then 1 when 1 then 2 else 3 end) as h from cbo_t1 group by c_int) cbo_t1 order by a;
select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from cbo_t1) cbo_t1;
select f,a,e,b from (select count(*) as a, count(distinct c_int) as b, sum(distinct c_int) as c, avg(distinct c_int) as d, max(distinct c_int) as e, min(distinct c_int) as f from cbo_t1) cbo_t1;
select key,count(c_int) as a, avg(c_float) from cbo_t1 group by key order by a;
select count(distinct c_int) as a, avg(c_float) from cbo_t1 group by c_float order by a;
select count(distinct c_int) as a, avg(c_float) from cbo_t1 group by c_int order by a;
select count(distinct c_int) as a, avg(c_float) from cbo_t1 group by c_float, c_int order by a;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS

-- 11. Union All
select * from (select * from cbo_t1 order by key, c_boolean, value, dt)a union all select * from (select * from cbo_t2 order by key, c_boolean, value, dt)b;
select key from (select key, c_int from (select * from cbo_t1 union all select * from cbo_t2 where cbo_t2.key >=0)r1 union all select key, c_int from cbo_t3)r2 where key >=0 order by key;
select r2.key from (select key, c_int from (select key, c_int from cbo_t1 union all select key, c_int from cbo_t3 )r1 union all select key, c_int from cbo_t3)r2 join   (select key, c_int from (select * from cbo_t1 union all select * from cbo_t2 where cbo_t2.key >=0)r1 union all select key, c_int from cbo_t3)r3 on r2.key=r3.key where r3.key >=0 order by r2.key;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 10. Test views
create view v1 as select c_int, value, c_boolean, dt from cbo_t1;
create view v2 as select c_int, value from cbo_t2;

select value from v1 where c_boolean=false;
select max(c_int) from v1 group by (c_boolean);

select count(v1.c_int)  from v1 join cbo_t2 on v1.c_int = cbo_t2.c_int;
select count(v1.c_int)  from v1 join v2 on v1.c_int = v2.c_int;

select count(*) from v1 a join v1 b on a.value = b.value;

create view v3 as select v1.value val from v1 join cbo_t1 on v1.c_boolean = cbo_t1.c_boolean;

select count(val) from v3 where val != '1';
with q1 as ( select key from cbo_t1 where key = '1')
select count(*) from q1;

with q1 as ( select value from v1 where c_boolean = false)
select count(value) from q1 ;

create view v4 as
with q1 as ( select key,c_int from cbo_t1  where key = '1')
select * from q1
;

with q1 as ( select c_int from q2 where c_boolean = false),
q2 as ( select c_int,c_boolean from v1  where value = '1')
select sum(c_int) from (select c_int from q1) a;

with q1 as ( select cbo_t1.c_int c_int from q2 join cbo_t1 where q2.c_int = cbo_t1.c_int  and cbo_t1.dt='2014'),
q2 as ( select c_int,c_boolean from v1  where value = '1' or dt = '14')
select count(*) from q1 join q2 join v4 on q1.c_int = q2.c_int and v4.c_int = q2.c_int;


drop view v1;
drop view v2;
drop view v3;
drop view v4;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- 9. Test Windowing Functions
-- SORT_QUERY_RESULTS

select count(c_int) over() from cbo_t1;
select count(c_int) over(partition by c_float order by key), sum(c_float) over(partition by c_float order by key), max(c_int) over(partition by c_float order by key), min(c_int) over(partition by c_float order by key), row_number() over(partition by c_float order by key) as rn, rank() over(partition by c_float order by key), dense_rank() over(partition by c_float order by key), round(percent_rank() over(partition by c_float order by key), 2), lead(c_int, 2, c_int) over(partition by c_float order by key), lag(c_float, 2, c_float) over(partition by c_float order by key) from cbo_t1 order by rn;
select * from (select count(c_int) over(partition by c_float order by key), sum(c_float) over(partition by c_float order by key), max(c_int) over(partition by c_float order by key), min(c_int) over(partition by c_float order by key), row_number() over(partition by c_float order by key) as rn, rank() over(partition by c_float order by key), dense_rank() over(partition by c_float order by key), round(percent_rank() over(partition by c_float order by key),2), lead(c_int, 2, c_int) over(partition by c_float   order by key  ), lag(c_float, 2, c_float) over(partition by c_float   order by key) from cbo_t1 order by rn) cbo_t1;
select x from (select count(c_int) over() as x, sum(c_float) over() from cbo_t1) cbo_t1;
select 1+sum(c_int) over() from cbo_t1;
select sum(c_int)+sum(sum(c_int)) over() from cbo_t1;
select * from (select max(c_int) over (partition by key order by value Rows UNBOUNDED PRECEDING), min(c_int) over (partition by key order by value rows current row), count(c_int) over(partition by key order by value ROWS 1 PRECEDING), avg(value) over (partition by key order by value Rows between unbounded preceding and unbounded following), sum(value) over (partition by key order by value rows between unbounded preceding and current row), avg(c_float) over (partition by key order by value Rows between 1 preceding and unbounded following), sum(c_float) over (partition by key order by value rows between 1 preceding and current row), max(c_float) over (partition by key order by value rows between 1 preceding and unbounded following), min(c_float) over (partition by key order by value rows between 1 preceding and 1 following) from cbo_t1) cbo_t1;
select i, a, h, b, c, d, e, f, g, a as x, a +1 as y from (select max(c_int) over (partition by key order by value range UNBOUNDED PRECEDING) a, min(c_int) over (partition by key order by value range current row) b, count(c_int) over(partition by key order by value range 1 PRECEDING) c, avg(value) over (partition by key order by value range between unbounded preceding and unbounded following) d, sum(value) over (partition by key order by value range between unbounded preceding and current row) e, avg(c_float) over (partition by key order by value range between 1 preceding and unbounded following) f, sum(c_float) over (partition by key order by value range between 1 preceding and current row) g, max(c_float) over (partition by key order by value range between 1 preceding and unbounded following) h, min(c_float) over (partition by key order by value range between 1 preceding and 1 following) i from cbo_t1) cbo_t1;
select *, rank() over(partition by key order by value) as rr from src1;
select *, rank() over(partition by key order by value) from src1;

-- SORT_QUERY_RESULTS

drop table char1;
drop table char1_1;

create table char1 (key char(10), value char(20));
create table char1_1 (key string, value string);

-- load from file
load data local inpath '../../data/files/srcbucket0.txt' overwrite into table char1;
select * from char1 order by key, value limit 2;

-- insert overwrite, from same/different length char
insert overwrite table char1
  select cast(key as char(10)), cast(value as char(15)) from src order by key, value limit 2;
select key, value from char1;

-- insert overwrite, from string
insert overwrite table char1
  select key, value from src order by key, value limit 2;
select key, value from char1;

-- insert string from char
insert overwrite table char1_1
  select key, value from char1 order by key, value limit 2;
select key, value from char1_1;

-- respect string length
insert overwrite table char1
  select key, cast(value as char(3)) from src order by key, value limit 2;
select key, value from char1;

drop table char1;
drop table char1_1;
drop table char_2;

create table char_2 (
  key char(10),
  value char(20)
);

insert overwrite table char_2 select * from src;

select value, sum(cast(key as int)), count(*) numrows
from src
group by value
order by value asc
limit 5;

-- should match the query from src
select value, sum(cast(key as int)), count(*) numrows
from char_2
group by value
order by value asc
limit 5;

select value, sum(cast(key as int)), count(*) numrows
from src
group by value
order by value desc
limit 5;

-- should match the query from src
select value, sum(cast(key as int)), count(*) numrows
from char_2
group by value
order by value desc
limit 5;

drop table char_2;

-- Cast from char to other data types
select
  cast(cast('11' as string) as tinyint),
  cast(cast('11' as string) as smallint),
  cast(cast('11' as string) as int),
  cast(cast('11' as string) as bigint),
  cast(cast('11.00' as string) as float),
  cast(cast('11.00' as string) as double),
  cast(cast('11.00' as string) as decimal)
from src limit 1;

select
  cast(cast('11' as char(10)) as tinyint),
  cast(cast('11' as char(10)) as smallint),
  cast(cast('11' as char(10)) as int),
  cast(cast('11' as char(10)) as bigint),
  cast(cast('11.00' as char(10)) as float),
  cast(cast('11.00' as char(10)) as double),
  cast(cast('11.00' as char(10)) as decimal)
from src limit 1;

select
  cast(cast('2011-01-01' as string) as date),
  cast(cast('2011-01-01 01:02:03' as string) as timestamp)
from src limit 1;

select
  cast(cast('2011-01-01' as char(10)) as date),
  cast(cast('2011-01-01 01:02:03' as char(30)) as timestamp)
from src limit 1;

-- no tests from string/char to boolean, that conversion doesn't look useful
select
  cast(cast('abc123' as string) as string),
  cast(cast('abc123' as string) as varchar(10)),
  cast(cast('abc123' as string) as char(10))
from src limit 1;

select
  cast(cast('abc123' as char(10)) as string),
  cast(cast('abc123' as char(10)) as varchar(10)),
  cast(cast('abc123' as char(10)) as char(10))
from src limit 1;

select
  cast(cast('abc123' as varchar(10)) as string),
  cast(cast('abc123' as varchar(10)) as varchar(10)),
  cast(cast('abc123' as varchar(10)) as char(10))
from src limit 1;

-- cast from other types to char
select
  cast(cast(11 as tinyint) as string),
  cast(cast(11 as smallint) as string),
  cast(cast(11 as int) as string),
  cast(cast(11 as bigint) as string),
  cast(cast(11.00 as float) as string),
  cast(cast(11.00 as double) as string),
  cast(cast(11.00 as decimal) as string)
from src limit 1;

select
  cast(cast(11 as tinyint) as char(10)),
  cast(cast(11 as smallint) as char(10)),
  cast(cast(11 as int) as char(10)),
  cast(cast(11 as bigint) as char(10)),
  cast(cast(11.00 as float) as char(10)),
  cast(cast(11.00 as double) as char(10)),
  cast(cast(11.00 as decimal) as char(10))
from src limit 1;

select
  cast(date '2011-01-01' as string),
  cast(timestamp('2011-01-01 01:02:03') as string)
from src limit 1;

select
  cast(date '2011-01-01' as char(10)),
  cast(timestamp('2011-01-01 01:02:03') as char(30))
from src limit 1;

select
  cast(true as string),
  cast(false as string)
from src limit 1;

select
  cast(true as char(10)),
  cast(false as char(10))
from src limit 1;


-- Should all be true
select
  cast('abc' as char(10)) =  cast('abc' as char(10)),
  cast('abc' as char(10)) <= cast('abc' as char(10)),
  cast('abc' as char(10)) >= cast('abc' as char(10)),
  cast('abc' as char(10)) <  cast('abd' as char(10)),
  cast('abc' as char(10)) >  cast('abb' as char(10)),
  cast('abc' as char(10)) <> cast('abb' as char(10))
from src limit 1;

-- Different char lengths should still compare the same
select
  cast('abc' as char(10)) =  cast('abc' as char(3)),
  cast('abc' as char(10)) <= cast('abc' as char(3)),
  cast('abc' as char(10)) >= cast('abc' as char(3)),
  cast('abc' as char(10)) <  cast('abd' as char(3)),
  cast('abc' as char(10)) >  cast('abb' as char(3)),
  cast('abc' as char(10)) <> cast('abb' as char(3))
from src limit 1;

-- Should work with string types as well
select
  cast('abc' as char(10)) =  'abc',
  cast('abc' as char(10)) <= 'abc',
  cast('abc' as char(10)) >= 'abc',
  cast('abc' as char(10)) <  'abd',
  cast('abc' as char(10)) >  'abb',
  cast('abc' as char(10)) <> 'abb'
from src limit 1;

-- leading space is significant for char
select
  cast(' abc' as char(10)) <> cast('abc' as char(10))
from src limit 1;

-- trailing space is not significant for char
select
  cast('abc ' as char(10)) = cast('abc' as char(10))
from src limit 1;
-- SORT_QUERY_RESULTS

drop table char_join1_ch1;
drop table char_join1_ch2;
drop table char_join1_str;

create table  char_join1_ch1 (
  c1 int,
  c2 char(10)
);

create table  char_join1_ch2 (
  c1 int,
  c2 char(20)
);

create table  char_join1_str (
  c1 int,
  c2 string
);

load data local inpath '../../data/files/vc1.txt' into table char_join1_ch1;
load data local inpath '../../data/files/vc1.txt' into table char_join1_ch2;
load data local inpath '../../data/files/vc1.txt' into table char_join1_str;

-- Join char with same length char
select * from char_join1_ch1 a join char_join1_ch1 b on (a.c2 = b.c2);

-- Join char with different length char
select * from char_join1_ch1 a join char_join1_ch2 b on (a.c2 = b.c2);

-- Join char with string
select * from char_join1_ch1 a join char_join1_str b on (a.c2 = b.c2);

drop table char_join1_ch1;
drop table char_join1_ch2;
drop table char_join1_str;
drop table char_nested_1;
drop table char_nested_array;
drop table char_nested_map;
drop table char_nested_struct;
drop table char_nested_cta;
drop table char_nested_view;

create table char_nested_1 (key int, value char(20));
insert overwrite table char_nested_1
  select key, value from src order by key limit 1;

-- arrays
create table char_nested_array (c1 array<char(20)>);
insert overwrite table char_nested_array
  select array(value, value) from char_nested_1;
describe char_nested_array;
select * from char_nested_array;

-- maps
create table char_nested_map (c1 map<int, char(20)>);
insert overwrite table char_nested_map
  select map(key, value) from char_nested_1;
describe char_nested_map;
select * from char_nested_map;

-- structs
create table char_nested_struct (c1 struct<a:int, b:char(20), c:string>);
insert overwrite table char_nested_struct
  select named_struct('a', key,
                      'b', value,
                      'c', cast(value as string))
  from char_nested_1;
describe char_nested_struct;
select * from char_nested_struct;

-- nested type with create table as
create table char_nested_cta as
  select * from char_nested_struct;
describe char_nested_cta;
select * from char_nested_cta;

-- nested type with view
create table char_nested_view as
  select * from char_nested_struct;
describe char_nested_view;
select * from char_nested_view;

drop table char_nested_1;
drop table char_nested_array;
drop table char_nested_map;
drop table char_nested_struct;
drop table char_nested_cta;
drop table char_nested_view;

create table over1k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over1k' into table over1k;

-- Pass non-strings for the first and third arguments to test argument conversion

-- Integers
select lpad(t, 4, ' '),
       lpad(si, 2, ' '),
       lpad(i, 9, 'z'),
       lpad(b, 2, 'a') from over1k limit 5;

select lpad("oh", 10, t),
       lpad("my", 6, si),
       lpad("other", 14, i),
       lpad("one", 12, b) from over1k limit 5;

-- Integers
select rpad(t, 4, ' '),
       rpad(si, 2, ' '),
       rpad(i, 9, 'z'),
       rpad(b, 2, 'a') from over1k limit 5;

select rpad("oh", 10, t),
       rpad("my", 6, si),
       rpad("other", 14, i),
       rpad("one", 12, b) from over1k limit 5;

-- More
select lpad(f, 4, ' '),
       lpad(d, 2, ' '),
       lpad(bo, 9, 'z'),
       lpad(ts, 2, 'a'),
       lpad(dec, 7, 'd'),
       lpad(bin, 8, 'b') from over1k limit 5;

select lpad("oh", 10, f),
       lpad("my", 6, d),
       lpad("other", 14, bo),
       lpad("one", 12, ts),
       lpad("two", 7, dec),
       lpad("three", 8, bin) from over1k limit 5;

select rpad(f, 4, ' '),
       rpad(d, 2, ' '),
       rpad(bo, 9, 'z'),
       rpad(ts, 2, 'a'),
       rpad(dec, 7, 'd'),
       rpad(bin, 8, 'b') from over1k limit 5;

select rpad("oh", 10, f),
       rpad("my", 6, d),
       rpad("other", 14, bo),
       rpad("one", 12, ts),
       rpad("two", 7, dec),
       rpad("three", 8, bin) from over1k limit 5;
create table over1k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over1k' into table over1k;

-- Pass non-strings for the first and third arguments to test argument conversion
-- Negative tests: LIST, MAP, STRUCT, UNION

-- Integers
select lpad(t, array(1,2,3), ' ') from over1k limit 5;
create table over1k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over1k' into table over1k;

-- Pass non-strings for the first and third arguments to test argument conversion
-- Negative tests: LIST, MAP, STRUCT, UNION

-- Integers
select lpad(array(1,2,3), 4, ' ') from over1k limit 5;
create table over1k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over1k' into table over1k;

-- Pass non-strings for the first and third arguments to test argument conversion
-- For negative testing, try LIST, MAP, STRUCT, UNION

-- Integers
select lpad({"key1":[1,2,3],"key2":[6,7,8]}, 4, ' ') from over1k limit 5;
create table over1k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over1k' into table over1k;

-- Pass non-strings for the first and third arguments to test argument conversion
-- For negative testing, try LIST, MAP, STRUCT, UNION

-- Integers
select lpad(create_union(0, 'Union'), 4, ' ') from over1k limit 5;drop table if exists char_serde_regex;
drop table if exists char_serde_lb;
drop table if exists char_serde_ls;
drop table if exists char_serde_c;
drop table if exists char_serde_lbc;
drop table if exists char_serde_orc;

--
-- RegexSerDe
--
create table  char_serde_regex (
  key char(15),
  value char(20)
)
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe'
with serdeproperties (
  "input.regex" = "([^]*)([^]*)"
)
stored as textfile;

load data local inpath '../../data/files/srcbucket0.txt' overwrite into table char_serde_regex;

select * from char_serde_regex limit 5;
select value, count(*) from char_serde_regex group by value limit 5;

--
-- LazyBinary
--
create table  char_serde_lb (
  key char(15),
  value char(20)
);
alter table char_serde_lb set serde 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe';

insert overwrite table char_serde_lb
  select key, value from char_serde_regex;
select * from char_serde_lb limit 5;
select value, count(*) from char_serde_lb group by value limit 5;

--
-- LazySimple
--
create table  char_serde_ls (
  key char(15),
  value char(20)
);
alter table char_serde_ls set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';

insert overwrite table char_serde_ls
  select key, value from char_serde_lb;
select * from char_serde_ls limit 5;
select value, count(*) from char_serde_ls group by value limit 5;

--
-- Columnar
--
create table  char_serde_c (
  key char(15),
  value char(20)
) stored as rcfile;
alter table char_serde_c set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';

insert overwrite table char_serde_c
  select key, value from char_serde_ls;
select * from char_serde_c limit 5;
select value, count(*) from char_serde_c group by value limit 5;

--
-- LazyBinaryColumnar
--
create table char_serde_lbc (
  key char(15),
  value char(20)
) stored as rcfile;
alter table char_serde_lbc set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';

insert overwrite table char_serde_lbc
  select key, value from char_serde_c;
select * from char_serde_lbc limit 5;
select value, count(*) from char_serde_lbc group by value limit 5;

--
-- ORC
--
create table char_serde_orc (
  key char(15),
  value char(20)
) stored as orc;
alter table char_serde_orc set serde 'org.apache.hadoop.hive.ql.io.orc.OrcSerde';


insert overwrite table char_serde_orc
  select key, value from char_serde_lbc;
select * from char_serde_orc limit 5;
select value, count(*) from char_serde_orc group by value limit 5;

drop table if exists char_serde_regex;
drop table if exists char_serde_lb;
drop table if exists char_serde_ls;
drop table if exists char_serde_c;
drop table if exists char_serde_lbc;
drop table if exists char_serde_orc;
drop table char_udf_1;

create table char_udf_1 (c1 string, c2 string, c3 char(10), c4 char(20));
insert overwrite table char_udf_1
  select key, value, key, value from src where key = '238' limit 1;

-- JAVA_VERSION_SPECIFIC_OUTPUT

-- UDFs with char support
select
  concat(c1, c2),
  concat(c3, c4),
  concat(c1, c2) = concat(c3, c4)
from char_udf_1 limit 1;

select
  upper(c2),
  upper(c4),
  upper(c2) = upper(c4)
from char_udf_1 limit 1;

select
  lower(c2),
  lower(c4),
  lower(c2) = lower(c4)
from char_udf_1 limit 1;

-- Scalar UDFs
select
  ascii(c2),
  ascii(c4),
  ascii(c2) = ascii(c4)
from char_udf_1 limit 1;

select
  concat_ws('|', c1, c2),
  concat_ws('|', c3, c4),
  concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
from char_udf_1 limit 1;

select
  decode(encode(c2, 'US-ASCII'), 'US-ASCII'),
  decode(encode(c4, 'US-ASCII'), 'US-ASCII'),
  decode(encode(c2, 'US-ASCII'), 'US-ASCII') = decode(encode(c4, 'US-ASCII'), 'US-ASCII')
from char_udf_1 limit 1;

select
  instr(c2, '_'),
  instr(c4, '_'),
  instr(c2, '_') = instr(c4, '_')
from char_udf_1 limit 1;

select
  length(c2),
  length(c4),
  length(c2) = length(c4)
from char_udf_1 limit 1;

select
  locate('a', 'abcdabcd', 3),
  locate(cast('a' as char(1)), cast('abcdabcd' as char(10)), 3),
  locate('a', 'abcdabcd', 3) = locate(cast('a' as char(1)), cast('abcdabcd' as char(10)), 3)
from char_udf_1 limit 1;

select
  lpad(c2, 15, ' '),
  lpad(c4, 15, ' '),
  lpad(c2, 15, ' ') = lpad(c4, 15, ' ')
from char_udf_1 limit 1;

select
  ltrim(c2),
  ltrim(c4),
  ltrim(c2) = ltrim(c4)
from char_udf_1 limit 1;

-- In hive wiki page https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF
-- we only allow A regexp B, not regexp (A,B).

select
  c2 regexp 'val',
  c4 regexp 'val',
  (c2 regexp 'val') = (c4 regexp 'val')
from char_udf_1 limit 1;

select
  regexp_extract(c2, 'val_([0-9]+)', 1),
  regexp_extract(c4, 'val_([0-9]+)', 1),
  regexp_extract(c2, 'val_([0-9]+)', 1) = regexp_extract(c4, 'val_([0-9]+)', 1)
from char_udf_1 limit 1;

select
  regexp_replace(c2, 'val', 'replaced'),
  regexp_replace(c4, 'val', 'replaced'),
  regexp_replace(c2, 'val', 'replaced') = regexp_replace(c4, 'val', 'replaced')
from char_udf_1 limit 1;

select
  reverse(c2),
  reverse(c4),
  reverse(c2) = reverse(c4)
from char_udf_1 limit 1;

select
  rpad(c2, 15, ' '),
  rpad(c4, 15, ' '),
  rpad(c2, 15, ' ') = rpad(c4, 15, ' ')
from char_udf_1 limit 1;

select
  rtrim(c2),
  rtrim(c4),
  rtrim(c2) = rtrim(c4)
from char_udf_1 limit 1;

select
  sentences('See spot run.  See jane run.'),
  sentences(cast('See spot run.  See jane run.' as char(50)))
from char_udf_1 limit 1;

select
  split(c2, '_'),
  split(c4, '_')
from char_udf_1 limit 1;

select
  str_to_map('a:1,b:2,c:3',',',':'),
  str_to_map(cast('a:1,b:2,c:3' as char(20)),',',':')
from char_udf_1 limit 1;

select
  substr(c2, 1, 3),
  substr(c4, 1, 3),
  substr(c2, 1, 3) = substr(c4, 1, 3)
from char_udf_1 limit 1;

select
  trim(c2),
  trim(c4),
  trim(c2) = trim(c4)
from char_udf_1 limit 1;


-- Aggregate Functions
select
  compute_stats(c2, 16),
  compute_stats(c4, 16)
from char_udf_1;

select
  min(c2),
  min(c4)
from char_udf_1;

select
  max(c2),
  max(c4)
from char_udf_1;


drop table char_udf_1;
drop table char_union1_ch1;
drop table char_union1_ch2;
drop table char_union1_str;

create table  char_union1_ch1 (
  c1 int,
  c2 char(10)
);

create table  char_union1_ch2 (
  c1 int,
  c2 char(20)
);

create table  char_union1_str (
  c1 int,
  c2 string
);

load data local inpath '../../data/files/vc1.txt' into table char_union1_ch1;
load data local inpath '../../data/files/vc1.txt' into table char_union1_ch2;
load data local inpath '../../data/files/vc1.txt' into table char_union1_str;

-- union char with same length char
select * from (
  select * from char_union1_ch1
  union all
  select * from char_union1_ch1 limit 1
) q1 sort by c1;

-- union char with different length char
select * from (
  select * from char_union1_ch1
  union all
  select * from char_union1_ch2 limit 1
) q1 sort by c1;

-- union char with string
select * from (
  select * from char_union1_ch1
  union all
  select * from char_union1_str limit 1
) q1 sort by c1;

drop table char_union1_ch1;
drop table char_union1_ch2;
drop table char_union1_str;
DROP TABLE IF EXISTS  char_varchar_udf;

CREATE TABLE char_varchar_udf (c char(8), vc varchar(10)) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';
LOAD DATA LOCAL INPATH '../../data/files/char_varchar_udf.txt' INTO TABLE char_varchar_udf;

SELECT ROUND(c, 2), ROUND(vc, 3) FROM char_varchar_udf;
SELECT AVG(c), AVG(vc), SUM(c), SUM(vc) FROM char_varchar_udf;

DROP TABLE char_varchar_udf;EXPLAIN
SELECT * FROM SRC x where x.key = 10 CLUSTER BY x.key;
SELECT * FROM SRC x where x.key = 10 CLUSTER BY x.key;

EXPLAIN
SELECT * FROM SRC x  where x.key = 20 CLUSTER BY key ;
SELECT * FROM SRC x where x.key = 20 CLUSTER BY key ;

EXPLAIN
SELECT x.* FROM SRC x where x.key = 20 CLUSTER BY key;
SELECT x.* FROM SRC x where x.key = 20 CLUSTER BY key;

EXPLAIN
SELECT x.*  FROM SRC x where x.key = 20 CLUSTER BY x.key;
SELECT x.*  FROM SRC x where x.key = 20 CLUSTER BY x.key;

EXPLAIN
SELECT x.key, x.value as v1 FROM SRC x where x.key = 20 CLUSTER BY key ;
SELECT x.key, x.value as v1 FROM SRC x where x.key = 20 CLUSTER BY key ;

EXPLAIN
SELECT x.key, x.value as v1 FROM SRC x where x.key = 20 CLUSTER BY x.key;
SELECT x.key, x.value as v1 FROM SRC x where x.key = 20 CLUSTER BY x.key;

EXPLAIN
SELECT x.key, x.value as v1  FROM SRC x where x.key = 20 CLUSTER BY v1;
SELECT x.key, x.value as v1  FROM SRC x where x.key = 20 CLUSTER BY v1;

EXPLAIN
SELECT y.* from (SELECT x.* FROM SRC x CLUSTER BY x.key) y where y.key = 20;
SELECT y.* from (SELECT x.* FROM SRC x CLUSTER BY x.key) y where y.key = 20;


EXPLAIN
SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key)  where x.key = 20 CLUSTER BY v1;;
SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY v1;

EXPLAIN
SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY v1;
SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY v1;

EXPLAIN
SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY x.key ;
SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY x.key ;

EXPLAIN
SELECT x.key, x.value as v1, y.key as yk  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY key ;
SELECT x.key, x.value as v1, y.key as yk  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY key ;

EXPLAIN
SELECT unioninput.*
FROM (
  FROM src select src.key, src.value WHERE src.key < 100
  UNION ALL
  FROM src SELECT src.* WHERE src.key > 100
) unioninput
CLUSTER BY unioninput.key;

SELECT unioninput.*
FROM (
  FROM src select src.key, src.value WHERE src.key < 100
  UNION ALL
  FROM src SELECT src.* WHERE src.key > 100
) unioninput
CLUSTER BY unioninput.key;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
CLUSTER BY tvalue, tkey
DISTRIBUTE BY tvalue, tkey;
FROM src
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
CLUSTER BY tvalue, tkey
ORDER BY ten, one;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
CLUSTER BY tvalue, tkey
SORT BY ten, one;
EXPLAIN
SELECT x.key, x.value as v1, y.*  FROM SRC x JOIN SRC y ON (x.key = y.key) CLUSTER BY key;

EXPLAIN
SELECT x.key as k1, x.value FROM SRC x CLUSTER BY x.key;
EXPLAIN
SELECT x.key as k1, x.value FROM SRC x CLUSTER BY key;
-- TaskLog retrieval upon Null Pointer Exception in Cluster

CREATE TEMPORARY FUNCTION evaluate_npe AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFEvaluateNPE';

FROM src
SELECT evaluate_npe(src.key) LIMIT 1;
CREATE TABLE src_null(a bigint) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/nulls.txt' INTO TABLE src_null;

create table all_nulls as SELECT a, cast(a as double) as b, cast(a as decimal) as c  FROM src_null where a is null limit 5;
analyze table all_nulls compute statistics for columns;

describe formatted all_nulls a;
describe formatted all_nulls b;

drop table all_nulls;
drop table src_null;
-- SORT_QUERY_RESULTS

CREATE TABLE columnarserde_create_shortcut(a array<int>, b array<string>, c map<string,string>, d int, e string) STORED AS RCFILE;

EXPLAIN
FROM src_thrift
INSERT OVERWRITE TABLE columnarserde_create_shortcut SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring DISTRIBUTE BY 1;

FROM src_thrift
INSERT OVERWRITE TABLE columnarserde_create_shortcut SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring DISTRIBUTE BY 1;

SELECT columnarserde_create_shortcut.* FROM columnarserde_create_shortcut CLUSTER BY 1;

SELECT columnarserde_create_shortcut.a[0], columnarserde_create_shortcut.b[0], columnarserde_create_shortcut.c['key2'], columnarserde_create_shortcut.d, columnarserde_create_shortcut.e FROM columnarserde_create_shortcut CLUSTER BY 1;

CREATE table columnShortcutTable (key STRING, value STRING) STORED AS RCFILE;

FROM src
INSERT OVERWRITE TABLE columnShortcutTable SELECT src.key, src.value LIMIT 10;
describe columnShortcutTable;
SELECT * FROM columnShortcutTable;

ALTER TABLE columnShortcutTable ADD COLUMNS (c string);
SELECT * FROM columnShortcutTable;
set hive.metastore.disallow.incompatible.col.type.changes=false;
ALTER TABLE columnShortcutTable REPLACE COLUMNS (key int);
reset hive.metastore.disallow.incompatible.col.type.changes;
SELECT * FROM columnShortcutTable;
set hive.stats.fetch.column.stats=true;
set hive.stats.fetch.partition.stats=true;
set hive.compute.query.using.stats=true;
set hive.mapred.mode=nonstrict;

drop table calendar;

CREATE TABLE calendar (year int, month int);

insert into calendar values (2010, 10), (2011, 11), (2012, 12);

desc formatted calendar;

analyze table calendar compute statistics;

desc formatted calendar;

explain select count(1) from calendar;

explain select max(year) from calendar;

select max(year) from calendar;

select max(month) from calendar;

analyze table calendar compute statistics for columns;

desc formatted calendar;

explain select max(year) from calendar;

select max(year) from calendar;

insert into calendar values (2015, 15);

desc formatted calendar;

explain select max(year) from calendar;

select max(year) from calendar;

explain select max(month) from calendar;

select max(month) from calendar;

analyze table calendar compute statistics for columns year;

desc formatted calendar;

explain select max(year) from calendar;

select max(year) from calendar;

explain select max(month) from calendar;

select max(month) from calendar;

analyze table calendar compute statistics for columns month;

desc formatted calendar;

explain select max(month) from calendar;

select max(month) from calendar;

CREATE TABLE calendarp (`year` int)  partitioned by (p int);

insert into table calendarp partition (p=1) values (2010), (2011), (2012);

desc formatted calendarp partition (p=1);

explain select max(year) from calendarp where p=1;

select max(year) from calendarp where p=1;

analyze table calendarp partition (p=1) compute statistics for columns;

desc formatted calendarp partition (p=1);

explain select max(year) from calendarp where p=1;

insert into table calendarp partition (p=1) values (2015);

desc formatted calendarp partition (p=1);

explain select max(year) from calendarp where p=1;

select max(year) from calendarp where p=1;

create table t (key string, value string);

load data local inpath '../../data/files/kv1.txt' into table t;

desc formatted t;

analyze table t compute statistics;

desc formatted t;




set hive.stats.fetch.column.stats=true;
set hive.stats.fetch.partition.stats=true;
set hive.compute.query.using.stats=true;


drop table calendar;

CREATE TABLE calendar (year int, month int) clustered by (month) into 2 buckets stored as orc;

insert into calendar values (2010, 10), (2011, 11), (2012, 12);

desc formatted calendar;

analyze table calendar compute statistics for columns year;

desc formatted calendar;

explain select max(year) from calendar;

select max(year) from calendar;

explain select count(1) from calendar;

select count(1) from calendar;

ALTER TABLE calendar CHANGE year year1 INT;

--after patch, should be old stats rather than -1

desc formatted calendar;

--but basic/column stats can not be used by optimizer

explain select max(month) from calendar;

select max(month) from calendar;

explain select count(1) from calendar;

select count(1) from calendar;

truncate table calendar;

--after patch, should be 0

desc formatted calendar;

--but column stats can not be used by optimizer

explain select max(month) from calendar;

select max(month) from calendar;

--basic stats can be used by optimizer

explain select count(1) from calendar;

select count(1) from calendar;
set hive.mapred.mode=nonstrict;

DROP TABLE Employee_Part;

CREATE TABLE Employee_Part(employeeID int, employeeName String) partitioned by (employeeSalary double)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/employee.dat" INTO TABLE Employee_Part partition(employeeSalary=2000.0);
LOAD DATA LOCAL INPATH "../../data/files/employee.dat" INTO TABLE Employee_Part partition(employeeSalary=4000.0);

explain
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID;
explain extended
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID;
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns employeeID;

explain
analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID;
explain extended
analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID;
analyze table Employee_Part partition (employeeSalary=4000.0) compute statistics for columns employeeID;

explain
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns;
analyze table Employee_Part partition (employeeSalary=2000.0) compute statistics for columns;

describe formatted Employee_Part partition (employeeSalary=2000.0) employeeID;
describe formatted Employee_Part partition (employeeSalary=2000.0) employeeName;

explain
analyze table Employee_Part  compute statistics for columns;
analyze table Employee_Part  compute statistics for columns;

describe formatted Employee_Part partition(employeeSalary=2000.0) employeeID;
describe formatted Employee_Part partition(employeeSalary=4000.0) employeeID;

set hive.analyze.stmt.collect.partlevel.stats=false;
explain
analyze table Employee_Part  compute statistics for columns;
analyze table Employee_Part  compute statistics for columns;

describe formatted Employee_Part employeeID;

set hive.analyze.stmt.collect.partlevel.stats=true;

create database if not exists dummydb;

use dummydb;

analyze table default.Employee_Part partition (employeeSalary=2000.0) compute statistics for columns;

describe formatted default.Employee_Part partition (employeeSalary=2000.0) employeeID;

analyze table default.Employee_Part  compute statistics for columns;

use default;

drop database dummydb;

set hive.mapred.mode=nonstrict;
DROP TABLE Employee_Part;

CREATE TABLE Employee_Part(employeeID int, employeeName String) partitioned by (employeeSalary double, country string)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/employee.dat"  INTO TABLE Employee_Part partition(employeeSalary='2000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='2000.0', country='UK');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='3000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='4000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='3500.0', country='UK');
LOAD DATA LOCAL INPATH "../../data/files/employee.dat"  INTO TABLE Employee_Part partition(employeeSalary='3000.0', country='UK');

-- dynamic partitioning syntax
explain
analyze table Employee_Part partition (employeeSalary='4000.0', country) compute statistics for columns employeeName, employeeID;
analyze table Employee_Part partition (employeeSalary='4000.0', country) compute statistics for columns employeeName, employeeID;

describe formatted Employee_Part partition (employeeSalary='4000.0', country='USA') employeeName;

-- don't specify all partitioning keys
explain
analyze table Employee_Part partition (employeeSalary='2000.0') compute statistics for columns employeeID;
analyze table Employee_Part partition (employeeSalary='2000.0') compute statistics for columns employeeID;

describe formatted Employee_Part partition (employeeSalary='2000.0', country='USA') employeeID;
describe formatted Employee_Part partition (employeeSalary='2000.0', country='UK') employeeID;
-- don't specify any partitioning keys
explain
analyze table Employee_Part partition (employeeSalary) compute statistics for columns employeeID;
analyze table Employee_Part partition (employeeSalary) compute statistics for columns employeeID;

describe formatted Employee_Part partition (employeeSalary='3000.0', country='UK') employeeID;
explain
analyze table Employee_Part partition (employeeSalary,country) compute statistics for columns;
analyze table Employee_Part partition (employeeSalary,country) compute statistics for columns;

describe formatted Employee_Part partition (employeeSalary='3500.0', country='UK') employeeName;

-- partially populated stats
drop table Employee;
CREATE TABLE Employee(employeeID int, employeeName String) partitioned by (employeeSalary double, country string)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/employee.dat"  INTO TABLE Employee partition(employeeSalary='2000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee partition(employeeSalary='2000.0', country='UK');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee partition(employeeSalary='3500.0', country='UK');
LOAD DATA LOCAL INPATH "../../data/files/employee.dat"  INTO TABLE Employee partition(employeeSalary='3000.0', country='UK');

analyze table Employee partition (employeeSalary,country) compute statistics for columns;

describe formatted Employee partition (employeeSalary='3500.0', country='UK') employeeName;
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee partition(employeeSalary='3000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee partition(employeeSalary='4000.0', country='USA');

analyze table Employee partition (employeeSalary) compute statistics for columns;

describe formatted Employee partition (employeeSalary='3000.0', country='USA') employeeName;

-- add columns
alter table Employee add columns (c int ,d string);

LOAD DATA LOCAL INPATH "../../data/files/employee_part.txt"  INTO TABLE Employee partition(employeeSalary='6000.0', country='UK');

analyze table Employee partition (employeeSalary='6000.0',country='UK') compute statistics for columns;

describe formatted Employee partition (employeeSalary='6000.0', country='UK') employeeName;
describe formatted Employee partition (employeeSalary='6000.0', country='UK') c;
describe formatted Employee partition (employeeSalary='6000.0', country='UK') d;

-- JAVA_VERSION_SPECIFIC_OUTPUT

DROP TABLE Employee_Part;

CREATE TABLE Employee_Part(employeeID int, employeeName String) partitioned by (employeeSalary double, country string)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='2000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='2000.0', country='UK');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='3000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='4000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='3500.0', country='UK');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='3000.0', country='UK');

-- specify invalid values for the partitioning keys
explain
analyze table Employee_Part partition (employeeSalary='4000.0', country='Canada') compute statistics for columns employeeName, employeeID;
analyze table Employee_Part partition (employeeSalary='4000.0', country='Canada') compute statistics for columns employeeName, employeeID;
DROP TABLE Employee_Part;

CREATE TABLE Employee_Part(employeeID int, employeeName String) partitioned by (employeeSalary double, country string)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='2000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='2000.0', country='UK');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='3000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='4000.0', country='USA');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='3500.0', country='UK');
LOAD DATA LOCAL INPATH "../../data/files/employee2.dat" INTO TABLE Employee_Part partition(employeeSalary='3000.0', country='UK');

-- specify partitioning clause multiple times
explain
analyze table Employee_Part partition (employeeSalary='4000.0', country='USA') partition(employeeSalary='2000.0', country='USA') compute statistics for columns employeeName, employeeID;
analyze table Employee_Part partition (employeeSalary='4000.0', country='USA') partition(employeeSalary='2000.0', country='USA') compute statistics for columns employeeName, employeeID;
set hive.mapred.mode=nonstrict;
-- Test type date, int, and string in partition column
drop table if exists partcolstats;

create table partcolstats (key int, value string) partitioned by (ds date, hr int, part string);
insert into partcolstats partition (ds=date '2015-04-02', hr=2, part='partA') select key, value from src limit 20;
insert into partcolstats partition (ds=date '2015-04-02', hr=2, part='partB') select key, value from src limit 20;
insert into partcolstats partition (ds=date '2015-04-02', hr=3, part='partA') select key, value from src limit 30;
insert into partcolstats partition (ds=date '2015-04-03', hr=3, part='partA') select key, value from src limit 40;
insert into partcolstats partition (ds=date '2015-04-03', hr=3, part='partB') select key, value from src limit 60;

analyze table partcolstats partition (ds=date '2015-04-02', hr=2, part='partA') compute statistics for columns;
describe formatted partcolstats partition (ds=date '2015-04-02', hr=2, part='partA') key;
describe formatted partcolstats partition (ds=date '2015-04-02', hr=2, part='partA') value;

describe formatted partcolstats partition (ds=date '2015-04-02', hr=2, part='partB') key;
describe formatted partcolstats partition (ds=date '2015-04-02', hr=2, part='partB') value;

analyze table partcolstats partition (ds=date '2015-04-02', hr=2, part) compute statistics for columns;
describe formatted partcolstats partition (ds=date '2015-04-02', hr=2, part='partB') key;
describe formatted partcolstats partition (ds=date '2015-04-02', hr=2, part='partB') value;

describe formatted partcolstats partition (ds=date '2015-04-02', hr=3, part='partA') key;
describe formatted partcolstats partition (ds=date '2015-04-02', hr=3, part='partA') value;

analyze table partcolstats partition (ds=date '2015-04-02', hr, part) compute statistics for columns;
describe formatted partcolstats partition (ds=date '2015-04-02', hr=3, part='partA') key;
describe formatted partcolstats partition (ds=date '2015-04-02', hr=3, part='partA') value;

describe formatted partcolstats partition (ds=date '2015-04-03', hr=3, part='partA') key;
describe formatted partcolstats partition (ds=date '2015-04-03', hr=3, part='partA') value;
describe formatted partcolstats partition (ds=date '2015-04-03', hr=3, part='partB') key;
describe formatted partcolstats partition (ds=date '2015-04-03', hr=3, part='partB') value;

analyze table partcolstats partition (ds, hr, part) compute statistics for columns;
describe formatted partcolstats partition (ds=date '2015-04-03', hr=3, part='partA') key;
describe formatted partcolstats partition (ds=date '2015-04-03', hr=3, part='partA') value;
describe formatted partcolstats partition (ds=date '2015-04-03', hr=3, part='partB') key;
describe formatted partcolstats partition (ds=date '2015-04-03', hr=3, part='partB') value;

drop table partcolstats;

-- Test type tinyint, smallint, and bigint in partition column
drop table if exists partcolstatsnum;
create table partcolstatsnum (key int, value string) partitioned by (tint tinyint, sint smallint, bint bigint);
insert into partcolstatsnum partition (tint=100, sint=1000, bint=1000000) select key, value from src limit 30;

analyze table partcolstatsnum partition (tint=100, sint=1000, bint=1000000) compute statistics for columns;
describe formatted partcolstatsnum partition (tint=100, sint=1000, bint=1000000) value;

drop table partcolstatsnum;

-- Test type decimal in partition column
drop table if exists partcolstatsdec;
create table partcolstatsdec (key int, value string) partitioned by (decpart decimal(8,4));
insert into partcolstatsdec partition (decpart='1000.0001') select key, value from src limit 30;

analyze table partcolstatsdec partition (decpart='1000.0001') compute statistics for columns;
describe formatted partcolstatsdec partition (decpart='1000.0001') value;

drop table partcolstatsdec;

-- Test type varchar and char in partition column
drop table if exists partcolstatschar;
create table partcolstatschar (key int, value string) partitioned by (varpart varchar(5), charpart char(3));
insert into partcolstatschar partition (varpart='part1', charpart='aaa') select key, value from src limit 30;

analyze table partcolstatschar partition (varpart='part1', charpart='aaa') compute statistics for columns;
describe formatted partcolstatschar partition (varpart='part1', charpart='aaa') value;

drop table partcolstatschar;

DROP TABLE IF EXISTS user_web_events;
create temporary table user_web_events(`user id` bigint, `user name` string);

explain analyze table user_web_events compute statistics for columns;
analyze table user_web_events compute statistics for columns;

explain analyze table user_web_events compute statistics for columns `user id`;
analyze table user_web_events compute statistics for columns `user id`;

DROP TABLE IF EXISTS UserVisits_web_text_none;

CREATE TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none;

explain
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

explain extended
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

explain
analyze table default.UserVisits_web_text_none compute statistics for columns;

analyze table default.UserVisits_web_text_none compute statistics for columns;

describe formatted UserVisits_web_text_none destURL;
describe formatted UserVisits_web_text_none adRevenue;
describe formatted UserVisits_web_text_none avgTimeOnSite;

CREATE TABLE empty_tab(
   a int,
   b double,
   c string,
   d boolean,
   e binary)
row format delimited fields terminated by '|'  stored as textfile;

explain
analyze table empty_tab compute statistics for columns a,b,c,d,e;

analyze table empty_tab compute statistics for columns a,b,c,d,e;

create database if not exists dummydb;

use dummydb;

analyze table default.UserVisits_web_text_none compute statistics for columns destURL;

describe formatted default.UserVisits_web_text_none destURL;

CREATE TABLE UserVisits_in_dummy_db (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/UserVisits.dat" INTO TABLE UserVisits_in_dummy_db;

use default;

explain
analyze table dummydb.UserVisits_in_dummy_db compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

explain extended
analyze table dummydb.UserVisits_in_dummy_db compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

analyze table dummydb.UserVisits_in_dummy_db compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

explain
analyze table dummydb.UserVisits_in_dummy_db compute statistics for columns;

analyze table dummydb.UserVisits_in_dummy_db compute statistics for columns;

describe formatted dummydb.UserVisits_in_dummy_db destURL;
describe formatted dummydb.UserVisits_in_dummy_db adRevenue;
describe formatted dummydb.UserVisits_in_dummy_db avgTimeOnSite;

drop table dummydb.UserVisits_in_dummy_db;

drop database dummydb;






DROP TABLE IF EXISTS UserVisits_web_text_none;

CREATE TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none;

explain
analyze table UserVisits_web_text_none compute statistics for columns destIP;

analyze table UserVisits_web_text_none compute statistics for columns destIP;


DROP TABLE IF EXISTS table_complex_type;

CREATE TABLE table_complex_type (
       a STRING,
       b ARRAY<STRING>,
       c ARRAY<MAP<STRING,STRING>>,
       d MAP<STRING,ARRAY<STRING>>
       ) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/create_nested_type.txt' OVERWRITE INTO TABLE table_complex_type;


explain
analyze table table_complex_type compute statistics for columns d;

analyze table table_complex_type  compute statistics for columns d;

DROP TABLE IF EXISTS UserVisits_web_text_none;

CREATE TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none;

explain
analyze table UserVisits_web_text_none compute statistics for columns destIP;

analyze table UserVisits_web_text_none compute statistics for columns destIP;

set hive.mapred.mode=nonstrict;
SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook;
SET hive.stats.collect.scancols=true;

-- SORT_QUERY_RESULTS
-- This test is used for testing the ColumnAccessAnalyzer

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T4(key STRING, val STRING) PARTITIONED BY (p STRING);

-- Simple select queries
SELECT key FROM T1;
SELECT key, val FROM T1;
SELECT 1 FROM T1;
SELECT key, val from T4 where p=1;
SELECT val FROM T4 where p=1;
SELECT p, val FROM T4 where p=1;

-- More complicated select queries
EXPLAIN SELECT key FROM (SELECT key, val FROM T1) subq1;
SELECT key FROM (SELECT key, val FROM T1) subq1;
EXPLAIN SELECT k FROM (SELECT key as k, val as v FROM T1) subq1;
SELECT k FROM (SELECT key as k, val as v FROM T1) subq1;
SELECT key + 1 as k FROM T1;
SELECT key + val as k FROM T1;

-- Work with union
EXPLAIN
SELECT * FROM (
SELECT key as c FROM T1
 UNION ALL
SELECT val as c FROM T1
) subq1;

SELECT * FROM (
SELECT key as c FROM T1
 UNION ALL
SELECT val as c FROM T1
) subq1;

EXPLAIN
SELECT * FROM (
SELECT key as c FROM T1
 UNION ALL
SELECT key as c FROM T1
) subq1;

SELECT * FROM (
SELECT key as c FROM T1
 UNION ALL
SELECT key as c FROM T1
) subq1;

-- Work with insert overwrite
FROM T1
INSERT OVERWRITE TABLE T2 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE T3 SELECT key, sum(val) GROUP BY key;

-- Simple joins
SELECT *
FROM T1 JOIN T2
ON T1.key = T2.key ;

EXPLAIN
SELECT T1.key
FROM T1 JOIN T2
ON T1.key = T2.key;

SELECT T1.key
FROM T1 JOIN T2
ON T1.key = T2.key;

SELECT *
FROM T1 JOIN T2
ON T1.key = T2.key AND T1.val = T2.val;

-- Map join
SELECT /*+ MAPJOIN(a) */ *
FROM T1 a JOIN T2 b
ON a.key = b.key;

-- More joins
EXPLAIN
SELECT *
FROM T1 JOIN T2
ON T1.key = T2.key AND T1.val = 3 and T2.val = 3;

SELECT *
FROM T1 JOIN T2
ON T1.key = T2.key AND T1.val = 3 and T2.val = 3;

EXPLAIN
SELECT subq1.val
FROM
(
  SELECT val FROM T1 WHERE key = 5
) subq1
JOIN
(
  SELECT val FROM T2 WHERE key = 6
) subq2
ON subq1.val = subq2.val;

SELECT subq1.val
FROM
(
  SELECT val FROM T1 WHERE key = 5
) subq1
JOIN
(
  SELECT val FROM T2 WHERE key = 6
) subq2
ON subq1.val = subq2.val;

-- Join followed by join
EXPLAIN
SELECT *
FROM
(
  SELECT subq1.key as key
  FROM
  (
    SELECT key, val FROM T1
  ) subq1
  JOIN
  (
    SELECT key, 'teststring' as val FROM T2
  ) subq2
  ON subq1.key = subq2.key
) T4
JOIN T3
ON T3.key = T4.key;

SELECT *
FROM
(
  SELECT subq1.key as key
  FROM
  (
    SELECT key, val FROM T1
  ) subq1
  JOIN
  (
    SELECT key, 'teststring' as val FROM T2
  ) subq2
  ON subq1.key = subq2.key
) T4
JOIN T3
ON T3.key = T4.key;

-- for partitioned table
SELECT * FROM srcpart TABLESAMPLE (10 ROWS);
SELECT key,ds FROM srcpart TABLESAMPLE (10 ROWS) WHERE hr='11';
SELECT value FROM srcpart TABLESAMPLE (10 ROWS) WHERE ds='2008-04-08';
CREATE TABLE skewedtable (key STRING, value STRING) SKEWED BY (key) ON (1,5,6);

ALTER TABLE skewedtable CHANGE key key INT;
drop table tstsrc;
create table tstsrc like src;
insert overwrite table tstsrc
select key, value from src;

alter table tstsrc change src_not_exist key_value string;
drop table tstsrc;
create table tstsrc like src;
insert overwrite table tstsrc
select key, value from src;

alter table tstsrc change key value string;
alter table src change key key;drop table tstsrc;
create table tstsrc like src;
insert overwrite table tstsrc
select key, value from src;

alter table tstsrc change key key2 string after key_value;
CREATE TABLE skewedtable (key STRING, value STRING) SKEWED BY (key) ON (1,5,6);

ALTER TABLE skewedtable CHANGE key key_new STRING;

set hive.exec.compress.output = true;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;

set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;

-- SORT_QUERY_RESULTS

create table combine1_1(key string, value string) stored as textfile;

insert overwrite table combine1_1
select * from src;


select key, value from combine1_1;

set hive.mapred.mode=nonstrict;
USE default;

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set mapred.cache.shared.enabled=false;
set hive.merge.smallfiles.avgsize=0;

-- EXCLUDE_OS_WINDOWS
-- excluded on windows because of difference in file name encoding logic

-- SORT_QUERY_RESULTS

create table combine2(key string) partitioned by (value string);

-- EXCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)
-- This test sets mapred.max.split.size=256 and hive.merge.smallfiles.avgsize=0
-- in an attempt to force the generation of multiple splits and multiple output files.
-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
-- when using CombineFileInputFormat, so only one split is generated. This has a
-- significant impact on the results results of this test.
-- This issue was fixed in MAPREDUCE-2046 which is included in 0.22.

insert overwrite table combine2 partition(value)
select * from (
   select key, value from src where key < 10
   union all
   select key, '|' as value from src where key = 11
   union all
   select key, '2010-04-21 09:45:00' value from src where key = 19) s;

show partitions combine2;

explain
select key, value from combine2 where value is not null;

select key, value from combine2 where value is not null;

explain extended
select count(1) from combine2 where value is not null;

select count(1) from combine2 where value is not null;

explain
select ds, count(1) from srcpart where ds is not null group by ds;

select ds, count(1) from srcpart where ds is not null group by ds;
USE default;

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set mapred.cache.shared.enabled=false;
set hive.merge.smallfiles.avgsize=0;

-- EXCLUDE_OS_WINDOWS
-- excluded on windows because of difference in file name encoding logic

-- SORT_QUERY_RESULTS

create table combine2(key string) partitioned by (value string);

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)
-- This test sets mapred.max.split.size=256 and hive.merge.smallfiles.avgsize=0
-- in an attempt to force the generation of multiple splits and multiple output files.
-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
-- when using CombineFileInputFormat, so only one split is generated. This has a
-- significant impact on the results results of this test.
-- This issue was fixed in MAPREDUCE-2046 which is included in 0.22.

insert overwrite table combine2 partition(value)
select * from (
   select key, value from src where key < 10
   union all
   select key, '|' as value from src where key = 11
   union all
   select key, '2010-04-21 09:45:00' value from src where key = 19) s;

show partitions combine2;

explain
select key, value from combine2 where value is not null;

select key, value from combine2 where value is not null;

explain extended
select count(1) from combine2 where value is not null;

select count(1) from combine2 where value is not null;

explain
select ds, count(1) from srcpart where ds is not null group by ds;

select ds, count(1) from srcpart where ds is not null group by ds;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set mapred.cache.shared.enabled=false;
set hive.merge.smallfiles.avgsize=0;

-- INCLUDE_OS_WINDOWS
-- included only on  windows because of difference in file name encoding logic

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)

create table combine2(key string) partitioned by (value string);

insert overwrite table combine2 partition(value)
select * from (
   select key, value from src where key < 10
   union all
   select key, '|' as value from src where key = 11
   union all
   select key, '2010-04-21 09:45:00' value from src where key = 19) s;

show partitions combine2;

explain
select key, value from combine2 where value is not null order by key;

select key, value from combine2 where value is not null order by key;

explain extended
select count(1) from combine2 where value is not null;

select count(1) from combine2 where value is not null;

explain
select ds, count(1) from srcpart where ds is not null group by ds;

select ds, count(1) from srcpart where ds is not null group by ds;
set hive.exec.compress.output = true;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;


drop table combine_3_srcpart_seq_rc;

create table combine_3_srcpart_seq_rc (key int , value string) partitioned by (ds string, hr string) stored as sequencefile;

insert overwrite table combine_3_srcpart_seq_rc partition (ds="2010-08-03", hr="00") select * from src;

alter table combine_3_srcpart_seq_rc set fileformat rcfile;
insert overwrite table combine_3_srcpart_seq_rc partition (ds="2010-08-03", hr="001") select * from src;

desc extended combine_3_srcpart_seq_rc partition(ds="2010-08-03", hr="00");
desc extended combine_3_srcpart_seq_rc partition(ds="2010-08-03", hr="001");

select key, value, ds, hr from combine_3_srcpart_seq_rc where ds="2010-08-03" order by key, hr limit 30;

;
set hive.exec.reducers.max = 1;

drop table bucket3_1;
CREATE TABLE combine_3_srcpart_seq_rc_bucket(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS stored as sequencefile;

insert overwrite table combine_3_srcpart_seq_rc_bucket partition (ds='1')
select * from src;

alter table combine_3_srcpart_seq_rc_bucket set fileformat rcfile;

insert overwrite table combine_3_srcpart_seq_rc_bucket partition (ds='11')
select * from src;

select key, ds from combine_3_srcpart_seq_rc_bucket tablesample (bucket 1 out of 2) s where ds = '1' or ds= '11' order by key, ds limit 30;

drop table combine_3_srcpart_seq_rc_bucket;

drop table combine_3_srcpart_seq_rc;
set hive.mapred.mode=strict;

-- This should fail until we fix the issue with precision when casting a bigint to a double

select * from src where cast(1 as bigint) = 1.0 limit 10;set hive.mapred.mode=strict;

--This should fail until we fix the issue with precision when casting a bigint to a double

select * from src where cast(1 as bigint) = '1' limit 10;
compile `import org.apache.hadoop.hive.ql.exec.UDF \;
public class Pyth extends UDF {
  public double evaluate(double a, double b){
    return Math.sqrt((a*a) + (b*b)) \;
  }
} ` AS GROOVY NAMED Pyth.groovy;
CREATE TEMPORARY FUNCTION Pyth as 'Pyth';

SELECT Pyth(3,4) FROM src tablesample (1 rows);

DROP TEMPORARY FUNCTION Pyth;

compile `import org.apache.hadoop.hive.ql.exec.UDF \;
public class Pyth extsfgsfgfsends UDF {
  public double evaluate(double a, double b){
    return Math.sqrt((a*a) + (b*b)) \;
  }
} ` AS GROOVY NAMED Pyth.groovy;

set hive.mapred.mode=nonstrict;
CREATE TABLE agg1 (col0 INT, col1 STRING, col2 DOUBLE);

INSERT INTO TABLE agg1 select key,value,key from src tablesample (1 rows);

EXPLAIN
SELECT single_use_subq11.a1 AS a1,
       single_use_subq11.a2 AS a2
FROM   (SELECT Sum(agg1.col2) AS a1
        FROM   agg1
        GROUP  BY agg1.col0) single_use_subq12
       JOIN (SELECT alias.a2 AS a0,
                    alias.a1 AS a1,
                    alias.a1 AS a2
             FROM   (SELECT agg1.col1 AS a0,
                            '42'      AS a1,
                            agg1.col0 AS a2
                     FROM   agg1
                     UNION ALL
                     SELECT agg1.col1 AS a0,
                            '41'      AS a1,
                            agg1.col0 AS a2
                     FROM   agg1) alias
             GROUP  BY alias.a2,
                       alias.a1) single_use_subq11
         ON ( single_use_subq11.a0 = single_use_subq11.a0 );

SELECT single_use_subq11.a1 AS a1,
       single_use_subq11.a2 AS a2
FROM   (SELECT Sum(agg1.col2) AS a1
        FROM   agg1
        GROUP  BY agg1.col0) single_use_subq12
       JOIN (SELECT alias.a2 AS a0,
                    alias.a1 AS a1,
                    alias.a1 AS a2
             FROM   (SELECT agg1.col1 AS a0,
                            '42'      AS a1,
                            agg1.col0 AS a2
                     FROM   agg1
                     UNION ALL
                     SELECT agg1.col1 AS a0,
                            '41'      AS a1,
                            agg1.col0 AS a2
                     FROM   agg1) alias
             GROUP  BY alias.a2,
                       alias.a1) single_use_subq11
         ON ( single_use_subq11.a0 = single_use_subq11.a0 );
drop table if exists testAvro;

dfs -cp ${system:hive.root}data/files/grad.avsc ${system:test.tmp.dir}/;

-- File URIs using system:hive.root (using file:/) don't seem to work properly in DDL statements on Windows,
-- so use dfs to copy them over to system:test.tmp.dir (which uses pfile:/), which does appear to work

create table testAvro
   ROW FORMAT SERDE
     'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
   STORED AS INPUTFORMAT
     'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
   OUTPUTFORMAT
     'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
   TBLPROPERTIES ('avro.schema.url'='${system:test.tmp.dir}/grad.avsc');

describe formatted testAvro col1;

analyze table testAvro compute statistics for columns col1,col3;

describe formatted testAvro col1;
create table tab_binary(a binary);

-- insert some data
LOAD DATA LOCAL INPATH "../../data/files/binary.txt" INTO TABLE tab_binary;

select count(*) from tab_binary;

-- compute statistical summary of data
select compute_stats(a, 16) from tab_binary;
create table tab_bool(a boolean);

-- insert some data
LOAD DATA LOCAL INPATH "../../data/files/bool.txt" INTO TABLE tab_bool;

select count(*) from tab_bool;

-- compute statistical summary of data
select compute_stats(a, 16) from tab_bool;

create table tab_date (
  origin_city_name string,
  dest_city_name string,
  fl_date date,
  arr_delay float,
  fl_num int
);

-- insert some data
load data local inpath '../../data/files/flights_join.txt' overwrite into table tab_date;

select count(*) from tab_date;

-- compute statistical summary of data
select compute_stats(fl_date, 16) from tab_date;

explain
analyze table tab_date compute statistics for columns fl_date;

analyze table tab_date compute statistics for columns fl_date;

describe formatted tab_date fl_date;

-- Update stats manually. Try both yyyy-mm-dd and integer value for high/low value
alter table tab_date update statistics for column fl_date set ('numDVs'='19', 'highValue'='2015-01-01', 'lowValue'='0');

describe formatted tab_date fl_date;
set hive.stats.autogather=true;

create table tab_decimal(a decimal(10,3));

-- insert some data
LOAD DATA LOCAL INPATH "../../data/files/decimal.txt" INTO TABLE tab_decimal;

select count(*) from tab_decimal;

-- compute statistical summary of data
select compute_stats(a, 18) from tab_decimal;
create table tab_double(a double);

-- insert some data
LOAD DATA LOCAL INPATH "../../data/files/double.txt" INTO TABLE tab_double;

select count(*) from tab_double;

-- compute statistical summary of data
select compute_stats(a, 16) from tab_double;
create table tab_empty(a boolean, b int, c double, d string, e binary);

select count(*) from tab_empty;

-- compute statistical summary of data
select compute_stats(a, 16) from tab_empty;
select compute_stats(b, 16) from tab_empty;
select compute_stats(c, 16) from tab_empty;
select compute_stats(d, 16) from tab_empty;
select compute_stats(e, 16) from tab_empty;


create table tab_int(a int);

-- insert some data
LOAD DATA LOCAL INPATH "../../data/files/int.txt" INTO TABLE tab_int;

select count(*) from tab_int;

-- compute statistical summary of data
select compute_stats(a, 16) from tab_int;
create table tab_int(a int);

-- insert some data
LOAD DATA LOCAL INPATH "../../data/files/int.txt" INTO TABLE tab_int;

-- compute stats should raise an error since the number of bit vectors > 1024
select compute_stats(a, 10000) from tab_int;
create table tab_string(a string);

-- insert some data
LOAD DATA LOCAL INPATH "../../data/files/string.txt" INTO TABLE tab_string;

select count(*) from tab_string;

-- compute statistical summary of data
select compute_stats(a, 16) from tab_string;
CREATE TABLE citl_table (key STRING, value STRING) PARTITIONED BY (part STRING)
STORED AS RCFILE
LOCATION 'pfile:${system:test.tmp.dir}/citl_table';

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsSubdirectoryOfTableHook;

INSERT OVERWRITE TABLE citl_table PARTITION (part = '1') SELECT * FROM src;

SET hive.exec.post.hooks=;

ALTER TABLE citl_table SET LOCATION 'file:${system:test.tmp.dir}/citl_table';

ALTER TABLE citl_table PARTITION (part = '1') CONCATENATE;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsSubdirectoryOfTableHook;

SELECT count(*) FROM citl_table where part = '1';

SET hive.exec.post.hooks=;

DROP TABLE citl_table;
describe extended src;

describe formatted src key;

describe extended src1;

describe formatted src1 value;

describe extended src_json;

describe formatted src_json json;

describe extended src_sequencefile;

describe formatted src_sequencefile value;

describe extended srcbucket;

describe formatted srcbucket value;

describe extended srcbucket2;

describe formatted srcbucket2 value;

describe extended srcpart;

describe formatted srcpart PARTITION (ds="2008-04-09", hr="12") key;

describe extended alltypesorc;

describe formatted alltypesorc ctinyint;

describe formatted alltypesorc cfloat;

describe formatted alltypesorc ctimestamp1;

describe formatted alltypesorc cboolean2;






set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

explain extended
 select * from (select a.key as ak, a.value as av, b.key as bk, b.value as bv from src a join src1 b where a.key = '429' ) c;

 select * from (select a.key as ak, a.value as av, b.key as bk, b.value as bv from src a join src1 b where a.key = '429' ) c;
set hive.mapred.mode=nonstrict;

drop table test_1;

create table test_1 (id int, id2 int);

insert into table test_1 values (123, NULL), (NULL, NULL), (NULL, 123), (123, 123);

explain SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1;

SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1;

explain SELECT cast(CASE id when id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1;

SELECT cast(CASE id when id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1;

explain SELECT cast(CASE WHEN id = id2 THEN TRUE ELSE FALSE END AS BOOLEAN) AS b FROM test_1;

SELECT cast(CASE WHEN id = id2 THEN TRUE ELSE FALSE END AS BOOLEAN) AS b FROM test_1;

explain SELECT cast(CASE id when id2 THEN TRUE ELSE FALSE END AS BOOLEAN) AS b FROM test_1;

SELECT cast(CASE id when id2 THEN TRUE ELSE FALSE END AS BOOLEAN) AS b FROM test_1;


set hive.cbo.enable=false;

explain SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1;

SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1;

explain SELECT cast(CASE id when id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1;

SELECT cast(CASE id when id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1;

explain SELECT cast(CASE WHEN id = id2 THEN TRUE ELSE FALSE END AS BOOLEAN) AS b FROM test_1;

SELECT cast(CASE WHEN id = id2 THEN TRUE ELSE FALSE END AS BOOLEAN) AS b FROM test_1;

explain SELECT cast(CASE id when id2 THEN TRUE ELSE FALSE END AS BOOLEAN) AS b FROM test_1;

SELECT cast(CASE id when id2 THEN TRUE ELSE FALSE END AS BOOLEAN) AS b FROM test_1;

set hive.fetch.task.conversion=more;

EXPLAIN
SELECT NAMED_STRUCT(
         IF(ARRAY_CONTAINS(ARRAY(1, 2), 3), "F1", "B1"), 1,
         IF(ARRAY_CONTAINS(MAP_KEYS(MAP("b", "x")), "b"), "F2", "B2"), 2
       ),
       NAMED_STRUCT(
         IF(ARRAY_CONTAINS(ARRAY(1, 2), 3), "F1", "B1"), 1,
         IF(ARRAY_CONTAINS(MAP_KEYS(MAP("b", "x")), "b"), "F2", "B2"), 2
       ).F2
       FROM src tablesample (1 rows);

SELECT NAMED_STRUCT(
         IF(ARRAY_CONTAINS(ARRAY(1, 2), 3), "F1", "B1"), 1,
         IF(ARRAY_CONTAINS(MAP_KEYS(MAP("b", "x")), "b"), "F2", "B2"), 2
       ),
       NAMED_STRUCT(
         IF(ARRAY_CONTAINS(ARRAY(1, 2), 3), "F1", "B1"), 1,
         IF(ARRAY_CONTAINS(MAP_KEYS(MAP("b", "x")), "b"), "F2", "B2"), 2
       ).F2
       FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;
set hive.optimize.constant.propagation=true;

EXPLAIN
SELECT IF(INSTR(CONCAT('foo', 'bar'), 'foob') > 0, "F1", "B1")
       FROM src tablesample (1 rows);

SELECT IF(INSTR(CONCAT('foo', 'bar'), 'foob') > 0, "F1", "B1")
       FROM src tablesample (1 rows);
set hive.optimize.constant.propagation=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table dest(key string, value string) partitioned by (ds string);

EXPLAIN
from srcpart
insert overwrite table dest partition (ds) select key, value, ds where ds='2008-04-08';

from srcpart
insert overwrite table dest partition (ds) select key, value, ds where ds='2008-04-08';
set hive.mapred.mode=nonstrict;
set hive.execution.engine=tez;
set hive.optimize.constant.propagation=true;
set hive.tez.dynamic.partition.pruning=true;
set hive.explain.user=true;

drop table if exists tb1;
create table tb1 (id int);

drop table if exists tb2;
create table tb2 (id smallint);

explain
select a.id from tb1 a
left outer join
(select id from tb2
union all
select 2 as id from tb2 limit 1) b
on a.id=b.id;
set hive.optimize.constant.propagation=true;

CREATE TABLE dest1(d date, t timestamp);

EXPLAIN
INSERT OVERWRITE TABLE dest1
SELECT cast('2013-11-17' as date), cast(cast('1.3041352164485E9' as double) as timestamp)
       FROM src tablesample (1 rows);

INSERT OVERWRITE TABLE dest1
SELECT cast('2013-11-17' as date), cast(cast('1.3041352164485E9' as double) as timestamp)
       FROM src tablesample (1 rows);

SELECT * FROM dest1;

SELECT key, value FROM src WHERE key = cast(86 as double);

CREATE TABLE primitives1 (
  id INT  ,
  bool_col BOOLEAN  ,
  tinyint_col TINYINT  ,
  smallint_col SMALLINT  ,
  int_col INT  ,
  bigint_col BIGINT  ,
  float_col FLOAT  ,
  double_col DOUBLE  ,
  date_string_col STRING  ,
  string_col STRING  ,
  timestamp_col TIMESTAMP  )
ROW FORMAT DELIMITED
  FIELDS TERMINATED BY ','
  ESCAPED BY '\\'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/types/primitives/090101.txt'
OVERWRITE INTO TABLE primitives1 ;


select id,bool_col,tinyint_col,smallint_col,int_col,bigint_col,float_col,double_col from primitives1 where id = cast (0 as float) and bool_col = cast('true' as boolean) and tinyint_col = cast(0 as double) and smallint_col = cast(0 as bigint) and int_col = cast (0 as double) and bigint_col = cast(0 as tinyint) and float_col = cast(0.0 as string) and  double_col = cast (0.0 as float);
set hive.fetch.task.conversion=none;
set hive.explain.user=false;
set hive.vectorized.execution.enabled=true;

create table src_orc (bool0 boolean, key0 string, key1 string, key2 string) stored as orc;
insert overwrite table src_orc select true, key, value, key from src;
insert into table src_orc select true, key, value, key from src;

explain SELECT IF ( ( (CASE WHEN bool0 THEN 1 WHEN NOT bool0 THEN 0 END) = (CASE WHEN TRUE THEN 1 WHEN NOT TRUE THEN 0 END) ), key0, IF ( ( (CASE WHEN bool0 THEN 1 WHEN NOT bool0 THEN 0 END) = (CASE WHEN FALSE THEN 1 WHEN NOT FALSE THEN 0 END) ), key1, key2 ) ) FROM src_orc;

SELECT IF ( ( (CASE WHEN bool0 THEN 1 WHEN NOT bool0 THEN 0 END) = (CASE WHEN TRUE THEN 1 WHEN NOT TRUE THEN 0 END) ), key0, IF ( ( (CASE WHEN bool0 THEN 1 WHEN NOT bool0 THEN 0 END) = (CASE WHEN FALSE THEN 1 WHEN NOT FALSE THEN 0 END) ), key1, key2 ) ) FROM src_orc;

-- Ensure Enum fields are converted to strings (instead of struct<value:int>)

create table convert_enum_to_string
  partitioned by (b string)
  row format serde "org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer"
    with serdeproperties (
      "serialization.class"="org.apache.hadoop.hive.serde2.thrift.test.MegaStruct",
      "serialization.format"="org.apache.thrift.protocol.TBinaryProtocol");

describe convert_enum_to_string;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- This query has a GroupByOperator folling JoinOperator and they share the same keys.
-- When Correlation Optimizer is turned off, three MR jobs will be generated.
-- When Correlation Optimizer is turned on, two MR jobs will be generated
-- and JoinOperator (on the column of key) and GroupByOperator (also on the column
-- of key) will be executed in the first MR job.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.auto.convert.join=true;
set hive.optimize.correlation=true;
-- Enable hive.auto.convert.join.
-- Correlation Optimizer will detect that the join will be converted to a Map-join,
-- so it will not try to optimize this query.
-- We should generate 1 MR job for subquery tmp.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- If the key of a GroupByOperator is the left table's key in
-- a Left Semi Join, these two operators will be executed in
-- the same MR job when Correlation Optimizer is enabled.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x LEFT SEMI JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x LEFT SEMI JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x LEFT SEMI JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x LEFT SEMI JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- If the key of a GroupByOperator is the left table's key in
-- a Left Outer Join, these two operators will be executed in
-- the same MR job when Correlation Optimizer is enabled.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;


set hive.optimize.correlation=false;
-- If the key of a GroupByOperator is the right table's key in
-- a Left Outer Join, we cannot use a single MR to execute these two
-- operators because those keys with a null value are not grouped.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY y.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY y.key) tmp;

set hive.optimize.correlation=false;
-- If a column of the key of a GroupByOperator is the right table's key in
-- a Left Outer Join, we cannot use a single MR to execute these two
-- operators because those keys with a null value are not grouped.
EXPLAIN
SELECT x.key, y.value, count(1) AS cnt
FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key AND x.value = y.value)
GROUP BY x.key, y.value;

SELECT x.key, y.value, count(1) AS cnt
FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key AND x.value = y.value)
GROUP BY x.key, y.value;

set hive.optimize.correlation=true;
EXPLAIN
SELECT x.key, y.value, count(1) AS cnt
FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key AND x.value = y.value)
GROUP BY x.key, y.value;

SELECT x.key, y.value, count(1) AS cnt
FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key AND x.value = y.value)
GROUP BY x.key, y.value;

set hive.optimize.correlation=false;
-- If the key of a GroupByOperator is the right table's key in
-- a Right Outer Join, these two operators will be executed in
-- the same MR job when Correlation Optimizer is enabled.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM src1 x RIGHT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM src1 x RIGHT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY y.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM src1 x RIGHT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM src1 x RIGHT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY y.key) tmp;


set hive.optimize.correlation=false;
-- If the key of a GroupByOperator is the left table's key in
-- a Right Outer Join, we cannot use a single MR to execute these two
-- operators because those keys with a null value are not grouped.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x RIGHT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x RIGHT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x RIGHT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x RIGHT OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=false;
-- This query has a Full Outer Join followed by a GroupByOperator and
-- they share the same key. Because those keys with a null value are not grouped
-- in the output of the Full Outer Join, we cannot use a single MR to execute
-- these two operators.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x FULL OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x FULL OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x FULL OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x FULL OUTER JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- Currently, we only handle exactly same keys, this query will not be optimized
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.value)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, x.value AS value, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key, x.value) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.value)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, x.value AS value, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key, x.value) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.value)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, x.value AS value, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key, x.value) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.value)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, x.value AS value, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key, x.value) tmp;

set hive.optimize.correlation=false;
-- Currently, we only handle exactly same keys, this query will not be optimized
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key AND x.value = y.value)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key AND x.value = y.value)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key AND x.value = y.value)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key AND x.value = y.value)
      GROUP BY x.key) tmp;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery xx and xx join yy.
-- This case is used to test LEFT SEMI JOIN since Hive will
-- introduce a GroupByOperator before the ReduceSinkOperator of
-- the right table (yy in queries below)
-- of LEFT SEMI JOIN.

-- SORT_AND_HASH_QUERY_RESULTS

EXPLAIN
SELECT xx.key, xx.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
LEFT SEMI JOIN src yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
LEFT SEMI JOIN src yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
LEFT SEMI JOIN src yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
LEFT SEMI JOIN src yy
ON xx.key=yy.key;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery xx and xx join yy.
-- This case is used to test LEFT SEMI JOIN since Hive will
-- introduce a GroupByOperator before the ReduceSinkOperator of
-- the right table (yy in queries below)
-- of LEFT SEMI JOIN.
EXPLAIN
SELECT xx.key, xx.value
FROM
src1 xx
LEFT SEMI JOIN
(SELECT x.key as key
 FROM src x JOIN src y ON (x.key = y.key)
 WHERE x.key < 200 AND
       y.key > 20) yy
ON xx.key=yy.key;

SELECT xx.key, xx.value
FROM
src1 xx
LEFT SEMI JOIN
(SELECT x.key as key
 FROM src x JOIN src y ON (x.key = y.key)
 WHERE x.key < 200 AND
       y.key > 20) yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.value
FROM
src1 xx
LEFT SEMI JOIN
(SELECT x.key as key
 FROM src x JOIN src y ON (x.key = y.key)
 WHERE x.key < 200 AND
       y.key > 20) yy
ON xx.key=yy.key;

SELECT xx.key, xx.value
FROM
src1 xx
LEFT SEMI JOIN
(SELECT x.key as key
 FROM src x JOIN src y ON (x.key = y.key)
 WHERE x.key < 200 AND
       y.key > 20) yy
ON xx.key=yy.key;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- This test is used to test if we can use shared scan for
-- xx, yy:x, and yy:y.
EXPLAIN
SELECT xx.key, xx.value
FROM
src xx
LEFT SEMI JOIN
(SELECT x.key as key
 FROM src x JOIN src y ON (x.key = y.key)
 WHERE x.key < 200 AND x.key > 180) yy
ON xx.key=yy.key;

SELECT xx.key, xx.value
FROM
src xx
LEFT SEMI JOIN
(SELECT x.key as key
 FROM src x JOIN src y ON (x.key = y.key)
 WHERE x.key < 200 AND x.key > 180) yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.value
FROM
src xx
LEFT SEMI JOIN
(SELECT x.key as key
 FROM src x JOIN src y ON (x.key = y.key)
 WHERE x.key < 200 AND x.key > 180) yy
ON xx.key=yy.key;

SELECT xx.key, xx.value
FROM
src xx
LEFT SEMI JOIN
(SELECT x.key as key
 FROM src x JOIN src y ON (x.key = y.key)
 WHERE x.key < 200 AND x.key > 180) yy
ON xx.key=yy.key;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=false;
-- Tests in this file are used to make sure Correlation Optimizer
-- can correctly handle tables with partitions

CREATE TABLE part_table(key string, value string) PARTITIONED BY (partitionId int);
INSERT OVERWRITE TABLE part_table PARTITION (partitionId=1)
  SELECT key, value FROM src ORDER BY key, value LIMIT 100;
INSERT OVERWRITE TABLE part_table PARTITION (partitionId=2)
  SELECT key, value FROM src1 ORDER BY key, value;

set hive.optimize.correlation=false;
-- In this case, we should not do shared scan on part_table
-- because left and right tables of JOIN use different partitions
-- of part_table. With Correlation Optimizer we will generate
-- 1 MR job.
EXPLAIN
SELECT x.key AS key, count(1) AS cnt
FROM part_table x JOIN part_table y ON (x.key = y.key)
WHERE x.partitionId = 1 AND
      y.partitionId = 2
GROUP BY x.key;

SELECT x.key AS key, count(1) AS cnt
FROM part_table x JOIN part_table y ON (x.key = y.key)
WHERE x.partitionId = 1 AND
      y.partitionId = 2
GROUP BY x.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT x.key AS key, count(1) AS cnt
FROM part_table x JOIN part_table y ON (x.key = y.key)
WHERE x.partitionId = 1 AND
      y.partitionId = 2
GROUP BY x.key;

SELECT x.key AS key, count(1) AS cnt
FROM part_table x JOIN part_table y ON (x.key = y.key)
WHERE x.partitionId = 1 AND
      y.partitionId = 2
GROUP BY x.key;

set hive.optimize.correlation=false;
-- In this case, we should do shared scan on part_table
-- because left and right tables of JOIN use the same partition
-- of part_table. With Correlation Optimizer we will generate
-- 1 MR job.
EXPLAIN
SELECT x.key AS key, count(1) AS cnt
FROM part_table x JOIN part_table y ON (x.key = y.key)
WHERE x.partitionId = 2 AND
      y.partitionId = 2
GROUP BY x.key;

SELECT x.key AS key, count(1) AS cnt
FROM part_table x JOIN part_table y ON (x.key = y.key)
WHERE x.partitionId = 2 AND
      y.partitionId = 2
GROUP BY x.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT x.key AS key, count(1) AS cnt
FROM part_table x JOIN part_table y ON (x.key = y.key)
WHERE x.partitionId = 2 AND
      y.partitionId = 2
GROUP BY x.key;

SELECT x.key AS key, count(1) AS cnt
FROM part_table x JOIN part_table y ON (x.key = y.key)
WHERE x.partitionId = 2 AND
      y.partitionId = 2
GROUP BY x.key;
set hive.auto.convert.join=false;
set hive.optimize.correlation=true;
-- Currently, correlation optimizer does not support PTF operator
EXPLAIN SELECT xx.key, xx.cnt, yy.key, yy.cnt
FROM
(SELECT x.key as key, count(x.value) OVER (PARTITION BY x.key) AS cnt FROM src x) xx
JOIN
(SELECT y.key as key, count(y.value) OVER (PARTITION BY y.key) AS cnt FROM src1 y) yy
ON (xx.key=yy.key);
set hive.mapred.mode=nonstrict;
CREATE TABLE tmp(c1 INT, c2 INT, c3 STRING, c4 STRING);
INSERT OVERWRITE TABLE tmp
SELECT x.key, y.key, x.value, y.value FROM src x JOIN src y ON (x.key = y.key);

set hive.optimize.correlation=true;
-- The query in this file have operators with same set of keys
-- but having different sorting orders.
-- Correlation optimizer currently do not optimize this case.
-- This case will be optimized latter (need a follow-up jira).

EXPLAIN
SELECT xx.key1, xx.key2, yy.key1, yy.key2, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key1, x.c3 AS key2, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c3, x.c1) xx
JOIN
(SELECT x1.c1 AS key1, x1.c3 AS key2, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c3, x1.c1) yy
ON (xx.key1 = yy.key1 AND xx.key2 == yy.key2) ORDER BY xx.key1, xx.key2, yy.key1, yy.key2, xx.cnt, yy.cnt;

set hive.mapred.mode=nonstrict;
set hive.optimize.reducededuplication=true;
set hive.optimize.reducededuplication.min.reducer=1;
set hive.optimize.correlation=true;
-- This file is used to show plans of queries involving cluster by, distribute by,
-- order by, and sort by.
-- Right now, Correlation optimizer check the most restrictive condition
-- when determining if a ReduceSinkOperator is not necessary.
-- This condition is that two ReduceSinkOperators should have same sorting columns,
-- same partitioning columns, same sorting orders and no conflict on the numbers of reducers.

-- Distribute by will not be optimized because distribute by does not introduce
-- sorting columns.
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x DISTRIBUTE BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y DISTRIBUTE BY key) yy
ON (xx.key=yy.key);

-- Sort by will not be optimized because sort by does not introduce partitioning columns
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x SORT BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y SORT BY key) yy
ON (xx.key=yy.key);

set hive.optimize.correlation=false;
-- Distribute by and sort by on the same key(s) should be optimized
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x DISTRIBUTE BY key SORT BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y DISTRIBUTE BY key SORT BY key) yy
ON (xx.key=yy.key);

SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x DISTRIBUTE BY key SORT BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y DISTRIBUTE BY key SORT BY key) yy
ON (xx.key=yy.key);

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x DISTRIBUTE BY key SORT BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y DISTRIBUTE BY key SORT BY key) yy
ON (xx.key=yy.key);

SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x DISTRIBUTE BY key SORT BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y DISTRIBUTE BY key SORT BY key) yy
ON (xx.key=yy.key);

set hive.optimize.correlation=true;
-- Because for join we use ascending order, if sort by uses descending order,
-- this query will not be optimized
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x DISTRIBUTE BY key SORT BY key DESC) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y DISTRIBUTE BY key SORT BY key DESC) yy
ON (xx.key=yy.key);

-- Even if hive.optimize.reducededuplication.min.reducer=1, order by will not be optimized
-- because order by does not introduce partitioning columns
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x ORDER BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y ORDER BY key) yy
ON (xx.key=yy.key);

set hive.optimize.correlation=false;
-- Cluster by will be optimized
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x Cluster BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y Cluster BY key) yy
ON (xx.key=yy.key);

SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x Cluster BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y Cluster BY key) yy
ON (xx.key=yy.key);

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x Cluster BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y Cluster BY key) yy
ON (xx.key=yy.key);

SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x Cluster BY key) xx
JOIN
(SELECT y.key as key, y.value as value FROM src1 y Cluster BY key) yy
ON (xx.key=yy.key);

set hive.optimize.correlation=false;
-- If hive.optimize.reducededuplication.min.reducer=1,
-- group by and then order by should be optimized
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x CLUSTER BY key) xx
JOIN
(SELECT y.key as key, count(*) as value FROM src1 y GROUP BY y.key ORDER BY key) yy
ON (xx.key=yy.key);

SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x CLUSTER BY key) xx
JOIN
(SELECT y.key as key, count(*) as value FROM src1 y GROUP BY y.key ORDER BY key) yy
ON (xx.key=yy.key);

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x CLUSTER BY key) xx
JOIN
(SELECT y.key as key, count(*) as value FROM src1 y GROUP BY y.key ORDER BY key) yy
ON (xx.key=yy.key);

SELECT xx.key, xx.value, yy.key, yy.value
FROM
(SELECT x.key as key, x.value as value FROM src x CLUSTER BY key) xx
JOIN
(SELECT y.key as key, count(*) as value FROM src1 y GROUP BY y.key ORDER BY key) yy
ON (xx.key=yy.key);
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=false;
set hive.optimize.correlation=false;

-- SORT_QUERY_RESULTS

-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery xx and xx join yy.
EXPLAIN
SELECT xx.key, xx.cnt, yy.key
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN src yy
ON xx.key=yy.key ORDER BY xx.key, xx.cnt, yy.key;

SELECT xx.key, xx.cnt, yy.key
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN src yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
set hive.join.emit.interval=1;
EXPLAIN
SELECT xx.key, xx.cnt, yy.key
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN src yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN src yy
ON xx.key=yy.key;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- In this query, subquery a and b both have a GroupByOperator and the a and b will be
-- joined. The key of JoinOperator is the same with both keys of GroupByOperators in subquery
-- a and b. When Correlation Optimizer is turned off, we have four MR jobs.
-- When Correlation Optimizer is turned on, 2 MR jobs will be generated.
-- The first job will evaluate subquery tmp (including subquery a, b, and the JoinOperator on a
-- and b).
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=false;
-- Left Outer Join should be handled.
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      LEFT OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      LEFT OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      LEFT OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      LEFT OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=false;
-- Right Outer Join should be handled.
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      RIGHT OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      RIGHT OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      RIGHT OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      RIGHT OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=false;
-- Full Outer Join should be handled.
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      FULL OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      FULL OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      FULL OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.cnt AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      FULL OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=false;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT a.key AS key, count(1) AS cnt
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      FULL OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)
      GROUP BY a.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT a.key AS key, count(1) AS cnt
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      FULL OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)
      GROUP BY a.key) tmp;

set hive.optimize.correlation=true;
-- After FULL OUTER JOIN, keys with null values are not grouped, right now,
-- we have to generate 2 MR jobs for tmp, 1 MR job for a join b and another for the
-- GroupByOperator on key.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT a.key AS key, count(1) AS cnt
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      FULL OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)
      GROUP BY a.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT a.key AS key, count(1) AS cnt
      FROM (SELECT x.key as key, count(x.value) AS cnt FROM src x group by x.key) a
      FULL OUTER JOIN (SELECT y.key as key, count(y.value) AS cnt FROM src1 y group by y.key) b
      ON (a.key = b.key)
      GROUP BY a.key) tmp;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, we need 4 MR jobs.
-- When Correlation Optimizer is turned on, the subquery of tmp will be evaluated in
-- a single MR job (including the subquery a, the subquery b, and a join b). So, we
-- will have 2 MR jobs.
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.val AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key AS key, x.value AS val FROM src1 x JOIN src y ON (x.key = y.key)) a
      JOIN (SELECT z.key AS key, count(z.value) AS cnt FROM src1 z group by z.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.val AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key AS key, x.value AS val FROM src1 x JOIN src y ON (x.key = y.key)) a
      JOIN (SELECT z.key AS key, count(z.value) AS cnt FROM src1 z group by z.key) b
      ON (a.key = b.key)) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.val AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key AS key, x.value AS val FROM src1 x JOIN src y ON (x.key = y.key)) a
      JOIN (SELECT z.key AS key, count(z.value) AS cnt FROM src1 z group by z.key) b
      ON (a.key = b.key)) tmp;

SELECT SUM(HASH(key1)), SUM(HASH(cnt1)), SUM(HASH(key2)), SUM(HASH(cnt2))
FROM (SELECT a.key AS key1, a.val AS cnt1, b.key AS key2, b.cnt AS cnt2
      FROM (SELECT x.key AS key, x.value AS val FROM src1 x JOIN src y ON (x.key = y.key)) a
      JOIN (SELECT z.key AS key, count(z.value) AS cnt FROM src1 z group by z.key) b
      ON (a.key = b.key)) tmp;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 5 MR jobs will be generated.
-- When Correlation Optimizer is turned on, the subquery tmp will be evalauted
-- in a single MR job (including the subquery b, the subquery d, and b join d).
-- At the reduce side of the MR job evaluating tmp, two operation paths
-- (for subquery b and d) have different depths. The path starting from subquery b
-- is JOIN->GBY->JOIN, which has a depth of 3. While, the path starting from subquery d
-- is JOIN->JOIN. We should be able to handle this case.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT b.key AS key, b.cnt AS cnt, d.value AS value
      FROM (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) b
      JOIN (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) d
      ON b.key = d.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT b.key AS key, b.cnt AS cnt, d.value AS value
      FROM (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) b
      JOIN (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) d
      ON b.key = d.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT b.key AS key, b.cnt AS cnt, d.value AS value
      FROM (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) b
      JOIN (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) d
      ON b.key = d.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT b.key AS key, b.cnt AS cnt, d.value AS value
      FROM (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) b
      JOIN (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) d
      ON b.key = d.key) tmp;

set hive.optimize.correlation=true;
set hive.auto.convert.join=true;
-- Enable hive.auto.convert.join.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT b.key AS key, b.cnt AS cnt, d.value AS value
      FROM (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) b
      JOIN (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) d
      ON b.key = d.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT b.key AS key, b.cnt AS cnt, d.value AS value
      FROM (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) b
      JOIN (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) d
      ON b.key = d.key) tmp;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT d.key AS key, d.cnt AS cnt, b.value as value
      FROM (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) b
      JOIN (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) d
      ON b.key = d.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT d.key AS key, d.cnt AS cnt, b.value as value
      FROM (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) b
      JOIN (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) d
      ON b.key = d.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT d.key AS key, d.cnt AS cnt, b.value as value
      FROM (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) b
      JOIN (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) d
      ON b.key = d.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT d.key AS key, d.cnt AS cnt, b.value as value
      FROM (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) b
      JOIN (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) d
      ON b.key = d.key) tmp;

set hive.optimize.correlation=true;
set hive.auto.convert.join=true;
-- Enable hive.auto.convert.join.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT d.key AS key, d.cnt AS cnt, b.value as value
      FROM (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) b
      JOIN (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) d
      ON b.key = d.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt)), SUM(HASH(tmp.value))
FROM (SELECT d.key AS key, d.cnt AS cnt, b.value as value
      FROM (SELECT x.key, x.value FROM src1 x JOIN src y ON (x.key = y.key)) b
      JOIN (SELECT x.key, count(1) AS cnt FROM src1 x JOIN src y ON (x.key = y.key) group by x.key) d
      ON b.key = d.key) tmp;
set hive.mapred.mode=nonstrict;
CREATE TABLE T1(key INT, val STRING);
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;
CREATE TABLE T2(key INT, val STRING);
LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;
CREATE TABLE T3(key INT, val STRING);
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T3;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, this query will be evaluated
-- by 3 MR jobs.
-- When Correlation Optimizer is turned on, this query will be evaluated by
-- 2 MR jobs. The subquery tmp will be evaluated in a single MR job.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x JOIN T1 y ON (x.key = y.key) JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x JOIN T1 y ON (x.key = y.key) JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x JOIN T1 y ON (x.key = y.key) JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x JOIN T1 y ON (x.key = y.key) JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

set hive.optimize.correlation=true;
set hive.auto.convert.join=true;
-- Enable hive.auto.convert.join.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x JOIN T1 y ON (x.key = y.key) JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x JOIN T1 y ON (x.key = y.key) JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- This case should be optimized, since the key of GroupByOperator is from the leftmost table
-- of a chain of LEFT OUTER JOINs.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM T2 x LEFT OUTER JOIN T1 y ON (x.key = y.key) LEFT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM T2 x LEFT OUTER JOIN T1 y ON (x.key = y.key) LEFT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM T2 x LEFT OUTER JOIN T1 y ON (x.key = y.key) LEFT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY x.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM T2 x LEFT OUTER JOIN T1 y ON (x.key = y.key) LEFT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
-- This query will not be optimized by correlation optimizer because
-- GroupByOperator uses y.key (a right table of a left outer join)
-- as the key.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x LEFT OUTER JOIN T1 y ON (x.key = y.key) LEFT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x LEFT OUTER JOIN T1 y ON (x.key = y.key) LEFT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

set hive.optimize.correlation=false;
-- This case should be optimized, since the key of GroupByOperator is from the rightmost table
-- of a chain of RIGHT OUTER JOINs.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT z.key AS key, count(1) AS cnt
      FROM T2 x RIGHT OUTER JOIN T1 y ON (x.key = y.key) RIGHT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY z.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT z.key AS key, count(1) AS cnt
      FROM T2 x RIGHT OUTER JOIN T1 y ON (x.key = y.key) RIGHT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY z.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT z.key AS key, count(1) AS cnt
      FROM T2 x RIGHT OUTER JOIN T1 y ON (x.key = y.key) RIGHT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY z.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT z.key AS key, count(1) AS cnt
      FROM T2 x RIGHT OUTER JOIN T1 y ON (x.key = y.key) RIGHT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY z.key) tmp;

set hive.optimize.correlation=true;
-- This query will not be optimized by correlation optimizer because
-- GroupByOperator uses y.key (a left table of a right outer join)
-- as the key.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x RIGHT OUTER JOIN T1 y ON (x.key = y.key) RIGHT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x RIGHT OUTER JOIN T1 y ON (x.key = y.key) RIGHT OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

set hive.optimize.correlation=false;
-- This case should not be optimized because afer the FULL OUTER JOIN, rows with null keys
-- are not grouped.
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x FULL OUTER JOIN T1 y ON (x.key = y.key) FULL OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x FULL OUTER JOIN T1 y ON (x.key = y.key) FULL OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

set hive.optimize.correlation=true;
EXPLAIN
SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x FULL OUTER JOIN T1 y ON (x.key = y.key) FULL OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;

SELECT SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (SELECT y.key AS key, count(1) AS cnt
      FROM T2 x FULL OUTER JOIN T1 y ON (x.key = y.key) FULL OUTER JOIN T3 z ON (y.key = z.key)
      GROUP BY y.key) tmp;
set hive.mapred.mode=nonstrict;
CREATE TABLE T1(key INT, val STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE T1;
CREATE TABLE T2(key INT, val STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv2.txt' INTO TABLE T2;
CREATE TABLE T3(key INT, val STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv3.txt' INTO TABLE T3;
CREATE TABLE T4(key INT, val STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv5.txt' INTO TABLE T4;

CREATE TABLE dest_co1(key INT, val STRING);
CREATE TABLE dest_co2(key INT, val STRING);
CREATE TABLE dest_co3(key INT, val STRING);

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 3 MR jobs are needed.
-- When Correlation Optimizer is turned on, only a single MR job is needed.
EXPLAIN
INSERT OVERWRITE TABLE dest_co1
SELECT b.key, d.val
FROM
(SELECT x.key, x.val FROM T1 x JOIN T2 y ON (x.key = y.key)) b
JOIN
(SELECT m.key, n.val FROM T3 m JOIN T4 n ON (m.key = n.key)) d
ON b.key = d.key;

INSERT OVERWRITE TABLE dest_co1
SELECT b.key, d.val
FROM
(SELECT x.key, x.val FROM T1 x JOIN T2 y ON (x.key = y.key)) b
JOIN
(SELECT m.key, n.val FROM T3 m JOIN T4 n ON (m.key = n.key)) d
ON b.key = d.key;

set hive.optimize.correlation=true;
EXPLAIN
INSERT OVERWRITE TABLE dest_co2
SELECT b.key, d.val
FROM
(SELECT x.key, x.val FROM T1 x JOIN T2 y ON (x.key = y.key)) b
JOIN
(SELECT m.key, n.val FROM T3 m JOIN T4 n ON (m.key = n.key)) d
ON b.key = d.key;

INSERT OVERWRITE TABLE dest_co2
SELECT b.key, d.val
FROM
(SELECT x.key, x.val FROM T1 x JOIN T2 y ON (x.key = y.key)) b
JOIN
(SELECT m.key, n.val FROM T3 m JOIN T4 n ON (m.key = n.key)) d
ON b.key = d.key;

set hive.optimize.correlation=true;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask.size=10000000000;
-- Enable hive.auto.convert.join.
EXPLAIN
INSERT OVERWRITE TABLE dest_co3
SELECT b.key, d.val
FROM
(SELECT x.key, x.val FROM T1 x JOIN T2 y ON (x.key = y.key)) b
JOIN
(SELECT m.key, n.val FROM T3 m JOIN T4 n ON (m.key = n.key)) d
ON b.key = d.key;

INSERT OVERWRITE TABLE dest_co3
SELECT b.key, d.val
FROM
(SELECT x.key, x.val FROM T1 x JOIN T2 y ON (x.key = y.key)) b
JOIN
(SELECT m.key, n.val FROM T3 m JOIN T4 n ON (m.key = n.key)) d
ON b.key = d.key;

-- dest_co1, dest_co2 and dest_co3 should be same
-- SELECT * FROM dest_co1 x ORDER BY x.key, x.val;
-- SELECT * FROM dest_co2 x ORDER BY x.key, x.val;
SELECT SUM(HASH(key)), SUM(HASH(val)) FROM dest_co1;
SELECT SUM(HASH(key)), SUM(HASH(val)) FROM dest_co2;
SELECT SUM(HASH(key)), SUM(HASH(val)) FROM dest_co3;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=false;
set hive.optimize.correlation=false;

-- SORT_QUERY_RESULTS

-- When Correlation Optimizer is turned off, 6 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery xx, subquery yy, and xx join yy.
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
set hive.auto.convert.join=true;
-- Enable hive.auto.convert.join.
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 3 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery yy and xx join yy.
EXPLAIN
SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x GROUP BY x.key) yy
ON xx.key=yy.key;

SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x GROUP BY x.key) yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x GROUP BY x.key) yy
ON xx.key=yy.key;

SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x GROUP BY x.key) yy
ON xx.key=yy.key;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery yy and xx join yy.
EXPLAIN
SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery xx and xx join yy.
EXPLAIN
SELECT xx.key, xx.cnt, yy.key
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN src yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN src yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.cnt, yy.key
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN src yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN src yy
ON xx.key=yy.key;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery xx and xx join yy join zz.
EXPLAIN
SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN src zz ON xx.key=zz.key
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON zz.key=yy.key;

SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN src zz ON xx.key=zz.key
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON zz.key=yy.key;

set hive.optimize.correlation=true;
-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery yy and xx join yy join zz.
EXPLAIN
SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN src zz ON xx.key=zz.key
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON zz.key=yy.key;

SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN src zz ON xx.key=zz.key
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON zz.key=yy.key;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 4 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery yy and xx join yy join zz.
EXPLAIN
SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key JOIN src zz
ON yy.key=zz.key;

SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key JOIN src zz
ON yy.key=zz.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key JOIN src zz
ON yy.key=zz.key;

SELECT xx.key, yy.key, yy.cnt
FROM src1 xx
JOIN
(SELECT x.key as key, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key) yy
ON xx.key=yy.key JOIN src zz
ON yy.key=zz.key;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 6 MR jobs are needed.
-- When Correlation Optimizer is turned on, 2 MR jobs are needed.
-- The first job will evaluate subquery tmp and tmp join z.
EXPLAIN
SELECT tmp.key, tmp.sum1, tmp.sum2, z.key, z.value
FROM
(SELECT xx.key as key, sum(xx.cnt) as sum1, sum(yy.cnt) as sum2
 FROM (SELECT x.key as key, count(*) AS cnt FROM src x group by x.key) xx
 JOIN (SELECT y.key as key, count(*) AS cnt FROM src1 y group by y.key) yy
 ON (xx.key=yy.key) GROUP BY xx.key) tmp
JOIN src z ON tmp.key=z.key;

SELECT tmp.key, tmp.sum1, tmp.sum2, z.key, z.value
FROM
(SELECT xx.key as key, sum(xx.cnt) as sum1, sum(yy.cnt) as sum2
 FROM (SELECT x.key as key, count(*) AS cnt FROM src x group by x.key) xx
 JOIN (SELECT y.key as key, count(*) AS cnt FROM src1 y group by y.key) yy
 ON (xx.key=yy.key) GROUP BY xx.key) tmp
JOIN src z ON tmp.key=z.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT tmp.key, tmp.sum1, tmp.sum2, z.key, z.value
FROM
(SELECT xx.key as key, sum(xx.cnt) as sum1, sum(yy.cnt) as sum2
 FROM (SELECT x.key as key, count(*) AS cnt FROM src x group by x.key) xx
 JOIN (SELECT y.key as key, count(*) AS cnt FROM src1 y group by y.key) yy
 ON (xx.key=yy.key) GROUP BY xx.key) tmp
JOIN src z ON tmp.key=z.key;

SELECT tmp.key, tmp.sum1, tmp.sum2, z.key, z.value
FROM
(SELECT xx.key as key, sum(xx.cnt) as sum1, sum(yy.cnt) as sum2
 FROM (SELECT x.key as key, count(*) AS cnt FROM src x group by x.key) xx
 JOIN (SELECT y.key as key, count(*) AS cnt FROM src1 y group by y.key) yy
 ON (xx.key=yy.key) GROUP BY xx.key) tmp
JOIN src z ON tmp.key=z.key;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is turned off, 6 MR jobs are needed.
-- When Correlation Optimizer is turned on, 4 MR jobs are needed.
-- 2 MR jobs are used to evaluate yy, 1 MR is used to evaluate xx and xx join yy.
-- The last MR is used for ordering.
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.value, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, x.value as value, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key, x.value) yy
ON xx.key=yy.key ORDER BY xx.key, xx.cnt, yy.key, yy.value, yy.cnt;

SELECT xx.key, xx.cnt, yy.key, yy.value, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, x.value as value, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key, x.value) yy
ON xx.key=yy.key ORDER BY xx.key, xx.cnt, yy.key, yy.value, yy.cnt;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.value, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, x.value as value, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key, x.value) yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.value, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, x.value as value, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key, x.value) yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
set hive.auto.convert.join=true;
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.value, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, x.value as value, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key, x.value) yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.value, yy.cnt
FROM
(SELECT x.key as key, count(1) as cnt FROM src1 x JOIN src1 y ON (x.key = y.key) group by x.key) xx
JOIN
(SELECT x.key as key, x.value as value, count(1) as cnt FROM src x JOIN src y ON (x.key = y.key) group by x.key, x.value) yy
ON xx.key=yy.key;

set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.optimize.correlation=false;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.value
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src x JOIN src1 y ON (x.key = y.key)
      GROUP BY x.key) xx
JOIN src1 yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.value
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src x JOIN src1 y ON (x.key = y.key)
      GROUP BY x.key) xx
JOIN src1 yy
ON xx.key=yy.key;


set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.value
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src x JOIN src1 y ON (x.key = y.key)
      GROUP BY x.key) xx
JOIN src1 yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.value
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src x JOIN src1 y ON (x.key = y.key)
      GROUP BY x.key) xx
JOIN src1 yy
ON xx.key=yy.key;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000000;

set hive.optimize.correlation=false;
-- Without correlation optimizer, we will have 3 MR jobs.
-- The first one is a MapJoin and Aggregation (in the Reduce Phase).
-- The second one is another MapJoin. The third one is for ordering.
-- With the correlation optimizer, right now, we have
-- 1 MR jobs, evaluatinf the sub-query xx and the join of
-- xx and yy.
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.value
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src x JOIN src1 y ON (x.key = y.key)
      GROUP BY x.key) xx
JOIN src1 yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.value
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src x JOIN src1 y ON (x.key = y.key)
      GROUP BY x.key) xx
JOIN src1 yy
ON xx.key=yy.key;

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key, xx.cnt, yy.key, yy.value
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src x JOIN src1 y ON (x.key = y.key)
      GROUP BY x.key) xx
JOIN src1 yy
ON xx.key=yy.key;

SELECT xx.key, xx.cnt, yy.key, yy.value
FROM (SELECT x.key AS key, count(1) AS cnt
      FROM src x JOIN src1 y ON (x.key = y.key)
      GROUP BY x.key) xx
JOIN src1 yy
ON xx.key=yy.key;

set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=false;
set hive.optimize.correlation=false;

-- SORT_QUERY_RESULTS

-- When the Correlation Optimizer is turned off, this query will be evaluated by
-- 4 MR jobs.
-- When the Correlation Optimizer is turned on, because both inputs of the
-- UnionOperator are correlated, we can use 2 MR jobs to evaluate this query.
-- The first MR job will evaluate subquery subq1 and subq1 join x. The second
-- MR is for ordering.
EXPLAIN
SELECT x.key, x.value, subq1.cnt
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.key as key, count(1) as cnt from src x1 where x1.key > 100 group by x1.key
) subq1
JOIN src1 x ON (x.key = subq1.key);

SELECT x.key, x.value, subq1.cnt
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.key as key, count(1) as cnt from src x1 where x1.key > 100 group by x1.key
) subq1
JOIN src1 x ON (x.key = subq1.key);

set hive.optimize.correlation=true;
EXPLAIN
SELECT x.key, x.value, subq1.cnt
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.key as key, count(1) as cnt from src x1 where x1.key > 100 group by x1.key
) subq1
JOIN src1 x ON (x.key = subq1.key);

SELECT x.key, x.value, subq1.cnt
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.key as key, count(1) as cnt from src x1 where x1.key > 100 group by x1.key
) subq1
JOIN src1 x ON (x.key = subq1.key);

set hive.optimize.correlation=false;
-- When the Correlation Optimizer is turned off, this query will be evaluated by
-- 4 MR jobs.
-- When the Correlation Optimizer is turned on, because both inputs of the
-- UnionOperator are correlated, we can use 2 MR jobs to evaluate this query.
-- The first MR job will evaluate subquery subq1 and subq1 join x. The second
-- MR is for ordering.
EXPLAIN
SELECT subq1.key, subq1.cnt, x.key, x.value
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.value as key, count(1) as cnt from src1 x1 where x1.key > 100 group by x1.value
) subq1
LEFT OUTER JOIN src1 x ON (x.key = subq1.key);

SELECT subq1.key, subq1.cnt, x.key, x.value
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.value as key, count(1) as cnt from src1 x1 where x1.key > 100 group by x1.value
) subq1
LEFT OUTER JOIN src1 x ON (x.key = subq1.key);

set hive.optimize.correlation=true;
EXPLAIN
SELECT subq1.key, subq1.cnt, x.key, x.value
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.value as key, count(1) as cnt from src1 x1 where x1.key > 100 group by x1.value
) subq1
LEFT OUTER JOIN src1 x ON (x.key = subq1.key);

SELECT subq1.key, subq1.cnt, x.key, x.value
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.value as key, count(1) as cnt from src1 x1 where x1.key > 100 group by x1.value
) subq1
LEFT OUTER JOIN src1 x ON (x.key = subq1.key);

set hive.optimize.correlation=true;
-- When the Correlation Optimizer is turned on, because a input of UnionOperator is
-- not correlated, we cannot handle this case right now. So, this query will not be
-- optimized.
EXPLAIN
SELECT x.key, x.value, subq1.cnt
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.key as key, count(1) as cnt from src x1 where x1.key > 100 group by x1.key, x1.value
) subq1
JOIN src1 x ON (x.key = subq1.key);

set hive.optimize.correlation=true;
-- When the Correlation Optimizer is turned on, because a input of UnionOperator is
-- not correlated, we cannot handle this case right now. So, this query will not be
-- optimized.
EXPLAIN
SELECT subq1.key, subq1.value, x.key, x.value
FROM
( SELECT cast(x.key as INT) as key, count(1) as value from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT count(1) as key, cast(x1.key as INT) as value from src x1 where x1.key > 100 group by x1.key
) subq1
FULL OUTER JOIN src1 x ON (x.key = subq1.key);
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE tmp(c1 INT, c2 INT, c3 STRING, c4 STRING);

set hive.auto.convert.join=false;

INSERT OVERWRITE TABLE tmp
SELECT x.key, y.key, x.value, y.value FROM src x JOIN src y ON (x.key = y.key);

set hive.optimize.correlation=false;
EXPLAIN
SELECT xx.key, yy.key, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c1) xx
JOIN
(SELECT x1.c2 AS key, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c2) yy
ON (xx.key = yy.key);

SELECT xx.key, yy.key, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c1) xx
JOIN
(SELECT x1.c2 AS key, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c2) yy
ON (xx.key = yy.key);

set hive.optimize.correlation=true;
-- The merged table scan should be able to load both c1 and c2
EXPLAIN
SELECT xx.key, yy.key, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c1) xx
JOIN
(SELECT x1.c2 AS key, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c2) yy
ON (xx.key = yy.key);

SELECT xx.key, yy.key, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c1) xx
JOIN
(SELECT x1.c2 AS key, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c2) yy
ON (xx.key = yy.key);

set hive.optimize.correlation=false;
EXPLAIN
SELECT xx.key1, xx.key2, yy.key1, yy.key2, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key1, x.c3 AS key2, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c1, x.c3) xx
JOIN
(SELECT x1.c1 AS key1, x1.c3 AS key2, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c1, x1.c3) yy
ON (xx.key1 = yy.key1 AND xx.key2 == yy.key2);

SELECT xx.key1, xx.key2, yy.key1, yy.key2, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key1, x.c3 AS key2, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c1, x.c3) xx
JOIN
(SELECT x1.c1 AS key1, x1.c3 AS key2, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c1, x1.c3) yy
ON (xx.key1 = yy.key1 AND xx.key2 == yy.key2);

set hive.optimize.correlation=true;
EXPLAIN
SELECT xx.key1, xx.key2, yy.key1, yy.key2, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key1, x.c3 AS key2, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c1, x.c3) xx
JOIN
(SELECT x1.c1 AS key1, x1.c3 AS key2, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c1, x1.c3) yy
ON (xx.key1 = yy.key1 AND xx.key2 == yy.key2);

SELECT xx.key1, xx.key2, yy.key1, yy.key2, xx.cnt, yy.cnt
FROM
(SELECT x.c1 AS key1, x.c3 AS key2, count(1) AS cnt FROM tmp x WHERE x.c1 < 120 GROUP BY x.c1, x.c3) xx
JOIN
(SELECT x1.c1 AS key1, x1.c3 AS key2, count(1) AS cnt FROM tmp x1 WHERE x1.c2 > 100 GROUP BY x1.c1, x1.c3) yy
ON (xx.key1 = yy.key1 AND xx.key2 == yy.key2);
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS
create table abcd (a int, b int, c int, d int);
LOAD DATA LOCAL INPATH '../../data/files/in4.txt' INTO TABLE abcd;

select * from abcd;
set hive.map.aggr=true;
explain select a, count(distinct b), count(distinct c), sum(d) from abcd group by a;
select a, count(distinct b), count(distinct c), sum(d) from abcd group by a;

explain select count(1), count(*), count(a), count(b), count(c), count(d), count(distinct a), count(distinct b), count(distinct c), count(distinct d), count(distinct a,b), count(distinct b,c), count(distinct c,d), count(distinct a,d), count(distinct a,c), count(distinct b,d), count(distinct a,b,c), count(distinct b,c,d), count(distinct a,c,d), count(distinct a,b,d), count(distinct a,b,c,d) from abcd;
select count(1), count(*), count(a), count(b), count(c), count(d), count(distinct a), count(distinct b), count(distinct c), count(distinct d), count(distinct a,b), count(distinct b,c), count(distinct c,d), count(distinct a,d), count(distinct a,c), count(distinct b,d), count(distinct a,b,c), count(distinct b,c,d), count(distinct a,c,d), count(distinct a,b,d), count(distinct a,b,c,d) from abcd;

set hive.map.aggr=false;
explain select a, count(distinct b), count(distinct c), sum(d) from abcd group by a;
select a, count(distinct b), count(distinct c), sum(d) from abcd group by a;

explain select count(1), count(*), count(a), count(b), count(c), count(d), count(distinct a), count(distinct b), count(distinct c), count(distinct d), count(distinct a,b), count(distinct b,c), count(distinct c,d), count(distinct a,d), count(distinct a,c), count(distinct b,d), count(distinct a,b,c), count(distinct b,c,d), count(distinct a,c,d), count(distinct a,b,d), count(distinct a,b,c,d) from abcd;
select count(1), count(*), count(a), count(b), count(c), count(d), count(distinct a), count(distinct b), count(distinct c), count(distinct d), count(distinct a,b), count(distinct b,c), count(distinct c,d), count(distinct a,d), count(distinct a,c), count(distinct b,d), count(distinct a,b,c), count(distinct b,c,d), count(distinct a,c,d), count(distinct a,b,d), count(distinct a,b,c,d) from abcd;
create table src_six_columns (k1 string, v1 string, k2 string, v2 string, k3 string, v3 string) stored as rcfile;
insert overwrite table src_six_columns select value, value, key, value, value, value from src;
create table src_two_columns (k1 string, v1 string) stored as rcfile;
insert overwrite table src_two_columns select key, value from src;
SELECT /*+ MAPJOIN(six) */ six.*, two.k1 from src_six_columns six join src_two_columns two on (six.k3=two.k1);

SELECT /*+ MAPJOIN(two) */ two.*, six.k3 from src_six_columns six join src_two_columns two on (six.k3=two.k1);
set hive.mapred.mode=nonstrict;
explain
select key,value,'hello' as ds, 'world' as hr from srcpart where hr=11 order by 1 limit 1;
select key,value,'hello' as ds, 'world' as hr from srcpart where hr=11 order by 1 limit 1;
set hive.exec.dynamic.partition.mode=nonstrict;
create table testpartbucket (key string, value string) partitioned by (ds string, hr string) clustered by(key) sorted by(key) into 2 buckets;
explain
insert overwrite table testpartbucket partition(ds,hr) select key,value,'hello' as ds, 'world' as hr from srcpart where hr=11;
insert overwrite table testpartbucket partition(ds,hr) select key,value,'hello' as ds, 'world' as hr from srcpart where hr=11;
select * from testpartbucket limit 3;
drop table testpartbucket;
reset hive.exec.dynamic.partition.mode;
set fs.default.name=invalidscheme:///;

CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE;
DESCRIBE table1;
DESCRIBE EXTENDED table1;

CREATE TABLE IF NOT EXISTS table1 (a STRING, b STRING) STORED AS TEXTFILE;

CREATE TABLE IF NOT EXISTS table2 (a STRING, b INT) STORED AS TEXTFILE;
DESCRIBE table2;
DESCRIBE EXTENDED table2;

CREATE TABLE table3 (a STRING, b STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;
DESCRIBE table3;
DESCRIBE EXTENDED table3;

CREATE TABLE table4 (a STRING, b STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS SEQUENCEFILE;
DESCRIBE table4;
DESCRIBE EXTENDED table4;

CREATE TABLE table5 (a STRING, b STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS RCFILE;
DESCRIBE table5;
DESCRIBE EXTENDED table5;
-- Test stored as directories
-- it covers a few cases

-- 1. create a table with stored as directories
CREATE TABLE  if not exists stored_as_dirs_multiple (col1 STRING, col2 int, col3 STRING)
SKEWED BY (col1, col2) ON (('s1',1), ('s3',3), ('s13',13), ('s78',78))  stored as DIRECTORIES;
describe formatted stored_as_dirs_multiple;

-- 2. turn off stored as directories but table is still a skewed table
alter table stored_as_dirs_multiple not stored as DIRECTORIES;
describe formatted stored_as_dirs_multiple;

-- 3. turn off skewed
alter table stored_as_dirs_multiple not skewed;
describe formatted stored_as_dirs_multiple;

-- 4. alter a table to stored as directories
CREATE TABLE stored_as_dirs_single (key STRING, value STRING);
alter table stored_as_dirs_single SKEWED BY (key) ON ('1','5','6')
stored as DIRECTORIES;
describe formatted stored_as_dirs_single;

-- 5. turn off skewed should turn off stored as directories too
alter table stored_as_dirs_single not skewed;
describe formatted stored_as_dirs_single;

-- 6. turn on stored as directories again
alter table stored_as_dirs_single SKEWED BY (key) ON ('1','5','6')
stored as DIRECTORIES;
describe formatted stored_as_dirs_single;

-- 7. create table like
create table stored_as_dirs_single_like like stored_as_dirs_single;
describe formatted stored_as_dirs_single_like;

-- cleanup
drop table stored_as_dirs_single;
drop table stored_as_dirs_multiple;
DROP VIEW big_view;

-- Define a view with long SQL text to test metastore and other limits.

CREATE VIEW big_view AS SELECT
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa' AS a,
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',
'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
FROM src;

SELECT a FROM big_view
LIMIT 1;

DROP VIEW big_view;
set hive.table.parameters.default=p1=v1,P2=v21=v22=v23;
CREATE TABLE table_p1 (a STRING);
DESC EXTENDED table_p1;

set hive.table.parameters.default=p3=v3;
CREATE TABLE table_p2 LIKE table_p1;
DESC EXTENDED table_p2;

CREATE TABLE table_p3 AS SELECT * FROM table_p1;
DESC EXTENDED table_p3;
CREATE TABLE table1 (a STRING, b STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' ESCAPED BY '\\'
STORED AS TEXTFILE;

DESCRIBE table1;
DESCRIBE EXTENDED table1;

INSERT OVERWRITE TABLE table1 SELECT key, '\\\t\\' FROM src WHERE key = 86;

SELECT * FROM table1;

-- qtest_get_java_boolean should already be created during test initialization
select qtest_get_java_boolean('true'), qtest_get_java_boolean('false') from src limit 1;

describe function extended qtest_get_java_boolean;

create database mydb;
create function mydb.func1 as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper';

show functions mydb.func1;

describe function extended mydb.func1;


select mydb.func1('abc') from src limit 1;

drop function mydb.func1;

-- function should now be gone
show functions mydb.func1;

-- To test function name resolution
create function mydb.qtest_get_java_boolean as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper';

use default;
-- unqualified function should resolve to one in default db
select qtest_get_java_boolean('abc'), default.qtest_get_java_boolean('abc'), mydb.qtest_get_java_boolean('abc') from default.src limit 1;

use mydb;
-- unqualified function should resolve to one in mydb db
select qtest_get_java_boolean('abc'), default.qtest_get_java_boolean('abc'), mydb.qtest_get_java_boolean('abc') from default.src limit 1;

drop function mydb.qtest_get_java_boolean;

drop database mydb cascade;
create function default.badfunc as 'my.nonexistent.class';
create function nonexistentdb.badfunc as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFUpper';
create function default.badfunc as 'java.lang.String';
set hive.mapred.mode=nonstrict;
EXPLAIN
CREATE TEMPORARY FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage';

CREATE TEMPORARY FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage';

EXPLAIN
SELECT
    test_avg(1),
    test_avg(substr(value,5))
FROM src;

SELECT
    test_avg(1),
    test_avg(substr(value,5))
FROM src;

DROP TEMPORARY FUNCTIOn test_avg;
EXPLAIN
CREATE TEMPORARY FUNCTION test_translate AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate';

CREATE TEMPORARY FUNCTION test_translate AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate';

CREATE TABLE dest1(c1 STRING, c2 STRING, c3 STRING, c4 STRING, c5 STRING, c6 STRING, c7 STRING);

FROM src
INSERT OVERWRITE TABLE dest1
SELECT
    test_translate('abc', 'a', 'b'),
    test_translate('abc', 'ab', 'bc'),
    test_translate(NULL, 'a', 'b'),
    test_translate('a', NULL, 'b'),
    test_translate('a', 'a', NULL),
    test_translate('abc', 'ab', 'b'),
    test_translate('abc', 'a', 'ab');

SELECT dest1.* FROM dest1 LIMIT 1;

DROP TEMPORARY FUNCTION test_translate;


CREATE TABLE table_test_output_format(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';

FROM src
INSERT OVERWRITE TABLE table_test_output_format SELECT src.key, src.value LIMIT 10;
describe table_test_output_format;



CREATE TABLE table_test_output_format_sequencefile(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileOutputFormat';

FROM src
INSERT OVERWRITE TABLE table_test_output_format_sequencefile SELECT src.key, src.value LIMIT 10;
describe table_test_output_format_sequencefile;



CREATE TABLE table_test_output_format_hivesequencefile(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat';

FROM src
INSERT OVERWRITE TABLE table_test_output_format_hivesequencefile SELECT src.key, src.value LIMIT 10;
describe table_test_output_format_hivesequencefile;



CREATE TABLE table_test_output_format(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.mapred.MapFileOutputFormat';

FROM src
INSERT OVERWRITE TABLE table_test_output_format SELECT src.key, src.value LIMIT 10;

describe table_test_output_format;





CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE;
DESCRIBE FORMATTED table1;

CREATE TABLE table2 LIKE table1;
DESCRIBE FORMATTED table2;

CREATE TABLE IF NOT EXISTS table2 LIKE table1;

CREATE EXTERNAL TABLE IF NOT EXISTS table2 LIKE table1;

CREATE EXTERNAL TABLE IF NOT EXISTS table3 LIKE table1;
DESCRIBE FORMATTED table3;

INSERT OVERWRITE TABLE table1 SELECT key, value FROM src WHERE key = 86;
INSERT OVERWRITE TABLE table2 SELECT key, value FROM src WHERE key = 100;

SELECT * FROM table1;
SELECT * FROM table2;

dfs -cp ${system:hive.root}/data/files/ext_test ${system:test.tmp.dir}/ext_test;

CREATE EXTERNAL TABLE table4 (a INT) LOCATION '${system:test.tmp.dir}/ext_test';
CREATE EXTERNAL TABLE table5 LIKE table4 LOCATION '${system:test.tmp.dir}/ext_test';

SELECT * FROM table4;
SELECT * FROM table5;

DROP TABLE table5;
SELECT * FROM table4;
DROP TABLE table4;

CREATE EXTERNAL TABLE table4 (a INT) LOCATION '${system:test.tmp.dir}/ext_test';
SELECT * FROM table4;

CREATE TABLE doctors STORED AS AVRO TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int",
      "doc":"Order of playing the role"
    },
    {
      "name":"first_name",
      "type":"string",
      "doc":"first name of actor playing role"
    },
    {
      "name":"last_name",
      "type":"string",
      "doc":"last name of actor playing role"
    }
  ]
}');

alter table doctors set tblproperties ('k1'='v1', 'k2'='v2');
DESCRIBE FORMATTED doctors;

CREATE TABLE doctors2 like doctors;
DESCRIBE FORMATTED doctors2;

CREATE TABLE PropertiedParquetTable(a INT, b STRING) STORED AS PARQUET TBLPROPERTIES("parquet.compression"="LZO");
CREATE TABLE LikePropertiedParquetTable LIKE PropertiedParquetTable;

DESCRIBE FORMATTED LikePropertiedParquetTable;

CREATE TABLE table5(col1 int, col2 string) stored as TEXTFILE;
DESCRIBE FORMATTED table5;

CREATE TABLE table6 like table5 stored as RCFILE;
DESCRIBE FORMATTED table6;

drop table table6;

CREATE  TABLE table6 like table5 ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileInputFormat' OUTPUTFORMAT   'org.apache.hadoop.hive.ql.io.RCFileOutputFormat' LOCATION '${system:hive.root}/data/files/table6';
DESCRIBE FORMATTED table6;

drop table table5;

create table orc_table (
time string)
stored as ORC tblproperties ("orc.compress"="SNAPPY");

create table orc_table_using_like like orc_table;

describe formatted orc_table_using_like;

drop table orc_table_using_like;

drop table orc_table;

-- Tests the copying over of Table Parameters according to a HiveConf setting
-- when doing a CREATE TABLE LIKE.

CREATE TABLE table1(a INT, b STRING);
ALTER TABLE table1 SET TBLPROPERTIES ('a'='1', 'b'='2', 'c'='3', 'd' = '4');

SET hive.ddl.createtablelike.properties.whitelist=a,c,D;
CREATE TABLE table2 LIKE table1;
DESC FORMATTED table2;
-- Test that CREATE TABLE LIKE commands can take explicit table properties

CREATE TABLE test_table LIKE src TBLPROPERTIES('key'='value');

DESC FORMATTED test_table;

set hive.table.parameters.default=key1=value1;

--Test that CREATE TABLE LIKE commands can take default table properties

CREATE TABLE test_table1 LIKE src;

DESC FORMATTED test_table1;

-- Test that CREATE TABLE LIKE commands can take default and explicit table properties

CREATE TABLE test_table2 LIKE src TBLPROPERTIES('key2' = 'value2');

DESC FORMATTED test_table2;

set hive.ddl.createtablelike.properties.whitelist=key2;

-- Test that properties inherited are overwritten by explicitly set ones

CREATE TABLE test_table3 LIKE test_table2 TBLPROPERTIES('key2' = 'value3');

DESC FORMATTED test_table3;

--Test that CREATE TALBE LIKE on a view can take explicit table properties

CREATE VIEW test_view (key, value) AS SELECT * FROM src;

CREATE TABLE test_table4 LIKE test_view TBLPROPERTIES('key'='value');

DESC FORMATTED test_table4;
-- SORT_QUERY_RESULTS

DROP TABLE IF EXISTS table1;
DROP TABLE IF EXISTS table2;
DROP TABLE IF EXISTS table3;
DROP VIEW IF EXISTS view1;

CREATE TABLE table1 (a STRING, b STRING) STORED AS TEXTFILE;
DESCRIBE table1;
DESCRIBE FORMATTED table1;

CREATE VIEW view1 AS SELECT * FROM table1;

CREATE TABLE table2 LIKE view1;
DESCRIBE table2;
DESCRIBE FORMATTED table2;

CREATE TABLE IF NOT EXISTS table2 LIKE view1;

CREATE EXTERNAL TABLE IF NOT EXISTS table2 LIKE view1;

CREATE EXTERNAL TABLE IF NOT EXISTS table3 LIKE view1;
DESCRIBE table3;
DESCRIBE FORMATTED table3;

INSERT OVERWRITE TABLE table1 SELECT key, value FROM src WHERE key = 86;
INSERT OVERWRITE TABLE table2 SELECT key, value FROM src WHERE key = 100;

SELECT * FROM table1;
SELECT * FROM table2;

DROP TABLE table1;
DROP TABLE table2;
DROP VIEW view1;

-- check partitions
create view view1 partitioned on (ds, hr) as select * from srcpart;
create table table1 like view1;
describe formatted table1;
DROP TABLE table1;
DROP VIEW view1;create table src_rc_merge_test(key int, value string) stored as rcfile;

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_merge_test;

set hive.exec.compress.output = true;

create table tgt_rc_merge_test(key int, value string) stored as rcfile;
insert into table tgt_rc_merge_test select * from src_rc_merge_test;
insert into table tgt_rc_merge_test select * from src_rc_merge_test;

show table extended like `tgt_rc_merge_test`;

select count(1) from tgt_rc_merge_test;
select sum(hash(key)), sum(hash(value)) from tgt_rc_merge_test;

alter table tgt_rc_merge_test concatenate;

show table extended like `tgt_rc_merge_test`;

select count(1) from tgt_rc_merge_test;
select sum(hash(key)), sum(hash(value)) from tgt_rc_merge_test;

drop table src_rc_merge_test;
drop table tgt_rc_merge_test;

CREATE TABLE table1 (
       a STRING,
       b ARRAY<STRING>,
       c ARRAY<MAP<STRING,STRING>>,
       d MAP<STRING,ARRAY<STRING>>
       ) STORED AS TEXTFILE;
DESCRIBE table1;
DESCRIBE EXTENDED table1;

LOAD DATA LOCAL INPATH '../../data/files/create_nested_type.txt' OVERWRITE INTO TABLE table1;

SELECT * from table1;


set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_notbucketed(a int, b varchar(128)) stored as orc TBLPROPERTIES ('transactional'='true');

create database vt;

create view vt.v as select * from srcpart;
describe formatted vt.v;

-- modifying definition of unpartitioned view
create or replace view vt.v partitioned on (ds, hr) as select * from srcpart;
alter view vt.v add partition (ds='2008-04-08',hr='11');
alter view vt.v add partition (ds='2008-04-08',hr='12');
select * from vt.v where value='val_409' and ds='2008-04-08' and hr='11';
describe formatted vt.v;
show partitions vt.v;

alter view vt.v drop partition (ds='2008-04-08',hr='11');
alter view vt.v drop partition (ds='2008-04-08',hr='12');
show partitions vt.v;

-- altering partitioned view 1
create or replace view vt.v partitioned on (ds, hr) as select value, ds, hr from srcpart;
select * from vt.v where value='val_409' and ds='2008-04-08' and hr='11';
describe formatted vt.v;
show partitions vt.v;

-- altering partitioned view 2
create or replace view vt.v partitioned on (ds, hr) as select key, value, ds, hr from srcpart;
select * from vt.v where value='val_409' and ds='2008-04-08' and hr='11';
describe formatted vt.v;
show partitions vt.v;
drop view vt.v;

-- updating to fix view with invalid definition
create table srcpart_temp like srcpart;
create view vt.v partitioned on (ds, hr) as select * from srcpart_temp;
drop table srcpart_temp; -- vt.v is now invalid
create or replace view vt.v partitioned on (ds, hr) as select * from srcpart;
describe formatted vt.v;
drop view vt.v;

drop database vt;-- Cannot add or drop partition columns with CREATE OR REPLACE VIEW if partitions currently exist (must specify partition columns)

drop view v;
create view v partitioned on (ds, hr) as select * from srcpart;
alter view v add partition (ds='1',hr='2');
create or replace view v as select * from srcpart;-- Cannot add or drop partition columns with CREATE OR REPLACE VIEW if partitions currently exist

drop view v;
create view v partitioned on (ds, hr) as select * from srcpart;
alter view v add partition (ds='1',hr='2');
create or replace view v partitioned on (hr) as select * from srcpart;-- Existing table is not a view

create or replace view src as select ds, hr from srcpart;-- View must have at least one non-partition column.

drop view v;
create view v partitioned on (ds, hr) as select * from srcpart;
create or replace view v partitioned on (ds, hr) as select ds, hr from srcpart;-- Can't combine IF NOT EXISTS and OR REPLACE.

drop view v;
create view v partitioned on (ds, hr) as select * from srcpart;
create or replace view if not exists v as select * from srcpart;-- Can't update view to have an invalid definition

drop view v;
create view v partitioned on (ds, hr) as select * from srcpart;
create or replace view v partitioned on (ds, hr) as blah;-- Can't update view to have a view cycle (1)

drop view v;
create view v1 partitioned on (ds, hr) as select * from srcpart;
create view v2 partitioned on (ds, hr) as select * from v1;
create view v3 partitioned on (ds, hr) as select * from v2;
create or replace view v1 partitioned on (ds, hr) as select * from v3;-- Can't update view to have a view cycle (2)

drop view v;
create view v1 partitioned on (ds, hr) as select * from srcpart;
create or replace view v1 partitioned on (ds, hr) as select * from v1;CREATE TABLE list_bucket_single (key STRING, value STRING) SKEWED BY (key) ON ('1','5','6');
CREATE TABLE list_bucket_single_2 (key STRING, value STRING) SKEWED BY (key) ON ((1),(5),(6));
CREATE TABLE list_bucket_multiple (col1 STRING, col2 int, col3 STRING) SKEWED BY (col1, col2) ON (('s1',1), ('s3',3), ('s13',13), ('s78',78));
describe formatted list_bucket_single_2;
describe formatted list_bucket_single;
describe formatted list_bucket_multiple;
drop table list_bucket_single;
drop table list_bucket_multiple;
drop table list_bucket_single_2;
CREATE TABLE skewed_table (key STRING, value STRING) SKEWED BY (key) ON ((1),(5,8),(6));
CREATE TABLE skewed_table (key STRING, value STRING) SKEWED BY (key,key) ON ((1),(5),(6));
CREATE TABLE skewed_table (key STRING, value STRING) SKEWED BY (key_non) ON ((1),(5),(6));

create table abc(strct struct<a:int, b:string, c:string>)
row format delimited
  fields terminated by '\t'
  collection items terminated by '\001';

load data local inpath '../../data/files/kv1.txt'
overwrite into table abc;

SELECT strct, strct.a, strct.b FROM abc LIMIT 10;


create table table_in_database_creation_not_exist.test as select * from src limit 1;create table `table_in_database_creation_not_exist.test` as select * from src limit 1;create table table_in_database_creation_not_exist.test (a string);create table `table_in_database_creation_not_exist.test` (a string);drop table aa;
create table aa ( test STRING )
  ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
  WITH SERDEPROPERTIES ("input.regex" = "[^\\](.*)", "output.format.string" = "$1s");
EXPLAIN
CREATE TEMPORARY FUNCTION test_max AS 'org.apache.hadoop.hive.ql.udf.UDAFTestMax';

CREATE TEMPORARY FUNCTION test_max AS 'org.apache.hadoop.hive.ql.udf.UDAFTestMax';

CREATE TABLE dest1(col INT);

FROM src INSERT OVERWRITE TABLE dest1 SELECT test_max(length(src.value));

SELECT dest1.* FROM dest1;

-- cover all the other value types:
SELECT test_max(CAST(length(src.value) AS SMALLINT)) FROM src;
SELECT test_max(CAST(length(src.value) AS BIGINT)) FROM src;
SELECT test_max(CAST(length(src.value) AS DOUBLE)) FROM src;
SELECT test_max(CAST(length(src.value) AS FLOAT)) FROM src;
SELECT test_max(substr(src.value,5)) FROM src;

DROP TEMPORARY FUNCTION test_max;
CREATE TEMPORARY FUNCTION test_udaf AS 'org.apache.hadoop.hive.ql.udf.UDAFWrongArgLengthForTestCase';

EXPLAIN
SELECT test_udaf(length(src.value)) FROM src;

SELECT test_udaf(length(src.value)) FROM src;
explain create table abc(mydata uniontype<int,double,array<string>,struct<a:int,b:string>>,
strct struct<a:int, b:string, c:string>);

create table abc(mydata uniontype<int,double,array<string>,struct<a:int,b:string>>,
strct struct<a:int, b:string, c:string>);

load data local inpath '../../data/files/union_input.txt'
overwrite into table abc;

SELECT * FROM abc;
CREATE TEMPORARY FUNCTION dummy_genericudf AS 'org.apache.hadoop.hive.ql.udf.generic.DummyGenericUDF';
CREATE TEMPORARY FUNCTION dummy_function AS 'org.apache.hadoop.hive.ql.udf.DummyFunction';
set hive.mapred.mode=nonstrict;
DROP VIEW view1;
DROP VIEW view2;
DROP VIEW view3;
DROP VIEW view4;
DROP VIEW view5;
DROP VIEW view6;
DROP VIEW view7;
DROP VIEW view8;
DROP VIEW view9;
DROP VIEW view10;
DROP VIEW view11;
DROP VIEW view12;
DROP VIEW view13;
DROP VIEW view14;
DROP VIEW view15;
DROP VIEW view16;
DROP TEMPORARY FUNCTION test_translate;
DROP TEMPORARY FUNCTION test_max;
DROP TEMPORARY FUNCTION test_explode;


SELECT * FROM src WHERE key=86;
CREATE VIEW view1 AS SELECT value FROM src WHERE key=86;
CREATE VIEW view2 AS SELECT * FROM src;
CREATE VIEW view3(valoo)
TBLPROPERTIES ("fear" = "factor")
AS SELECT upper(value) FROM src WHERE key=86;
SELECT * from view1;
SELECT * from view2 where key=18;
SELECT * from view3;

-- test EXPLAIN output for CREATE VIEW
EXPLAIN
CREATE VIEW view0(valoo) AS SELECT upper(value) FROM src WHERE key=86;

-- make sure EXPLAIN works with a query which references a view
EXPLAIN
SELECT * from view2 where key=18;

SHOW TABLES 'view.*';
DESCRIBE view1;
DESCRIBE EXTENDED view1;
DESCRIBE FORMATTED view1;
DESCRIBE view2;
DESCRIBE EXTENDED view2;
DESCRIBE FORMATTED view2;
DESCRIBE view3;
DESCRIBE EXTENDED view3;
DESCRIBE FORMATTED view3;

ALTER VIEW view3 SET TBLPROPERTIES ("biggest" = "loser");
DESCRIBE EXTENDED view3;
DESCRIBE FORMATTED view3;

CREATE TABLE table1 (key int);

-- use DESCRIBE EXTENDED on a base table and an external table as points
-- of comparison for view descriptions
DESCRIBE EXTENDED table1;
DESCRIBE EXTENDED src1;

-- use DESCRIBE EXTENDED on a base table as a point of comparison for
-- view descriptions
DESCRIBE EXTENDED table1;


INSERT OVERWRITE TABLE table1 SELECT key FROM src WHERE key = 86;

SELECT * FROM table1;
CREATE VIEW view4 AS SELECT * FROM table1;
SELECT * FROM view4;
DESCRIBE view4;
ALTER TABLE table1 ADD COLUMNS (value STRING);
SELECT * FROM table1;
SELECT * FROM view4;
DESCRIBE table1;
DESCRIBE view4;

CREATE VIEW view5 AS SELECT v1.key as key1, v2.key as key2
FROM view4 v1 join view4 v2;
SELECT * FROM view5;
DESCRIBE view5;

-- verify that column name and comment in DDL portion
-- overrides column alias in SELECT
CREATE VIEW view6(valoo COMMENT 'I cannot spell') AS
SELECT upper(value) as blarg FROM src WHERE key=86;
DESCRIBE view6;

-- verify that ORDER BY and LIMIT are both supported in view def
CREATE VIEW view7 AS
SELECT * FROM src
WHERE key > 80 AND key < 100
ORDER BY key, value
LIMIT 10;

SELECT * FROM view7;

-- top-level ORDER BY should override the one inside the view
-- (however, the inside ORDER BY should still influence the evaluation
-- of the limit)
SELECT * FROM view7 ORDER BY key DESC, value;

-- top-level LIMIT should override if lower
SELECT * FROM view7 LIMIT 5;

-- but not if higher
SELECT * FROM view7 LIMIT 20;

-- test usage of a function within a view
CREATE TEMPORARY FUNCTION test_translate AS
'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate';
CREATE VIEW view8(c) AS
SELECT test_translate('abc', 'a', 'b')
FROM table1;
DESCRIBE EXTENDED view8;
DESCRIBE FORMATTED view8;
SELECT * FROM view8;

-- test usage of a UDAF within a view
CREATE TEMPORARY FUNCTION test_max AS
'org.apache.hadoop.hive.ql.udf.UDAFTestMax';
set hive.map.aggr=false;
-- disable map-side aggregation
CREATE VIEW view9(m) AS
SELECT test_max(length(value))
FROM src;
DESCRIBE EXTENDED view9;
DESCRIBE FORMATTED view9;
SELECT * FROM view9;
DROP VIEW view9;
set hive.map.aggr=true;
-- enable map-side aggregation
CREATE VIEW view9(m) AS
SELECT test_max(length(value))
FROM src;
DESCRIBE EXTENDED view9;
DESCRIBE FORMATTED view9;
SELECT * FROM view9;

-- test usage of a subselect within a view
CREATE VIEW view10 AS
SELECT slurp.* FROM (SELECT * FROM src WHERE key=86) slurp;
DESCRIBE EXTENDED view10;
DESCRIBE FORMATTED view10;
SELECT * FROM view10;

-- test usage of a UDTF within a view
CREATE TEMPORARY FUNCTION test_explode AS
'org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode';
CREATE VIEW view11 AS
SELECT test_explode(array(1,2,3)) AS (boom)
FROM table1;
DESCRIBE EXTENDED view11;
DESCRIBE FORMATTED view11;
SELECT * FROM view11;

-- test usage of LATERAL within a view
CREATE VIEW view12 AS
SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol;
DESCRIBE EXTENDED view12;
DESCRIBE FORMATTED view12;
SELECT * FROM view12
ORDER BY key ASC, myCol ASC LIMIT 1;

-- test usage of LATERAL with a view as the LHS
SELECT * FROM view2 LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
ORDER BY key ASC, myCol ASC LIMIT 1;

-- test usage of TABLESAMPLE within a view
CREATE VIEW view13 AS
SELECT s.key
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 ON key) s;
DESCRIBE EXTENDED view13;
DESCRIBE FORMATTED view13;
SELECT * FROM view13
ORDER BY key LIMIT 12;

-- test usage of JOIN+UNION+AGG all within same view
CREATE VIEW view14 AS
SELECT unionsrc1.key as k1, unionsrc1.value as v1,
       unionsrc2.key as k2, unionsrc2.value as v2
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2 where s2.key < 10) unionsrc1
JOIN
     (select 'tst1' as key, cast(count(1) as string) as value from src s3
                         UNION  ALL
      select s4.key as key, s4.value as value from src s4 where s4.key < 10) unionsrc2
ON (unionsrc1.key = unionsrc2.key);
DESCRIBE EXTENDED view14;
DESCRIBE FORMATTED view14;
SELECT * FROM view14
ORDER BY k1;

-- test usage of GROUP BY within view
CREATE VIEW view15 AS
SELECT key,COUNT(value) AS value_count
FROM src
GROUP BY key;
DESCRIBE EXTENDED view15;
DESCRIBE FORMATTED view15;
SELECT * FROM view15
ORDER BY value_count DESC, key
LIMIT 10;

-- test usage of DISTINCT within view
CREATE VIEW view16 AS
SELECT DISTINCT value
FROM src;
DESCRIBE EXTENDED view16;
DESCRIBE FORMATTED view16;
SELECT * FROM view16
ORDER BY value
LIMIT 10;

-- HIVE-2133:  DROP TABLE IF EXISTS should ignore a matching view name
DROP TABLE IF EXISTS view16;
DESCRIBE view16;

-- Likewise, DROP VIEW IF EXISTS should ignore a matching table name
DROP VIEW IF EXISTS table1;
DESCRIBE table1;

-- this should work since currently we don't track view->table
-- dependencies for implementing RESTRICT


DROP VIEW view1;
DROP VIEW view2;
DROP VIEW view3;
DROP VIEW view4;
DROP VIEW view5;
DROP VIEW view6;
DROP VIEW view7;
DROP VIEW view8;
DROP VIEW view9;
DROP VIEW view10;
DROP VIEW view11;
DROP VIEW view12;
DROP VIEW view13;
DROP VIEW view14;
DROP VIEW view15;
DROP VIEW view16;
DROP TEMPORARY FUNCTION test_translate;
DROP TEMPORARY FUNCTION test_max;
DROP TEMPORARY FUNCTION test_explode;

DROP VIEW xxx12;

-- views and tables share the same namespace
CREATE TABLE xxx12(key int);
CREATE VIEW xxx12 AS SELECT key FROM src;
-- CREATE VIEW should fail if it references a temp table
create temporary table tmp1 (c1 string, c2 string);
create view tmp1_view as select c1, count(*) from tmp1 group by c1;

DROP VIEW xxx4;

-- views and tables share the same namespace
CREATE VIEW xxx4 AS SELECT key FROM src;
CREATE TABLE xxx4(key int);
DROP VIEW xxx13;

-- number of explicit view column defs must match underlying SELECT
CREATE VIEW xxx13(x,y,z) AS
SELECT key FROM src;
DROP VIEW xxx5;

-- duplicate column names are illegal
CREATE VIEW xxx5(x,x) AS
SELECT key,value FROM src;
DROP VIEW xxx14;

-- Ideally (and according to SQL:200n), this should actually be legal,
-- but since internally we impose the new column descriptors by
-- reference to underlying name rather than position, we have to make
-- it illegal.  There's an easy workaround (provide the unique names
-- via direct column aliases, e.g. SELECT key AS x, key AS y)
CREATE VIEW xxx14(x,y) AS
SELECT key,key FROM src;
DROP VIEW xxx15;

-- should fail:  baz is not a column
CREATE VIEW xxx15
PARTITIONED ON (baz)
AS SELECT key FROM src;
DROP VIEW xxx16;

-- should fail:  must have at least one non-partitioning column
CREATE VIEW xxx16
PARTITIONED ON (key)
AS SELECT key FROM src;
DROP VIEW xxx17;

-- should fail:  partitioning key must be at end
CREATE VIEW xxx17
PARTITIONED ON (key)
AS SELECT key,value FROM src;
DROP VIEW xxx18;

-- should fail:  partitioning columns out of order
CREATE VIEW xxx18
PARTITIONED ON (value,key)
AS SELECT key+1 as k2,key,value FROM src;
DROP VIEW vp1;
DROP VIEW vp2;
DROP VIEW vp3;

-- test partitioned view definition
-- (underlying table is not actually partitioned)
CREATE VIEW vp1
PARTITIONED ON (value)
AS
SELECT key, value
FROM src
WHERE key=86;
DESCRIBE EXTENDED vp1;
DESCRIBE FORMATTED vp1;

SELECT * FROM vp1;

SELECT key FROM vp1;

SELECT value FROM vp1;

ALTER VIEW vp1
ADD PARTITION (value='val_86') PARTITION (value='val_xyz');

-- should work since we use IF NOT EXISTS
ALTER VIEW vp1
ADD IF NOT EXISTS PARTITION (value='val_xyz');

SHOW PARTITIONS vp1;

SHOW PARTITIONS vp1 PARTITION(value='val_86');

SHOW TABLE EXTENDED LIKE vp1;

SHOW TABLE EXTENDED LIKE vp1 PARTITION(value='val_86');

ALTER VIEW vp1
DROP PARTITION (value='val_xyz');

SET hive.exec.drop.ignorenonexistent=false;

-- should work since we use IF EXISTS
ALTER VIEW vp1
DROP IF EXISTS PARTITION (value='val_xyz');

SHOW PARTITIONS vp1;

SET hive.mapred.mode=strict;

-- Even though no partition predicate is specified in the next query,
-- the WHERE clause inside of the view should satisfy strict mode.
-- In other words, strict only applies to underlying tables
-- (regardless of whether or not the view is partitioned).
SELECT * FROM vp1;

SET hive.mapred.mode=nonstrict;

-- test a partitioned view on top of an underlying partitioned table,
-- but with only a suffix of the partitioning columns
CREATE VIEW vp2
PARTITIONED ON (hr)
AS SELECT * FROM srcpart WHERE key < 10;
DESCRIBE FORMATTED vp2;

ALTER VIEW vp2 ADD PARTITION (hr='11') PARTITION (hr='12');
SELECT key FROM vp2 WHERE hr='12' ORDER BY key;

-- test a partitioned view where the PARTITIONED ON clause references
-- an imposed column name
CREATE VIEW vp3(k,v)
PARTITIONED ON (v)
AS
SELECT key, value
FROM src
WHERE key=86;
DESCRIBE FORMATTED vp3;

ALTER VIEW vp3
ADD PARTITION (v='val_86');

DROP VIEW vp1;
DROP VIEW vp2;
DROP VIEW vp3;
drop view if exists v;
drop view if exists w;

create view v as select cast(key as string) from src;
describe formatted v;

create view w as select key, value from (
  select key, value from src
) a;
describe formatted w;

drop view v;
drop view w;


-- HIVE-4116 Can't use views using map datatype.

CREATE TABLE items (id INT, name STRING, info MAP<STRING,STRING>);

explain
CREATE VIEW priceview AS SELECT items.id, items.info['price'] FROM items;
CREATE VIEW priceview AS SELECT items.id, items.info['price'] FROM items;

select * from priceview;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- current
explain select src.key from src join src src2;
-- ansi cross join
explain select src.key from src cross join src src2;
-- appending condition is allowed
explain select src.key from src cross join src src2 on src.key=src2.key;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;
set hive.mapjoin.hybridgrace.hashtable=true;

explain select src.key from src join src src2;
explain select src.key from src cross join src src2;
explain select src.key from src cross join src src2 on src.key=src2.key;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

create table A as
select * from src;

create table B as
select * from src
limit 10;

set hive.auto.convert.join.noconditionaltask.size=100;

explain select * from A join B;

explain select * from B d1 join B d2 on d1.key = d2.key join A;

explain select * from A join
         (select d1.key
          from B d1 join B d2 on d1.key = d2.key
          where 1 = 1 group by d1.key) od1;

explain select * from A join (select d1.key from B d1 join B d2 where 1 = 1  group by d1.key) od1;

explain select * from
(select A.key from A  group by key) ss join
(select d1.key from B d1 join B d2 on d1.key = d2.key where 1 = 1 group by d1.key) od1;


set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

create table A as
select * from src;

create table B as
select * from src order by key
limit 10;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;

explain select * from A join B;

explain select * from B d1 join B d2 on d1.key = d2.key join A;

explain select * from A join
         (select d1.key
          from B d1 join B d2 on d1.key = d2.key
          where 1 = 1 group by d1.key) od1;

explain select * from A join (select d1.key from B d1 join B d2 where 1 = 1 group by d1.key) od1;

explain select * from
(select A.key from A group by key) ss join
(select d1.key from B d1 join B d2 on d1.key = d2.key where 1 = 1 group by d1.key) od1;


dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/tmpsepatest;
CREATE TABLE separator_test
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES ("separatorChar" = "|","quoteChar"="\"","escapeChar"="
")
STORED AS TEXTFILE
LOCATION 'file:${system:test.tmp.dir}/tmpsepatest'
AS
SELECT * FROM src where key = 100 limit 1;
dfs -cat ${system:test.tmp.dir}/tmpsepatest/000000_0;
drop table separator_test;
set hive.explain.user=false;
-- EXCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)
-- SORT_QUERY_RESULTS

create table nzhang_Tmp(a int, b string);
select * from nzhang_Tmp;

explain create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10;

create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10;

select * from nzhang_CTAS1;

describe formatted nzhang_CTAS1;


explain create table nzhang_ctas2 as select * from src sort by key, value limit 10;

create table nzhang_ctas2 as select * from src sort by key, value limit 10;

select * from nzhang_ctas2;

describe formatted nzhang_CTAS2;


explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;

create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;

select * from nzhang_ctas3;

describe formatted nzhang_CTAS3;


explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;

create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;

select * from nzhang_ctas3;

describe formatted nzhang_CTAS3;


explain create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10;

create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10;

select * from nzhang_ctas4;

describe formatted nzhang_CTAS4;

explain extended create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10;

set mapreduce.framework.name=yarn;
set mapreduce.jobtracker.address=localhost:58;
set hive.exec.mode.local.auto=true;

create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10;

create table nzhang_ctas6 (key string, `to` string);
insert overwrite table nzhang_ctas6 select key, value from src tablesample (10 rows);
create table nzhang_ctas7 as select key, `to` from nzhang_ctas6;











create external table nzhang_ctas4 as select key, value from src;


set hive.mapred.mode=nonstrict;
drop table if exists orc_table_with_null;
CREATE TABLE orc_table_with_null STORED AS ORC AS SELECT key, null FROM src;
drop table ctas_char_1;
drop table ctas_char_2;
drop view ctas_char_3;

create table ctas_char_1 (key char(10), value string);
insert overwrite table ctas_char_1
  select key, value from src sort by key, value limit 5;

-- create table as with char column
create table ctas_char_2 as select key, value from ctas_char_1;

-- view with char column
create view ctas_char_3 as select key, value from ctas_char_2;

select key, value from ctas_char_1;
select * from ctas_char_2;
select * from ctas_char_3;


drop table ctas_char_1;
drop table ctas_char_2;
drop view ctas_char_3;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

-- HIVE-4392, column aliases from expressionRR (GBY, etc.) are not valid name for table

-- group by


explain
create table summary as select *, key + 1, concat(value, value) from src limit 20;
create table summary as select *, key + 1, concat(value, value) from src limit 20;
describe formatted summary;
select * from summary;

-- window functions
explain
create table x4 as select *, rank() over(partition by key order by value) as rr from src1;
create table x4 as select *, rank() over(partition by key order by value) as rr from src1;
describe formatted x4;
select * from x4;

explain
create table x5 as select *, lead(key,1) over(partition by key order by value) as lead1 from src limit 20;
create table x5 as select *, lead(key,1) over(partition by key order by value) as lead1 from src limit 20;
describe formatted x5;
select * from x5;

-- sub queries
explain
create table x6 as select * from (select *, key + 1 from src1) a;
create table x6 as select * from (select *, key + 1 from src1) a;
describe formatted x6;
select * from x6;

explain
create table x7 as select * from (select *, count(value) from src group by key, value) a;
create table x7 as select * from (select *, count(value) from src group by key, value) a;
describe formatted x7;
select * from x7;

explain
create table x8 as select * from (select *, count(value) from src group by key, value having key < 9) a;
create table x8 as select * from (select *, count(value) from src group by key, value having key < 9) a;
describe formatted x8;
select * from x8;

explain
create table x9 as select * from (select max(value),key from src group by key having key < 9 AND max(value) IS NOT NULL) a;
create table x9 as select * from (select max(value),key from src group by key having key < 9 AND max(value) IS NOT NULL) a;
describe formatted x9;
select * from x9;

drop table ctas_date_1;
drop table ctas_date_2;
drop view ctas_date_3;
drop view ctas_date_4;

create table ctas_date_1 (key int, value string, dd date);
insert overwrite table ctas_date_1
  select key, value, date '2012-01-01' from src sort by key, value limit 5;

-- create table as with date column
create table ctas_date_2 as select key, value, dd, date '1980-12-12' from ctas_date_1;

-- view with date column
create view ctas_date_3 as select * from ctas_date_2 where dd > date '2000-01-01';
create view ctas_date_4 as select * from ctas_date_2 where dd < date '2000-01-01';

select key, value, dd, date '1980-12-12' from ctas_date_1;
select * from ctas_date_2;
select * from ctas_date_3;
select count(*) from ctas_date_4;


drop table ctas_date_1;
drop table ctas_date_2;
drop view ctas_date_3;
drop view ctas_date_4;
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)

create table nzhang_Tmp(a int, b string);
select * from nzhang_Tmp;

explain create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10;

create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10;

select * from nzhang_CTAS1;

describe formatted nzhang_CTAS1;


explain create table nzhang_ctas2 as select * from src sort by key, value limit 10;

create table nzhang_ctas2 as select * from src sort by key, value limit 10;

select * from nzhang_ctas2;

describe formatted nzhang_CTAS2;


explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;

create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;

select * from nzhang_ctas3;

describe formatted nzhang_CTAS3;


explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;

create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;

select * from nzhang_ctas3;

describe formatted nzhang_CTAS3;


explain create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10;

create table nzhang_ctas4 row format delimited fields terminated by ',' stored as textfile as select key, value from src sort by key, value limit 10;

select * from nzhang_ctas4;

describe formatted nzhang_CTAS4;

explain extended create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10;

set mapred.job.tracker=localhost:58;
set hive.exec.mode.local.auto=true;

create table nzhang_ctas5 row format delimited fields terminated by ',' lines terminated by '\012' stored as textfile as select key, value from src sort by key, value limit 10;

create table nzhang_ctas6 (key string, `to` string);
insert overwrite table nzhang_ctas6 select key, value from src limit 10;
create table nzhang_ctas7 as select key, `to` from nzhang_ctas6;

create table nzhang_ctas8 as select 3.14BD from nzhang_ctas6 limit 1;
desc nzhang_ctas8;
drop table nzhang_ctas8;
create table ctas1
location 'file:${system:test.tmp.dir}/ctastmpfolder'
as
select * from src limit 3;

create table ctas2
location 'file:${system:test.tmp.dir}/ctastmpfolder'
as
select * from src limit 2;

set hive.metastore.warehouse.dir=invalid_scheme://${system:test.tmp.dir};

-- Tests that CTAS queries in non-default databases use the location of the database
-- not the hive.metastore.warehouse.dir for intermediate files (FileSinkOperator output).
-- If hive.metastore.warehouse.dir were used this would fail because the scheme is invalid.

CREATE DATABASE db1
LOCATION 'pfile://${system:test.tmp.dir}/db1';

USE db1;
EXPLAIN CREATE TABLE table_db1 AS SELECT * FROM default.src;
CREATE TABLE table_db1 AS SELECT * FROM default.src;

DESCRIBE FORMATTED table_db1;
drop table ctas_varchar_1;
drop table ctas_varchar_2;
drop view ctas_varchar_3;

create table ctas_varchar_1 (key varchar(10), value string);
insert overwrite table ctas_varchar_1
  select key, value from src sort by key, value limit 5;

-- create table as with varchar column
create table ctas_varchar_2 as select key, value from ctas_varchar_1;

-- view with varchar column
create view ctas_varchar_3 as select key, value from ctas_varchar_2;

select key, value from ctas_varchar_1;
select * from ctas_varchar_2;
select * from ctas_varchar_3;


drop table ctas_varchar_1;
drop table ctas_varchar_2;
drop view ctas_varchar_3;
with src1 as (select key from src order by key limit 5)
select * from src1;

use default;
drop view v;
create view v as with cte as (select key, value from src order by key limit 5)
select key from cte;

describe extended v;

create database bug;
use bug;
select * from default.v;
drop database bug;

use default;
drop view v;
create view v as with cte as (select * from src  order by key limit 5)
select * from cte;

describe extended v;

create database bug;
use bug;
select * from default.v;
drop database bug;


use default;
drop view v;
create view v as with src1 as (select key from src order by key limit 5)
select * from src1;

describe extended v;

create database bug;
use bug;
select * from default.v;
use default;
drop view v;
drop database bug;
explain
with q1 as ( select key from src where key = '5')
select *
from q1
;

with q1 as ( select key from src where key = '5')
select *
from q1
;

-- in subquery
explain
with q1 as ( select key from src where key = '5')
select * from (select key from q1) a;

with q1 as ( select key from src where key = '5')
select * from (select key from q1) a;

-- chaining
explain
with q1 as ( select key from q2 where key = '5'),
q2 as ( select key from src where key = '5')
select * from (select key from q1) a;

with q1 as ( select key from q2 where key = '5'),
q2 as ( select key from src where key = '5')
select * from (select key from q1) a;
-- union test
with q1 as (select * from src where key= '5'),
q2 as (select * from src s2 where key = '4')
select * from q1 union all select * from q2
;

-- insert test
create table s1 like src;
with q1 as ( select key, value from src where key = '5')
from q1
insert overwrite table s1
select *
;
select * from s1;
drop table s1;

-- from style
with q1 as (select * from src where key= '5')
from q1
select *
;

-- ctas
create table s2 as
with q1 as ( select key from src where key = '4')
select * from q1
;

select * from s2;
drop table s2;

-- view test
create view v1 as
with q1 as ( select key from src where key = '5')
select * from q1
;

select * from v1;

drop view v1;


-- view test, name collision
create view v1 as
with q1 as ( select key from src where key = '5')
select * from q1
;

with q1 as ( select key from src where key = '4')
select * from v1
;

drop view v1;


explain
with q1 as ( select key from q2 where key = '5'),
q2 as ( select key from q1 where key = '5')
select * from (select key from q1) a;select * from (with q1 as ( select key from q2 where key = '5') select * from q1) a;
CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;
DROP TABLE tmp_pyang_bucket3;
CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) SORTED BY (USERID) INTO 32 BUCKETS;
select current_timestamp = current_timestamp(), current_date = current_date() from src limit 5;

set hive.test.currenttimestamp =2012-01-01 01:02:03;
select current_date, current_timestamp from src limit 5;
-- SORT_QUERY_RESULTS

CREATE TABLE src1_rot13_iof(key STRING, value STRING)
  STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.udf.Rot13InputFormat'
            OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.udf.Rot13OutputFormat';
DESCRIBE EXTENDED src1_rot13_iof;
SELECT * FROM src1;
INSERT OVERWRITE TABLE src1_rot13_iof SELECT * FROM src1;
SELECT * FROM src1_rot13_iof;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency = true;

-- SORT_QUERY_RESULTS

SHOW DATABASES;

-- CREATE with comment
CREATE DATABASE test_db COMMENT 'Hive test database';
SHOW DATABASES;

-- CREATE INE already exists
CREATE DATABASE IF NOT EXISTS test_db;
SHOW DATABASES;

-- SHOW DATABASES synonym
SHOW SCHEMAS;

-- DROP
DROP DATABASE test_db;
SHOW DATABASES;

-- CREATE INE doesn't exist
CREATE DATABASE IF NOT EXISTS test_db COMMENT 'Hive test database';
SHOW DATABASES;

-- DROP IE exists
DROP DATABASE IF EXISTS test_db;
SHOW DATABASES;

-- DROP IE doesn't exist
DROP DATABASE IF EXISTS test_db;

-- SHOW
CREATE DATABASE test_db;
SHOW DATABASES;

-- SHOW pattern
SHOW DATABASES LIKE 'test*';

-- SHOW pattern
SHOW DATABASES LIKE '*ef*';


USE test_db;
SHOW DATABASES;

-- CREATE table in non-default DB
CREATE TABLE test_table (col1 STRING) STORED AS TEXTFILE;
SHOW TABLES;

-- DESCRIBE table in non-default DB
DESCRIBE test_table;

-- DESCRIBE EXTENDED in non-default DB
DESCRIBE EXTENDED test_table;

-- CREATE LIKE in non-default DB
CREATE TABLE test_table_like LIKE test_table;
SHOW TABLES;
DESCRIBE EXTENDED test_table_like;

-- LOAD and SELECT
LOAD DATA LOCAL INPATH '../../data/files/test.dat'
OVERWRITE INTO TABLE test_table;
SELECT * FROM test_table;

-- DROP and CREATE w/o LOAD
DROP TABLE test_table;
SHOW TABLES;

CREATE TABLE test_table (col1 STRING) STORED AS TEXTFILE;
SHOW TABLES;

SELECT * FROM test_table;

-- CREATE table that already exists in DEFAULT
USE test_db;
CREATE TABLE src (col1 STRING) STORED AS TEXTFILE;
SHOW TABLES;

SELECT * FROM src LIMIT 10;

USE default;
SELECT * FROM src LIMIT 10;

-- DROP DATABASE
USE test_db;

DROP TABLE src;
DROP TABLE test_table;
DROP TABLE test_table_like;
SHOW TABLES;

USE default;
DROP DATABASE test_db;
SHOW DATABASES;

-- DROP EMPTY DATABASE CASCADE
CREATE DATABASE to_drop_db1;
SHOW DATABASES;
USE default;
DROP DATABASE to_drop_db1 CASCADE;
SHOW DATABASES;

-- DROP NON-EMPTY DATABASE CASCADE
CREATE DATABASE to_drop_db2;
SHOW DATABASES;
USE to_drop_db2;
CREATE TABLE temp_tbl (c STRING);
CREATE TABLE temp_tbl2 LIKE temp_tbl;
INSERT OVERWRITE TABLE temp_tbl2 SELECT COUNT(*) FROM temp_tbl;
USE default;
DROP DATABASE to_drop_db2 CASCADE;
SHOW DATABASES;

-- DROP NON-EMPTY DATABASE CASCADE IF EXISTS
CREATE DATABASE to_drop_db3;
SHOW DATABASES;
USE to_drop_db3;
CREATE TABLE temp_tbl (c STRING);
USE default;
DROP DATABASE IF EXISTS to_drop_db3 CASCADE;
SHOW DATABASES;

-- DROP NON-EXISTING DATABASE CASCADE IF EXISTS
DROP DATABASE IF EXISTS non_exists_db3 CASCADE;
SHOW DATABASES;

-- DROP NON-EXISTING DATABASE RESTRICT IF EXISTS
DROP DATABASE IF EXISTS non_exists_db3 RESTRICT;

-- DROP EMPTY DATABASE RESTRICT
CREATE DATABASE to_drop_db4;
SHOW DATABASES;
DROP DATABASE to_drop_db4 RESTRICT;
SHOW DATABASES;


--
-- Canonical Name Tests
--

CREATE DATABASE db1;
CREATE DATABASE db2;

-- CREATE foreign table
CREATE TABLE db1.src(key STRING, value STRING)
STORED AS TEXTFILE;

-- LOAD into foreign table
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt'
OVERWRITE INTO TABLE db1.src;

-- SELECT from foreign table
SELECT * FROM db1.src;

-- CREATE Partitioned foreign table
CREATE TABLE db1.srcpart(key STRING, value STRING)
PARTITIONED BY (ds STRING, hr STRING)
STORED AS TEXTFILE;

-- LOAD data into Partitioned foreign table
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt'
OVERWRITE INTO TABLE db1.srcpart
PARTITION (ds='2008-04-08', hr='11');

-- SELECT from Partitioned foreign table
SELECT key, value FROM db1.srcpart
WHERE key < 100 AND ds='2008-04-08' AND hr='11';

-- SELECT JOINed product of two foreign tables
USE db2;
SELECT a.* FROM db1.src a JOIN default.src1 b
ON (a.key = b.key);

-- CREATE TABLE AS SELECT from foreign table
CREATE TABLE conflict_name AS
SELECT value FROM default.src WHERE key = 66;

-- CREATE foreign table
CREATE TABLE db1.conflict_name AS
SELECT value FROM db1.src WHERE key = 8;

-- query tables with the same names in different DBs
SELECT * FROM (
  SELECT value FROM db1.conflict_name
UNION ALL
  SELECT value FROM conflict_name
) subq ORDER BY value;

USE default;
SELECT * FROM (
  SELECT value FROM db1.conflict_name
UNION ALL
  SELECT value FROM db2.conflict_name
) subq;

-- TABLESAMPLES
CREATE TABLE bucketized_src (key INT, value STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS;

INSERT OVERWRITE TABLE bucketized_src
SELECT key, value FROM src WHERE key=66;

SELECT key FROM bucketized_src TABLESAMPLE(BUCKET 1 out of 1);

-- CREATE TABLE LIKE
CREATE TABLE db2.src1 LIKE default.src;

USE db2;
DESC EXTENDED src1;

-- character escaping
SELECT key FROM `default`.src ORDER BY key LIMIT 1;
SELECT key FROM `default`.`src` ORDER BY key LIMIT 1;
SELECT key FROM default.`src` ORDER BY key LIMIT 1;

USE default;
SHOW DATABASES;

-- Try to create a database that already exists
CREATE DATABASE test_db;
CREATE DATABASE test_db;
SHOW DATABASES;

-- Try to create a database with an invalid name
CREATE DATABASE `test.db`;
-- create database with multiple tables, indexes and views.
-- Use both partitioned and non-partitioned tables, as well as
-- tables and indexes with specific storage locations
-- verify the drop the database with cascade works and that the directories
-- outside the database's default storage are removed as part of the drop

CREATE DATABASE db5;
SHOW DATABASES;
USE db5;

set hive.stats.dbclass=fs;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/temp;
dfs -rmr ${system:test.tmp.dir}/dbcascade;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade;

-- add a table, index and view
CREATE TABLE temp_tbl (id INT, name STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE temp_tbl;
CREATE VIEW temp_tbl_view AS SELECT * FROM temp_tbl;
CREATE INDEX idx1 ON TABLE temp_tbl(id) AS 'COMPACT' with DEFERRED REBUILD;
ALTER INDEX idx1 ON temp_tbl REBUILD;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/temp_tbl2;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/temp_tbl2_idx2;
-- add a table, index and view with a different storage location
CREATE TABLE temp_tbl2 (id INT, name STRING) LOCATION 'file:${system:test.tmp.dir}/dbcascade/temp_tbl2';
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' into table temp_tbl2;
CREATE VIEW temp_tbl2_view AS SELECT * FROM temp_tbl2;
CREATE INDEX idx2 ON TABLE temp_tbl2(id) AS 'COMPACT' with DEFERRED REBUILD LOCATION 'file:${system:test.tmp.dir}/dbcascade/temp_tbl2_idx2';
ALTER INDEX idx2 ON temp_tbl2 REBUILD;

-- add a partitioned table, index and view
CREATE TABLE part_tab (id INT, name STRING)  PARTITIONED BY (ds string);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE part_tab PARTITION (ds='2008-04-09');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE part_tab PARTITION (ds='2009-04-09');
CREATE INDEX idx3 ON TABLE part_tab(id) AS 'COMPACT' with DEFERRED REBUILD;
ALTER INDEX idx3 ON part_tab PARTITION (ds='2008-04-09') REBUILD;
ALTER INDEX idx3 ON part_tab PARTITION (ds='2009-04-09') REBUILD;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/part_tab2;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/part_tab2_idx4;
-- add a partitioned table, index and view with a different storage location
CREATE TABLE part_tab2 (id INT, name STRING)  PARTITIONED BY (ds string)
		LOCATION 'file:${system:test.tmp.dir}/dbcascade/part_tab2';
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE part_tab2 PARTITION (ds='2008-04-09');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE part_tab2 PARTITION (ds='2009-04-09');
CREATE INDEX idx4 ON TABLE part_tab2(id) AS 'COMPACT' with DEFERRED REBUILD
		LOCATION 'file:${system:test.tmp.dir}/dbcascade/part_tab2_idx4';
ALTER INDEX idx4 ON part_tab2 PARTITION (ds='2008-04-09') REBUILD;
ALTER INDEX idx4 ON part_tab2 PARTITION (ds='2009-04-09') REBUILD;


dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/part_tab3;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/part_tab3_p1;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/part_tab3_idx5;
-- add a partitioned table, index and view with a different storage location
CREATE TABLE part_tab3 (id INT, name STRING)  PARTITIONED BY (ds string)
		LOCATION 'file:${system:test.tmp.dir}/dbcascade/part_tab3';
ALTER TABLE part_tab3 ADD PARTITION  (ds='2007-04-09') LOCATION 'file:${system:test.tmp.dir}/dbcascade/part_tab3_p1';
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE part_tab3 PARTITION (ds='2008-04-09');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE part_tab3 PARTITION (ds='2009-04-09');
CREATE INDEX idx5 ON TABLE part_tab3(id) AS 'COMPACT' with DEFERRED REBUILD
		LOCATION 'file:${system:test.tmp.dir}/dbcascade/part_tab3_idx5';
ALTER INDEX idx5 ON part_tab3 PARTITION (ds='2008-04-09') REBUILD;
ALTER INDEX idx5 ON part_tab3 PARTITION (ds='2009-04-09') REBUILD;



dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/dbcascade/extab1;
dfs -touchz ${system:test.tmp.dir}/dbcascade/extab1/file1.txt;
-- add an external table
CREATE EXTERNAL TABLE extab1(id INT, name STRING) ROW FORMAT
              DELIMITED FIELDS TERMINATED BY ''
              LINES TERMINATED BY '\n'
              STORED AS TEXTFILE
              LOCATION 'file:${system:test.tmp.dir}/dbcascade/extab1';

-- add a table, create index (give a name for index table)
CREATE TABLE temp_tbl3 (id INT, name STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' into table temp_tbl3;
CREATE INDEX temp_tbl3_idx ON TABLE temp_tbl3(id) AS 'COMPACT' with DEFERRED REBUILD IN TABLE temp_tbl3_idx_tbl;
ALTER INDEX temp_tbl3_idx ON temp_tbl3 REBUILD;

-- drop the database with cascade
DROP DATABASE db5 CASCADE;

dfs -test -d ${system:test.tmp.dir}/dbcascade/extab1;
dfs -rmr ${system:test.tmp.dir}/dbcascade;
SHOW DATABASES;

-- Try to drop a database that does not exist
DROP DATABASE does_not_exist;
SHOW DATABASES;

-- Try to drop a non-empty database
CREATE DATABASE test_db;
USE test_db;
CREATE TABLE t(a INT);
USE default;
DROP DATABASE test_db;
SHOW DATABASES;

-- Try to drop a non-empty database in restrict mode
CREATE DATABASE db_drop_non_empty_restrict;
USE db_drop_non_empty_restrict;
CREATE TABLE t(a INT);
USE default;
DROP DATABASE db_drop_non_empty_restrict;
CREATE DATABASE db1;
DESCRIBE DATABASE EXTENDED db1;

USE db1;
CREATE TABLE table_db1 (name STRING, value INT);

DESCRIBE FORMATTED table_db1;
SHOW TABLES;

CREATE DATABASE db2
COMMENT 'database 2'
LOCATION '${hiveconf:hive.metastore.warehouse.dir}/db2';

DESCRIBE DATABASE EXTENDED db2;

USE db2;
CREATE TABLE table_db2 (name STRING, value INT);

DESCRIBE FORMATTED table_db2;
SHOW TABLES;
set datanucleus.cache.collections=false;
set datanucleus.cache.collections.lazy=false;

create database db1;

show databases;

create database db2 with dbproperties (
  'mapred.jobtracker.url'='http://my.jobtracker.com:53000',
  'hive.warehouse.dir' = '/user/hive/warehouse',
  'mapred.scratch.dir' = 'hdfs://tmp.dfs.com:50029/tmp');

describe database db2;

describe database extended db2;


set datanucleus.cache.collections=false;
set datanucleus.cache.collections.lazy=false;

alter database db2 set dbproperties (
  'new.property' = 'some new props',
  'hive.warehouse.dir' = 'new/warehouse/dir');

describe database extended db2;

SHOW DATABASES;

-- Try to switch to a database that does not exist
USE does_not_exist;
set hive.fetch.task.conversion=more;

drop table date_1;

create table date_1 (d date);

insert overwrite table date_1
  select cast('2011-01-01' as date) from src tablesample (1 rows);

select * from date_1 limit 1;
select d, count(d) from date_1 group by d;

insert overwrite table date_1
  select date '2011-01-01' from src tablesample (1 rows);

select * from date_1 limit 1;
select d, count(d) from date_1 group by d;

insert overwrite table date_1
  select cast(cast('2011-01-01 00:00:00' as timestamp) as date) from src tablesample (1 rows);

select * from date_1 limit 1;
select d, count(d) from date_1 group by d;

-- Valid casts
select
  cast('2012-01-01' as string),
  cast(d as string),
  cast(d as timestamp),
  cast(cast(d as timestamp) as date),
  cast(d as date)
from date_1 limit 1;

-- Invalid casts.
select
  cast(d as boolean),
  cast(d as tinyint),
  cast(d as smallint),
  cast(d as int),
  cast(d as bigint),
  cast(d as float),
  cast(d as double)
from date_1 limit 1;

-- These comparisons should all be true
select
  date '2011-01-01' = date '2011-01-01',
  unix_timestamp(date '2011-01-01') = unix_timestamp(date '2011-01-01'),
  unix_timestamp(date '2011-01-01') = unix_timestamp(cast(date '2011-01-01' as timestamp)),
  unix_timestamp(date '2011-01-01') = unix_timestamp(cast(cast('2011-01-01 12:13:14' as timestamp) as date)),
  unix_timestamp(date '2011-01-01') < unix_timestamp(cast('2011-01-01 00:00:01' as timestamp)),
  unix_timestamp(date '2011-01-01') = unix_timestamp(cast('2011-01-01 00:00:00' as timestamp)),
  unix_timestamp(date '2011-01-01') > unix_timestamp(cast('2010-12-31 23:59:59' as timestamp)),
  date '2011-01-01' = cast(timestamp('2011-01-01 23:24:25') as date),
  '2011-01-01' = cast(d as string),
  '2011-01-01' = cast(date '2011-01-01' as string)
from date_1 limit 1;

select
  date('2001-01-28'),
  date('2001-02-28'),
  date('2001-03-28'),
  date('2001-04-28'),
  date('2001-05-28'),
  date('2001-06-28'),
  date('2001-07-28'),
  date('2001-08-28'),
  date('2001-09-28'),
  date('2001-10-28'),
  date('2001-11-28'),
  date('2001-12-28')
from date_1 limit 1;

select
  unix_timestamp(date('2001-01-28')) = unix_timestamp(cast('2001-01-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-02-28')) = unix_timestamp(cast('2001-02-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-03-28')) = unix_timestamp(cast('2001-03-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-04-28')) = unix_timestamp(cast('2001-04-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-05-28')) = unix_timestamp(cast('2001-05-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-06-28')) = unix_timestamp(cast('2001-06-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-07-28')) = unix_timestamp(cast('2001-07-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-08-28')) = unix_timestamp(cast('2001-08-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-09-28')) = unix_timestamp(cast('2001-09-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-10-28')) = unix_timestamp(cast('2001-10-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-11-28')) = unix_timestamp(cast('2001-11-28 0:0:0' as timestamp)),
  unix_timestamp(date('2001-12-28')) = unix_timestamp(cast('2001-12-28 0:0:0' as timestamp))
from date_1 limit 1;

drop table date_1;
set hive.mapred.mode=nonstrict;
drop table if exists date_2;

create table date_2 (
  ORIGIN_CITY_NAME string,
  DEST_CITY_NAME string,
  FL_DATE date,
  ARR_DELAY float,
  FL_NUM int
);

LOAD DATA LOCAL INPATH '../../data/files/flights_tiny.txt.1' OVERWRITE INTO TABLE date_2;

select fl_date, fl_num from date_2 order by fl_date asc, fl_num desc;
select fl_date, fl_num from date_2 order by fl_date desc, fl_num asc;

select fl_date, count(*) from date_2 group by fl_date;

drop table date_2;
drop table date_3;

create table date_3 (
  c1 int
);

alter table date_3 add columns (c2 date);

insert overwrite table date_3
  select 1, cast(cast('2011-01-01 00:00:00' as timestamp) as date) from src tablesample (1 rows);

select * from date_3;

drop table date_3;
set hive.fetch.task.conversion=more;

drop table date_4;

create table date_4 (d date);
alter table date_4 set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';

-- Test date literal syntax
insert overwrite table date_4
  select date '2011-01-01' from src tablesample (1 rows);
select d, date '2011-01-01' from date_4 limit 1;

drop table date_4;
set hive.fetch.task.conversion=more;

-- Comparisons against same value
select cast('2011-05-06' as date) >
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-06' as date) <
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-06' as date) =
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-06' as date) <>
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-06' as date) >=
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-06' as date) <=
  cast('2011-05-06' as date) from src limit 1;

-- Now try with differing values
select cast('2011-05-05' as date) >
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-05' as date) <
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-05' as date) =
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-05' as date) <>
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-05' as date) >=
  cast('2011-05-06' as date) from src limit 1;

select cast('2011-05-05' as date) <=
  cast('2011-05-06' as date) from src limit 1;

drop table date_join1;

-- SORT_QUERY_RESULTS

create table date_join1 (
  ORIGIN_CITY_NAME string,
  DEST_CITY_NAME string,
  FL_DATE date,
  ARR_DELAY float,
  FL_NUM int
);

LOAD DATA LOCAL INPATH '../../data/files/flights_join.txt' OVERWRITE INTO TABLE date_join1;

-- Note that there are 2 rows with date 2000-11-28, so we should expect 4 rows with that date in the join results
select t1.fl_num, t1.fl_date, t2.fl_num, t2.fl_date
  from date_join1 t1
  join date_join1 t2
  on (t1.fl_date = t2.fl_date);

drop table date_join1;
-- Not in YYYY-MM-DD format
SELECT DATE '2001/01/01' FROM src LIMIT 2;
-- Invalid date value
SELECT DATE '2001-01-32' FROM src;
drop table if exists date_serde_regex;
drop table date_serde_lb;
drop table date_serde_ls;
drop table date_serde_c;
drop table date_serde_lbc;
drop table date_serde_orc;


--
-- RegexSerDe
--
create table date_serde_regex (
  ORIGIN_CITY_NAME string,
  DEST_CITY_NAME string,
  FL_DATE date,
  ARR_DELAY float,
  FL_NUM int
)
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe'
with serdeproperties (
  "input.regex" = "([^]*)([^]*)([^]*)([^]*)([0-9]*)"
)
stored as textfile;

load data local inpath '../../data/files/flights_tiny.txt.1' overwrite into table date_serde_regex;

select * from date_serde_regex;
select fl_date, count(*) from date_serde_regex group by fl_date;

--
-- LazyBinary
--
create table date_serde_lb (
  c1 date,
  c2 int
);
alter table date_serde_lb set serde 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe';

insert overwrite table date_serde_lb
  select fl_date, fl_num from date_serde_regex limit 1;

select * from date_serde_lb;
select c1, sum(c2) from date_serde_lb group by c1;

--
-- LazySimple
--
create table date_serde_ls (
  c1 date,
  c2 int
);
alter table date_serde_ls set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';

insert overwrite table date_serde_ls
  select c1, c2 from date_serde_lb limit 1;

select * from date_serde_ls;
select c1, sum(c2) from date_serde_ls group by c1;

--
-- Columnar
--
create table date_serde_c (
  c1 date,
  c2 int
) stored as rcfile;
alter table date_serde_c set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';

insert overwrite table date_serde_c
  select c1, c2 from date_serde_ls limit 1;

select * from date_serde_c;
select c1, sum(c2) from date_serde_c group by c1;

--
-- LazyBinaryColumnar
--
create table date_serde_lbc (
  c1 date,
  c2 int
) stored as rcfile;
alter table date_serde_lbc set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';

insert overwrite table date_serde_lbc
  select c1, c2 from date_serde_c limit 1;

select * from date_serde_lbc;
select c1, sum(c2) from date_serde_lbc group by c1;

--
-- ORC
--
create table date_serde_orc (
  c1 date,
  c2 int
) stored as orc;
alter table date_serde_orc set serde 'org.apache.hadoop.hive.ql.io.orc.OrcSerde';

insert overwrite table date_serde_orc
  select c1, c2 from date_serde_lbc limit 1;

select * from date_serde_orc;
select c1, sum(c2) from date_serde_orc group by c1;



drop table date_serde_regex;
drop table date_serde_lb;
drop table date_serde_ls;
drop table date_serde_c;
drop table date_serde_lbc;
drop table date_serde_orc;
drop table date_udf;
drop table date_udf_string;
drop table date_udf_flight;

create table date_udf (d date);
create table date_udf_string (d string);
from src
  insert overwrite table date_udf
    select '2011-05-06' limit 1
  insert overwrite table date_udf_string
    select '2011-05-06' limit 1;

create table date_udf_flight (
  ORIGIN_CITY_NAME string,
  DEST_CITY_NAME string,
  FL_DATE date,
  ARR_DELAY float,
  FL_NUM int
);
LOAD DATA LOCAL INPATH '../../data/files/flights_tiny.txt.1' OVERWRITE INTO TABLE date_udf_flight;

-- Test UDFs with date input
select unix_timestamp(d), year(d), month(d), day(d), dayofmonth(d),
    weekofyear(d), to_date(d)
  from date_udf;

select date_add(d, 5), date_sub(d, 10)
  from date_udf;

select datediff(d, d), datediff(d, '2002-03-21'), datediff('2002-03-21', d),
    datediff(cast ('2002-03-21 00:00:00' as timestamp), d),
    datediff(d, cast ('2002-03-21 00:00:00' as timestamp))
  from date_udf;

-- Test UDFs with string input
select unix_timestamp(d), year(d), month(d), day(d), dayofmonth(d),
    weekofyear(d), to_date(d)
  from date_udf_string;

select date_add(d, 5), date_sub(d, 10)  from date_udf_string;

select datediff(d, d), datediff(d, '2002-03-21'), datediff('2002-03-21', d),
    datediff('2002-03-21 00:00:00', d),
    datediff(d, '2002-03-21 00:00:00')
  from date_udf_string;

select
    to_utc_timestamp(date '1970-01-01', 'America/Los_Angeles'),
    from_utc_timestamp(date '1970-01-01', 'America/Los_Angeles'),
    to_utc_timestamp(date '2013-06-19', 'America/Los_Angeles'),
    from_utc_timestamp(date '2013-06-19', 'America/Los_Angeles')
  from date_udf;

-- should all be true
select
    to_utc_timestamp(date '1970-01-01', 'America/Los_Angeles') = to_utc_timestamp(timestamp('1970-01-01 00:00:00'), 'America/Los_Angeles'),
    from_utc_timestamp(date '1970-01-01', 'America/Los_Angeles') = from_utc_timestamp(timestamp('1970-01-01 00:00:00'), 'America/Los_Angeles'),
    to_utc_timestamp(date '2013-06-19', 'America/Los_Angeles') = to_utc_timestamp(timestamp('2013-06-19 00:00:00'), 'America/Los_Angeles'),
    from_utc_timestamp(date '2013-06-19', 'America/Los_Angeles') = from_utc_timestamp(timestamp('2013-06-19 00:00:00'), 'America/Los_Angeles')
  from date_udf;

-- Aggregation functions (min/max)
select min(fl_date) from date_udf_flight;
select max(fl_date) from date_udf_flight;


drop table date_udf;
drop table date_udf_string;
drop table date_udf_flight;
set hive.mapred.mode=nonstrict;
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION dboutput AS 'org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput';

set mapred.map.tasks.speculative.execution=false;
set mapred.reduce.tasks.speculative.execution=false;
set mapred.map.tasks=1;
set mapred.reduce.tasks=1;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;

ADD JAR ${system:maven.local.repository}/org/apache/derby/derby/${system:derby.version}/derby-${system:derby.version}.jar;

DESCRIBE FUNCTION dboutput;

DESCRIBE FUNCTION EXTENDED dboutput;

EXPLAIN FROM src

SELECT dboutput ( 'jdbc:derby:../build/test_dboutput_db\;create=true','','',
'CREATE TABLE app_info ( kkey VARCHAR(255) NOT NULL, vvalue VARCHAR(255) NOT NULL, UNIQUE(kkey))' ),

dboutput('jdbc:derby:../build/test_dboutput_db','','',
'INSERT INTO app_info (kkey,vvalue) VALUES (?,?)','20','a'),

dboutput('jdbc:derby:../build/test_dboutput_db','','',
'INSERT INTO app_info (kkey,vvalue) VALUES (?,?)','20','b')

limit 1;


FROM src

SELECT dboutput ( 'jdbc:derby:../build/test_dboutput_db\;create=true','','',
'CREATE TABLE app_info ( kkey INTEGER NOT NULL, vvalue VARCHAR(255) NOT NULL, UNIQUE(kkey))' ),

dboutput('jdbc:derby:../build/test_dboutput_db','','',
'INSERT INTO app_info (kkey,vvalue) VALUES (?,?)','20','a'),

dboutput('jdbc:derby:../build/test_dboutput_db','','',
'INSERT INTO app_info (kkey,vvalue) VALUES (?,?)','20','b')

limit 1;

EXPLAIN SELECT

dboutput('jdbc:derby:../build/test_dboutput_db','','',
'INSERT INTO app_info (kkey,vvalue) VALUES (?,?)',key,value)

FROM src WHERE key < 10;


SELECT

dboutput('jdbc:derby:../build/test_dboutput_db','','',
'INSERT INTO app_info (kkey,vvalue) VALUES (?,?)',key,value)

FROM src WHERE key < 10;

dfs -rmr ../build/test_dboutput_db;
dfs -rmr derby.log;

DROP TEMPORARY FUNCTION dboutput;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create table T1(key string, val string) stored as textfile;

alter table T1 compact 'major';

alter table T1 compact 'minor';

drop table T1;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create table T1(key string, val string) partitioned by (ds string) stored as textfile;

alter table T1 add partition (ds = 'today');
alter table T1 add partition (ds = 'yesterday');

alter table T1 partition (ds = 'today') compact 'major';

alter table T1 partition (ds = 'yesterday') compact 'minor';

drop table T1;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create database D1;

use D1;

create table T1(key string, val string) stored as textfile;

alter table T1 compact 'major';

alter table T1 compact 'minor';

drop table T1;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create database D1;

alter database D1 set dbproperties('test'='yesthisis');

drop database D1;

create table T1(key string, val string) stored as textfile;

create table T2 like T1;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

select * from T1;

create table T3 as select * from T1;

create table T4 (key char(10), val decimal(5,2), b int)
    partitioned by (ds string)
    clustered by (b) into 10 buckets
    stored as orc;

alter table T3 rename to newT3;

alter table T2 set tblproperties ('test'='thisisatest');

alter table T2 set serde 'org.apache.hadoop.hive.ql.io.orc.OrcSerde';
alter table T2 set serdeproperties ('test'='thisisatest');

alter table T2 clustered by (key) into 32 buckets;

alter table T4 add partition (ds='today');

alter table T4 partition (ds='today') rename to partition(ds='yesterday');

alter table T4 drop partition (ds='yesterday');

alter table T4 add partition (ds='tomorrow');

create table T5 (a string, b int);
alter table T5 set fileformat orc;

create table T7 (a string, b int);
alter table T7 set location 'file:///tmp';
alter table T4 partition (ds='tomorrow') set location 'file:///tmp';

alter table T2 touch;
alter table T4 touch partition (ds='tomorrow');

create view V1 as select key from T1;
alter view V1 set tblproperties ('test'='thisisatest');
drop view V1;



drop table T1;
drop table T2;
drop table newT3;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

drop database if exists drop_nodblock;
create database drop_nodblock;
lock database drop_nodblock shared;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

drop database if exists drop_nodbunlock;
create database drop_nodbunlock;
unlock database drop_nodbunlock;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

drop table if exists drop_notablelock;
create table drop_notablelock (c int);
lock table drop_notablelock shared;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

drop table if exists drop_notableunlock;
create table drop_notableunlock (c int);
unlock table drop_notableunlock;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create table T1(key string, val string) stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

select * from T1;

create table T2(key string, val string) stored as textfile;

insert into table T2 select * from T1;

select * from T2;

drop table T1;
drop table T2;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create table T1(key string, val string) stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

select * from T1;

create table T2(key string, val string) stored as textfile;

insert overwrite table T2 select * from T1;

select * from T2;

drop table T1;
drop table T2;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create table T1(key string, val string) stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

select * from T1;

create table T2(key string, val string) partitioned by (pval string) stored as textfile;

insert into table T2 partition (pval = '1') select * from T1;

select * from T2;

insert overwrite table T2 partition (pval = '1') select * from T1;

select * from T2;

drop table T1;
drop table T2;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

create table T1(key string, val string) stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

select * from T1;

create table T2(key string) partitioned by (val string) stored as textfile;

insert overwrite table T2 partition (val) select key, val from T1;

select * from T2;

drop table T1;
drop table T2;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

create database foo;

use foo;

create table T1(key string, val string) partitioned by (ds string) stored as textfile;

alter table T1 add partition (ds='today');

create view V1 as select key from T1;

show tables;

describe T1;

drop view V1;

drop table T1;

show databases;

drop database foo;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

show locks;

show locks extended;

show locks default;

show transactions;
set hive.fetch.task.conversion=more;

drop table if exists decimal_1;

create table decimal_1 (t decimal(4,2), u decimal(5), v decimal);
alter table decimal_1 set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';

desc decimal_1;

insert overwrite table decimal_1
  select cast('17.29' as decimal(4,2)), 3.1415926BD, 3115926.54321BD from src tablesample (1 rows);
select cast(t as boolean) from decimal_1;
select cast(t as tinyint) from decimal_1;
select cast(t as smallint) from decimal_1;
select cast(t as int) from decimal_1;
select cast(t as bigint) from decimal_1;
select cast(t as float) from decimal_1;
select cast(t as double) from decimal_1;
select cast(t as string) from decimal_1;
select cast(t as timestamp) from decimal_1;

drop table decimal_1;
DROP TABLE IF EXISTS `DECIMAL`;

CREATE TABLE `DECIMAL` (dec decimal);

LOAD DATA LOCAL INPATH '../../data/files/decimal_10_0.txt' OVERWRITE INTO TABLE `DECIMAL`;

SELECT dec FROM `DECIMAL`;

DROP TABLE `DECIMAL`;set hive.mapred.mode=nonstrict;
drop table if exists decimal_1_1;

create table decimal_1_1 (d decimal(1,1));
load data local inpath '../../data/files/decimal_1_1.txt' into table decimal_1_1;
select * from decimal_1_1;

select d from decimal_1_1 order by d desc;

drop table decimal_1_1;
set hive.fetch.task.conversion=more;

drop table decimal_2;

create table decimal_2 (t decimal(18,9));
alter table decimal_2 set serde 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe';

insert overwrite table decimal_2
  select cast('17.29' as decimal(4,2)) from src tablesample (1 rows);

select cast(t as boolean) from decimal_2;
select cast(t as tinyint) from decimal_2;
select cast(t as smallint) from decimal_2;
select cast(t as int) from decimal_2;
select cast(t as bigint) from decimal_2;
select cast(t as float) from decimal_2;
select cast(t as double) from decimal_2;
select cast(t as string) from decimal_2;

insert overwrite table decimal_2
  select cast('3404045.5044003' as decimal(18,9)) from src tablesample (1 rows);

select cast(t as boolean) from decimal_2;
select cast(t as tinyint) from decimal_2;
select cast(t as smallint) from decimal_2;
select cast(t as int) from decimal_2;
select cast(t as bigint) from decimal_2;
select cast(t as float) from decimal_2;
select cast(t as double) from decimal_2;
select cast(t as string) from decimal_2;

select cast(3.14 as decimal(4,2)) from decimal_2;
select cast(cast(3.14 as float) as decimal(4,2)) from decimal_2;
select cast(cast('2012-12-19 11:12:19.1234567' as timestamp) as decimal(30,8)) from decimal_2;
select cast(true as decimal) from decimal_2;
select cast(3Y as decimal) from decimal_2;
select cast(3S as decimal) from decimal_2;
select cast(cast(3 as int) as decimal) from decimal_2;
select cast(3L as decimal) from decimal_2;
select cast(0.99999999999999999999 as decimal(20,19)) from decimal_2;
select cast('0.99999999999999999999' as decimal(20,20)) from decimal_2;
drop table decimal_2;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_3;

CREATE TABLE DECIMAL_3(key decimal(38,18), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_3;

SELECT * FROM DECIMAL_3 ORDER BY key, value;

SELECT * FROM DECIMAL_3 ORDER BY key DESC, value DESC;

SELECT * FROM DECIMAL_3 ORDER BY key, value;

SELECT DISTINCT key FROM DECIMAL_3 ORDER BY key;

SELECT key, sum(value) FROM DECIMAL_3 GROUP BY key ORDER BY key;

SELECT value, sum(key) FROM DECIMAL_3 GROUP BY value ORDER BY value;

SELECT * FROM DECIMAL_3 a JOIN DECIMAL_3 b ON (a.key = b.key) ORDER BY a.key, a.value, b.value;

SELECT * FROM DECIMAL_3 WHERE key=3.14 ORDER BY key, value;

SELECT * FROM DECIMAL_3 WHERE key=3.140 ORDER BY key, value;

DROP TABLE DECIMAL_3;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_4_1;
DROP TABLE IF EXISTS DECIMAL_4_2;

CREATE TABLE DECIMAL_4_1(key decimal(35,25), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

CREATE TABLE DECIMAL_4_2(key decimal(35,25), value decimal(35,25))
STORED AS ORC;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_4_1;

INSERT OVERWRITE TABLE DECIMAL_4_2 SELECT key, key * 3 FROM DECIMAL_4_1;

SELECT * FROM DECIMAL_4_1 ORDER BY key, value;

SELECT * FROM DECIMAL_4_2 ORDER BY key;

DROP TABLE DECIMAL_4_1;
DROP TABLE DECIMAL_4_2;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_5;

CREATE TABLE DECIMAL_5(key decimal(10,5), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_5;

SELECT key FROM DECIMAL_5 ORDER BY key;

SELECT DISTINCT key FROM DECIMAL_5 ORDER BY key;

SELECT cast(key as decimal) FROM DECIMAL_5;

SELECT cast(key as decimal(6,3)) FROM DECIMAL_5;

DROP TABLE DECIMAL_5;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_6_1;
DROP TABLE IF EXISTS DECIMAL_6_2;
DROP TABLE IF EXISTS DECIMAL_6_3;

CREATE TABLE DECIMAL_6_1(key decimal(10,5), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

CREATE TABLE DECIMAL_6_2(key decimal(17,4), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv9.txt' INTO TABLE DECIMAL_6_1;
LOAD DATA LOCAL INPATH '../../data/files/kv9.txt' INTO TABLE DECIMAL_6_2;

SELECT T.key from (
  SELECT key, value from DECIMAL_6_1
  UNION ALL
  SELECT key, value from DECIMAL_6_2
) T order by T.key;

CREATE TABLE DECIMAL_6_3 AS SELECT key + 5.5 AS k, value * 11 AS v from DECIMAL_6_1 ORDER BY v;

desc DECIMAL_6_3;

-- HIVE-5292 Join on decimal columns fails
-- SORT_QUERY_RESULTS

create table src_dec (key decimal(3,0), value string);
load data local inpath '../../data/files/kv1.txt' into table src_dec;

select * from src_dec a join src_dec b on a.key=b.key+450;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_3_txt;
DROP TABLE IF EXISTS DECIMAL_3;

CREATE TABLE DECIMAL_3_txt(key decimal(38,18), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_3_txt;

CREATE TABLE DECIMAL_3 STORED AS ORC AS SELECT * FROM DECIMAL_3_txt;

set hive.auto.convert.join=false;
EXPLAIN
SELECT * FROM DECIMAL_3 a JOIN DECIMAL_3 b ON (a.key = b.key) ORDER BY a.key, a.value, b.key, b.value;

SELECT * FROM DECIMAL_3 a JOIN DECIMAL_3 b ON (a.key = b.key) ORDER BY a.key, a.value, b.key, b.value;

set hive.auto.convert.join=true;
EXPLAIN
SELECT * FROM DECIMAL_3 a JOIN DECIMAL_3 b ON (a.key = b.key) ORDER BY a.key, a.value, b.key, b.value;

SELECT * FROM DECIMAL_3 a JOIN DECIMAL_3 b ON (a.key = b.key) ORDER BY a.key, a.value, b.key, b.value;

DROP TABLE DECIMAL_3_txt;
DROP TABLE DECIMAL_3;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_PRECISION;

CREATE TABLE DECIMAL_PRECISION(dec decimal(20,10))
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv8.txt' INTO TABLE DECIMAL_PRECISION;

SELECT * FROM DECIMAL_PRECISION ORDER BY dec;

SELECT dec, dec + 1, dec - 1 FROM DECIMAL_PRECISION ORDER BY dec;
SELECT dec, dec * 2, dec / 3  FROM DECIMAL_PRECISION ORDER BY dec;
SELECT dec, dec / 9 FROM DECIMAL_PRECISION ORDER BY dec;
SELECT dec, dec / 27 FROM DECIMAL_PRECISION ORDER BY dec;
SELECT dec, dec * dec FROM DECIMAL_PRECISION ORDER BY dec;

EXPLAIN SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION;
SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION;

SELECT dec * cast('12345678901234567890.12345678' as decimal(38,18)) FROM DECIMAL_PRECISION LIMIT 1;
SELECT * from DECIMAL_PRECISION WHERE dec > cast('1234567890123456789012345678.12345678' as decimal(38,18)) LIMIT 1;
SELECT dec * 12345678901234567890.12345678 FROM DECIMAL_PRECISION LIMIT 1;

SELECT MIN(cast('12345678901234567890.12345678' as decimal(38,18))) FROM DECIMAL_PRECISION;
SELECT COUNT(cast('12345678901234567890.12345678' as decimal(38,18))) FROM DECIMAL_PRECISION;

DROP TABLE DECIMAL_PRECISION;
DROP TABLE IF EXISTS DECIMAL_PRECISION;

CREATE TABLE DECIMAL_PRECISION(dec decimal)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

SELECT dec * 123456789012345678901234567890.123456789bd FROM DECIMAL_PRECISION;

DROP TABLE DECIMAL_PRECISION;

explain select 100.001BD;

explain select 100.000BD;

explain select 0.000BD;

explain select 0.100BD;

explain select 0.010BD;

explain select cast(0.010 as decimal(6,3));

explain select 0.09765625BD * 0.09765625BD * 0.0125BD * 578992BD;
select 0.09765625BD * 0.09765625BD * 0.0125BD * 578992BD;
DROP TABLE IF EXISTS DECIMAL_PRECISION;

CREATE TABLE DECIMAL_PRECISION(dec decimal)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

SELECT * from DECIMAL_PRECISION WHERE dec > 1234567890123456789.0123456789bd;

DROP TABLE DECIMAL_PRECISION;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_TEXT;
DROP TABLE IF EXISTS DECIMAL_RC;
DROP TABLE IF EXISTS DECIMAL_LAZY_COL;
DROP TABLE IF EXISTS DECIMAL_SEQUENCE;

CREATE TABLE DECIMAL_TEXT (key decimal, value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_TEXT;

SELECT * FROM DECIMAL_TEXT ORDER BY key, value;

CREATE TABLE DECIMAL_RC
STORED AS RCFile AS
SELECT * FROM DECIMAL_TEXT;
describe formatted DECIMAL_RC;

CREATE TABLE DECIMAL_LAZY_COL
ROW FORMAT SERDE "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe"
STORED AS RCFile AS
SELECT * FROM DECIMAL_RC;

describe formatted DECIMAL_LAZY_COL;

CREATE TABLE DECIMAL_SEQUENCE
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\001'
COLLECTION ITEMS TERMINATED BY '\002'
MAP KEYS TERMINATED BY '\003'
STORED AS SEQUENCEFILE AS
SELECT * FROM DECIMAL_LAZY_COL ORDER BY key;

SELECT * FROM DECIMAL_SEQUENCE ORDER BY key, value;

DROP TABLE IF EXISTS DECIMAL_TEXT;
DROP TABLE IF EXISTS DECIMAL_RC;
DROP TABLE IF EXISTS DECIMAL_LAZY_COL;
DROP TABLE IF EXISTS DECIMAL_SEQUENCE;
set hive.stats.fetch.column.stats=true;
drop table if exists decimal_1;

create table decimal_1 (t decimal(4,2), u decimal(5), v decimal);

desc decimal_1;

insert overwrite table decimal_1
  select cast('17.29' as decimal(4,2)), 3.1415926BD, null from src;

analyze table decimal_1 compute statistics for columns;

desc formatted decimal_1 v;

explain select * from decimal_1 order by 1 limit 100;
drop table decimal_1;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_TRAILING;

CREATE TABLE DECIMAL_TRAILING (
  id int,
  a decimal(10,4),
  b decimal(15,8)
  )
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv10.txt' INTO TABLE DECIMAL_TRAILING;

SELECT * FROM DECIMAL_TRAILING ORDER BY id;

DROP TABLE DECIMAL_TRAILING;
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

DROP TABLE IF EXISTS DECIMAL_UDF;

CREATE TABLE DECIMAL_UDF (key decimal(20,10), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF;

-- addition
EXPLAIN SELECT key + key FROM DECIMAL_UDF;
SELECT key + key FROM DECIMAL_UDF;

EXPLAIN SELECT key + value FROM DECIMAL_UDF;
SELECT key + value FROM DECIMAL_UDF;

EXPLAIN SELECT key + (value/2) FROM DECIMAL_UDF;
SELECT key + (value/2) FROM DECIMAL_UDF;

EXPLAIN SELECT key + '1.0' FROM DECIMAL_UDF;
SELECT key + '1.0' FROM DECIMAL_UDF;

-- substraction
EXPLAIN SELECT key - key FROM DECIMAL_UDF;
SELECT key - key FROM DECIMAL_UDF;

EXPLAIN SELECT key - value FROM DECIMAL_UDF;
SELECT key - value FROM DECIMAL_UDF;

EXPLAIN SELECT key - (value/2) FROM DECIMAL_UDF;
SELECT key - (value/2) FROM DECIMAL_UDF;

EXPLAIN SELECT key - '1.0' FROM DECIMAL_UDF;
SELECT key - '1.0' FROM DECIMAL_UDF;

-- multiplication
EXPLAIN SELECT key * key FROM DECIMAL_UDF;
SELECT key * key FROM DECIMAL_UDF;

EXPLAIN SELECT key, value FROM DECIMAL_UDF where key * value > 0;
SELECT key, value FROM DECIMAL_UDF where key * value > 0;

EXPLAIN SELECT key * value FROM DECIMAL_UDF;
SELECT key * value FROM DECIMAL_UDF;

EXPLAIN SELECT key * (value/2) FROM DECIMAL_UDF;
SELECT key * (value/2) FROM DECIMAL_UDF;

EXPLAIN SELECT key * '2.0' FROM DECIMAL_UDF;
SELECT key * '2.0' FROM DECIMAL_UDF;

-- division
EXPLAIN SELECT key / 0 FROM DECIMAL_UDF limit 1;
SELECT key / 0 FROM DECIMAL_UDF limit 1;

EXPLAIN SELECT key / NULL FROM DECIMAL_UDF limit 1;
SELECT key / NULL FROM DECIMAL_UDF limit 1;

EXPLAIN SELECT key / key FROM DECIMAL_UDF WHERE key is not null and key <> 0;
SELECT key / key FROM DECIMAL_UDF WHERE key is not null and key <> 0;

EXPLAIN SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0;
SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0;

EXPLAIN SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0;
SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0;

EXPLAIN SELECT 1 + (key / '2.0') FROM DECIMAL_UDF;
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF;

-- abs
EXPLAIN SELECT abs(key) FROM DECIMAL_UDF;
SELECT abs(key) FROM DECIMAL_UDF;

-- avg
EXPLAIN SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF GROUP BY value ORDER BY value;
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF GROUP BY value ORDER BY value;

-- negative
EXPLAIN SELECT -key FROM DECIMAL_UDF;
SELECT -key FROM DECIMAL_UDF;

-- positive
EXPLAIN SELECT +key FROM DECIMAL_UDF;
SELECT +key FROM DECIMAL_UDF;

-- ceiling
EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF;
SELECT CEIL(key) FROM DECIMAL_UDF;

-- floor
EXPLAIN SELECT FLOOR(key) FROM DECIMAL_UDF;
SELECT FLOOR(key) FROM DECIMAL_UDF;

-- round
EXPLAIN SELECT ROUND(key, 2) FROM DECIMAL_UDF;
SELECT ROUND(key, 2) FROM DECIMAL_UDF;

-- power
EXPLAIN SELECT POWER(key, 2) FROM DECIMAL_UDF;
SELECT POWER(key, 2) FROM DECIMAL_UDF;

-- modulo
EXPLAIN SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF;
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF;

-- stddev, var
EXPLAIN SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF GROUP BY value;
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF GROUP BY value;

-- stddev_samp, var_samp
EXPLAIN SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value;
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value;

-- histogram
EXPLAIN SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF;
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF;

-- min
EXPLAIN SELECT MIN(key) FROM DECIMAL_UDF;
SELECT MIN(key) FROM DECIMAL_UDF;

-- max
EXPLAIN SELECT MAX(key) FROM DECIMAL_UDF;
SELECT MAX(key) FROM DECIMAL_UDF;

-- count
EXPLAIN SELECT COUNT(key) FROM DECIMAL_UDF;
SELECT COUNT(key) FROM DECIMAL_UDF;

DROP TABLE IF EXISTS DECIMAL_UDF;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS DECIMAL_UDF2;

CREATE TABLE DECIMAL_UDF2 (key decimal(20,10), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF2;

EXPLAIN
SELECT acos(key), asin(key), atan(key), cos(key), sin(key), tan(key), radians(key)
FROM DECIMAL_UDF2 WHERE key = 10;

SELECT acos(key), asin(key), atan(key), cos(key), sin(key), tan(key), radians(key)
FROM DECIMAL_UDF2 WHERE key = 10;

EXPLAIN
SELECT
  exp(key), ln(key),
  log(key), log(key, key), log(key, value), log(value, key),
  log10(key), sqrt(key)
FROM DECIMAL_UDF2 WHERE key = 10;

SELECT
  exp(key), ln(key),
  log(key), log(key, key), log(key, value), log(value, key),
  log10(key), sqrt(key)
FROM DECIMAL_UDF2 WHERE key = 10;

DROP TABLE IF EXISTS DECIMAL_UDF2;
create table t (c int);

set hive.default.fileformat.managed=orc;

create table o (c int);

create external table e (c int) location 'pfile://${system:test.tmp.dir}/foo';

create table i (c int) location 'pfile://${system:test.tmp.dir}/bar';

set hive.default.fileformat=orc;

create table io (c int);

describe formatted t;
describe formatted o;
describe formatted io;
describe formatted e;
describe formatted i;

drop table t;
drop table o;
drop table io;
drop table e;
drop table i;

set hive.default.fileformat=TextFile;
set hive.default.fileformat.managed=none;
create table default_partition_name (key int, value string) partitioned by (ds string);

set hive.exec.default.partition.name='some_other_default_partition_name';

alter table default_partition_name add partition(ds='__HIVE_DEFAULT_PARTITION__');

show partitions default_partition_name;
create table default_partition_name (key int, value string) partitioned by (ds string);

alter table default_partition_name add partition(ds='__HIVE_DEFAULT_PARTITION__');

ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
DELETE JAR ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
CREATE TABLE DELETEJAR(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe' STORED AS TEXTFILE;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_danp(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_danp select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint < 0 order by cint limit 10;

select a,b from acid_danp order by a;

delete from acid_danp;

select a,b from acid_danp;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_dap(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_dap partition (ds='today') select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null and cint < 0 order by cint limit 10;
insert into table acid_dap partition (ds='tomorrow') select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null and cint > 1000 order by cint limit 10;

select a,b,ds from acid_dap order by a,b;

delete from acid_dap;

select * from acid_dap;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;


create table not_an_acid_table2(a int, b varchar(128));

insert into table not_an_acid_table2 select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select a,b from not_an_acid_table2 order by a;

delete from not_an_acid_table2 where b = '0ruyd6Y50JpdGRf6HqD';
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;

create table foo(a int, b varchar(128)) clustered by (a) into 1 buckets stored as orc;

delete from foo;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/delete_orig_table;
dfs -copyFromLocal ../../data/files/alltypesorc ${system:test.tmp.dir}/delete_orig_table/00000_0;

create table acid_dot(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cstring2 STRING,
    ctimestamp1 TIMESTAMP,
    ctimestamp2 TIMESTAMP,
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN) clustered by (cint) into 1 buckets stored as orc location '${system:test.tmp.dir}/delete_orig_table' TBLPROPERTIES ('transactional'='true');

select count(*) from acid_dot;

delete from acid_dot where cint < -1070551679;

select count(*) from acid_dot;

dfs -rmr ${system:test.tmp.dir}/delete_orig_table;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_insertsort(a int, b varchar(128)) partitioned by (ds string) clustered by (a) sorted by (b) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

delete from acid_insertsort where a = 3;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create temporary table acid_dtt(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_dtt select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select * from acid_dtt order by a;

delete from acid_dtt where b = '0ruyd6Y50JpdGRf6HqD' or b = '2uLyD28144vklju213J1mr';

select a,b from acid_dtt order by b;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_dwnp(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_dwnp select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select * from acid_dwnp order by a;

delete from acid_dwnp where b = '0ruyd6Y50JpdGRf6HqD';

select a,b from acid_dwnp order by b;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_dwnm(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_dwnm select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select * from acid_dwnm order by a;

delete from acid_dwnm where b = 'nosuchvalue';

select a,b from acid_dwnm order by b;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_dwp(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_dwp partition (ds='today') select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null and cint < 0 order by cint limit 10;
insert into table acid_dwp partition (ds='tomorrow') select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null and cint > -10000000 order by cint limit 10;

select a,b,ds from acid_dwp order by a, ds;

delete from acid_dwp where a = '-1071363017';

select * from acid_dwp order by a, ds;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_dwhp(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_dwhp partition (ds='today') select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null and cint < 0 order by cint limit 10;
insert into table acid_dwhp partition (ds='tomorrow') select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null and cint > -10000000 order by cint limit 10;

select a,b,ds from acid_dwhp order by a, ds;

delete from acid_dwhp where ds = 'today';

select * from acid_dwhp order by a, ds;
create table impressions (imp string, msg string)
row format delimited
fields terminated by '\t'
lines terminated by '\n'
stored as textfile;
LOAD DATA LOCAL INPATH '../../data/files/in7.txt' INTO TABLE impressions;

select * from impressions;

select imp,msg from impressions;

drop table impressions;-- test comment indent processing for multi-line comments

CREATE TABLE test_table(
    col1 INT COMMENT 'col1 one line comment',
    col2 STRING COMMENT 'col2
two lines comment',
    col3 STRING COMMENT 'col3
three lines
comment')
COMMENT 'table comment
two lines';

DESCRIBE test_table;
DESCRIBE FORMATTED test_table;
-- HIVE-2905 showing non-ascii comments

create table dummy (col1 string, col2 string, col3 string);

alter table dummy change col1 col1 string comment '한글_col1';
alter table dummy change col2 col2 string comment '漢字_col2';
alter table dummy change col3 col3 string comment 'わご_col3';

DESCRIBE FORMATTED dummy;
create database test_db with dbproperties ('key1' = 'value1', 'key2' = 'value2');
desc database extended test_db;
desc schema extended test_db;
drop database test_db;
set hive.ddl.output.format=json;

CREATE DATABASE IF NOT EXISTS jsondb1 COMMENT 'Test database' LOCATION '${hiveconf:hive.metastore.warehouse.dir}/jsondb1' WITH DBPROPERTIES ('id' = 'jsondb1');

DESCRIBE DATABASE jsondb1;

DESCRIBE DATABASE EXTENDED jsondb1;

DESCRIBE SCHEMA jsondb1;

DESCRIBE SCHEMA EXTENDED jsondb1;

SHOW DATABASES;

SHOW DATABASES LIKE 'json*';

DROP DATABASE jsondb1;

CREATE DATABASE jsondb1;

DESCRIBE DATABASE jsondb1;

DESCRIBE DATABASE EXTENDED jsondb1;

DROP DATABASE jsondb1;

set hive.ddl.output.format=text;
DROP VIEW view_partitioned;

CREATE VIEW view_partitioned
PARTITIONED ON (value)
AS
SELECT key, value
FROM src
WHERE key=86;

ALTER VIEW view_partitioned
ADD PARTITION (value='val_86');

DESCRIBE FORMATTED view_partitioned PARTITION (value='val_86');

DROP VIEW view_partitioned;
set hive.ddl.output.format=json;

DROP VIEW view_partitioned;

CREATE VIEW view_partitioned
PARTITIONED ON (value)
AS
SELECT key, value
FROM src
WHERE key=86;

ALTER VIEW view_partitioned
ADD PARTITION (value='val_86');

DESCRIBE FORMATTED view_partitioned PARTITION (value='val_86');

DROP VIEW view_partitioned;
-- test comment indent processing for multi-line comments

CREATE TABLE test_table(
    col1 INT COMMENT 'col1 one line comment',
    col2 STRING COMMENT 'col2
two lines comment',
    col3 STRING COMMENT 'col3
three lines
comment',
    col4 STRING COMMENT 'col4 very long comment that is greater than 80 chars and is likely to spill into multiple lines',
    col5 STRING COMMENT 'col5 very long multi-line comment where each line is very long by itself and is likely to spill
into multiple lines.  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin in dolor nisl, sodales
adipiscing tortor. Integer venenatis',
    col6 STRING COMMENT 'This comment has a very long single word ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijklmnopqrstuvzxyz123 which will not fit in a line by itself for small column widths.',
    col7_NoComment STRING)
COMMENT 'table comment
two lines';

SET hive.cli.pretty.output.num.cols=80;

-- There will be an extra tab at the end of each comment line in the output.
-- This is because DESCRIBE <table_name> command separates the column, type and
-- comment field using a \t. DESCRIBE PRETTY <table_name> uses spaces instead
-- of \t to separate columns. Hive gets confused when it parses the string
-- table description constructed in MetaDataPrettyFormatUtils, and adds a tab
-- at the end of each line.
-- There are three ways to address this:
-- 1. Pad each row to the full terminal width with extra spaces.
-- 2. Assume a maximum tab width of 8, and subtract 2 * 8 spaces from the
--    available line width. This approach wastes upto 2 * 8 - 2 columns.
-- 3. Since the pretty output is meant only for human consumption, do nothing.
--    Just add a comment to the unit test file explaining what is happening.
--    This is the approach chosen.

DESCRIBE PRETTY test_table;

SET hive.cli.pretty.output.num.cols=200;
DESCRIBE PRETTY test_table;

SET hive.cli.pretty.output.num.cols=50;
DESCRIBE PRETTY test_table;

SET hive.cli.pretty.output.num.cols=60;
DESCRIBE PRETTY test_table;

CREATE TABLE test_table_very_long_column_name(
    col1 INT COMMENT 'col1 one line comment',
    col2_abcdefghiklmnopqrstuvxyz STRING COMMENT 'col2
two lines comment',
    col3 STRING COMMENT 'col3
three lines
comment',
    col4 STRING COMMENT 'col4 very long comment that is greater than 80 chars and is likely to spill into multiple lines')
;

SET hive.cli.pretty.output.num.cols=80;
DESCRIBE PRETTY test_table_very_long_column_name;

SET hive.cli.pretty.output.num.cols=20;
DESCRIBE PRETTY test_table_very_long_column_name;

CREATE TABLE test_table_partitioned(
    col1 INT COMMENT 'col1 one line comment',
    col2 STRING COMMENT 'col2
two lines comment',
    col3 STRING COMMENT 'col3
three lines
comment',
    col4 STRING COMMENT 'col4 very long comment that is greater than 80 chars and is likely to spill into multiple lines',
    col5 STRING COMMENT 'col5 very long multi-line comment where each line is very long by itself and is likely to spill
into multiple lines.  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin in dolor nisl, sodales
adipiscing tortor. Integer venenatis',
    col6 STRING COMMENT 'This comment has a very long single word ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijklmnopqrstuvzxyz123 which will not fit in a line by itself for small column widths.',
    col7_NoComment STRING)
COMMENT 'table comment
two lines'
PARTITIONED BY (ds STRING);

SET hive.cli.pretty.output.num.cols=60;
DESCRIBE PRETTY test_table_partitioned;

CREATE DATABASE db1;
CREATE TABLE db1.t1(key1 INT, value1 STRING) PARTITIONED BY (ds STRING, part STRING);

use db1;

ALTER TABLE t1 ADD PARTITION (ds='3', part='3');
ALTER TABLE t1 ADD PARTITION (ds='4', part='4');
ALTER TABLE t1 ADD PARTITION (ds='4', part='5');

-- describe table
DESCRIBE t1;
DESCRIBE EXTENDED t1;
DESCRIBE FORMATTED t1;

-- describe database.table
DESCRIBE db1.t1;
DESCRIBE EXTENDED db1.t1;
DESCRIBE FORMATTED db1.t1;

-- describe table column
DESCRIBE t1 key1;
DESCRIBE EXTENDED t1 key1;
DESCRIBE FORMATTED t1 key1;

-- describe database.tabe column
DESCRIBE db1.t1 key1;
DESCRIBE EXTENDED db1.t1 key1;
DESCRIBE FORMATTED db1.t1 key1;

-- describe table column
DESCRIBE t1 key1;
DESCRIBE EXTENDED t1 key1;
DESCRIBE FORMATTED t1 key1;

-- describe table partition
DESCRIBE t1 PARTITION(ds='4', part='5');
DESCRIBE EXTENDED t1 PARTITION(ds='4', part='5');
DESCRIBE FORMATTED t1 PARTITION(ds='4', part='5');

-- describe database.table partition
DESCRIBE db1.t1 PARTITION(ds='4', part='5');
DESCRIBE EXTENDED db1.t1 PARTITION(ds='4', part='5');
DESCRIBE FORMATTED db1.t1 PARTITION(ds='4', part='5');
describe srcpart;
describe srcpart key;
describe srcpart PARTITION(ds='2008-04-08', hr='12');

describe `srcpart`;
describe `srcpart` `key`;
describe `srcpart` PARTITION(ds='2008-04-08', hr='12');

describe extended srcpart;
describe extended srcpart key;
describe extended srcpart PARTITION(ds='2008-04-08', hr='12');

describe extended `srcpart`;
describe extended `srcpart` `key`;
describe extended `srcpart` PARTITION(ds='2008-04-08', hr='12');

describe formatted srcpart;
describe formatted srcpart key;
describe formatted srcpart PARTITION(ds='2008-04-08', hr='12');

describe formatted `srcpart`;
describe formatted `srcpart` `key`;
describe formatted `srcpart` PARTITION(ds='2008-04-08', hr='12');

create table srcpart_serdeprops like srcpart;
alter table srcpart_serdeprops set serdeproperties('xyz'='0');
alter table srcpart_serdeprops set serdeproperties('pqrs'='1');
alter table srcpart_serdeprops set serdeproperties('abcd'='2');
alter table srcpart_serdeprops set serdeproperties('A1234'='3');
describe formatted srcpart_serdeprops;
drop table srcpart_serdeprops;

CREATE DATABASE IF NOT EXISTS name1;
CREATE DATABASE IF NOT EXISTS name2;
use name1;
CREATE TABLE IF NOT EXISTS name1 (name1 int, name2 string) PARTITIONED BY (name3 int);
ALTER TABLE name1 ADD PARTITION (name3=1);
CREATE TABLE IF NOT EXISTS name2 (name3 int, name4 string);
use name2;
CREATE TABLE IF NOT EXISTS table1 (col1 int, col2 string);

use default;
DESCRIBE name1.name1;
DESCRIBE name1.name1 name2;
DESCRIBE name1.name1 PARTITION (name3=1);
DESCRIBE name1.name2;
DESCRIBE name1.name2 name3;
DESCRIBE name1.name2 name4;

use name1;
DESCRIBE name1;
DESCRIBE name1 name2;
DESCRIBE name1 PARTITION (name3=1);
DESCRIBE name1.name1;
DESCRIBE name1.name1 name2;
DESCRIBE name1.name1 PARTITION (name3=1);
DESCRIBE name2;
DESCRIBE name2 name3;
DESCRIBE name2 name4;
DESCRIBE name1.name2;
DESCRIBE name1.name2 name3;
DESCRIBE name1.name2 name4;

DESCRIBE name2.table1;
DESCRIBE name2.table1 col1;
DESCRIBE name2.table1 col2;
use name2;
DESCRIBE table1;
DESCRIBE table1 col1;
DESCRIBE table1 col2;

DESCRIBE name2.table1;
DESCRIBE name2.table1 col1;
DESCRIBE name2.table1 col2;

DROP TABLE IF EXISTS table1;
use name1;
DROP TABLE IF EXISTS name1;
DROP TABLE IF EXISTS name2;
use name2;
DROP TABLE IF EXISTS table1;
DROP DATABASE IF EXISTS name1;
DROP DATABASE IF EXISTS name2;
set hive.ddl.output.format=json;

CREATE TABLE IF NOT EXISTS jsontable (key INT, value STRING) COMMENT 'json table' STORED AS TEXTFILE;

SHOW TABLES;

SHOW TABLES LIKE 'json*';

SHOW TABLE EXTENDED LIKE 'json*';

ALTER TABLE jsontable SET TBLPROPERTIES ('id' = 'jsontable');

DESCRIBE jsontable;

DESCRIBE extended jsontable;

DROP TABLE jsontable;

set hive.ddl.output.format=text;
-- Describe a list structure in a thrift table
describe src_thrift lint;

-- Describe the element of a list
describe src_thrift lint.$elem$;

-- Describe the key of a map
describe src_thrift mStringString.$key$;

-- Describe the value of a map
describe src_thrift mStringString.$value$;

-- Describe a complex element of a list
describe src_thrift lintString.$elem$;

-- Describe a member of an element of a list
describe src_thrift lintString.$elem$.myint;
describe src_thrift $elem$;
describe src_thrift $key$;
describe src_thrift lint.abc;
describe src_thrift mStringString.abc;
DESC NonExistentTable;
DESC srcpart;
DESC srcpart PARTITION(ds='2012-04-08', hr='15');
CREATE DATABASE db1;
CREATE TABLE db1.t1(key1 INT, value1 STRING) PARTITIONED BY (ds STRING, part STRING);

-- describe database.table.column
DESCRIBE db1.t1.key1;
CREATE DATABASE IF NOT EXISTS db1;
use db1;
CREATE TABLE IF NOT EXISTS name1 (col1 string);

DESCRIBE name1.col1;
create table t1 (a int, b string) partitioned by (c int, d string);
describe t1;

set hive.display.partition.cols.separately=false;
describe t1;

set hive.display.partition.cols.separately=true;
-- Tests the case where a table is changed from sequence file to a RC file,
-- resulting in partitions in both file formats. If no valid partitions are
-- selected, then it should still use RC file for reading the dummy partition.
CREATE TABLE part_test (key STRING, value STRING) PARTITIONED BY (ds STRING) STORED AS SEQUENCEFILE;
ALTER TABLE part_test ADD PARTITION(ds='1');
ALTER TABLE part_test SET FILEFORMAT RCFILE;
ALTER TABLE part_test ADD PARTITION(ds='2');
SELECT count(1) FROM part_test WHERE ds='3';

set hive.fileformat.check = false;
create table kv_fileformat_check_txt (key string, value string) stored as textfile;
load data local inpath '../../data/files/kv1.seq' overwrite into table kv_fileformat_check_txt;

create table kv_fileformat_check_seq (key string, value string) stored as sequencefile;
load data local inpath '../../data/files/kv1.txt' overwrite into table kv_fileformat_check_seq;



set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
;
set hive.exec.reducers.max = 1;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;


CREATE TABLE bucket2_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS;

explain extended
insert overwrite table bucket2_1
select * from src;

insert overwrite table bucket2_1
select * from src;

explain
select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key;

select * from bucket2_1 tablesample (bucket 1 out of 2) s order by key;


set hive.fetch.task.conversion=more;

SET hive.metastore.disallow.incompatible.col.type.changes=false;
SELECT * FROM src LIMIT 1;
CREATE TABLE test_table123 (a INT, b MAP<STRING, STRING>) PARTITIONED BY (ds STRING) STORED AS SEQUENCEFILE;
INSERT OVERWRITE TABLE test_table123 PARTITION(ds="foo1") SELECT 1, MAP("a1", "b1") FROM src tablesample (1 rows);
SELECT * from test_table123 WHERE ds="foo1";
-- This should now work as hive.metastore.disallow.incompatible.col.type.changes is false
ALTER TABLE test_table123 REPLACE COLUMNS (a INT, b STRING);
SET hive.metastore.disallow.incompatible.col.type.changes=false;
SELECT * FROM src LIMIT 1;
CREATE TABLE test_table123 (a INT, b MAP<STRING, STRING>) PARTITIONED BY (ds STRING) STORED AS SEQUENCEFILE;
INSERT OVERWRITE TABLE test_table123 PARTITION(ds="foo1") SELECT 1, MAP("a1", "b1") FROM src LIMIT 1;
SELECT * from test_table123 WHERE ds="foo1";
ALTER TABLE test_table123 REPLACE COLUMNS (a INT, b MAP<STRING, STRING>);
ALTER TABLE test_table123 REPLACE COLUMNS (a BIGINT, b MAP<STRING, STRING>);
ALTER TABLE test_table123 REPLACE COLUMNS (a INT, b MAP<STRING, STRING>);
ALTER TABLE test_table123 REPLACE COLUMNS (a DOUBLE, b MAP<STRING, STRING>);
ALTER TABLE test_table123 REPLACE COLUMNS (a TINYINT, b MAP<STRING, STRING>);
ALTER TABLE test_table123 REPLACE COLUMNS (a BOOLEAN, b MAP<STRING, STRING>);
ALTER TABLE test_table123 REPLACE COLUMNS (a TINYINT, b MAP<STRING, STRING>);
ALTER TABLE test_table123 CHANGE COLUMN a a_new BOOLEAN;

SET hive.metastore.disallow.incompatible.col.type.changes=true;
-- All the above ALTERs will succeed since they are between compatible types.
-- The following ALTER will fail as MAP<STRING, STRING> and STRING are not
-- compatible.

ALTER TABLE test_table123 REPLACE COLUMNS (a INT, b STRING);
reset hive.metastore.disallow.incompatible.col.type.changes;
SET hive.metastore.disallow.incompatible.col.type.changes=true;
SELECT * FROM src LIMIT 1;
CREATE TABLE test_table123 (a INT, b STRING) PARTITIONED BY (ds STRING) STORED AS SEQUENCEFILE;
INSERT OVERWRITE TABLE test_table123 PARTITION(ds="foo1") SELECT 1, "one" FROM src LIMIT 1;
SELECT * from test_table123 WHERE ds="foo1";
ALTER TABLE test_table123 CHANGE COLUMN b b MAP<STRING, STRING>;
DROP TABLE IF EXISTS UserVisits_web_text_none;

CREATE TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none;

desc extended UserVisits_web_text_none sourceIP;
desc formatted UserVisits_web_text_none sourceIP;

explain
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

explain extended
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;
desc formatted UserVisits_web_text_none sourceIP;
desc formatted UserVisits_web_text_none avgTimeOnSite;
desc formatted UserVisits_web_text_none adRevenue;

CREATE TABLE empty_tab(
   a int,
   b double,
   c string,
   d boolean,
   e binary)
row format delimited fields terminated by '|'  stored as textfile;

desc formatted empty_tab a;
explain
analyze table empty_tab compute statistics for columns a,b,c,d,e;

analyze table empty_tab compute statistics for columns a,b,c,d,e;
desc formatted empty_tab a;
desc formatted empty_tab b;

CREATE DATABASE test;
USE test;

CREATE TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none;

desc extended UserVisits_web_text_none sourceIP;
desc extended test.UserVisits_web_text_none sourceIP;
desc extended default.UserVisits_web_text_none sourceIP;
desc formatted UserVisits_web_text_none sourceIP;
desc formatted test.UserVisits_web_text_none sourceIP;
desc formatted default.UserVisits_web_text_none sourceIP;

analyze table UserVisits_web_text_none compute statistics for columns sKeyword;
desc extended UserVisits_web_text_none sKeyword;
desc formatted UserVisits_web_text_none sKeyword;
desc formatted test.UserVisits_web_text_none sKeyword;

set hive.stats.autogather=true;

set hive.compute.query.using.stats=true;
create table t1 (a string, b string);

insert into table t1 select * from src;

analyze table t1 compute statistics for columns a,b;

explain
select count(distinct b) from t1 group by a;

explain
select distinct(b) from t1;

explain
select a, count(*) from t1 group by a;

drop table t1;
set hive.compute.query.using.stats = false;
drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

explain
select distinct first_value(t) over ( partition by si order by i ) from over10k limit 10;

select distinct first_value(t) over ( partition by si order by i ) from over10k limit 10;

explain
select distinct last_value(i) over ( partition by si order by i )
from over10k limit 10;

select distinct last_value(i) over ( partition by si order by i )
from over10k limit 10;

explain
select distinct last_value(i) over ( partition by si order by i ),
                first_value(t)  over ( partition by si order by i )
from over10k limit 50;

select distinct last_value(i) over ( partition by si order by i ),
                first_value(t)  over ( partition by si order by i )
from over10k limit 50;
drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select count(distinct last_value(i) over ( partition by si order by i )) from over10k;
drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select distinct last_value(i) over ( partition by si order by i ),
       distinct first_value(t)  over ( partition by si order by i )
from over10k ;
SET hive.exec.driver.run.hooks=org.apache.hadoop.hive.ql.hooks.DriverTestHook;

-- This query should appear in the Hive CLI output.
-- We test DriverTestHook, which does exactly that.
-- This should not break.
SELECT * FROM src LIMIT 1;
-- This test verifies that if the functions and tables unregistered when the database is dropped
-- and other databases are not affected

CREATE DATABASE TEST_database;

USE TEST_database;

CREATE TABLE test_table (key STRING, value STRING);

CREATE FUNCTION test_func as 'org.apache.hadoop.hive.ql.udf.UDFAscii';

USE default;

CREATE TABLE test_table (key STRING, value STRING);

CREATE FUNCTION test_func as 'org.apache.hadoop.hive.ql.udf.UDFAscii';

DROP DATABASE TEST_database CASCADE;

describe test_table;

describe function test_func;

describe function TEST_database.test_func;

describe TEST_database.test_table;
-- This test verifies that if a partition exists outside a table's current location when the
-- database is dropped the partition's location is dropped as well.

CREATE DATABASE test_database;

USE test_database;

CREATE TABLE test_table (key STRING, value STRING)
PARTITIONED BY (part STRING)
STORED AS RCFILE
LOCATION 'file:${system:test.tmp.dir}/drop_database_removes_partition_dirs_table';

ALTER TABLE test_table ADD PARTITION (part = '1')
LOCATION 'file:${system:test.tmp.dir}/drop_database_removes_partition_dirs_table2/part=1';

INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT * FROM default.src;

dfs -ls ${system:test.tmp.dir}/drop_database_removes_partition_dirs_table2;

USE default;

DROP DATABASE test_database CASCADE;

dfs -ls ${system:test.tmp.dir}/drop_database_removes_partition_dirs_table2;

dfs -rmr ${system:test.tmp.dir}/drop_database_removes_partition_dirs_table2;
SET hive.exec.drop.ignorenonexistent=false;
DROP TEMPORARY FUNCTION IF EXISTS UnknownFunction;
set hive.exec.drop.ignorenonexistent=false;
-- Can't use DROP TEMPORARY FUNCTION if the function doesn't exist and IF EXISTS isn't specified
DROP TEMPORARY FUNCTION UnknownFunction;
set hive.exec.drop.ignorenonexistent=false;
-- Can't use DROP FUNCTION if the function doesn't exist and IF EXISTS isn't specified
drop function nonexistent_function;
DROP INDEX IF EXISTS UnknownIndex ON src;
DROP INDEX IF EXISTS UnknownIndex ON UnknownTable;
set hive.exec.drop.ignorenonexistent=false;
-- Can't use DROP INDEX if the index doesn't exist and IF EXISTS isn't specified
DROP INDEX UnknownIndex ON src;
-- This test verifies that if a partition exists outside an index table's current location when the
-- index is dropped the partition's location is dropped as well.

CREATE TABLE test_table (key STRING, value STRING)
PARTITIONED BY (part STRING)
STORED AS RCFILE
LOCATION 'file:${system:test.tmp.dir}/drop_database_removes_partition_dirs_table';

CREATE INDEX test_index ON
TABLE test_table(key) AS 'compact' WITH DEFERRED REBUILD
IN TABLE test_index_table;

ALTER TABLE test_index_table ADD PARTITION (part = '1')
LOCATION 'file:${system:test.tmp.dir}/drop_index_removes_partition_dirs_index_table2/part=1';

dfs -ls ${system:test.tmp.dir}/drop_index_removes_partition_dirs_index_table2;

DROP INDEX test_index ON test_table;

dfs -ls ${system:test.tmp.dir}/drop_index_removes_partition_dirs_index_table2;

dfs -rmr ${system:test.tmp.dir}/drop_index_removes_partition_dirs_index_table2;
create database dmp;

create table dmp.mp (a string) partitioned by (b string, c string);

alter table dmp.mp add partition (b='1', c='1');
alter table dmp.mp add partition (b='1', c='2');
alter table dmp.mp add partition (b='2', c='2');

show partitions dmp.mp;

explain extended alter table dmp.mp drop partition (b='1');
alter table dmp.mp drop partition (b='1');

show partitions dmp.mp;

set hive.exec.drop.ignorenonexistent=false;
alter table dmp.mp drop if exists partition (b='3');

show partitions dmp.mp;

drop table dmp.mp;

drop database dmp;
DROP FUNCTION max;
create table ptestfilter (a string, b int) partitioned by (c string, d string);
describe ptestfilter;

alter table ptestfilter add partition (c='US', d=1);
alter table ptestfilter add partition (c='US', d=2);
alter table ptestFilter add partition (c='Uganda', d=2);
alter table ptestfilter add partition (c='Germany', d=2);
alter table ptestfilter add partition (c='Canada', d=3);
alter table ptestfilter add partition (c='Russia', d=3);
alter table ptestfilter add partition (c='Greece', d=2);
alter table ptestfilter add partition (c='India', d=3);
alter table ptestfilter add partition (c='France', d=4);
show partitions ptestfilter;

alter table ptestfilter drop partition (c='US', d<'2');
show partitions ptestfilter;

alter table ptestfilter drop partition (c>='US', d<='2');
show partitions ptestfilter;

alter table ptestfilter drop partition (c >'India');
show partitions ptestfilter;

alter table ptestfilter drop partition (c >='India'),
                             partition (c='Greece', d='2');
show partitions ptestfilter;

alter table ptestfilter drop partition (c != 'France');
show partitions ptestfilter;

set hive.exec.drop.ignorenonexistent=false;
alter table ptestfilter drop if exists partition (c='US');
show partitions ptestfilter;

drop table ptestfilter;


create table ptestfilter (a string, b int) partitioned by (c int, d int);
describe ptestfilter;

alter table ptestfilter add partition (c=1, d=1);
alter table ptestfilter add partition (c=1, d=2);
alter table ptestFilter add partition (c=2, d=1);
alter table ptestfilter add partition (c=2, d=2);
alter table ptestfilter add partition (c=3, d=1);
alter table ptestfilter add partition (c=30, d=2);
show partitions ptestfilter;

alter table ptestfilter drop partition (c=1, d=1);
show partitions ptestfilter;

alter table ptestfilter drop partition (c=2);
show partitions ptestfilter;

alter table ptestfilter drop partition (c<4);
show partitions ptestfilter;

drop table ptestfilter;


create table ptestfilter (a string, b int) partitioned by (c string, d int);
describe ptestfilter;

alter table ptestfilter add partition (c='1', d=1);
alter table ptestfilter add partition (c='1', d=2);
alter table ptestFilter add partition (c='2', d=1);
alter table ptestfilter add partition (c='2', d=2);
alter table ptestfilter add partition (c='3', d=1);
alter table ptestfilter add partition (c='3', d=2);
show partitions ptestfilter;

alter table ptestfilter drop partition (c='1', d=1);
show partitions ptestfilter;

alter table ptestfilter drop partition (c='2');
show partitions ptestfilter;

drop table ptestfilter;


create table mp (a string) partitioned by (b string, c string);

alter table mp add partition (b='1', c='1');
alter table mp add partition (b='1', c='2');
alter table mp add partition (b='2', c='2');

show partitions mp;

set hive.exec.drop.ignorenonexistent=false;
-- Can't use DROP PARTITION if the partition doesn't exist and IF EXISTS isn't specified
alter table mp drop partition (b='3');
create table ptestfilter1 (a string, b int) partitioned by (c string, d string);

alter table ptestfilter1 add partition (c='US', d=1);
show partitions ptestfilter1;

set hive.exec.drop.ignorenonexistent=false;
alter table ptestfilter1 drop partition (c='US', d<1);

set hive.mapred.mode=nonstrict;
-- This test verifies that a table partition could be dropped with columns stats computed
-- The column stats for a partitioned table will go to PART_COL_STATS
CREATE DATABASE IF NOT EXISTS partstatsdb1;
USE partstatsdb1;
CREATE TABLE IF NOT EXISTS testtable (key STRING, value STRING) PARTITIONED BY (part1 STRING, Part2 STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE testtable PARTITION (part1='p11', Part2='P12');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE testtable PARTITION (part1='p21', Part2='P22');
ANALYZE TABLE testtable COMPUTE STATISTICS FOR COLUMNS key;
ANALYZE TABLE testtable PARTITION (part1='p11', Part2='P12') COMPUTE STATISTICS FOR COLUMNS key;


CREATE TABLE IF NOT EXISTS TestTable1 (key STRING, value STRING) PARTITIONED BY (part1 STRING, Part2 STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1 PARTITION (part1='p11', Part2='P11');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1 PARTITION (part1='p11', Part2='P12');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1 PARTITION (part1='p21', Part2='P22');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1 PARTITION (part1='p31', Part2='P32');
ANALYZE TABLE TestTable1 COMPUTE STATISTICS FOR COLUMNS key;
ANALYZE TABLE TestTable1 PARTITION (part1='p11') COMPUTE STATISTICS FOR COLUMNS key;
ANALYZE TABLE TestTable1 PARTITION (part1='p11', Part2='P12') COMPUTE STATISTICS FOR COLUMNS key;

CREATE TABLE IF NOT EXISTS TESTTABLE2 (key STRING, value STRING) PARTITIONED BY (part1 STRING, Part2 STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TESTTABLE2 PARTITION (part1='p11', Part2='P12');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TESTTABLE2 PARTITION (part1='p21', Part2='P22');
ANALYZE TABLE TESTTABLE2 COMPUTE STATISTICS FOR COLUMNS key;
ANALYZE TABLE TESTTABLE2 PARTITION (part1='p11', Part2='P12') COMPUTE STATISTICS FOR COLUMNS key;

ALTER TABLE partstatsdb1.testtable DROP PARTITION (part1='p11', Part2='P12');
ALTER TABLE partstatsdb1.TestTable1 DROP PARTITION (part1='p11', Part2='P12');
ALTER TABLE partstatsdb1.TESTTABLE2 DROP PARTITION (part1='p11', Part2='P12');

DROP TABLE partstatsdb1.testtable;
DROP TABLE partstatsdb1.TestTable1;
DROP TABLE partstatsdb1.TESTTABLE2;
DROP DATABASE partstatsdb1;

CREATE DATABASE IF NOT EXISTS PARTSTATSDB2;
USE PARTSTATSDB2;
CREATE TABLE IF NOT EXISTS testtable (key STRING, value STRING) PARTITIONED BY (part1 STRING, Part2 STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE testtable PARTITION (part1='p11', Part2='P12');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE testtable PARTITION (part1='p21', Part2='P22');
ANALYZE TABLE testtable COMPUTE STATISTICS FOR COLUMNS key;
ANALYZE TABLE testtable PARTITION (part1='p11', Part2='P12') COMPUTE STATISTICS FOR COLUMNS key;


CREATE TABLE IF NOT EXISTS TestTable1 (key STRING, value STRING) PARTITIONED BY (part1 STRING, Part2 STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1 PARTITION (part1='p11', Part2='P11');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1 PARTITION (part1='p11', Part2='P12');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1 PARTITION (part1='p21', Part2='P22');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1 PARTITION (part1='p31', Part2='P32');
ANALYZE TABLE TestTable1 COMPUTE STATISTICS FOR COLUMNS key;
ANALYZE TABLE TestTable1 PARTITION (part1='p11') COMPUTE STATISTICS FOR COLUMNS key;
ANALYZE TABLE TestTable1 PARTITION (part1='p11', Part2='P12') COMPUTE STATISTICS FOR COLUMNS key;

CREATE TABLE IF NOT EXISTS TESTTABLE2 (key STRING, value STRING) PARTITIONED BY (part1 STRING, Part2 STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TESTTABLE2 PARTITION (part1='p11', Part2='P12');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TESTTABLE2 PARTITION (part1='p21', Part2='P22');
ANALYZE TABLE TESTTABLE2 COMPUTE STATISTICS FOR COLUMNS key;
ANALYZE TABLE TESTTABLE2 PARTITION (part1='p11', Part2='P12') COMPUTE STATISTICS FOR COLUMNS key;

ALTER TABLE PARTSTATSDB2.testtable DROP PARTITION (part1='p11', Part2='P12');
ALTER TABLE PARTSTATSDB2.TestTable1 DROP PARTITION (part1='p11', Part2='P12');
ALTER TABLE PARTSTATSDB2.TESTTABLE2 DROP PARTITION (part1='p11', Part2='P12');

DROP TABLE PARTSTATSDB2.testtable;
DROP TABLE PARTSTATSDB2.TestTable1;
DROP TABLE PARTSTATSDB2.TESTTABLE2;
DROP DATABASE PARTSTATSDB2;

SET hive.exec.drop.ignorenonexistent=false;
DROP TABLE IF EXISTS UnknownTable;
SET hive.metastore.batch.retrieve.max=1;
create table if not exists temp(col STRING) partitioned by (p STRING);
alter table temp add if not exists partition (p ='p1');
alter table temp add if not exists partition (p ='p2');
alter table temp add if not exists partition (p ='p3');

show partitions temp;

drop table temp;

create table if not exists temp(col STRING) partitioned by (p STRING);

show partitions temp;

drop table temp;
set hive.exec.drop.ignorenonexistent=false;
-- Can't use DROP TABLE if the table doesn't exist and IF EXISTS isn't specified
DROP TABLE UnknownTable;
CREATE VIEW xxx6 AS SELECT key FROM src;
-- Can't use DROP TABLE on a view
DROP TABLE xxx6;
SET hive.metastore.batch.retrieve.max=1;
CREATE TABLE IF NOT EXISTS temp(col STRING);

DROP TABLE temp PURGE;
-- This test verifies that if a partition exists outside the table's current location when the
-- table is dropped the partition's location is dropped as well.

CREATE TABLE test_table (key STRING, value STRING)
PARTITIONED BY (part STRING)
STORED AS RCFILE
LOCATION 'file:${system:test.tmp.dir}/drop_table_removes_partition_dirs_table';

ALTER TABLE test_table ADD PARTITION (part = '1')
LOCATION 'file:${system:test.tmp.dir}/drop_table_removes_partition_dirs_table2/part=1';

INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT * FROM src;

dfs -ls ${system:test.tmp.dir}/drop_table_removes_partition_dirs_table2;

DROP TABLE test_table;

dfs -ls ${system:test.tmp.dir}/drop_table_removes_partition_dirs_table2;

dfs -rmr ${system:test.tmp.dir}/drop_table_removes_partition_dirs_table2;
set hive.stats.dbclass=fs;
set hive.stats.autogather=true;
set hive.cbo.enable=true;

DROP TABLE IF EXISTS aa;
CREATE TABLE aa (L_ORDERKEY      INT,
                                L_PARTKEY       INT,
                                L_SUPPKEY       INT,
                                L_LINENUMBER    INT,
                                L_QUANTITY      DOUBLE,
                                L_EXTENDEDPRICE DOUBLE,
                                L_DISCOUNT      DOUBLE,
                                L_TAX           DOUBLE,
                                L_RETURNFLAG    STRING,
                                L_LINESTATUS    STRING,
                                l_shipdate      STRING,
                                L_COMMITDATE    STRING,
                                L_RECEIPTDATE   STRING,
                                L_SHIPINSTRUCT  STRING,
                                L_SHIPMODE      STRING,
                                L_COMMENT       STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|';

LOAD DATA LOCAL INPATH '../../data/files/lineitem.txt' OVERWRITE INTO TABLE aa;

CREATE INDEX aa_lshipdate_idx ON TABLE aa(l_shipdate) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(l_shipdate)");
ALTER INDEX aa_lshipdate_idx ON aa REBUILD;

show tables;

explain select l_shipdate, count(l_shipdate)
from aa
group by l_shipdate;

-- This test verifies that a table could be dropped with columns stats computed
-- The column stats for table without partition will go to TAB_COL_STATS
CREATE DATABASE IF NOT EXISTS tblstatsdb1;
USE tblstatsdb1;
CREATE TABLE IF NOT EXISTS testtable (key STRING, value STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE testtable;
ANALYZE TABLE testtable COMPUTE STATISTICS FOR COLUMNS key;

CREATE TABLE IF NOT EXISTS TestTable1 (key STRING, value STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1;
ANALYZE TABLE TestTable1 COMPUTE STATISTICS FOR COLUMNS key;

CREATE TABLE IF NOT EXISTS TESTTABLE2 (key STRING, value STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TESTTABLE2;
ANALYZE TABLE TESTTABLE2 COMPUTE STATISTICS FOR COLUMNS key;

DROP TABLE tblstatsdb1.testtable;
DROP TABLE tblstatsdb1.TestTable1;
DROP TABLE tblstatsdb1.TESTTABLE2;
DROP DATABASE tblstatsdb1;

CREATE DATABASE IF NOT EXISTS TBLSTATSDB2;
USE TBLSTATSDB2;
CREATE TABLE IF NOT EXISTS testtable (key STRING, value STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE testtable;
ANALYZE TABLE testtable COMPUTE STATISTICS FOR COLUMNS key;


CREATE TABLE IF NOT EXISTS TestTable1 (key STRING, value STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TestTable1;
ANALYZE TABLE TestTable1 COMPUTE STATISTICS FOR COLUMNS key;


CREATE TABLE IF NOT EXISTS TESTTABLE2 (key STRING, value STRING);
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE TESTTABLE2;
ANALYZE TABLE TESTTABLE2 COMPUTE STATISTICS FOR COLUMNS key;


DROP TABLE TBLSTATSDB2.testtable;
DROP TABLE TBLSTATSDB2.TestTable1;
DROP TABLE TBLSTATSDB2.TESTTABLE2;
DROP DATABASE TBLSTATSDB2;

CREATE TEMPORARY FUNCTION test_translate AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestTranslate';

EXPLAIN
DROP TEMPORARY FUNCTION test_translate;

DROP TEMPORARY FUNCTION test_translate;
SET hive.exec.drop.ignorenonexistent=false;
DROP VIEW IF EXISTS UnknownView;


CREATE TABLE xxx1(key int);

-- Can't use DROP VIEW on a base table
DROP VIEW xxx1;
SET hive.exec.drop.ignorenonexistent=false;
-- Can't use DROP VIEW if the view doesn't exist and IF EXISTS isn't specified
DROP VIEW UnknownView;
set hive.lock.numretries=1;
set hive.lock.sleep.between.retries=1;
set hive.support.concurrency=true;
set hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager;

drop table if exists drop_with_concurrency_1;
create table drop_with_concurrency_1 (c1 int);
drop table drop_with_concurrency_1;
FROM src a JOIN src a ON (a.key = a.key)
INSERT OVERWRITE TABLE dest1 SELECT a.key, a.value
FROM src SELECT TRANSFORM (key, value) USING "awk -F'\001' '{print $0}'" AS (foo, foo);FROM src SELECT TRANSFORM (key, value) USING "awk -F'\001' '{print $0}'" AS (foo STRING, foo STRING);
create table dest1_din1(key int, value string);

from src
insert overwrite table dest1_din1 select key, value
insert overwrite table dest1_din1 select key, value;


create table dest1_din2(key int, value string) partitioned by (ds string);

from src
insert overwrite table dest1_din2 partition (ds='1') select key, value
insert overwrite table dest1_din2 partition (ds='1') select key, value;

from src
insert overwrite directory '${system:test.tmp.dir}/dest1' select key, value
insert overwrite directory '${system:test.tmp.dir}/dest1' select key, value;
set hive.mapred.mode=nonstrict;
SET hive.metastore.partition.name.whitelist.pattern=[^9]*;
set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.VerifyTableDirectoryIsEmptyHook;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table source_table like srcpart;

create table dest_table like srcpart;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE source_table partition(ds='2008-04-08', hr=11);

-- Tests creating dynamic partitions with characters not in the whitelist (i.e. 9)
-- If the directory is not empty the hook will throw an error, instead the error should come from the metastore
-- This shows that no dynamic partitions were created and left behind or had directories created

insert overwrite table dest_table partition (ds, hr) select key, hr, ds, value from source_table where ds='2008-04-08' and value='val_129' order by value asc;
set hive.mapred.mode=nonstrict;
CREATE TABLE t1 (c1 BIGINT, c2 STRING);

CREATE TABLE t2 (c1 INT, c2 STRING)
PARTITIONED BY (p1 STRING);

LOAD DATA LOCAL INPATH '../../data/files/dynamic_partition_insert.txt' INTO TABLE t1;
LOAD DATA LOCAL INPATH '../../data/files/dynamic_partition_insert.txt' INTO TABLE t1;
LOAD DATA LOCAL INPATH '../../data/files/dynamic_partition_insert.txt' INTO TABLE t1;
LOAD DATA LOCAL INPATH '../../data/files/dynamic_partition_insert.txt' INTO TABLE t1;
LOAD DATA LOCAL INPATH '../../data/files/dynamic_partition_insert.txt' INTO TABLE t1;

SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE TABLE t2 partition(p1) SELECT *,c1 AS p1 FROM t1 DISTRIBUTE BY p1;

SELECT * FROM t2;

DROP TABLE t1;
DROP TABLE t2;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.tez.dynamic.partition.pruning=true;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;


select distinct ds from srcpart;
select distinct hr from srcpart;

EXPLAIN create table srcpart_date as select ds as ds, ds as `date`  from srcpart group by ds;
create table srcpart_date as select ds as ds, ds as `date` from srcpart group by ds;
create table srcpart_hour as select hr as hr, hr as hour from srcpart group by hr;
create table srcpart_date_hour as select ds as ds, ds as `date`, hr as hr, hr as hour from srcpart group by ds, hr;
create table srcpart_double_hour as select (hr*2) as hr, hr as hour from srcpart group by hr;

-- single column, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = '2008-04-08';

-- single column, single key, udf with typechange
EXPLAIN select count(*) from srcpart join srcpart_date on (day(srcpart.ds) = day(srcpart_date.ds)) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (day(srcpart.ds) = day(srcpart_date.ds)) where srcpart_date.`date` = '2008-04-08';
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (day(srcpart.ds) = day(srcpart_date.ds)) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (day(srcpart.ds) = day(srcpart_date.ds)) where srcpart_date.`date` = '2008-04-08';
set hive.tez.dynamic.partition.pruning=true;

-- multiple sources, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where hr = 11 and ds = '2008-04-08';

-- multiple columns single source
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = '2008-04-08' and hr = 11;

-- empty set
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = 'I DONT EXIST';

-- expressions
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (cast(srcpart.hr*2 as string) = cast(srcpart_double_hour.hr as string)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (cast(srcpart.hr*2 as string) = cast(srcpart_double_hour.hr as string)) where srcpart_double_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where cast(hr as string) = 11;


-- parent is reduce tasks
EXPLAIN select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- non-equi join
EXPLAIN select count(*) from srcpart, srcpart_date_hour where (srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11) and (srcpart.ds = srcpart_date_hour.ds or srcpart.hr = srcpart_date_hour.hr);
select count(*) from srcpart, srcpart_date_hour where (srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11) and (srcpart.ds = srcpart_date_hour.ds or srcpart.hr = srcpart_date_hour.hr);

-- old style join syntax
EXPLAIN select count(*) from srcpart, srcpart_date_hour where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11 and srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr;
select count(*) from srcpart, srcpart_date_hour where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11 and srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr;

-- left join
EXPLAIN select count(*) from srcpart left join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
EXPLAIN select count(*) from srcpart_date left join srcpart on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- full outer
EXPLAIN select count(*) from srcpart full outer join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- with static pruning
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;

-- union + subquery
EXPLAIN select count(*) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select count(*) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
EXPLAIN select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
EXPLAIN select ds from (select distinct(ds) as ds from srcpart union all select distinct(ds) as ds from srcpart) s where s.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select ds from (select distinct(ds) as ds from srcpart union all select distinct(ds) as ds from srcpart) s where s.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask = true;
set hive.auto.convert.join.noconditionaltask.size = 10000000;

-- single column, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- single column, single key, udf with typechange
EXPLAIN select count(*) from srcpart join srcpart_date on (day(srcpart.ds) = day(srcpart_date.ds)) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (day(srcpart.ds) = day(srcpart_date.ds)) where srcpart_date.`date` = '2008-04-08';

-- multiple sources, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart where hr = 11 and ds = '2008-04-08';

-- multiple columns single source
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart where ds = '2008-04-08' and hr = 11;

-- empty set
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
-- Disabled until TEZ-1486 is fixed
-- select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';

-- expressions
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart where hr = 11;

-- parent is reduce tasks
EXPLAIN select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- left join
EXPLAIN select count(*) from srcpart left join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
EXPLAIN select count(*) from srcpart_date left join srcpart on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- full outer
EXPLAIN select count(*) from srcpart full outer join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- with static pruning
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;
-- Disabled until TEZ-1486 is fixed
-- select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
-- where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;

-- union + subquery
EXPLAIN select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);


-- different file format
create table srcpart_orc (key int, value string) partitioned by (ds string, hr int) stored as orc;


set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false;
set hive.exec.max.dynamic.partitions=1000;

insert into table srcpart_orc partition (ds, hr) select key, value, ds, hr from srcpart;
EXPLAIN select count(*) from srcpart_orc join srcpart_date_hour on (srcpart_orc.ds = srcpart_date_hour.ds and srcpart_orc.hr = srcpart_date_hour.hr) where srcpart_date_hour.hour = 11 and (srcpart_date_hour.`date` = '2008-04-08' or srcpart_date_hour.`date` = '2008-04-09');
select count(*) from srcpart_orc join srcpart_date_hour on (srcpart_orc.ds = srcpart_date_hour.ds and srcpart_orc.hr = srcpart_date_hour.hr) where srcpart_date_hour.hour = 11 and (srcpart_date_hour.`date` = '2008-04-08' or srcpart_date_hour.`date` = '2008-04-09');
select count(*) from srcpart where (ds = '2008-04-08' or ds = '2008-04-09') and hr = 11;

drop table srcpart_orc;
drop table srcpart_date;
drop table srcpart_hour;
drop table srcpart_date_hour;
drop table srcpart_double_hour;
set hive.explain.user=false;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.tez.dynamic.partition.pruning=true;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask = true;
set hive.auto.convert.join.noconditionaltask.size = 10000000;

create table dim_shops (id int, label string) row format delimited fields terminated by ',' stored as textfile;
load data local inpath '../../data/files/dim_shops.txt' into table dim_shops;

create table agg_01 (amount decimal) partitioned by (dim_shops_id int) row format delimited fields terminated by ',' stored as textfile;
alter table agg_01 add partition (dim_shops_id = 1);
alter table agg_01 add partition (dim_shops_id = 2);
alter table agg_01 add partition (dim_shops_id = 3);

load data local inpath '../../data/files/agg_01-p1.txt' into table agg_01 partition (dim_shops_id=1);
load data local inpath '../../data/files/agg_01-p2.txt' into table agg_01 partition (dim_shops_id=2);
load data local inpath '../../data/files/agg_01-p3.txt' into table agg_01 partition (dim_shops_id=3);

analyze table dim_shops compute statistics;
analyze table agg_01 partition (dim_shops_id) compute statistics;

select * from dim_shops;
select * from agg_01;

EXPLAIN SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

set hive.tez.dynamic.partition.pruning.max.event.size=1000000;
set hive.tez.dynamic.partition.pruning.max.data.size=1;

EXPLAIN SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

EXPLAIN SELECT d1.label
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id;

SELECT d1.label
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id;

EXPLAIN SELECT agg.amount
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and agg.dim_shops_id = 1;

SELECT agg.amount
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and agg.dim_shops_id = 1;

set hive.tez.dynamic.partition.pruning.max.event.size=1;
set hive.tez.dynamic.partition.pruning.max.data.size=1000000;

EXPLAIN SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

set hive.tez.dynamic.partition.pruning.max.event.size=100000;
set hive.tez.dynamic.partition.pruning.max.data.size=1000000;

EXPLAIN
SELECT amount FROM agg_01, dim_shops WHERE dim_shops_id = id AND label = 'foo'
UNION ALL
SELECT amount FROM agg_01, dim_shops WHERE dim_shops_id = id AND label = 'bar';

SELECT amount FROM agg_01, dim_shops WHERE dim_shops_id = id AND label = 'foo'
UNION ALL
SELECT amount FROM agg_01, dim_shops WHERE dim_shops_id = id AND label = 'bar';

set hive.tez.dynamic.partition.pruning.max.event.size=1000000;
set hive.tez.dynamic.partition.pruning.max.data.size=10000;
-- Dynamic partition pruning will be removed as data size exceeds the limit;
-- and for self join on partitioning column, it should not fail (HIVE-10559).
explain
select count(*)
from srcpart s1,
     srcpart s2
where s1.ds = s2.ds
;

select count(*)
from srcpart s1,
     srcpart s2
where s1.ds = s2.ds
;create table dynamic_part_table(intcol string) partitioned by (partcol1 string, partcol2 string);

set hive.exec.dynamic.partition.mode=nonstrict;

insert into table dynamic_part_table partition(partcol1, partcol2) select '1', '1', '1' from src where key=150;

insert into table dynamic_part_table partition(partcol1, partcol2) select '1', NULL, '1' from src where key=150;

insert into table dynamic_part_table partition(partcol1, partcol2) select '1', '1', NULL from src where key=150;

insert into table dynamic_part_table partition(partcol1, partcol2) select '1', NULL, NULL from src where key=150;

explain extended select intcol from dynamic_part_table where partcol1='1' and partcol2='1';

set hive.exec.dynamic.partition.mode=strict;

explain extended select intcol from dynamic_part_table where partcol1='1' and partcol2='1';

explain extended select intcol from dynamic_part_table where (partcol1='1' and partcol2='1')or (partcol1='1' and partcol2='__HIVE_DEFAULT_PARTITION__');
set hive.mapred.mode=nonstrict;
SET hive.map.aggr=true;
SET hive.multigroupby.singlereducer=false;
SET hive.groupby.skewindata=false;
SET mapred.reduce.tasks=31;
SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

-- JOIN TEST

EXPLAIN
FROM
(SELECT src.* FROM src sort by key) X
RIGHT OUTER JOIN
(SELECT src.* FROM src sort by value) Y
ON (X.key = Y.key)
JOIN
(SELECT src.* FROM src sort by value) Z
ON (X.key = Z.key)
SELECT sum(hash(Y.key,Y.value)) GROUP BY Y.key;


CREATE TABLE dest1(key INT, value STRING);
CREATE TABLE dest2(key INT, value STRING);

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(SUBSTR(src.value,5)) GROUP BY src.key
INSERT OVERWRITE TABLE dest2 SELECT src.key, sum(SUBSTR(src.value,5)) GROUP BY src.key;

SELECT dest1.* FROM dest1;
SELECT dest2.* FROM dest2;

DROP TABLE dest1;
DROP TABLE dest2;


-- UNION TEST

CREATE TABLE tmptable(key STRING, value INT);

EXPLAIN
INSERT OVERWRITE TABLE tmptable
  SELECT unionsrc.key, unionsrc.value FROM (SELECT 'tst1' AS key, count(1) AS value FROM src s1
                                        UNION  ALL
                                            SELECT 'tst2' AS key, count(1) AS value FROM src s2
                                        UNION ALL
                                            SELECT 'tst3' AS key, count(1) AS value FROM src s3) unionsrc;
SELECT * FROM tmptable x SORT BY x.key;

DROP TABLE tmtable;


EXPLAIN
SELECT unionsrc1.key, unionsrc1.value, unionsrc2.key, unionsrc2.value
FROM (SELECT 'tst1' AS key, cast(count(1) AS string) AS value FROM src s1
                         UNION  ALL
      SELECT s2.key AS key, s2.value AS value FROM src s2 WHERE s2.key < 10) unionsrc1
JOIN
     (SELECT 'tst1' AS key, cast(count(1) AS string) AS value FROM src s3
                         UNION  ALL
      SELECT s4.key AS key, s4.value AS value FROM src s4 WHERE s4.key < 10) unionsrc2
ON (unionsrc1.key = unionsrc2.key);


-- CWE TEST

CREATE TABLE inv(w_warehouse_name STRING , w_warehouse_sk INT , stdev INT , d_moy INT , mean INT , cov INT , inv_quantity_on_hand INT);
CREATE TABLE inventory(inv_date_sk INT , inv_item_sk INT ,inv_quantity_on_hand INT ,inv_warehouse_sk INT);
CREATE TABLE item(i_item_sk INT);
CREATE TABLE warehouse(w_warehouse_sk INT , w_warehouse_name STRING);
CREATE TABLE date_dim(d_date_sk INT , d_year INT , d_moy INT);

EXPLAIN
WITH inv AS
(SELECT w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy
       ,stdev,mean, CASE mean WHEN 0 THEN null ELSE stdev/mean END cov
FROM(SELECT w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy
            ,STDDEV_SAMP(inv_quantity_on_hand) stdev,AVG(inv_quantity_on_hand) mean
      FROM inventory
          ,item
          ,warehouse
          ,date_dim
      WHERE inv_item_sk = i_item_sk
        AND inv_warehouse_sk = w_warehouse_sk
        AND inv_date_sk = d_date_sk
        AND d_year =1999
      GROUP BY w_warehouse_name,w_warehouse_sk,i_item_sk,d_moy) foo
 WHERE CASE mean WHEN 0 THEN 0 ELSE stdev/mean END > 1)
SELECT inv1.w_warehouse_sk,inv1.i_item_sk,inv1.d_moy,inv1.mean, inv1.cov
        ,inv2.w_warehouse_sk,inv2.i_item_sk,inv2.d_moy,inv2.mean, inv2.cov
FROM inv inv1,inv inv2
WHERE inv1.i_item_sk = inv2.i_item_sk
  AND inv1.w_warehouse_sk =  inv2.w_warehouse_sk
  AND inv1.d_moy=3
  AND inv2.d_moy=3+1
ORDER BY inv1.w_warehouse_sk,inv1.i_item_sk,inv1.d_moy,inv1.mean,inv1.cov
        ,inv2.d_moy,inv2.mean, inv2.cov
;

EXPLAIN
WITH test AS
(SELECT inv_date_sk , inv_item_sk ,inv_quantity_on_hand FROM inventory
  UNION ALL
 SELECT inv_date_sk , inv_item_sk ,inv_quantity_on_hand FROM inventory)
SELECT inv_date_sk , inv_item_sk ,inv_quantity_on_hand FROM test SORT BY inv_quantity_on_hand;

DROP TABLE inv;
DROP TABLE inventory;
DROP TABLE item;
DROP TABLE warehouse;
DROP TABLE date_dim;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=strict;
set hive.optimize.sort.dynamic.partition=false;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

create external table sdp (
  dataint bigint,
  hour int,
  req string,
  cid string,
  caid string
)
row format delimited
fields terminated by ',';

load data local inpath '../../data/files/dynpartdata1.txt' into table sdp;
load data local inpath '../../data/files/dynpartdata2.txt' into table sdp;

create table tdp (cid string, caid string)
partitioned by (dataint bigint, hour int, req string);

insert overwrite table tdp partition (dataint=20150316, hour=16, req)
select cid, caid, req from sdp where dataint=20150316 and hour=16;

select * from tdp order by caid;
show partitions tdp;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.optimize.sort.dynamic.partition=true;
set hive.exec.dynamic.partition=true;
set hive.exec.max.dynamic.partitions=1000;
set hive.exec.max.dynamic.partitions.pernode=1000;
set hive.exec.dynamic.partition.mode=nonstrict;



create table over1k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over1k' into table over1k;

create table over1k_part(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (ds string, t tinyint);

create table over1k_part_limit like over1k_part;

create table over1k_part_buck(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (t tinyint)
       clustered by (si) into 4 buckets;

create table over1k_part_buck_sort(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (t tinyint)
       clustered by (si)
       sorted by (f) into 4 buckets;

-- map-only jobs converted to map-reduce job by hive.optimize.sort.dynamic.partition optimization
explain insert overwrite table over1k_part partition(ds="foo", t) select si,i,b,f,t from over1k where t is null or t=27;
explain insert overwrite table over1k_part_limit partition(ds="foo", t) select si,i,b,f,t from over1k where t is null or t=27 limit 10;
explain insert overwrite table over1k_part_buck partition(t) select si,i,b,f,t from over1k where t is null or t=27;
explain insert overwrite table over1k_part_buck_sort partition(t) select si,i,b,f,t from over1k where t is null or t=27;

insert overwrite table over1k_part partition(ds="foo", t) select si,i,b,f,t from over1k where t is null or t=27;
insert overwrite table over1k_part_limit partition(ds="foo", t) select si,i,b,f,t from over1k where t is null or t=27 limit 10;
insert overwrite table over1k_part_buck partition(t) select si,i,b,f,t from over1k where t is null or t=27;
insert overwrite table over1k_part_buck_sort partition(t) select si,i,b,f,t from over1k where t is null or t=27;




-- map-reduce jobs modified by hive.optimize.sort.dynamic.partition optimization
explain insert into table over1k_part partition(ds="foo", t) select si,i,b,f,t from over1k where t is null or t=27;
explain insert into table over1k_part_limit partition(ds="foo", t) select si,i,b,f,t from over1k where t is null or t=27 limit 10;
explain insert into table over1k_part_buck partition(t) select si,i,b,f,t from over1k where t is null or t=27;
explain insert into table over1k_part_buck_sort partition(t) select si,i,b,f,t from over1k where t is null or t=27;

insert into table over1k_part partition(ds="foo", t) select si,i,b,f,t from over1k where t is null or t=27;
insert into table over1k_part_limit partition(ds="foo", t) select si,i,b,f,t from over1k where t is null or t=27 limit 10;
insert into table over1k_part_buck partition(t) select si,i,b,f,t from over1k where t is null or t=27;
insert into table over1k_part_buck_sort partition(t) select si,i,b,f,t from over1k where t is null or t=27;

desc formatted over1k_part partition(ds="foo",t=27);
desc formatted over1k_part partition(ds="foo",t="__HIVE_DEFAULT_PARTITION__");
desc formatted over1k_part_limit partition(ds="foo",t=27);
desc formatted over1k_part_limit partition(ds="foo",t="__HIVE_DEFAULT_PARTITION__");
desc formatted over1k_part_buck partition(t=27);
desc formatted over1k_part_buck partition(t="__HIVE_DEFAULT_PARTITION__");
desc formatted over1k_part_buck_sort partition(t=27);
desc formatted over1k_part_buck_sort partition(t="__HIVE_DEFAULT_PARTITION__");

select count(*) from over1k_part;
select count(*) from over1k_part_limit;
select count(*) from over1k_part_buck;
select count(*) from over1k_part_buck_sort;

-- tests for HIVE-6883
create table over1k_part2(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (ds string, t tinyint);

set hive.optimize.sort.dynamic.partition=false;
explain insert overwrite table over1k_part2 partition(ds="foo",t) select si,i,b,f,t from over1k where t is null or t=27 order by i;
set hive.optimize.sort.dynamic.partition=true;
explain insert overwrite table over1k_part2 partition(ds="foo",t) select si,i,b,f,t from over1k where t is null or t=27 order by i;
explain insert overwrite table over1k_part2 partition(ds="foo",t) select si,i,b,f,t from (select * from over1k order by i limit 10) tmp where t is null or t=27;

set hive.optimize.sort.dynamic.partition=false;
explain insert overwrite table over1k_part2 partition(ds="foo",t) select si,i,b,f,t from over1k where t is null or t=27 group by si,i,b,f,t;
set hive.optimize.sort.dynamic.partition=true;
-- tests for HIVE-8162, only partition column 't' should be in last RS operator
explain insert overwrite table over1k_part2 partition(ds="foo",t) select si,i,b,f,t from over1k where t is null or t=27 group by si,i,b,f,t;

set hive.optimize.sort.dynamic.partition=false;
insert overwrite table over1k_part2 partition(ds="foo",t) select si,i,b,f,t from over1k where t is null or t=27 order by i;

desc formatted over1k_part2 partition(ds="foo",t=27);
desc formatted over1k_part2 partition(ds="foo",t="__HIVE_DEFAULT_PARTITION__");

-- SORT_BEFORE_DIFF
select * from over1k_part2;
select count(*) from over1k_part2;

set hive.optimize.sort.dynamic.partition=true;
insert overwrite table over1k_part2 partition(ds="foo",t) select si,i,b,f,t from over1k where t is null or t=27 order by i;

desc formatted over1k_part2 partition(ds="foo",t=27);
desc formatted over1k_part2 partition(ds="foo",t="__HIVE_DEFAULT_PARTITION__");

-- SORT_BEFORE_DIFF
select * from over1k_part2;
select count(*) from over1k_part2;

-- hadoop-1 does not honor number of reducers in local mode. There is always only 1 reducer irrespective of the number of buckets.
-- Hence all records go to one bucket and all other buckets will be empty. Similar to HIVE-6867. However, hadoop-2 honors number
-- of reducers and records are spread across all reducers. To avoid this inconsistency we will make number of buckets to 1 for this test.
create table over1k_part_buck_sort2(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (t tinyint)
       clustered by (si)
       sorted by (f) into 1 buckets;

set hive.optimize.sort.dynamic.partition=false;
explain insert overwrite table over1k_part_buck_sort2 partition(t) select si,i,b,f,t from over1k where t is null or t=27;
set hive.optimize.sort.dynamic.partition=true;
explain insert overwrite table over1k_part_buck_sort2 partition(t) select si,i,b,f,t from over1k where t is null or t=27;

set hive.optimize.sort.dynamic.partition=false;
insert overwrite table over1k_part_buck_sort2 partition(t) select si,i,b,f,t from over1k where t is null or t=27;

desc formatted over1k_part_buck_sort2 partition(t=27);
desc formatted over1k_part_buck_sort2 partition(t="__HIVE_DEFAULT_PARTITION__");

select * from over1k_part_buck_sort2;
select count(*) from over1k_part_buck_sort2;

set hive.optimize.sort.dynamic.partition=true;
insert overwrite table over1k_part_buck_sort2 partition(t) select si,i,b,f,t from over1k where t is null or t=27;

desc formatted over1k_part_buck_sort2 partition(t=27);
desc formatted over1k_part_buck_sort2 partition(t="__HIVE_DEFAULT_PARTITION__");

select * from over1k_part_buck_sort2;
select count(*) from over1k_part_buck_sort2;

create table over1k_part3(
           si smallint,
           b bigint,
           f float)
       partitioned by (s string, t tinyint, i int);

set hive.optimize.sort.dynamic.partition=true;
explain insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where s="foo";
explain insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where t=27;
explain insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100;
explain insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and t=27;
explain insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and s="foo";
explain insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where t=27 and s="foo";
explain insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and t=27 and s="foo";

insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where s="foo";
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where t=27;
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100;
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and t=27;
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and s="foo";
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where t=27 and s="foo";
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and t=27 and s="foo";

select sum(hash(*)) from over1k_part3;

-- cross verify results with SDPO disabled
drop table over1k_part3;
create table over1k_part3(
           si smallint,
           b bigint,
           f float)
       partitioned by (s string, t tinyint, i int);
set hive.optimize.sort.dynamic.partition=false;
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where s="foo";
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where t=27;
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100;
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and t=27;
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and s="foo";
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where t=27 and s="foo";
insert overwrite table over1k_part3 partition(s,t,i) select si,b,f,s,t,i from over1k where i=100 and t=27 and s="foo";

select sum(hash(*)) from over1k_part3;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.optimize.sort.dynamic.partition=true;
set hive.exec.dynamic.partition=true;
set hive.exec.max.dynamic.partitions=1000;
set hive.exec.max.dynamic.partitions.pernode=1000;
set hive.exec.dynamic.partition.mode=nonstrict;



-- SORT_QUERY_RESULTS

drop table ss;
drop table ss_orc;
drop table ss_part;
drop table ss_part_orc;

create table ss (
ss_sold_date_sk int,
ss_net_paid_inc_tax float,
ss_net_profit float);

create table ss_part (
ss_net_paid_inc_tax float,
ss_net_profit float)
partitioned by (ss_sold_date_sk int);

load data local inpath '../../data/files/dynpart_test.txt' overwrite into table ss;

explain insert overwrite table ss_part partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
  group by ss_sold_date_sk,
    ss_net_paid_inc_tax,
    ss_net_profit
    distribute by ss_sold_date_sk;

insert overwrite table ss_part partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
  group by ss_sold_date_sk,
    ss_net_paid_inc_tax,
    ss_net_profit
    distribute by ss_sold_date_sk;

desc formatted ss_part partition(ss_sold_date_sk=2452617);
select * from ss_part where ss_sold_date_sk=2452617;

desc formatted ss_part partition(ss_sold_date_sk=2452638);
select * from ss_part where ss_sold_date_sk=2452638;

explain insert overwrite table ss_part partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
    distribute by ss_sold_date_sk;

insert overwrite table ss_part partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
    distribute by ss_sold_date_sk;

desc formatted ss_part partition(ss_sold_date_sk=2452617);
select * from ss_part where ss_sold_date_sk=2452617;

desc formatted ss_part partition(ss_sold_date_sk=2452638);
select * from ss_part where ss_sold_date_sk=2452638;

set hive.optimize.sort.dynamic.partition=false;
-- SORT DYNAMIC PARTITION DISABLED

explain insert overwrite table ss_part partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
  group by ss_sold_date_sk,
    ss_net_paid_inc_tax,
    ss_net_profit
    distribute by ss_sold_date_sk;

insert overwrite table ss_part partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
  group by ss_sold_date_sk,
    ss_net_paid_inc_tax,
    ss_net_profit
    distribute by ss_sold_date_sk;

desc formatted ss_part partition(ss_sold_date_sk=2452617);
select * from ss_part where ss_sold_date_sk=2452617;

desc formatted ss_part partition(ss_sold_date_sk=2452638);
select * from ss_part where ss_sold_date_sk=2452638;

explain insert overwrite table ss_part partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
    distribute by ss_sold_date_sk;

insert overwrite table ss_part partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
    distribute by ss_sold_date_sk;

desc formatted ss_part partition(ss_sold_date_sk=2452617);
select * from ss_part where ss_sold_date_sk=2452617;

desc formatted ss_part partition(ss_sold_date_sk=2452638);
select * from ss_part where ss_sold_date_sk=2452638;

set hive.vectorized.execution.enabled=true;
-- VECTORIZATION IS ENABLED

create table ss_orc (
ss_sold_date_sk int,
ss_net_paid_inc_tax float,
ss_net_profit float) stored as orc;

create table ss_part_orc (
ss_net_paid_inc_tax float,
ss_net_profit float)
partitioned by (ss_sold_date_sk int) stored as orc;

insert overwrite table ss_orc select * from ss;

drop table ss;
drop table ss_part;

explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss_orc
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
  group by ss_sold_date_sk,
    ss_net_paid_inc_tax,
    ss_net_profit
    distribute by ss_sold_date_sk;

insert overwrite table ss_part_orc partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss_orc
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
  group by ss_sold_date_sk,
    ss_net_paid_inc_tax,
    ss_net_profit
    distribute by ss_sold_date_sk;

desc formatted ss_part_orc partition(ss_sold_date_sk=2452617);
select * from ss_part_orc where ss_sold_date_sk=2452617;

desc formatted ss_part_orc partition(ss_sold_date_sk=2452638);
select * from ss_part_orc where ss_sold_date_sk=2452638;

explain insert overwrite table ss_part_orc partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss_orc
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
    distribute by ss_sold_date_sk;

insert overwrite table ss_part_orc partition (ss_sold_date_sk)
select ss_net_paid_inc_tax,
  ss_net_profit,
  ss_sold_date_sk
  from ss_orc
  where ss_sold_date_sk>=2452617 and ss_sold_date_sk<=2452638
    distribute by ss_sold_date_sk;

desc formatted ss_part_orc partition(ss_sold_date_sk=2452617);
select * from ss_part_orc where ss_sold_date_sk=2452617;

desc formatted ss_part_orc partition(ss_sold_date_sk=2452638);
select * from ss_part_orc where ss_sold_date_sk=2452638;

drop table ss_orc;
drop table ss_part_orc;

drop table if exists hive13_dp1;
create table if not exists hive13_dp1 (
    k1 int,
    k2 int
)
PARTITIONED BY(`day` string)
STORED AS ORC;

set hive.optimize.sort.dynamic.partition=false;
explain insert overwrite table `hive13_dp1` partition(`day`)
select
    key k1,
    count(value) k2,
    "day" `day`
from src
group by "day", key;

insert overwrite table `hive13_dp1` partition(`day`)
select
    key k1,
    count(value) k2,
    "day" `day`
from src
group by "day", key;
select * from hive13_dp1 order by k1, k2 limit 5;

set hive.optimize.sort.dynamic.partition=true;
explain insert overwrite table `hive13_dp1` partition(`day`)
select
    key k1,
    count(value) k2,
    "day" `day`
from src
group by "day", key;

insert overwrite table `hive13_dp1` partition(`day`)
select
    key k1,
    count(value) k2,
    "day" `day`
from src
group by "day", key;

select * from hive13_dp1 order by k1, k2 limit 5;

drop table hive13_dp1;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

set hive.exec.dynamic.partition.mode=nonstrict;

set hive.optimize.sort.dynamic.partition=false;

-- single level partition, sorted dynamic partition disabled
drop table acid;
CREATE TABLE acid(key string, value string) PARTITIONED BY(ds string) CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid partition(ds)  select key,value,ds from srcpart;
select count(*) from acid where ds='2008-04-08';

insert into table acid partition(ds='2008-04-08') values("foo", "bar");
select count(*) from acid where ds='2008-04-08';

explain update acid set value = 'bar' where key = 'foo' and ds='2008-04-08';
update acid set value = 'bar' where key = 'foo' and ds='2008-04-08';
select count(*) from acid where ds='2008-04-08';

explain update acid set value = 'bar' where key = 'foo' and ds in ('2008-04-08');
update acid set value = 'bar' where key = 'foo' and ds in ('2008-04-08');
select count(*) from acid where ds in ('2008-04-08');

delete from acid where key = 'foo' and ds='2008-04-08';
select count(*) from acid where ds='2008-04-08';

set hive.optimize.sort.dynamic.partition=true;

-- single level partition, sorted dynamic partition enabled
drop table acid;
CREATE TABLE acid(key string, value string) PARTITIONED BY(ds string) CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid partition(ds)  select key,value,ds from srcpart;
select count(*) from acid where ds='2008-04-08';

insert into table acid partition(ds='2008-04-08') values("foo", "bar");
select count(*) from acid where ds='2008-04-08';

explain update acid set value = 'bar' where key = 'foo' and ds='2008-04-08';
update acid set value = 'bar' where key = 'foo' and ds='2008-04-08';
select count(*) from acid where ds='2008-04-08';

explain update acid set value = 'bar' where key = 'foo' and ds in ('2008-04-08');
update acid set value = 'bar' where key = 'foo' and ds in ('2008-04-08');
select count(*) from acid where ds in ('2008-04-08');

delete from acid where key = 'foo' and ds='2008-04-08';
select count(*) from acid where ds='2008-04-08';

set hive.optimize.sort.dynamic.partition=false;

-- 2 level partition, sorted dynamic partition disabled
drop table acid;
CREATE TABLE acid(key string, value string) PARTITIONED BY(ds string, hr int) CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid partition(ds,hr)  select * from srcpart;
select count(*) from acid where ds='2008-04-08' and hr=11;

insert into table acid partition(ds='2008-04-08',hr=11) values("foo", "bar");
select count(*) from acid where ds='2008-04-08' and hr=11;

explain update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr=11;
update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr=11;
select count(*) from acid where ds='2008-04-08' and hr=11;

explain update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr>=11;
update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr>=11;
select count(*) from acid where ds='2008-04-08' and hr>=11;

delete from acid where key = 'foo' and ds='2008-04-08' and hr=11;
select count(*) from acid where ds='2008-04-08' and hr=11;

set hive.optimize.sort.dynamic.partition=true;

-- 2 level partition, sorted dynamic partition enabled
drop table acid;
CREATE TABLE acid(key string, value string) PARTITIONED BY(ds string, hr int) CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid partition(ds,hr)  select * from srcpart;
select count(*) from acid where ds='2008-04-08' and hr=11;

insert into table acid partition(ds='2008-04-08',hr=11) values("foo", "bar");
select count(*) from acid where ds='2008-04-08' and hr=11;

explain update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr=11;
update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr=11;
select count(*) from acid where ds='2008-04-08' and hr=11;

explain update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr>=11;
update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr>=11;
select count(*) from acid where ds='2008-04-08' and hr>=11;

delete from acid where key = 'foo' and ds='2008-04-08' and hr=11;
select count(*) from acid where ds='2008-04-08' and hr=11;

set hive.optimize.sort.dynamic.partition=true;
set hive.optimize.constant.propagation=false;

-- 2 level partition, sorted dynamic partition enabled, constant propagation disabled
drop table acid;
CREATE TABLE acid(key string, value string) PARTITIONED BY(ds string, hr int) CLUSTERED BY(key) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid partition(ds,hr)  select * from srcpart;
select count(*) from acid where ds='2008-04-08' and hr=11;

insert into table acid partition(ds='2008-04-08',hr=11) values("foo", "bar");
select count(*) from acid where ds='2008-04-08' and hr=11;

explain update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr=11;
update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr=11;
select count(*) from acid where ds='2008-04-08' and hr=11;

explain update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr>=11;
update acid set value = 'bar' where key = 'foo' and ds='2008-04-08' and hr>=11;
select count(*) from acid where ds='2008-04-08' and hr>=11;

delete from acid where key = 'foo' and ds='2008-04-08' and hr=11;
select count(*) from acid where ds='2008-04-08' and hr=11;

set hive.optimize.sort.dynamic.partition=true;
set hive.mapred.mode=nonstrict;
set hive.vectorized.execution.enabled=false;

drop table if exists t1_staging;
create table t1_staging(
a string,
b int,
c int,
d string)
partitioned by (e  string)
clustered by(a)
sorted by(a desc)
into 256 buckets stored as textfile;

load data local inpath '../../data/files/sortdp.txt' overwrite into table t1_staging partition (e='epart');

set hive.optimize.sort.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;



drop table t1;

create table t1(
a string,
b int,
c int,
d string)
partitioned by (e string)
clustered by(a)
sorted by(a desc) into 10 buckets stored as textfile;

insert overwrite table t1 partition(e) select a,b,c,d,'epart' from t1_staging;

select 'bucket_0';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000000_0;
select 'bucket_2';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000002_0;
select 'bucket_4';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000004_0;
select 'bucket_6';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000006_0;
select 'bucket_8';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000008_0;

set hive.optimize.sort.dynamic.partition=false;
set hive.exec.dynamic.partition.mode=nonstrict;



-- disable sorted dynamic partition optimization to make sure the results are correct
drop table t1;

create table t1(
a string,
b int,
c int,
d string)
partitioned by (e string)
clustered by(a)
sorted by(a desc) into 10 buckets stored as textfile;

insert overwrite table t1 partition(e) select a,b,c,d,'epart' from t1_staging;

select 'bucket_0';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000000_0;
select 'bucket_2';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000002_0;
select 'bucket_4';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000004_0;
select 'bucket_6';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000006_0;
select 'bucket_8';
dfs -cat ${hiveconf:hive.metastore.warehouse.dir}/t1/e=epart/000008_0;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.optimize.sort.dynamic.partition=true;
set hive.exec.dynamic.partition=true;
set hive.exec.max.dynamic.partitions=1000;
set hive.exec.max.dynamic.partitions.pernode=1000;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=true;



create table over1k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over1k' into table over1k;

create table over1k_orc like over1k;
alter table over1k_orc set fileformat orc;
insert overwrite table over1k_orc select * from over1k;

create table over1k_part_orc(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (ds string, t tinyint) stored as orc;

create table over1k_part_limit_orc like over1k_part_orc;
alter table over1k_part_limit_orc set fileformat orc;

create table over1k_part_buck_orc(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (t tinyint)
       clustered by (si) into 4 buckets stored as orc;

create table over1k_part_buck_sort_orc(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (t tinyint)
       clustered by (si)
       sorted by (f) into 4 buckets stored as orc;

-- map-only jobs converted to map-reduce job by hive.optimize.sort.dynamic.partition optimization
explain insert overwrite table over1k_part_orc partition(ds="foo", t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by si;
explain insert overwrite table over1k_part_limit_orc partition(ds="foo", t) select si,i,b,f,t from over1k_orc where t is null or t=27 limit 10;
explain insert overwrite table over1k_part_buck_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;
explain insert overwrite table over1k_part_buck_sort_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;

insert overwrite table over1k_part_orc partition(ds="foo", t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by si;
insert overwrite table over1k_part_limit_orc partition(ds="foo", t) select si,i,b,f,t from over1k_orc where t is null or t=27 limit 10;
insert overwrite table over1k_part_buck_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;
insert overwrite table over1k_part_buck_sort_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;




-- map-reduce jobs modified by hive.optimize.sort.dynamic.partition optimization
explain insert into table over1k_part_orc partition(ds="foo", t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by si;
explain insert into table over1k_part_limit_orc partition(ds="foo", t) select si,i,b,f,t from over1k_orc where t is null or t=27 limit 10;
explain insert into table over1k_part_buck_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;
explain insert into table over1k_part_buck_sort_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;

insert into table over1k_part_orc partition(ds="foo", t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by si;
insert into table over1k_part_limit_orc partition(ds="foo", t) select si,i,b,f,t from over1k_orc where t is null or t=27 limit 10;
insert into table over1k_part_buck_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;
insert into table over1k_part_buck_sort_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;

desc formatted over1k_part_orc partition(ds="foo",t=27);
desc formatted over1k_part_orc partition(ds="foo",t="__HIVE_DEFAULT_PARTITION__");
desc formatted over1k_part_limit_orc partition(ds="foo",t=27);
desc formatted over1k_part_limit_orc partition(ds="foo",t="__HIVE_DEFAULT_PARTITION__");
desc formatted over1k_part_buck_orc partition(t=27);
desc formatted over1k_part_buck_orc partition(t="__HIVE_DEFAULT_PARTITION__");
desc formatted over1k_part_buck_sort_orc partition(t=27);
desc formatted over1k_part_buck_sort_orc partition(t="__HIVE_DEFAULT_PARTITION__");

select count(*) from over1k_part_orc;
select count(*) from over1k_part_limit_orc;
select count(*) from over1k_part_buck_orc;
select count(*) from over1k_part_buck_sort_orc;

-- tests for HIVE-6883
create table over1k_part2_orc(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (ds string, t tinyint);

set hive.optimize.sort.dynamic.partition=false;
explain insert overwrite table over1k_part2_orc partition(ds="foo",t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by i;
set hive.optimize.sort.dynamic.partition=true;
explain insert overwrite table over1k_part2_orc partition(ds="foo",t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by i;
explain insert overwrite table over1k_part2_orc partition(ds="foo",t) select si,i,b,f,t from (select * from over1k_orc order by i limit 10) tmp where t is null or t=27;

set hive.optimize.sort.dynamic.partition=false;
explain insert overwrite table over1k_part2_orc partition(ds="foo",t) select si,i,b,f,t from over1k_orc where t is null or t=27 group by si,i,b,f,t;
set hive.optimize.sort.dynamic.partition=true;
-- tests for HIVE-8162, only partition column 't' should be in last RS operator
explain insert overwrite table over1k_part2_orc partition(ds="foo",t) select si,i,b,f,t from over1k_orc where t is null or t=27 group by si,i,b,f,t;

set hive.optimize.sort.dynamic.partition=false;
insert overwrite table over1k_part2_orc partition(ds="foo",t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by i;

desc formatted over1k_part2_orc partition(ds="foo",t=27);
desc formatted over1k_part2_orc partition(ds="foo",t="__HIVE_DEFAULT_PARTITION__");

-- SORT_BEFORE_DIFF
select * from over1k_part2_orc;
select count(*) from over1k_part2_orc;

set hive.optimize.sort.dynamic.partition=true;
insert overwrite table over1k_part2_orc partition(ds="foo",t) select si,i,b,f,t from over1k_orc where t is null or t=27 order by i;

desc formatted over1k_part2_orc partition(ds="foo",t=27);
desc formatted over1k_part2_orc partition(ds="foo",t="__HIVE_DEFAULT_PARTITION__");

-- SORT_BEFORE_DIFF
select * from over1k_part2_orc;
select count(*) from over1k_part2_orc;

-- hadoop-1 does not honor number of reducers in local mode. There is always only 1 reducer irrespective of the number of buckets.
-- Hence all records go to one bucket and all other buckets will be empty. Similar to HIVE-6867. However, hadoop-2 honors number
-- of reducers and records are spread across all reducers. To avoid this inconsistency we will make number of buckets to 1 for this test.
create table over1k_part_buck_sort2_orc(
           si smallint,
           i int,
           b bigint,
           f float)
       partitioned by (t tinyint)
       clustered by (si)
       sorted by (f) into 1 buckets;

set hive.optimize.sort.dynamic.partition=false;
explain insert overwrite table over1k_part_buck_sort2_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;
set hive.optimize.sort.dynamic.partition=true;
explain insert overwrite table over1k_part_buck_sort2_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;

set hive.optimize.sort.dynamic.partition=false;
insert overwrite table over1k_part_buck_sort2_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;

desc formatted over1k_part_buck_sort2_orc partition(t=27);
desc formatted over1k_part_buck_sort2_orc partition(t="__HIVE_DEFAULT_PARTITION__");

explain select * from over1k_part_buck_sort2_orc;
select * from over1k_part_buck_sort2_orc;
explain select count(*) from over1k_part_buck_sort2_orc;
select count(*) from over1k_part_buck_sort2_orc;

set hive.optimize.sort.dynamic.partition=true;
insert overwrite table over1k_part_buck_sort2_orc partition(t) select si,i,b,f,t from over1k_orc where t is null or t=27;

desc formatted over1k_part_buck_sort2_orc partition(t=27);
desc formatted over1k_part_buck_sort2_orc partition(t="__HIVE_DEFAULT_PARTITION__");

explain select * from over1k_part_buck_sort2_orc;
select * from over1k_part_buck_sort2_orc;
explain select count(*) from over1k_part_buck_sort2_orc;
select count(*) from over1k_part_buck_sort2_orc;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nostrict;
set hive.exec.max.dynamic.partitions=2;


create table dynamic_partition (key string) partitioned by (value string);

insert overwrite table dynamic_partition partition(hr) select key, value from src;




create table nzhang_part1 (key string, value string) partitioned by (ds string, hr string);

set hive.exec.dynamic.partition=true;

insert overwrite table nzhang_part1 partition(ds='11', hr) select key, value from srcpart where ds is not null;

show partitions nzhang_part1;



set hive.exec.max.dynamic.partitions=600;
set hive.exec.max.dynamic.partitions.pernode=600;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.max.created.files=100;

create table nzhang_part( key string) partitioned by (value string);

insert overwrite table nzhang_part partition(value) select key, value from src;
create table nzhang_part4 (key string) partitioned by (ds string, hr string, value string);

set hive.exec.dynamic.partition=true;

insert overwrite table nzhang_part4 partition(value = 'aaa', ds='11', hr) select key, hr from srcpart where ds is not null;

drop table nzhang_part4;
USE default;

-- Test of hive.exec.max.dynamic.partitions
-- Set hive.exec.max.dynamic.partitions.pernode to a large value so it will be ignored

CREATE TABLE max_parts(key STRING) PARTITIONED BY (value STRING);

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions=10;
set hive.exec.max.dynamic.partitions.pernode=1000;

INSERT OVERWRITE TABLE max_parts PARTITION(value)
SELECT key, value
FROM src
LIMIT 50;
USE default;

-- Test of hive.exec.max.dynamic.partitions.pernode

CREATE TABLE max_parts(key STRING) PARTITIONED BY (value STRING);

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions=1000;
set hive.exec.max.dynamic.partitions.pernode=10;

INSERT OVERWRITE TABLE max_parts PARTITION(value)
SELECT key, value
FROM src
LIMIT 50;
set hive.mapred.mode=nonstrict;
dfs ${system:test.dfs.mkdir} hdfs:///target/tmp/test_empty_table;

create external table roottable (key string) row format delimited fields terminated by '\\t' stored as textfile location 'hdfs:///target/tmp/test_empty_table';
select count(*) from roottable;

insert into table roottable select key from src where (key < 20) order by key;
select count(*) from roottable;

dfs ${system:test.dfs.mkdir} hdfs:///target/tmp/test_empty_table/empty;
select count(*) from roottable;set hive.auto.convert.join=true;
set hive.mapjoin.hybridgrace.hashtable=false;

DROP TABLE IF EXISTS test_1;
CREATE TABLE test_1 AS SELECT 1 AS id;

DROP TABLE IF EXISTS test_2;
CREATE TABLE test_2 (id INT);

DROP TABLE IF EXISTS test_3;
CREATE TABLE test_3 AS SELECT 1 AS id;

explain
SELECT t1.id, t2.id, t3.id
FROM test_1 t1
LEFT JOIN test_2 t2 ON t1.id = t2.id
INNER JOIN test_3 t3 ON t1.id = t3.id;

SELECT t1.id, t2.id, t3.id
FROM test_1 t1
LEFT JOIN test_2 t2 ON t1.id = t2.id
INNER JOIN test_3 t3 ON t1.id = t3.id
;
drop table if exists encodelat1;
create table encodelat1 (name STRING)
 ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
 WITH SERDEPROPERTIES ('serialization.encoding'='ISO8859_1');
load data local inpath '../../data/files/encoding_iso-8859-1.txt' overwrite into table encodelat1;
select * from encodelat1;

-- SORT_QUERY_RESULTS;

-- we're setting this so that TestNegaiveCliDriver.vm doesn't stop processing after DROP TABLE fails;

set hive.cli.errors.ignore=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS encrypted_table_dp PURGE;
CREATE TABLE encrypted_table_dp (key INT, value STRING) partitioned by (p STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table_dp';
CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table_dp;

INSERT INTO encrypted_table_dp PARTITION(p)(p,key,value) values('2014-09-23', 1, 'foo'),('2014-09-24', 2, 'bar');
SELECT * FROM encrypted_table_dp;

CREATE EXTERNAL TABLE encrypted_ext_table_dp (key INT, value STRING) partitioned by (p STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table_dp';
ALTER TABLE encrypted_ext_table_dp ADD PARTITION (p='2014-09-23') LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table_dp/p=2014-09-23';
SELECT * FROM encrypted_ext_table_dp;
ALTER TABLE encrypted_ext_table_dp DROP PARTITION (p='2014-09-23');
SELECT * FROM encrypted_ext_table_dp;
DROP TABLE encrypted_ext_table_dp;

SELECT * FROM encrypted_table_dp;
ALTER TABLE encrypted_table_dp DROP PARTITION (p='2014-09-23');
SELECT * FROM encrypted_table_dp;
ALTER TABLE encrypted_table_dp DROP PARTITION (p='2014-09-23') PURGE;
SELECT * FROM encrypted_table_dp;
DROP TABLE encrypted_table_dp PURGE;
-- SORT_QUERY_RESULTS;

-- we're setting this so that TestNegaiveCliDriver.vm doesn't stop processing after DROP TABLE fails;

set hive.cli.errors.ignore=true;

DROP TABLE IF EXISTS encrypted_table PURGE;
CREATE TABLE encrypted_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table';
CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table;

INSERT OVERWRITE TABLE encrypted_table SELECT * FROM src;

CREATE EXTERNAL TABLE encrypted_ext_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table';
SHOW TABLES;

DROP TABLE default.encrypted_ext_table;
SHOW TABLES;

DROP TABLE default.encrypted_table;
SHOW TABLES;

DROP TABLE default.encrypted_table PURGE;
SHOW TABLES;
CRYPTO DELETE_KEY --keyName key_128;
DROP TABLE IF EXISTS dve_encrypted_table PURGE;
CREATE TABLE dve_encrypted_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/dve_encrypted_table';
CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/dve_encrypted_table;
CREATE VIEW dve_view AS SELECT * FROM dve_encrypted_table;
DROP VIEW dve_view;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


-- SORT_QUERY_RESULTS

-- init
drop table IF EXISTS encryptedTable PURGE;
drop table IF EXISTS unencryptedTable PURGE;

create table encryptedTable(value string)
    partitioned by (key string) clustered by (value) into 2 buckets stored as orc
    LOCATION '${hiveconf:hive.metastore.warehouse.dir}/encryptedTable' TBLPROPERTIES ('transactional'='true');
CRYPTO CREATE_KEY --keyName key_1 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_1 --path ${hiveconf:hive.metastore.warehouse.dir}/encryptedTable;

create table unencryptedTable(value string)
    partitioned by (key string) clustered by (value) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

-- insert encrypted table from values
explain extended insert into table encryptedTable partition (key) values
    ('val_501', '501'),
    ('val_502', '502');

insert into table encryptedTable partition (key) values
    ('val_501', '501'),
    ('val_502', '502');

select * from encryptedTable order by key;

-- insert encrypted table from unencrypted source
explain extended from src
insert into table encryptedTable partition (key)
    select value, key limit 2;

from src
insert into table encryptedTable partition (key)
    select value, key limit 2;

select * from encryptedTable order by key;

-- insert unencrypted table from encrypted source
explain extended from encryptedTable
insert into table unencryptedTable partition (key)
    select value, key;

from encryptedTable
insert into table unencryptedTable partition (key)
    select value, key;

select * from unencryptedTable order by key;

-- clean up
drop table encryptedTable PURGE;
CRYPTO DELETE_KEY --keyName key_1;
drop table unencryptedTable PURGE;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.mapred.mode=nonstrict;

-- SORT_QUERY_RESULTS

-- init
drop table IF EXISTS encryptedTable PURGE;
drop table IF EXISTS unencryptedTable PURGE;

create table encryptedTable(key string,
    value string) partitioned by (ds string) clustered by (key) into 2 buckets stored as orc
    LOCATION '${hiveconf:hive.metastore.warehouse.dir}/encryptedTable' TBLPROPERTIES ('transactional'='true');
CRYPTO CREATE_KEY --keyName key_1 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_1 --path ${hiveconf:hive.metastore.warehouse.dir}/encryptedTable;

create table unencryptedTable(key string,
    value string) partitioned by (ds string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

-- insert encrypted table from values
insert into table encryptedTable partition
    (ds='today') values
    ('501', 'val_501'),
    ('502', 'val_502');

select * from encryptedTable order by key;

-- insert encrypted table from unencrypted source
insert into table encryptedTable partition (ds='yesterday')
select * from src where key in ('238', '86');

select * from encryptedTable order by key;

-- insert unencrypted table from encrypted source
insert into table unencryptedTable partition (ds='today')
select key, value from encryptedTable where ds='today';

insert into table unencryptedTable partition (ds='yesterday')
select key, value from encryptedTable where ds='yesterday';

select * from unencryptedTable order by key;

-- clean up
drop table encryptedTable PURGE;
CRYPTO DELETE_KEY --keyName key_1;
drop table unencryptedTable PURGE;
-- SORT_QUERY_RESULTS;

DROP TABLE IF EXISTS encrypted_table PURGE;
CREATE TABLE encrypted_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table';
CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table;

INSERT INTO encrypted_table values(1,'foo'),(2,'bar');

select * from encrypted_table;

-- this checks that we've actually created temp table data under encrypted_table folder
describe formatted values__tmp__table__1;

CRYPTO DELETE_KEY --keyName key_128;--SORT_QUERY_RESULTS

DROP TABLE IF EXISTS encrypted_table PURGE;
CREATE TABLE encrypted_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table';
CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table;
set hive.mapred.mode=nonstrict;
INSERT OVERWRITE TABLE encrypted_table SELECT * FROM src;

SELECT * FROM encrypted_table;

EXPLAIN EXTENDED SELECT * FROM src t1 JOIN encrypted_table t2 WHERE t1.key = t2.key;

drop table encrypted_table PURGE;
CRYPTO DELETE_KEY --keyName key_128;
--SORT_QUERY_RESULTS

-- Java JCE must be installed in order to hava a key length of 256 bits
DROP TABLE IF EXISTS table_key_1 PURGE;
CREATE TABLE table_key_1 (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/table_key_1';
CRYPTO CREATE_KEY --keyName key_1 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_1 --path ${hiveconf:hive.metastore.warehouse.dir}/table_key_1;

DROP TABLE IF EXISTS table_key_2 PURGE;
CREATE TABLE table_key_2 (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/table_key_2';
CRYPTO CREATE_KEY --keyName key_2 --bitLength 256;
CRYPTO CREATE_ZONE --keyName key_2 --path ${hiveconf:hive.metastore.warehouse.dir}/table_key_2;
set hive.mapred.mode=nonstrict;
INSERT OVERWRITE TABLE table_key_1 SELECT * FROM src;
INSERT OVERWRITE TABLE table_key_2 SELECT * FROM src;

EXPLAIN EXTENDED SELECT * FROM table_key_1 t1 JOIN table_key_2 t2 WHERE (t1.key = t2.key);
SELECT * FROM table_key_1 t1 JOIN table_key_2 t2 WHERE (t1.key = t2.key);

DROP TABLE table_key_1 PURGE;
DROP TABLE table_key_2 PURGE;

CRYPTO DELETE_KEY --keyName key_1;
CRYPTO DELETE_KEY --keyName key_2;
DROP TABLE IF EXISTS encrypted_table PURGE;

CREATE TABLE encrypted_table (key STRING, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/encrypted_table';

-- Create encryption key and zone;
crypto create_key --keyName key1;
crypto create_zone --keyName key1 --path ${hiveconf:hive.metastore.warehouse.dir}/encrypted_table;

-- Test loading data from the local filesystem;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE encrypted_table;
SELECT * FROM encrypted_table;

-- Test loading data from the hdfs filesystem;
dfs -copyFromLocal ../../data/files/kv1.txt hdfs:///tmp/kv1.txt;
LOAD DATA INPATH '/tmp/kv1.txt' OVERWRITE INTO TABLE encrypted_table;
SELECT * FROM encrypted_table;

DROP TABLE encrypted_table PURGE;

crypto delete_key --keyName key1;-- SORT_QUERY_RESULTS;

-- we're setting this so that TestNegaiveCliDriver.vm doesn't stop processing after ALTER TABLE fails;

set hive.cli.errors.ignore=true;

DROP TABLE IF EXISTS encrypted_table PURGE;
CREATE TABLE encrypted_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table';
CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table;

INSERT OVERWRITE TABLE encrypted_table SELECT * FROM src;
SHOW TABLES;
ALTER TABLE default.encrypted_table RENAME TO default.plain_table;
SHOW TABLES;

DROP TABLE encrypted_table PURGE;

CRYPTO DELETE_KEY --keyName key_128;

-- SORT_QUERY_RESULTS

DROP TABLE IF EXISTS encrypted_table PURGE;
CREATE TABLE encrypted_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table';

CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table;

LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE encrypted_table;

dfs -chmod -R 555 ${hiveconf:hive.metastore.warehouse.dir}/default/encrypted_table;

SELECT count(*) FROM encrypted_table;

drop table encrypted_table PURGE;
CRYPTO DELETE_KEY --keyName key_128;
-- SORT_QUERY_RESULTS

DROP TABLE IF EXISTS unencrypted_table;
CREATE TABLE unencrypted_table(key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/unencrypted_table';

LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE unencrypted_table;

dfs -chmod -R 555 ${hiveconf:hive.metastore.warehouse.dir}/default/unencrypted_table;

SELECT count(*) FROM unencrypted_table;

drop table unencrypted_table;-- This test does not test encrypted data, but it makes sure that external tables out of HDFS can
-- be queried due to internal encryption functions;

DROP TABLE mydata;

CREATE EXTERNAL TABLE mydata (key STRING, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
LOCATION 'pfile://${system:test.tmp.dir}/external_mydata';

LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' OVERWRITE INTO TABLE mydata;

SELECT * from mydata;

DROP TABLE mydata;set fs.trash.interval=5

-- SORT_QUERY_RESULTS

-- init
drop table IF EXISTS encryptedTableSrc PURGE;
drop table IF EXISTS unencryptedTable PURGE;

create table encryptedTableSrc(key string, value string)
LOCATION '${hiveconf:hive.metastore.warehouse.dir}/encryptedTableSrc';

create table encryptedTable(key string, value string) partitioned by (ds string)
    LOCATION '${hiveconf:hive.metastore.warehouse.dir}/encryptedTable';
CRYPTO CREATE_KEY --keyName key_1 --bitLength 128;
CRYPTO CREATE_ZONE --keyName key_1 --path ${hiveconf:hive.metastore.warehouse.dir}/encryptedTableSrc;
CRYPTO CREATE_ZONE --keyName key_1 --path ${hiveconf:hive.metastore.warehouse.dir}/encryptedTable;

-- insert src table from values
insert into table encryptedTableSrc values ('501', 'val_501'), ('502', 'val_502');

insert into table encryptedTable partition (ds='today') select key, value from encryptedTableSrc;
select count(*) from encryptedTable where ds='today';
insert into table encryptedTable partition (ds='today') select key, value from encryptedTableSrc;
select count(*) from encryptedTable where ds='today';

insert overwrite table encryptedTable partition (ds='today') select key, value from encryptedTableSrc;
select count(*) from encryptedTable where ds='today';

-- clean up
drop table encryptedTable PURGE;
drop table unencryptedTable PURGE;
CRYPTO DELETE_KEY --keyName key_1;
set fs.trash.interval=0
drop table table_asc;
drop table table_desc;



create table table_asc(key string, value string) clustered by (key) sorted by (key ASC) into 1 BUCKETS;
create table table_desc(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS;

insert overwrite table table_asc select key, value from src;
insert overwrite table table_desc select key, value from src;

select * from table_asc limit 10;
select * from table_desc limit 10;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.max.dynamic.partitions.pernode=200;

-- EXCLUDE_OS_WINDOWS
-- excluded on windows because of difference in file name encoding logic

DROP TABLE escape1;
DROP TABLE escape_raw;

CREATE TABLE escape_raw (s STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/escapetest.txt' INTO TABLE  escape_raw;

SELECT count(*) from escape_raw;
SELECT * from escape_raw;

CREATE TABLE escape1 (a STRING) PARTITIONED BY (ds STRING, part STRING);
INSERT OVERWRITE TABLE escape1 PARTITION (ds='1', part) SELECT '1', s from
escape_raw;

SELECT count(*) from escape1;
SELECT * from escape1;
SHOW PARTITIONS escape1;

ALTER TABLE escape1 DROP PARTITION (ds='1');
SHOW PARTITIONS escape1;

DROP TABLE escape1;
DROP TABLE escape_raw;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.max.dynamic.partitions.pernode=200;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.default.fileformat=RCFILE;

-- EXCLUDE_OS_WINDOWS
-- excluded on windows because of difference in file name encoding logic

DROP TABLE IF EXISTS escape2;
DROP TABLE IF EXISTS escape_raw;

CREATE TABLE escape_raw (s STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/escapetest.txt' INTO TABLE  escape_raw;

SELECT count(*) from escape_raw;
SELECT * from escape_raw;

CREATE TABLE escape2(a STRING) PARTITIONED BY (ds STRING, part STRING);
INSERT OVERWRITE TABLE escape2 PARTITION (ds='1', part) SELECT '1', s from
escape_raw;

SELECT count(*) from escape2;
SELECT * from escape2;
SHOW PARTITIONS escape2;

-- ASCII values 1-31, 59, 92, 127 were not included in the below commands

ALTER table escape2 PARTITION (ds='1', part=' ') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='!') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='"') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='#') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='$') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='%') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='&') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part="'") CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='(') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part=')') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='*') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='+') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part=',') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='-') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='.') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='/') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='0') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='1') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='2') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='3') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='4') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='5') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='6') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='7') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='8') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='9') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part=':') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='<') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='=') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='>') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='?') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='@') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='A') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='B') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='C') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='D') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='E') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='F') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='G') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='H') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='I') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='J') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='K') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='L') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='M') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='N') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='O') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='P') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='Q') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='R') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='S') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='T') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='U') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='V') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='W') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='X') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='Y') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='Z') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='[') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part=']') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='_') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='`') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='a') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='b') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='c') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='d') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='e') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='f') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='g') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='h') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='i') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='j') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='k') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='l') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='m') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='n') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='o') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='p') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='q') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='r') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='s') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='t') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='u') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='v') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='w') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='x') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='y') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='z') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='{') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='|') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='}') CONCATENATE;
ALTER TABLE escape2 PARTITION (ds='1', part='~') CONCATENATE;

DROP TABLE escape2;
DROP TABLE escape_raw;
-- with string
CREATE TABLE escape3_1
(
GERUND STRING,
ABBREV STRING,
CODE SMALLINT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|' ESCAPED BY '\134'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/data_with_escape.txt' INTO TABLE escape3_1;

select * from escape3_1;

-- with varchar
CREATE TABLE escape3_2
(
GERUND VARCHAR(10),
ABBREV VARCHAR(3),
CODE SMALLINT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|' ESCAPED BY '\134'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/data_with_escape.txt' INTO TABLE escape3_2;

select * from escape3_2;

-- with char
CREATE TABLE escape3_3
(
GERUND CHAR(10),
ABBREV CHAR(3),
CODE SMALLINT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|' ESCAPED BY '\134'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/data_with_escape.txt' INTO TABLE escape3_3;

select * from escape3_3;

DROP TABLE escape3_1;
DROP TABLE escape3_2;
DROP TABLE escape3_3;
-- escaped column names in cluster by are not working jira 3267
explain
select key, value from src cluster by key, value;

explain
select `key`, value from src cluster by `key`, value;
DROP TABLE IF EXISTS base_tab;
CREATE TABLE base_tab(a STRING, b STRING)
ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '|';
DESCRIBE EXTENDED base_tab;

LOAD DATA LOCAL INPATH '../../data/files/escape_crlf.txt' OVERWRITE INTO TABLE base_tab;
-- No crlf escaping
SELECT * FROM base_tab;

-- Crlf escaping
ALTER TABLE base_tab SET SERDEPROPERTIES ('escape.delim'='\\', 'serialization.escape.crlf'='true');
SELECT * FROM base_tab;

SET hive.fetch.task.conversion=none;
-- Make sure intermediate serde works correctly
SELECT * FROM base_tab;

DROP TABLE base_tab;
-- escaped column names in distribute by by are not working jira 3267
explain
select key, value from src distribute by key, value;

explain
select `key`, value from src distribute by `key`, value;
set hive.mapred.mode=nonstrict;
-- escaped column names in order by are not working jira 3267
explain
select key, value from src order by key, value;

explain
select `key`, value from src order by `key`, value;
-- escaped column names in sort by are not working jira 3267
explain
select key, value from src sort by key, value;

explain
select `key`, value from src sort by `key`, value;
create database ex1;
create database ex2;

CREATE TABLE ex1.exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING);
CREATE TABLE ex2.exchange_part_test2 (f1 string) PARTITIONED BY (ds STRING);
SHOW PARTITIONS ex1.exchange_part_test1;
SHOW PARTITIONS ex2.exchange_part_test2;

ALTER TABLE ex2.exchange_part_test2 ADD PARTITION (ds='2013-04-05');
SHOW PARTITIONS ex1.exchange_part_test1;
SHOW PARTITIONS ex2.exchange_part_test2;

ALTER TABLE ex1.exchange_part_test1 EXCHANGE PARTITION (ds='2013-04-05') WITH TABLE ex2.exchange_part_test2;
SHOW PARTITIONS ex1.exchange_part_test1;
SHOW PARTITIONS ex2.exchange_part_test2;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/ex_table1;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/ex_table1/part=part1;
CREATE EXTERNAL TABLE ex_table1 ( key INT, value STRING)
    PARTITIONED BY (part STRING)
    STORED AS textfile
        LOCATION 'file:${system:test.tmp.dir}/ex_table1';

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/ex_table2;

CREATE EXTERNAL TABLE ex_table2 ( key INT, value STRING)
    PARTITIONED BY (part STRING)
    STORED AS textfile
	LOCATION 'file:${system:test.tmp.dir}/ex_table2';

INSERT OVERWRITE TABLE ex_table2 PARTITION (part='part1')
SELECT key, value FROM src WHERE key < 10;
SHOW PARTITIONS ex_table2;

ALTER TABLE ex_table1 EXCHANGE PARTITION (part='part1') WITH TABLE ex_table2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
CREATE TABLE exchange_part_test2 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

ALTER TABLE exchange_part_test2 ADD PARTITION (ds='2013-04-05', hr='1');
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds='2013-04-05', hr='1') WITH TABLE exchange_part_test2;
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
CREATE TABLE exchange_part_test2 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

ALTER TABLE exchange_part_test1 ADD PARTITION (ds='2014-01-03', hr='1');
ALTER TABLE exchange_part_test2 ADD PARTITION (ds='2013-04-05', hr='1');
ALTER TABLE exchange_part_test2 ADD PARTITION (ds='2013-04-05', hr='2');
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

-- This will exchange both partitions hr=1 and hr=2
ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds='2013-04-05') WITH TABLE exchange_part_test2;
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
CREATE TABLE exchange_part_test2 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

ALTER TABLE exchange_part_test2 ADD PARTITION (ds='2013-04-05', hr='h1');
ALTER TABLE exchange_part_test2 ADD PARTITION (ds='2013-04-05', hr='h2');
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

-- for exchange_part_test1 the value of ds is not given and the value of hr is given, thus this query will fail
alter table exchange_part_test1 exchange partition (hr='h1') with table exchange_part_test2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING);
CREATE TABLE exchange_part_test2 (f1 string) PARTITIONED BY (ds STRING);
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

ALTER TABLE exchange_part_test1 ADD PARTITION (ds='2013-04-05');
ALTER TABLE exchange_part_test2 ADD PARTITION (ds='2013-04-05');
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

-- exchange_part_test1 table partition (ds='2013-04-05') already exists thus this query will fail
alter table exchange_part_test1 exchange partition (ds='2013-04-05') with table exchange_part_test2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
CREATE TABLE exchange_part_test2 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

ALTER TABLE exchange_part_test1 ADD PARTITION (ds='2013-04-05', hr='1');
ALTER TABLE exchange_part_test1 ADD PARTITION (ds='2013-04-05', hr='2');
ALTER TABLE exchange_part_test2 ADD PARTITION (ds='2013-04-05', hr='3');
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

-- exchange_part_test1 table partition (ds='2013-04-05') already exists thus this query will fail
alter table exchange_part_test1 exchange partition (ds='2013-04-05') with table exchange_part_test2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
CREATE TABLE exchange_part_test2 (f1 string) PARTITIONED BY (ds STRING, hr STRING);
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

ALTER TABLE exchange_part_test1 ADD PARTITION (ds='2013-04-05', hr='1');
ALTER TABLE exchange_part_test1 ADD PARTITION (ds='2013-04-05', hr='2');
ALTER TABLE exchange_part_test2 ADD PARTITION (ds='2013-04-05', hr='1');
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

-- exchange_part_test2 table partition (ds='2013-04-05') already exists thus this query will fail
alter table exchange_part_test1 exchange partition (ds='2013-04-05') with table exchange_part_test2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING);
CREATE TABLE exchange_part_test2 (f1 string) PARTITIONED BY (ds STRING);
SHOW PARTITIONS exchange_part_test1;

-- exchange_part_test2 partition (ds='2013-04-05') does not exist thus this query will fail
alter table exchange_part_test1 exchange partition (ds='2013-04-05') with table exchange_part_test2;
-- t1 does not exist and the query fails
alter table t1 exchange partition (ds='2013-04-05') with table t2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING);
SHOW PARTITIONS exchange_part_test1;

ALTER TABLE exchange_part_test1 ADD PARTITION (ds='2013-04-05');
SHOW PARTITIONS exchange_part_test1;

-- exchange_part_test2 table does not exist thus this query will fail
alter table exchange_part_test1 exchange partition (ds='2013-04-05') with table exchange_part_test2;
CREATE TABLE exchange_part_test1 (f1 string) PARTITIONED BY (ds STRING);
CREATE TABLE exchange_part_test2 (f1 string, f2 string) PARTITIONED BY (ds STRING);
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

ALTER TABLE exchange_part_test1 ADD PARTITION (ds='2013-04-05');
SHOW PARTITIONS exchange_part_test1;
SHOW PARTITIONS exchange_part_test2;

-- exchange_part_test1 and exchange_part_test2 do not have the same scheme and thus they fail
ALTER TABLE exchange_part_test1 EXCHANGE PARTITION (ds='2013-04-05') WITH TABLE exchange_part_test2;
DROP TABLE IF EXISTS t1;
DROP TABLE IF EXISTS t2;
DROP TABLE IF EXISTS t3;
DROP TABLE IF EXISTS t4;

CREATE TABLE t1 (a int) PARTITIONED BY (d1 int);
CREATE TABLE t2 (a int) PARTITIONED BY (d1 int);
CREATE TABLE t3 (a int) PARTITIONED BY (d1 int, d2 int);
CREATE TABLE t4 (a int) PARTITIONED BY (d1 int, d2 int);
CREATE TABLE t5 (a int) PARTITIONED BY (d1 int, d2 int, d3 int);
CREATE TABLE t6 (a int) PARTITIONED BY (d1 int, d2 int, d3 int);
set hive.mapred.mode=nonstrict;
INSERT OVERWRITE TABLE t1 PARTITION (d1 = 1) SELECT key FROM src where key = 100 limit 1;
INSERT OVERWRITE TABLE t3 PARTITION (d1 = 1, d2 = 1) SELECT key FROM src where key = 100 limit 1;
INSERT OVERWRITE TABLE t5 PARTITION (d1 = 1, d2 = 1, d3=1) SELECT key FROM src where key = 100 limit 1;

SELECT * FROM t1;

SELECT * FROM t3;

ALTER TABLE t2 EXCHANGE PARTITION (d1 = 1) WITH TABLE t1;
SELECT * FROM t1;
SELECT * FROM t2;

ALTER TABLE t4 EXCHANGE PARTITION (d1 = 1, d2 = 1) WITH TABLE t3;
SELECT * FROM t3;
SELECT * FROM t4;

ALTER TABLE t6 EXCHANGE PARTITION (d1 = 1, d2 = 1, d3 = 1) WITH TABLE t5;
SELECT * FROM t5;
SELECT * FROM t6;

set hive.exec.parallel=true;

explain analyze table src compute statistics for columns;

analyze table src compute statistics for columns;set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_department';
describe extended exim_department;
show table extended like exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
select * from exim_department;
drop table exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'nosuchschema://nosuchauthority/ql/test/data/exports/exim_department';
drop table exim_department;

set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_department';
describe extended exim_department;
show table extended like exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
select * from exim_department;
drop table exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department identifier")
	stored as textfile
	tblproperties("maker"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

create table exim_employee ( emp_id int comment "employee id")
	comment "table of employees"
	partitioned by (emp_country string comment "iso code", emp_state string comment "free-form text")
	stored as textfile
	tblproperties("maker"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
import from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
select * from exim_employee;
drop table exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_key int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department identifier")
	stored as textfile
	tblproperties("maker"="krishna");
import from 'ql/test/data/exports/exim_department';
describe extended exim_department;
select * from exim_department;
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee (emp_id int comment 'employee id', emp_name string, emp_dob string comment 'employee date of birth', emp_sex string comment 'M/F')
 comment 'employee table'
 partitioned by (emp_country string comment '2-char code', emp_state string comment '2-char code')
 clustered by (emp_sex) sorted by (emp_id ASC) into 10 buckets
 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" with serdeproperties ('serialization.format'='1')
 stored as rcfile;

alter table exim_employee add columns (emp_dept int);
alter table exim_employee clustered by (emp_sex, emp_dept) sorted by (emp_id desc) into 5 buckets;
alter table exim_employee add partition (emp_country='in', emp_state='tn');

alter table exim_employee set fileformat
	inputformat  "org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat"
	outputformat "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat"
        serde        "org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe";

;
alter table exim_employee set serde "org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe" with serdeproperties ('serialization.format'='2');

alter table exim_employee add partition (emp_country='in', emp_state='ka');
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
describe extended exim_employee partition (emp_country='in', emp_state='tn');
describe extended exim_employee partition (emp_country='in', emp_state='ka');
show table extended like exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id", dep_name string)
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id bigint comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee partition (emp_state="ka") to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	stored as rcfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee partition (emp_country="in",emp_state="ka") to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

create table exim_employee ( emp_id int comment "employee id")
	comment "table of employees"
	partitioned by (emp_country string comment "iso code", emp_state string comment "free-form text")
	stored as textfile
	tblproperties("maker"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="al");
import from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
select * from exim_employee;
drop table exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	stored as inputformat "org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat"
		outputformat "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat"
		inputdriver "org.apache.hadoop.hive.howl.rcfile.RCFileInputDriver"
		outputdriver "org.apache.hadoop.hive.howl.rcfile.RCFileOutputDriver"
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe"
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee,exim_imported_dept;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;
create table exim_department ( dep_id int comment "department id")
	partitioned by (emp_org string)
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department partition (emp_org="hr");
import table exim_imported_dept from 'ql/test/data/exports/exim_department';
describe extended exim_imported_dept;
select * from exim_imported_dept;
drop table exim_imported_dept;
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	row format serde "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"
		with serdeproperties ("serialization.format"="0")
	stored as inputformat "org.apache.hadoop.mapred.TextInputFormat"
		outputformat "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"
		inputdriver "org.apache.hadoop.hive.howl.rcfile.RCFileInputDriver"
		outputdriver "org.apache.hadoop.hive.howl.rcfile.RCFileOutputDriver"
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;
create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
import table exim_employee partition (emp_country="us", emp_state="tn") from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
select * from exim_employee;
drop table exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_department;
create external table exim_department ( dep_id int comment "department id")
	stored as textfile
	location 'ql/test/data/tablestore/exim_department'
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_department;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_department';
describe extended exim_department;
select * from exim_department;
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	clustered by (dep_id) into 10 buckets
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

import external table exim_department from 'ql/test/data/exports/exim_department';
describe extended exim_department;
select * from exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
select * from exim_department;
drop table exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	clustered by (dep_id) sorted by (dep_id desc) into 10 buckets
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	clustered by (dep_id) sorted by (dep_id asc) into 10 buckets
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/ql/test/data/exports/exim_department/temp;
dfs -rmr ${system:test.tmp.dir}/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/ql/test/data/tablestore/exim_department/temp;
dfs -rmr ${system:test.tmp.dir}/ql/test/data/tablestore/exim_department;

import external table exim_department from 'ql/test/data/exports/exim_department'
	location 'ql/test/data/tablestore/exim_department';
describe extended exim_department;
dfs -rmr ${system:test.tmp.dir}/ql/test/data/exports/exim_department;
select * from exim_department;
dfs -rmr ${system:test.tmp.dir}/ql/test/data/tablestore/exim_department;
select * from exim_department;
drop table exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	clustered by (dep_id) sorted by (dep_id desc) into 10 buckets
	stored by "org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler"
	tblproperties("creator"="krishna");
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_department;

import table exim_department from 'ql/test/data/exports/exim_department'
	location 'ql/test/data/tablestore/exim_department';
describe extended exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
select * from exim_department;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_department;
select * from exim_department;
drop table exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	stored by "org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler"
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
	set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_department;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	location 'ql/test/data/tablestore/exim_department'
	tblproperties("creator"="krishna");
import table exim_department from 'ql/test/data/exports/exim_department'
	location 'ql/test/data/tablestore/exim_department';
describe extended exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
select * from exim_department;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_department;
select * from exim_department;
drop table exim_department;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	partitioned by (dep_org string)
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
	set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;

create external table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	location 'ql/test/data/tablestore/exim_employee'
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
import external table exim_employee partition (emp_country="us", emp_state="tn")
	from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;
select * from exim_employee;
drop table exim_employee;


drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	partitioned by (dep_org string)
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department partition (dep_org="hr");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
	set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore2/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/tablestore2/exim_employee;

create external table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	location 'ql/test/data/tablestore2/exim_employee'
	tblproperties("creator"="krishna");
import table exim_employee partition (emp_country="us", emp_state="tn")
	from 'ql/test/data/exports/exim_employee'
	location 'ql/test/data/tablestore/exim_employee';
show table extended like exim_employee;
show table extended like exim_employee partition (emp_country="us", emp_state="tn");
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;
select * from exim_employee;
drop table exim_employee;
dfs -rmr target/tmp/ql/test/data/tablestore2/exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	partitioned by (dep_org string)
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department partition (dep_org="hr");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int comment "department id")
	partitioned by (dep_mgr string)
	stored as textfile
	tblproperties("creator"="krishna");
import from 'ql/test/data/exports/exim_department';
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

drop database importer;
	set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
import table exim_employee partition (emp_country="us", emp_state="tn")
	from 'ql/test/data/exports/exim_employee'
	location 'ql/test/data/tablestore/exim_employee';
alter table exim_employee add partition	(emp_country="us", emp_state="ap")
	location 'ql/test/data/tablestore2/exim_employee';
show table extended like exim_employee;
show table extended like exim_employee partition (emp_country="us", emp_state="tn");
show table extended like exim_employee partition (emp_country="us", emp_state="ap");
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;
import table exim_employee partition (emp_country="us") from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
select * from exim_employee;
drop table exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

import external table exim_employee partition (emp_country="us", emp_state="tn")
	from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
show table extended like exim_employee partition (emp_country="us", emp_state="tn");
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;
import table exim_employee partition (emp_country="us", emp_state="kl") from 'ql/test/data/exports/exim_employee';
describe extended exim_employee;
select * from exim_employee;
drop table exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test2.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;

import external table exim_employee
	from 'ql/test/data/exports/exim_employee'
	location 'ql/test/data/tablestore/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
show table extended like exim_employee partition (emp_country="in", emp_state="tn");
show table extended like exim_employee partition (emp_country="in", emp_state="ka");
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create  table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
import external table exim_department from 'ql/test/data/exports/exim_department';
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
drop table exim_department;

drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;

import external table exim_employee partition (emp_country="us", emp_state="tn")
	from 'ql/test/data/exports/exim_employee'
	location 'ql/test/data/tablestore/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
show table extended like exim_employee partition (emp_country="us", emp_state="tn");
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_department;

create table exim_department ( dep_id int comment "department id")
	stored as textfile
	location 'ql/test/data/tablestore/exim_department'
	tblproperties("creator"="krishna");
import table exim_department from 'ql/test/data/exports/exim_department'
	location 'ql/test/data/tablestore2/exim_department';
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
drop table exim_department;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_department;


drop database importer;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/tablestore/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;

import table exim_employee partition (emp_country="us", emp_state="tn")
	from 'ql/test/data/exports/exim_employee'
	location 'ql/test/data/tablestore/exim_employee';
describe extended exim_employee;
show table extended like exim_employee;
show table extended like exim_employee partition (emp_country="us", emp_state="tn");
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/tablestore/exim_employee;
select * from exim_employee;
drop table exim_employee;

drop database importer;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int) stored as textfile;
load data local inpath "../../data/files/test.dat" into table exim_department;

set hive.security.authorization.enabled=true;

grant Select on table exim_department to user hive_test_user;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';

set hive.security.authorization.enabled=false;
drop table exim_department;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="ka");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="tn");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="us", emp_state="ka");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
import external table exim_employee partition (emp_country="us", emp_state="tn")
	from 'ql/test/data/exports/exim_employee';
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
drop table exim_employee;

drop database importer;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int) stored as textfile;

set hive.security.authorization.enabled=true;

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';

set hive.security.authorization.enabled=false;
drop table exim_department;

set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

create table exim_department ( dep_id int) stored as textfile;
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int) stored as textfile;
set hive.security.authorization.enabled=true;
grant Alter on table exim_department to user hive_test_user;
grant Update on table exim_department to user hive_test_user;
import from 'ql/test/data/exports/exim_department';

set hive.security.authorization.enabled=false;
select * from exim_department;
drop table exim_department;
drop database importer;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table exim_department ( dep_id int) stored as textfile;
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

create table exim_department ( dep_id int) stored as textfile;
set hive.security.authorization.enabled=true;
import from 'ql/test/data/exports/exim_department';

set hive.security.authorization.enabled=false;
drop table exim_department;
drop database importer;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;
create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");

set hive.security.authorization.enabled=true;
grant Alter on table exim_employee to user hive_test_user;
grant Update on table exim_employee to user hive_test_user;
import from 'ql/test/data/exports/exim_employee';

set hive.security.authorization.enabled=false;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
drop table exim_employee;
drop database importer;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

create table exim_department ( dep_id int) stored as textfile;
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/test;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

set hive.security.authorization.enabled=true;
grant Create on database importer to user hive_test_user;
import from 'ql/test/data/exports/exim_department';

set hive.security.authorization.enabled=false;
select * from exim_department;
drop table exim_department;
drop database importer;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");
load data local inpath "../../data/files/test.dat"
	into table exim_employee partition (emp_country="in", emp_state="tn");
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_employee/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;
create table exim_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("creator"="krishna");

set hive.security.authorization.enabled=true;
import from 'ql/test/data/exports/exim_employee';
set hive.security.authorization.enabled=false;

dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
drop table exim_employee;
drop database importer;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_department ( dep_id int) stored as textfile;
load data local inpath "../../data/files/test.dat" into table exim_department;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/exim_department/temp;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;
export table exim_department to 'ql/test/data/exports/exim_department';
drop table exim_department;

create database importer;
use importer;

set hive.security.authorization.enabled=true;
import from 'ql/test/data/exports/exim_department';

set hive.security.authorization.enabled=false;
select * from exim_department;
drop table exim_department;
drop database importer;
dfs -rmr target/tmp/ql/test/data/exports/exim_department;

set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=exim_department,exim_employee;

create table exim_employee ( emp_id int) partitioned by (emp_country string);
load data local inpath "../../data/files/test.dat" into table exim_employee partition (emp_country="in");

dfs ${system:test.dfs.mkdir} ${system:test.warehouse.dir}/exim_employee/emp_country=in/_logs;
dfs -touchz ${system:test.warehouse.dir}/exim_employee/emp_country=in/_logs/job.xml;
export table exim_employee to 'ql/test/data/exports/exim_employee';
drop table exim_employee;

create database importer;
use importer;

import from 'ql/test/data/exports/exim_employee';
describe formatted exim_employee;
select * from exim_employee;
dfs -rmr target/tmp/ql/test/data/exports/exim_employee;
drop table exim_employee;
drop database importer;
use default;
set hive.mapred.mode=nonstrict;
set hive.explain.user=true;

explain create table src_orc_merge_test_part(key int, value string) partitioned by (ds string, ts string) stored as orc;
create table src_orc_merge_test_part(key int, value string) partitioned by (ds string, ts string) stored as orc;

alter table src_orc_merge_test_part add partition (ds='2012-01-03', ts='2012-01-03+14:46:31');
desc extended src_orc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31');

explain insert overwrite table src_orc_merge_test_part partition(ds='2012-01-03', ts='2012-01-03+14:46:31') select * from src;
insert overwrite table src_orc_merge_test_part partition(ds='2012-01-03', ts='2012-01-03+14:46:31') select * from src;
explain insert into table src_orc_merge_test_part partition(ds='2012-01-03', ts='2012-01-03+14:46:31') select * from src limit 100;

explain select count(1) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';
explain select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';

alter table src_orc_merge_test_part partition (ds='2012-01-03', ts='2012-01-03+14:46:31') concatenate;


explain select count(1) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';
explain select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31';

drop table src_orc_merge_test_part;

set hive.auto.convert.join=true;

explain select sum(hash(a.k1,a.v1,a.k2, a.v2))
from (
select src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (select * FROM src WHERE src.key < 10) src1
    JOIN
  (select * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2
) a;

set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;

set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

explain select key, (c_int+1)+2 as x, sum(c_int) from cbo_t1 group by c_float, cbo_t1.c_int, key;
explain select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from cbo_t1 group by c_float, cbo_t1.c_int, key) R group by y, x;

explain select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0) group by c_float, cbo_t1.c_int, key order by a) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c;

explain select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b % c asc, b desc) cbo_t1 left outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key  having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p left outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c  having cbo_t3.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0  order by cbo_t3.c_int % c asc, cbo_t3.c_int desc;

explain select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b+c, a desc) cbo_t1 right outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 2) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c;

explain select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by c+a desc) cbo_t1 full outer join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by p+q desc, r asc) cbo_t2 on cbo_t1.a=p full outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c having cbo_t3.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0 order by cbo_t3.c_int;

explain select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c;

explain select unionsrc.key FROM (select 'tst1' as key, count(1) as value from src) unionsrc;

explain select unionsrc.key FROM (select 'max' as key, max(c_int) as value from cbo_t3 s1
	UNION  ALL
    	select 'min' as key,  min(c_int) as value from cbo_t3 s2
    UNION ALL
        select 'avg' as key,  avg(c_int) as value from cbo_t3 s3) unionsrc order by unionsrc.key;

explain select unionsrc.key, count(1) FROM (select 'max' as key, max(c_int) as value from cbo_t3 s1
    UNION  ALL
        select 'min' as key,  min(c_int) as value from cbo_t3 s2
    UNION ALL
        select 'avg' as key,  avg(c_int) as value from cbo_t3 s3) unionsrc group by unionsrc.key order by unionsrc.key;

explain select cbo_t1.key from cbo_t1 join cbo_t3 where cbo_t1.key=cbo_t3.key and cbo_t1.key >= 1;
explain select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 left outer join  cbo_t2 on cbo_t1.key=cbo_t2.key;
explain select cbo_t1.c_int, cbo_t2.c_int from cbo_t1 full outer join  cbo_t2 on cbo_t1.key=cbo_t2.key;

explain select b, cbo_t1.c, cbo_t2.p, q, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1) cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key;
explain select key, cbo_t1.c_int, cbo_t2.p, q from cbo_t1 join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2) cbo_t2 on cbo_t1.key=p join (select key as a, c_int as b, cbo_t3.c_float as c from cbo_t3)cbo_t3 on cbo_t1.key=a;

explain select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 full outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

explain select * from (select q, b, cbo_t2.p, cbo_t1.c, cbo_t3.c_int from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 right outer join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p right outer join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);

explain select key, (c_int+1)+2 as x, sum(c_int) from cbo_t1 group by c_float, cbo_t1.c_int, key order by x limit 1;
explain select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from cbo_t1 group by c_float, cbo_t1.c_int, key) R group by y, x order by x,y limit 1;
explain select key from(select key from (select key from cbo_t1 limit 5)cbo_t2  limit 5)cbo_t3  limit 5;
explain select key, c_int from(select key, c_int from (select key, c_int from cbo_t1 order by c_int limit 5)cbo_t1  order by c_int limit 5)cbo_t2  order by c_int limit 5;

explain select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0) group by c_float, cbo_t1.c_int, key order by a limit 5) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc limit 5) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c limit 5;

explain select cbo_t1.c_int           from cbo_t1 left semi join   cbo_t2 on cbo_t1.key=cbo_t2.key where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0);
explain select * from (select c, b, a from (select key as a, c_int as b, cbo_t1.c_float as c from cbo_t1  where (cbo_t1.c_int + 1 == 2) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)) cbo_t1 left semi join (select cbo_t2.key as p, cbo_t2.c_int as q, c_float as r from cbo_t2  where (cbo_t2.c_int + 1 == 2) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)) cbo_t2 on cbo_t1.a=p left semi join cbo_t3 on cbo_t1.a=key where (b + 1 == 2) and (b > 0 or c >= 0)) R where  (b + 1 = 2) and (R.b > 0 or c >= 0);
explain select a, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0)  group by c_float, cbo_t1.c_int, key having cbo_t1.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by a+b desc, c asc) cbo_t1 left semi join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key having cbo_t2.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by q+r/10 desc, p) cbo_t2 on cbo_t1.a=p left semi join cbo_t3 on cbo_t1.a=key where (b + 1  >= 0) and (b > 0 or a >= 0) group by a, c  having a > 0 and (a >=1 or c >= 1) and (a + c) >= 0 order by c, a;

explain select cbo_t1.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from cbo_t1;

explain select null from cbo_t1;

explain select key from cbo_t1 where c_int = -6  or c_int = +6;

explain select count(cbo_t1.dt) from cbo_t1 join cbo_t2 on cbo_t1.dt  = cbo_t2.dt  where cbo_t1.dt = '2014' ;

explain select *
from src_cbo b
where not exists
  (select distinct a.key
  from src_cbo a
  where b.value = a.value and a.value > 'val_2'
  )
;

explain select *
from src_cbo b
group by key, value
having not exists
  (select a.key
  from src_cbo a
  where b.value = a.value  and a.key = b.key and a.value > 'val_12'
  )
;

create view cv1 as
select *
from src_cbo b
where exists
  (select a.key
  from src_cbo a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9')
;

explain select * from cv1;

explain select *
from (select *
      from src_cbo b
      where exists
          (select a.key
          from src_cbo a
          where b.value = a.value  and a.key = b.key and a.value > 'val_9')
     ) a
;


explain select *
from src_cbo
where src_cbo.key in (select key from src_cbo s1 where s1.key > '9')
;


explain select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR' and l_linenumber = li.l_linenumber)
;

explain select key, value, count(*)
from src_cbo b
where b.key in (select key from src_cbo where src_cbo.key > '8')
group by key, value
having count(*) in (select count(*) from src_cbo s1 where s1.key > '9' group by s1.key )
;

explain select p_mfgr, p_name, avg(p_size)
from part
group by p_mfgr, p_name
having p_name in
  (select first_value(p_name) over(partition by p_mfgr order by p_size) from part)
;

explain select *
from src_cbo
where src_cbo.key not in
  ( select key  from src_cbo s1
    where s1.key > '2'
  ) order by key
;

explain select p_mfgr, b.p_name, p_size
from part b
where b.p_name not in
  (select p_name
  from (select p_mfgr, p_name, p_size as r from part) a
  where r < 10 and b.p_mfgr = a.p_mfgr
  )
;

explain select p_name, p_size
from
part where part.p_size not in
  (select avg(p_size)
  from (select p_size from part) a
  where p_size < 10
  ) order by p_name
;

explain select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from (select p_mfgr, min(p_retailprice) l, max(p_retailprice) r, avg(p_retailprice) a from part group by p_mfgr) a
  where min(p_retailprice) = l and r - l > 600
  )
  order by b.p_mfgr
;

explain select count(c_int) over(), sum(c_float) over(), max(c_int) over(), min(c_int) over(), row_number() over(), rank() over(), dense_rank() over(), percent_rank() over(), lead(c_int, 2, c_int) over(), lag(c_float, 2, c_float) over() from cbo_t1;
explain select * from (select count(c_int) over(), sum(c_float) over(), max(c_int) over(), min(c_int) over(), row_number() over(), rank() over(), dense_rank() over(), percent_rank() over(), lead(c_int, 2, c_int) over(), lag(c_float, 2, c_float) over() from cbo_t1) cbo_t1;
explain select i, a, h, b, c, d, e, f, g, a as x, a +1 as y from (select max(c_int) over (partition by key order by value range UNBOUNDED PRECEDING) a, min(c_int) over (partition by key order by value range current row) b, count(c_int) over(partition by key order by value range 1 PRECEDING) c, avg(value) over (partition by key order by value range between unbounded preceding and unbounded following) d, sum(value) over (partition by key order by value range between unbounded preceding and current row) e, avg(c_float) over (partition by key order by value range between 1 preceding and unbounded following) f, sum(c_float) over (partition by key order by value range between 1 preceding and current row) g, max(c_float) over (partition by key order by value range between 1 preceding and unbounded following) h, min(c_float) over (partition by key order by value range between 1 preceding and 1 following) i from cbo_t1) cbo_t1;
explain select *, rank() over(partition by key order by value) as rr from src1;


set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
explain
select SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (select x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.optimize.correlation=true;
explain
select SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (select x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.auto.convert.join=true;
set hive.optimize.correlation=true;
explain
select SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (select x.key AS key, count(1) AS cnt
      FROM src1 x JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

set hive.auto.convert.join=false;
set hive.optimize.correlation=false;
explain
select SUM(HASH(tmp.key)), SUM(HASH(tmp.cnt))
FROM (select x.key AS key, count(1) AS cnt
      FROM src1 x LEFT SEMI JOIN src y ON (x.key = y.key)
      GROUP BY x.key) tmp;

explain create table abcd (a int, b int, c int, d int);
create table abcd (a int, b int, c int, d int);
LOAD DATA LOCAL INPATH '../../data/files/in4.txt' INTO TABLE abcd;

set hive.map.aggr=true;
explain select a, count(distinct b), count(distinct c), sum(d) from abcd group by a;

set hive.map.aggr=false;
explain select a, count(distinct b), count(distinct c), sum(d) from abcd group by a;

explain create table src_rc_merge_test(key int, value string) stored as rcfile;
create table src_rc_merge_test(key int, value string) stored as rcfile;

load data local inpath '../../data/files/smbbucket_1.rc' into table src_rc_merge_test;

set hive.exec.compress.output = true;

explain create table tgt_rc_merge_test(key int, value string) stored as rcfile;
create table tgt_rc_merge_test(key int, value string) stored as rcfile;
insert into table tgt_rc_merge_test select * from src_rc_merge_test;

show table extended like `tgt_rc_merge_test`;

explain select count(1) from tgt_rc_merge_test;
explain select sum(hash(key)), sum(hash(value)) from tgt_rc_merge_test;

alter table tgt_rc_merge_test concatenate;

show table extended like `tgt_rc_merge_test`;

explain select count(1) from tgt_rc_merge_test;
explain select sum(hash(key)), sum(hash(value)) from tgt_rc_merge_test;

drop table src_rc_merge_test;
drop table tgt_rc_merge_test;

explain select src.key from src cross join src src2;


explain create table nzhang_Tmp(a int, b string);
create table nzhang_Tmp(a int, b string);

explain create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10;
create table nzhang_CTAS1 as select key k, value from src sort by k, value limit 10;


explain create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;

create table nzhang_ctas3 row format serde "org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe" stored as RCFile as select key/2 half_key, concat(value, "_con") conb  from src sort by half_key, conb limit 10;

explain create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;

create table if not exists nzhang_ctas3 as select key, value from src sort by key, value limit 2;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


explain create temporary table acid_dtt(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
create temporary table acid_dtt(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

set hive.map.aggr=false;
set hive.groupby.skewindata=true;


explain
select src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (select * FROM src WHERE src.key < 10) src1
    JOIN
  (select * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2;


CREATE TABLE myinput1(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in8.txt' INTO TABLE myinput1;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key=c.key;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key<=>c.key;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value=b.key join myinput1 c on a.key<=>c.key AND a.value=c.value;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value<=>b.key join myinput1 c on a.key<=>c.key AND a.value<=>c.value;

explain select * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key<=>b.value;
explain select * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key<=>b.value;
explain select * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key<=>b.value;

explain select /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key<=>b.value;

CREATE TABLE smb_input(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in4.txt' into table smb_input;
LOAD DATA LOCAL INPATH '../../data/files/in5.txt' into table smb_input;


;

CREATE TABLE smb_input1(key int, value int) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE smb_input2(key int, value int) CLUSTERED BY (value) SORTED BY (value) INTO 2 BUCKETS;

from smb_input
insert overwrite table smb_input1 select *
insert overwrite table smb_input2 select *;

SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
SET hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

analyze table smb_input1 compute statistics;

explain select /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key <=> b.key;
explain select /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key <=> b.key AND a.value <=> b.value;
explain select /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input1 b ON a.key <=> b.key;
explain select /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key <=> b.key;
explain select /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input1 b ON a.key <=> b.key;

drop table sales;
drop table things;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

CREATE TABLE sales (name STRING, id INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

CREATE TABLE things (id INT, name STRING) partitioned by (ds string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

load data local inpath '../../data/files/sales.txt' INTO TABLE sales;
load data local inpath '../../data/files/things.txt' INTO TABLE things partition(ds='2011-10-23');
load data local inpath '../../data/files/things2.txt' INTO TABLE things partition(ds='2011-10-24');

explain select name,id FROM sales LEFT SEMI JOIN things ON (sales.id = things.id);

drop table sales;
drop table things;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

explain select srcpart.key from srcpart join src on (srcpart.value=src.value) join src1 on (srcpart.key=src1.key) where srcpart.value > 'val_450';

set hive.mapjoin.optimized.hashtable=false;

explain select srcpart.key from srcpart join src on (srcpart.value=src.value) join src1 on (srcpart.key=src1.key) where srcpart.value > 'val_450';

set hive.mapjoin.optimized.hashtable=true;

explain select srcpart.key from srcpart join src on (srcpart.value=src.value) join src1 on (srcpart.key=src1.key) where srcpart.value > 'val_450';

explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
  partition by p_mfgr
  order by p_name
  );

explain
select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part p1 join part p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
;

explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
  partition by p_mfgr
  order by p_name
  ) abc;

explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part
          partition by p_mfgr
          order by p_name
          )
;

explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part
          partition by p_mfgr
          order by p_name
          )
group by p_mfgr, p_name, p_size
;

explain
select abc.*
from noop(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey;


explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part
partition by p_mfgr
order by p_name, p_size desc);

explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopwithmap(on part
  partition by p_mfgr
  order by p_name);

explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
partition by p_mfgr
order by p_name)
;

explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on noopwithmap(on noop(on part
partition by p_mfgr
order by p_mfgr DESC, p_name
)));

explain
select p_mfgr, p_name,
sub1.cd, sub1.s1
from (select p_mfgr, p_name,
count(p_size) over (partition by p_mfgr order by p_name) as cd,
p_retailprice,
sum(p_retailprice) over w1  as s1
from noop(on part
partition by p_mfgr
order by p_name)
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following)
) sub1 ;


explain
select abc.p_mfgr, abc.p_name,
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr,
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noop(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey
;


explain create view IF NOT EXISTS mfgr_price_view as
select p_mfgr, p_brand,
sum(p_retailprice) as s
from part
group by p_mfgr, p_brand;

CREATE TABLE part_4(
p_mfgr STRING,
p_name STRING,
p_size INT,
r INT,
dr INT,
s DOUBLE);

CREATE TABLE part_5(
p_mfgr STRING,
p_name STRING,
p_size INT,
s2 INT,
r INT,
dr INT,
cud DOUBLE,
fv1 INT);

explain
from noop(on part
partition by p_mfgr
order by p_name)
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size,
rank() over (distribute by p_mfgr sort by p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr,
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud,
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);


explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noopwithmap(on
          noop(on
              noop(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr,p_name
        order by p_mfgr,p_name) ;

explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noop(on
          noop(on
              noop(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr
        order by p_mfgr ) ;

explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1
from noop(on
        noop(on
          noop(on
              noop(on part
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr));

explain select distinct src.* from src;

explain select explode(array('a', 'b'));

set hive.optimize.skewjoin = true;
set hive.skewjoin.key = 2;

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T4(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;
LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T4;


explain
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 select src1.key, src2.value;

FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 select src1.key, src2.value;



explain
select /*+ STREAMTABLE(a) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;

explain
select /*+ STREAMTABLE(a,c) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;

explain FROM T1 a JOIN src c ON c.key+1=a.key select /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));
FROM T1 a JOIN src c ON c.key+1=a.key select /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));

explain
select * FROM
(select src.* FROM src) x
JOIN
(select src.* FROM src) Y
ON (x.key = Y.key);


explain select /*+ mapjoin(k)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.val;

explain select sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.key;

explain select count(1) from  T1 a join T1 b on a.key = b.key;

explain FROM T1 a LEFT OUTER JOIN T2 c ON c.key+1=a.key select sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));

explain FROM T1 a RIGHT OUTER JOIN T2 c ON c.key+1=a.key select /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));

explain FROM T1 a FULL OUTER JOIN T2 c ON c.key+1=a.key select /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));

explain select /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k left outer join T1 v on k.key+1=v.key;
set hive.explain.user=true;
set hive.metastore.aggregate.stats.cache.enabled=false;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;

CREATE TABLE ss(k1 STRING,v1 STRING,k2 STRING,v2 STRING,k3 STRING,v3 STRING) STORED AS TEXTFILE;

CREATE TABLE sr(k1 STRING,v1 STRING,k2 STRING,v2 STRING,k3 STRING,v3 STRING) STORED AS TEXTFILE;

CREATE TABLE cs(k1 STRING,v1 STRING,k2 STRING,v2 STRING,k3 STRING,v3 STRING) STORED AS TEXTFILE;

INSERT OVERWRITE TABLE ss
SELECT x.key,x.value,y.key,y.value,z.key,z.value
FROM src1 x
JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

INSERT OVERWRITE TABLE sr
SELECT x.key,x.value,y.key,y.value,z.key,z.value
FROM src1 x
JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=12);

INSERT OVERWRITE TABLE cs
SELECT x.key,x.value,y.key,y.value,z.key,z.value
FROM src1 x
JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08');


ANALYZE TABLE ss COMPUTE STATISTICS;
ANALYZE TABLE ss COMPUTE STATISTICS FOR COLUMNS k1,v1,k2,v2,k3,v3;

ANALYZE TABLE sr COMPUTE STATISTICS;
ANALYZE TABLE sr COMPUTE STATISTICS FOR COLUMNS k1,v1,k2,v2,k3,v3;

ANALYZE TABLE cs COMPUTE STATISTICS;
ANALYZE TABLE cs COMPUTE STATISTICS FOR COLUMNS k1,v1,k2,v2,k3,v3;

set hive.auto.convert.join=false;

EXPLAIN
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

EXPLAIN
select
ss.k1,sr.k2,cs.k3,count(ss.v1),count(sr.v2),count(cs.v3)
FROM
ss,sr,cs,src d1,src d2,src d3,src1,srcpart
where
    ss.k1 = d1.key
and sr.k1 = d2.key
and cs.k1 = d3.key
and ss.k2 = sr.k2
and ss.k3 = sr.k3
and ss.v1 = src1.value
and ss.v2 = srcpart.value
and sr.v2 = cs.v2
and sr.v3 = cs.v3
and ss.v3='ssv3'
and sr.v1='srv1'
and src1.key = 'src1key'
and srcpart.key = 'srcpartkey'
and d1.value = 'd1value'
and d2.value in ('2000Q1','2000Q2','2000Q3')
and d3.value in ('2000Q1','2000Q2','2000Q3')
group by
ss.k1,sr.k2,cs.k3
order by
ss.k1,sr.k2,cs.k3
limit 100;

explain
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union select * from src)z ON (x.value = z.value)
union
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union select * from src)z ON (x.value = z.value);

explain
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union select * from src)z ON (x.value = z.value)
union
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select key, value from src1 union select key, value from src union select key, value from src)z ON (x.value = z.value)
union
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select key, value from src1 union select key, value from src union select key, value from src union select key, value from src)z ON (x.value = z.value);


set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

EXPLAIN
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

EXPLAIN
select
ss.k1,sr.k2,cs.k3,count(ss.v1),count(sr.v2),count(cs.v3)
FROM
ss,sr,cs,src d1,src d2,src d3,src1,srcpart
where
    ss.k1 = d1.key
and sr.k1 = d2.key
and cs.k1 = d3.key
and ss.k2 = sr.k2
and ss.k3 = sr.k3
and ss.v1 = src1.value
and ss.v2 = srcpart.value
and sr.v2 = cs.v2
and sr.v3 = cs.v3
and ss.v3='ssv3'
and sr.v1='srv1'
and src1.key = 'src1key'
and srcpart.key = 'srcpartkey'
and d1.value = 'd1value'
and d2.value in ('2000Q1','2000Q2','2000Q3')
and d3.value in ('2000Q1','2000Q2','2000Q3')
group by
ss.k1,sr.k2,cs.k3
order by
ss.k1,sr.k2,cs.k3
limit 100;

explain
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union select * from src)z ON (x.value = z.value)
union
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union select * from src)z ON (x.value = z.value);

explain
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union select * from src)z ON (x.value = z.value)
union
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select key, value from src1 union select key, value from src union select key, value from src)z ON (x.value = z.value)
union
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select key, value from src1 union select key, value from src union select key, value from src union select key, value from src)z ON (x.value = z.value);


set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin_part;

CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

CREATE TABLE tab2(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab2 partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

set hive.convert.join.bucket.mapjoin.tez = false;
set hive.auto.convert.sortmerge.join = true;

set hive.auto.convert.join.noconditionaltask.size=500;

explain
select s1.key as key, s1.value as value from tab s1 join tab s3 on s1.key=s3.key;

explain
select s1.key as key, s1.value as value from tab s1 join tab s3 on s1.key=s3.key join tab s2 on s1.value=s2.value;

explain
select s1.key as key, s1.value as value from tab s1 join tab2 s3 on s1.key=s3.key;

explain
select s1.key as key, s1.value as value from tab s1 join tab2 s3 on s1.key=s3.key join tab2 s2 on s1.value=s2.value;

explain
select count(*) from (select s1.key as key, s1.value as value from tab s1 join tab s3 on s1.key=s3.key
UNION  ALL
select s2.key as key, s2.value as value from tab s2
) a join tab_part b on (a.key = b.key);

explain
select count(*) from (select s1.key as key, s1.value as value from tab s1 join tab s3 on s1.key=s3.key join tab s2 on s1.value=s2.value
UNION  ALL
select s2.key as key, s2.value as value from tab s2
) a join tab_part b on (a.key = b.key);set hive.explain.user=true;

explain
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union all select * from src)z ON (x.value = z.value)
union all
SELECT x.key, y.value
FROM src x JOIN src y ON (x.key = y.key)
JOIN (select key, value from src1 union all select key, value from src union all select key, value from src)z ON (x.value = z.value)
union all
SELECT x.key, y.value
FROM src1 x JOIN src1 y ON (x.key = y.key)
JOIN (select key, value from src1 union all select key, value from src union all select key, value from src union all select key, value from src)z ON (x.value = z.value);

explain
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union select * from src)z ON (x.value = z.value)
union
SELECT x.key, y.value
FROM src x JOIN src y ON (x.key = y.key)
JOIN (select key, value from src1 union select key, value from src union select key, value from src)z ON (x.value = z.value)
union
SELECT x.key, y.value
FROM src1 x JOIN src1 y ON (x.key = y.key)
JOIN (select key, value from src1 union select key, value from src union select key, value from src union select key, value from src)z ON (x.value = z.value);

CREATE TABLE a(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE b(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE c(key STRING, value STRING) STORED AS TEXTFILE;

explain
from
(
SELECT x.key, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union all select * from src)z ON (x.value = z.value)
union all
SELECT x.key, y.value
FROM src x JOIN src y ON (x.key = y.key)
JOIN (select key, value from src1 union all select key, value from src union all select key, value from src)z ON (x.value = z.value)
union all
SELECT x.key, y.value
FROM src1 x JOIN src1 y ON (x.key = y.key)
JOIN (select key, value from src1 union all select key, value from src union all select key, value from src union all select key, value from src)z ON (x.value = z.value)
) tmp
INSERT OVERWRITE TABLE a SELECT tmp.key, tmp.value
INSERT OVERWRITE TABLE b SELECT tmp.key, tmp.value
INSERT OVERWRITE TABLE c SELECT tmp.key, tmp.value;

explain
FROM
(
SELECT x.key as key, y.value as value from src1 x JOIN src y ON (x.key = y.key)
JOIN (select * from src1 union select * from src)z ON (x.value = z.value)
union
SELECT x.key as key, y.value as value from src x JOIN src y ON (x.key = y.key)
JOIN (select key, value from src1 union select key, value from src union select key, value from src)z ON (x.value = z.value)
union
SELECT x.key as key, y.value as value from src1 x JOIN src1 y ON (x.key = y.key)
JOIN (select key, value from src1 union select key, value from src union select key, value from src union select key, value from src)z ON (x.value = z.value)
) tmp
INSERT OVERWRITE TABLE a SELECT tmp.key, tmp.value
INSERT OVERWRITE TABLE b SELECT tmp.key, tmp.value
INSERT OVERWRITE TABLE c SELECT tmp.key, tmp.value;


CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value;

EXPLAIN FROM UNIQUEJOIN PRESERVE src a (a.key), PRESERVE src1 b (b.key), PRESERVE srcpart c (c.key) SELECT a.key, b.key, c.key;

set hive.entity.capture.transform=true;

EXPLAIN
SELECT
TRANSFORM(a.key, a.value) USING 'cat' AS (tkey, tvalue)
FROM src a join src b
on a.key = b.key;

explain
FROM (
      select key, value from (
      select 'tst1' as key, cast(count(1) as string) as value, 'tst1' as value2 from src s1
                         UNION all
      select s2.key as key, s2.value as value, 'tst1' as value2 from src s2) unionsub
                         UNION all
      select key, value from src s0
                             ) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

explain
FROM (
      select 'tst1' as key, cast(count(1) as string) as value, 'tst1' as value2 from src s1
                         UNION all
      select s2.key as key, s2.value as value, 'tst1' as value2 from src s2
                             ) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set hive.metastore.filter.hook=org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl;
set hive.mapred.mode=nonstrict;
set hive.explain.user=true;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=true;

CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true');
insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10;
explain select a, b from acid_vectorized order by a, b;

explain select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol;

explain show tables;

explain create database newDB location "/tmp/";

create database newDB location "/tmp/";

explain describe database extended newDB;

describe database extended newDB;

explain use newDB;

use newDB;

create table tab (name string);

explain alter table tab rename to newName;

explain drop table tab;

drop table tab;

explain use default;

use default;

drop database newDB;

explain analyze table src compute statistics;

explain analyze table src compute statistics for columns;

explain
CREATE TEMPORARY MACRO SIGMOID (x DOUBLE) 1.0 / (1.0 + EXP(-x));

CREATE TEMPORARY MACRO SIGMOID (x DOUBLE) 1.0 / (1.0 + EXP(-x));

EXPLAIN SELECT SIGMOID(2) FROM src LIMIT 1;
explain DROP TEMPORARY MACRO SIGMOID;
DROP TEMPORARY MACRO SIGMOID;

explain create table src_autho_test as select * from src;
create table src_autho_test as select * from src;

set hive.security.authorization.enabled=true;

explain grant select on table src_autho_test to user hive_test_user;
grant select on table src_autho_test to user hive_test_user;

explain show grant user hive_test_user on table src_autho_test;
explain show grant user hive_test_user on table src_autho_test(key);

select key from src_autho_test order by key limit 20;

explain revoke select on table src_autho_test from user hive_test_user;

explain grant select(key) on table src_autho_test to user hive_test_user;

explain revoke select(key) on table src_autho_test from user hive_test_user;

explain
create role sRc_roLE;

create role sRc_roLE;

explain
grant role sRc_roLE to user hive_test_user;

grant role sRc_roLE to user hive_test_user;

explain show role grant user hive_test_user;

explain drop role sRc_roLE;
drop role sRc_roLE;

set hive.security.authorization.enabled=false;
drop table src_autho_test;

explain drop view v;

explain create view v as with cte as (select * from src  order by key limit 5)
select * from cte;

explain with cte as (select * from src  order by key limit 5)
select * from cte;

create table orc_merge5 (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_merge5;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=50000;
SET hive.optimize.index.filter=true;
set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.compute.splits.in.am=true;
set tez.grouping.min-size=1000;
set tez.grouping.max-size=50000;

set hive.merge.orcfile.stripe.level=true;
set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain insert overwrite table orc_merge5 select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;

drop table orc_merge5;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin_part;

CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

set hive.convert.join.bucket.mapjoin.tez = true;
explain
select a.key, a.value, b.value
from tab a join tab_part b on a.key = b.key;



-- This test is used for testing explain for DDL/DML statements

-- Create some views and tabels
CREATE VIEW V1 AS SELECT key, value from src;
select count(*) from V1 where key > 0;

CREATE TABLE M1 AS SELECT key, value from src;
select count(*) from M1 where key > 0;

EXPLAIN CREATE TABLE M1 AS select * from src;
EXPLAIN CREATE TABLE M1 AS select * from M1;
EXPLAIN CREATE TABLE M1 AS select * from V1;

EXPLAIN CREATE TABLE V1 AS select * from M1;
EXPLAIN CREATE VIEW V1 AS select * from M1;

EXPLAIN CREATE TABLE M1 LIKE src;
EXPLAIN CREATE TABLE M1 LIKE M1;

EXPLAIN DROP TABLE M1;
select count(*) from M1 where key > 0;

EXPLAIN INSERT INTO M1 SELECT * FROM M1;
select count(*) from M1 where key > 0;

EXPLAIN TRUNCATE TABLE M1;
select count(*) from M1 where key > 0;

set hive.mapred.mode=nonstrict;
-- This test is used for testing EXPLAIN DEPENDENCY command

-- Create some views
CREATE VIEW V1 AS SELECT key, value from src;
CREATE VIEW V2 AS SELECT ds, key, value FROM srcpart WHERE ds IS NOT NULL;
CREATE VIEW V3 AS
  SELECT src1.key, src2.value FROM V2 src1
  JOIN src src2 ON src1.key = src2.key WHERE src1.ds IS NOT NULL;
CREATE VIEW V4 AS
  SELECT src1.key, src2.value as value1, src3.value as value2
  FROM V1 src1 JOIN V2 src2 on src1.key = src2.key JOIN src src3 ON src2.key = src3.key;

-- Simple select queries, union queries and join queries
EXPLAIN DEPENDENCY
  SELECT key, count(1) FROM srcpart WHERE ds IS NOT NULL GROUP BY key;
EXPLAIN DEPENDENCY
  SELECT key, count(1) FROM (SELECT key, value FROM src) subq1 GROUP BY key;
EXPLAIN DEPENDENCY
  SELECT * FROM (
    SELECT key, value FROM src UNION ALL SELECT key, value FROM srcpart WHERE ds IS NOT NULL
  ) S1;
EXPLAIN DEPENDENCY
  SELECT S1.key, S2.value FROM src S1 JOIN srcpart S2 ON S1.key = S2.key WHERE ds IS NOT NULL;

-- With views
EXPLAIN DEPENDENCY SELECT * FROM V1;
EXPLAIN DEPENDENCY SELECT * FROM V2;
EXPLAIN DEPENDENCY SELECT * FROM V3;
EXPLAIN DEPENDENCY SELECT * FROM V4;

-- The table should show up in the explain dependency even if none
-- of the partitions are selected.
CREATE VIEW V5 as SELECT * FROM srcpart where ds = '10';
EXPLAIN DEPENDENCY SELECT * FROM V5;
-- This test is used for testing EXPLAIN DEPENDENCY command

-- select from a table which does not involve a map-reduce job
EXPLAIN DEPENDENCY SELECT * FROM src;

-- select from a table which involves a map-reduce job
EXPLAIN DEPENDENCY SELECT count(*) FROM src;

-- select from a partitioned table which does not involve a map-reduce job
-- and some partitions are being selected
EXPLAIN DEPENDENCY SELECT * FROM srcpart where ds is not null;

-- select from a partitioned table which does not involve a map-reduce job
-- and none of the partitions are being selected
EXPLAIN DEPENDENCY SELECT * FROM srcpart where ds = '1';

-- select from a partitioned table which involves a map-reduce job
-- and some partitions are being selected
EXPLAIN DEPENDENCY SELECT count(*) FROM srcpart where ds is not null;

-- select from a partitioned table which involves a map-reduce job
-- and none of the partitions are being selected
EXPLAIN DEPENDENCY SELECT count(*) FROM srcpart where ds = '1';

create table tstsrcpart like srcpart;

-- select from a partitioned table with no partitions which does not involve a map-reduce job
EXPLAIN DEPENDENCY SELECT * FROM tstsrcpart where ds is not null;

-- select from a partitioned table with no partitions which involves a map-reduce job
EXPLAIN DEPENDENCY SELECT count(*) FROM tstsrcpart where ds is not null;
set hive.mapred.mode=nonstrict;
-- This test is used for testing EXPLAIN LOGICAL command

-- Create some views
CREATE VIEW V1 AS SELECT key, value from src;
CREATE VIEW V2 AS SELECT ds, key, value FROM srcpart WHERE ds IS NOT NULL;
CREATE VIEW V3 AS
  SELECT src1.key, src2.value FROM V2 src1
  JOIN src src2 ON src1.key = src2.key WHERE src1.ds IS NOT NULL;
CREATE VIEW V4 AS
  SELECT src1.key, src2.value as value1, src3.value as value2
  FROM V1 src1 JOIN V2 src2 on src1.key = src2.key JOIN src src3 ON src2.key = src3.key;

-- Simple select queries, union queries and join queries
EXPLAIN LOGICAL
  SELECT key, count(1) FROM srcpart WHERE ds IS NOT NULL GROUP BY key;
EXPLAIN LOGICAL
  SELECT key, count(1) FROM (SELECT key, value FROM src) subq1 GROUP BY key;
EXPLAIN LOGICAL
  SELECT * FROM (
    SELECT key, value FROM src UNION ALL SELECT key, value FROM srcpart WHERE ds IS NOT NULL
  ) S1;
EXPLAIN LOGICAL
  SELECT S1.key, S2.value FROM src S1 JOIN srcpart S2 ON S1.key = S2.key WHERE ds IS NOT NULL;

-- With views
EXPLAIN LOGICAL SELECT * FROM V1;
EXPLAIN LOGICAL SELECT * FROM V2;
EXPLAIN LOGICAL SELECT * FROM V3;
EXPLAIN LOGICAL SELECT * FROM V4;

-- The table should show up in the explain logical even if none
-- of the partitions are selected.
CREATE VIEW V5 as SELECT * FROM srcpart where ds = '10';
EXPLAIN LOGICAL SELECT * FROM V5;

EXPLAIN LOGICAL SELECT s1.key, s1.cnt, s2.value FROM (SELECT key, count(value) as cnt FROM src GROUP BY key) s1 JOIN src s2 ON (s1.key = s2.key) ORDER BY s1.key;
set hive.mapred.mode=nonstrict;
-- query from auto_sortmerge_join_9.q

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

set hive.auto.convert.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.sortmerge.join=true;
set hive.auto.convert.sortmerge.join.to.mapjoin=false;

set hive.explain.dependency.append.tasktype=true;

-- default behavior

explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key
order by src1.key, src1.cnt1, src2.cnt1;

set hive.stageid.rearrange=IDONLY;

-- changes id only

explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key
order by src1.key, src1.cnt1, src2.cnt1;

set hive.stageid.rearrange=TRAVERSE;

-- assign ids in traverse order

explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key
order by src1.key, src1.cnt1, src2.cnt1;

set hive.stageid.rearrange=EXECUTION;

-- assign ids in execution order

explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key
order by src1.key, src1.cnt1, src2.cnt1;
SELECT explode(col) AS myCol FROM
    (select * from (SELECT array(1,2,3) AS col FROM src LIMIT 1)ea
     UNION ALL
     select * from (SELECT IF(false, array(1,2,3), NULL) AS col FROM src LIMIT 1)eb) a;

SELECT explode(col) AS (myCol1,myCol2) FROM
    (select * from (SELECT map(1,'one',2,'two',3,'three') AS col FROM src LIMIT 1)ea
     UNION ALL
     select * from (SELECT IF(false, map(1,'one',2,'two',3,'three'), NULL) AS col FROM src LIMIT 1)eb ) a;
     set hive.fetch.task.conversion=more;

-- should return a value
select * from src tablesample (1 rows) where length(key) <> reverse(key);
create external table external1(a int, b int) location 'invalidscheme://data.s3ndemo.hive/kv';
describe external1;

create external table external2(a int, b int) partitioned by (ds string);
alter table external2 add partition (ds='2008-01-01') location 'invalidscheme://data.s3ndemo.hive/pkv/2008-01-01';
describe external2 partition (ds='2008-01-01');
DROP TABLE t_hbase;

CREATE TABLE t_hbase(key STRING,
                     tinyint_col TINYINT,
                     smallint_col SMALLINT,
                     int_col INT,
                     bigint_col BIGINT,
                     float_col FLOAT,
                     double_col DOUBLE,
                     boolean_col BOOLEAN)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:binarykey#-,cf:binarybyte#-,cf:binaryshort#-,:key#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
TBLPROPERTIES ("hbase.table.name" = "t_hive",
               "hbase.table.default.storage.type" = "binary");

DESCRIBE FORMATTED t_hbase;

INSERT OVERWRITE TABLE t_hbase
SELECT 'user1', 1, 11, 10, 1, 1.0, 1.0, true
FROM src
WHERE key=100 OR key=125 OR key=126;

INSERT OVERWRITE TABLE t_hbase
SELECT 'user2', 127, 327, 2147, 9223372036854775807, 211.31, 268746532.0571, false
FROM src
WHERE key=100 OR key=125 OR key=126;

INSERT OVERWRITE TABLE t_hbase
SELECT 'user3', -128, -327, -214748, -9223372036854775808, -201.17, -2110789.37145, true
FROM src
WHERE key=100 OR key=125 OR key=126;

explain SELECT * FROM t_hbase where int_col > 0;
SELECT * FROM t_hbase where int_col > 0;

DROP TABLE t_hbase;

set hive.mapred.mode=nonstrict;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/test/;

dfs -copyFromLocal ../../data/files/ext_test_space hdfs:///tmp/test/ext_test_space;

CREATE EXTERNAL TABLE spacetest (id int, message string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LOCATION 'hdfs:///tmp/test/ext_test_space/folder+with space';

SELECT * FROM spacetest;

SELECT count(*) FROM spacetest;

DROP TABLE spacetest;

CREATE EXTERNAL TABLE spacetestpartition (id int, message string) PARTITIONED BY (day int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

ALTER TABLE spacetestpartition ADD PARTITION (day=10) LOCATION 'hdfs:///tmp/test/ext_test_space/folder+with space';

SELECT * FROM spacetestpartition;

SELECT count(*) FROM spacetestpartition;

DROP TABLE spacetestpartition;

dfs -rmr hdfs:///tmp/test;
set hive.mapred.mode=nonstrict;
set hive.stats.fetch.column.stats=true;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table if not exists ext_loc (
  state string,
  locid int,
  zip int,
  year string
) row format delimited fields terminated by '|' stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/extrapolate_stats_full.txt' OVERWRITE INTO TABLE ext_loc;

create table if not exists loc_orc_1d (
  state string,
  locid int,
  zip int
) partitioned by(year string) stored as orc;

insert overwrite table loc_orc_1d partition(year) select * from ext_loc;

analyze table loc_orc_1d partition(year='2000') compute statistics for columns state,locid;

analyze table loc_orc_1d partition(year='2001') compute statistics for columns state,locid;

describe formatted loc_orc_1d PARTITION(year='2001') state;

-- basicStatState: COMPLETE colStatState: PARTIAL
explain extended select state from loc_orc_1d;

-- column statistics for __HIVE_DEFAULT_PARTITION__ is not supported yet. Hence colStatState reports PARTIAL
-- basicStatState: COMPLETE colStatState: PARTIAL
explain extended select state,locid from loc_orc_1d;

create table if not exists loc_orc_2d (
  state string,
  locid int
) partitioned by(zip int, year string) stored as orc;

insert overwrite table loc_orc_2d partition(zip, year) select * from ext_loc;

analyze table loc_orc_2d partition(zip=94086, year='2000') compute statistics for columns state,locid;

analyze table loc_orc_2d partition(zip=94087, year='2000') compute statistics for columns state,locid;

analyze table loc_orc_2d partition(zip=94086, year='2001') compute statistics for columns state,locid;

analyze table loc_orc_2d partition(zip=94087, year='2001') compute statistics for columns state,locid;

explain extended select state from loc_orc_2d;

explain extended select state,locid from loc_orc_2d;
set hive.mapred.mode=nonstrict;
set hive.stats.fetch.column.stats=true;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.metastore.aggregate.stats.cache.enabled=false;


create table if not exists ext_loc (
  state string,
  locid int,
  zip int,
  year string
) row format delimited fields terminated by '|' stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/extrapolate_stats_partial.txt' OVERWRITE INTO TABLE ext_loc;

create table if not exists loc_orc_1d (
  state string,
  locid int,
  zip int
) partitioned by(year string) stored as orc;

insert overwrite table loc_orc_1d partition(year) select * from ext_loc;

analyze table loc_orc_1d partition(year='2001') compute statistics for columns state,locid;

analyze table loc_orc_1d partition(year='2002') compute statistics for columns state,locid;

describe formatted loc_orc_1d PARTITION(year='2001') state;

describe formatted loc_orc_1d PARTITION(year='2002') state;

-- basicStatState: COMPLETE colStatState: PARTIAL
explain extended select state from loc_orc_1d;

-- column statistics for __HIVE_DEFAULT_PARTITION__ is not supported yet. Hence colStatState reports PARTIAL
-- basicStatState: COMPLETE colStatState: PARTIAL
explain extended select state,locid from loc_orc_1d;

analyze table loc_orc_1d partition(year='2000') compute statistics for columns state;

analyze table loc_orc_1d partition(year='2003') compute statistics for columns state;

explain extended select state from loc_orc_1d;

explain extended select state,locid from loc_orc_1d;

create table if not exists loc_orc_2d (
  state string,
  locid int
) partitioned by(zip int, year string) stored as orc;

insert overwrite table loc_orc_2d partition(zip, year) select * from ext_loc;

analyze table loc_orc_2d partition(zip=94086, year='2001') compute statistics for columns state,locid;

analyze table loc_orc_2d partition(zip=94087, year='2002') compute statistics for columns state,locid;

explain extended select state from loc_orc_2d;

explain extended select state,locid from loc_orc_2d;
set hive.mapred.mode=nonstrict;
set hive.metastore.stats.ndv.densityfunction=true;
set hive.stats.fetch.column.stats=true;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.metastore.aggregate.stats.cache.enabled=false;


drop table if exists ext_loc;

create table ext_loc (
  state string,
  locid double,
  cnt decimal,
  zip int,
  year string
) row format delimited fields terminated by '|' stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/extrapolate_stats_partial_ndv.txt' OVERWRITE INTO TABLE ext_loc;

drop table if exists loc_orc_1d;

create table loc_orc_1d (
  state string,
  locid double,
  cnt decimal,
  zip int
) partitioned by(year string) stored as orc;

insert overwrite table loc_orc_1d partition(year) select * from ext_loc;

analyze table loc_orc_1d partition(year='2001') compute statistics for columns state,locid,cnt,zip;

analyze table loc_orc_1d partition(year='2002') compute statistics for columns state,locid,cnt,zip;

describe formatted loc_orc_1d PARTITION(year='2001') state;

describe formatted loc_orc_1d PARTITION(year='2002') state;

describe formatted loc_orc_1d PARTITION(year='2001') locid;

describe formatted loc_orc_1d PARTITION(year='2002') locid;

describe formatted loc_orc_1d PARTITION(year='2001') cnt;

describe formatted loc_orc_1d PARTITION(year='2002') cnt;

describe formatted loc_orc_1d PARTITION(year='2001') zip;

describe formatted loc_orc_1d PARTITION(year='2002') zip;

explain extended select state,locid,cnt,zip from loc_orc_1d;

analyze table loc_orc_1d partition(year='2000') compute statistics for columns state,locid,cnt,zip;

analyze table loc_orc_1d partition(year='2003') compute statistics for columns state,locid,cnt,zip;

describe formatted loc_orc_1d PARTITION(year='2000') state;

describe formatted loc_orc_1d PARTITION(year='2003') state;

describe formatted loc_orc_1d PARTITION(year='2000') locid;

describe formatted loc_orc_1d PARTITION(year='2003') locid;

describe formatted loc_orc_1d PARTITION(year='2000') cnt;

describe formatted loc_orc_1d PARTITION(year='2003') cnt;

describe formatted loc_orc_1d PARTITION(year='2000') zip;

describe formatted loc_orc_1d PARTITION(year='2003') zip;

explain extended select state,locid,cnt,zip from loc_orc_1d;

drop table if exists loc_orc_2d;

create table loc_orc_2d (
  state string,
  locid int,
  cnt decimal
) partitioned by(zip int, year string) stored as orc;

insert overwrite table loc_orc_2d partition(zip, year) select * from ext_loc;

analyze table loc_orc_2d partition(zip=94086, year='2001') compute statistics for columns state,locid,cnt;

analyze table loc_orc_2d partition(zip=94087, year='2002') compute statistics for columns state,locid,cnt;

describe formatted loc_orc_2d partition(zip=94086, year='2001') state;

describe formatted loc_orc_2d partition(zip=94087, year='2002') state;

describe formatted loc_orc_2d partition(zip=94086, year='2001') locid;

describe formatted loc_orc_2d partition(zip=94087, year='2002') locid;

describe formatted loc_orc_2d partition(zip=94086, year='2001') cnt;

describe formatted loc_orc_2d partition(zip=94087, year='2002') cnt;

explain extended select state,locid,cnt,zip from loc_orc_2d;
CREATE TABLE fetchtask_ioexception (
  KEY STRING,
  VALUE STRING) STORED AS SEQUENCEFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv1_broken.seq' OVERWRITE INTO TABLE fetchtask_ioexception;

SELECT * FROM fetchtask_ioexception;
set hive.fetch.task.aggr=true;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;

explain
select count(key),sum(key),avg(key),min(key),max(key),std(key),variance(key) from src;

select count(key),sum(key),avg(key),min(key),max(key),std(key),variance(key) from src;
CREATE TABLE dest1(key INT, value STRING) STORED AS
  INPUTFORMAT 'ClassDoesNotExist'
  OUTPUTFORMAT 'java.lang.Void';
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

DROP TABLE base64_test;

EXPLAIN
CREATE TABLE base64_test(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat';

CREATE TABLE base64_test(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat';

DESCRIBE EXTENDED base64_test;

FROM src
INSERT OVERWRITE TABLE base64_test
SELECT key, value WHERE key < 10;

SELECT * FROM base64_test;


set base64.text.input.format.signature=TFT;
set base64.text.output.format.signature=TFT;

-- Base64TextInput/OutputFormat supports signature (a prefix to check the validity of
-- the data). These queries test that prefix capabilities.

FROM src
INSERT OVERWRITE TABLE base64_test
SELECT key, value WHERE key < 10;

SELECT * FROM base64_test;


DROP TABLE base64_test;
set hive.mapred.mode=nonstrict;


create table fileformat_mix_test (src int, value string) partitioned by (ds string);
alter table fileformat_mix_test set fileformat Sequencefile;

insert overwrite table fileformat_mix_test partition (ds='1')
select key, value from src;

alter table fileformat_mix_test add partition (ds='2');

alter table fileformat_mix_test set fileformat rcfile;

select count(1) from fileformat_mix_test;

select src from fileformat_mix_test;

EXPLAIN
CREATE TABLE dest1(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileOutputFormat';

CREATE TABLE dest1(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.mapred.SequenceFileOutputFormat';

DESCRIBE EXTENDED dest1;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 10;

SELECT dest1.* FROM dest1;


EXPLAIN
CREATE TABLE dest1(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';

CREATE TABLE dest1(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';

DESCRIBE EXTENDED dest1;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 10;

SELECT dest1.* FROM dest1;


CREATE TABLE dest1(key INT, value STRING) STORED AS
  INPUTFORMAT 'java.lang.Void'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 10;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(key INT, value STRING) STORED AS
  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
  OUTPUTFORMAT 'java.lang.Void';

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 10;
set hive.mapred.mode=nonstrict;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/test/;

dfs -copyFromLocal ../../data/files/header_footer_table_1 hdfs:///tmp/test/header_footer_table_1;

dfs -copyFromLocal ../../data/files/header_footer_table_2 hdfs:///tmp/test/header_footer_table_2;

dfs -copyFromLocal ../../data/files/header_footer_table_3 hdfs:///tmp/test/header_footer_table_3;

CREATE EXTERNAL TABLE header_footer_table_1 (name string, message string, id int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LOCATION 'hdfs:///tmp/test/header_footer_table_1' tblproperties ("skip.header.line.count"="1", "skip.footer.line.count"="2");

SELECT * FROM header_footer_table_1;

SELECT * FROM header_footer_table_1 WHERE id < 50;

CREATE EXTERNAL TABLE header_footer_table_2 (name string, message string, id int) PARTITIONED BY (year int, month int, day int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' tblproperties ("skip.header.line.count"="1", "skip.footer.line.count"="2");

ALTER TABLE header_footer_table_2 ADD PARTITION (year=2012, month=1, day=1) location 'hdfs:///tmp/test/header_footer_table_2/2012/01/01';

ALTER TABLE header_footer_table_2 ADD PARTITION (year=2012, month=1, day=2) location 'hdfs:///tmp/test/header_footer_table_2/2012/01/02';

ALTER TABLE header_footer_table_2 ADD PARTITION (year=2012, month=1, day=3) location 'hdfs:///tmp/test/header_footer_table_2/2012/01/03';

SELECT * FROM header_footer_table_2;

SELECT * FROM header_footer_table_2 WHERE id < 50;

CREATE EXTERNAL TABLE emptytable (name string, message string, id int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LOCATION 'hdfs:///tmp/test/header_footer_table_3' tblproperties ("skip.header.line.count"="1", "skip.footer.line.count"="2");

SELECT * FROM emptytable;

SELECT * FROM emptytable WHERE id < 50;

DROP TABLE header_footer_table_1;

DROP TABLE header_footer_table_2;

DROP TABLE emptytable;

dfs -rmr hdfs:///tmp/test;dfs ${system:test.dfs.mkdir} hdfs:///tmp/test_file_with_header_footer_negative/;

dfs -copyFromLocal ../../data/files/header_footer_table_1 hdfs:///tmp/test_file_with_header_footer_negative/header_footer_table_1;

dfs -copyFromLocal ../../data/files/header_footer_table_2 hdfs:///tmp/test_file_with_header_footer_negative/header_footer_table_2;

CREATE EXTERNAL TABLE header_footer_table_1 (name string, message string, id int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LOCATION 'hdfs:///tmp/test_file_with_header_footer_negative/header_footer_table_1' tblproperties ("skip.header.line.count"="1", "skip.footer.line.count"="200");

SELECT * FROM header_footer_table_1;

DROP TABLE header_footer_table_1;

dfs -rmr hdfs:///tmp/test_file_with_header_footer_negative;
set hive.mapred.mode=nonstrict;
EXPLAIN
SELECT f.key, g.value
FROM src f JOIN src m JOIN src g ON(g.value = m.value AND m.value is not null AND m.value !='')
WHERE (f.key = m.key AND f.value='2008-04-08' AND m.value='2008-04-08') OR (f.key = m.key AND f.value='2008-04-09');

EXPLAIN
SELECT f.key, g.value
FROM src f JOIN src m JOIN src g ON(g.value = m.value AND m.value is not null AND m.value !='')
WHERE (f.key = m.key AND f.value IN ('2008-04-08','2008-04-10') AND m.value='2008-04-08') OR (f.key = m.key AND f.value='2008-04-09');

EXPLAIN
SELECT t1.key
FROM cbo_t1 t1
JOIN (
  SELECT t2.key
  FROM cbo_t2 t2
  JOIN (SELECT * FROM cbo_t3 t3 WHERE c_int=1) t3 ON t2.key=t3.c_int
  WHERE ((t2.key=t3.key) AND (t2.c_float + t3.c_float > 2)) OR
      ((t2.key=t3.key) AND (t2.c_int + t3.c_int > 2))) t4 ON t1.key=t4.key;

EXPLAIN
SELECT f.key, f.value, m.value
FROM src f JOIN src m ON(f.key = m.key AND m.value is not null AND m.value !='')
WHERE (f.value IN ('2008-04-08','2008-04-10') AND f.value IN ('2008-04-08','2008-04-09') AND m.value='2008-04-10') OR (m.value='2008-04-08');
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE filter_join_breaktask(key int, value string) partitioned by (ds string);

INSERT OVERWRITE TABLE filter_join_breaktask PARTITION(ds='2008-04-08')
SELECT key, value from src1;


EXPLAIN EXTENDED
SELECT f.key, g.value
FROM filter_join_breaktask f JOIN filter_join_breaktask m ON( f.key = m.key AND f.ds='2008-04-08' AND m.ds='2008-04-08' AND f.key is not null)
JOIN filter_join_breaktask g ON(g.value = m.value AND g.ds='2008-04-08' AND m.ds='2008-04-08' AND m.value is not null AND m.value !='');

SELECT f.key, g.value
FROM filter_join_breaktask f JOIN filter_join_breaktask m ON( f.key = m.key AND f.ds='2008-04-08' AND m.ds='2008-04-08' AND f.key is not null)
JOIN filter_join_breaktask g ON(g.value = m.value AND g.ds='2008-04-08' AND m.ds='2008-04-08' AND m.value is not null AND m.value !='');

set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

create table T1(c1 string, c2 string, c3 string, c4 string, c5 string, c6 string, c7 string)
partitioned by (ds string);

create table T2(c1 string, c2 string, c3 string, c0 string, c4 string, c5 string, c6 string, c7 string, c8 string, c9 string, c10 string, c11 string, c12 string, c13 string, c14 string, c15 string, c16 string, c17 string, c18 string, c19 string, c20 string, c21 string, c22 string, c23 string, c24 string,  c25 string) partitioned by (ds string);

create table T3 (c0 bigint,  c1 bigint, c2 int) partitioned by (ds string);

create table T4 (c0 bigint, c1 string, c2 string, c3 string, c4 string, c5 string, c6 string, c7 string, c8 string, c9 string, c10 string, c11 string, c12 string, c13 string, c14 string, c15 string, c16 string, c17 string, c18 string, c19 string, c20 string, c21 string, c22 string, c23 string, c24 string, c25 string, c26 string, c27 string, c28 string, c29 string, c30 string, c31 string, c32 string, c33 string, c34 string, c35 string, c36 string, c37 string, c38 string, c39 string, c40 string, c41 string, c42 string, c43 string, c44 string, c45 string, c46 string, c47 string, c48 string, c49 string, c50 string, c51 string, c52 string, c53 string, c54 string, c55 string, c56 string, c57 string, c58 string, c59 string, c60 string, c61 string, c62 string, c63 string, c64 string, c65 string, c66 string, c67 bigint, c68 string, c69 string, c70 bigint, c71 bigint, c72 bigint, c73 string, c74 string, c75 string, c76 string, c77 string, c78 string, c79 string, c80 string, c81 bigint, c82 bigint, c83 bigint) partitioned by (ds string);

insert overwrite table T1 partition (ds='2010-04-17') select '5', '1', '1', '1',  0, 0,4 from src tablesample (1 rows);

insert overwrite table T2 partition(ds='2010-04-17') select '5','name', NULL, '2', 'kavin',NULL, '9', 'c', '8', '0', '0', '7', '1','2', '0', '3','2', NULL, '1', NULL, '3','2','0','0','5','10' from src tablesample (1 rows);

insert overwrite table T3 partition (ds='2010-04-17') select 4,5,0 from src tablesample (1 rows);

insert overwrite table T4 partition(ds='2010-04-17')
select 4,'1','1','8','4','5','1','0','9','U','2','2', '0','2','1','1','J','C','A','U', '2','s', '2',NULL, NULL, NULL,NULL, NULL, NULL,'1','j', 'S', '6',NULL,'1', '2', 'J', 'g', '1', 'e', '2', '1', '2', 'U', 'P', 'p', '3', '0', '0', '0', '1', '1', '1', '0', '0', '0', '6', '2', 'j',NULL, NULL, NULL,NULL,NULL, NULL, '5',NULL, 'j', 'j', 2, 2, 1, '2', '2', '1', '1', '1', '1', '1', '1', 1, 1, 32,NULL from src limit 1;

select * from T2;
select * from T1;
select * from T3;
select * from T4;

SELECT a.c1 as a_c1, b.c1 b_c1, d.c0 as d_c0
FROM T1 a JOIN T2 b
       ON (a.c1 = b.c1 AND a.ds='2010-04-17' AND b.ds='2010-04-17')
     JOIN T3 c
       ON (a.c1 = c.c1 AND a.ds='2010-04-17' AND c.ds='2010-04-17')
     JOIN T4 d
       ON (c.c0 = d.c0 AND c.ds='2010-04-17' AND d.ds='2010-04-17');





set hive.exec.dynamic.partition.mode=nonstrict;

-- SORT_AND_HASH_QUERY_RESULTS

create table partint(key string, value string) partitioned by (ds string, hr int);
insert overwrite table partint partition(ds, hr) select * from srcpart where ds = '2008-04-08';

explain select key, value, hr from partint where hr < 11;
select key, value, hr from partint where hr < 11;

explain select key, value, hr from partint where hr <= 12 and hr > 11;
select key, value, hr from partint where hr <= 12 and hr > 11;

explain select key, value, hr from partint where hr between 11 and 12;
select key, value, hr from partint where hr between 11 and 12;

explain select key, value, hr from partint where hr not between 12 and 14;
select key, value, hr from partint where hr not between 12 and 14;

explain select key, value, hr from partint where hr < 13;
select key, value, hr from partint where hr < 13;

drop table partint;set hive.optimize.point.lookup=false;

explain
SELECT key
FROM src
WHERE
   ((key = '0'
   AND value = '8') OR (key = '1'
   AND value = '5') OR (key = '2'
   AND value = '6') OR (key = '3'
   AND value = '8') OR (key = '4'
   AND value = '1') OR (key = '5'
   AND value = '6') OR (key = '6'
   AND value = '1') OR (key = '7'
   AND value = '1') OR (key = '8'
   AND value = '1') OR (key = '9'
   AND value = '1') OR (key = '10'
   AND value = '3'))
;
drop table if exists predicate_fold_tb;

create table predicate_fold_tb(value int);
insert into predicate_fold_tb values(NULL), (1), (2), (3), (4), (5);

explain
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value = 3);
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value = 3);

explain
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value >= 3);
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value >= 3);

explain
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value <= 3);
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value <= 3);

explain
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value > 3);
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value > 3);

explain
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value < 3);
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value < 3);

explain
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value <> 3);
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value <> 3);

explain
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value > 1 AND value <=3);
SELECT * FROM predicate_fold_tb WHERE not(value IS NOT NULL AND value > 1 AND value <=3);
explain
select count(1) from src where (case key when '238' then true else false end);
explain
select count(1) from src where (case key when '238' then 1=2 else 1=1 end);
explain
select count(1) from src where (case key when '238' then 1=2 else 1=31 end);
explain
select count(1) from src where (case key when '238' then true else 1=1 end);
explain
select count(1) from src where (case key when '238' then 1=1 else 1=null end);
explain
select count(1) from src where (case key when '238' then 1=null  end);
explain
select count(1) from src where (case key when '238' then 2 = cast('2' as bigint) end);
explain
select (case key when '238' then null else false end) from src where (case key when '238' then 2 = cast('1' as bigint)  else true end);
explain
select (case key when '238' then null else null end) from src where (case key when '238' then 2 = null else 3 = null  end);
explain
select count(1) from src where (case key when '238' then null else 1=1 end);
set hive.mapred.mode=nonstrict;
explain
SELECT
SUM((CASE WHEN 1000000 = 0 THEN NULL ELSE l_partkey / 1000000 END)),
SUM(1) AS `sum_number_of_records_ok` FROM lineitem
WHERE
(((CASE WHEN ('N' = l_returnflag) THEN 1 ELSE 1 END) = 1) AND
((CASE WHEN ('MAIL' = l_shipmode) THEN 1 ELSE 1 END) = 1) AND
((CASE WHEN ('O' = l_linestatus) THEN 1 ELSE 1 END) = 1) AND
((CASE WHEN ('NONE' = l_shipinstruct) THEN 1 ELSE 1 END) = 1) AND
((CASE WHEN ('All' = (CASE WHEN (l_shipmode = 'TRUCK') THEN 'East' WHEN (l_shipmode = 'MAIL') THEN 'West' WHEN (l_shipmode = 'REG AIR') THEN 'BizDev' ELSE 'Other' END)) THEN 1 ELSE 1 END) = 1) AND
((CASE WHEN ('AIR' = l_shipmode) THEN 1 ELSE 1 END) = 1) AND
((CASE WHEN ('1996-03-30' = TO_DATE(l_shipdate)) THEN 1 ELSE NULL END) = 1) AND
((CASE WHEN ('RAIL' = l_shipmode) THEN 1 ELSE NULL END) = 1) AND (1 = 1) AND
((CASE WHEN (1 = l_linenumber) THEN 1 ELSE 1 END) = 1) AND (1 = 1))
GROUP BY l_orderkey;


explain select key from src where (case key when '238' then 1 else 2 end) = 1;
explain select key from src where (case key when '238' then 1  when '94' then 1 else 3 end) = cast('1' as int);
explain select key from src where (case key when '238' then 1 else 2 end) = (case when key != '238' then 1 else 1 end);
explain select key from src where (case key when '238' then 1 end) = (case when key != '238' then 1 when key = '23' then 1 end);
explain
select key from src where ((case when (key = '238') then null     end) = 1);
explain
select key from src where ((case when (key = '238') then null else null end) = 1);
explain
select key from src where ((case when (key = '238') then 1 else 1 end) = 1);
explain
select key from src where ((case when (key = '238') then 1 else 1 end) = 2);
explain
select key from src where ((case when (key = '238') then 1 else null end) = 1);
explain
select key from src where ((case when (key = '238') then 1=1 else null=1 end));
explain
select key from src where ((case when (key = '238') then 1=1 else 2=2 end));
explain
select key from src where ((case when (key = '238') then 1=3 else 2=1 end));
explain
select key from src where ((case when (key = '238') then 1=1 else 2=1 end));
explain
select key from src where ((case when (key = '238') then 1=3 else 1=1 end));
explain
select key from src where ((case when ('23' = '23') then 1 else 1 end) = 1);
explain
select key from src where ((case when ('2' = '238') then 1 else 2 end) = 2);
explain
select key from src where ((case when (true=null) then 1 else 1 end) = 1);
explain
select key from src where ((case when (key = (case when (key = '238') then '11' else '11'  end)) then false else true end));
explain
select key from src where ((case when (key = (case when (key = '238') then '12' else '11'  end)) then 2=2   else true end));

set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;

-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

EXPLAIN EXTENDED
 FROM
  srcpart a
 FULL OUTER JOIN
  src b
 ON (a.key = b.key AND a.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  srcpart a
 FULL OUTER JOIN
  src b
 ON (a.key = b.key AND a.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;


EXPLAIN EXTENDED
 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';

 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';

EXPLAIN EXTENDED
 FROM
  srcpart a
 FULL OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND a.ds = '2008-04-08';

 FROM
  srcpart a
 FULL OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND a.ds = '2008-04-08';

set fs.default.name='http://www.example.com;
show tables;
set fs.default.name='http://www.example.com;
SELECT * FROM src;
this is totally garbage SELECT src.key WHERE a lot of garbage
set hive.mapred.mode=nonstrict;
explain
select *, sum(key) from src group by key, value limit 10;
select *, sum(key) from src group by key, value limit 10;

explain
select *, sum(key) from src where key < 100 group by key, value limit 10;
select *, sum(key) from src where key < 100 group by key, value limit 10;

explain
select *, sum(key) from (select key from src where key < 100) a group by key limit 10;
select *, sum(key) from (select key from src where key < 100) a group by key limit 10;

explain
select a.*, sum(src.key) from (select key from src where key < 100) a
inner join src on a.key = src.key group by a.key limit 10;
select a.*, sum(src.key) from (select key from src where key < 100) a
inner join src on a.key = src.key group by a.key limit 10;
select *, count(value) from src group by key;
select *, sum(key) from src;
-- -*- mode:sql -*-

DROP TABLE IF EXISTS hbase_bulk;

CREATE TABLE hbase_bulk (key INT, value STRING)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ('hbase.columns.mapping' = ':key,cf:string');

SET hive.hbase.generatehfiles = true;
INSERT OVERWRITE TABLE hbase_bulk SELECT * FROM src CLUSTER BY key;
create table testFail (a int) stored as foo;
set hive.mapred.mode=nonstrict;
set hive.limit.optimize.enable=true;
set hive.limit.optimize.limit.file=2;

drop table gl_tgt;
drop table gl_src1;
drop table gl_src2;
drop table gl_src_part1;


create table gl_src1 (key int, value string) stored as textfile;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE gl_src1;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE gl_src1;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE gl_src1;




set hive.limit.row.max.size=100;

-- need one file
create table gl_tgt as select key from gl_src1 limit 1;
select * from gl_tgt ORDER BY key ASC;
-- need two files
select 'x' as key_new , split(value,',') as value_new from gl_src1 ORDER BY key_new ASC, value_new[0] ASC limit 20;
-- no sufficient files
select key, value, split(value,',') as value_new from gl_src1 ORDER BY key ASC, value ASC, value_new[0] ASC limit 30;
-- need all files
select key from gl_src1 ORDER BY key ASC limit 100;
set hive.limit.optimize.limit.file=4;
select key from gl_src1 ORDER BY key ASC limit 30;

-- not qualified cases
select key, count(1) from gl_src1 group by key ORDER BY key ASC limit 5;
select distinct key from gl_src1 ORDER BY key ASC limit 10;
select count(1) from gl_src1 limit 1;
select transform(*) using "tr _ \n" as t from
(select "a_a_a_a_a_a_" from gl_src1 limit 100) subq ORDER BY t;
select key from (select * from (select key,value from gl_src1)t1 limit 10)t2 ORDER BY key ASC limit 2000;

-- complicated queries
select key from (select * from (select key,value from gl_src1 limit 10)t1 )t2 ORDER BY key ASC;
select key from (select * from (select key,value from gl_src1)t1 limit 10)t2 ORDER BY key ASC;
insert overwrite table gl_tgt select key+1 from (select * from (select key,value from gl_src1)t1)t2 limit 10;
select * from gl_tgt ORDER BY key ASC;

-- empty table
create table gl_src2 (key int, value string) stored as textfile;
select key from gl_src2 ORDER BY key ASC limit 10;

-- partition
create table gl_src_part1 (key int, value string) partitioned by (p string) stored as textfile;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE gl_src_part1 partition(p='11');
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE gl_src_part1 partition(p='12');
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE gl_src_part1 partition(p='12');
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE gl_src_part1 partition(p='12');

select key from gl_src_part1 where p like '1%' ORDER BY key ASC limit 10;
select key from gl_src_part1 where p='11' ORDER BY key ASC limit 10;
select key from gl_src_part1 where p='12' ORDER BY key ASC limit 10;
select key from gl_src_part1 where p='13' ORDER BY key ASC limit 10;
alter table gl_src_part1 add partition (p='13');
select key from gl_src_part1 where p='13' ORDER BY key ASC limit 10;
select key from gl_src_part1 where p='12' ORDER BY key ASC limit 1000;

set hive.fetch.task.conversion=none;
select * from gl_src1 limit 1;

drop table gl_src1;
drop table gl_src2;
drop table gl_src_part1;
drop table gl_tgt;
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_g1(key INT, value DOUBLE) STORED AS TEXTFILE;

set fs.default.name=invalidscheme:///;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

set fs.default.name=file:///;

FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

SELECT dest_g1.* FROM dest_g1;
set hive.map.aggr=false;
set hive.multigroupby.singlereducer=false;
set hive.groupby.skewindata=true;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, val1 INT, val2 INT);
CREATE TABLE dest2(key INT, val1 INT, val2 INT);

CREATE TABLE INPUT(key INT, value STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv5.txt' INTO TABLE INPUT;

EXPLAIN
FROM INPUT
INSERT OVERWRITE TABLE dest1 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key;

FROM INPUT
INSERT OVERWRITE TABLE dest1 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key;

SELECT * from dest1;
SELECT * from dest2;

set hive.multigroupby.singlereducer=true;

EXPLAIN
FROM INPUT
INSERT OVERWRITE TABLE dest1 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key;

FROM INPUT
INSERT OVERWRITE TABLE dest1 SELECT INPUT.key, count(substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(substr(INPUT.value,5)), sum(distinct substr(INPUT.value,5))   GROUP BY INPUT.key;

SELECT * from dest1;
SELECT * from dest2;

set hive.groupby.skewindata=false;
-- HIVE-3852 Multi-groupby optimization fails when same distinct column is used twice or more

EXPLAIN
FROM INPUT
INSERT OVERWRITE TABLE dest1 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), avg(distinct substr(INPUT.value,5)) GROUP BY INPUT.key;

FROM INPUT
INSERT OVERWRITE TABLE dest1 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), count(distinct substr(INPUT.value,5)) GROUP BY INPUT.key
INSERT OVERWRITE TABLE dest2 SELECT INPUT.key, sum(distinct substr(INPUT.value,5)), avg(distinct substr(INPUT.value,5)) GROUP BY INPUT.key;

SELECT * from dest1;
SELECT * from dest2;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key STRING, val1 INT, val2 INT) partitioned by (ds string);
CREATE TABLE dest2(key STRING, val1 INT, val2 INT) partitioned by (ds string);

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 partition(ds='111')
  SELECT src.value, count(src.key), count(distinct src.key) GROUP BY src.value
INSERT OVERWRITE TABLE dest2  partition(ds='111')
  SELECT substr(src.value, 5), count(src.key), count(distinct src.key) GROUP BY substr(src.value, 5);

FROM src
INSERT OVERWRITE TABLE dest1 partition(ds='111')
  SELECT src.value, count(src.key), count(distinct src.key) GROUP BY src.value
INSERT OVERWRITE TABLE dest2  partition(ds='111')
  SELECT substr(src.value, 5), count(src.key), count(distinct src.key) GROUP BY substr(src.value, 5);

SELECT * from dest1;
SELECT * from dest2;



set hive.map.aggr=false;

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT COUNT(src.key), COUNT(DISTINCT value) GROUP BY src.key;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT COUNT(src.key), COUNT(DISTINCT value) GROUP BY src.key;

SELECT dest1.* FROM dest1;

CREATE TABLE grpby_test (int_col_5 INT,
  int_col_7 INT);

SET hive.mapred.mode=strict;

EXPLAIN
SELECT
int_col_7,
MAX(LEAST(COALESCE(int_col_5, -279),
  COALESCE(int_col_7, 476))) AS int_col
FROM grpby_test
GROUP BY
int_col_7,
int_col_7,
LEAST(COALESCE(int_col_5, -279),
  COALESCE(int_col_7, 476));
set hive.mapred.mode=nonstrict;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key LIMIT 5;

FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key ORDER BY src.key LIMIT 5;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set hive.groupby.mapaggr.checkinterval=20;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

FROM src INSERT OVERWRITE TABLE dest1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_g1(key INT, value DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,5)) GROUP BY src.key;

SELECT dest_g1.* FROM dest_g1;
FROM src
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1)
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

CREATE TABLE dest_g2(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);

-- SORT_QUERY_RESULTS

SELECT dest_g2.* FROM dest_g2;
set hive.mapred.mode=nonstrict;
set mapred.reduce.tasks=31;

EXPLAIN
SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 5;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 5;

set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;

-- HIVE-5560 when group by key is used in distinct funtion, invalid result are returned

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.key,1,1)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.key,1,1)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1 order by key;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

CREATE TABLE dest_g2(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

SELECT dest_g2.* FROM dest_g2;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_g2(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))) GROUP BY substr(src.key,1,1);

SELECT dest_g2.* FROM dest_g2;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_g2(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

SELECT dest_g2.* FROM dest_g2;
FROM src
SELECT sum(substr(src.value,5)), avg(substr(src.value,5)), avg(DISTINCT substr(src.value,5)), max(substr(src.value,5)), min(substr(src.value,5))
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5));

FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5));

SELECT dest1.* FROM dest1;


set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5));

FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5));

SELECT
c1,
c2,
round(c3, 11) c3,
c4,
c5,
round(c6, 11) c6,
round(c7, 11) c7,
round(c8, 5) c8,
round(c9, 9) c9
FROM dest1;


set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE, c10 DOUBLE, c11 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5));

FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5));

SELECT ROUND(c1, 1), ROUND(c2, 3), ROUND(c3, 5), ROUND(c4, 1), ROUND(c5, 1), ROUND(c6, 5),
ROUND(c7,5), ROUND(c8, 5), ROUND(c9, 5) FROM dest1;

set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE, c10 DOUBLE, c11 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

SELECT dest1.* FROM dest1;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE, c10 DOUBLE, c11 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));


FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;

set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5));

FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5));

SELECT dest1.* FROM dest1;



set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;

set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(c1 DOUBLE, c2 DOUBLE, c3 DOUBLE, c4 DOUBLE, c5 DOUBLE, c6 DOUBLE, c7 DOUBLE, c8 DOUBLE, c9 DOUBLE, c10 DOUBLE, c11 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

FROM src
INSERT OVERWRITE TABLE dest1 SELECT
  sum(substr(src.value,5)),
  avg(substr(src.value,5)),
  avg(DISTINCT substr(src.value,5)),
  max(substr(src.value,5)),
  min(substr(src.value,5)),
  std(substr(src.value,5)),
  stddev_samp(substr(src.value,5)),
  variance(substr(src.value,5)),
  var_samp(substr(src.value,5)),
  sum(DISTINCT substr(src.value, 5)),
  count(DISTINCT substr(src.value, 5));

SELECT dest1.* FROM dest1;

FROM src
SELECT substr(src.key,1,1) GROUP BY substr(src.key,1,1)
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1) GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;

set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(key INT) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT count(1);

FROM src INSERT OVERWRITE TABLE dest1 SELECT count(1);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(key INT) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT count(1);

FROM src INSERT OVERWRITE TABLE dest1 SELECT count(1);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;

set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr(src.key,1,1) GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;


SELECT src.key, sum(substr(src.value,5))
FROM src
GROUP BY src.key
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
INSERT OVERWRITE TABLE dest1
SELECT src.key, sum(substr(src.value,5))
FROM src
GROUP BY src.key;

INSERT OVERWRITE TABLE dest1
SELECT src.key, sum(substr(src.value,5))
FROM src
GROUP BY src.key;

SELECT dest1.* FROM dest1;

set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(key INT) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT sum(src.key);

FROM src INSERT OVERWRITE TABLE dest1 SELECT sum(src.key);

SELECT dest1.* FROM dest1;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

CREATE TABLE dest1(key INT) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT sum(src.key);

FROM src INSERT OVERWRITE TABLE dest1 SELECT sum(src.key);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;

set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
INSERT OVERWRITE TABLE dest1
SELECT src.key, sum(substr(src.value,5))
FROM src
GROUP BY src.key;

INSERT OVERWRITE TABLE dest1
SELECT src.key, sum(substr(src.value,5))
FROM src
GROUP BY src.key;

SELECT dest1.* FROM dest1;

FROM src
SELECT DISTINCT substr(src.value,5,1)
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);

SELECT dest1.* FROM dest1;


set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);

SELECT dest1.* FROM dest1;


set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);

SELECT dest1.* FROM dest1;


set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;

set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);

FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT substr(src.value,5,1);

SELECT dest1.* FROM dest1;


set hive.map.aggr=false;
set hive.groupby.skewindata=true;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
set hive.map.aggr=true;
set hive.multigroupby.singlereducer=false;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
set hive.map.aggr=false;
set hive.multigroupby.singlereducer=false;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
set hive.map.aggr=false;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key ORDER BY SRC.key limit 10
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key ORDER BY SRC.key limit 10;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key ORDER BY SRC.key limit 10
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, sum(SUBSTR(SRC.value,5)) GROUP BY SRC.key ORDER BY SRC.key limit 10;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;

set hive.multigroupby.singlereducer=false;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;

set hive.map.aggr=true;
set hive.groupby.skewindata=true;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;

set hive.map.aggr=false;

set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key INT, val1 STRING, val2 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.value, SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.value, SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;

set hive.multigroupby.singlereducer=false;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(SUBSTR(SRC.value,5)) GROUP BY SRC.key, SRC.value;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.value, SRC.key;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.key
INSERT OVERWRITE TABLE DEST2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) GROUP BY SRC.value, SRC.key;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;


set hive.map.aggr.hash.percentmemory = 0.3;
set hive.mapred.local.mem = 384;

add file ../../data/scripts/dumpdata_script.py;

select count(distinct subq.key) from
(FROM src MAP src.key USING 'python dumpdata_script.py' AS key WHERE src.key = 10) subq;
-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key ARRAY<STRING>, value BIGINT) STORED AS TEXTFILE;
CREATE TABLE DEST2(key MAP<STRING, STRING>, value BIGINT) STORED AS TEXTFILE;
CREATE TABLE DEST3(key STRUCT<col1:STRING, col2:STRING>, value BIGINT) STORED AS TEXTFILE;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT ARRAY(SRC.key), COUNT(1) GROUP BY ARRAY(SRC.key)
INSERT OVERWRITE TABLE DEST2 SELECT MAP(SRC.key, SRC.value), COUNT(1) GROUP BY MAP(SRC.key, SRC.value)
INSERT OVERWRITE TABLE DEST3 SELECT STRUCT(SRC.key, SRC.value), COUNT(1) GROUP BY STRUCT(SRC.key, SRC.value);

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT ARRAY(SRC.key), COUNT(1) GROUP BY ARRAY(SRC.key)
INSERT OVERWRITE TABLE DEST2 SELECT MAP(SRC.key, SRC.value), COUNT(1) GROUP BY MAP(SRC.key, SRC.value)
INSERT OVERWRITE TABLE DEST3 SELECT STRUCT(SRC.key, SRC.value), COUNT(1) GROUP BY STRUCT(SRC.key, SRC.value);

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
SELECT DEST3.* FROM DEST3;

set hive.multigroupby.singlereducer=true;

-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key ARRAY<STRING>, value BIGINT) STORED AS TEXTFILE;
CREATE TABLE DEST2(key MAP<STRING, STRING>, value BIGINT) STORED AS TEXTFILE;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT ARRAY(SRC.key) as keyarray, COUNT(1) GROUP BY ARRAY(SRC.key) ORDER BY keyarray limit 10
INSERT OVERWRITE TABLE DEST2 SELECT MAP(SRC.key, SRC.value) as kvmap, COUNT(1) GROUP BY MAP(SRC.key, SRC.value) ORDER BY kvmap limit 10;

FROM SRC
INSERT OVERWRITE TABLE DEST1 SELECT ARRAY(SRC.key) as keyarray, COUNT(1) GROUP BY ARRAY(SRC.key) ORDER BY keyarray limit 10
INSERT OVERWRITE TABLE DEST2 SELECT MAP(SRC.key, SRC.value) as kvmap, COUNT(1) GROUP BY MAP(SRC.key, SRC.value) ORDER BY kvmap limit 10;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;

set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

EXPLAIN
SELECT key, val, count(1) FROM T1 GROUP BY key, val with cube;

SELECT key, val, count(1) FROM T1 GROUP BY key, val with cube;

EXPLAIN
SELECT key, val, GROUPING__ID, count(1) FROM T1 GROUP BY key, val with cube;

SELECT key, val, GROUPING__ID, count(1) FROM T1 GROUP BY key, val with cube;

EXPLAIN
SELECT key, count(distinct val) FROM T1 GROUP BY key with cube;

SELECT key, count(distinct val) FROM T1 GROUP BY key with cube;

set hive.groupby.skewindata=true;

EXPLAIN
SELECT key, val, count(1) FROM T1 GROUP BY key, val with cube;

SELECT key, val, count(1) FROM T1 GROUP BY key, val with cube;

EXPLAIN
SELECT key, count(distinct val) FROM T1 GROUP BY key with cube;

SELECT key, count(distinct val) FROM T1 GROUP BY key with cube;


set hive.multigroupby.singlereducer=true;

CREATE TABLE T2(key1 STRING, key2 STRING, val INT) STORED AS TEXTFILE;
CREATE TABLE T3(key1 STRING, key2 STRING, val INT) STORED AS TEXTFILE;

EXPLAIN
FROM T1
INSERT OVERWRITE TABLE T2 SELECT key, val, count(1) group by key, val with cube
INSERT OVERWRITE TABLE T3 SELECT key, val, sum(1) group by key, val with cube;


FROM T1
INSERT OVERWRITE TABLE T2 SELECT key, val, count(1) group by key, val with cube
INSERT OVERWRITE TABLE T3 SELECT key, val, sum(1) group by key, val with cube;

set hive.map.aggr=false;

SELECT key, count(distinct value) FROM src GROUP BY key with cube;

set hive.map.aggr=true;

SELECT key, value, count(distinct value) FROM src GROUP BY key, value with cube;

set hive.multigroupby.singlereducer=false;

create table t1 like src;
create table t2 like src;

explain from src
insert into table t1 select
key, GROUPING__ID
group by key, value with cube
insert into table t2 select
key, value
group by key, value grouping sets ((key), (key, value));create table t1 like src;
create table t2 like src;

explain from src
insert into table t1 select
key, GROUPING__ID
group by key, value with cube
insert into table t2 select
key, value
group by key, value grouping sets ((key), (key, value));set hive.mapred.mode=nonstrict;
-- This test covers HIVE-2332

create table t1 (int1 int, int2 int, str1 string, str2 string);

set hive.optimize.reducededuplication=false;
--disabled RS-dedup for keeping intention of test

insert into table t1 select cast(key as int), cast(key as int), value, value from src where key < 6;
explain select Q1.int1, sum(distinct Q1.int1) from (select * from t1 order by int1) Q1 group by Q1.int1;
explain select int1, sum(distinct int1) from t1 group by int1;

select Q1.int1, sum(distinct Q1.int1) from (select * from t1 order by int1) Q1 group by Q1.int1;
select int1, sum(distinct int1) from t1 group by int1;

drop table t1;
explain
select distinct key, "" as dummy1, "" as dummy2 from src tablesample (10 rows);

select distinct key, "" as dummy1, "" as dummy2 from src tablesample (10 rows);

explain
create table dummy as
select distinct key, "X" as dummy1, "X" as dummy2 from src tablesample (10 rows);

create table dummy as
select distinct key, "X" as dummy1, "X" as dummy2 from src tablesample (10 rows);

select key,dummy1,dummy2 from dummy;

explain
select max('pants'), max('pANTS') from src group by key limit 1;
select max('pants'), max('pANTS') from src group by key limit 1;
CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

SELECT key, val, GROUPING__ID from T1 group by key, val with cube;

SELECT GROUPING__ID, key, val from T1 group by key, val with rollup;

SELECT key, val, GROUPING__ID, CASE WHEN GROUPING__ID == 0 THEN "0" WHEN GROUPING__ID == 1 THEN "1" WHEN GROUPING__ID == 2 THEN "2" WHEN GROUPING__ID == 3 THEN "3" ELSE "nothing" END from T1 group by key, val with cube;

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;

SELECT GROUPING__ID FROM T1;

CREATE TABLE T1(key INT, value INT) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/groupby_groupingid.txt' INTO TABLE T1;

set hive.groupby.skewindata = true;

-- SORT_QUERY_RESULTS

SELECT key, value, GROUPING__ID, count(*) from T1 GROUP BY key, value WITH ROLLUP;

SELECT GROUPING__ID, count(*)
FROM
(
SELECT key, value, GROUPING__ID, count(*) from T1 GROUP BY key, value WITH ROLLUP
) t
GROUP BY GROUPING__ID;

SELECT t1.GROUPING__ID, t2.GROUPING__ID FROM (SELECT GROUPING__ID FROM T1  GROUP BY key,value WITH ROLLUP) t1
JOIN
(SELECT GROUPING__ID FROM T1 GROUP BY key, value WITH ROLLUP) t2
ON t1.GROUPING__ID = t2.GROUPING__ID;





set hive.groupby.skewindata = false;

SELECT key, value, GROUPING__ID, count(*) from T1 GROUP BY key, value WITH ROLLUP;

SELECT GROUPING__ID, count(*)
FROM
(
SELECT key, value, GROUPING__ID, count(*) from T1 GROUP BY key, value WITH ROLLUP
) t
GROUP BY GROUPING__ID;

SELECT t1.GROUPING__ID, t2.GROUPING__ID FROM (SELECT GROUPING__ID FROM T1  GROUP BY key,value WITH ROLLUP) t1
JOIN
(SELECT GROUPING__ID FROM T1 GROUP BY key, value WITH ROLLUP) t2
ON t1.GROUPING__ID = t2.GROUPING__ID;


CREATE TABLE T1(key INT, value INT) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/groupby_groupingid.txt' INTO TABLE T1;

set hive.cbo.enable = false;

-- SORT_QUERY_RESULTS

SELECT key, value, GROUPING__ID, count(*)
FROM T1
GROUP BY key, value
GROUPING SETS ((), (key))
HAVING GROUPING__ID = 1;

set hive.cbo.enable = true;

SELECT key, value, GROUPING__ID, count(*)
FROM T1
GROUP BY key, value
GROUPING SETS ((), (key))
HAVING GROUPING__ID = 1;

CREATE TABLE T1(a STRING, b STRING, c STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/grouping_sets.txt' INTO TABLE T1;

SELECT * FROM T1;

SELECT a, b, count(*) from T1 group by a, b with cube;

SELECT a, b, count(*) FROM T1 GROUP BY a, b  GROUPING SETS (a, (a, b), b, ());

SELECT a, b, count(*) FROM T1 GROUP BY a, b GROUPING SETS (a, (a, b));

SELECT a FROM T1 GROUP BY a, b, c GROUPING SETS (a, b, c);

SELECT a FROM T1 GROUP BY a GROUPING SETS ((a), (a));

SELECT a + b, count(*) FROM T1 GROUP BY a + b GROUPING SETS (a+b);

CREATE TABLE T1(a STRING, b STRING, c STRING);

-- Check for empty grouping set
SELECT * FROM T1 GROUP BY a GROUPING SETS (());

set hive.mapred.mode=nonstrict;
set hive.new.job.grouping.set.cardinality=2;

CREATE TABLE T1(a STRING, b STRING, c STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/grouping_sets.txt' INTO TABLE T1;

-- Since 4 grouping sets would be generated for the query below, an additional MR job should be created
EXPLAIN
SELECT a, b, count(*) from T1 group by a, b with cube;
SELECT a, b, count(*) from T1 group by a, b with cube;

EXPLAIN
SELECT a, b, sum(c) from T1 group by a, b with cube;
SELECT a, b, sum(c) from T1 group by a, b with cube;

CREATE TABLE T2(a STRING, b STRING, c int, d int);

INSERT OVERWRITE TABLE T2
SELECT a, b, c, c from T1;

EXPLAIN
SELECT a, b, sum(c+d) from T2 group by a, b with cube;
SELECT a, b, sum(c+d) from T2 group by a, b with cube;
CREATE TABLE T1(a STRING, b STRING, c STRING);

-- Check for mupltiple empty grouping sets
SELECT * FROM T1 GROUP BY b GROUPING SETS ((), (), ());
-- In this test, 2 files are loaded into table T1. The data contains rows with the same value of a and b,
-- with different number of rows for a and b in each file. Since bucketizedHiveInputFormat is used,
-- this tests that the aggregate function stores the partial aggregate state correctly even if an
-- additional MR job is created for processing the grouping sets.
CREATE TABLE T1(a STRING, b STRING, c STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/grouping_sets1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/grouping_sets2.txt' INTO TABLE T1;

set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.new.job.grouping.set.cardinality = 30;

-- The query below will execute in a single MR job, since 4 rows are generated per input row
-- (cube of a,b will lead to (a,b), (a, null), (null, b) and (null, null) and
-- hive.new.job.grouping.set.cardinality is more than 4.
EXPLAIN
SELECT a, b, avg(c), count(*) from T1 group by a, b with cube;
SELECT a, b, avg(c), count(*) from T1 group by a, b with cube;

set hive.new.job.grouping.set.cardinality=2;

-- The query below will execute in 2 MR jobs, since hive.new.job.grouping.set.cardinality is set to 2.
-- The partial aggregation state should be maintained correctly across MR jobs.
EXPLAIN
SELECT a, b, avg(c), count(*) from T1 group by a, b with cube;
SELECT a, b, avg(c), count(*) from T1 group by a, b with cube;

CREATE TABLE T1(a STRING, b STRING, c STRING);

-- Grouping sets expression is not in GROUP BY clause
SELECT a FROM T1 GROUP BY a GROUPING SETS (a, b);
set hive.mapred.mode=nonstrict;
set hive.merge.mapfiles = false;
set hive.merge.mapredfiles = false;

-- SORT_QUERY_RESULTS

-- Set merging to false above to make the explain more readable

CREATE TABLE T1(a STRING, b STRING, c STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/grouping_sets.txt' INTO TABLE T1;

-- This tests that cubes and rollups work fine inside sub-queries.
EXPLAIN
SELECT * FROM
(SELECT a, b, count(*) from T1 where a < 3 group by a, b with cube) subq1
join
(SELECT a, b, count(*) from T1 where a < 3 group by a, b with cube) subq2
on subq1.a = subq2.a;

SELECT * FROM
(SELECT a, b, count(*) from T1 where a < 3 group by a, b with cube) subq1
join
(SELECT a, b, count(*) from T1 where a < 3 group by a, b with cube) subq2
on subq1.a = subq2.a;

set hive.new.job.grouping.set.cardinality=2;

-- Since 4 grouping sets would be generated for each sub-query, an additional MR job should be created
-- for each of them
EXPLAIN
SELECT * FROM
(SELECT a, b, count(*) from T1 where a < 3 group by a, b with cube) subq1
join
(SELECT a, b, count(*) from T1 where a < 3 group by a, b with cube) subq2
on subq1.a = subq2.a;

SELECT * FROM
(SELECT a, b, count(*) from T1 where a < 3 group by a, b with cube) subq1
join
(SELECT a, b, count(*) from T1 where a < 3 group by a, b with cube) subq2
on subq1.a = subq2.a;

CREATE TABLE T1(a STRING, b STRING, c STRING);

-- Expression 'a' is not in GROUP BY clause
SELECT a FROM T1 GROUP BY b GROUPING SETS (b);
set hive.mapred.mode=nonstrict;
set hive.merge.mapfiles = false;
set hive.merge.mapredfiles = false;
-- Set merging to false above to make the explain more readable

CREATE TABLE T1(a STRING, b STRING, c STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/grouping_sets.txt' INTO TABLE T1;

-- This tests that cubes and rollups work fine where the source is a sub-query
EXPLAIN
SELECT a, b, count(*) FROM
(SELECT a, b, count(1) from T1 group by a, b) subq1 group by a, b with cube;

SELECT a, b, count(*) FROM
(SELECT a, b, count(1) from T1 group by a, b) subq1 group by a, b with cube;

set hive.new.job.grouping.set.cardinality=2;

-- Since 4 grouping sets would be generated for the cube, an additional MR job should be created
EXPLAIN
SELECT a, b, count(*) FROM
(SELECT a, b, count(1) from T1 group by a, b) subq1 group by a, b with cube;

SELECT a, b, count(*) FROM
(SELECT a, b, count(1) from T1 group by a, b) subq1 group by a, b with cube;
CREATE TABLE T1(a STRING, b STRING, c STRING);

-- Alias in GROUPING SETS
SELECT a as c, count(*) FROM T1 GROUP BY c GROUPING SETS (c);

set hive.mapred.mode=nonstrict;
CREATE TABLE T1(a STRING, b STRING, c STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/grouping_sets.txt' INTO TABLE T1;

set hive.optimize.ppd = false;

-- This filter is not pushed down
EXPLAIN
SELECT a, b FROM
(SELECT a, b from T1 group by a, b grouping sets ( (a,b),a )) res
WHERE res.a=5;

SELECT a, b FROM
(SELECT a, b from T1 group by a, b grouping sets ( (a,b),a )) res
WHERE res.a=5;

set hive.cbo.enable = true;

-- This filter is pushed down through aggregate with grouping sets by Calcite
EXPLAIN
SELECT a, b FROM
(SELECT a, b from T1 group by a, b grouping sets ( (a,b),a )) res
WHERE res.a=5;

SELECT a, b FROM
(SELECT a, b from T1 group by a, b grouping sets ( (a,b),a )) res
WHERE res.a=5;
set hive.new.job.grouping.set.cardinality=2;

CREATE TABLE T1(a STRING, b STRING, c STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

-- Since 4 grouping sets would be generated for the query below, an additional MR job should be created
-- This is not allowed with distincts.
SELECT a, b, count(distinct c) from T1 group by a, b with cube;

set hive.new.job.grouping.set.cardinality=2;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;

CREATE TABLE T1(a STRING, b STRING, c STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE;

-- Since 4 grouping sets would be generated for the query below, an additional MR job should be created
-- This is not allowed with map-side aggregation and skew
SELECT a, b, count(1) from T1 group by a, b with cube;

create table t(category int, live int, comments int);
insert into table t select key, 0, 2 from src tablesample(3 rows);

explain
select category, max(live) live, max(comments) comments, rank() OVER (PARTITION BY category ORDER BY comments) rank1
FROM t
GROUP BY category
GROUPING SETS ((), (category))
HAVING max(comments) > 0;

select category, max(live) live, max(comments) comments, rank() OVER (PARTITION BY category ORDER BY comments) rank1
FROM t
GROUP BY category
GROUPING SETS ((), (category))
HAVING max(comments) > 0;
set hive.groupby.orderby.position.alias=true;

-- invalid position alias in group by
SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY 3;
set hive.mapred.mode=nonstrict;
set hive.transpose.aggr.join=true;
EXPLAIN
SELECT f.key, g.key, count(g.key)
FROM src f JOIN src g ON(f.key = g.key)
GROUP BY f.key, g.key;

EXPLAIN
SELECT f.key, g.key
FROM src f JOIN src g ON(f.key = g.key)
GROUP BY f.key, g.key;

EXPLAIN
SELECT DISTINCT f.value, g.value
FROM src f JOIN src g ON(f.value = g.value);

EXPLAIN
SELECT f.key, g.key, COUNT(*)
FROM src f JOIN src g ON(f.key = g.key)
GROUP BY f.key, g.key;

EXPLAIN
SELECT  f.ctinyint, g.ctinyint, SUM(f.cbigint)
FROM alltypesorc f JOIN alltypesorc g ON(f.cint = g.cint)
GROUP BY f.ctinyint, g.ctinyint ;

EXPLAIN
SELECT  f.cbigint, g.cbigint, MAX(f.cint)
FROM alltypesorc f JOIN alltypesorc g ON(f.cbigint = g.cbigint)
GROUP BY f.cbigint, g.cbigint ;

explain
SELECT  f.ctinyint, g.ctinyint, MIN(f.ctinyint)
FROM alltypesorc f JOIN alltypesorc g ON(f.ctinyint = g.ctinyint)
GROUP BY f.ctinyint, g.ctinyint;

explain
SELECT   MIN(f.cint)
FROM alltypesorc f JOIN alltypesorc g ON(f.ctinyint = g.ctinyint)
GROUP BY f.ctinyint, g.ctinyint;

explain
SELECT   count(f.ctinyint)
FROM alltypesorc f JOIN alltypesorc g ON(f.ctinyint = g.ctinyint)
GROUP BY f.ctinyint, g.ctinyint;

explain
SELECT   count(f.cint), f.ctinyint
FROM alltypesorc f JOIN alltypesorc g ON(f.ctinyint = g.ctinyint)
GROUP BY f.ctinyint, g.ctinyint;

explain
SELECT   sum(f.cint), f.ctinyint
FROM alltypesorc f JOIN alltypesorc g ON(f.ctinyint = g.ctinyint)
GROUP BY f.ctinyint, g.ctinyint;

SELECT concat(value, concat(value)) FROM src GROUP BY concat(value);
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;

EXPLAIN EXTENDED
FROM srcpart src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5)))
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1);

FROM srcpart src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5)))
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;
set mapred.reduce.tasks=31;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING, C3 INT, c4 INT) STORED AS TEXTFILE;

EXPLAIN EXTENDED
FROM srcpart src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(DISTINCT src.value)
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1);

FROM srcpart src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(DISTINCT src.value)
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;
set hive.map.aggr=true;

-- SORT_QUERY_RESULTS

create table dest1(key int, cnt int);
create table dest2(key int, cnt int);

explain
from src
insert overwrite table dest1 select key, count(distinct value) group by key
insert overwrite table dest2 select key+key, count(distinct value) group by key+key;

from src
insert overwrite table dest1 select key, count(distinct value) group by key
insert overwrite table dest2 select key+key, count(distinct value) group by key+key;


select * from dest1 where key < 10;
select * from dest2 where key < 20 order by key limit 10;

set hive.multigroupby.singlereducer=true;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_g2(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;
CREATE TABLE dest_g3(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;
CREATE TABLE dest_g4(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;
CREATE TABLE dest_h2(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;
CREATE TABLE dest_h3(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g3 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) < 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g4 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g3 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) < 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g4 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1);

SELECT * FROM dest_g2;
SELECT * FROM dest_g3;
SELECT * FROM dest_g4;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g3 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) < 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g4 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_h2 SELECT substr(src.key,1,1) as c1, count(DISTINCT substr(src.value,5)) as c2, concat(substr(src.key,1,1),sum(substr(src.value,5))) as c3, sum(substr(src.value, 5)) as c4, count(src.value) as c6 GROUP BY substr(src.key,1,1), substr(src.key,2,1) ORDER BY c1, c2  LIMIT 10
INSERT OVERWRITE TABLE dest_h3 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1), substr(src.key,2,1);

FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g3 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) < 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g4 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(src.value) GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_h2 SELECT substr(src.key,1,1) as c1, count(DISTINCT substr(src.value,5)) as c2, concat(substr(src.key,1,1),sum(substr(src.value,5))) as c3, sum(substr(src.value, 5)) as c4, count(src.value) as c6 GROUP BY substr(src.key,1,1), substr(src.key,2,1) ORDER BY c1, c2  LIMIT 10
INSERT OVERWRITE TABLE dest_h3 SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(substr(src.value, 5)), count(src.value) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1), substr(src.key,2,1);

SELECT * FROM dest_g2;
SELECT * FROM dest_g3;
SELECT * FROM dest_g4;
SELECT * FROM dest_h2;
SELECT * FROM dest_h3;

DROP TABLE dest_g2;
DROP TABLE dest_g3;
DROP TABLE dest_g4;
DROP TABLE dest_h2;
DROP TABLE dest_h3;
set hive.multigroupby.singlereducer=true;

CREATE TABLE dest_g2(key STRING, c1 INT) STORED AS TEXTFILE;
CREATE TABLE dest_g3(key STRING, c1 INT, c2 INT) STORED AS TEXTFILE;

-- SORT_QUERY_RESULTS

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT src.key) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g3 SELECT substr(src.key,1,1), count(DISTINCT src.key), count(src.value) WHERE substr(src.key,1,1) < 5 GROUP BY substr(src.key,1,1);

FROM src
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT src.key) WHERE substr(src.key,1,1) >= 5 GROUP BY substr(src.key,1,1)
INSERT OVERWRITE TABLE dest_g3 SELECT substr(src.key,1,1), count(DISTINCT src.key), count(src.value) WHERE substr(src.key,1,1) < 5 GROUP BY substr(src.key,1,1);

SELECT * FROM dest_g2;
SELECT * FROM dest_g3;

DROP TABLE dest_g2;
DROP TABLE dest_g3;
-- HIVE-3849 Aliased column in where clause for multi-groupby single reducer cannot be resolved

-- SORT_QUERY_RESULTS

create table e1 (key string, count int);
create table e2 (key string, count int);

explain
from src
insert overwrite table e1
select key, count(*)
where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
group by key
insert overwrite table e2
select key, count(*)
where src.value in ('val_400', 'val_500') AND key in (400, 450)
group by key;

from src
insert overwrite table e1
select key, count(*)
where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
group by key
insert overwrite table e2
select key, count(*)
where src.value in ('val_400', 'val_500') AND key in (400, 450)
group by key;

select * from e1;
select * from e2;

explain
from src
insert overwrite table e1
select value, count(*)
where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
group by value
insert overwrite table e2
select value, count(*)
where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
group by value;

from src
insert overwrite table e1
select value, count(*)
where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
group by value
insert overwrite table e2
select value, count(*)
where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
group by value;

select * from e1;
select * from e2;

set hive.optimize.ppd=false;

explain
from src
insert overwrite table e1
select key, count(*)
where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
group by key
insert overwrite table e2
select key, count(*)
where src.value in ('val_400', 'val_500') AND key in (400, 450)
group by key;

from src
insert overwrite table e1
select key, count(*)
where src.value in ('val_100', 'val_200', 'val_300') AND key in (100, 150, 200)
group by key
insert overwrite table e2
select key, count(*)
where src.value in ('val_400', 'val_500') AND key in (400, 450)
group by key;

select * from e1;
select * from e2;

explain
from src
insert overwrite table e1
select value, count(*)
where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
group by value
insert overwrite table e2
select value, count(*)
where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
group by value;

from src
insert overwrite table e1
select value, count(*)
where src.key + src.key = 200 or src.key - 100 = 100 or src.key = 300 AND VALUE IS NOT NULL
group by value
insert overwrite table e2
select value, count(*)
where src.key + src.key = 400 or src.key - 100 = 500 AND VALUE IS NOT NULL
group by value;

select * from e1;
select * from e2;
FROM src
SELECT cast('-30.33' as DOUBLE)
GROUP BY cast('-30.33' as DOUBLE)
LIMIT 1;


FROM src
SELECT '-30.33'
GROUP BY '-30.33'
LIMIT 1;
set hive.mapred.mode=nonstrict;
set hive.groupby.orderby.position.alias=true;

CREATE TABLE testTable1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE testTable2(key INT, val1 STRING, val2 STRING) STORED AS TEXTFILE;

-- Position Alias in GROUP BY and ORDER BY

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1, 2;

FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1, 2;

SELECT key, value FROM testTable1 ORDER BY 1, 2;
SELECT key, val1, val2 FROM testTable2 ORDER BY 1, 2, 3;

EXPLAIN
FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 2, 1;

FROM SRC
INSERT OVERWRITE TABLE testTable1 SELECT SRC.key, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 1
INSERT OVERWRITE TABLE testTable2 SELECT SRC.key, SRC.value, COUNT(DISTINCT SUBSTR(SRC.value,5)) WHERE SRC.key < 20 GROUP BY 2, 1;

SELECT key, value FROM testTable1 ORDER BY 1, 2;
SELECT key, val1, val2 FROM testTable2 ORDER BY 1, 2, 3;

-- Position Alias in subquery

EXPLAIN
SELECT t.key, t.value
FROM (SELECT b.key as key, count(1) as value FROM src b WHERE b.key <= 20 GROUP BY 1) t
ORDER BY 2 DESC, 1 ASC;

SELECT t.key, t.value
FROM (SELECT b.key as key, count(1) as value FROM src b WHERE b.key <= 20 GROUP BY 1) t
ORDER BY 2 DESC, 1 ASC;

EXPLAIN
SELECT c1, c2, c3, c4
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2, COUNT(DISTINCT SUBSTR(src1.value,5)) AS c3 WHERE src1.key > 10 and src1.key < 20 GROUP BY 1, 2
  ) a
 JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25 GROUP BY 1, 2
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
ORDER BY 1 DESC, 2 DESC, 3 ASC, 4 ASC;

SELECT c1, c2, c3, c4
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2, COUNT(DISTINCT SUBSTR(src1.value,5)) AS c3 WHERE src1.key > 10 and src1.key < 20 GROUP BY 1, 2
  ) a
 JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25 GROUP BY 1, 2
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
ORDER BY 1 DESC, 2 DESC, 3 ASC, 4 ASC;
set hive.mapred.mode=nonstrict;
-- see HIVE-2382
create table invites (id int, foo int, bar int);
explain select * from (select foo, bar from (select bar, foo from invites c union all select bar, foo from invites d) b) a group by bar, foo having bar=1;
drop table invites;set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=false;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;

EXPLAIN EXTENDED
FROM srcpart src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5)))
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1);

FROM srcpart src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5)))
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=false;
set hive.groupby.skewindata=false;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key STRING, c1 INT, c2 STRING, c3 INT, c4 INT) STORED AS TEXTFILE;

EXPLAIN EXTENDED
FROM srcpart src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(DISTINCT src.value)
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1);

FROM srcpart src
INSERT OVERWRITE TABLE dest1
SELECT substr(src.key,1,1), count(DISTINCT substr(src.value,5)), concat(substr(src.key,1,1),sum(substr(src.value,5))), sum(DISTINCT substr(src.value, 5)), count(DISTINCT src.value)
WHERE src.ds = '2008-04-08'
GROUP BY substr(src.key,1,1);

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;


set hive.map.aggr=false;
set hive.groupby.skewindata=false;
explain select key, count(*) from src b group by b.key;
explain select b.key, count(*) from src b group by key;

set hive.map.aggr=false;
set hive.groupby.skewindata=true;
explain select key, count(*) from src b group by b.key;
explain select b.key, count(*) from src b group by key;

set hive.map.aggr=true;
set hive.groupby.skewindata=false;
explain select key, count(*) from src b group by b.key;
explain select b.key, count(*) from src b group by key;

set hive.map.aggr=true;
set hive.groupby.skewindata=true;
explain select key, count(*) from src b group by b.key;
explain select b.key, count(*) from src b group by key;

-- windowing after group by
select key, count(*), rank() over(order by count(*))
from src b
where key < '12'
group by b.key
order by b.key;

-- having after group by
select key, count(*)
from src b
group by b.key
having key < '12'
order by b.key;

-- having and windowing
select key, count(*), rank() over(order by count(*))
from src b
group by b.key
having key < '12'
order by b.key
;

explain
select key, count(*), rank() over(order by count(*))
from src b
group by b.key
having key < '12'
;

-- order by
select key
from src t
where key < '12'
group by t.key
order by t.key;

-- cluster by
EXPLAIN
SELECT x.key, x.value as key FROM SRC x CLUSTER BY key;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

EXPLAIN
SELECT key, val, count(1) FROM T1 GROUP BY key, val with rollup;

SELECT key, val, count(1) FROM T1 GROUP BY key, val with rollup;

EXPLAIN
SELECT key, count(distinct val) FROM T1 GROUP BY key with rollup;

SELECT key, count(distinct val) FROM T1 GROUP BY key with rollup;

set hive.groupby.skewindata=true;

EXPLAIN
SELECT key, val, count(1) FROM T1 GROUP BY key, val with rollup;

SELECT key, val, count(1) FROM T1 GROUP BY key, val with rollup;

EXPLAIN
SELECT key, count(distinct val) FROM T1 GROUP BY key with rollup;

SELECT key, count(distinct val) FROM T1 GROUP BY key with rollup;


set hive.multigroupby.singlereducer=true;

CREATE TABLE T2(key1 STRING, key2 STRING, val INT) STORED AS TEXTFILE;
CREATE TABLE T3(key1 STRING, key2 STRING, val INT) STORED AS TEXTFILE;

EXPLAIN
FROM T1
INSERT OVERWRITE TABLE T2 SELECT key, val, count(1) group by key, val with rollup
INSERT OVERWRITE TABLE T3 SELECT key, val, sum(1) group by key, val with rollup;


FROM T1
INSERT OVERWRITE TABLE T2 SELECT key, val, count(1) group by key, val with rollup
INSERT OVERWRITE TABLE T3 SELECT key, val, sum(1) group by key, val with rollup;

set hive.map.aggr=false;

SELECT key, value, count(1) FROM src GROUP BY key, value with rollup;

set hive.map.aggr=true;

SELECT key, value, count(key) FROM src GROUP BY key, value with rollup;

;

set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)
-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(key int, cnt int);

-- The plan should be converted to a map-side group by if the group by key
-- matches the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl2(key1 int, key2 string, cnt int);

-- no map-side group by even if the group by key is a superset of sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

SELECT * FROM outputTbl2;

-- It should work for sub-queries
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;

SELECT * FROM outputTbl1;

-- It should work for sub-queries with column aliases
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

INSERT OVERWRITE TABLE outputTbl1
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl3(key1 int, key2 int, cnt int);

-- The plan should be converted to a map-side group by if the group by key contains a constant followed
-- by a match to the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl3
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;

INSERT OVERWRITE TABLE outputTbl3
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;

SELECT * FROM outputTbl3;

CREATE TABLE outputTbl4(key1 int, key2 int, key3 string, cnt int);

-- no map-side group by if the group by key contains a constant followed by another column
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;

SELECT * FROM outputTbl4;

-- no map-side group by if the group by key contains a function
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl3
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

INSERT OVERWRITE TABLE outputTbl3
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

SELECT * FROM outputTbl3;

-- it should not matter what follows the group by
-- test various cases

-- group by followed by another group by
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

SELECT * FROM outputTbl1;

-- group by followed by a union
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key, count(1) FROM T1 GROUP BY key
) subq1;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key, count(1) FROM T1 GROUP BY key
) subq1;

SELECT * FROM outputTbl1;

-- group by followed by a union where one of the sub-queries is map-side group by
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
) subq1;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) as cnt FROM T1 GROUP BY key
  UNION ALL
SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
) subq1;

SELECT * FROM outputTbl1;

-- group by followed by a join
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT subq1.key, subq1.cnt+subq2.cnt FROM
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

INSERT OVERWRITE TABLE outputTbl1
SELECT subq1.key, subq1.cnt+subq2.cnt FROM
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

SELECT * FROM outputTbl1;

-- group by followed by a join where one of the sub-queries can be performed in the mapper
EXPLAIN EXTENDED
SELECT * FROM
(SELECT key, count(1) FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, val, count(1) FROM T1 GROUP BY key, val) subq2
ON subq1.key = subq2.key;

CREATE TABLE T2(key STRING, val STRING)
CLUSTERED BY (key, val) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T2 select key, val from T1;

-- no mapside sort group by if the group by is a prefix of the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T2 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T2 GROUP BY key;

SELECT * FROM outputTbl1;

-- The plan should be converted to a map-side group by if the group by key contains a constant in between the
-- sorted keys
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;

SELECT * FROM outputTbl4;

CREATE TABLE outputTbl5(key1 int, key2 int, key3 string, key4 int, cnt int);

-- The plan should be converted to a map-side group by if the group by key contains a constant in between the
-- sorted keys followed by anything
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl5
SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;

INSERT OVERWRITE TABLE outputTbl5
SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;

SELECT * FROM outputTbl5;

-- contants from sub-queries should work fine
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T2)subq
group by key, constant, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T2)subq
group by key, constant, val;

SELECT * FROM outputTbl4;

-- multiple levels of contants from sub-queries should work fine
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
select key, constant3, val, count(1) from
(
SELECT key, constant as constant2, val, 2 as constant3 from
(SELECT key, 1 as constant, val from T2)subq
)subq2
group by key, constant3, val;

INSERT OVERWRITE TABLE outputTbl4
select key, constant3, val, count(1) from
(
SELECT key, constant as constant2, val, 2 as constant3 from
(SELECT key, 1 as constant, val from T2)subq
)subq2
group by key, constant3, val;

SELECT * FROM outputTbl4;

set hive.map.aggr=true;
set hive.multigroupby.singlereducer=false;
set mapred.reduce.tasks=31;

CREATE TABLE DEST1(key INT, cnt INT);
CREATE TABLE DEST2(key INT, val STRING, cnt INT);

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM T2
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

FROM T2
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

select * from DEST1;
select * from DEST2;

-- multi-table insert with a sub-query
EXPLAIN
FROM (select key, val from T2 where key = 8) x
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

FROM (select key, val from T2 where key = 8) x
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

select * from DEST1;
select * from DEST2;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 PARTITION (ds='1')
SELECT * from src where key = 0 or key = 11;

-- The plan is converted to a map-side plan
EXPLAIN select distinct key from T1;
select distinct key from T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 PARTITION (ds='2')
SELECT * from src where key = 0 or key = 11;

-- The plan is not converted to a map-side, since although the sorting columns and grouping
-- columns match, the user is querying multiple input partitions
EXPLAIN select distinct key from T1;
select distinct key from T1;

DROP TABLE T1;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 1;
set hive.map.groupby.sorted=true;

CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 PARTITION (ds='1')
SELECT * from src where key < 10;

-- The plan is optimized to perform partial aggregation on the mapper
EXPLAIN select count(distinct key) from T1;
select count(distinct key) from T1;

-- The plan is optimized to perform partial aggregation on the mapper
EXPLAIN select count(distinct key), count(1), count(key), sum(distinct key) from T1;
select count(distinct key), count(1), count(key), sum(distinct key) from T1;

-- The plan is not changed in the presence of a grouping key
EXPLAIN select count(distinct key), count(1), count(key), sum(distinct key) from T1 group by key;
select count(distinct key), count(1), count(key), sum(distinct key) from T1 group by key;

-- The plan is not changed in the presence of a grouping key
EXPLAIN select key, count(distinct key), count(1), count(key), sum(distinct key) from T1 group by key;
select key, count(distinct key), count(1), count(key), sum(distinct key) from T1 group by key;

-- The plan is not changed in the presence of a grouping key expression
EXPLAIN select count(distinct key+key) from T1;
select count(distinct key+key) from T1;

EXPLAIN select count(distinct 1) from T1;
select count(distinct 1) from T1;

set hive.map.aggr=false;

-- no plan change if map aggr is turned off
EXPLAIN select count(distinct key) from T1;
select count(distinct key) from T1;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)
-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(key int, cnt int);

-- The plan should be converted to a map-side group by if the group by key
-- matches the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl2(key1 int, key2 string, cnt int);

-- no map-side group by even if the group by key is a superset of sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

SELECT * FROM outputTbl2;

-- It should work for sub-queries
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;

SELECT * FROM outputTbl1;

-- It should work for sub-queries with column aliases
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

INSERT OVERWRITE TABLE outputTbl1
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl3(key1 int, key2 int, cnt int);

-- The plan should be converted to a map-side group by if the group by key contains a constant followed
-- by a match to the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl3
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;

INSERT OVERWRITE TABLE outputTbl3
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;

SELECT * FROM outputTbl3;

CREATE TABLE outputTbl4(key1 int, key2 int, key3 string, cnt int);

-- no map-side group by if the group by key contains a constant followed by another column
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;

SELECT * FROM outputTbl4;

-- no map-side group by if the group by key contains a function
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl3
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

INSERT OVERWRITE TABLE outputTbl3
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

SELECT * FROM outputTbl3;

-- it should not matter what follows the group by
-- test various cases

-- group by followed by another group by
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

SELECT * FROM outputTbl1;

-- group by followed by a union
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key, count(1) FROM T1 GROUP BY key
) subq1;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key, count(1) FROM T1 GROUP BY key
) subq1;

SELECT * FROM outputTbl1;

-- group by followed by a union where one of the sub-queries is map-side group by
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
) subq1;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) as cnt FROM T1 GROUP BY key
  UNION ALL
SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
) subq1;

SELECT * FROM outputTbl1;

-- group by followed by a join
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT subq1.key, subq1.cnt+subq2.cnt FROM
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

INSERT OVERWRITE TABLE outputTbl1
SELECT subq1.key, subq1.cnt+subq2.cnt FROM
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

SELECT * FROM outputTbl1;

-- group by followed by a join where one of the sub-queries can be performed in the mapper
EXPLAIN EXTENDED
SELECT * FROM
(SELECT key, count(1) FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, val, count(1) FROM T1 GROUP BY key, val) subq2
ON subq1.key = subq2.key;

CREATE TABLE T2(key STRING, val STRING)
CLUSTERED BY (key, val) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T2 select key, val from T1;

-- no mapside sort group by if the group by is a prefix of the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T2 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T2 GROUP BY key;

SELECT * FROM outputTbl1;

-- The plan should be converted to a map-side group by if the group by key contains a constant in between the
-- sorted keys
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;

SELECT * FROM outputTbl4;

CREATE TABLE outputTbl5(key1 int, key2 int, key3 string, key4 int, cnt int);

-- The plan should be converted to a map-side group by if the group by key contains a constant in between the
-- sorted keys followed by anything
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl5
SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;

INSERT OVERWRITE TABLE outputTbl5
SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;

SELECT * FROM outputTbl5
ORDER BY key1, key2, key3, key4;

-- contants from sub-queries should work fine
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T2)subq
group by key, constant, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T2)subq
group by key, constant, val;

SELECT * FROM outputTbl4;

-- multiple levels of contants from sub-queries should work fine
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
select key, constant3, val, count(1) from
(
SELECT key, constant as constant2, val, 2 as constant3 from
(SELECT key, 1 as constant, val from T2)subq
)subq2
group by key, constant3, val;

INSERT OVERWRITE TABLE outputTbl4
select key, constant3, val, count(1) from
(
SELECT key, constant as constant2, val, 2 as constant3 from
(SELECT key, 1 as constant, val from T2)subq
)subq2
group by key, constant3, val;

SELECT * FROM outputTbl4;

set hive.map.aggr=true;
set hive.multigroupby.singlereducer=false;
set mapred.reduce.tasks=31;

CREATE TABLE DEST1(key INT, cnt INT);
CREATE TABLE DEST2(key INT, val STRING, cnt INT);

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM T2
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

FROM T2
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

select * from DEST1;
select * from DEST2;

-- multi-table insert with a sub-query
EXPLAIN
FROM (select key, val from T2 where key = 8) x
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

FROM (select key, val from T2 where key = 8) x
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

select * from DEST1;
select * from DEST2;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) SORTED BY (val) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(val string, cnt int);

-- The plan should not be converted to a map-side group by even though the group by key
-- matches the sorted key.
EXPLAIN
INSERT OVERWRITE TABLE outputTbl1
SELECT val, count(1) FROM T1 GROUP BY val;

INSERT OVERWRITE TABLE outputTbl1
SELECT val, count(1) FROM T1 GROUP BY val;

SELECT * FROM outputTbl1;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(key string, val string, cnt int);

-- The plan should be converted to a map-side group by
EXPLAIN
INSERT OVERWRITE TABLE outputTbl1
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl2(key string, cnt int);

-- The plan should be converted to a map-side group by
EXPLAIN
INSERT OVERWRITE TABLE outputTbl2
SELECT key, count(1) FROM T1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl2
SELECT key, count(1) FROM T1 GROUP BY key;

SELECT * FROM outputTbl2;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key, val) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(key STRING, cnt INT);

-- The plan should not be converted to a map-side group by.
-- However, there should no hash-based aggregation on the map-side
EXPLAIN
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl2(key STRING, val STRING, cnt INT);

-- The plan should not be converted to a map-side group by.
-- Hash-based aggregations should be performed on the map-side
EXPLAIN
INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

SELECT * FROM outputTbl2;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (val) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(key STRING, val STRING, cnt INT);

-- The plan should be converted to a map-side group by, since the
-- sorting columns and grouping columns match, and all the bucketing columns
-- are part of sorting columns
EXPLAIN
INSERT OVERWRITE TABLE outputTbl1
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

SELECT * FROM outputTbl1;

DROP TABLE T1;

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (val, key) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

-- The plan should be converted to a map-side group by, since the
-- sorting columns and grouping columns match, and all the bucketing columns
-- are part of sorting columns
EXPLAIN
INSERT OVERWRITE TABLE outputTbl1
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

SELECT * FROM outputTbl1;

DROP TABLE T1;

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (val) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl2(key STRING, cnt INT);

-- The plan should not be converted to a map-side group by, since although the
-- sorting columns and grouping columns match, all the bucketing columns
-- are not part of sorting columns. However, no hash map aggregation is required
-- on the mapside.
EXPLAIN
INSERT OVERWRITE TABLE outputTbl2
SELECT key, count(1) FROM T1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl2
SELECT key, count(1) FROM T1 GROUP BY key;

SELECT * FROM outputTbl2;

DROP TABLE T1;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string);

CREATE TABLE outputTbl1(key int, cnt int);

-- The plan should not be converted to a map-side group since no partition is being accessed
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key;

SELECT * FROM outputTbl1;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1 PARTITION (ds='2');

-- The plan should not be converted to a map-side group since no partition is being accessed
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 where ds = '1' GROUP BY key;

SELECT * FROM outputTbl1;

-- The plan should not be converted to a map-side group since the partition being accessed
-- is neither bucketed not sorted
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 where ds = '2' GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 where ds = '2' GROUP BY key;

SELECT * FROM outputTbl1;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string)
CLUSTERED BY (val) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1 PARTITION (ds='1');

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 PARTITION (ds='1') select key, val from T1 where ds = '1';

CREATE TABLE outputTbl1(key STRING, val STRING, cnt INT);

-- The plan should be converted to a map-side group by, since the
-- sorting columns and grouping columns match, and all the bucketing columns
-- are part of sorting columns
EXPLAIN
INSERT OVERWRITE TABLE outputTbl1
SELECT key, val, count(1) FROM T1 where ds = '1' GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, val, count(1) FROM T1 where ds = '1' GROUP BY key, val;

SELECT * FROM outputTbl1;

DROP TABLE T1;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1 PARTITION (ds='1');

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 PARTITION (ds='1') select key, val from T1 where ds = '1';

-- The plan is not converted to a map-side, since although the sorting columns and grouping
-- columns match, the user is issueing a distinct.
-- However, after HIVE-4310, partial aggregation is performed on the mapper
EXPLAIN
select count(distinct key) from T1;
select count(distinct key) from T1;

DROP TABLE T1;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

CREATE TABLE T1(key STRING, val STRING) PARTITIONED BY (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1 PARTITION (ds='1');

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 PARTITION (ds='1') select key, val from T1 where ds = '1';
INSERT OVERWRITE TABLE T1 PARTITION (ds='2') select key, val from T1 where ds = '1';

-- The plan is not converted to a map-side, since although the sorting columns and grouping
-- columns match, the user is querying multiple input partitions
EXPLAIN
select key, count(1) from T1 group by key;
select key, count(1) from T1 group by key;

DROP TABLE T1;
;

set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;
set hive.groupby.skewindata=true;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)
-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(key int, cnt int);

-- The plan should be converted to a map-side group by if the group by key
-- matches the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl2(key1 int, key2 string, cnt int);

-- no map-side group by even if the group by key is a superset of sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

SELECT * FROM outputTbl2;

-- It should work for sub-queries
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;

SELECT * FROM outputTbl1;

-- It should work for sub-queries with column aliases
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

INSERT OVERWRITE TABLE outputTbl1
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl3(key1 int, key2 int, cnt int);

-- The plan should be converted to a map-side group by if the group by key contains a constant followed
-- by a match to the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl3
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;

INSERT OVERWRITE TABLE outputTbl3
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;

SELECT * FROM outputTbl3;

CREATE TABLE outputTbl4(key1 int, key2 int, key3 string, cnt int);

-- no map-side group by if the group by key contains a constant followed by another column
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;

SELECT * FROM outputTbl4;

-- no map-side group by if the group by key contains a function
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl3
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

INSERT OVERWRITE TABLE outputTbl3
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

SELECT * FROM outputTbl3;

-- it should not matter what follows the group by
-- test various cases

-- group by followed by another group by
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

SELECT * FROM outputTbl1;

-- group by followed by a union
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key, count(1) FROM T1 GROUP BY key
) subq1;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key, count(1) FROM T1 GROUP BY key
) subq1;

SELECT * FROM outputTbl1;

-- group by followed by a union where one of the sub-queries is map-side group by
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
) subq1;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) as cnt FROM T1 GROUP BY key
  UNION ALL
SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
) subq1;

SELECT * FROM outputTbl1;

-- group by followed by a join
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT subq1.key, subq1.cnt+subq2.cnt FROM
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

INSERT OVERWRITE TABLE outputTbl1
SELECT subq1.key, subq1.cnt+subq2.cnt FROM
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

SELECT * FROM outputTbl1;

-- group by followed by a join where one of the sub-queries can be performed in the mapper
EXPLAIN EXTENDED
SELECT * FROM
(SELECT key, count(1) FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, val, count(1) FROM T1 GROUP BY key, val) subq2
ON subq1.key = subq2.key;

CREATE TABLE T2(key STRING, val STRING)
CLUSTERED BY (key, val) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T2 select key, val from T1;

-- no mapside sort group by if the group by is a prefix of the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T2 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T2 GROUP BY key;

SELECT * FROM outputTbl1;

-- The plan should be converted to a map-side group by if the group by key contains a constant in between the
-- sorted keys
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;

SELECT * FROM outputTbl4;

CREATE TABLE outputTbl5(key1 int, key2 int, key3 string, key4 int, cnt int);

-- The plan should be converted to a map-side group by if the group by key contains a constant in between the
-- sorted keys followed by anything
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl5
SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;

INSERT OVERWRITE TABLE outputTbl5
SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;

SELECT * FROM outputTbl5
ORDER BY key1, key2, key3, key4;

-- contants from sub-queries should work fine
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T2)subq
group by key, constant, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T2)subq
group by key, constant, val;

SELECT * FROM outputTbl4;

-- multiple levels of contants from sub-queries should work fine
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
select key, constant3, val, count(1) from
(
SELECT key, constant as constant2, val, 2 as constant3 from
(SELECT key, 1 as constant, val from T2)subq
)subq2
group by key, constant3, val;

INSERT OVERWRITE TABLE outputTbl4
select key, constant3, val, count(1) from
(
SELECT key, constant as constant2, val, 2 as constant3 from
(SELECT key, 1 as constant, val from T2)subq
)subq2
group by key, constant3, val;

SELECT * FROM outputTbl4;

set hive.map.aggr=true;
set hive.multigroupby.singlereducer=false;
set mapred.reduce.tasks=31;

CREATE TABLE DEST1(key INT, cnt INT);
CREATE TABLE DEST2(key INT, val STRING, cnt INT);

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM T2
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

FROM T2
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

select * from DEST1;
select * from DEST2;

-- multi-table insert with a sub-query
EXPLAIN
FROM (select key, val from T2 where key = 8) x
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

FROM (select key, val from T2 where key = 8) x
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

select * from DEST1;
select * from DEST2;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;
set hive.groupby.skewindata=true;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)
-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(key int, cnt int);

-- The plan should be converted to a map-side group by if the group by key
-- matches the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl2(key1 int, key2 string, cnt int);

-- no map-side group by even if the group by key is a superset of sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

INSERT OVERWRITE TABLE outputTbl2
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

SELECT * FROM outputTbl2;

-- It should work for sub-queries
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;

SELECT * FROM outputTbl1;

-- It should work for sub-queries with column aliases
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

INSERT OVERWRITE TABLE outputTbl1
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

SELECT * FROM outputTbl1;

CREATE TABLE outputTbl3(key1 int, key2 int, cnt int);

-- The plan should be converted to a map-side group by if the group by key contains a constant followed
-- by a match to the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl3
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;

INSERT OVERWRITE TABLE outputTbl3
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;

SELECT * FROM outputTbl3;

CREATE TABLE outputTbl4(key1 int, key2 int, key3 string, cnt int);

-- no map-side group by if the group by key contains a constant followed by another column
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;

SELECT * FROM outputTbl4;

-- no map-side group by if the group by key contains a function
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl3
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

INSERT OVERWRITE TABLE outputTbl3
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

SELECT * FROM outputTbl3;

-- it should not matter what follows the group by
-- test various cases

-- group by followed by another group by
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

SELECT * FROM outputTbl1;

-- group by followed by a union
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key, count(1) FROM T1 GROUP BY key
) subq1;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key, count(1) FROM T1 GROUP BY key
) subq1;

SELECT * FROM outputTbl1;

-- group by followed by a union where one of the sub-queries is map-side group by
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) FROM T1 GROUP BY key
  UNION ALL
SELECT key + key as key, count(1) FROM T1 GROUP BY key + key
) subq1;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM (
SELECT key, count(1) as cnt FROM T1 GROUP BY key
  UNION ALL
SELECT key + key as key, count(1) as cnt FROM T1 GROUP BY key + key
) subq1;

SELECT * FROM outputTbl1;

-- group by followed by a join
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT subq1.key, subq1.cnt+subq2.cnt FROM
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

INSERT OVERWRITE TABLE outputTbl1
SELECT subq1.key, subq1.cnt+subq2.cnt FROM
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

SELECT * FROM outputTbl1;

-- group by followed by a join where one of the sub-queries can be performed in the mapper
EXPLAIN EXTENDED
SELECT * FROM
(SELECT key, count(1) FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, val, count(1) FROM T1 GROUP BY key, val) subq2
ON subq1.key = subq2.key;

CREATE TABLE T2(key STRING, val STRING)
CLUSTERED BY (key, val) SORTED BY (key, val) INTO 2 BUCKETS STORED AS TEXTFILE;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T2 select key, val from T1;

-- no mapside sort group by if the group by is a prefix of the sorted key
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T2 GROUP BY key;

INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T2 GROUP BY key;

SELECT * FROM outputTbl1;

-- The plan should be converted to a map-side group by if the group by key contains a constant in between the
-- sorted keys
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, 1, val, count(1) FROM T2 GROUP BY key, 1, val;

SELECT * FROM outputTbl4;

CREATE TABLE outputTbl5(key1 int, key2 int, key3 string, key4 int, cnt int);

-- The plan should be converted to a map-side group by if the group by key contains a constant in between the
-- sorted keys followed by anything
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl5
SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;

INSERT OVERWRITE TABLE outputTbl5
SELECT key, 1, val, 2, count(1) FROM T2 GROUP BY key, 1, val, 2;

SELECT * FROM outputTbl5
ORDER BY key1, key2, key3, key4;

-- contants from sub-queries should work fine
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T2)subq
group by key, constant, val;

INSERT OVERWRITE TABLE outputTbl4
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T2)subq
group by key, constant, val;

SELECT * FROM outputTbl4;

-- multiple levels of contants from sub-queries should work fine
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE outputTbl4
select key, constant3, val, count(1) from
(
SELECT key, constant as constant2, val, 2 as constant3 from
(SELECT key, 1 as constant, val from T2)subq
)subq2
group by key, constant3, val;

INSERT OVERWRITE TABLE outputTbl4
select key, constant3, val, count(1) from
(
SELECT key, constant as constant2, val, 2 as constant3 from
(SELECT key, 1 as constant, val from T2)subq
)subq2
group by key, constant3, val;

SELECT * FROM outputTbl4;

set hive.map.aggr=true;
set hive.multigroupby.singlereducer=false;
set mapred.reduce.tasks=31;

CREATE TABLE DEST1(key INT, cnt INT);
CREATE TABLE DEST2(key INT, val STRING, cnt INT);

SET hive.exec.compress.intermediate=true;
SET hive.exec.compress.output=true;

EXPLAIN
FROM T2
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

FROM T2
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

select * from DEST1;
select * from DEST2;

-- multi-table insert with a sub-query
EXPLAIN
FROM (select key, val from T2 where key = 8) x
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

FROM (select key, val from T2 where key = 8) x
INSERT OVERWRITE TABLE DEST1 SELECT key, count(1) GROUP BY key
INSERT OVERWRITE TABLE DEST2 SELECT key, val, count(1) GROUP BY key, val;

select * from DEST1;
select * from DEST2;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 10;
set hive.map.groupby.sorted=true;

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

-- perform an insert to make sure there are 2 files
INSERT OVERWRITE TABLE T1 select key, val from T1;

CREATE TABLE outputTbl1(key int, cnt int);

-- The plan should be converted to a map-side group by if the group by key
-- matches the sorted key. However, in test mode, the group by wont be converted.
EXPLAIN
INSERT OVERWRITE TABLE outputTbl1
SELECT key, count(1) FROM T1 GROUP BY key;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS
EXPLAIN SELECT count(value) AS c FROM src GROUP BY key HAVING c > 3;
SELECT count(value) AS c FROM src GROUP BY key HAVING c > 3;

EXPLAIN SELECT key, max(value) AS c FROM src GROUP BY key HAVING key != 302;
SELECT key, max(value) AS c FROM src GROUP BY key HAVING key != 302;

EXPLAIN SELECT key FROM src GROUP BY key HAVING max(value) > "val_255";
SELECT key FROM src GROUP BY key HAVING max(value) > "val_255";

EXPLAIN SELECT key FROM src where key > 300 GROUP BY key HAVING max(value) > "val_255";
SELECT key FROM src where key > 300 GROUP BY key HAVING max(value) > "val_255";

EXPLAIN SELECT key, max(value) FROM src GROUP BY key HAVING max(value) > "val_255";
SELECT key, max(value) FROM src GROUP BY key HAVING max(value) > "val_255";

EXPLAIN SELECT key, COUNT(value) FROM src GROUP BY key HAVING count(value) >= 4;
SELECT key, COUNT(value) FROM src GROUP BY key HAVING count(value) >= 4;EXPLAIN SELECT * FROM src HAVING key > 300;
SELECT * FROM src HAVING key > 300;
set hive.mapred.mode=nonstrict;

CREATE TABLE TestV1_Staples (
      Item_Count INT,
      Ship_Priority STRING,
      Order_Priority STRING,
      Order_Status STRING,
      Order_Quantity DOUBLE,
      Sales_Total DOUBLE,
      Discount DOUBLE,
      Tax_Rate DOUBLE,
      Ship_Mode STRING,
      Fill_Time DOUBLE,
      Gross_Profit DOUBLE,
      Price DOUBLE,
      Ship_Handle_Cost DOUBLE,
      Employee_Name STRING,
      Employee_Dept STRING,
      Manager_Name STRING,
      Employee_Yrs_Exp DOUBLE,
      Employee_Salary DOUBLE,
      Customer_Name STRING,
      Customer_State STRING,
      Call_Center_Region STRING,
      Customer_Balance DOUBLE,
      Customer_Segment STRING,
      Prod_Type1 STRING,
      Prod_Type2 STRING,
      Prod_Type3 STRING,
      Prod_Type4 STRING,
      Product_Name STRING,
      Product_Container STRING,
      Ship_Promo STRING,
      Supplier_Name STRING,
      Supplier_Balance DOUBLE,
      Supplier_Region STRING,
      Supplier_State STRING,
      Order_ID STRING,
      Order_Year INT,
      Order_Month INT,
      Order_Day INT,
      Order_Date_ STRING,
      Order_Quarter STRING,
      Product_Base_Margin DOUBLE,
      Product_ID STRING,
      Receive_Time DOUBLE,
      Received_Date_ STRING,
      Ship_Date_ STRING,
      Ship_Charge DOUBLE,
      Total_Cycle_Time DOUBLE,
      Product_In_Stock STRING,
      PID INT,
      Market_Segment STRING
      );

explain
SELECT customer_name, SUM(customer_balance), SUM(order_quantity) FROM default.testv1_staples s1 GROUP BY customer_name HAVING (
(COUNT(s1.discount) <= 822) AND
(SUM(customer_balance) <= 4074689.000000041)
);

explain
SELECT customer_name, SUM(customer_balance), SUM(order_quantity) FROM default.testv1_staples s1 GROUP BY customer_name HAVING (
(SUM(customer_balance) <= 4074689.000000041)
AND (COUNT(s1.discount) <= 822)
);

explain
SELECT s1.customer_name FROM default.testv1_staples s1 join default.src s2 on s1.customer_name = s2.key
GROUP BY s1.customer_name
HAVING (
(SUM(s1.customer_balance) <= 4074689.000000041)
AND (AVG(s1.discount) <= 822)
AND (COUNT(s2.value) > 4)
);

explain
SELECT s1.customer_name FROM default.testv1_staples s1 join default.src s2 on s1.customer_name = s2.key
GROUP BY s1.customer_name, s1.customer_name
HAVING (
(SUM(s1.customer_balance) <= 4074689.000000041)
AND (AVG(s1.discount) <= 822)
AND (COUNT(s2.value) > 4)
);

explain
SELECT distinct s1.customer_name as x, s1.customer_name as y
FROM default.testv1_staples s1 join default.src s2 on s1.customer_name = s2.key
HAVING (
(SUM(s1.customer_balance) <= 4074689.000000041)
AND (AVG(s1.discount) <= 822)
AND (COUNT(s2.value) > 4)
);
drop table if exists testhbaseb;
CREATE TABLE testhbaseb (key int, val binary)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = ":key,cf:val#b"
);
insert into table testhbaseb values(1, 'hello');
insert into table testhbaseb values(2, 'hi');
select * from testhbaseb;
drop table testhbaseb;


DROP TABLE t_ext_hbase_1;

CREATE EXTERNAL TABLE t_ext_hbase_1
(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
 c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable");

SELECT * FROM t_ext_hbase_1;

DROP TABLE t_ext_hbase_1;
DROP TABLE t_ext_hbase_2;

CREATE EXTERNAL TABLE t_ext_hbase_2
(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
 c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:cq-boolean#b,cf:cq-byte#b,cf:cq-short#b,cf:cq-int#b,cf:cq-long#b,cf:cq-string#b,cf:cq-float#b,cf:cq-double#b")
TBLPROPERTIES ("hbase.table.name" = "HiveExternalTable");

SELECT * FROM t_ext_hbase_2;

DROP TABLE t_ext_hbase_2;
DROP TABLE t_ext_hbase_3;

CREATE EXTERNAL TABLE t_ext_hbase_3
(key STRING, c_bool BOOLEAN, c_byte TINYINT, c_short SMALLINT,
 c_int INT, c_long BIGINT, c_string STRING, c_float FLOAT, c_double DOUBLE)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:cq-boolean,cf:cq-byte,cf:cq-short,cf:cq-int,cf:cq-long,cf:cq-string,cf:cq-float,cf:cq-double")
TBLPROPERTIES (
"hbase.table.name" = "HiveExternalTable",
"hbase.table.default.storage.type" = "binary");

SELECT * from t_ext_hbase_3;

--HIVE-2958
SELECT c_int, count(*) FROM t_ext_hbase_3 GROUP BY c_int;

DROP table t_ext_hbase_3;
DROP TABLE hbase_src;

CREATE TABLE hbase_src(key STRING,
                       tinyint_col TINYINT,
                       smallint_col SMALLINT,
                       int_col INT,
                       bigint_col BIGINT,
                       float_col FLOAT,
                       double_col DOUBLE,
                       string_col STRING);

INSERT OVERWRITE TABLE hbase_src
  SELECT key, key, key, key, key, key, key, value
  FROM src
  WHERE key = 125 OR key = 126 OR key = 127;

DROP TABLE t_hbase_maps;

CREATE TABLE t_hbase_maps(key STRING,
                          tinyint_map_col MAP<TINYINT, TINYINT>,
                          smallint_map_col MAP<SMALLINT, SMALLINT>,
                          int_map_col MAP<INT, INT>,
                          bigint_map_col MAP<BIGINT, BIGINT>,
                          float_map_col MAP<FLOAT, FLOAT>,
                          double_map_col MAP<DOUBLE, DOUBLE>,
                          string_map_col MAP<STRING, STRING>,
                          boolean_map_col MAP<BOOLEAN, BOOLEAN>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps");

INSERT OVERWRITE TABLE t_hbase_maps
  SELECT key,
         map(tinyint_col, tinyint_col),
         map(smallint_col, smallint_col),
         map(int_col, int_col),
         map(bigint_col, bigint_col),
         map(float_col, float_col),
         map(double_col, double_col),
         map(key, string_col),
         map(true, true)
  FROM hbase_src
  WHERE key = 125;

INSERT OVERWRITE TABLE t_hbase_maps
  SELECT key,
         map(tinyint_col, tinyint_col),
         map(smallint_col, smallint_col),
         map(int_col, int_col),
         map(bigint_col, bigint_col),
         map(float_col, float_col),
         map(double_col, double_col),
         map(key, string_col),
         map(false, false)
  FROM hbase_src
  WHERE key = 126;

SELECT * FROM t_hbase_maps ORDER BY key;

DROP TABLE t_ext_hbase_maps;

CREATE EXTERNAL TABLE t_ext_hbase_maps(key STRING,
                                       tinyint_map_col MAP<TINYINT, TINYINT>,
                                       smallint_map_col MAP<SMALLINT, SMALLINT>,
                                       int_map_col MAP<INT, INT>,
                                       bigint_map_col MAP<BIGINT, BIGINT>,
                                       float_map_col MAP<FLOAT, FLOAT>,
                                       double_map_col MAP<DOUBLE, DOUBLE>,
                                       string_map_col MAP<STRING, STRING>,
                                       boolean_map_col MAP<BOOLEAN, BOOLEAN>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps");

SELECT * FROM t_ext_hbase_maps ORDER BY key;

DROP TABLE t_ext_hbase_maps;

DROP TABLE t_ext_hbase_maps_1;

CREATE EXTERNAL TABLE t_ext_hbase_maps_1(key STRING,
                                         tinyint_map_col MAP<TINYINT, TINYINT>,
                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
                                         int_map_col MAP<INT, INT>,
                                         bigint_map_col MAP<BIGINT, BIGINT>,
                                         float_map_col MAP<FLOAT, FLOAT>,
                                         double_map_col MAP<DOUBLE, DOUBLE>,
                                         string_map_col MAP<STRING, STRING>,
                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps");

SELECT * FROM t_ext_hbase_maps_1 ORDER BY key;

DROP TABLE t_ext_hbase_maps_1;

DROP TABLE t_ext_hbase_maps_2;

CREATE EXTERNAL TABLE t_ext_hbase_maps_2(key STRING,
                                         tinyint_map_col MAP<TINYINT, TINYINT>,
                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
                                         int_map_col MAP<INT, INT>,
                                         bigint_map_col MAP<BIGINT, BIGINT>,
                                         float_map_col MAP<FLOAT, FLOAT>,
                                         double_map_col MAP<DOUBLE, DOUBLE>,
                                         string_map_col MAP<STRING, STRING>,
                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
TBLPROPERTIES (
"hbase.table.name"="t_hive_maps",
"hbase.table.default.storage.type"="binary");

SELECT * FROM t_ext_hbase_maps_2 ORDER BY key;

DROP TABLE t_ext_hbase_maps_2;

DROP TABLE t_hbase_maps_1;

CREATE TABLE t_hbase_maps_1(key STRING,
                            tinyint_map_col MAP<TINYINT, TINYINT>,
                            smallint_map_col MAP<SMALLINT, SMALLINT>,
                            int_map_col MAP<INT, INT>,
                            bigint_map_col MAP<BIGINT, BIGINT>,
                            float_map_col MAP<FLOAT, FLOAT>,
                            double_map_col MAP<DOUBLE, DOUBLE>,
                            string_map_col MAP<STRING, STRING>,
                            boolean_map_col MAP<BOOLEAN, BOOLEAN>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#b:b,cf-smallint:#b:b,cf-int:#b:b,cf-bigint:#b:b,cf-float:#b:b,cf-double:#b:b,cf-string:#b:b,cf-boolean:#b:b")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");

INSERT OVERWRITE TABLE t_hbase_maps_1
  SELECT key,
         map(tinyint_col, tinyint_col),
         map(smallint_col, smallint_col),
         map(int_col, int_col),
         map(bigint_col, bigint_col),
         map(float_col, float_col),
         map(double_col, double_col),
         map(key, string_col),
         map(true, true)
  FROM hbase_src
  WHERE key = 125;

INSERT OVERWRITE TABLE t_hbase_maps_1
  SELECT key,
         map(tinyint_col, tinyint_col),
         map(smallint_col, smallint_col),
         map(int_col, int_col),
         map(bigint_col, bigint_col),
         map(float_col, float_col),
         map(double_col, double_col),
         map(key, string_col),
         map(false, false)
  FROM hbase_src
  WHERE key = 126;

SELECT * FROM t_hbase_maps_1 ORDER BY key;

DROP TABLE t_ext_hbase_maps_3;

CREATE EXTERNAL TABLE t_ext_hbase_maps_3(key STRING,
                                         tinyint_map_col MAP<TINYINT, TINYINT>,
                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
                                         int_map_col MAP<INT, INT>,
                                         bigint_map_col MAP<BIGINT, BIGINT>,
                                         float_map_col MAP<FLOAT, FLOAT>,
                                         double_map_col MAP<DOUBLE, DOUBLE>,
                                         string_map_col MAP<STRING, STRING>,
                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key#b,cf-tinyint:#bi:bi,cf-smallint:#bin:bin,cf-int:#bina:bina,cf-bigint:#binar:binar,cf-float:#binary:binary,cf-double:#b:b,cf-string:#bi:bi,cf-boolean:#bin:bin")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");

SELECT * FROM t_ext_hbase_maps_3 ORDER BY key;

DROP TABLE t_ext_hbase_maps_3;

DROP TABLE t_ext_hbase_maps_4;

CREATE EXTERNAL TABLE t_ext_hbase_maps_4(key STRING,
                                         tinyint_map_col MAP<TINYINT, TINYINT>,
                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
                                         int_map_col MAP<INT, INT>,
                                         bigint_map_col MAP<BIGINT, BIGINT>,
                                         float_map_col MAP<FLOAT, FLOAT>,
                                         double_map_col MAP<DOUBLE, DOUBLE>,
                                         string_map_col MAP<STRING, STRING>,
                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps_1");

SELECT * FROM t_ext_hbase_maps_4 ORDER BY key;

DROP TABLE t_ext_hbase_maps_4;

DROP TABLE t_ext_hbase_maps_5;

CREATE EXTERNAL TABLE t_ext_hbase_maps_5(key STRING,
                                         tinyint_map_col MAP<TINYINT, TINYINT>,
                                         smallint_map_col MAP<SMALLINT, SMALLINT>,
                                         int_map_col MAP<INT, INT>,
                                         bigint_map_col MAP<BIGINT, BIGINT>,
                                         float_map_col MAP<FLOAT, FLOAT>,
                                         double_map_col MAP<DOUBLE, DOUBLE>,
                                         string_map_col MAP<STRING, STRING>,
                                         boolean_map_col MAP<BOOLEAN, BOOLEAN>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-tinyint:,cf-smallint:,cf-int:,cf-bigint:,cf-float:,cf-double:,cf-string:,cf-boolean:")
TBLPROPERTIES (
"hbase.table.name"="t_hive_maps_1",
"hbase.table.default.storage.type"="binary");

SELECT * FROM t_ext_hbase_maps_5 ORDER BY key;

DROP TABLE t_ext_hbase_maps_5;

DROP TABLE t_hbase_maps_1;

DROP TABLE t_hbase_maps;

DROP TABLE hbase_src;
DROP TABLE hbase_src;

CREATE TABLE hbase_src(key STRING,
                       tinyint_col TINYINT,
                       smallint_col SMALLINT,
                       int_col INT,
                       bigint_col BIGINT,
                       float_col FLOAT,
                       double_col DOUBLE,
                       string_col STRING);

INSERT OVERWRITE TABLE hbase_src
  SELECT key, key, key, key, key, key, key, value
  FROM src
  WHERE key = 125 OR key = 126 OR key = 127;

DROP TABLE t_hbase_maps;

CREATE TABLE t_hbase_maps(key STRING,
                          string_map_col MAP<STRING, STRING>,
                          simple_string_col STRING)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-string:,cf-string:simple_string_col")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps");

INSERT OVERWRITE TABLE t_hbase_maps
  SELECT key,
         map("string_col", string_col),
         string_col
  FROM hbase_src
  WHERE key = 125;

INSERT OVERWRITE TABLE t_hbase_maps
  SELECT key,
         map("string_col", string_col),
         string_col
  FROM hbase_src
  WHERE key = 126;

SELECT * FROM t_hbase_maps ORDER BY key;

DROP TABLE t_ext_hbase_maps;

CREATE EXTERNAL TABLE t_ext_hbase_maps(key STRING,
                                       string_map_cols MAP<STRING, STRING>, simple_string_col STRING)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-string:string_col.*,cf-string:simple_string_col")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps");

SELECT * FROM t_ext_hbase_maps ORDER BY key;

DROP TABLE t_ext_hbase_maps;

DROP TABLE t_ext_hbase_maps_cut_prefix;

CREATE EXTERNAL TABLE t_ext_hbase_maps_cut_prefix(key STRING,
                                       string_map_cols MAP<STRING, STRING>, simple_string_col STRING)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping"=":key,cf-string:string_.*,cf-string:simple_string_col"
    ,"hbase.columns.mapping.prefix.hide"="true")
TBLPROPERTIES ("hbase.table.name"="t_hive_maps");

SELECT * FROM t_ext_hbase_maps_cut_prefix ORDER BY key;

DROP TABLE t_ext_hbase_maps_cut_prefix;
DROP TABLE t_hbase;

CREATE TABLE t_hbase(key STRING,
                     tinyint_col TINYINT,
                     smallint_col SMALLINT,
                     int_col INT,
                     bigint_col BIGINT,
                     float_col FLOAT,
                     double_col DOUBLE,
                     boolean_col BOOLEAN)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
TBLPROPERTIES ("hbase.table.name" = "t_hive",
               "hbase.table.default.storage.type" = "binary");

DESCRIBE FORMATTED t_hbase;

INSERT OVERWRITE TABLE t_hbase
SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
FROM src
WHERE key=100 OR key=125 OR key=126;

INSERT OVERWRITE TABLE t_hbase
SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
FROM src
WHERE key=100 OR key=125 OR key=126;

INSERT OVERWRITE TABLE t_hbase
SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
FROM src
WHERE key=100 OR key=125 OR key=126;

SELECT * FROM t_hbase;

SELECT tinyint_col,
       smallint_col,
       int_col,
       bigint_col,
       float_col,
       double_col,
       boolean_col
FROM t_hbase
WHERE key='user1' OR key='user2' OR key='user3';

SELECT sum(tinyint_col),
       sum(smallint_col),
       sum(int_col),
       sum(bigint_col),
       sum(float_col),
       sum(double_col),
       count(boolean_col)
FROM t_hbase;

DROP TABLE t_hbase_1;

CREATE EXTERNAL TABLE t_hbase_1(key STRING,
                                tinyint_col TINYINT,
                                smallint_col SMALLINT,
                                int_col INT,
                                bigint_col BIGINT,
                                float_col FLOAT,
                                double_col DOUBLE,
                                boolean_col BOOLEAN)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
TBLPROPERTIES ("hbase.table.name" = "t_hive");

DESCRIBE FORMATTED t_hbase_1;

SELECT * FROM t_hbase_1;

SELECT tinyint_col,
       smallint_col,
       int_col,
       bigint_col,
       float_col,
       double_col,
       boolean_col
FROM t_hbase_1
WHERE key='user1' OR key='user2' OR key='user3';

SELECT sum(tinyint_col),
       sum(smallint_col),
       sum(int_col),
       sum(bigint_col),
       sum(float_col),
       sum(double_col),
       count(boolean_col)
FROM t_hbase_1;

DROP TABLE t_hbase_1;
DROP TABLE t_hbase;
DROP TABLE t_hbase_2;

CREATE TABLE t_hbase_2(key STRING,
                     tinyint_col TINYINT,
                     smallint_col SMALLINT,
                     int_col INT,
                     bigint_col BIGINT,
                     float_col FLOAT,
                     double_col DOUBLE,
                     boolean_col BOOLEAN)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
TBLPROPERTIES ("hbase.table.name" = "t_hive_2");

INSERT OVERWRITE TABLE t_hbase_2
SELECT 'user1', 1, 1, 1, 1, 1.0, 1.0, true
FROM src
WHERE key=100 OR key=125 OR key=126;

INSERT OVERWRITE TABLE t_hbase_2
SELECT 'user2', 127, 32767, 2147483647, 9223372036854775807, 211.31, 268746532.0571, false
FROM src
WHERE key=100 OR key=125 OR key=126;

INSERT OVERWRITE TABLE t_hbase_2
SELECT 'user3', -128, -32768, -2147483648, -9223372036854775808, -201.17, -2110789.37145, true
FROM src
WHERE key=100 OR key=125 OR key=126;

SELECT * FROM t_hbase_2;

SELECT tinyint_col,
       smallint_col,
       int_col,
       bigint_col,
       float_col,
       double_col,
       boolean_col
FROM t_hbase_2
WHERE key='user1' OR key='user2' OR key='user3';

SELECT sum(tinyint_col),
       sum(smallint_col),
       sum(int_col),
       sum(bigint_col),
       sum(float_col),
       sum(double_col),
       count(boolean_col)
FROM t_hbase_2;

DROP TABLE t_hbase_3;

CREATE EXTERNAL TABLE t_hbase_3(key STRING,
                                tinyint_col TINYINT,
                                smallint_col SMALLINT,
                                int_col INT,
                                bigint_col BIGINT,
                                float_col FLOAT,
                                double_col DOUBLE,
                                boolean_col BOOLEAN)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#b,cf:binarybyte#b,cf:binaryshort#b,cf:binaryint#b,cf:binarylong#b,cf:binaryfloat#b,cf:binarydouble#b,cf:binaryboolean#b")
TBLPROPERTIES ("hbase.table.name" = "t_hive_2");

SELECT * FROM t_hbase_3;

SELECT tinyint_col,
       smallint_col,
       int_col,
       bigint_col,
       float_col,
       double_col,
       boolean_col
FROM t_hbase_3
WHERE key='user1' OR key='user2' OR key='user3';

SELECT sum(tinyint_col),
       sum(smallint_col),
       sum(int_col),
       sum(bigint_col),
       sum(float_col),
       sum(double_col),
       count(boolean_col)
FROM t_hbase_3;

DROP TABLE t_hbase_3;

DROP TABLE t_hbase_4;

CREATE EXTERNAL TABLE t_hbase_4(key STRING,
                     tinyint_col TINYINT,
                     smallint_col SMALLINT,
                     int_col INT,
                     bigint_col BIGINT,
                     float_col FLOAT,
                     double_col DOUBLE,
                     boolean_col BOOLEAN)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#-,cf:binarybyte#-,cf:binaryshort#-,cf:binaryint#-,cf:binarylong#-,cf:binaryfloat#-,cf:binarydouble#-,cf:binaryboolean#-")
TBLPROPERTIES (
"hbase.table.name" = "t_hive_2",
"hbase.table.default.storage.type" = "binary");

SELECT * FROM t_hbase_4;

SELECT tinyint_col,
       smallint_col,
       int_col,
       bigint_col,
       float_col,
       double_col,
       boolean_col
FROM t_hbase_4
WHERE key='user1' OR key='user2' OR key='user3';

SELECT sum(tinyint_col),
       sum(smallint_col),
       sum(int_col),
       sum(bigint_col),
       sum(float_col),
       sum(double_col),
       count(boolean_col)
FROM t_hbase_4;

DROP TABLE t_hbase_4;
DROP TABLE t_hbase_2;
CREATE TABLE hbase_ck_1(key struct<col1:string,col2:string,col3:string>, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
    "hbase.table.name" = "hbase_custom",
    "hbase.columns.mapping" = ":key,cf:string",
    "hbase.composite.key.factory"="org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory");

CREATE EXTERNAL TABLE hbase_ck_2(key string, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
    "hbase.table.name" = "hbase_custom",
    "hbase.columns.mapping" = ":key,cf:string");

from src tablesample (1 rows)
insert into table hbase_ck_1 select struct('1000','2000','3000'),'value';

select * from hbase_ck_1;
select * from hbase_ck_2;
CREATE TABLE hbase_ck_4(key struct<col1:string,col2:string,col3:string>, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
    "hbase.table.name" = "hbase_custom2",
    "hbase.columns.mapping" = ":key,cf:string",
    "hbase.composite.key.factory"="org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory2");

from src tablesample (5 rows)
insert into table hbase_ck_4 select
struct(
  cast(key as string),
  cast(cast(key + 1000 as int) as string),
  cast(cast(key + 2000 as int) as string)),
value;

set hive.fetch.task.conversion=more;

-- 165,238,27,311,86
select * from hbase_ck_4;

-- 238
explain
select * from hbase_ck_4 where key.col1 = '238' AND key.col2 = '1238';
select * from hbase_ck_4 where key.col1 = '238' AND key.col2 = '1238';

-- 165,238
explain
select * from hbase_ck_4 where key.col1 >= '165' AND key.col1 < '27';
select * from hbase_ck_4 where key.col1 >= '165' AND key.col1 < '27';

-- 238,311
explain
select * from hbase_ck_4 where key.col1 > '100' AND key.col2 >= '1238';
select * from hbase_ck_4 where key.col1 > '100' AND key.col2 >= '1238';

CREATE TABLE hbase_ck_5(key struct<col1:string,col2:string,col3:string>, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
    "hbase.table.name" = "hbase_custom3",
    "hbase.columns.mapping" = ":key,cf:string",
    "hbase.composite.key.factory"="org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory3");

from src tablesample (5 rows)
insert into table hbase_ck_5 select
struct(
  cast(key as string),
  cast(cast(key + 1000 as int) as string),
  cast(cast(key + 2000 as int) as string)),
value;

set hive.fetch.task.conversion=more;

-- 165,238,27,311,86
select * from hbase_ck_5;

-- 238
explain
select * from hbase_ck_5 where key.col1 = '238' AND key.col2 = '1238';
select * from hbase_ck_5 where key.col1 = '238' AND key.col2 = '1238';

-- 165,238
explain
select * from hbase_ck_5 where key.col1 >= '165' AND key.col1 < '27';
select * from hbase_ck_5 where key.col1 >= '165' AND key.col1 < '27';

-- 238,311
explain
select * from hbase_ck_5 where key.col1 > '100' AND key.col2 >= '1238';
select * from hbase_ck_5 where key.col1 > '100' AND key.col2 >= '1238';

explain
select * from hbase_ck_5 where key.col1 < '50' AND key.col2 >= '3238';
select * from hbase_ck_5 where key.col1 < '50' AND key.col2 >= '3238';
-- -*- mode:sql -*-

drop table if exists hb_target;

-- this is the target HBase table
create table hb_target(key int, val string)
stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
with serdeproperties ('hbase.columns.mapping' = ':key,cf:val')
tblproperties ('hbase.table.name' = 'positive_hbase_handler_bulk');

set hive.hbase.generatehfiles=true;
set hfile.family.path=/tmp/hb_target/cf;

-- this should produce three files in /tmp/hb_target/cf
insert overwrite table hb_target select distinct key, value from src cluster by key;

-- To get the files out to your local filesystem for loading into
-- HBase, run mkdir -p /tmp/blah/cf, then uncomment and
-- semicolon-terminate the line below before running this test:
-- dfs -copyToLocal /tmp/hb_target/cf/* /tmp/blah/cf

drop table hb_target;
dfs -rmr /tmp/hb_target/cf;
SET hive.hbase.snapshot.name=src_hbase_snapshot;
SET hive.hbase.snapshot.restoredir=/tmp;

SELECT * FROM src_hbase LIMIT 5;

SELECT value FROM src_hbase LIMIT 5;

select count(*) from src_hbase;
DROP TABLE users;
DROP TABLE states;
DROP TABLE countries;
DROP TABLE users_level;

-- From HIVE-1257

CREATE TABLE users(key string, state string, country string, country_id int)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "info:state,info:country,info:country_id"
);

CREATE TABLE states(key string, name string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "state:name"
);

CREATE TABLE countries(key string, name string, country string, country_id int)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "info:name,info:country,info:country_id"
);

INSERT OVERWRITE TABLE users SELECT 'user1', 'IA', 'USA', 0
FROM src WHERE key=100;

INSERT OVERWRITE TABLE states SELECT 'IA', 'Iowa'
FROM src WHERE key=100;

INSERT OVERWRITE TABLE countries SELECT 'USA', 'United States', 'USA', 1
FROM src WHERE key=100;

set hive.input.format = org.apache.hadoop.hive.ql.io.HiveInputFormat;

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country = c.key);

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country = c.country);

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country_id = c.country_id);

SELECT u.key, u.state, s.name FROM users u JOIN states s
ON (u.state = s.key);

set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country = c.key);

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country = c.country);

SELECT u.key, u.country, c.name, c.key FROM users u JOIN countries c
ON (u.country_id = c.country_id);

SELECT u.key, u.state, s.name FROM users u JOIN states s
ON (u.state = s.key);

DROP TABLE users;
DROP TABLE states;
DROP TABLE countries;

CREATE TABLE users(key int, userid int, username string, created int)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:nickname,f:created");

CREATE TABLE users_level(key int, userid int, level int)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,f:userid,f:level");

-- HIVE-1903:  the problem fixed here showed up even without any data,
-- so no need to load any to test it
SELECT year(from_unixtime(users.created)) AS year, level, count(users.userid) AS num
 FROM users JOIN users_level ON (users.userid = users_level.userid)
 GROUP BY year(from_unixtime(users.created)), level;

DROP TABLE users;
DROP TABLE users_level;
DROP TABLE src_null;
DROP TABLE hbase_null;

CREATE TABLE src_null(a STRING, b STRING, c STRING, d STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/null.txt' INTO TABLE src_null;

CREATE TABLE hbase_null(key string, col1 string, col2 string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = ":key,cf1:c1,cf1:c2"
);

SELECT d, a, c FROM src_null;

INSERT INTO TABLE hbase_null SELECT d, a, c FROM src_null;

SELECT COUNT(d) FROM src_null;
SELECT COUNT(key) FROM hbase_null;
SELECT COUNT(*) FROM hbase_null;

DROP TABLE src_null;
DROP TABLE hbase_null;
--create hive hbase table 1
drop table if exists hive1_tbl_data_hbase1;
drop table if exists hive1_tbl_data_hbase2;
drop view if exists hive1_view_data_hbase1;
drop view if exists hive1_view_data_hbase2;

CREATE TABLE hive1_tbl_data_hbase1 (COLUMID string,COLUMN_FN string,COLUMN_LN string,EMAIL string,COL_UPDATED_DATE timestamp, PK_COLUM string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES("hbase.columns.mapping" = "default:COLUMID,default:COLUMN_FN,default:COLUMN_LN,default:EMAIL,default:COL_UPDATED_DATE,:key"
)
;

--create hive view for the above hive table 1
CREATE VIEW hive1_view_data_hbase1
AS
SELECT *
FROM hive1_tbl_data_hbase1
WHERE PK_COLUM >='4000-00000'
and PK_COLUM <='4000-99999'
AND COL_UPDATED_DATE IS NOT NULL
;


--load data to hive table 1
insert into table hive1_tbl_data_hbase1 select '00001','john','doe','john@hotmail.com','2014-01-01 12:01:02','4000-10000' from src where key = 100;

--create hive hbase table 2
CREATE TABLE hive1_tbl_data_hbase2 (COLUMID string,COLUMN_FN string,COLUMN_LN string,EMAIL string,COL_UPDATED_DATE timestamp, PK_COLUM string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES("hbase.columns.mapping" = "default:COLUMID,default:COLUMN_FN,default:COLUMN_LN,default:EMAIL,default:COL_UPDATED_DATE,:key"
)
;

--create hive view for the above hive hbase table 2
CREATE VIEW hive1_view_data_hbase2
AS
SELECT *
FROM hive1_tbl_data_hbase2
where COL_UPDATED_DATE IS NOT NULL
;


--load data to hive hbase table 2
insert into table hive1_tbl_data_hbase2 select '00001','john','doe','john@hotmail.com','2014-01-01 12:01:02','00001' from src where key = 100;
;

set hive.optimize.ppd = true;
set hive.auto.convert.join=false;

-- do not return value without fix

select x.FIRST_NAME1, x.EMAIL1 from (
select p.COLUMN_FN as first_name1, a.EMAIL as email1 from hive1_view_data_hbase2 p inner join hive1_view_data_hbase1 a on p.COLUMID =a.COLUMID) x;

set hive.auto.convert.join=true;

-- return value with/without fix

select x.FIRST_NAME1, x.EMAIL1 from (
select p.COLUMN_FN as first_name1, a.EMAIL as email1 from hive1_view_data_hbase2 p inner join hive1_view_data_hbase1 a on p.COLUMID =a.COLUMID) x;

CREATE TABLE hbase_pushdown(key string, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string");

INSERT OVERWRITE TABLE hbase_pushdown
SELECT cast(key as string), value
FROM src;

-- with full pushdown
explain select * from hbase_pushdown where key>'90';

select * from hbase_pushdown where key>'90';
select * from hbase_pushdown where key<'1';
select * from hbase_pushdown where key<='2';
select * from hbase_pushdown where key>='90';

-- with cnostant expressinon
explain select * from hbase_pushdown where key>=cast(40 + 50 as string);
select * from hbase_pushdown where key>=cast(40 + 50 as string);

-- with partial pushdown

explain select * from hbase_pushdown where key>'90' and value like '%9%';

select * from hbase_pushdown where key>'90' and value like '%9%';

-- with two residuals

explain select * from hbase_pushdown
where key>='90' and value like '%9%' and key=cast(value as int);

select * from hbase_pushdown
where key>='90' and value like '%9%' and key=cast(value as int);


-- with contradictory pushdowns

explain select * from hbase_pushdown
where key<'80' and key>'90' and value like '%90%';

select * from hbase_pushdown
where key<'80' and key>'90' and value like '%90%';

-- with nothing to push down

explain select * from hbase_pushdown;

-- with a predicate which is not actually part of the filter, so
-- it should be ignored by pushdown

explain select * from hbase_pushdown
where (case when key<'90' then 2 else 4 end) > 3;

-- with a predicate which is under an OR, so it should
-- be ignored by pushdown

explain select * from hbase_pushdown
where key<='80' or value like '%90%';

-- following will get pushed into hbase after HIVE-2819
explain select * from hbase_pushdown where key > '281'
and key < '287';

select * from hbase_pushdown where key > '281'
and key < '287';

set hive.optimize.ppd.storage=false;

-- with pushdown disabled

explain select * from hbase_pushdown where key<='90';
CREATE TABLE hbase_pushdown(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string");

INSERT OVERWRITE TABLE hbase_pushdown
SELECT *
FROM src;

-- with full pushdown
explain select * from hbase_pushdown where key=90;

select * from hbase_pushdown where key=90;

-- with partial pushdown

explain select * from hbase_pushdown where key=90 and value like '%90%';

select * from hbase_pushdown where key=90 and value like '%90%';

set hive.optimize.index.filter=true;
-- with partial pushdown with optimization (HIVE-6650)
explain select * from hbase_pushdown where key=90 and value like '%90%';
select * from hbase_pushdown where key=90 and value like '%90%';
set hive.optimize.index.filter=false;

-- with two residuals

explain select * from hbase_pushdown
where key=90 and value like '%90%' and key=cast(value as int);

-- with contradictory pushdowns

explain select * from hbase_pushdown
where key=80 and key=90 and value like '%90%';

select * from hbase_pushdown
where key=80 and key=90 and value like '%90%';

-- with nothing to push down

explain select * from hbase_pushdown;

-- with a predicate which is not actually part of the filter, so
-- it should be ignored by pushdown

explain select * from hbase_pushdown
where (case when key=90 then 2 else 4 end) > 3;

-- with a predicate which is under an OR, so it should
-- be ignored by pushdown

explain select * from hbase_pushdown
where key=80 or value like '%90%';

set hive.optimize.ppd.storage=false;

-- with pushdown disabled

explain select * from hbase_pushdown where key=90;
DROP TABLE hbase_table_1;
CREATE TABLE hbase_table_1(key int comment 'It is a column key', value string comment 'It is the column string value')
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
TBLPROPERTIES ("hbase.table.name" = "hbase_table_0");

DESCRIBE EXTENDED hbase_table_1;

select * from hbase_table_1;

EXPLAIN FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0;
FROM src INSERT OVERWRITE TABLE hbase_table_1 SELECT * WHERE (key%2)=0;

DROP TABLE hbase_table_2;
CREATE EXTERNAL TABLE hbase_table_2(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string")
TBLPROPERTIES ("hbase.table.name" = "hbase_table_0");

EXPLAIN
SELECT Y.*
FROM
(SELECT hbase_table_1.* FROM hbase_table_1) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
ORDER BY key, value LIMIT 20;

SELECT Y.*
FROM
(SELECT hbase_table_1.* FROM hbase_table_1) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
ORDER BY key, value LIMIT 20;

EXPLAIN
SELECT Y.*
FROM
(SELECT hbase_table_1.* FROM hbase_table_1 WHERE 100 < hbase_table_1.key) x
JOIN
(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
ON (x.key = Y.key)
ORDER BY key, value;

SELECT Y.*
FROM
(SELECT hbase_table_1.* FROM hbase_table_1 WHERE 100 < hbase_table_1.key) x
JOIN
(SELECT hbase_table_2.* FROM hbase_table_2 WHERE hbase_table_2.key < 120) Y
ON (x.key = Y.key)
ORDER BY key,value;

DROP TABLE empty_hbase_table;
CREATE TABLE empty_hbase_table(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf:string");

DROP TABLE empty_normal_table;
CREATE TABLE empty_normal_table(key int, value string);

select * from (select count(1) as c from empty_normal_table union all select count(1) as c from empty_hbase_table) x order by c;
select * from (select count(1) c from empty_normal_table union all select count(1) as c from hbase_table_1) x order by c;
select * from (select count(1) c from src union all select count(1) as c from empty_hbase_table) x order by c;
select * from (select count(1) c from src union all select count(1) as c from hbase_table_1) x order by c;

CREATE TABLE hbase_table_3(key int, value string, count int)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "cf:val,cf2:count"
);

EXPLAIN
INSERT OVERWRITE TABLE hbase_table_3
SELECT x.key, x.value, Y.count
FROM
(SELECT hbase_table_1.* FROM hbase_table_1) x
JOIN
(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
ON (x.key = Y.key);

INSERT OVERWRITE TABLE hbase_table_3
SELECT x.key, x.value, Y.count
FROM
(SELECT hbase_table_1.* FROM hbase_table_1) x
JOIN
(SELECT src.key, count(src.key) as count FROM src GROUP BY src.key) Y
ON (x.key = Y.key);

select count(1) from hbase_table_3;
select * from hbase_table_3 order by key, value limit 5;
select key, count from hbase_table_3 order by key, count desc limit 5;

DROP TABLE hbase_table_4;
CREATE TABLE hbase_table_4(key int, value1 string, value2 int, value3 int)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "a:b,a:c,d:e"
);

INSERT OVERWRITE TABLE hbase_table_4 SELECT key, value, key+1, key+2
FROM src WHERE key=98 OR key=100;

SELECT * FROM hbase_table_4 ORDER BY key;

DROP TABLE hbase_table_5;
CREATE EXTERNAL TABLE hbase_table_5(key int, value map<string,string>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = "a:")
TBLPROPERTIES ("hbase.table.name" = "hbase_table_4");

SELECT * FROM hbase_table_5 ORDER BY key;

DROP TABLE hbase_table_6;
CREATE TABLE hbase_table_6(key int, value map<string,string>)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = ":key,cf:"
);
INSERT OVERWRITE TABLE hbase_table_6 SELECT key, map(value, key) FROM src
WHERE key=98 OR key=100;

SELECT * FROM hbase_table_6 ORDER BY key;

DROP TABLE hbase_table_7;
CREATE TABLE hbase_table_7(value map<string,string>, key int)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "cf:,:key"
);
INSERT OVERWRITE TABLE hbase_table_7
SELECT map(value, key, upper(value), key+1), key FROM src
WHERE key=98 OR key=100;

SELECT * FROM hbase_table_7 ORDER BY key;

set hive.hbase.wal.enabled=false;

DROP TABLE hbase_table_8;
CREATE TABLE hbase_table_8(key int, value1 string, value2 int, value3 int)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
"hbase.columns.mapping" = "a:b,a:c,d:e"
);

INSERT OVERWRITE TABLE hbase_table_8 SELECT key, value, key+1, key+2
FROM src WHERE key=98 OR key=100;

SELECT * FROM hbase_table_8 ORDER BY key;

DROP TABLE IF EXISTS hbase_table_3_like;
CREATE TABLE hbase_table_3_like LIKE hbase_table_3;
DESCRIBE EXTENDED hbase_table_3_like;

INSERT OVERWRITE TABLE hbase_table_3_like SELECT * FROM hbase_table_3;
SELECT * FROM hbase_table_3_like ORDER BY key, value LIMIT 5;

DROP TABLE IF EXISTS hbase_table_1_like;
CREATE EXTERNAL TABLE hbase_table_1_like LIKE hbase_table_1;
DESCRIBE EXTENDED hbase_table_1_like;

INSERT OVERWRITE TABLE hbase_table_1_like SELECT * FROM hbase_table_1;
SELECT COUNT(*) FROM hbase_table_1_like;

SHOW CREATE TABLE hbase_table_1_like;

DROP TABLE hbase_table_1;
DROP TABLE hbase_table_1_like;
DROP TABLE hbase_table_2;
DROP TABLE hbase_table_3;
DROP TABLE hbase_table_3_like;
DROP TABLE hbase_table_4;
DROP TABLE hbase_table_5;
DROP TABLE hbase_table_6;
DROP TABLE hbase_table_7;
DROP TABLE hbase_table_8;
DROP TABLE empty_hbase_table;
DROP TABLE empty_normal_table;
CREATE TABLE hbase_pushdown(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string",
"hbase.scan.cache" = "500", "hbase.scan.cacheblocks" = "true", "hbase.scan.batch" = "1");

INSERT OVERWRITE TABLE hbase_pushdown SELECT * FROM src;

select * from hbase_pushdown;
-- HIVE-4375 Single sourced multi insert consists of native and non-native table mixed throws NPE
CREATE TABLE src_x1(key string, value string);
CREATE TABLE src_x2(key string, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key, cf:value");

explain
from src a
insert overwrite table src_x1
select key,"" where a.key > 0 AND a.key < 50
insert overwrite table src_x2
select value,"" where a.key > 50 AND a.key < 100;

from src a
insert overwrite table src_x1
select key,"" where a.key > 0 AND a.key < 50
insert overwrite table src_x2
select value,"" where a.key > 50 AND a.key < 100;

select * from src_x1 order by key;
select * from src_x2 order by key;
DROP TABLE hbase_table;
CREATE TABLE hbase_table (key string, value string, time timestamp)
  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string,:timestamp");
DESC extended hbase_table;
FROM src INSERT OVERWRITE TABLE hbase_table SELECT key, value, "2012-02-23 10:14:52" WHERE (key % 17) = 0;
SELECT * FROM hbase_table;

DROP TABLE hbase_table;
CREATE TABLE hbase_table (key string, value string, time bigint)
  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string,:timestamp");
FROM src INSERT OVERWRITE TABLE hbase_table SELECT key, value, 1329959754000 WHERE (key % 17) = 0;
SELECT key, value, cast(time as timestamp) FROM hbase_table;

DROP TABLE hbase_table;
CREATE TABLE hbase_table (key string, value string, time bigint)
  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf:string,:timestamp");
insert overwrite table hbase_table select key,value,ts FROM
(
  select key, value, 100000000000 as ts from src WHERE (key % 33) = 0
  UNION ALL
  select key, value, 200000000000 as ts from src WHERE (key % 37) = 0
) T;

explain
SELECT key, value, cast(time as timestamp) FROM hbase_table WHERE key > 100 AND key < 400 AND time < 200000000000;
SELECT key, value, cast(time as timestamp) FROM hbase_table WHERE key > 100 AND key < 400 AND time < 200000000000;

explain
SELECT key, value, cast(time as timestamp) FROM hbase_table WHERE key > 100 AND key < 400 AND time > 100000000000;
SELECT key, value, cast(time as timestamp) FROM hbase_table WHERE key > 100 AND key < 400 AND time > 100000000000;

explain
SELECT key, value, cast(time as timestamp) FROM hbase_table WHERE key > 100 AND key < 400 AND time <= 100000000000;
SELECT key, value, cast(time as timestamp) FROM hbase_table WHERE key > 100 AND key < 400 AND time <= 100000000000;

explain
SELECT key, value, cast(time as timestamp) FROM hbase_table WHERE key > 100 AND key < 400 AND time >= 200000000000;
SELECT key, value, cast(time as timestamp) FROM hbase_table WHERE key > 100 AND key < 400 AND time >= 200000000000;

create table hbase_str(rowkey string,mytime string,mystr string)
  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  WITH SERDEPROPERTIES ('hbase.columns.mapping' = 'm:mytime,m:mystr')
  TBLPROPERTIES ('hbase.table.name' = 'hbase_ts');

describe hbase_str;
insert overwrite table hbase_str select key, '2001-02-03-04.05.06.123456', value from src limit 3;
select * from hbase_str;

-- Timestamp string does not match the default timestamp format, specify a custom timestamp format
create external table hbase_ts(rowkey string,mytime timestamp,mystr string)
  STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
  WITH SERDEPROPERTIES ('hbase.columns.mapping' = 'm:mytime,m:mystr', 'timestamp.formats' = 'yyyy-MM-dd-HH.mm.ss.SSSSSS')
  TBLPROPERTIES ('hbase.table.name' = 'hbase_ts');

describe hbase_ts;
select * from hbase_ts;

drop table hbase_str;
drop table hbase_ts;
drop table vcsc;
CREATE TABLE vcsc (c STRING) PARTITIONED BY (ds STRING);
ALTER TABLE vcsc ADD partition (ds='dummy') location '${system:test.tmp.dir}/VerifyContentSummaryCacheHook';

set hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook;
SELECT a.c, b.c FROM vcsc a JOIN vcsc b ON a.ds = 'dummy' AND b.ds = 'dummy' AND a.c = b.c;

set mapred.job.tracker=local;
set hive.exec.pre.hooks = ;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyContentSummaryCacheHook;
SELECT a.c, b.c FROM vcsc a JOIN vcsc b ON a.ds = 'dummy' AND b.ds = 'dummy' AND a.c = b.c;

set hive.exec.post.hooks=;
drop table vcsc;
SET hive.exec.pre.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder$RunFirst,org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder$RunSecond;
SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder$RunFirst,org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder$RunSecond;
SET hive.semantic.analyzer.hook=org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder$RunFirstSemanticAnalysisHook,org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder$RunSecondSemanticAnalysisHook;
SET hive.exec.driver.run.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder$RunFirstDriverRunHook,org.apache.hadoop.hive.ql.hooks.VerifyHooksRunInOrder$RunSecondDriverRunHook;

SELECT count(*) FROM src;

SET hive.exec.pre.hooks=;
SET hive.exec.post.hooks=;
SET hive.semantic.analyzer.hook=;
SET hive.exec.driver.run.hooks=;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- Hybrid Grace Hash Join
-- Test basic functionalities:
-- 1. Various cases when hash partitions spill
-- 2. Partitioned table spilling
-- 3. Vectorization

SELECT 1;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask.size=1300000;
set hive.mapjoin.optimized.hashtable.wbsize=880000;
set hive.mapjoin.hybridgrace.memcheckfrequency=1024;

set hive.mapjoin.hybridgrace.hashtable=false;

-- Base result for inner join
explain
select count(*) from
(select c.ctinyint
 from alltypesorc c
 inner join alltypesorc cd
 on cd.cint = c.cint
 where c.cint < 2000000000) t1
;

select count(*) from
(select c.ctinyint
 from alltypesorc c
 inner join alltypesorc cd
 on cd.cint = c.cint
 where c.cint < 2000000000) t1
;

set hive.mapjoin.hybridgrace.hashtable=true;

-- Two partitions are created. One in memory, one on disk on creation.
-- The one in memory will eventually exceed memory limit, but won't spill.
explain
select count(*) from
(select c.ctinyint
 from alltypesorc c
 inner join alltypesorc cd
 on cd.cint = c.cint
 where c.cint < 2000000000) t1
;

select count(*) from
(select c.ctinyint
 from alltypesorc c
 inner join alltypesorc cd
 on cd.cint = c.cint
 where c.cint < 2000000000) t1
;

set hive.auto.convert.join.noconditionaltask.size=3000000;
set hive.mapjoin.optimized.hashtable.wbsize=100000;

set hive.mapjoin.hybridgrace.hashtable=false;

-- Base result for inner join
explain
select count(*) from
(select c.ctinyint
 from alltypesorc c
 inner join alltypesorc cd
 on cd.cint = c.cint) t1
;

select count(*) from
(select c.ctinyint
 from alltypesorc c
 inner join alltypesorc cd
 on cd.cint = c.cint) t1
;

set hive.mapjoin.hybridgrace.hashtable=true;

-- 16 partitions are created: 3 in memory, 13 on disk on creation.
-- 1 partition is spilled during first round processing, which ends up having 2 in memory, 14 on disk
explain
select count(*) from
(select c.ctinyint
 from alltypesorc c
 inner join alltypesorc cd
 on cd.cint = c.cint) t1
;

select count(*) from
(select c.ctinyint
 from alltypesorc c
 inner join alltypesorc cd
 on cd.cint = c.cint) t1
;



set hive.mapjoin.hybridgrace.hashtable=false;

-- Base result for outer join
explain
select count(*) from
(select c.ctinyint
 from alltypesorc c
 left outer join alltypesorc cd
 on cd.cint = c.cint) t1
;

select count(*) from
(select c.ctinyint
 from alltypesorc c
 left outer join alltypesorc cd
 on cd.cint = c.cint) t1
;

set hive.mapjoin.hybridgrace.hashtable=true;

-- 32 partitions are created. 3 in memory, 29 on disk on creation.
explain
select count(*) from
(select c.ctinyint
 from alltypesorc c
 left outer join alltypesorc cd
 on cd.cint = c.cint) t1
;

select count(*) from
(select c.ctinyint
 from alltypesorc c
 left outer join alltypesorc cd
 on cd.cint = c.cint) t1
;


-- Partitioned table
create table parttbl (key string, value char(20)) partitioned by (dt char(10));
insert overwrite table parttbl partition(dt='2000-01-01')
  select * from src;
insert overwrite table parttbl partition(dt='2000-01-02')
  select * from src1;

set hive.auto.convert.join.noconditionaltask.size=30000000;
set hive.mapjoin.optimized.hashtable.wbsize=10000000;

set hive.mapjoin.hybridgrace.hashtable=false;

-- No spill, base result
explain
select count(*) from
(select p1.value
 from parttbl p1
 inner join parttbl p2
 on p1.key = p2.key) t1
;

select count(*) from
(select p1.value
 from parttbl p1
 inner join parttbl p2
 on p1.key = p2.key) t1
;

set hive.mapjoin.hybridgrace.hashtable=true;

-- No spill, 2 partitions created in memory
explain
select count(*) from
(select p1.value
 from parttbl p1
 inner join parttbl p2
 on p1.key = p2.key) t1
;

select count(*) from
(select p1.value
 from parttbl p1
 inner join parttbl p2
 on p1.key = p2.key) t1
;


set hive.auto.convert.join.noconditionaltask.size=20000;
set hive.mapjoin.optimized.hashtable.wbsize=10000;

set hive.mapjoin.hybridgrace.hashtable=false;

-- Spill case base result
explain
select count(*) from
(select p1.value
 from parttbl p1
 inner join parttbl p2
 on p1.key = p2.key) t1
;

select count(*) from
(select p1.value
 from parttbl p1
 inner join parttbl p2
 on p1.key = p2.key) t1
;

set hive.mapjoin.hybridgrace.hashtable=true;

-- Spill case, one partition in memory, one spilled on creation
explain
select count(*) from
(select p1.value
 from parttbl p1
 inner join parttbl p2
 on p1.key = p2.key) t1
;

select count(*) from
(select p1.value
 from parttbl p1
 inner join parttbl p2
 on p1.key = p2.key) t1
;

drop table parttbl;


-- Test vectorization
-- Test case borrowed from vector_decimal_mapjoin.q
CREATE TABLE decimal_mapjoin STORED AS ORC AS
  SELECT cdouble, CAST (((cdouble*22.1)/37) AS DECIMAL(20,10)) AS cdecimal1,
  CAST (((cdouble*9.3)/13) AS DECIMAL(23,14)) AS cdecimal2,
  cint
  FROM alltypesorc;

SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=50000000;
set hive.mapjoin.optimized.hashtable.wbsize=10000;
SET hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=false;

EXPLAIN SELECT l.cint, r.cint, l.cdecimal1, r.cdecimal2
  FROM decimal_mapjoin l
  JOIN decimal_mapjoin r ON l.cint = r.cint
  WHERE l.cint = 6981;
SELECT l.cint, r.cint, l.cdecimal1, r.cdecimal2
  FROM decimal_mapjoin l
  JOIN decimal_mapjoin r ON l.cint = r.cint
  WHERE l.cint = 6981;

set hive.mapjoin.hybridgrace.hashtable=true;

EXPLAIN SELECT l.cint, r.cint, l.cdecimal1, r.cdecimal2
  FROM decimal_mapjoin l
  JOIN decimal_mapjoin r ON l.cint = r.cint
  WHERE l.cint = 6981;
SELECT l.cint, r.cint, l.cdecimal1, r.cdecimal2
  FROM decimal_mapjoin l
  JOIN decimal_mapjoin r ON l.cint = r.cint
  WHERE l.cint = 6981;

DROP TABLE decimal_mapjoin;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- Hybrid Grace Hash Join
-- Test n-way join
SELECT 1;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;
set hive.cbo.enable=false;


-- 3-way mapjoin (1 big table, 2 small tables)
SELECT 1;

set hive.mapjoin.hybridgrace.hashtable=false;

EXPLAIN
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN src y ON (y.key = x.key);

SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN src y ON (y.key = x.key);

set hive.mapjoin.hybridgrace.hashtable=true;

EXPLAIN
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN src y ON (y.key = x.key);

SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN src y ON (y.key = x.key);


-- 4-way mapjoin (1 big table, 3 small tables)
SELECT 1;

set hive.mapjoin.hybridgrace.hashtable=false;

EXPLAIN
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN srcpart w ON (x.key = w.key)
JOIN src y ON (y.key = x.key);

SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN srcpart w ON (x.key = w.key)
JOIN src y ON (y.key = x.key);

set hive.mapjoin.hybridgrace.hashtable=true;

EXPLAIN
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN srcpart w ON (x.key = w.key)
JOIN src y ON (y.key = x.key);

SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN srcpart w ON (x.key = w.key)
JOIN src y ON (y.key = x.key);


-- 2 sets of 3-way mapjoin under 2 different tasks
SELECT 1;

set hive.mapjoin.hybridgrace.hashtable=false;

EXPLAIN
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN src y ON (y.key = x.key)
UNION
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.value = z.value)
JOIN src y ON (y.value = x.value);

SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN src y ON (y.key = x.key)
UNION
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.value = z.value)
JOIN src y ON (y.value = x.value);

set hive.mapjoin.hybridgrace.hashtable=true;

EXPLAIN
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN src y ON (y.key = x.key)
UNION
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.value = z.value)
JOIN src y ON (y.value = x.value);

SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.key = z.key)
JOIN src y ON (y.key = x.key)
UNION
SELECT COUNT(*)
FROM src1 x JOIN srcpart z ON (x.value = z.value)
JOIN src y ON (y.value = x.value);


-- A chain of 2 sets of 3-way mapjoin under the same task
SELECT 1;

set hive.mapjoin.hybridgrace.hashtable=false;

EXPLAIN
SELECT COUNT(*)
FROM src1 x
JOIN srcpart z1 ON (x.key = z1.key)
JOIN src y1     ON (x.key = y1.key)
JOIN srcpart z2 ON (x.value = z2.value)
JOIN src y2     ON (x.value = y2.value)
WHERE z1.key < 'zzzzzzzz' AND z2.key < 'zzzzzzzzzz'
 AND y1.value < 'zzzzzzzz' AND y2.value < 'zzzzzzzzzz';

SELECT COUNT(*)
FROM src1 x
JOIN srcpart z1 ON (x.key = z1.key)
JOIN src y1     ON (x.key = y1.key)
JOIN srcpart z2 ON (x.value = z2.value)
JOIN src y2     ON (x.value = y2.value)
WHERE z1.key < 'zzzzzzzz' AND z2.key < 'zzzzzzzzzz'
 AND y1.value < 'zzzzzzzz' AND y2.value < 'zzzzzzzzzz';

set hive.mapjoin.hybridgrace.hashtable=true;

EXPLAIN
SELECT COUNT(*)
FROM src1 x
JOIN srcpart z1 ON (x.key = z1.key)
JOIN src y1     ON (x.key = y1.key)
JOIN srcpart z2 ON (x.value = z2.value)
JOIN src y2     ON (x.value = y2.value)
WHERE z1.key < 'zzzzzzzz' AND z2.key < 'zzzzzzzzzz'
 AND y1.value < 'zzzzzzzz' AND y2.value < 'zzzzzzzzzz';

SELECT COUNT(*)
FROM src1 x
JOIN srcpart z1 ON (x.key = z1.key)
JOIN src y1     ON (x.key = y1.key)
JOIN srcpart z2 ON (x.value = z2.value)
JOIN src y2     ON (x.value = y2.value)
WHERE z1.key < 'zzzzzzzz' AND z2.key < 'zzzzzzzzzz'
 AND y1.value < 'zzzzzzzz' AND y2.value < 'zzzzzzzzzz';


reset hive.cbo.enable;
set hive.optimize.remove.identity.project=true;
set hive.auto.convert.join=true;
set hive.optimize.ppd=true;
set hive.explain.user=false;

explain
select t2.*
from
  (select key,value from (select key,value from src) t1 sort by key) t2
  join
  (select * from src sort by key) t3
  on (t2.key=t3.key )
  where t2.value='val_105' and t3.key='105';

select t2.*
from
  (select key,value from (select key,value from src) t1 sort by key) t2
  join
  (select * from src sort by key) t3
  on (t2.key=t3.key )
  where t2.value='val_105' and t3.key='105';
-- begin part(string, int) pass(string, string)
CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='second');

select * from tab1;
drop table tab1;

create table tab1 (id1 int, id2 string) PARTITIONED BY(month string,day int) row format delimited fields terminated by ',';
alter table tab1 add partition (month='June', day='second');
drop table tab1;
create table tab1(c int) partitioned by (i int);
alter table tab1 add partition(i = "some name");

drop table tab1;
create table tab1(s string) PARTITIONED BY(dt date, st string);
alter table tab1 add partition (dt=date 'foo', st='foo');
drop table tab1;
CREATE TABLE implicit_test1(a BIGINT, b STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe' WITH SERDEPROPERTIES('serialization.format'= 'org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol') STORED AS TEXTFILE;

EXPLAIN
SELECT implicit_test1.*
FROM implicit_test1
WHERE implicit_test1.a <> 0;

SELECT implicit_test1.*
FROM implicit_test1
WHERE implicit_test1.a <> 0;



set hive.mapred.mode=nonstrict;
create table implicit_cast_during_insert (c1 int, c2 string)
  partitioned by (p1 string) stored as orc;

set hive.exec.dynamic.partition.mode=nonstrict;

explain
insert overwrite table implicit_cast_during_insert partition (p1)
  select key, value, key key1 from (select * from src where key in (0,1)) q
  distribute by key1 sort by key1;

insert overwrite table implicit_cast_during_insert partition (p1)
  select key, value, key key1 from (select * from src where key in (0,1)) q
  distribute by key1 sort by key1;

select * from implicit_cast_during_insert;

drop table implicit_cast_during_insert;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/test_import_exported_table/;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/test_import_exported_table/exported_table/;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/test_import_exported_table/exported_table/data/;

dfs -copyFromLocal ../../data/files/exported_table/_metadata hdfs:///tmp/test_import_exported_table/exported_table;
dfs -copyFromLocal ../../data/files/exported_table/data/data hdfs:///tmp/test_import_exported_table/exported_table/data;

IMPORT FROM '/tmp/test_import_exported_table/exported_table';
DESCRIBE j1_41;
SELECT * from j1_41;

dfs -rmr hdfs:///tmp/test_import_exported_table;

set hive.stats.dbclass=fs;
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

create table foobar(key int, value string) PARTITIONED BY (ds string, hr string);
alter table foobar add partition (ds='2008-04-08',hr='12');

CREATE INDEX srcpart_AUTH_index ON TABLE foobar(key) as 'BITMAP' WITH DEFERRED REBUILD;
SHOW INDEXES ON foobar;

grant select on table foobar to user hive_test_user;
grant select on table default__foobar_srcpart_auth_indeX__ to user hive_test_user;
grant update on table default__foobar_srcpart_auth_indEx__ to user hive_test_user;
grant create on table default__foobar_srcpart_auth_inDex__ to user hive_test_user;
set hive.security.authorization.enabled=true;

ALTER INDEX srcpart_auth_INDEX ON foobar PARTITION (ds='2008-04-08',hr='12')  REBUILD;
set hive.security.authorization.enabled=false;
DROP INDEX srcpart_auth_index on foobar;
DROP TABLE foobar;
set hive.mapred.mode=nonstrict;
-- try the query without indexing, with manual indexing, and with automatic indexing
-- SORT_QUERY_RESULTS

-- without indexing
SELECT key, value FROM src WHERE key > 80 AND key < 100;

set hive.stats.dbclass=fs;
CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- manual indexing
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_where" SELECT `_bucketname` ,  `_offsets` FROM default__src_src_index__ WHERE key > 80 AND key < 100;
SET hive.index.compact.file=${system:test.tmp.dir}/index_where;
SET hive.optimize.index.filter=false;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;

EXPLAIN SELECT key, value FROM src WHERE key > 80 AND key < 100;
SELECT key, value FROM src WHERE key > 80 AND key < 100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- automatic indexing
EXPLAIN SELECT key, value FROM src WHERE key > 80 AND key < 100;
SELECT key, value FROM src WHERE key > 80 AND key < 100;

DROP INDEX src_index on src;
set hive.mapred.mode=nonstrict;
-- Test to ensure that an empty index result is propagated correctly

CREATE DATABASE it;
-- Create temp, and populate it with some values in src.
CREATE TABLE it.temp(key STRING, val STRING) STORED AS TEXTFILE;

set hive.stats.dbclass=fs;
-- Build an index on it.temp.
CREATE INDEX temp_index ON TABLE it.temp(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX temp_index ON it.temp REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- query should not return any values
SELECT * FROM it.it__temp_temp_index__ WHERE key = 86;
EXPLAIN SELECT * FROM it.temp WHERE key  = 86;
SELECT * FROM it.temp WHERE key  = 86;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=false;
DROP table it.temp;

DROP DATABASE it;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS
-- test automatic use of index on different file formats
CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

EXPLAIN SELECT key, value FROM src WHERE key=86;
SELECT key, value FROM src WHERE key=86;

SET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

EXPLAIN SELECT key, value FROM src WHERE key=86;
SELECT key, value FROM src WHERE key=86;

DROP INDEX src_index on src;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS
-- With multiple indexes, make sure we choose which to use in a consistent order

CREATE INDEX src_key_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
CREATE INDEX src_val_index ON TABLE src(value) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_key_index ON src REBUILD;
ALTER INDEX src_val_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

EXPLAIN SELECT key, value FROM src WHERE key=86;
SELECT key, value FROM src WHERE key=86;

DROP INDEX src_key_index ON src;
DROP INDEX src_val_index ON src;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- try the query without indexing, with manual indexing, and with automatic indexing

-- without indexing
EXPLAIN SELECT a.key, a.value FROM src a JOIN srcpart b ON (a.key = b.key) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;
SELECT a.key, a.value FROM src a JOIN srcpart b ON (a.key = b.key) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;

set hive.stats.dbclass=fs;

CREATE INDEX src_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

CREATE INDEX srcpart_index ON TABLE srcpart(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX srcpart_index ON srcpart REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

EXPLAIN SELECT a.key, a.value FROM src a JOIN srcpart b ON (a.key = b.key) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;
SELECT a.key, a.value FROM src a JOIN srcpart b ON (a.key = b.key) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;

DROP INDEX src_index on src;
DROP INDEX srcpart_index on src;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- try the query without indexing, with manual indexing, and with automatic indexing

-- without indexing
EXPLAIN SELECT a.key, a.value FROM src a JOIN srcpart b ON (a.key = b.key) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;
SELECT a.key, a.value FROM src a JOIN srcpart b ON (a.key = b.key) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;

set hive.stats.dbclass=fs;

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

CREATE INDEX srcpart_index ON TABLE srcpart(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX srcpart_index ON srcpart REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- automatic indexing
EXPLAIN SELECT a.key, a.value FROM src a JOIN srcpart b ON (a.key = b.key) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;
SELECT a.key, a.value FROM src a JOIN srcpart b ON (a.key = b.key) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;

DROP INDEX src_index on src;
DROP INDEX srcpart_index on src;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;
set hive.fetch.task.conversion=none;

-- SORT_QUERY_RESULTS
-- test automatic use of index on table with partitions
CREATE INDEX src_part_index ON TABLE srcpart(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_part_index ON srcpart REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

EXPLAIN SELECT key, value FROM srcpart WHERE key=86 AND ds='2008-04-09';
SELECT key, value FROM srcpart WHERE key=86 AND ds='2008-04-09';

DROP INDEX src_part_index ON srcpart;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- try the query without indexing, with manual indexing, and with automatic indexing

EXPLAIN SELECT a.key, b.key FROM src a JOIN src b ON (a.value = b.value) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;
SELECT a.key, b.key FROM src a JOIN src b ON (a.value = b.value) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;

set hive.stats.dbclass=fs;
CREATE INDEX src_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

EXPLAIN SELECT a.key, b.key FROM src a JOIN src b ON (a.value = b.value) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;
SELECT a.key, b.key FROM src a JOIN src b ON (a.value = b.value) WHERE a.key > 80 AND a.key < 100 AND b.key > 70 AND b.key < 90;

DROP INDEX src_index on src;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS
-- test cases where the index should not be used automatically

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=5368709120;
SET hive.optimize.index.filter.compact.maxsize=-1;

-- min size too large (src is less than 5G)
EXPLAIN SELECT * FROM src WHERE key > 80 AND key < 100;
SELECT * FROM src WHERE key > 80 AND key < 100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;
SET hive.optimize.index.filter.compact.maxsize=1;

-- max size too small
EXPLAIN SELECT * FROM src WHERE key > 80 AND key < 100;
SELECT * FROM src WHERE key > 80 AND key < 100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;
SET hive.optimize.index.filter.compact.maxsize=-1;

-- OR predicate not supported by compact indexes
EXPLAIN SELECT * FROM src WHERE key < 10 OR key > 480;
SELECT * FROM src WHERE key < 10 OR key > 480;

 SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;
SET hive.optimize.index.filter.compact.maxsize=-1;

-- columns are not covered by indexes
DROP INDEX src_index on src;
CREATE INDEX src_val_index ON TABLE src(value) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_val_index ON src REBUILD;

EXPLAIN SELECT * FROM src WHERE key > 80 AND key < 100;
SELECT * FROM src WHERE key > 80 AND key < 100;

DROP INDEX src_val_index on src;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;
SET hive.optimize.index.filter.compact.maxsize=-1;

-- required partitions have not been built yet
CREATE INDEX src_part_index ON TABLE srcpart(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_part_index ON srcpart PARTITION (ds='2008-04-08', hr=11) REBUILD;

EXPLAIN SELECT * FROM srcpart WHERE ds='2008-04-09' AND hr=12 AND key < 10;
SELECT * FROM srcpart WHERE ds='2008-04-09' AND hr=12 AND key < 10;

DROP INDEX src_part_index on srcpart;
set hive.mapred.mode=nonstrict;
-- Test if index is actually being used.

-- Create temp, and populate it with some values in src.
CREATE TABLE temp(key STRING, val STRING) STORED AS TEXTFILE;
INSERT OVERWRITE TABLE temp SELECT * FROM src WHERE key < 50;

-- Build an index on temp.
CREATE INDEX temp_index ON TABLE temp(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX temp_index ON temp REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.autoupdate=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- overwrite temp table so index is out of date
EXPLAIN INSERT OVERWRITE TABLE temp SELECT * FROM src;
INSERT OVERWRITE TABLE temp SELECT * FROM src;

-- query should return indexed values
EXPLAIN SELECT * FROM temp WHERE key  = 86;
SELECT * FROM temp WHERE key  = 86;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=false;
DROP table temp;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

DROP INDEX srcpart_index_proj on srcpart;

EXPLAIN
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) as 'BITMAP' WITH DEFERRED REBUILD;
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX srcpart_index_proj ON srcpart REBUILD;
SELECT x.* FROM default__srcpart_srcpart_index_proj__ x WHERE x.ds = '2008-04-08' and x.hr = 11;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_test_index_result" SELECT `_bucketname`,
COLLECT_SET(`_offset`) as `_offsets` FROM default__srcpart_srcpart_index_proj__
x WHERE NOT EWAH_BITMAP_EMPTY(`_bitmaps`) AND x.key=100 AND x.ds = '2008-04-08' GROUP BY `_bucketname`;
SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_test_index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;
SELECT key, value FROM srcpart WHERE key=100 AND ds = '2008-04-08';

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_test_index_result" SELECT `_bucketname` ,
COLLECT_SET(`_offset`) as `_offsets` FROM default__srcpart_srcpart_index_proj__
x WHERE NOT EWAH_BITMAP_EMPTY(`_bitmaps`) AND x.key=100 AND x.ds = '2008-04-08' and x.hr = 11 GROUP BY `_bucketname`;
SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_test_index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;
SELECT key, value FROM srcpart WHERE key=100 AND ds = '2008-04-08' and hr = 11;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM srcpart WHERE key=100 AND ds = '2008-04-08' and hr = 11;

DROP INDEX srcpart_index_proj on srcpart;

EXPLAIN
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) as 'BITMAP' WITH DEFERRED REBUILD;
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER  INDEX srcpart_index_proj ON srcpart REBUILD;
SELECT x.* FROM default__srcpart_srcpart_index_proj__ x;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname` ,
COLLECT_SET(`_offset`) as `_offsets` FROM default__srcpart_srcpart_index_proj__
WHERE NOT EWAH_BITMAP_EMPTY(`_bitmaps`) AND key=100 GROUP BY `_bucketname`;
SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;
SELECT key, value FROM srcpart WHERE key=100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM srcpart WHERE key=100;

DROP INDEX srcpart_index_proj on srcpart;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

EXPLAIN
CREATE INDEX src_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
CREATE INDEX src_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;
SELECT x.* FROM default__src_src_index__ x;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname`,
COLLECT_SET(`_offset`) as `_offsets` FROM default__src_src_index__ WHERE NOT
EWAH_BITMAP_EMPTY(`_bitmaps`) AND key=100 GROUP BY `_bucketname`;
SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;
SELECT key, value FROM src WHERE key=100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM src WHERE key=100;

DROP INDEX src_index ON src;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

EXPLAIN
CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
EXPLAIN
CREATE INDEX src2_index ON TABLE src(value) as 'BITMAP' WITH DEFERRED REBUILD;

CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
CREATE INDEX src2_index ON TABLE src(value) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src1_index ON src REBUILD;
ALTER INDEX src2_index ON src REBUILD;
SELECT * FROM default__src_src1_index__;
SELECT * FROM default__src_src2_index__;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result"
SELECT t.bucketname as `_bucketname`, COLLECT_SET(t.offset) AS `_offsets` FROM
  (SELECT `_bucketname` AS bucketname, `_offset` AS offset
      FROM default__src_src1_index__
      WHERE key = 0 AND NOT EWAH_BITMAP_EMPTY(`_bitmaps`) UNION ALL
   SELECT `_bucketname` AS bucketname, `_offset` AS offset
      FROM default__src_src2_index__
      WHERE value = "val2" AND NOT EWAH_BITMAP_EMPTY(`_bitmaps`)) t
GROUP BY t.bucketname;

SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;

SELECT key, value FROM src WHERE key=0 OR value = "val_2";

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM src WHERE key=0 OR value = "val_2";

DROP INDEX src1_index ON src;
DROP INDEX src2_index ON src;

set hive.mapred.mode=nonstrict;
set hive.stats.autogather=true;

-- SORT_QUERY_RESULTS

EXPLAIN
CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
EXPLAIN
CREATE INDEX src2_index ON TABLE src(value) as 'BITMAP' WITH DEFERRED REBUILD;

CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
CREATE INDEX src2_index ON TABLE src(value) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src1_index ON src REBUILD;
ALTER INDEX src2_index ON src REBUILD;
SELECT * FROM default__src_src1_index__;
SELECT * FROM default__src_src2_index__;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

EXPLAIN
SELECT a.bucketname AS `_bucketname`, COLLECT_SET(a.offset) as `_offsets`
FROM (SELECT `_bucketname` AS bucketname, `_offset` AS offset, `_bitmaps` AS bitmaps FROM default__src_src1_index__
        WHERE key = 0) a
  JOIN
    (SELECT `_bucketname` AS bucketname, `_offset` AS offset, `_bitmaps` AS bitmaps FROM default__src_src2_index__
       WHERE value = "val_0") b
 ON
   a.bucketname = b.bucketname AND a.offset = b.offset WHERE NOT
EWAH_BITMAP_EMPTY(EWAH_BITMAP_AND(a.bitmaps, b.bitmaps)) GROUP BY a.bucketname;

INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result"
SELECT a.bucketname AS `_bucketname`, COLLECT_SET(a.offset) as `_offsets`
FROM (SELECT `_bucketname` AS bucketname, `_offset` AS offset, `_bitmaps` AS bitmaps FROM default__src_src1_index__
        WHERE key = 0) a
  JOIN
     (SELECT `_bucketname` AS bucketname, `_offset` AS offset, `_bitmaps` AS bitmaps FROM default__src_src2_index__
        WHERE value = "val_0") b
  ON
    a.bucketname = b.bucketname AND a.offset = b.offset WHERE NOT
EWAH_BITMAP_EMPTY(EWAH_BITMAP_AND(a.bitmaps, b.bitmaps)) GROUP BY a.bucketname;

SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;

SELECT key, value FROM src WHERE key=0 AND value = "val_0";

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM src WHERE key=0 AND value = "val_0";

DROP INDEX src1_index ON src;
DROP INDEX src2_index ON src;

set hive.mapred.mode=nonstrict;
set hive.stats.autogather=true;

-- SORT_QUERY_RESULTS

-- try the query without indexing, with manual indexing, and with automatic indexing
-- without indexing
SELECT key, value FROM src WHERE key=0 AND value = "val_0";

-- create indices
EXPLAIN
CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
EXPLAIN
CREATE INDEX src2_index ON TABLE src(value) as 'BITMAP' WITH DEFERRED REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
CREATE INDEX src2_index ON TABLE src(value) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src1_index ON src REBUILD;
ALTER INDEX src2_index ON src REBUILD;
SELECT * FROM default__src_src1_index__;
SELECT * FROM default__src_src2_index__;


-- manual indexing
EXPLAIN
SELECT a.bucketname AS `_bucketname`, COLLECT_SET(a.offset) as `_offsets`
FROM (SELECT `_bucketname` AS bucketname, `_offset` AS offset, `_bitmaps` AS bitmaps FROM default__src_src1_index__
       WHERE key = 0) a
 JOIN
    (SELECT `_bucketname` AS bucketname, `_offset` AS offset, `_bitmaps` AS bitmaps FROM default__src_src2_index__
       WHERE value = "val_0") b
 ON
   a.bucketname = b.bucketname AND a.offset = b.offset WHERE NOT
EWAH_BITMAP_EMPTY(EWAH_BITMAP_AND(a.bitmaps, b.bitmaps)) GROUP BY a.bucketname;

INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result"
SELECT a.bucketname AS `_bucketname`, COLLECT_SET(a.offset) as `_offsets`
FROM (SELECT `_bucketname` AS bucketname, `_offset` AS offset, `_bitmaps` AS bitmaps FROM default__src_src1_index__
        WHERE key = 0) a
  JOIN
     (SELECT `_bucketname` AS bucketname, `_offset` AS offset, `_bitmaps` AS bitmaps FROM default__src_src2_index__
        WHERE value = "val_0") b
  ON
    a.bucketname = b.bucketname AND a.offset = b.offset WHERE NOT
EWAH_BITMAP_EMPTY(EWAH_BITMAP_AND(a.bitmaps, b.bitmaps)) GROUP BY a.bucketname;

SELECT key, value FROM src WHERE key=0 AND value = "val_0";


SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SELECT key, value FROM src WHERE key=0 AND value = "val_0";

DROP INDEX src1_index ON src;
DROP INDEX src2_index ON src;

set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;
set hive.fetch.task.conversion=none;

-- SORT_QUERY_RESULTS

-- test automatic use of index on table with partitions
CREATE INDEX src_part_index ON TABLE srcpart(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src_part_index ON srcpart REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;

EXPLAIN SELECT key, value FROM srcpart WHERE key=86 AND ds='2008-04-09';
SELECT key, value FROM srcpart WHERE key=86 AND ds='2008-04-09';

DROP INDEX src_part_index ON srcpart;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;
SET hive.exec.compress.output=true;

-- SORT_QUERY_RESULTS

CREATE INDEX src_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- automatic indexing
EXPLAIN SELECT key, value FROM src WHERE key > 80 AND key < 100;
SELECT key, value FROM src WHERE key > 80 AND key < 100;

DROP INDEX src_index on src;
EXPLAIN
CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.map.aggr=false;
CREATE INDEX src1_index ON TABLE src(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX src1_index ON src REBUILD;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

CREATE TABLE srcpart_rc (key int, value string) PARTITIONED BY (ds string, hr int) STORED AS RCFILE;

INSERT OVERWRITE TABLE srcpart_rc PARTITION (ds='2008-04-08', hr=11) SELECT key, value FROM srcpart WHERE ds = '2008-04-08' AND hr = 11;
INSERT OVERWRITE TABLE srcpart_rc PARTITION (ds='2008-04-08', hr=12) SELECT key, value FROM srcpart WHERE ds = '2008-04-08' AND hr = 12;
INSERT OVERWRITE TABLE srcpart_rc PARTITION (ds='2008-04-09', hr=11) SELECT key, value FROM srcpart WHERE ds = '2008-04-09' AND hr = 11;
INSERT OVERWRITE TABLE srcpart_rc PARTITION (ds='2008-04-09', hr=12) SELECT key, value FROM srcpart WHERE ds = '2008-04-09' AND hr = 12;

EXPLAIN
CREATE INDEX srcpart_rc_index ON TABLE srcpart_rc(key) as 'BITMAP' WITH DEFERRED REBUILD;
CREATE INDEX srcpart_rc_index ON TABLE srcpart_rc(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER INDEX srcpart_rc_index ON srcpart_rc REBUILD;
SELECT x.* FROM default__srcpart_rc_srcpart_rc_index__ x WHERE x.ds = '2008-04-08' and x.hr = 11;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_test_index_result" SELECT `_bucketname`,
COLLECT_SET(`_offset`) as `_offsets` FROM default__srcpart_rc_srcpart_rc_index__
x WHERE NOT EWAH_BITMAP_EMPTY(`_bitmaps`) AND x.key=100 AND x.ds = '2008-04-08' GROUP BY `_bucketname`;
SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_test_index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100 AND ds = '2008-04-08';

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_test_index_result" SELECT `_bucketname` ,
COLLECT_SET(`_offset`) as `_offsets` FROM default__srcpart_rc_srcpart_rc_index__
x WHERE NOT EWAH_BITMAP_EMPTY(`_bitmaps`) AND x.key=100 AND x.ds = '2008-04-08' and x.hr = 11 GROUP BY `_bucketname`;
SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_test_index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100 AND ds = '2008-04-08' and hr = 11;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100 AND ds = '2008-04-08' and hr = 11;

DROP INDEX srcpart_rc_index on srcpart_rc;

EXPLAIN
CREATE INDEX srcpart_rc_index ON TABLE srcpart_rc(key) as 'BITMAP' WITH DEFERRED REBUILD;
CREATE INDEX srcpart_rc_index ON TABLE srcpart_rc(key) as 'BITMAP' WITH DEFERRED REBUILD;
ALTER  INDEX srcpart_rc_index ON srcpart_rc REBUILD;
SELECT x.* FROM default__srcpart_rc_srcpart_rc_index__ x WHERE x.key = 100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname` ,
COLLECT_SET(`_offset`) as `_offsets` FROM default__srcpart_rc_srcpart_rc_index__
WHERE NOT EWAH_BITMAP_EMPTY(`_bitmaps`) AND key=100 GROUP BY `_bucketname`;
SET hive.index.blockfilter.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100;

DROP INDEX srcpart_rc_index on srcpart_rc;
DROP TABLE srcpart_rc;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

DROP INDEX srcpart_index_proj on srcpart;

EXPLAIN
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) as 'COMPACT' WITH DEFERRED REBUILD;
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX srcpart_index_proj ON srcpart REBUILD;
SELECT x.* FROM default__srcpart_srcpart_index_proj__ x WHERE x.ds = '2008-04-08' and x.hr = 11;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_test_index_result" SELECT `_bucketname` ,  `_offsets` FROM default__srcpart_srcpart_index_proj__ x WHERE x.key=100 AND x.ds = '2008-04-08';
SET hive.index.compact.file=${system:test.tmp.dir}/index_test_index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SELECT key, value FROM srcpart WHERE key=100 AND ds = '2008-04-08';

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_test_index_result" SELECT `_bucketname` ,  `_offsets` FROM default__srcpart_srcpart_index_proj__ x WHERE x.key=100 AND x.ds = '2008-04-08' and x.hr = 11;
SET hive.index.compact.file=${system:test.tmp.dir}/index_test_index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SELECT key, value FROM srcpart WHERE key=100 AND ds = '2008-04-08' and hr = 11;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM srcpart WHERE key=100 AND ds = '2008-04-08' and hr = 11;

DROP INDEX srcpart_index_proj on srcpart;

EXPLAIN
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) as 'COMPACT' WITH DEFERRED REBUILD;
CREATE INDEX srcpart_index_proj ON TABLE srcpart(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER  INDEX srcpart_index_proj ON srcpart REBUILD;
SELECT x.* FROM default__srcpart_srcpart_index_proj__ x;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname` ,  `_offsets` FROM default__srcpart_srcpart_index_proj__ WHERE key=100;
SET hive.index.compact.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SELECT key, value FROM srcpart WHERE key=100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM srcpart WHERE key=100;

DROP INDEX srcpart_index_proj on srcpart;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

EXPLAIN
CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;
SELECT x.* FROM default__src_src_index__ x;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname` ,  `_offsets` FROM default__src_src_index__ WHERE key=100;
SET hive.index.compact.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SELECT key, value FROM src WHERE key=100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM src WHERE key=100;

DROP INDEX src_index on src;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

CREATE TABLE srcpart_rc (key int, value string) PARTITIONED BY (ds string, hr int) STORED AS RCFILE;

INSERT OVERWRITE TABLE srcpart_rc PARTITION (ds='2008-04-08', hr=11) SELECT key, value FROM srcpart WHERE ds = '2008-04-08' AND hr = 11;
INSERT OVERWRITE TABLE srcpart_rc PARTITION (ds='2008-04-08', hr=12) SELECT key, value FROM srcpart WHERE ds = '2008-04-08' AND hr = 12;
INSERT OVERWRITE TABLE srcpart_rc PARTITION (ds='2008-04-09', hr=11) SELECT key, value FROM srcpart WHERE ds = '2008-04-09' AND hr = 11;
INSERT OVERWRITE TABLE srcpart_rc PARTITION (ds='2008-04-09', hr=12) SELECT key, value FROM srcpart WHERE ds = '2008-04-09' AND hr = 12;

CREATE INDEX srcpart_rc_index ON TABLE srcpart_rc(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX srcpart_rc_index ON srcpart_rc REBUILD;
SELECT x.* FROM default__srcpart_rc_srcpart_rc_index__ x WHERE x.ds = '2008-04-08' and x.hr = 11;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_test_index_result" SELECT `_bucketname` ,  `_offsets` FROM default__srcpart_rc_srcpart_rc_index__ x WHERE x.key=100 AND x.ds = '2008-04-08';
SET hive.index.compact.file=${system:test.tmp.dir}/index_test_index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100 AND ds = '2008-04-08';

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_test_index_result" SELECT `_bucketname` ,  `_offsets` FROM default__srcpart_rc_srcpart_rc_index__ x WHERE x.key=100 AND x.ds = '2008-04-08' and x.hr = 11;
SET hive.index.compact.file=${system:test.tmp.dir}/index_test_index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100 AND ds = '2008-04-08' and hr = 11;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100 AND ds = '2008-04-08' and hr = 11;

DROP INDEX srcpart_rc_index on srcpart_rc;

EXPLAIN
CREATE INDEX srcpart_rc_index ON TABLE srcpart_rc(key) as 'COMPACT' WITH DEFERRED REBUILD;
CREATE INDEX srcpart_rc_index ON TABLE srcpart_rc(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER  INDEX srcpart_rc_index ON srcpart_rc REBUILD;
SELECT x.* FROM default__srcpart_rc_srcpart_rc_index__ x;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname` ,  `_offsets` FROM default__srcpart_rc_srcpart_rc_index__ WHERE key=100;
SET hive.index.compact.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM srcpart_rc WHERE key=100;

DROP INDEX srcpart_rc_index on srcpart_rc;
DROP TABLE srcpart_rc;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

CREATE TABLE src_index_test_rc (key int, value string) STORED AS RCFILE;

INSERT OVERWRITE TABLE src_index_test_rc SELECT * FROM src;

CREATE INDEX src_index ON TABLE src_index_test_rc(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src_index_test_rc REBUILD;
SELECT x.* FROM default__src_index_test_rc_src_index__ x;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname` ,  `_offsets` FROM default__src_index_test_rc_src_index__ WHERE key=100;
SET hive.index.compact.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SELECT key, value FROM src_index_test_rc WHERE key=100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT key, value FROM src_index_test_rc WHERE key=100;

DROP INDEX src_index on src_index_test_rc;
DROP TABLE src_index_test_rc;
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.default.fileformat=TextFile;
set hive.stats.dbclass=fs;
CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=1;
SET hive.index.compact.binary.search=true;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook;

SELECT * FROM src WHERE key = '0';

SELECT * FROM src WHERE key < '1';

SELECT * FROM src WHERE key <= '0';

SELECT * FROM src WHERE key > '8';

SELECT * FROM src WHERE key >= '9';

SET hive.exec.post.hooks=;

DROP INDEX src_index ON src;

SET hive.default.fileformat=RCFILE;

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook;

SELECT * FROM src WHERE key = '0';

SELECT * FROM src WHERE key < '1';

SELECT * FROM src WHERE key <= '0';

SELECT * FROM src WHERE key > '8';

SELECT * FROM src WHERE key >= '9';

SET hive.exec.post.hooks=;

DROP INDEX src_index ON src;

SET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
SET hive.default.fileformat=TextFile;

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook;

SELECT * FROM src WHERE key = '0';

SELECT * FROM src WHERE key < '1';

SELECT * FROM src WHERE key <= '0';

SELECT * FROM src WHERE key > '8';

SELECT * FROM src WHERE key >= '9';

SET hive.exec.post.hooks=;

DROP INDEX src_index ON src;

SET hive.default.fileformat=RCFILE;

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook;

SELECT * FROM src WHERE key = '0';

SELECT * FROM src WHERE key < '1';

SELECT * FROM src WHERE key <= '0';

SELECT * FROM src WHERE key > '8';

SELECT * FROM src WHERE key >= '9';

SET hive.exec.post.hooks=;

DROP INDEX src_index ON src;

SET hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
SET hive.default.fileformat=TextFile;

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook;

SELECT * FROM src WHERE key = '0';

SELECT * FROM src WHERE key < '1';

SELECT * FROM src WHERE key <= '0';

SELECT * FROM src WHERE key > '8';

SELECT * FROM src WHERE key >= '9';

SET hive.exec.post.hooks=;

DROP INDEX src_index ON src;

SET hive.default.fileformat=RCFILE;

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyHiveSortedInputFormatUsedHook;

SELECT * FROM src WHERE key = '0';

SELECT * FROM src WHERE key < '1';

SELECT * FROM src WHERE key <= '0';

SELECT * FROM src WHERE key > '8';

SELECT * FROM src WHERE key >= '9';

SET hive.exec.post.hooks=;

DROP INDEX src_index ON src;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;
drop index src_index on src;

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname` ,  `_offsets` FROM default__src_src_index__ WHERE key<1000;
SET hive.index.compact.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SET hive.index.compact.query.max.entries=5;
SELECT key, value FROM src WHERE key=100 ORDER BY key;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;
drop index src_index on src;

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_result" SELECT `_bucketname` ,  `_offsets` FROM default__src_src_index__ WHERE key<1000;
SET hive.index.compact.file=${system:test.tmp.dir}/index_result;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;
SET hive.index.compact.query.max.size=1024;
SELECT key, value FROM src WHERE key=100 ORDER BY key;

set hive.mapred.mode=nonstrict;
SET hive.exec.compress.output=true;
SET hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS

CREATE INDEX src_index ON TABLE src(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON src REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- automatic indexing
EXPLAIN SELECT key, value FROM src WHERE key > 80 AND key < 100;
SELECT key, value FROM src WHERE key > 80 AND key < 100;

DROP INDEX src_index on src;
set hive.stats.dbclass=fs;
drop index src_index_2 on src;
drop index src_index_3 on src;
drop index src_index_4 on src;
drop index src_index_5 on src;
drop index src_index_6 on src;
drop index src_index_7 on src;
drop index src_index_8 on src;
drop index src_index_9 on src;
drop table `_t`;

create index src_index_2 on table src(key) as 'compact' WITH DEFERRED REBUILD;
desc extended default__src_src_index_2__;

create index src_index_3 on table src(key) as 'compact' WITH DEFERRED REBUILD in table src_idx_src_index_3;
desc extended src_idx_src_index_3;

create index src_index_4 on table src(key) as 'compact' WITH DEFERRED REBUILD ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;
desc extended default__src_src_index_4__;

create index src_index_5 on table src(key) as 'compact' WITH DEFERRED REBUILD ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' ESCAPED BY '\\';
desc extended default__src_src_index_5__;

create index src_index_6 on table src(key) as 'compact' WITH DEFERRED REBUILD STORED AS RCFILE;
desc extended default__src_src_index_6__;

create index src_index_7 on table src(key) as 'compact' WITH DEFERRED REBUILD in table src_idx_src_index_7 STORED AS RCFILE;
desc extended src_idx_src_index_7;

create index src_index_8 on table src(key) as 'compact' WITH DEFERRED REBUILD IDXPROPERTIES ("prop1"="val1", "prop2"="val2");
desc extended default__src_src_index_8__;

create index src_index_9 on table src(key) as 'compact' WITH DEFERRED REBUILD TBLPROPERTIES ("prop1"="val1", "prop2"="val2");
desc extended default__src_src_index_9__;

create table `_t`(`_i` int, `_j` int);
create index x on table `_t`(`_j`) as 'compact' WITH DEFERRED REBUILD;
alter index x on `_t` rebuild;

create index x2 on table `_t`(`_i`,`_j`) as 'compact' WITH DEFERRED
REBUILD;
alter index x2 on `_t` rebuild;

drop index src_index_2 on src;
drop index src_index_3 on src;
drop index src_index_4 on src;
drop index src_index_5 on src;
drop index src_index_6 on src;
drop index src_index_7 on src;
drop index src_index_8 on src;
drop index src_index_9 on src;
drop table `_t`;

show tables;
set hive.optimize.index.filter=true;
drop database if exists index_test_db cascade;
-- Test selecting selecting from a table that is backed by an index
-- create table, index in a db, then set default db as current db, and try selecting

create database index_test_db;

use index_test_db;
create table testtb (id int, name string);
create index id_index on table testtb (id) as 'COMPACT' WITH DEFERRED REBUILD  in table testdb_id_idx_tb;

use default;
select * from index_test_db.testtb where id>2;
set hive.stats.dbclass=fs;

-- SORT_QUERY_RESULTS
-- Want to ensure we can build and use indices on tables stored with SerDes
-- Build the (Avro backed) table
CREATE TABLE doctors
ROW FORMAT
SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
TBLPROPERTIES ('avro.schema.literal'='{
  "namespace": "testing.hive.avro.serde",
  "name": "doctors",
  "type": "record",
  "fields": [
    {
      "name":"number",
      "type":"int",
      "doc":"Order of playing the role"
    },
    {
      "name":"first_name",
      "type":"string",
      "doc":"first name of actor playing role"
    },
    {
      "name":"last_name",
      "type":"string",
      "doc":"last name of actor playing role"
    }
  ]
}');

DESCRIBE doctors;

LOAD DATA LOCAL INPATH '../../data/files/doctors.avro' INTO TABLE doctors;

-- Create and build an index
CREATE INDEX doctors_index ON TABLE doctors(number) AS 'COMPACT' WITH DEFERRED REBUILD;
DESCRIBE EXTENDED default__doctors_doctors_index__;
ALTER INDEX doctors_index ON doctors REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

EXPLAIN SELECT * FROM doctors WHERE number > 6;
SELECT * FROM doctors WHERE number > 6;

DROP INDEX doctors_index ON doctors;
DROP TABLE doctors;
set hive.mapred.mode=nonstrict;
-- Test creating an index on skewed table

-- Create a skew table
CREATE TABLE kv(key STRING, value STRING) SKEWED BY (key) ON ((3), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE kv;

-- Create and build an index
CREATE INDEX kv_index ON TABLE kv(value) AS 'COMPACT' WITH DEFERRED REBUILD;
DESCRIBE FORMATTED default__kv_kv_index__;
ALTER INDEX kv_index ON kv REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- Run a query that uses the index
EXPLAIN SELECT * FROM kv WHERE value > '15' ORDER BY value;
SELECT * FROM kv WHERE value > '15' ORDER BY value;

DROP INDEX kv_index ON kv;
DROP TABLE kv;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;
-- test that stale indexes are not used

CREATE TABLE temp(key STRING, val STRING) STORED AS TEXTFILE;
INSERT OVERWRITE TABLE temp SELECT * FROM src WHERE key < 50;

-- Build an index on temp.
CREATE INDEX temp_index ON TABLE temp(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX temp_index ON temp REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- overwrite temp table so index is out of date
INSERT OVERWRITE TABLE temp SELECT * FROM src;

-- should return correct results bypassing index
EXPLAIN SELECT * FROM temp WHERE key  = 86;
SELECT * FROM temp WHERE key  = 86;
DROP table temp;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;
-- Test if index is actually being used.

-- Create temp, and populate it with some values in src.
CREATE TABLE temp(key STRING, val STRING) PARTITIONED BY (foo string) STORED AS TEXTFILE;
ALTER TABLE temp ADD PARTITION (foo = 'bar');
INSERT OVERWRITE TABLE temp PARTITION (foo = 'bar') SELECT * FROM src WHERE key < 50;

-- Build an index on temp.
CREATE INDEX temp_index ON TABLE temp(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX temp_index ON temp PARTITION (foo = 'bar') REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- overwrite temp table so index is out of date
INSERT OVERWRITE TABLE temp PARTITION (foo = 'bar') SELECT * FROM src;

-- query should not return any values
SELECT * FROM default__temp_temp_index__ WHERE key = 86 AND foo='bar';
EXPLAIN SELECT * FROM temp WHERE key  = 86 AND foo = 'bar';
SELECT * FROM temp WHERE key  = 86 AND foo = 'bar';

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=false;
DROP table temp;
set hive.mapred.mode=nonstrict;
set hive.exec.infer.bucket.sort=true;

-- This tests inferring how data is bucketed/sorted from the operators in the reducer
-- and populating that information in partitions' metadata

CREATE TABLE test_table (key STRING, value STRING) PARTITIONED BY (part STRING);

-- Test group by, should be bucketed and sorted by group by key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, count(*) FROM src GROUP BY key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by where a key isn't selected, should not be bucketed or sorted
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, count(*) FROM src GROUP BY key, value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join, should be bucketed and sorted by join key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, a.value FROM src a JOIN src b ON a.key = b.key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join with two keys, should be bucketed and sorted by join keys
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, a.value FROM src a JOIN src b ON a.key = b.key AND a.value = b.value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join with two keys and only one selected, should not be bucketed or sorted
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, '1' FROM src a JOIN src b ON a.key = b.key AND a.value = b.value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join on three tables on same key, should be bucketed and sorted by join key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, c.value FROM src a JOIN src b ON (a.key = b.key) JOIN src c ON (b.key = c.key);

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join on three tables on different keys, should be bucketed and sorted by latter key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, c.value FROM src a JOIN src b ON (a.key = b.key) JOIN src c ON (b.value = c.value);

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test distribute by, should only be bucketed by key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM src DISTRIBUTE BY key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test sort by, should be sorted by key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM src SORT BY key ASC;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test sort by desc, should be sorted by key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM src SORT BY key DESC;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test cluster by, should be bucketed and sorted by key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM src CLUSTER BY key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test distribute by and sort by different keys, should be bucketed by one key sorted by the other
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM src DISTRIBUTE BY key SORT BY value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join in simple subquery, should be bucketed and sorted on key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value from (SELECT a.key, b.value FROM src a JOIN src b ON (a.key = b.key)) subq;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join in simple subquery renaming key column, should be bucketed and sorted on key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT k, value FROM (SELECT a.key as k, b.value FROM src a JOIN src b ON (a.key = b.key)) subq;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in simple subquery, should be bucketed and sorted on key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, cnt from (SELECT key, count(*) as cnt FROM src GROUP BY key) subq;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in simple subquery renaming key column, should be bucketed and sorted on key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT k, cnt FROM (SELECT key as k, count(*) as cnt FROM src GROUP BY key) subq;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery with where outside, should still be bucketed and sorted on key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM (SELECT key, count(1) AS value FROM src group by key) a where key < 10;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery with expression on value, should still be bucketed and sorted on key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value + 1 FROM (SELECT key, count(1) AS value FROM src group by key) a where key < 10;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery with lateral view outside, should still be bucketed and sorted
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM (SELECT key FROM src group by key) a lateral view explode(array(1, 2)) value as value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery with another group by outside, should be bucketed and sorted by the
-- key of the outer group by
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT count(1), value FROM (SELECT key, count(1) as value FROM src group by key) a group by value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery with select on outside reordering the columns, should be bucketed and
-- sorted by the column the group by key ends up in
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT value, key FROM (SELECT key, count(1) as value FROM src group by key) a;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery followed by distribute by, should only be bucketed by the distribute key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM (SELECT key, count(1) as value FROM src group by key) a distribute by key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery followed by sort by, should only be sorted by the sort key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM (SELECT key, count(1) as value FROM src group by key) a sort by key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery followed by transform script, should not be bucketed or sorted
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT TRANSFORM (a.key, a.value) USING 'cat' AS (key, value) FROM (SELECT key, count(1) AS value FROM src GROUP BY KEY) a;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by on function, should be bucketed and sorted by key and value because the function is applied in the mapper
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, value FROM (SELECT concat(key, "a") AS key, value, count(*)  FROM src GROUP BY concat(key, "a"), value) a;

DESCRIBE FORMATTED test_table PARTITION (part = '1');


set hive.exec.infer.bucket.sort=true;

-- Test writing to a bucketed table, the output should be bucketed by the bucketing key into the
-- a number of files equal to the number of buckets
CREATE TABLE test_table_bucketed (key STRING, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (value) SORTED BY (value) INTO 3 BUCKETS;

-- Despite the fact that normally inferring would say this table is bucketed and sorted on key,
-- this should be bucketed and sorted by value into 3 buckets
INSERT OVERWRITE TABLE test_table_bucketed PARTITION (part = '1')
SELECT key, count(1) FROM src GROUP BY KEY;

DESCRIBE FORMATTED test_table_bucketed PARTITION (part = '1');

-- If the count(*) from sampling the buckets matches the count(*) from each file, the table is
-- bucketed
SELECT COUNT(*) FROM test_table_bucketed TABLESAMPLE (BUCKET 1 OUT OF 3) WHERE part = '1';

SELECT COUNT(*) FROM test_table_bucketed TABLESAMPLE (BUCKET 2 OUT OF 3) WHERE part = '1';

SELECT COUNT(*) FROM test_table_bucketed TABLESAMPLE (BUCKET 3 OUT OF 3) WHERE part = '1';

SELECT cnt FROM (SELECT INPUT__FILE__NAME, COUNT(*) cnt FROM test_table_bucketed WHERE part = '1'
GROUP BY INPUT__FILE__NAME ORDER BY INPUT__FILE__NAME ASC LIMIT 3) a;set hive.mapred.mode=nonstrict;
set hive.exec.infer.bucket.sort=true;
set hive.exec.infer.bucket.sort.num.buckets.power.two=true;
set hive.auto.convert.join=true;

-- This tests inferring how data is bucketed/sorted from the operators in the reducer
-- and populating that information in partitions' metadata.  In particular, those cases
-- where joins may be auto converted to map joins.

CREATE TABLE test_table (key STRING, value STRING) PARTITIONED BY (part STRING);

-- Tests a join which is converted to a map join, the output should be neither bucketed nor sorted
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, b.value FROM src a JOIN src b ON a.key = b.key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

set hive.mapjoin.check.memory.rows=1;
set hive.mapjoin.localtask.max.memory.usage = 0.0001;
set hive.auto.convert.join.noconditionaltask = false;

-- This test tests the scenario when the mapper dies. So, create a conditional task for the mapjoin.
-- Tests a join which is not converted to a map join, the output should be bucketed and sorted.

INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, b.value FROM src a JOIN src b ON a.key = b.key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');
set hive.mapred.mode=nonstrict;
set hive.exec.infer.bucket.sort=true;
set hive.exec.infer.bucket.sort.num.buckets.power.two=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- This tests inferring how data is bucketed/sorted from the operators in the reducer
-- and populating that information in partitions' metadata.  In particular, those cases
-- where dynamic partitioning is used.

CREATE TABLE test_table LIKE srcpart;
ALTER TABLE test_table SET FILEFORMAT RCFILE;

-- Simple case, this should not be bucketed or sorted

INSERT OVERWRITE TABLE test_table PARTITION (ds, hr)
SELECT key, value, ds, hr FROM srcpart
WHERE ds = '2008-04-08';

DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='11');
DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='12');

-- This should not be bucketed or sorted since the partition keys are in the set of bucketed
-- and sorted columns for the output

INSERT OVERWRITE TABLE test_table PARTITION (ds, hr)
SELECT key, COUNT(*), ds, hr FROM srcpart
WHERE ds = '2008-04-08'
GROUP BY key, ds, hr;

DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='11');
DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='12');

-- Both partitions should be bucketed and sorted by key

INSERT OVERWRITE TABLE test_table PARTITION (ds, hr)
SELECT key, value, '2008-04-08', IF (key % 2 == 0, '11', '12') FROM
(SELECT key, COUNT(*) AS value FROM srcpart
WHERE ds = '2008-04-08'
GROUP BY key) a;

DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='11');
DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='12');

CREATE TABLE srcpart_merge_dp LIKE srcpart;

CREATE TABLE srcpart_merge_dp_rc LIKE srcpart;
ALTER TABLE srcpart_merge_dp_rc SET FILEFORMAT RCFILE;

LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp PARTITION(ds='2008-04-08', hr=11);
LOAD DATA LOCAL INPATH '../../data/files/srcbucket21.txt' INTO TABLE srcpart_merge_dp PARTITION(ds='2008-04-08', hr=11);
LOAD DATA LOCAL INPATH '../../data/files/srcbucket22.txt' INTO TABLE srcpart_merge_dp PARTITION(ds='2008-04-08', hr=11);
LOAD DATA LOCAL INPATH '../../data/files/srcbucket23.txt' INTO TABLE srcpart_merge_dp PARTITION(ds='2008-04-08', hr=11);

LOAD DATA LOCAL INPATH '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp PARTITION(ds='2008-04-08', hr=12);

INSERT OVERWRITE TABLE srcpart_merge_dp_rc PARTITION (ds = '2008-04-08', hr)
SELECT key, value, hr FROM srcpart_merge_dp WHERE ds = '2008-04-08';

set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=200;
set hive.exec.compress.output=false;
set hive.exec.dynamic.partition=true;
set mapred.reduce.tasks=2;

-- Tests dynamic partitions where bucketing/sorting can be inferred, but some partitions are
-- merged and some are moved.  Currently neither should be bucketed or sorted, in the future,
-- (ds='2008-04-08', hr='12') may be bucketed and sorted, (ds='2008-04-08', hr='11') should
-- definitely not be.

EXPLAIN
INSERT OVERWRITE TABLE test_table PARTITION (ds = '2008-04-08', hr)
SELECT key, value, IF (key % 100 == 0, '11', '12') FROM
(SELECT key, COUNT(*) AS value FROM srcpart
WHERE ds = '2008-04-08'
GROUP BY key) a;

INSERT OVERWRITE TABLE test_table PARTITION (ds = '2008-04-08', hr)
SELECT key, value, IF (key % 100 == 0, '11', '12') FROM
(SELECT key, COUNT(*) AS value FROM srcpart
WHERE ds = '2008-04-08'
GROUP BY key) a;

DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='11');
DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='12');
set hive.mapred.mode=nonstrict;
set hive.exec.infer.bucket.sort=true;

-- This tests inferring how data is bucketed/sorted from the operators in the reducer
-- and populating that information in partitions' metadata, in particular, this tests
-- the grouping operators rollup/cube/grouping sets

CREATE TABLE test_table_out (key STRING, value STRING, agg STRING) PARTITIONED BY (part STRING);

CREATE TABLE test_table_out_2 (key STRING, value STRING, grouping_key STRING, agg STRING) PARTITIONED BY (part STRING);

-- Test rollup, should not be bucketed or sorted because its missing the grouping ID
EXPLAIN INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT key, value, count(1) FROM src GROUP BY key, value WITH ROLLUP;

INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT key, value, count(1) FROM src GROUP BY key, value WITH ROLLUP;

DESCRIBE FORMATTED test_table_out PARTITION (part = '1');

-- Test rollup, should be bucketed and sorted on key, value, grouping_key

INSERT OVERWRITE TABLE test_table_out_2 PARTITION (part = '1')
SELECT key, value, GROUPING__ID, count(1) FROM src GROUP BY key, value WITH ROLLUP;

DESCRIBE FORMATTED test_table_out_2 PARTITION (part = '1');

-- Test cube, should not be bucketed or sorted because its missing the grouping ID
EXPLAIN INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT key, value, count(1) FROM src GROUP BY key, value WITH CUBE;

INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT key, value, count(1) FROM src GROUP BY key, value WITH CUBE;

DESCRIBE FORMATTED test_table_out PARTITION (part = '1');

-- Test cube, should be bucketed and sorted on key, value, grouping_key

INSERT OVERWRITE TABLE test_table_out_2 PARTITION (part = '1')
SELECT key, value, GROUPING__ID, count(1) FROM src GROUP BY key, value WITH CUBE;

DESCRIBE FORMATTED test_table_out_2 PARTITION (part = '1');

-- Test grouping sets, should not be bucketed or sorted because its missing the grouping ID
EXPLAIN INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT key, value, count(1) FROM src GROUP BY key, value GROUPING SETS (key, value);

INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT key, value, count(1) FROM src GROUP BY key, value GROUPING SETS (key, value);

DESCRIBE FORMATTED test_table_out PARTITION (part = '1');

-- Test grouping sets, should be bucketed and sorted on key, value, grouping_key

INSERT OVERWRITE TABLE test_table_out_2 PARTITION (part = '1')
SELECT key, value, GROUPING__ID, count(1) FROM src GROUP BY key, value GROUPING SETS (key, value);

DESCRIBE FORMATTED test_table_out_2 PARTITION (part = '1');
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This tests that bucketing/sorting metadata is not inferred for tables with list bucketing

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

-- create a skewed table
CREATE TABLE list_bucketing_table (key STRING, value STRING)
PARTITIONED BY (part STRING)
SKEWED BY (key) ON ("484")
STORED AS DIRECTORIES;

-- Tests group by, the output should neither be bucketed nor sorted

INSERT OVERWRITE TABLE list_bucketing_table PARTITION (part = '1')
SELECT key, count(*) FROM src GROUP BY key;

DESC FORMATTED list_bucketing_table PARTITION (part = '1');

-- create a table skewed on a key which doesnt exist in the data
CREATE TABLE list_bucketing_table2 (key STRING, value STRING)
PARTITIONED BY (part STRING)
SKEWED BY (key) ON ("abc")
STORED AS DIRECTORIES;

-- should not be bucketed or sorted
INSERT OVERWRITE TABLE list_bucketing_table2 PARTITION (part = '1')
SELECT key, count(*) FROM src GROUP BY key;

DESC FORMATTED list_bucketing_table2 PARTITION (part = '1');
set hive.mapred.mode=nonstrict;
set hive.exec.infer.bucket.sort=true;
;


-- This tests inferring how data is bucketed/sorted from the operators in the reducer
-- and populating that information in partitions' metadata, in particular, this tests
-- that operators in the mapper have no effect

CREATE TABLE test_table1 (key STRING, value STRING)
CLUSTERED BY (key) SORTED BY (key DESC) INTO 2 BUCKETS;

CREATE TABLE test_table2 (key STRING, value STRING)
CLUSTERED BY (key) SORTED BY (key DESC) INTO 2 BUCKETS;

INSERT OVERWRITE TABLE test_table1 SELECT key, value FROM src;

INSERT OVERWRITE TABLE test_table2 SELECT key, value FROM src;

CREATE TABLE test_table_out (key STRING, value STRING) PARTITIONED BY (part STRING);

set hive.map.groupby.sorted=true;

-- Test map group by doesn't affect inference, should not be bucketed or sorted
EXPLAIN INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT key, count(*) FROM test_table1 GROUP BY key;

INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT key, count(*) FROM test_table1 GROUP BY key;

DESCRIBE FORMATTED test_table_out PARTITION (part = '1');

-- Test map group by doesn't affect inference, should be bucketed and sorted by value
EXPLAIN INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT a.key, a.value FROM (
	SELECT key, count(*) AS value FROM test_table1 GROUP BY key
) a JOIN (
 	SELECT key, value FROM src
) b
ON (a.value = b.value);

INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT a.key, a.value FROM (
	SELECT key, cast(count(*) AS STRING) AS value FROM test_table1 GROUP BY key
) a JOIN (
 	SELECT key, value FROM src
) b
ON (a.value = b.value);

DESCRIBE FORMATTED test_table_out PARTITION (part = '1');

set hive.map.groupby.sorted=false;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

-- Test SMB join doesn't affect inference, should not be bucketed or sorted
EXPLAIN INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT /*+ MAPJOIN(a) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key;

INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT /*+ MAPJOIN(a) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key;

DESCRIBE FORMATTED test_table_out PARTITION (part = '1');

-- Test SMB join doesn't affect inference, should be bucketed and sorted by key
EXPLAIN INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT /*+ MAPJOIN(a) */ b.value, count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
GROUP BY b.value;

INSERT OVERWRITE TABLE test_table_out PARTITION (part = '1')
SELECT /*+ MAPJOIN(a) */ b.value, count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
GROUP BY b.value;

DESCRIBE FORMATTED test_table_out PARTITION (part = '1');

set hive.mapred.mode=nonstrict;
set hive.exec.infer.bucket.sort=true;
set hive.exec.infer.bucket.sort.num.buckets.power.two=true;
set hive.merge.mapredfiles=true;
set mapred.reduce.tasks=2;

-- This tests inferring how data is bucketed/sorted from the operators in the reducer
-- and populating that information in partitions' metadata.  In particular, those cases
-- where where merging may or may not be used.

CREATE TABLE test_table (key STRING, value STRING) PARTITIONED BY (part STRING);

-- Tests a reduce task followed by a merge.  The output should be neither bucketed nor sorted.
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, b.value FROM src a JOIN src b ON a.key = b.key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

set hive.merge.smallfiles.avgsize=2;
set hive.exec.compress.output=false;

-- Tests a reduce task followed by a move. The output should be bucketed and sorted.
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, b.value FROM src a JOIN src b ON a.key = b.key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');
set hive.exec.infer.bucket.sort=true;
set hive.exec.infer.bucket.sort.num.buckets.power.two=true;
set hive.stats.dbclass=fs;

-- This tests inferring how data is bucketed/sorted from the operators in the reducer
-- and populating that information in partitions' metadata.  In particular, those cases
-- where multi insert is used.

CREATE TABLE test_table (key STRING, value STRING) PARTITIONED BY (part STRING);

-- Simple case, neither partition should be bucketed or sorted

FROM src
INSERT OVERWRITE TABLE test_table PARTITION (part = '1') SELECT key, value
INSERT OVERWRITE TABLE test_table PARTITION (part = '2') SELECT value, key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');
DESCRIBE FORMATTED test_table PARTITION (part = '2');

-- The partitions should be bucketed and sorted by different keys

FROM src
INSERT OVERWRITE TABLE test_table PARTITION (part = '1') SELECT key, COUNT(*) GROUP BY key
INSERT OVERWRITE TABLE test_table PARTITION (part = '2') SELECT COUNT(*), value GROUP BY value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');
DESCRIBE FORMATTED test_table PARTITION (part = '2');

-- The first partition should be bucketed and sorted, the second should not

FROM src
INSERT OVERWRITE TABLE test_table PARTITION (part = '1') SELECT key, COUNT(*) GROUP BY key
INSERT OVERWRITE TABLE test_table PARTITION (part = '2') SELECT key, value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');
DESCRIBE FORMATTED test_table PARTITION (part = '2');

set hive.multigroupby.singlereducer=true;

-- Test the multi group by single reducer optimization
-- Both partitions should be bucketed by key
FROM src
INSERT OVERWRITE TABLE test_table PARTITION (part = '1') SELECT key, COUNT(*) GROUP BY key
INSERT OVERWRITE TABLE test_table PARTITION (part = '2') SELECT key, SUM(SUBSTR(value, 5)) GROUP BY key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');
DESCRIBE FORMATTED test_table PARTITION (part = '2');
set hive.exec.infer.bucket.sort=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.reduce.tasks=2;

CREATE TABLE test_table (key INT, value STRING) PARTITIONED BY (ds STRING, hr STRING);

-- Tests dynamic partitions where bucketing/sorting can be inferred, but not all reducers write
-- all partitions.  The subquery produces rows as follows
-- key = 0:
--    0, <value>, 0
-- key = 1:
--    0, <value>, 1
-- key = 2:
--    1, <value>, 0
-- This means that by distributing by the first column into two reducers, and using the third
-- columns as a dynamic partition, the dynamic partition for 0 will get written in both reducers
-- and the partition for 1 will get written in one reducer.  So hr=0 should be bucketed by key
-- and hr=1 should not.

EXPLAIN
INSERT OVERWRITE TABLE test_table PARTITION (ds = '2008-04-08', hr)
SELECT key2, value, cast(hr as int) FROM
(SELECT if ((key % 3) < 2, 0, 1) as key2, value, (key % 2) as hr
FROM srcpart
WHERE ds = '2008-04-08') a
DISTRIBUTE BY key2;

INSERT OVERWRITE TABLE test_table PARTITION (ds = '2008-04-08', hr)
SELECT key2, value, cast(hr as int) FROM
(SELECT if ((key % 3) < 2, 0, 1) as key2, value, (key % 3 % 2) as hr
FROM srcpart
WHERE ds = '2008-04-08') a
DISTRIBUTE BY key2;

DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='0');
DESCRIBE FORMATTED test_table PARTITION (ds='2008-04-08', hr='1');
set hive.mapred.mode=nonstrict;
set hive.exec.infer.bucket.sort=true;
set hive.exec.infer.bucket.sort.num.buckets.power.two=true;
set hive.exec.reducers.bytes.per.reducer=2500;

-- This tests inferring how data is bucketed/sorted from the operators in the reducer
-- and populating that information in partitions' metadata, it also verifies that the
-- number of reducers chosen will be a power of two

CREATE TABLE test_table (key STRING, value STRING) PARTITIONED BY (part STRING);

-- Test group by, should be bucketed and sorted by group by key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT key, count(*) FROM src GROUP BY key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join, should be bucketed and sorted by join key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, a.value FROM src a JOIN src b ON a.key = b.key;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join with two keys, should be bucketed and sorted by join keys
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, a.value FROM src a JOIN src b ON a.key = b.key AND a.value = b.value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join on three tables on same key, should be bucketed and sorted by join key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, c.value FROM src a JOIN src b ON (a.key = b.key) JOIN src c ON (b.key = c.key);

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test join on three tables on different keys, should be bucketed and sorted by latter key
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT a.key, c.value FROM src a JOIN src b ON (a.key = b.key) JOIN src c ON (b.value = c.value);

DESCRIBE FORMATTED test_table PARTITION (part = '1');

-- Test group by in subquery with another group by outside, should be bucketed and sorted by the
-- key of the outer group by
INSERT OVERWRITE TABLE test_table PARTITION (part = '1')
SELECT count(1), value FROM (SELECT key, count(1) as value FROM src group by key) a group by value;

DESCRIBE FORMATTED test_table PARTITION (part = '1');
set hive.mapred.mode=nonstrict;
DROP TABLE infertypes;
CREATE TABLE infertypes(ti TINYINT, si SMALLINT, i INT, bi BIGINT, fl FLOAT, db DOUBLE, str STRING);

LOAD DATA LOCAL INPATH '../../data/files/infer_const_type.txt' OVERWRITE INTO TABLE infertypes;

SELECT * FROM infertypes;

EXPLAIN SELECT * FROM infertypes WHERE
  ti  = '127' AND
  si  = 32767 AND
  i   = '12345' AND
  bi  = '-12345' AND
  fl  = '0906' AND
  db  = '-307' AND
  str = 1234;

SELECT * FROM infertypes WHERE
  ti  = '127' AND
  si  = 32767 AND
  i   = '12345' AND
  bi  = '-12345' AND
  fl  = '0906' AND
  db  = '-307' AND
  str = 1234;

-- all should return false as all numbers exceeed the largest number
-- which could be represented by the corresponding type
-- and string_col = long_const should return false
EXPLAIN SELECT * FROM infertypes WHERE
  ti  = '128' OR
  si  = 32768 OR
  i   = '2147483648' OR
  bi  = '9223372036854775808' OR
  fl  = 'float' OR
  db  = 'double';

SELECT * FROM infertypes WHERE
  ti  = '128' OR
  si  = 32768 OR
  i   = '2147483648' OR
  bi  = '9223372036854775808' OR
  fl  = 'float' OR
  db  = 'double';

-- for the query like: int_col = double, should return false
EXPLAIN SELECT * FROM infertypes WHERE
  ti  = '127.0' OR
  si  = 327.0 OR
  i   = '-100.0';

SELECT * FROM infertypes WHERE
  ti  = '127.0' OR
  si  = 327.0 OR
  i   = '-100.0';

EXPLAIN SELECT * FROM infertypes WHERE
  ti < '127.0' AND
  i > '100.0' AND
  str = 1.57;

SELECT * FROM infertypes WHERE
  ti < '127.0' AND
  i > '100.0' AND
  str = 1.57;

DROP TABLE infertypes;
-- tbl_created_by_init is supposed to have been created for us
-- automatically by test_init_file.sql

select * from tbl_created_by_init;

set hive.mapred.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src src1 INNER JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

FROM src src1 INNER JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

SELECT dest_j1.* FROM dest_j1;

-- verify that INNER is a non-reserved word for backwards compatibility
-- change from HIVE-6617, inner is a SQL2011 reserved keyword.
create table `inner`(i int);

select i from `inner`;

create table i(`inner` int);

select `inner` from i;

explain select * from (select * from src) `inner` left outer join src
on `inner`.key=src.key;
create table test (a int) stored as inputformat 'org.apache.hadoop.hive.ql.io.RCFileInputFormat' outputformat 'org.apache.hadoop.hive.ql.io.RCFileOutputFormat' inputdriver 'RCFileInDriver' outputdriver 'RCFileOutDriver';
desc extended test;
EXPLAIN
SELECT x.* FROM SRC x;

SELECT x.* FROM SRC x;
EXPLAIN
SELECT * FROM src;

SELECT * FROM src;

CREATE TABLE TEST1(A INT, B DOUBLE) STORED AS TEXTFILE;

EXPLAIN
DESCRIBE TEST1;

DESCRIBE TEST1;



FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100
SELECT a.* FROM src;
CREATE TABLE TEST10(key INT, value STRING) PARTITIONED BY(ds STRING, hr STRING) STORED AS TEXTFILE;

EXPLAIN
DESCRIBE TEST10;

DESCRIBE TEST10;



set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100 LIMIT 10;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100 LIMIT 10;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set mapreduce.framework.name=yarn;
set mapreduce.jobtracker.address=localhost:58;
set hive.exec.mode.local.auto=true;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE dest2(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE dest3(key INT) PARTITIONED BY(ds STRING, hr STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key >= 200;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key >= 200;

SELECT dest1.* FROM dest1;
SELECT dest2.* FROM dest2;
SELECT dest3.* FROM dest3;
set mapred.job.tracker=localhost:58;
set hive.exec.mode.local.auto=true;

-- INCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE dest2(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE dest3(key INT) PARTITIONED BY(ds STRING, hr STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key >= 200;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key >= 200;

SELECT dest1.* FROM dest1;
SELECT dest2.* FROM dest2;
SELECT dest3.* FROM dest3;
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE dest2(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE dest3(key INT) PARTITIONED BY(ds STRING, hr STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key >= 200 and src.key < 300
INSERT OVERWRITE DIRECTORY 'target/warehouse/dest4.out' SELECT src.value WHERE src.key >= 300;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key >= 200 and src.key < 300
INSERT OVERWRITE DIRECTORY 'target/warehouse/dest4.out' SELECT src.value WHERE src.key >= 300;

SELECT dest1.* FROM dest1;
SELECT dest2.* FROM dest2;
SELECT dest3.* FROM dest3;
dfs -cat ${system:test.warehouse.dir}/dest4.out/*;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

-- SORT_QUERY_RESULTS

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey LIMIT 20
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey LIMIT 20
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

SELECT dest1.* FROM dest1;
EXPLAIN
CREATE TABLE TEST15(key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;

CREATE TABLE TEST15(key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;

DESCRIBE TEST15;

-- TestSerDe is a user defined serde where the default delimiter is Ctrl-B
DROP TABLE INPUT16;
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
CREATE TABLE INPUT16(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe' STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1_cb.txt' INTO TABLE INPUT16;
SELECT INPUT16.VALUE, INPUT16.KEY FROM INPUT16;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- TestSerDe is a user defined serde where the default delimiter is Ctrl-B
-- the user is overwriting it with ctrlC

DROP TABLE INPUT16_CC;
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;
CREATE TABLE INPUT16_CC(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe'  with serdeproperties ('testserde.default.serialization.format'='\003', 'dummy.prop.not.used'='dummyy.val') STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1_cc.txt' INTO TABLE INPUT16_CC;
SELECT INPUT16_CC.VALUE, INPUT16_CC.KEY FROM INPUT16_CC;

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src_thrift
  SELECT TRANSFORM(src_thrift.aint + src_thrift.lint[0], src_thrift.lintstring[0])
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue;

FROM (
  FROM src_thrift
  SELECT TRANSFORM(src_thrift.aint + src_thrift.lint[0], src_thrift.lintstring[0])
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue;

-- SORT_QUERY_RESULTS

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value, 1+2, 3+4)
         USING 'cat'
  CLUSTER BY key
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.key, regexp_replace(tmap.value,'\t','+') WHERE tmap.key < 100;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value, 1+2, 3+4)
         USING 'cat'
  CLUSTER BY key
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.key, regexp_replace(tmap.value,'\t','+') WHERE tmap.key < 100;

-- SORT_QUERY_RESULTS

SELECT dest1.* FROM dest1;

create table apachelog(ipaddress STRING,identd STRING,user_name STRING,finishtime STRING,requestline string,returncode INT,size INT) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe' WITH SERDEPROPERTIES (  'serialization.format'= 'org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol',  'quote.delim'= '("|\\[|\\])',  'field.delim'=' ',  'serialization.null.format'='-'  ) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/apache.access.log' INTO TABLE apachelog;
SELECT a.* FROM apachelog a;

-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE dest2(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100 LIMIT 10
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key < 100 LIMIT 5;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value WHERE src.key < 100 LIMIT 10
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key < 100 LIMIT 5;

SELECT dest1.* FROM dest1;
SELECT dest2.* FROM dest2;




CREATE TABLE TEST2a(A INT, B DOUBLE) STORED AS TEXTFILE;
DESCRIBE TEST2a;
DESC TEST2a;
CREATE TABLE TEST2b(A ARRAY<INT>, B DOUBLE, C MAP<DOUBLE, INT>) STORED AS TEXTFILE;
DESCRIBE TEST2b;
SHOW TABLES;
DROP TABLE TEST2a;
SHOW TABLES;
DROP TABLE TEST2b;

EXPLAIN
SHOW TABLES;
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key, 2 WHERE src.key >= 200
SELECT a.key FROM src;
FROM (
  FROM src
  MAP src.key % 2, src.key % 5
  USING 'cat'
  CLUSTER BY key
) tmap
REDUCE tmap.key, tmap.value
USING 'uniq -c | sed "s@^ *@@" | sed "s@\t@_@" | sed "s@ @\t@"'
AS key, value
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

ADD FILE ../../data/scripts/input20_script.py;

EXPLAIN
FROM (
  FROM src
  MAP src.key, src.key
  USING 'cat'
  DISTRIBUTE BY key
  SORT BY key, value
) tmap
INSERT OVERWRITE TABLE dest1
REDUCE tmap.key, tmap.value
USING 'python input20_script.py'
AS key, value;

FROM (
  FROM src
  MAP src.key, src.key
  USING 'cat'
  DISTRIBUTE BY key
  SORT BY key, value
) tmap
INSERT OVERWRITE TABLE dest1
REDUCE tmap.key, tmap.value
USING 'python input20_script.py'
AS key, value;

SELECT * FROM dest1 SORT BY key, value;


CREATE TABLE src_null(a STRING, b STRING, c STRING, d STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/null.txt' INTO TABLE src_null;

EXPLAIN SELECT * FROM src_null DISTRIBUTE BY c SORT BY d;

SELECT * FROM src_null DISTRIBUTE BY c SORT BY d;


CREATE TABLE INPUT4(KEY STRING, VALUE STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE INPUT4;

EXPLAIN
SELECT a.KEY2
FROM (SELECT INPUT4.*, INPUT4.KEY as KEY2
      FROM INPUT4) a
ORDER BY KEY2 LIMIT 10;

SELECT a.KEY2
FROM (SELECT INPUT4.*, INPUT4.KEY as KEY2
      FROM INPUT4) a
ORDER BY KEY2 LIMIT 10;


set hive.mapred.mode=nonstrict;
explain extended
 select * from srcpart a join srcpart b where a.ds = '2008-04-08' and a.hr = '11' and b.ds = '2008-04-08' and b.hr = '14' limit 5;

select * from srcpart a join srcpart b where a.ds = '2008-04-08' and a.hr = '11' and b.ds = '2008-04-08' and b.hr = '14' limit 5;


create table tst(a int, b int) partitioned by (d string);
alter table tst add partition (d='2009-01-01');
explain
select count(1) from tst x where x.d='2009-01-01';

select count(1) from tst x where x.d='2009-01-01';



create table tst(a int, b int) partitioned by (d string);
alter table tst add partition (d='2009-01-01');
alter table tst add partition (d='2009-02-02');

explain
select * from (
  select * from (select * from tst x where x.d='2009-01-01' limit 10)a
    union all
  select * from (select * from tst x where x.d='2009-02-02' limit 10)b
) subq;

select * from (
  select * from (select * from tst x where x.d='2009-01-01' limit 10)a
    union all
  select * from (select * from tst x where x.d='2009-02-02' limit 10)b
) subq;


set hive.mapred.mode=nonstrict;
explain
select * from (
  select * from (select * from srcpart a where a.ds = '2008-04-08' and a.hr = '11' order by a.key limit 5)pa
    union all
  select * from (select * from srcpart b where b.ds = '2008-04-08' and b.hr = '14' limit 5)pb
)subq;

select * from (
  select * from (select * from srcpart a where a.ds = '2008-04-08' and a.hr = '11' order by a.key limit 5)pa
    union all
  select * from (select * from srcpart b where b.ds = '2008-04-08' and b.hr = '14' limit 5)pb
)subq;
set hive.mapred.mode=nonstrict;

create table tst(a string, b string) partitioned by (d string);
alter table tst add partition (d='2009-01-01');

insert overwrite table tst partition(d='2009-01-01')
select tst.a, src.value from tst join src ON (tst.a = src.key);

select * from tst where tst.d='2009-01-01';


set hive.mapred.mode=nonstrict;
EXPLAIN
SELECT x.* FROM SRC x WHERE x.key < 300 LIMIT 5;

SELECT x.* FROM SRC x WHERE x.key < 300 LIMIT 5;
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key < 100
INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key >= 100 and src.key < 200
INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key, 2 WHERE src.key >= 200 and src.key < 300
INSERT OVERWRITE DIRECTORY '../../../../build/contrib/hive/ql/test/data/warehouse/dest4.out' SELECT src.value WHERE src.key >= 300
CREATE TABLE TEST3a(A INT, B DOUBLE) STORED AS TEXTFILE;
DESCRIBE TEST3a;
CREATE TABLE TEST3b(A ARRAY<INT>, B DOUBLE, C MAP<DOUBLE, INT>) STORED AS TEXTFILE;
DESCRIBE TEST3b;
SHOW TABLES;
EXPLAIN
ALTER TABLE TEST3b ADD COLUMNS (X DOUBLE);
ALTER TABLE TEST3b ADD COLUMNS (X DOUBLE);
DESCRIBE TEST3b;
EXPLAIN
ALTER TABLE TEST3b RENAME TO TEST3c;
ALTER TABLE TEST3b RENAME TO TEST3c;
DESCRIBE TEST3c;
SHOW TABLES;
set hive.metastore.disallow.incompatible.col.type.changes=false;
EXPLAIN
ALTER TABLE TEST3c REPLACE COLUMNS (R1 INT, R2 DOUBLE);
ALTER TABLE TEST3c REPLACE COLUMNS (R1 INT, R2 DOUBLE);
reset hive.metastore.disallow.incompatible.col.type.changes;
DESCRIBE EXTENDED TEST3c;
set hive.mapred.mode=nonstrict;




create table dest30(a int);
create table tst_dest30(a int);

set hive.test.mode=true;
set hive.test.mode.prefix=tst_;

explain
insert overwrite table dest30
select count(1) from src;

insert overwrite table dest30
select count(1) from src;

set hive.test.mode=false;

select * from tst_dest30;



set hive.mapred.mode=nonstrict;



set hive.test.mode=true;
set hive.test.mode.prefix=tst_;

create table tst_dest31(a int);
create table dest31(a int);

explain
insert overwrite table dest31
select count(1) from srcbucket;

insert overwrite table dest31
select count(1) from srcbucket;

set hive.test.mode=false;

select * from tst_dest31;





set hive.mapred.mode=nonstrict;



set hive.test.mode=true;
set hive.test.mode.prefix=tst_;
set hive.test.mode.nosamplelist=src,srcbucket;

create table dest32(a int);
create table tst_dest32(a int);

explain
insert overwrite table dest32
select count(1) from srcbucket;

insert overwrite table dest32
select count(1) from srcbucket;

set hive.test.mode=false;

select * from tst_dest32;





CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

ADD FILE ../../data/scripts/input20_script.py;

EXPLAIN
FROM (
  FROM src
  MAP src.key, src.key
  USING 'cat'
  DISTRIBUTE BY key
  SORT BY key, value
) tmap
INSERT OVERWRITE TABLE dest1
REDUCE tmap.key, tmap.value
USING 'python input20_script.py'
AS (key STRING, value STRING);

FROM (
  FROM src
  MAP src.key, src.key
  USING 'cat'
  DISTRIBUTE BY key
  SORT BY key, value
) tmap
INSERT OVERWRITE TABLE dest1
REDUCE tmap.key, tmap.value
USING 'python input20_script.py'
AS (key STRING, value STRING);

SELECT * FROM dest1 SORT BY key, value;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\003'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\003'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

SELECT dest1.* FROM dest1;
create table documents(contents string) stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/docurl.txt' INTO TABLE documents;


select url, count(1)
FROM
(
  FROM documents
  MAP documents.contents
  USING 'java -cp ../util/target/classes/ org.apache.hadoop.hive.scripts.extracturl' AS (url, count)
) subq
group by url;




CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value, 1+2, 3+4)
         USING 'cat'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.key, tmap.value;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value, 1+2, 3+4)
         USING 'cat'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.key, tmap.value;


SELECT dest1.* FROM dest1;


-- EXCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)


create table t1(key string, value string) partitioned by (ds string);
create table t2(key string, value string) partitioned by (ds string);

insert overwrite table t1 partition (ds='1')
select key, value from src;

insert overwrite table t1 partition (ds='2')
select key, value from src;

insert overwrite table t2 partition (ds='1')
select key, value from src;

set hive.test.mode=true;
set hive.mapred.mode=strict;
set mapreduce.framework.name=yarn;
set mapreduce.jobtracker.address=localhost:58;
set hive.exec.mode.local.auto=true;

explain
select count(1) from t1 join t2 on t1.key=t2.key where t1.ds='1' and t2.ds='1';

select count(1) from t1 join t2 on t1.key=t2.key where t1.ds='1' and t2.ds='1';

set hive.test.mode=false;
set mapreduce.framework.name;
set mapreduce.jobtracker.address;



-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20, 0.20S)


create table t1(key string, value string) partitioned by (ds string);
create table t2(key string, value string) partitioned by (ds string);

insert overwrite table t1 partition (ds='1')
select key, value from src;

insert overwrite table t1 partition (ds='2')
select key, value from src;

insert overwrite table t2 partition (ds='1')
select key, value from src;

set hive.test.mode=true;
set hive.mapred.mode=strict;
set mapred.job.tracker=localhost:58;
set hive.exec.mode.local.auto=true;

explain
select count(1) from t1 join t2 on t1.key=t2.key where t1.ds='1' and t2.ds='1';

select count(1) from t1 join t2 on t1.key=t2.key where t1.ds='1' and t2.ds='1';

set hive.test.mode=false;
set mapred.job.tracker;




CREATE TABLE T1(key STRING, value STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/kv2.txt' INTO TABLE T1;


CREATE TABLE T2(key STRING, value STRING);

EXPLAIN
INSERT OVERWRITE TABLE T2 SELECT * FROM (SELECT * FROM T1 DISTRIBUTE BY key SORT BY key, value) T LIMIT 20;

INSERT OVERWRITE TABLE T2 SELECT * FROM (SELECT * FROM T1 DISTRIBUTE BY key SORT BY key, value) T LIMIT 20;

SELECT * FROM T2 SORT BY key, value;



-- JAVA_VERSION_SPECIFIC_OUTPUT

CREATE TABLE INPUT4(KEY STRING, VALUE STRING) STORED AS TEXTFILE;
EXPLAIN
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE INPUT4;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE INPUT4;
EXPLAIN FORMATTED
SELECT Input4Alias.VALUE, Input4Alias.KEY FROM INPUT4 AS Input4Alias;
SELECT Input4Alias.VALUE, Input4Alias.KEY FROM INPUT4 AS Input4Alias

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING '/bin/cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100
set hive.mapred.mode=strict;

select * from srcpart a join
  (select b.key, count(1) as count from srcpart b where b.ds = '2008-04-08' and b.hr = '14' group by b.key) subq
  where a.ds = '2008-04-08' and a.hr = '11' limit 10;
-- SORT_QUERY_RESULTS

create table tmp_insert_test (key string, value string) stored as textfile;
load data local inpath '../../data/files/kv1.txt' into table tmp_insert_test;
select * from tmp_insert_test;

create table tmp_insert_test_p (key string, value string) partitioned by (ds string) stored as textfile;

load data local inpath '../../data/files/kv1.txt' into table tmp_insert_test_p partition (ds = '2009-08-01');
select * from tmp_insert_test_p where ds= '2009-08-01';

load data local inpath '../../data/files/kv2.txt' into table tmp_insert_test_p partition (ds = '2009-08-01');
select * from tmp_insert_test_p where ds= '2009-08-01';
set hive.mapred.mode=strict;

create table dest_sp (cnt int);

insert overwrite table dest_sp
select * from
  (select count(1) as cnt from src
    union all
   select count(1) as cnt from srcpart where ds = '2009-08-09'
  )x;

select * from dest_sp x order by x.cnt limit 2;


select * from
  (select * from src
    union all
   select * from srcpart where ds = '2009-08-09'
  )x;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

explain extended
select * from srcpart a where a.ds='2008-04-08';

select * from srcpart a where a.ds='2008-04-08';


explain extended
select * from srcpart a where a.ds='2008-04-08' and key < 200;

select * from srcpart a where a.ds='2008-04-08' and key < 200;


explain extended
select * from srcpart a where a.ds='2008-04-08' and rand(100) < 0.1;

select * from srcpart a where a.ds='2008-04-08' and rand(100) < 0.1;
drop table tst_src1;
create table tst_src1 like src1;
load data local inpath '../../data/files/kv1.txt' into table tst_src1 ;
select count(1) from tst_src1;
load data local inpath '../../data/files/kv1.txt' into table tst_src1 ;
select count(1) from tst_src1;
drop table tst_src1;
CREATE TABLE dest(key INT, value STRING) STORED AS TEXTFILE;

SET hive.output.file.extension=.txt;
INSERT OVERWRITE TABLE dest SELECT src.* FROM src;

dfs -cat ${system:test.warehouse.dir}/dest/*.txt
SET hive.insert.into.multilevel.dirs=true;

SET hive.output.file.extension=.txt;

INSERT OVERWRITE DIRECTORY 'target/data/x/y/z/' SELECT src.* FROM src;

dfs -cat ${system:build.dir}/data/x/y/z/*.txt;

dfs -rmr ${system:build.dir}/data/x;
create database if not exists table_in_database_creation;
create table table_in_database_creation.test1  as select * from src limit 1;
create table `table_in_database_creation`.`test2` as select * from src limit 1;
create table table_in_database_creation.test3 (a string);
create table `table_in_database_creation`.`test4` (a string);
drop database table_in_database_creation cascade;create table intable (b boolean, d double, f float, i int, l bigint, s string, t tinyint);
insert overwrite table intable select 0, 29098519.0, 1410.0, 996, 40408519555, "test_string", 12 from src limit 1;
select * from intable where d in (29098519.0) and f in (1410.0) and i in (996) and l in (40408519555) and s in ('test_string') and t in (12);
drop table intable;CREATE TABLE INPUT4_CB(KEY STRING, VALUE STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\002' LINES TERMINATED BY '\012' STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1_cb.txt' INTO TABLE INPUT4_CB;
SELECT INPUT4_CB.VALUE, INPUT4_CB.KEY FROM INPUT4_CB;

explain
select * from src sort by key limit 10;


select * from src sort by key limit 10;
FROM (
  FROM src_thrift
  SELECT TRANSFORM(src_thrift.lint, src_thrift.lintstring)
         USING '/bin/cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue
CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src_thrift
  SELECT TRANSFORM(src_thrift.lint, src_thrift.lintstring)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue;

FROM (
  FROM src_thrift
  SELECT TRANSFORM(src_thrift.lint, src_thrift.lintstring)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue;

SELECT dest1.* FROM dest1;
FROM src1
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src1.value WHERE src1.key is null
CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src1
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src1.value WHERE src1.key is null;

FROM src1
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src1.value WHERE src1.key is null;

SELECT dest1.* FROM dest1;
FROM src1
INSERT OVERWRITE TABLE dest1 SELECT NULL, src1.key
CREATE TABLE dest1(c1 DOUBLE, c2 INT) STORED AS TEXTFILE;

EXPLAIN
FROM src1
INSERT OVERWRITE TABLE dest1 SELECT NULL, src1.key;

FROM src1
INSERT OVERWRITE TABLE dest1 SELECT NULL, src1.key;

SELECT dest1.* FROM dest1;
FROM src1
SELECT 4 + NULL, src1.key - NULL, NULL + NULL
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(c1 STRING, c2 INT, c3 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src1
INSERT OVERWRITE TABLE dest1 SELECT 4 + NULL, src1.key - NULL, NULL + NULL;

FROM src1
INSERT OVERWRITE TABLE dest1 SELECT 4 + NULL, src1.key - NULL, NULL + NULL;

SELECT dest1.* FROM dest1;
FROM src1
INSERT OVERWRITE TABLE dest1 SELECT NULL, src1.key where NULL = NULL
CREATE TABLE dest1(value STRING, key INT) STORED AS TEXTFILE;

EXPLAIN
FROM src1
INSERT OVERWRITE TABLE dest1 SELECT NULL, src1.key where NULL = NULL;

FROM src1
INSERT OVERWRITE TABLE dest1 SELECT NULL, src1.key where NULL = NULL;

SELECT dest1.* FROM dest1;

EXPLAIN
CREATE TABLE INPUTDDL1(key INT, value STRING) STORED AS TEXTFILE;

CREATE TABLE INPUTDDL1(key INT, value STRING) STORED AS TEXTFILE;

SELECT INPUTDDL1.* from INPUTDDL1;
EXPLAIN
CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE;
CREATE TABLE INPUTDDL2(key INT, value STRING) PARTITIONED BY(ds STRING, country STRING) STORED AS TEXTFILE;
DESCRIBE INPUTDDL2;


EXPLAIN
CREATE TABLE INPUTDDL3(key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;
CREATE TABLE INPUTDDL3(key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TEXTFILE;
DESCRIBE INPUTDDL3;

-- a simple test to test sorted/clustered syntax

CREATE TABLE INPUTDDL4(viewTime STRING, userid INT,
                       page_url STRING, referrer_url STRING,
                       friends ARRAY<BIGINT>, properties MAP<STRING, STRING>,
                       ip STRING COMMENT 'IP Address of the User')
    COMMENT 'This is the page view table'
    PARTITIONED BY(ds STRING, country STRING)
    CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS;
DESCRIBE INPUTDDL4;
DESCRIBE EXTENDED INPUTDDL4;

-- test for internationalization
-- kv4.txt contains the utf-8 character 0xE982B5E993AE which we are verifying later on
CREATE TABLE INPUTDDL5(name STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv4.txt' INTO TABLE INPUTDDL5;
DESCRIBE INPUTDDL5;
SELECT INPUTDDL5.name from INPUTDDL5;
SELECT count(1) FROM INPUTDDL5 WHERE INPUTDDL5.name = _UTF-8 0xE982B5E993AE;

-- test for describe extended table
-- test for describe extended table partition
-- test for alter table drop partition
CREATE TABLE INPUTDDL6(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE INPUTDDL6 PARTITION (ds='2008-04-09');
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE INPUTDDL6 PARTITION (ds='2008-04-08');
DESCRIBE EXTENDED INPUTDDL6;
DESCRIBE EXTENDED INPUTDDL6 PARTITION (ds='2008-04-08');
SHOW PARTITIONS INPUTDDL6;
ALTER TABLE INPUTDDL6 DROP PARTITION (ds='2008-04-08');
SHOW PARTITIONS INPUTDDL6;
EXPLAIN
DESCRIBE EXTENDED INPUTDDL6 PARTITION (ds='2008-04-09');
DROP TABLE INPUTDDL6;

-- test for loading into tables with the correct file format
-- test for loading into partitions with the correct file format


CREATE TABLE T1(name STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE T1;
SELECT COUNT(1) FROM T1;


CREATE TABLE T2(name STRING) STORED AS SEQUENCEFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.seq' INTO TABLE T2;
SELECT COUNT(1) FROM T2;


CREATE TABLE T3(name STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE T3 PARTITION (ds='2008-04-09');
SELECT COUNT(1) FROM T3 where T3.ds='2008-04-09';


CREATE TABLE T4(name STRING) PARTITIONED BY(ds STRING) STORED AS SEQUENCEFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.seq' INTO TABLE T4 PARTITION (ds='2008-04-09');
SELECT COUNT(1) FROM T4 where T4.ds='2008-04-09';

DESCRIBE EXTENDED T1;
DESCRIBE EXTENDED T2;
DESCRIBE EXTENDED T3 PARTITION (ds='2008-04-09');
DESCRIBE EXTENDED T4 PARTITION (ds='2008-04-09');







CREATE TABLE INPUTDDL8 COMMENT 'This is a thrift based table'
    PARTITIONED BY(ds STRING, country STRING)
    CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer'
    WITH SERDEPROPERTIES ('serialization.class' = 'org.apache.hadoop.hive.serde2.thrift.test.Complex',
                          'serialization.format' = 'com.facebook.thrift.protocol.TBinaryProtocol')
    STORED AS SEQUENCEFILE;
DESCRIBE EXTENDED INPUTDDL8;


CREATE TABLE input_columnarserde(a array<int>, b array<string>, c map<string,string>, d int, e string)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileOutputFormat';

EXPLAIN
FROM src_thrift
INSERT OVERWRITE TABLE input_columnarserde SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring DISTRIBUTE BY 1;

FROM src_thrift
INSERT OVERWRITE TABLE input_columnarserde SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring DISTRIBUTE BY 1;

SELECT input_columnarserde.* FROM input_columnarserde DISTRIBUTE BY 1;

SELECT input_columnarserde.a[0], input_columnarserde.b[0], input_columnarserde.c['key2'], input_columnarserde.d, input_columnarserde.e FROM input_columnarserde DISTRIBUTE BY 1;

dfs -cat ../../data/files/kv1.txt;

CREATE TABLE dest1(a array<int>, b array<string>, c map<string,string>, d int, e string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '1'
COLLECTION ITEMS TERMINATED BY '2'
MAP KEYS TERMINATED BY '3'
LINES TERMINATED BY '10'
STORED AS TEXTFILE;

EXPLAIN
FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring;

FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring;

SELECT dest1.* FROM dest1;

SELECT dest1.a[0], dest1.b[0], dest1.c['key2'], dest1.d, dest1.e FROM dest1;
-- SORT_QUERY_RESULTS

DROP TABLE dest1;
CREATE TABLE dest1(a array<int>, b array<string>, c map<string,string>, d int, e string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '1'
COLLECTION ITEMS TERMINATED BY '2'
MAP KEYS TERMINATED BY '3'
LINES TERMINATED BY '10'
STORED AS TEXTFILE;

EXPLAIN
FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring DISTRIBUTE BY 1;

FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint, src_thrift.lstring, src_thrift.mstringstring, src_thrift.aint, src_thrift.astring DISTRIBUTE BY 1;

SELECT dest1.* FROM dest1 CLUSTER BY 1;

SELECT dest1.a[0], dest1.b[0], dest1.c['key2'], dest1.d, dest1.e FROM dest1 CLUSTER BY 1;

DROP TABLE dest1;

CREATE TABLE dest1(a array<int>) ROW FORMAT DELIMITED FIELDS TERMINATED BY '1' ESCAPED BY '\\';
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint FROM src_thrift DISTRIBUTE BY 1;
SELECT * from dest1;
DROP TABLE dest1;

CREATE TABLE dest1(a map<string,string>) ROW FORMAT DELIMITED FIELDS TERMINATED BY '1' ESCAPED BY '\\';
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.mstringstring FROM src_thrift DISTRIBUTE BY 1;
SELECT * from dest1;

CREATE TABLE destBin(a UNIONTYPE<int, double, array<string>, struct<col1:int,col2:string>>) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe' STORED AS SEQUENCEFILE;
INSERT OVERWRITE TABLE destBin SELECT create_union( CASE WHEN key < 100 THEN 0 WHEN key < 200 THEN 1 WHEN key < 300 THEN 2 WHEN key < 400 THEN 3 ELSE 0 END, key, 2.0, array("one","two"), struct(5,"five")) FROM srcbucket2;
SELECT * from destBin;
DROP TABLE destBin;

DROP TABLE dest2;
DROP TABLE dest3;

CREATE TABLE dest2 (a map<string,map<string,map<string,uniontype<int, bigint, string, double, boolean, array<string>, map<string,string>>>>>)
  ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe' STORED AS SEQUENCEFILE;
INSERT OVERWRITE TABLE dest2 SELECT src_thrift.attributes FROM src_thrift;
SELECT a from dest2 limit 10;

CREATE TABLE dest3 (
unionfield1 uniontype<int, bigint, string, double, boolean, array<string>, map<string,string>>,
unionfield2 uniontype<int, bigint, string, double, boolean, array<string>, map<string,string>>,
unionfield3 uniontype<int, bigint, string, double, boolean, array<string>, map<string,string>>
) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe' STORED AS SEQUENCEFILE;
INSERT OVERWRITE TABLE dest3 SELECT src_thrift.unionField1,src_thrift.unionField2,src_thrift.unionField3 from src_thrift;
SELECT unionfield1, unionField2, unionfield3 from dest3 limit 10;
EXPLAIN
SELECT x.* FROM SRC x LIMIT 20;

SELECT x.* FROM SRC x LIMIT 20;
EXPLAIN
SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08';

SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08';
set hive.mapred.mode=strict;

SELECT x.* FROM SRCPART x WHERE key = '2008-04-08';
FROM srcpart
SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-08' and srcpart.hr = '12'
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(key INT, value STRING, hr STRING, ds STRING) STORED AS TEXTFILE;

EXPLAIN EXTENDED
FROM srcpart
INSERT OVERWRITE TABLE dest1 SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-08' and srcpart.hr = '12';

FROM srcpart
INSERT OVERWRITE TABLE dest1 SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-08' and srcpart.hr = '12';

SELECT dest1.* FROM dest1;

-- EXCLUDE_OS_WINDOWS
-- excluded on windows because of difference in file name encoding logic

CREATE TABLE part_special (
  a STRING,
  b STRING
) PARTITIONED BY (
  ds STRING,
  ts STRING
);

EXPLAIN
INSERT OVERWRITE TABLE part_special PARTITION(ds='2008 04 08', ts = '10:11:12=455')
SELECT 1, 2 FROM src LIMIT 1;

INSERT OVERWRITE TABLE part_special PARTITION(ds='2008 04 08', ts = '10:11:12=455')
SELECT 1, 2 FROM src LIMIT 1;

DESCRIBE EXTENDED part_special PARTITION(ds='2008 04 08', ts = '10:11:12=455');

SELECT * FROM part_special WHERE ds='2008 04 08' AND ts = '10:11:12=455';


-- INCLUDE_OS_WINDOWS
-- included only on  windows because of difference in file name encoding logic

CREATE TABLE part_special (
  a STRING,
  b STRING
) PARTITIONED BY (
  ds STRING,
  ts STRING
);

EXPLAIN
INSERT OVERWRITE TABLE part_special PARTITION(ds='2008 04 08', ts = '10:11:12=455')
SELECT 1, 2 FROM src LIMIT 1;

INSERT OVERWRITE TABLE part_special PARTITION(ds='2008 04 08', ts = '10:11:12=455')
SELECT 1, 2 FROM src LIMIT 1;

DESCRIBE EXTENDED part_special PARTITION(ds='2008 04 08', ts = '10:11:12=455');

SELECT * FROM part_special WHERE ds='2008 04 08' AND ts = '10:11:12=455';


CREATE TABLE dest1(key INT, value STRING, hr STRING, ds STRING) STORED AS TEXTFILE;
CREATE TABLE dest2(key INT, value STRING, hr STRING, ds STRING) STORED AS TEXTFILE;

-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
FROM srcpart
INSERT OVERWRITE TABLE dest1 SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-08' and srcpart.hr = '12'
INSERT OVERWRITE TABLE dest2 SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-09' and srcpart.hr = '12';

FROM srcpart
INSERT OVERWRITE TABLE dest1 SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-08' and srcpart.hr = '12'
INSERT OVERWRITE TABLE dest2 SELECT srcpart.key, srcpart.value, srcpart.hr, srcpart.ds WHERE srcpart.key < 100 and srcpart.ds = '2008-04-09' and srcpart.hr = '12';

SELECT dest1.* FROM dest1 sort by key,value,ds,hr;
SELECT dest2.* FROM dest2 sort by key,value,ds,hr;


EXPLAIN
SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.hr = 11;

SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.hr = 11;
set hive.mapred.mode=nonstrict;
EXPLAIN
SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.hr = 15;

SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.hr = 15;
set hive.mapred.mode=nonstrict;

create table tmptable(key string, value string, hr string, ds string);

EXPLAIN
insert overwrite table tmptable
SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.key < 100;

insert overwrite table tmptable
SELECT x.* FROM SRCPART x WHERE x.ds = '2008-04-08' and x.key < 100;

select * from tmptable x sort by x.key,x.value,x.ds,x.hr;

set hive.mapred.mode=nonstrict;
EXPLAIN
SELECT x.* FROM SRCPART x WHERE x.ds = 2008-04-08 LIMIT 10;

SELECT x.* FROM SRCPART x WHERE x.ds = 2008-04-08 LIMIT 10;
EXPLAIN EXTENDED
SELECT * FROM (
  SELECT X.* FROM SRCPART X WHERE X.ds = '2008-04-08' and X.key < 100
  UNION ALL
  SELECT Y.* FROM SRCPART Y WHERE Y.ds = '2008-04-08' and Y.key < 100
) A
SORT BY A.key, A.value, A.ds, A.hr;

SELECT * FROM (
  SELECT X.* FROM SRCPART X WHERE X.ds = '2008-04-08' and X.key < 100
  UNION ALL
  SELECT Y.* FROM SRCPART Y WHERE Y.ds = '2008-04-08' and Y.key < 100
) A
SORT BY A.key, A.value, A.ds, A.hr;
EXPLAIN
SELECT x.* FROM SRCPART x WHERE ds = '2008-04-08' LIMIT 10;

SELECT x.* FROM SRCPART x WHERE ds = '2008-04-08' LIMIT 10;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
SELECT x.* FROM SRCPART x WHERE key IS NOT NULL AND ds = '2008-04-08';

SELECT x.* FROM SRCPART x WHERE key IS NOT NULL AND ds = '2008-04-08';

FROM src
INSERT OVERWRITE TABLE dest4_sequencefile SELECT src.key, src.value
set mapred.output.compress=true;
set mapred.output.compression.type=BLOCK;

CREATE TABLE dest4_sequencefile(key INT, value STRING) STORED AS SEQUENCEFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest4_sequencefile SELECT src.key, src.value;

FROM src
INSERT OVERWRITE TABLE dest4_sequencefile SELECT src.key, src.value;

set mapred.output.compress=false;
SELECT dest4_sequencefile.* FROM dest4_sequencefile;
FROM src_thrift
SELECT src_thrift.lint[1], src_thrift.lintstring[0].mystring, src_thrift.mstringstring['key_2']
CREATE TABLE dest1(key INT, value STRING, mapvalue STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint[1], src_thrift.lintstring[0].mystring, src_thrift.mstringstring['key_2'];

FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT src_thrift.lint[1], src_thrift.lintstring[0].mystring, src_thrift.mstringstring['key_2'];

SELECT dest1.* FROM dest1;
FROM src_thrift
SELECT size(src_thrift.lint), size(src_thrift.lintstring), size(src_thrift.mstringstring) where src_thrift.lint IS NOT NULL AND NOT (src_thrift.mstringstring IS NULL)
CREATE TABLE dest1(lint_size INT, lintstring_size INT, mstringstring_size INT) STORED AS TEXTFILE;

EXPLAIN
FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT size(src_thrift.lint), size(src_thrift.lintstring), size(src_thrift.mstringstring) where src_thrift.lint IS NOT NULL AND NOT (src_thrift.mstringstring IS NULL);

FROM src_thrift
INSERT OVERWRITE TABLE dest1 SELECT size(src_thrift.lint), size(src_thrift.lintstring), size(src_thrift.mstringstring) where src_thrift.lint IS NOT NULL AND NOT (src_thrift.mstringstring IS NULL);

SELECT dest1.* FROM dest1;
EXPLAIN
FROM src_thrift
SELECT src_thrift.mstringstring['key_9'], src_thrift.lintstring.myint;

FROM src_thrift
SELECT src_thrift.mstringstring['key_9'], src_thrift.lintstring.myint;
set hive.optimize.ppd=false;

EXPLAIN
FROM src_thrift
SELECT src_thrift.mstringstring['key_9'], lintstring.myint
WHERE src_thrift.mstringstring['key_9'] IS NOT NULL
      AND lintstring.myint IS NOT NULL
      AND lintstring IS NOT NULL;

FROM src_thrift
SELECT src_thrift.mstringstring['key_9'], lintstring.myint
WHERE src_thrift.mstringstring['key_9'] IS NOT NULL
      OR lintstring.myint IS NOT NULL
      OR lintstring IS NOT NULL;

set hive.optimize.ppd=true;

EXPLAIN
FROM src_thrift
SELECT src_thrift.mstringstring['key_9'], lintstring.myint
WHERE src_thrift.mstringstring['key_9'] IS NOT NULL
      AND lintstring.myint IS NOT NULL
      AND lintstring IS NOT NULL;

FROM src_thrift
SELECT src_thrift.mstringstring['key_9'], lintstring.myint
WHERE src_thrift.mstringstring['key_9'] IS NOT NULL
      OR lintstring.myint IS NOT NULL
      OR lintstring IS NOT NULL;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;

DROP TABLE insert_into1;
DROP TABLE ctas_table;
DROP TABLE ctas_part;

CREATE TABLE insert_into1 (key int, value string);

INSERT OVERWRITE TABLE insert_into1 SELECT * from src ORDER BY key LIMIT 10;

select * from insert_into1 order by key;

INSERT INTO TABLE insert_into1 SELECT * from src ORDER BY key DESC LIMIT 10;

select * from insert_into1 order by key;

create table ctas_table as SELECT key, count(value) as foo from src GROUP BY key LIMIT 10;

describe extended ctas_table;

select * from ctas_table order by key;


set hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;

create table ctas_part (key int, value string) partitioned by (modkey bigint);

insert overwrite table ctas_part partition (modkey)
select key, value, ceil(key / 100) from src where key is not null limit 10;

select * from ctas_part order by key;



DROP TABLE insert_into1;
DROP TABLE ctas_table;
DROP TABLE ctas_part;set hive.explain.user=false;

-- SORT_QUERY_RESULTS

create table insert1(key int, value string) stored as textfile;
create table insert2(key int, value string) stored as textfile;
insert overwrite table insert1 select a.key, a.value from insert2 a WHERE (a.key=-1);

explain insert into table insert1 select a.key, a.value from insert2 a WHERE (a.key=-1);
explain insert into table INSERT1 select a.key, a.value from insert2 a WHERE (a.key=-1);

-- HIVE-3465
create database x;
create table x.insert1(key int, value string) stored as textfile;

explain insert into table x.INSERT1 select a.key, a.value from insert2 a WHERE (a.key=-1);

explain insert into table default.INSERT1 select a.key, a.value from insert2 a WHERE (a.key=-1);

explain
from insert2
insert into table insert1 select * where key < 10
insert overwrite table x.insert1 select * where key > 10 and key < 20;

-- HIVE-3676
CREATE DATABASE db2;
USE db2;
CREATE TABLE result(col1 STRING);
INSERT OVERWRITE TABLE result SELECT 'db2_insert1' FROM default.src LIMIT 1;
INSERT INTO TABLE result SELECT 'db2_insert2' FROM default.src LIMIT 1;
SELECT * FROM result;

USE default;
CREATE DATABASE db1;
CREATE TABLE db1.result(col1 STRING);
INSERT OVERWRITE TABLE db1.result SELECT 'db1_insert1' FROM src LIMIT 1;
INSERT INTO TABLE db1.result SELECT 'db1_insert2' FROM src LIMIT 1;
SELECT * FROM db1.result;
set hive.mapred.mode=nonstrict;
CREATE TABLE sourceTable (one string,two string) PARTITIONED BY (ds string,hr string);

load data local inpath '../../data/files/kv1.txt' INTO TABLE sourceTable partition(ds='2011-11-11', hr='11');

load data local inpath '../../data/files/kv3.txt' INTO TABLE sourceTable partition(ds='2011-11-11', hr='12');

CREATE TABLE destinTable (one string,two string) PARTITIONED BY (ds string,hr string);

EXPLAIN INSERT OVERWRITE TABLE destinTable PARTITION (ds='2011-11-11', hr='11') if not exists
SELECT one,two FROM sourceTable WHERE ds='2011-11-11' AND hr='11' order by one desc, two desc limit 5;

INSERT OVERWRITE TABLE destinTable PARTITION (ds='2011-11-11', hr='11') if not exists
SELECT one,two FROM sourceTable WHERE ds='2011-11-11' AND hr='11' order by one desc, two desc limit 5;

select one,two from destinTable order by one desc, two desc;

EXPLAIN INSERT OVERWRITE TABLE destinTable PARTITION (ds='2011-11-11', hr='11') if not exists
SELECT one,two FROM sourceTable WHERE ds='2011-11-11' AND hr='12' order by one desc, two desc limit 5;

INSERT OVERWRITE TABLE destinTable PARTITION (ds='2011-11-11', hr='11') if not exists
SELECT one,two FROM sourceTable WHERE ds='2011-11-11' AND hr='12' order by one desc, two desc limit 5;

select one,two from destinTable order by one desc, two desc;

drop table destinTable;

CREATE TABLE destinTable (one string,two string);

EXPLAIN INSERT OVERWRITE TABLE destinTable SELECT one,two FROM sourceTable WHERE ds='2011-11-11' AND hr='11' order by one desc, two desc limit 5;

INSERT OVERWRITE TABLE destinTable SELECT one,two FROM sourceTable WHERE ds='2011-11-11' AND hr='11' order by one desc, two desc limit 5;

drop table destinTable;

drop table sourceTable;
set hive.mapred.mode=nonstrict;
CREATE DATABASE db1;

CREATE DATABASE db2;

CREATE TABLE db1.sourceTable (one string,two string) PARTITIONED BY (ds string);

load data local inpath '../../data/files/kv1.txt' INTO TABLE db1.sourceTable partition(ds='2011-11-11');

load data local inpath '../../data/files/kv3.txt' INTO TABLE db1.sourceTable partition(ds='2011-11-11');

CREATE TABLE db2.destinTable (one string,two string) PARTITIONED BY (ds string);

EXPLAIN INSERT OVERWRITE TABLE db2.destinTable PARTITION (ds='2011-11-11')
SELECT one,two FROM db1.sourceTable WHERE ds='2011-11-11' order by one desc, two desc limit 5;

INSERT OVERWRITE TABLE db2.destinTable PARTITION (ds='2011-11-11')
SELECT one,two FROM db1.sourceTable WHERE ds='2011-11-11' order by one desc, two desc limit 5;

select one,two from db2.destinTable order by one desc, two desc;

EXPLAIN INSERT OVERWRITE TABLE db2.destinTable PARTITION (ds='2011-11-11')
SELECT one,two FROM db1.sourceTable WHERE ds='2011-11-11' order by one desc, two desc limit 5;

INSERT OVERWRITE TABLE db2.destinTable PARTITION (ds='2011-11-11')
SELECT one,two FROM db1.sourceTable WHERE ds='2011-11-11' order by one desc, two desc limit 5;

select one,two from db2.destinTable order by one desc, two desc;

drop table db2.destinTable;

drop table db1.sourceTable;

DROP DATABASE db1;

DROP DATABASE db2;


create table texternal(key string, val string) partitioned by (insertdate string);

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/texternal/temp;
dfs -rmr ${system:test.tmp.dir}/texternal;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/texternal/2008-01-01;

alter table texternal add partition (insertdate='2008-01-01') location 'pfile://${system:test.tmp.dir}/texternal/2008-01-01';
from src insert overwrite table texternal partition (insertdate='2008-01-01') select *;

select * from texternal where insertdate='2008-01-01';

dfs -rmr ${system:test.tmp.dir}/texternal;
set hive.insert.into.external.tables=false;


create external table texternal(key string, val string) partitioned by (insertdate string);

alter table texternal add partition (insertdate='2008-01-01') location 'pfile://${system:test.tmp.dir}/texternal/2008-01-01';
from src insert overwrite table texternal partition (insertdate='2008-01-01') select *;

CREATE TABLE IF NOT EXISTS bucketinput(
data string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
CREATE TABLE IF NOT EXISTS bucketoutput1(
data string
)CLUSTERED BY(data)
INTO 2 BUCKETS
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
CREATE TABLE IF NOT EXISTS bucketoutput2(
data string
)CLUSTERED BY(data)
INTO 2 BUCKETS
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
insert into table bucketinput values ("firstinsert1");
insert into table bucketinput values ("firstinsert2");
insert into table bucketinput values ("firstinsert3");
;

insert overwrite table bucketoutput1 select * from bucketinput where data like 'first%';
CREATE TABLE temp1
(
    change string,
    num string
)
CLUSTERED BY (num) SORTED BY (num) INTO 4 BUCKETS;
explain insert overwrite table temp1 select data, data from bucketinput;
CREATE TABLE temp2
(
    create_ts STRING ,
    change STRING,
    num STRING
)
CLUSTERED BY (create_ts) SORTED BY (num) INTO 4 BUCKETS;

explain
INSERT OVERWRITE TABLE temp2
SELECT change, change,num
FROM temp1;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
select * from bucketoutput1 a join bucketoutput2 b on (a.data=b.data);
drop table temp1;
drop table temp2;
drop table buckettestinput;
drop table buckettestoutput1;
drop table buckettestoutput2;

set hive.exec.dynamic.partition=true;

create table srcpart_dp like srcpart;

create table destpart_dp like srcpart;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_dp partition(ds='2008-04-08', hr=11);

insert overwrite table destpart_dp partition (ds='2008-04-08', hr) if not exists select key, value, hr from srcpart_dp where ds='2008-04-08';insert overwrite directory 'target/warehouse/aret.out' select a.key src a;
drop table if exists escstr;
create table escstr(val string);
insert into escstr values('It\'s a simple test');
select * from escstr;

set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

set hive.exec.dynamic.partition.mode=nonstrict;

create table acid_dynamic(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_dynamic partition (ds) select cint, cast(cstring1 as varchar(128)), cstring2 from alltypesorc where cint is not null and cint < 0 order by cint limit 5;

select * from acid_dynamic order by a,b;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_notbucketed(a int, b varchar(128)) stored as orc;

insert into table acid_notbucketed select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select * from acid_notbucketed;
set hive.exec.compress.output=true;

drop table insert_compressed;
create table insert_compressed (key int, value string);

insert overwrite table insert_compressed select * from src;
select count(*) from insert_compressed;

insert into table insert_compressed select * from src;
select count(*) from insert_compressed;

insert into table insert_compressed select * from src;
select count(*) from insert_compressed;

drop table insert_compressed;
set hive.exec.copyfile.maxsize=400;

set tez.am.log.level=INFO;
set tez.task.log.level=INFO;
-- see TEZ-2931 for using INFO logging

insert overwrite directory '/tmp/src' select * from src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/src/;
set hive.explain.user=false;
set hive.compute.query.using.stats=true;

-- SORT_QUERY_RESULTS

DROP TABLE insert_into1;

CREATE TABLE insert_into1 (key int, value string);

EXPLAIN INSERT INTO TABLE insert_into1 SELECT * from src ORDER BY key LIMIT 100;
INSERT INTO TABLE insert_into1 SELECT * from src ORDER BY key LIMIT 100;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into1
) t;
explain
select count(*) from insert_into1;
select count(*) from insert_into1;
EXPLAIN INSERT INTO TABLE insert_into1 SELECT * FROM src ORDER BY key LIMIT 100;
INSERT INTO TABLE insert_into1 SELECT * FROM src ORDER BY key LIMIT 100;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into1
) t;

explain
SELECT COUNT(*) FROM insert_into1;
select count(*) from insert_into1;

EXPLAIN INSERT OVERWRITE TABLE insert_into1 SELECT * FROM src ORDER BY key LIMIT 10;
INSERT OVERWRITE TABLE insert_into1 SELECT * FROM src ORDER BY key LIMIT 10;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into1
) t;

explain
SELECT COUNT(*) FROM insert_into1;
select count(*) from insert_into1;

explain insert overwrite table insert_into1 select 1, 'a';
insert overwrite table insert_into1 select 1, 'a';

explain insert into insert_into1 select 2, 'b';
insert into insert_into1 select 2, 'b';

select * from insert_into1;

set hive.stats.autogather=false;
explain
insert into table insert_into1 values(1, 'abc');
insert into table insert_into1 values(1, 'abc');
explain
SELECT COUNT(*) FROM insert_into1;
select count(*) from insert_into1;

DROP TABLE insert_into1;
set hive.stats.autogather=true;
set hive.compute.query.using.stats=false;
set hive.lock.numretries=5;
set hive.lock.sleep.between.retries=5;

DROP TABLE insert_into1_neg;

CREATE TABLE insert_into1_neg (key int, value string);

LOCK TABLE insert_into1_neg SHARED;
INSERT INTO TABLE insert_into1_neg SELECT * FROM src LIMIT 100;

DROP TABLE insert_into1_neg;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.compute.query.using.stats=true;
DROP TABLE insert_into2;
CREATE TABLE insert_into2 (key int, value string)
  PARTITIONED BY (ds string);

EXPLAIN INSERT INTO TABLE insert_into2 PARTITION (ds='1')
  SELECT * FROM src order by key LIMIT 100;
INSERT INTO TABLE insert_into2 PARTITION (ds='1') SELECT * FROM src order by key limit 100;
explain
select count (*) from insert_into2 where ds = '1';
select count (*) from insert_into2 where ds = '1';
INSERT INTO TABLE insert_into2 PARTITION (ds='1') SELECT * FROM src order by key limit 100;
explain
SELECT COUNT(*) FROM insert_into2 WHERE ds='1';
SELECT COUNT(*) FROM insert_into2 WHERE ds='1';
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into2
) t;

EXPLAIN INSERT OVERWRITE TABLE insert_into2 PARTITION (ds='2')
  SELECT * FROM src order by key LIMIT 100;
INSERT OVERWRITE TABLE insert_into2 PARTITION (ds='2')
  SELECT * FROM src order by key LIMIT 100;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into2
) t;
explain
SELECT COUNT(*) FROM insert_into2 WHERE ds='2';
SELECT COUNT(*) FROM insert_into2 WHERE ds='2';

EXPLAIN INSERT OVERWRITE TABLE insert_into2 PARTITION (ds='2')
  SELECT * FROM src order by key LIMIT 50;
INSERT OVERWRITE TABLE insert_into2 PARTITION (ds='2')
  SELECT * FROM src order by key LIMIT 50;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into2
) t;
explain
SELECT COUNT(*) FROM insert_into2 WHERE ds='2';
SELECT COUNT(*) FROM insert_into2 WHERE ds='2';

set hive.stats.autogather=false;

insert into table insert_into2 partition (ds='2') values(1, 'abc');
explain
SELECT COUNT(*) FROM insert_into2 where ds='2';
select count(*) from insert_into2 where ds='2';


DROP TABLE insert_into2;

set hive.stats.autogather=true;
set hive.compute.query.using.stats=false;
set hive.lock.numretries=5;
set hive.lock.sleep.between.retries=5;

DROP TABLE insert_into1_neg;
CREATE TABLE insert_into1_neg (key int, value string);

LOCK TABLE insert_into1_neg EXCLUSIVE;
INSERT INTO TABLE insert_into1_neg SELECT * FROM src LIMIT 100;

DROP TABLE insert_into1_neg;
DROP TABLE insert_into3a;
DROP TABLE insert_into3b;

CREATE TABLE insert_into3a (key int, value string);
CREATE TABLE insert_into3b (key int, value string);

EXPLAIN FROM src INSERT INTO TABLE insert_into3a SELECT * ORDER BY key, value LIMIT 50
                 INSERT INTO TABLE insert_into3b SELECT * ORDER BY key, value LIMIT 100;
FROM src INSERT INTO TABLE insert_into3a SELECT * ORDER BY key, value LIMIT 50
         INSERT INTO TABLE insert_into3b SELECT * ORDER BY key, value LIMIT 100;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into3a
) t;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into3b
) t;

EXPLAIN FROM src INSERT OVERWRITE TABLE insert_into3a SELECT * LIMIT 10
                 INSERT INTO TABLE insert_into3b SELECT * LIMIT 10;
FROM src INSERT OVERWRITE TABLE insert_into3a SELECT * LIMIT 10
         INSERT INTO TABLE insert_into3b SELECT * LIMIT 10;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into3a
) t;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into3b
) t;

DROP TABLE insert_into3a;
DROP TABLE insert_into3b;
set hive.lock.numretries=5;
set hive.lock.sleep.between.retries=5;

DROP TABLE insert_into3_neg;

CREATE TABLE insert_into3_neg (key int, value string)
  PARTITIONED BY (ds string);

INSERT INTO TABLE insert_into3_neg PARTITION (ds='1')
  SELECT * FROM src LIMIT 100;

LOCK TABLE insert_into3_neg PARTITION (ds='1') SHARED;
INSERT INTO TABLE insert_into3_neg PARTITION (ds='1')
  SELECT * FROM src LIMIT 100;

DROP TABLE insert_into3_neg;
set hive.merge.smallfiles.avgsize=16000000;

DROP TABLE insert_into4a;
DROP TABLE insert_into4b;

CREATE TABLE insert_into4a (key int, value string);
CREATE TABLE insert_into4b (key int, value string);

EXPLAIN INSERT INTO TABLE insert_into4a SELECT * FROM src LIMIT 10;
INSERT INTO TABLE insert_into4a SELECT * FROM src LIMIT 10;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into4a
) t;

EXPLAIN INSERT INTO TABLE insert_into4a SELECT * FROM src LIMIT 10;
INSERT INTO TABLE insert_into4a SELECT * FROM src LIMIT 10;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into4a
) t;

--At this point insert_into4a has 2 files (if INSERT INTO merges isn't fixed)

EXPLAIN INSERT INTO TABLE insert_into4b SELECT * FROM insert_into4a;
INSERT INTO TABLE insert_into4b SELECT * FROM insert_into4a;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into4b
) t;

DROP TABLE insert_into4a;
DROP TABLE insert_into4b;
set hive.lock.numretries=5;
set hive.lock.sleep.between.retries=5;

DROP TABLE insert_into3_neg;

CREATE TABLE insert_into3_neg (key int, value string)
  PARTITIONED BY (ds string);

INSERT INTO TABLE insert_into3_neg PARTITION (ds='1')
  SELECT * FROM src LIMIT 100;

LOCK TABLE insert_into3_neg PARTITION (ds='1') EXCLUSIVE;
INSERT INTO TABLE insert_into3_neg PARTITION (ds='1')
  SELECT * FROM src LIMIT 100;

DROP TABLE insert_into3_neg;
set hive.mapred.mode=nonstrict;
DROP TABLE insert_into5a;
DROP TABLE insert_into5b;

CREATE TABLE insert_into5a (key int, value string);
CREATE TABLE insert_into5b (key int, value string) PARTITIONED BY (ds string);

EXPLAIN INSERT INTO TABLE insert_into5a SELECT 1, 'one' FROM src LIMIT 10;
INSERT INTO TABLE insert_into5a SELECT 1, 'one' FROM src LIMIT 10;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into5a
) t;

EXPLAIN INSERT INTO TABLE insert_into5a SELECT * FROM insert_into5a;
INSERT INTO TABLE insert_into5a SELECT * FROM insert_into5a;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into5a
) t;

EXPLAIN INSERT INTO TABLE insert_into5b PARTITION (ds='1')
  SELECT * FROM insert_into5a;
INSERT INTO TABLE insert_into5b PARTITION (ds='1') SELECT * FROM insert_into5a;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into5b
) t;

EXPLAIN INSERT INTO TABLE insert_into5b PARTITION (ds='1')
  SELECT key, value FROM insert_into5b;
INSERT INTO TABLE insert_into5b PARTITION (ds='1')
  SELECT key, value FROM insert_into5b;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into5b
) t;

DROP TABLE insert_into5a;
DROP TABLE if exists insert_into5_neg;

CREATE TABLE insert_into5_neg (key int, value string) TBLPROPERTIES ("immutable"="true");

INSERT INTO TABLE insert_into5_neg SELECT * FROM src LIMIT 100;

INSERT INTO TABLE insert_into5_neg SELECT * FROM src LIMIT 100;

DROP TABLE insert_into5_neg;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

DROP TABLE insert_into6a;
DROP TABLE insert_into6b;
CREATE TABLE insert_into6a (key int, value string) PARTITIONED BY (ds string);
CREATE TABLE insert_into6b (key int, value string) PARTITIONED BY (ds string);

EXPLAIN INSERT INTO TABLE insert_into6a PARTITION (ds='1')
    SELECT * FROM src LIMIT 150;
INSERT INTO TABLE insert_into6a PARTITION (ds='1') SELECT * FROM src LIMIT 150;
INSERT INTO TABLE insert_into6a PARTITION (ds='2') SELECT * FROM src LIMIT 100;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into6a
) t;

EXPLAIN INSERT INTO TABLE insert_into6b PARTITION (ds)
    SELECT * FROM insert_into6a;
INSERT INTO TABLE insert_into6b PARTITION (ds) SELECT * FROM insert_into6a;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c) FROM insert_into6b
) t;

SHOW PARTITIONS insert_into6b;

DROP TABLE insert_into6a;
DROP TABLE insert_into6b;

set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS insert_into6_neg;

CREATE TABLE insert_into6_neg (key int, value string)
  PARTITIONED BY (ds string) TBLPROPERTIES("immutable"="true") ;

INSERT INTO TABLE insert_into6_neg PARTITION (ds='1')
  SELECT * FROM src LIMIT 100;

INSERT INTO TABLE insert_into6_neg PARTITION (ds='2')
  SELECT * FROM src LIMIT 100;

SELECT COUNT(*) from insert_into6_neg;

INSERT INTO TABLE insert_into6_neg PARTITION (ds='1')
  SELECT * FROM src LIMIT 100;

DROP TABLE insert_into6_neg;
set hive.mapred.mode=nonstrict;
-- set of tests HIVE-9481

drop database if exists x314 cascade;
create database x314;
use x314;
create table source(s1 int, s2 int);
create table target1(x int, y int, z int);
create table target2(x int, y int, z int);

insert into source(s2,s1) values(2,1);
-- expect source to contain 1 row (1,2)
select * from source;
insert into target1(z,x) select * from source;
-- expect target1 to contain 1 row (2,NULL,1)
select * from target1;

-- note that schema spec for target1 and target2 are different
from source insert into target1(x,y) select * insert into target2(x,z) select s2,s1;
--expect target1 to have 2rows (2,NULL,1), (1,2,NULL)
select * from target1 order by x,y,z;
-- expect target2 to have 1 row: (2,NULL,1)
select * from target2;


from source insert into target1(x,y,z) select null as x, * insert into target2(x,y,z) select null as x, source.*;
-- expect target1 to have 3 rows: (2,NULL,1), (1,2,NULL), (NULL, 1,2)
select * from target1 order by x,y,z;
-- expect target2 to have 2 rows: (2,NULL,1), (NULL, 1,2)
select * from target2 order by x,y,z;

truncate table target1;
create table source2(s1 int, s2 int);
insert into target1 (x,z) select source.s1,source2.s2 from source left outer join source2 on source.s1=source2.s2;
--expect target1 to have 1 row (1,NULL,NULL)
select * from target1;


-- partitioned tables
CREATE TABLE pageviews (userid VARCHAR(64), link STRING, source STRING) PARTITIONED BY (datestamp STRING, i int) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC;
INSERT INTO TABLE pageviews PARTITION (datestamp = '2014-09-23', i = 1)(userid,link) VALUES ('jsmith', 'mail.com');
-- expect 1 row: ('jsmith', 'mail.com', NULL) in partition '2014-09-23'/'1'
select * from pageviews;


-- dynamic partitioning



INSERT INTO TABLE pageviews PARTITION (datestamp='2014-09-23',i)(userid,i,link) VALUES ('jsmith', 7, '7mail.com');

set hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE pageviews PARTITION (datestamp,i)(userid,i,link,datestamp) VALUES ('jsmith', 17, '17mail.com', '2014-09-23');
INSERT INTO TABLE pageviews PARTITION (datestamp,i)(userid,i,link,datestamp) VALUES ('jsmith', 19, '19mail.com', '2014-09-24');
-- here the 'datestamp' partition column is not provided and will be NULL-filled
INSERT INTO TABLE pageviews PARTITION (datestamp,i)(userid,i,link) VALUES ('jsmith', 23, '23mail.com');
-- expect 5 rows:
-- expect ('jsmith', 'mail.com', NULL) in partition '2014-09-23'/'1'
-- expect ('jsmith', '7mail.com', NULL) in partition '2014-09-23'/'7'
-- expect ('jsmith', '17mail.com', NULL) in partition '2014-09-23'/'17'
-- expect ('jsmith', '19mail.com', NULL) in partition '2014-09-24'/'19'
-- expect ('jsmith', '23mail.com', NULL) in partition '__HIVE_DEFAULT_PARTITION__'/'23'
select * from pageviews order by link;


drop database if exists x314 cascade;
set hive.mapred.mode=nonstrict;
-- set of tests HIVE-9481
drop database if exists x314n cascade;
create database x314n;
use x314n;
create table source(s1 int, s2 int);
--column number mismatch
insert into source(s2) values(2,1);

drop database if exists x314n cascade;
set hive.mapred.mode=nonstrict;
-- set of tests HIVE-9481
drop database if exists x314n cascade;
create database x314n;
use x314n;
create table source(s1 int, s2 int);

--number of columns mismatched
insert into source(s2,s1) values(1);

drop database if exists x314n cascade;set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS;



create table studenttab10k (age2 int);
insert into studenttab10k values(1);

create table student_acid (age int, grade int)
 clustered by (age) into 1 buckets;

insert into student_acid(age) select * from studenttab10k;

select * from student_acid;

insert into student_acid(grade, age) select 3 g, * from studenttab10k;

select * from student_acid;

insert into student_acid(grade, age) values(20, 2);

insert into student_acid(age) values(22);

select * from student_acid;

set hive.exec.dynamic.partition.mode=nonstrict;

drop table if exists acid_partitioned;
create table acid_partitioned (a int, c string)
  partitioned by (p int)
  clustered by (a) into 1 buckets;

insert into acid_partitioned partition (p) (a,p) values(1,2);

select * from acid_partitioned;
set hive.mapred.mode=nonstrict;
-- set of tests HIVE-9481
drop database if exists x314n cascade;
create database x314n;
use x314n;
create table source(s1 int, s2 int);
create table target1(x int, y int, z int);

--number of columns mismatched
insert into target1(x,y,z) select * from source;

drop database if exists x314n cascade;-- set of tests HIVE-9481
drop database if exists x314n cascade;
create database x314n;
use x314n;
create table target1(x int, y int, z int);
create table source(s1 int, s2 int);

--invalid column name
insert into target1(a,z) select * from source;


drop database if exists x314n cascade;-- set of tests HIVE-9481
drop database if exists x314n cascade;
create database x314n;
use x314n;

CREATE TABLE pageviews (userid VARCHAR(64), link STRING, source STRING) PARTITIONED BY (datestamp STRING, i int) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC;
--datestamp is a static partition thus should not be supplied by producer side
INSERT INTO TABLE pageviews PARTITION (datestamp='2014-09-23',i)(userid,i,datestamp,link) VALUES ('jsmith', 7, '2014-07-12', '7mail.com');

drop database if exists x314n cascade;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


-- SORT_QUERY_RESULTS

-- This test checks that selecting from an acid table and inserting into a non-acid table works.
create table sample_06(name varchar(50), age int, gpa decimal(3, 2)) clustered by (age) into 2 buckets stored as orc TBLPROPERTIES ("transactional"="true");
insert into table sample_06 values ('aaa', 35, 3.00), ('bbb', 32, 3.00), ('ccc', 32, 3.00), ('ddd', 35, 3.00), ('eee', 32, 3.00);
select * from sample_06 where gpa = 3.00;

create table tab1 (name varchar(50), age int, gpa decimal(3, 2));
insert into table tab1 select * from sample_06 where gpa = 3.00;
select * from tab1;

drop table if exists table_with_utf8_encoding;

create table table_with_utf8_encoding (name STRING)
 ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
 WITH SERDEPROPERTIES ('serialization.encoding'='utf-8');

load data local inpath '../../data/files/encoding-utf8.txt' overwrite into table table_with_utf8_encoding;

select * from table_with_utf8_encoding;

drop table if exists table_with_non_utf8_encoding;

create table table_with_non_utf8_encoding (name STRING)
 ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
 WITH SERDEPROPERTIES ('serialization.encoding'='ISO8859_1');

insert overwrite table table_with_non_utf8_encoding  select name  from table_with_utf8_encoding;

select * from table_with_non_utf8_encoding;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

create table acid_iot(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cstring2 STRING,
    ctimestamp1 TIMESTAMP,
    ctimestamp2 TIMESTAMP,
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN) clustered by (cint) into 1 buckets stored as orc TBLPROPERTIES ('transactional'='true');

LOAD DATA LOCAL INPATH "../../data/files/alltypesorc" into table acid_iot;

select count(*) from acid_iot;

insert into table acid_iot select ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2,
       cboolean1, cboolean2 from alltypesorc where cint < 0 order by cint limit 10;

select count(*) from acid_iot;

insert overwrite directory '../../data/files/src_table_1'
select * from src ;
dfs -cat ../../data/files/src_table_1/000000_0;

insert overwrite directory '../../data/files/src_table_2'
row format delimited
FIELDS TERMINATED BY ':'
select * from src ;

dfs -cat ../../data/files/src_table_2/000000_0;

create table array_table (a array<string>, b array<string>)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ',';

load data local inpath "../../data/files/array_table.txt" overwrite into table array_table;

insert overwrite directory '../../data/files/array_table_1'
select * from array_table;
dfs -cat ../../data/files/array_table_1/000000_0;

insert overwrite directory '../../data/files/array_table_2'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY '#'
select * from array_table;

dfs -cat ../../data/files/array_table_2/000000_0;

insert overwrite directory '../../data/files/array_table_2_withfields'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY '#'
select b,a from array_table;

dfs -cat ../../data/files/array_table_2_withfields/000000_0;


create table map_table (foo STRING , bar MAP<STRING, STRING>)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':'
STORED AS TEXTFILE;

load data local inpath "../../data/files/map_table.txt" overwrite into table map_table;

insert overwrite directory '../../data/files/map_table_1'
select * from map_table;
dfs -cat ../../data/files/map_table_1/000000_0;

insert overwrite directory '../../data/files/map_table_2'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY '#'
MAP KEYS TERMINATED BY '='
select * from map_table;

dfs -cat ../../data/files/map_table_2/000000_0;

insert overwrite directory '../../data/files/map_table_2_withfields'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY '#'
MAP KEYS TERMINATED BY '='
select bar,foo from map_table;

dfs -cat ../../data/files/map_table_2_withfields/000000_0;

insert overwrite directory '../../data/files/array_table_3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.DelimitedJSONSerDe'
STORED AS TEXTFILE
select * from array_table;

dfs -cat ../../data/files/array_table_3/000000_0;


insert overwrite directory '../../data/files/array_table_4'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
'serialization.format'= 'org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol',
'quote.delim'= '("|\\[|\\])',  'field.delim'=', ',
'serialization.null.format'='-'  ) STORED AS TEXTFILE
select a, null, b from array_table;

dfs -cat ../../data/files/array_table_4/000000_0;

insert overwrite directory '../../data/files/map_table_3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.DelimitedJSONSerDe'
STORED AS TEXTFILE
select * from map_table;

dfs -cat ../../data/files/map_table_3/000000_0;

insert overwrite directory '../../data/files/map_table_4'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
'serialization.format'= 'org.apache.hadoop.hive.serde2.thrift.TCTLSeparatedProtocol',
'quote.delim'= '("|\\[|\\])',  'field.delim'=', ',
'serialization.null.format'='-'  ) STORED AS TEXTFILE
select foo, null, bar from map_table;

dfs -cat ../../data/files/map_table_4/000000_0;

insert overwrite directory '../../data/files/rctable'
STORED AS RCFILE
select value,key from src;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/rctable/temp;
dfs -rmr ${system:test.tmp.dir}/rctable;
dfs ${system:test.dfs.mkdir}  ${system:test.tmp.dir}/rctable;
dfs -put ../../data/files/rctable/000000_0 ${system:test.tmp.dir}/rctable/000000_0;

create external table rctable(value string, key string)
STORED AS RCFILE
LOCATION '${system:test.tmp.dir}/rctable';

insert overwrite directory '../../data/files/rctable_out'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
select key,value from rctable;

dfs -cat ../../data/files/rctable_out/000000_0;

drop table rctable;
drop table array_table;
drop table map_table;
dfs -rmr ${system:test.tmp.dir}/rctable;
dfs -rmr ../../data/files/array_table_1;
dfs -rmr ../../data/files/array_table_2;
dfs -rmr ../../data/files/array_table_3;
dfs -rmr ../../data/files/array_table_4;
dfs -rmr ../../data/files/map_table_1;
dfs -rmr ../../data/files/map_table_2;
dfs -rmr ../../data/files/map_table_3;
dfs -rmr ../../data/files/map_table_4;
dfs -rmr ../../data/files/rctable;
dfs -rmr ../../data/files/rctable_out;
dfs -rmr ../../data/files/src_table_1;
dfs -rmr ../../data/files/src_table_2;insert overwrite local directory '../../data/files/local_src_table_1'
select * from src ;
dfs -cat ../../data/files/local_src_table_1/000000_0;

insert overwrite local directory '../../data/files/local_src_table_2'
row format delimited
FIELDS TERMINATED BY ':'
select * from src ;

dfs -cat ../../data/files/local_src_table_2/000000_0;

create table array_table (a array<string>, b array<string>)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ',';

load data local inpath "../../data/files/array_table.txt" overwrite into table array_table;

insert overwrite local directory '../../data/files/local_array_table_1'
select * from array_table;
dfs -cat ../../data/files/local_array_table_1/000000_0;

insert overwrite local directory '../../data/files/local_array_table_2'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY '#'
select * from array_table;

dfs -cat ../../data/files/local_array_table_2/000000_0;

insert overwrite local directory '../../data/files/local_array_table_2_withfields'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY '#'
select b,a from array_table;

dfs -cat ../../data/files/local_array_table_2_withfields/000000_0;


create table map_table (foo STRING , bar MAP<STRING, STRING>)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':'
STORED AS TEXTFILE;

load data local inpath "../../data/files/map_table.txt" overwrite into table map_table;

insert overwrite local directory '../../data/files/local_map_table_1'
select * from map_table;
dfs -cat ../../data/files/local_map_table_1/000000_0;

insert overwrite local directory '../../data/files/local_map_table_2'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY '#'
MAP KEYS TERMINATED BY '='
select * from map_table;

dfs -cat ../../data/files/local_map_table_2/000000_0;

insert overwrite local directory '../../data/files/local_map_table_2_withfields'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ':'
COLLECTION ITEMS TERMINATED BY '#'
MAP KEYS TERMINATED BY '='
select bar,foo from map_table;

dfs -cat ../../data/files/local_map_table_2_withfields/000000_0;

insert overwrite local directory '../../data/files/local_array_table_3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.DelimitedJSONSerDe'
STORED AS TEXTFILE
select * from array_table;

dfs -cat ../../data/files/local_array_table_3/000000_0;

insert overwrite local directory '../../data/files/local_map_table_3'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.DelimitedJSONSerDe'
STORED AS TEXTFILE
select * from map_table;

dfs -cat ../../data/files/local_map_table_3/000000_0;

insert overwrite local directory '../../data/files/local_rctable'
STORED AS RCFILE
select value,key from src;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/local_rctable/temp;
dfs -rmr ${system:test.tmp.dir}/local_rctable;
dfs ${system:test.dfs.mkdir}  ${system:test.tmp.dir}/local_rctable;
dfs -put ../../data/files/local_rctable/000000_0 ${system:test.tmp.dir}/local_rctable/000000_0;

create external table local_rctable(value string, key string)
STORED AS RCFILE
LOCATION '${system:test.tmp.dir}/local_rctable';

insert overwrite local directory '../../data/files/local_rctable_out'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
select key,value from local_rctable;

dfs -cat ../../data/files/local_rctable_out/000000_0;

drop table local_rctable;
drop table array_table;
drop table map_table;
dfs -rmr ${system:test.tmp.dir}/local_rctable;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_insertsort(a int, b varchar(128)) clustered by (a) sorted by (b) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_insertsort select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_iud(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_iud select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint < 0 order by cint limit 10;

select a,b from acid_iud order by a;

update acid_iud set b = 'fred';

select a,b from acid_iud order by a;

delete from acid_iud;

select a,b from acid_iud order by a;

set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_notbucketed(a int, b varchar(128)) stored as orc;

insert into table acid_notbucketed values (1, 'abc'), (2, 'def');

select * from acid_notbucketed;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table ivdp(i int,
                 de decimal(5,2),
                 vc varchar(128)) partitioned by (ds string) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table ivdp partition (ds) values
    (1, 109.23, 'and everywhere that mary went', 'today'),
    (6553, 923.19, 'the lamb was sure to go', 'tomorrow');

select * from ivdp order by ds;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table insert_values_nonascii(t1 char(32), t2 string);

insert into insert_values_nonascii values("Абвгде Garçu 谢谢",  "Kôkaku ありがとう"), ("ございます", "kidôtai한국어");

select * from insert_values_nonascii;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_ivnp(ti tinyint,
                 si smallint,
                 i int,
                 bi bigint,
                 f float,
                 d double,
                 de decimal(5,2),
                 t timestamp,
                 dt date,
                 b boolean,
                 s string,
                 vc varchar(128),
                 ch char(12)) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_ivnp values
    (1, 257, 65537, 4294967297, 3.14, 3.141592654, 109.23, '2014-08-25 17:21:30.0', '2014-08-25', true, 'mary had a little lamb', 'ring around the rosie', 'red'),
    (null, null, null, null, null, null, null, null, null, null, null, null, null),
    (3, 25, 6553, null, 0.14, 1923.141592654, 1.2301, '2014-08-24 17:21:30.0', '2014-08-26', false, 'its fleece was white as snow', 'a pocket full of posies', 'blue' );

select * from acid_ivnp order by ti;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

create table acid_ivot(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cstring2 STRING,
    ctimestamp1 TIMESTAMP,
    ctimestamp2 TIMESTAMP,
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN) clustered by (cint) into 1 buckets stored as orc TBLPROPERTIES ('transactional'='true');

LOAD DATA LOCAL INPATH "../../data/files/alltypesorc" into table acid_ivot;

select count(*) from acid_ivot;

insert into table acid_ivot values
        (1, 2, 3, 4, 3.14, 2.34, 'fred', 'bob', '2014-09-01 10:34:23.111', '1944-06-06 06:00:00', true, true),
        (111, 222, 3333, 444, 13.14, 10239302.34239320, 'fred', 'bob', '2014-09-01 10:34:23.111', '1944-06-06 06:00:00', true, true);

select count(*) from acid_ivot;

set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_ivp(ti tinyint,
                 si smallint,
                 i int,
                 bi bigint,
                 f float,
                 d double,
                 de decimal(5,2),
                 t timestamp,
                 dt date,
                 s string,
                 vc varchar(128),
                 ch char(12)) partitioned by (ds string) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_ivp partition (ds='today') values
    (1, 257, 65537, 4294967297, 3.14, 3.141592654, 109.23, '2014-08-25 17:21:30.0', '2014-08-25', 'mary had a little lamb', 'ring around the rosie', 'red'),
    (3, 25, 6553, 429496729, 0.14, 1923.141592654, 1.2301, '2014-08-24 17:21:30.0', '2014-08-26', 'its fleece was white as snow', 'a pocket full of posies', 'blue');

select * from acid_ivp order by i;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_insertsort(a int, b varchar(128)) clustered by (a) sorted by (b) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_insertsort values (1, 'abc'),(2, 'def');
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create temporary table acid_ivtt(i int, de decimal(5,2), vc varchar(128)) clustered by (vc) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_ivtt values
    (1, 109.23, 'mary had a little lamb'),
    (429496729, 0.14, 'its fleece was white as snow'),
    (-29496729, -0.14, 'negative values test');

select i, de, vc from acid_ivtt order by i;
DROP VIEW xxx2;
CREATE VIEW xxx2 AS SELECT * FROM src;
INSERT OVERWRITE TABLE xxx2
SELECT key, value
FROM src;
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, src.value, 1 WHERE src.key < 100
select
  interval '10-11' year to month,
  interval '10' year,
  interval '11' month
from src limit 1;

select
  interval_year_month('10-11'),
  interval_year_month(cast('10-11' as string)),
  interval_year_month(cast('10-11' as varchar(10))),
  interval_year_month(cast('10-11' as char(10))),
  interval_year_month('10-11') = interval '10-11' year to month
from src limit 1;

-- Test normalization of interval values
select
  interval '49' month
from src limit 1;

select
  interval '10 9:8:7.987654321' day to second,
  interval '10' day,
  interval '11' hour,
  interval '12' minute,
  interval '13' second,
  interval '13.123456789' second
from src limit 1;

select
  interval_day_time('2 1:2:3'),
  interval_day_time(cast('2 1:2:3' as string)),
  interval_day_time(cast('2 1:2:3' as varchar(10))),
  interval_day_time(cast('2 1:2:3' as char(10))),
  interval_day_time('2 1:2:3') = interval '2 1:2:3' day to second
from src limit 1;

-- Test normalization of interval values
select
  interval '49' hour,
  interval '1470' minute,
  interval '90061.111111111' second
from src limit 1;
-- year-month/day-time intervals not compatible
select interval_day_time(interval '1' year) from src limit 1;
-- group-by/order-by/aggregation functions

select
  iym, count(*), min(key), max(key), min(iym), max(iym), min(idt), max(idt)
from (
  select
    key,
    interval_year_month(concat(key, '-1')) as iym,
    interval_day_time(concat(key, ' 1:1:1')) as idt
  from src) q1
group by iym
order by iym asc
limit 5;

select
  iym, count(*), min(key), max(key), min(iym), max(iym), min(idt), max(idt)
from (
  select
    key,
    interval_year_month(concat(key, '-1')) as iym,
    interval_day_time(concat(key, ' 1:1:1')) as idt
  from src) q1
group by iym
order by iym desc
limit 5;

-- same query as previous, with having clause
select
  iym, count(*), min(key), max(key), min(iym), max(iym), min(idt), max(idt)
from (
  select
    key,
    interval_year_month(concat(key, '-1')) as iym,
    interval_day_time(concat(key, ' 1:1:1')) as idt
  from src) q1
group by iym
having max(idt) > interval '496 0:0:0' day to second
order by iym desc
limit 5;

select
  idt, count(*), min(key), max(key), min(iym), max(iym), min(idt), max(idt)
from (
  select
    key,
    interval_year_month(concat(key, '-1')) as iym,
    interval_day_time(concat(key, ' 1:1:1')) as idt
  from src) q1
group by idt
order by idt asc
limit 5;

select
  idt, count(*), min(key), max(key), min(iym), max(iym), min(idt), max(idt)
from (
  select
    key,
    interval_year_month(concat(key, '-1')) as iym,
    interval_day_time(concat(key, ' 1:1:1')) as idt
  from src) q1
group by idt
order by idt desc
limit 5;

-- same query as previous, with having clause
select
  idt, count(*), min(key), max(key), min(iym), max(iym), min(idt), max(idt)
from (
  select
    key,
    interval_year_month(concat(key, '-1')) as iym,
    interval_day_time(concat(key, ' 1:1:1')) as idt
  from src) q1
group by idt
having max(iym) < interval '496-0' year to month
order by idt desc
limit 5;

select
  count(iym), count(idt), min(key), max(key), min(iym), max(iym), min(idt), max(idt)
from (
  select
    key,
    interval_year_month(concat(key, '-1')) as iym,
    interval_day_time(concat(key, ' 1:1:1')) as idt
  from src) q1;

-- year-month/day-time intervals not compatible
select interval '1' year - interval '365' day from src limit 1;

set hive.mapred.mode=nonstrict;
-- where clause
select
  l_orderkey, l_shipdate, l_receiptdate
from lineitem
  where (cast(l_shipdate as date) - date '1992-01-01') < interval '365 0:0:0' day to second
order by l_orderkey;

select
  l_orderkey, l_shipdate, l_receiptdate
from lineitem
  where (cast(l_shipdate as date) + interval '1-0' year to month) <= date '1994-01-01'
order by l_orderkey;

select
  l_orderkey, l_shipdate, l_receiptdate
from lineitem
  where (cast(l_shipdate as date) + interval '1-0' year to month) <= date '1994-01-01'
    and (cast(l_receiptdate as date) - cast(l_shipdate as date)) < interval '10' day
order by l_orderkey;


-- joins
select
  a.l_orderkey, b.l_orderkey, a.interval1
from
  (
    select
      l_orderkey, l_shipdate, l_receiptdate, (cast(l_receiptdate as date) - cast(l_shipdate as date)) as interval1
    from lineitem
  ) a
  join
  (
    select
      l_orderkey, l_shipdate, l_receiptdate, (cast(l_receiptdate as date) - date '1992-07-02') as interval2
    from lineitem
  ) b
  on a.interval1 = b.interval2 and a.l_orderkey = b.l_orderkey
order by a.l_orderkey;
-- year-month/day-time intervals not compatible
select interval '1' year + interval '365' day from src limit 1;

create table interval_arithmetic_1 (dateval date, tsval timestamp);
insert overwrite table interval_arithmetic_1
  select cast(ctimestamp1 as date), ctimestamp1 from alltypesorc;

-- interval year-month arithmetic
explain
select
  dateval,
  dateval - interval '2-2' year to month,
  dateval - interval '-2-2' year to month,
  dateval + interval '2-2' year to month,
  dateval + interval '-2-2' year to month,
  - interval '2-2' year to month + dateval,
  interval '2-2' year to month + dateval
from interval_arithmetic_1
limit 2;

select
  dateval,
  dateval - interval '2-2' year to month,
  dateval - interval '-2-2' year to month,
  dateval + interval '2-2' year to month,
  dateval + interval '-2-2' year to month,
  - interval '2-2' year to month + dateval,
  interval '2-2' year to month + dateval
from interval_arithmetic_1
limit 2;

explain
select
  dateval,
  dateval - date '1999-06-07',
  date '1999-06-07' - dateval,
  dateval - dateval
from interval_arithmetic_1
limit 2;

select
  dateval,
  dateval - date '1999-06-07',
  date '1999-06-07' - dateval,
  dateval - dateval
from interval_arithmetic_1
limit 2;

explain
select
  tsval,
  tsval - interval '2-2' year to month,
  tsval - interval '-2-2' year to month,
  tsval + interval '2-2' year to month,
  tsval + interval '-2-2' year to month,
  - interval '2-2' year to month + tsval,
  interval '2-2' year to month + tsval
from interval_arithmetic_1
limit 2;

select
  tsval,
  tsval - interval '2-2' year to month,
  tsval - interval '-2-2' year to month,
  tsval + interval '2-2' year to month,
  tsval + interval '-2-2' year to month,
  - interval '2-2' year to month + tsval,
  interval '2-2' year to month + tsval
from interval_arithmetic_1
limit 2;

explain
select
  interval '2-2' year to month + interval '3-3' year to month,
  interval '2-2' year to month - interval '3-3' year to month
from interval_arithmetic_1
limit 2;

select
  interval '2-2' year to month + interval '3-3' year to month,
  interval '2-2' year to month - interval '3-3' year to month
from interval_arithmetic_1
limit 2;


-- interval day-time arithmetic
explain
select
  dateval,
  dateval - interval '99 11:22:33.123456789' day to second,
  dateval - interval '-99 11:22:33.123456789' day to second,
  dateval + interval '99 11:22:33.123456789' day to second,
  dateval + interval '-99 11:22:33.123456789' day to second,
  -interval '99 11:22:33.123456789' day to second + dateval,
  interval '99 11:22:33.123456789' day to second + dateval
from interval_arithmetic_1
limit 2;

select
  dateval,
  dateval - interval '99 11:22:33.123456789' day to second,
  dateval - interval '-99 11:22:33.123456789' day to second,
  dateval + interval '99 11:22:33.123456789' day to second,
  dateval + interval '-99 11:22:33.123456789' day to second,
  -interval '99 11:22:33.123456789' day to second + dateval,
  interval '99 11:22:33.123456789' day to second + dateval
from interval_arithmetic_1
limit 2;

explain
select
  dateval,
  tsval,
  dateval - tsval,
  tsval - dateval,
  tsval - tsval
from interval_arithmetic_1
limit 2;

select
  dateval,
  tsval,
  dateval - tsval,
  tsval - dateval,
  tsval - tsval
from interval_arithmetic_1
limit 2;

explain
select
  tsval,
  tsval - interval '99 11:22:33.123456789' day to second,
  tsval - interval '-99 11:22:33.123456789' day to second,
  tsval + interval '99 11:22:33.123456789' day to second,
  tsval + interval '-99 11:22:33.123456789' day to second,
  -interval '99 11:22:33.123456789' day to second + tsval,
  interval '99 11:22:33.123456789' day to second + tsval
from interval_arithmetic_1
limit 2;

select
  tsval,
  tsval - interval '99 11:22:33.123456789' day to second,
  tsval - interval '-99 11:22:33.123456789' day to second,
  tsval + interval '99 11:22:33.123456789' day to second,
  tsval + interval '-99 11:22:33.123456789' day to second,
  -interval '99 11:22:33.123456789' day to second + tsval,
  interval '99 11:22:33.123456789' day to second + tsval
from interval_arithmetic_1
limit 2;

explain
select
  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
from interval_arithmetic_1
limit 2;

select
  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
from interval_arithmetic_1
limit 2;

drop table interval_arithmetic_1;

-- should all be true
select
  i1 = i1,
  i1 = i2,
  i1 >= i2,
  i1 <= i2,
  i3 = i3,
  i3 = i4,
  i3 <= i4,
  i3 >= i4,
  i1 < i3,
  i3 > i1,
  i1 != i3
from (
  select
    interval '2-0' year to month as i1,
    interval '2' year as i2,
    interval '2-1' year to month as i3,
    interval '25' month as i4
  from src limit 1
) q1;

-- should all be false
select
  i1 != i1,
  i1 != i2,
  i1 < i2,
  i1 > i2,
  i1 = i3,
  i1 > i3,
  i1 >= i3,
  i3 < i1,
  i3 <= i1
from (
  select
    interval '2-0' year to month as i1,
    interval '2' year as i2,
    interval '2-1' year to month as i3,
    interval '25' month as i4
  from src limit 1
) q1;

-- should all be true
select
  i1 = i1,
  i1 = i2,
  i1 >= i2,
  i1 <= i2,
  i3 = i3,
  i3 = i4,
  i3 <= i4,
  i3 >= i4,
  i1 < i3,
  i3 > i1,
  i1 != i3
from (
  select
    interval '1 0:0:0' day to second as i1,
    interval '24' hour as i2,
    interval '1 0:0:1' day to second as i3,
    interval '86401' second as i4
  from src limit 1
) q1;

-- should all be false
select
  i1 != i1,
  i1 != i2,
  i1 < i2,
  i1 > i2,
  i1 = i3,
  i1 > i3,
  i1 >= i3,
  i3 < i1,
  i3 <= i1
from (
  select
    interval '1 0:0:0' day to second as i1,
    interval '24' hour as i2,
    interval '1 0:0:1' day to second as i3,
    interval '86401' second as i4
  from src limit 1
) q1;


select
  year(iym), month(iym), day(idt), hour(idt), minute(idt), second(idt)
from (
  select interval '1-2' year to month iym, interval '3 4:5:6.789' day to second idt
  from src limit 1
) q;

DROP VIEW xxx8;
DROP VIEW xxx9;

-- create two levels of view reference, then invalidate intermediate view
-- by dropping a column from underlying table, and verify that
-- querying outermost view results in full error context
CREATE TABLE xxx10 (key int, value int);
CREATE VIEW xxx9 AS SELECT * FROM xxx10;
CREATE VIEW xxx8 AS SELECT * FROM xxx9 xxx;
ALTER TABLE xxx10 REPLACE COLUMNS (key int);
SELECT * FROM xxx8 yyy;
SELECT avg(*) FROM src;
create table tbl (a binary);
select cast (a as int) from tbl limit 1;
create table tbl (a binary);
select cast (a as tinyint) from tbl limit 1;
create table tbl (a binary);
select cast (a as smallint) from tbl limit 1;
create table tbl (a binary);
select cast (a as bigint) from tbl limit 1;
create table tbl (a binary);
select cast (a as float) from tbl limit 1;
create table tbl (a binary);
select cast (a as double) from tbl limit 1;
select cast (2 as binary) from src limit 1;
select cast(cast (2 as smallint) as binary) from src limit 1;
select cast(cast (2 as tinyint) as binary)  from src limit 1;
select cast(cast (2 as bigint) as binary) from src limit 1;
select cast(cast (2 as float) as binary)  from src limit 1;
select cast(cast (2 as double) as binary)  from src limit 1;
drop table invalid_char_length_1;
create table invalid_char_length_1 (c1 char(1000000));
select cast(value as char(100000)) from src limit 1;
drop table invalid_char_length_3;
create table invalid_char_length_3 (c1 char(0));

CREATE TABLE mytable (
  a INT
  b STRING
);

CREATE TABLE inv_valid_tbl1 COMMENT 'This is a thrift based table'
    PARTITIONED BY(aint DATETIME, country STRING)
    CLUSTERED BY(aint) SORTED BY(lint) INTO 32 BUCKETS
    ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer'
    WITH SERDEPROPERTIES ('serialization.class' = 'org.apache.hadoop.hive.serde2.thrift.test.Complex',
                          'serialization.format' = 'org.apache.thrift.protocol.TBinaryProtocol')
    STORED AS SEQUENCEFILE;
DESCRIBE EXTENDED inv_valid_tbl1;
create tabl tmp_zshao_22 (id int, name strin;
set hive.cbo.enable=false;
explain select hash(distinct value) from src;
explain select explode(distinct value) from src;
set hive.cbo.enable=false;
explain select hash(upper(distinct value)) from src;
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.value.member WHERE src.key < 100
FROM src
INSERT OVERWRITE TABLE dest1 SELECT substr('1234', 'abc'), src.value WHERE src.key < 100
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key[0], src.value
select /*+ MAPJOIN(a) ,MAPJOIN(b)*/ * from src a join src b on (a.key=b.key and a.value=b.value);
SELECT max(*) FROM src;
SELECT min(*) FROM src;
-- Verify that a stateful UDF cannot be used outside of the SELECT list

drop temporary function row_sequence;

add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

create temporary function row_sequence as
'org.apache.hadoop.hive.contrib.udf.UDFRowSequence';
set hive.mapred.mode=nonstrict;
select key
from (select key from src order by key) x
where row_sequence() < 5
order by key;
SELECT
  trim(trim(a))
  trim(b)
FROM src;
-- Create table
create table if not exists test_invalid_column(key string, value string ) partitioned by (year string, month string) stored as textfile ;

select * from test_invalid_column  where column1=123;
-- Create table
create table if not exists test_invalid_column(key string, value string ) partitioned by (year string, month string) stored as textfile ;

select * from (select * from test_invalid_column) subq where subq = 123;
-- Create table
create table if not exists test_invalid_column(key string, value string ) partitioned by (year string, month string) stored as textfile ;

select * from test_invalid_column  where test_invalid_column=123;
select foo from a a where foo > .foo;
SELECT stddev_samp(*) FROM src;
SELECT std(*) FROM src;
SELECT sum(*) FROM src;
create table invalid-name(a int, b string);
CREATE TABLE alter_test (d STRING);
ALTER TABLE alter_test CHANGE d d DATETIME;
CREATE TABLE alter_test (d STRING);
ALTER TABLE alter_test ADD COLUMNS (ds DATETIME);
CREATE TABLE datetime_test (d DATETIME);
SELECT TRANSFORM(*) USING 'cat' AS (key DATETIME) FROM src;
drop table if exists invalid_varchar_length_1;
create table invalid_varchar_length_1 (c1 varchar(1000000));
select cast(value as varchar(100000)) from src limit 1;
drop table if exists invalid_varchar_length_3;
create table invalid_varchar_length_3 (c1 varchar(0));

SELECT variance(*) FROM src;
SELECT var_samp(*) FROM src;
ADD JAR ivy://:udfexampleadd:1.0;

CREATE TEMPORARY FUNCTION example_add AS 'UDFExampleAdd';

EXPLAIN
SELECT example_add(1, 2),
       example_add(1, 2, 3),
       example_add(1, 2, 3, 4),
       example_add(1.1, 2.2),
       example_add(1.1, 2.2, 3.3),
       example_add(1.1, 2.2, 3.3, 4.4),
       example_add(1, 2, 3, 4.4)
FROM src LIMIT 1;

SELECT example_add(1, 2),
       example_add(1, 2, 3),
       example_add(1, 2, 3, 4),
       example_add(1.1, 2.2),
       example_add(1.1, 2.2, 3.3),
       example_add(1.1, 2.2, 3.3, 4.4),
       example_add(1, 2, 3, 4.4)
FROM src LIMIT 1;

DROP TEMPORARY FUNCTION example_add;

DELETE JAR ivy://:udfexampleadd:1.0;
CREATE TEMPORARY FUNCTION example_add AS 'UDFExampleAdd';
FROM (
  FROM src
   MAP value, key
 USING 'java -cp ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar org.apache.hadoop.hive.contrib.mr.example.IdentityMapper'
    AS k, v
 CLUSTER BY k) map_output
  REDUCE k, v
   USING 'java -cp ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar org.apache.hadoop.hive.contrib.mr.example.WordCountReduce'
   AS k, v
;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- JAVA_VERSION_SPECIFIC_OUTPUT
-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2;

EXPLAIN FORMATTED
SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2;

SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2;

FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

SELECT dest_j1.* FROM dest_j1;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

EXPLAIN FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
SELECT Y.*;

FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
SELECT Y.*;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100;

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 = src3.c5 AND src3.c5 < 80;

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 = src3.c5 AND src3.c5 < 80;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 + src2.c3 = src3.c5 AND src3.c5 < 200;

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 + src2.c3 = src3.c5 AND src3.c5 < 200;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)

CREATE TABLE dest1(c1 INT, c2 STRING) STORED AS TEXTFILE;

set mapreduce.framework.name=yarn;
set mapreduce.jobtracker.address=localhost:58;
set hive.exec.mode.local.auto=true;
set hive.exec.mode.local.auto.input.files.max=6;

EXPLAIN
FROM src JOIN srcpart ON src.key = srcpart.key AND srcpart.ds = '2008-04-08' and src.key > 100
INSERT OVERWRITE TABLE dest1 SELECT src.key, srcpart.value;

FROM src JOIN srcpart ON src.key = srcpart.key AND srcpart.ds = '2008-04-08' and src.key > 100
INSERT OVERWRITE TABLE dest1 SELECT src.key, srcpart.value;

select dest1.* from dest1;
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20, 0.20S)

CREATE TABLE dest1(c1 INT, c2 STRING) STORED AS TEXTFILE;

set mapred.job.tracker=localhost:58;
set hive.exec.mode.local.auto=true;

EXPLAIN
FROM src JOIN srcpart ON src.key = srcpart.key AND srcpart.ds = '2008-04-08' and src.key > 100
INSERT OVERWRITE TABLE dest1 SELECT src.key, srcpart.value;

FROM src JOIN srcpart ON src.key = srcpart.key AND srcpart.ds = '2008-04-08' and src.key > 100
INSERT OVERWRITE TABLE dest1 SELECT src.key, srcpart.value;

select dest1.* from dest1;
-- SORT_QUERY_RESULTS

EXPLAIN
SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key) SORT BY src1.key, src1.value, src2.key, src2.value;

SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key) SORT BY src1.key, src1.value, src2.key, src2.value;
set hive.mapred.mode=nonstrict;
EXPLAIN SELECT subq.key, tab.value FROM (select a.key, a.value from src a where a.key > 10 ) subq JOIN src tab ON (subq.key = tab.key and subq.key > 20 and subq.value = tab.value) where tab.value < 200;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key1 INT, value1 STRING, key2 INT, value2 STRING) STORED AS TEXTFILE;

EXPLAIN EXTENDED
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.*, src2.*;

FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.*, src2.*;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

EXPLAIN
 SELECT a.key, a.value, b.key, b.value
 FROM
  (
  SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key
  ) a
 FULL OUTER JOIN
 (
  SELECT src2.key as key, count(distinct(src2.value)) AS value
  FROM src1 src2 group by src2.key
 ) b
 ON (a.key = b.key);

 SELECT a.key, a.value, b.key, b.value
 FROM
  (
  SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key
  ) a
 FULL OUTER JOIN
 (
  SELECT src2.key as key, count(distinct(src2.value)) AS value
  FROM src1 src2 group by src2.key
 ) b
 ON (a.key = b.key);
-- SORT_QUERY_RESULTS

EXPLAIN
 SELECT a.key, a.value, b.key, b.value1,  b.value2
 FROM
  (
  SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key
  ) a
 FULL OUTER JOIN
 (
  SELECT src2.key as key, count(distinct(src2.value)) AS value1,
  count(distinct(src2.key)) AS value2
  FROM src1 src2 group by src2.key
 ) b
 ON (a.key = b.key);

 SELECT a.key, a.value, b.key, b.value1,  b.value2
 FROM
  (
  SELECT src1.key as key, count(src1.value) AS value FROM src src1 group by src1.key
  ) a
 FULL OUTER JOIN
 (
  SELECT src2.key as key, count(distinct(src2.value)) AS value1,
  count(distinct(src2.key)) AS value2
  FROM src1 src2 group by src2.key
 ) b
 ON (a.key = b.key);
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE triples (foo string, subject string, predicate string, object string, foo2 string);

EXPLAIN
SELECT t11.subject, t22.object , t33.subject , t55.object, t66.object
FROM
(
SELECT t1.subject
FROM triples t1
WHERE
t1.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL'
AND
t1.object='http://ontos/OntosMiner/Common.English/ontology#Citation'
) t11
JOIN
(
SELECT t2.subject , t2.object
FROM triples t2
WHERE
t2.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL'
) t22
ON (t11.subject=t22.subject)
JOIN
(
SELECT t3.subject , t3.object
FROM triples t3
WHERE
t3.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_from'

) t33
ON (t11.subject=t33.object)
JOIN
(
SELECT t4.subject
FROM triples t4
WHERE
t4.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__INSTANCEOF_REL'
AND
t4.object='http://ontos/OntosMiner/Common.English/ontology#Author'

) t44
ON (t44.subject=t33.subject)
JOIN
(
SELECT t5.subject, t5.object
FROM triples t5
WHERE
t5.predicate='http://www.ontosearch.com/2007/12/ontosofa-ns#_to'
) t55
ON (t55.subject=t44.subject)
JOIN
(
SELECT t6.subject, t6.object
FROM triples t6
WHERE
t6.predicate='http://sofa.semanticweb.org/sofa/v1.0/system#__LABEL_REL'
) t66
ON (t66.subject=t55.object);

set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j2(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;

FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
INSERT OVERWRITE TABLE dest_j2 SELECT src1.key, src3.value;

SELECT dest_j2.* FROM dest_j2;
FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value

SELECT /*+ MAPJOIN(x) */ x.key, x.value, y.value
FROM src1 x LEFT OUTER JOIN src y ON (x.key = y.key);



-- SORT_QUERY_RESULTS

EXPLAIN
SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;


EXPLAIN
SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key < 15) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key < 15) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;
-- SORT_QUERY_RESULTS

EXPLAIN
SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 LEFT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) RIGHT OUTER JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;
set hive.mapred.mode=nonstrict;
explain
SELECT src5.src1_value FROM (SELECT src3.*, src4.value as src4_value, src4.key as src4_key FROM src src4 JOIN (SELECT src2.*, src1.key as src1_key, src1.value as src1_value FROM src src1 JOIN src src2 ON src1.key = src2.key) src3 ON src3.src1_key = src4.key) src5;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

EXPLAIN
SELECT *  FROM src src1 JOIN src src2 WHERE src1.key < 10 and src2.key < 10 SORT BY src1.key, src1.value, src2.key, src2.value;

SELECT *  FROM src src1 JOIN src src2 WHERE src1.key < 10 and src2.key < 10 SORT BY src1.key, src1.value, src2.key, src2.value;
create table tst1(key STRING, cnt INT);

INSERT OVERWRITE TABLE tst1
SELECT a.key, count(1) FROM src a group by a.key;

SELECT sum(a.cnt)  FROM tst1 a JOIN tst1 b ON a.key = b.key;


-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE;

EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, x.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key);

INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, x.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key);

select * from dest_j1 x;



-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x,y) */ x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.key = z.key and z.ds='2008-04-08' and z.hr=11);

INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x,y) */ x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.key = z.key and z.ds='2008-04-08' and z.hr=11);

select * from dest_j1 x;



-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE;

EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, x.value, y.value
FROM src1 x JOIN src y ON (x.value = y.value);

INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, x.value, y.value
FROM src1 x JOIN src y ON (x.value = y.value);

select * from dest_j1;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING) STORED AS TEXTFILE;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT subq.key1, z.value
FROM
(SELECT x.key as key1, x.value as value1, y.key as key2, y.value as value2
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);

INSERT OVERWRITE TABLE dest_j1
SELECT subq.key1, z.value
FROM
(SELECT x.key as key1, x.value as value1, y.key as key2, y.value as value2
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);

select * from dest_j1;



CREATE TABLE dest_j1(key STRING, value STRING) STORED AS TEXTFILE;

-- Mapjoin followed by mapjoin is not supported.
-- The same query would work fine without the hint.
-- Note that there is a positive test with the same name in clientpositive
EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(z) */ subq.key1, z.value
FROM
(SELECT /*+ MAPJOIN(x) */ x.key as key1, x.value as value1, y.key as key2, y.value as value2
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);



set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, cnt1 INT, cnt2 INT);

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT subq1.key, subq1.cnt, subq2.cnt
FROM (select x.key, count(1) as cnt from src1 x group by x.key) subq1 JOIN
     (select y.key, count(1) as cnt from src y group by y.key) subq2 ON (subq1.key = subq2.key);

INSERT OVERWRITE TABLE dest_j1
SELECT subq1.key, subq1.cnt, subq2.cnt
FROM (select x.key, count(1) as cnt from src1 x group by x.key) subq1 JOIN
     (select y.key, count(1) as cnt from src y group by y.key) subq2 ON (subq1.key = subq2.key);

select * from dest_j1;
CREATE TABLE dest_j1(key STRING, cnt1 INT, cnt2 INT);

-- Mapjoin followed by group by is not supported.
-- The same query would work without the hint
-- Note that there is a positive test with the same name in clientpositive
EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(subq1) */ subq1.key, subq1.cnt, subq2.cnt
FROM (select x.key, count(1) as cnt from src1 x group by x.key) subq1 JOIN
     (select y.key, count(1) as cnt from src y group by y.key) subq2 ON (subq1.key = subq2.key);
FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key = src3.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value


set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key = src3.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;

FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key = src3.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;

SELECT dest1.* FROM dest1;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, cnt INT);

EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;

INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;

select * from dest_j1;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, cnt INT);

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT subq1.key, count(1) as cnt
FROM (select x.key, count(1) as cnt from src1 x group by x.key) subq1 JOIN
     (select y.key, count(1) as cnt from src y group by y.key) subq2 ON (subq1.key = subq2.key)
group by subq1.key;

INSERT OVERWRITE TABLE dest_j1
SELECT subq1.key, count(1) as cnt
FROM (select x.key, count(1) as cnt from src1 x group by x.key) subq1 JOIN
     (select y.key, count(1) as cnt from src y group by y.key) subq2 ON (subq1.key = subq2.key)
group by subq1.key;

select * from dest_j1;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

INSERT OVERWRITE TABLE dest_j1
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

select * from dest_j1;



CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;

-- Mapjoin followed by Mapjoin is not supported.
-- The same query would work without the hint
-- Note that there is a positive test with the same name in clientpositive
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x,z) */ x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);




set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;
CREATE TABLE dest_j2(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=6000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

INSERT OVERWRITE TABLE dest_j1
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

select * from dest_j1;

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT x.key, z.value, y.value
FROM src w JOIN src1 x ON (x.value = w.value)
JOIN src y ON (x.key = y.key)
JOIN src1 z ON (x.key = z.key);

INSERT OVERWRITE TABLE dest_j1
SELECT x.key, z.value, y.value
FROM src w JOIN src1 x ON (x.value = w.value)
JOIN src y ON (x.key = y.key)
JOIN src1 z ON (x.key = z.key);

select * from dest_j1;

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j2
SELECT res.key, z.value, res.value
FROM (select x.key, x.value from src1 x JOIN src y ON (x.key = y.key)) res
JOIN srcpart z ON (res.value = z.value and z.ds='2008-04-08' and z.hr=11);

INSERT OVERWRITE TABLE dest_j2
SELECT res.key, z.value, res.value
FROM (select x.key, x.value from src1 x JOIN src y ON (x.key = y.key)) res
JOIN srcpart z ON (res.value = z.value and z.ds='2008-04-08' and z.hr=11);

select * from dest_j2;

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j2
SELECT res.key, z.value, res.value
FROM (select x.key, x.value from src1 x LEFT OUTER JOIN src y ON (x.key = y.key)) res
JOIN srcpart z ON (res.value = z.value and z.ds='2008-04-08' and z.hr=11);

INSERT OVERWRITE TABLE dest_j2
SELECT res.key, z.value, res.value
FROM (select x.key, x.value from src1 x LEFT OUTER JOIN src y ON (x.key = y.key)) res
JOIN srcpart z ON (res.value = z.value and z.ds='2008-04-08' and z.hr=11);

select * from dest_j2;

EXPLAIN
INSERT OVERWRITE TABLE dest_j2
SELECT res.key, x.value, res.value
FROM (select x.key, x.value from src1 x JOIN src y ON (x.key = y.key)) res
JOIN srcpart x ON (res.value = x.value and x.ds='2008-04-08' and x.hr=11);

INSERT OVERWRITE TABLE dest_j2
SELECT res.key, x.value, res.value
FROM (select x.key, x.value from src1 x JOIN src y ON (x.key = y.key)) res
JOIN srcpart x ON (res.value = x.value and x.ds='2008-04-08' and x.hr=11);

select * from dest_j2;

EXPLAIN
INSERT OVERWRITE TABLE dest_j2
SELECT res.key, y.value, res.value
FROM (select x.key, x.value from src1 x JOIN src y ON (x.key = y.key)) res
JOIN srcpart y ON (res.value = y.value and y.ds='2008-04-08' and y.hr=11);

INSERT OVERWRITE TABLE dest_j2
SELECT res.key, y.value, res.value
FROM (select x.key, x.value from src1 x JOIN src y ON (x.key = y.key)) res
JOIN srcpart y ON (res.value = y.value and y.ds='2008-04-08' and y.hr=11);

select * from dest_j2;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

INSERT OVERWRITE TABLE dest_j1
SELECT x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

select * from dest_j1;



set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT x.key, x.value, subq1.value
FROM
( SELECT x.key as key, x.value as value from src x where x.key < 20
     UNION ALL
  SELECT x1.key as key, x1.value as value from src x1 where x1.key > 100
) subq1
JOIN src1 x ON (x.key = subq1.key);

INSERT OVERWRITE TABLE dest_j1
SELECT x.key, x.value, subq1.value
FROM
( SELECT x.key as key, x.value as value from src x where x.key < 20
     UNION ALL
  SELECT x1.key as key, x1.value as value from src x1 where x1.key > 100
) subq1
JOIN src1 x ON (x.key = subq1.key);

select * from dest_j1;



set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, val2 INT) STORED AS TEXTFILE;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT x.key, x.value, subq1.cnt
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.key as key, count(1) as cnt from src x1 where x1.key > 100 group by x1.key
) subq1
JOIN src1 x ON (x.key = subq1.key);

INSERT OVERWRITE TABLE dest_j1
SELECT x.key, x.value, subq1.cnt
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.key as key, count(1) as cnt from src x1 where x1.key > 100 group by x1.key
) subq1
JOIN src1 x ON (x.key = subq1.key);

select * from dest_j1;



CREATE TABLE dest_j1(key STRING, value STRING, val2 INT) STORED AS TEXTFILE;

-- Mapjoin followed by union is not supported.
-- The same query would work without the hint
-- Note that there is a positive test with the same name in clientpositive
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, x.value, subq1.cnt
FROM
( SELECT x.key as key, count(1) as cnt from src x where x.key < 20 group by x.key
     UNION ALL
  SELECT x1.key as key, count(1) as cnt from src x1 where x1.key > 100 group by x1.key
) subq1
JOIN src1 x ON (x.key = subq1.key);




-- SORT_QUERY_RESULTS

CREATE TABLE tmp1(key INT, cnt INT);
CREATE TABLE tmp2(key INT, cnt INT);
CREATE TABLE dest_j1(key INT, value INT, val2 INT);

INSERT OVERWRITE TABLE tmp1
SELECT key, count(1) from src group by key;

INSERT OVERWRITE TABLE tmp2
SELECT key, count(1) from src group by key;

EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, x.cnt, y.cnt
FROM tmp1 x JOIN tmp2 y ON (x.key = y.key);

INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x) */ x.key, x.cnt, y.cnt
FROM tmp1 x JOIN tmp2 y ON (x.key = y.key);

select * from dest_j1;



-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, value STRING, val2 STRING) STORED AS TEXTFILE;

EXPLAIN
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(X) */ x.key, x.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key);

INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(X) */ x.key, x.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key);

select * from dest_j1;





create table tmp(col0 string, col1 string,col2 string,col3 string,col4 string,col5 string,col6 string,col7 string,col8 string,col9 string,col10 string,col11 string);

insert overwrite table tmp select key, cast(key + 1 as int), key +2, key+3, key+4, cast(key+5 as int), key+6, key+7, key+8, key+9, key+10, cast(key+11 as int) from src where key = 100;

select * from tmp;

explain
FROM src a JOIN tmp b ON (a.key = b.col11)
SELECT /*+ MAPJOIN(a) */ a.value, b.col5, count(1) as count
where b.col11 = 111
group by a.value, b.col5;

FROM src a JOIN tmp b ON (a.key = b.col11)
SELECT /*+ MAPJOIN(a) */ a.value, b.col5, count(1) as count
where b.col11 = 111
group by a.value, b.col5;


-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, key1 string, val2 STRING) STORED AS TEXTFILE;

explain
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(y) */ x.key, x.value, y.key, y.value
FROM src x left outer JOIN (select * from src where key <= 100) y ON (x.key = y.key);


INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(y) */ x.key, x.value, y.key, y.value
FROM src x left outer JOIN (select * from src where key <= 100) y ON (x.key = y.key);

select * from dest_j1;



FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
SELECT c.c1, c.c2, c.c3, c.c4

set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.join.cache.size=1;

-- SORT_QUERY_RESULTS

EXPLAIN SELECT x.key, x.value, y.key, y.value
FROM src x left outer JOIN (select * from src where key <= 100) y ON (x.key = y.key);

SELECT x.key, x.value, y.key, y.value
FROM src x left outer JOIN (select * from src where key <= 100) y ON (x.key = y.key);


EXPLAIN select src1.key, src2.value
FROM src src1 JOIN src src2 ON (src1.key = src2.key);

select src1.key, src2.value
FROM src src1 JOIN src src2 ON (src1.key = src2.key);


EXPLAIN
SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;


EXPLAIN
SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key < 15) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key < 15) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key < 20)
SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;


EXPLAIN
SELECT /*+ MAPJOIN(y) */ x.key, x.value, y.key, y.value
FROM src x left outer JOIN (select * from src where key <= 100) y ON (x.key = y.key);

SELECT /*+ MAPJOIN(y) */ x.key, x.value, y.key, y.value
FROM src x left outer JOIN (select * from src where key <= 100) y ON (x.key = y.key);

EXPLAIN
SELECT COUNT(1) FROM SRC A JOIN SRC B ON (A.KEY=B.KEY);

SELECT COUNT(1) FROM SRC A JOIN SRC B ON (A.KEY=B.KEY);
set hive.mapred.mode=nonstrict;
create table s1 as select * from src where key = 0;

set hive.auto.convert.join.noconditionaltask=false;
EXPLAIN
SELECT * FROM s1 src1 LEFT OUTER JOIN s1 src2 ON (src1.key = src2.key AND src2.key > 10);
SELECT * FROM s1 src1 LEFT OUTER JOIN s1 src2 ON (src1.key = src2.key AND src2.key > 10);

set hive.auto.convert.join.noconditionaltask=true;

-- Make sure the big table is chosen correctly as part of HIVE-4146
EXPLAIN
SELECT * FROM s1 src1 LEFT OUTER JOIN s1 src2 ON (src1.key = src2.key AND src2.key > 10);
SELECT * FROM s1 src1 LEFT OUTER JOIN s1 src2 ON (src1.key = src2.key AND src2.key > 10);



set hive.mapred.mode=nonstrict;
create table L as select 4436 id;
create table LA as select 4436 loan_id, 4748 aid, 4415 pi_id;
create table FR as select 4436 loan_id;
create table A as select 4748 id;
create table PI as select 4415 id;

create table acct as select 4748 aid, 10 acc_n, 122 brn;
insert into table acct values(4748, null, null);
insert into table acct values(4748, null, null);

--[HIVE-10841] (WHERE col is not null) does not work sometimes for queries with many JOIN statements
explain select
  acct.ACC_N,
  acct.brn
FROM L
JOIN LA ON L.id = LA.loan_id
JOIN FR ON L.id = FR.loan_id
JOIN A ON LA.aid = A.id
JOIN PI ON PI.id = LA.pi_id
JOIN acct ON A.id = acct.aid
WHERE
  L.id = 4436
  and acct.brn is not null;

select
  acct.ACC_N,
  acct.brn
FROM L
JOIN LA ON L.id = LA.loan_id
JOIN FR ON L.id = FR.loan_id
JOIN A ON LA.aid = A.id
JOIN PI ON PI.id = LA.pi_id
JOIN acct ON A.id = acct.aid
WHERE
  L.id = 4436
  and acct.brn is not null;
set hive.mapred.mode=nonstrict;
create table purchase_history (s string, product string, price double, time int);
insert into purchase_history values ('1', 'Belt', 20.00, 21);
insert into purchase_history values ('1', 'Socks', 3.50, 31);
insert into purchase_history values ('3', 'Belt', 20.00, 51);
insert into purchase_history values ('4', 'Shirt', 15.50, 59);

create table cart_history (s string, cart_id int, time int);
insert into cart_history values ('1', 1, 10);
insert into cart_history values ('1', 2, 20);
insert into cart_history values ('1', 3, 30);
insert into cart_history values ('1', 4, 40);
insert into cart_history values ('3', 5, 50);
insert into cart_history values ('4', 6, 60);

create table events (s string, st2 string, n int, time int);
insert into events values ('1', 'Bob', 1234, 20);
insert into events values ('1', 'Bob', 1234, 30);
insert into events values ('1', 'Bob', 1234, 25);
insert into events values ('2', 'Sam', 1234, 30);
insert into events values ('3', 'Jeff', 1234, 50);
insert into events values ('4', 'Ted', 1234, 60);

explain
select s
from (
  select last.*, action.st2, action.n
  from (
    select purchase.s, purchase.time, max (mevt.time) as last_stage_time
    from (select * from purchase_history) purchase
    join (select * from cart_history) mevt
    on purchase.s = mevt.s
    where purchase.time > mevt.time
    group by purchase.s, purchase.time
  ) last
  join (select * from events) action
  on last.s = action.s and last.last_stage_time = action.time
) list;

select s
from (
  select last.*, action.st2, action.n
  from (
    select purchase.s, purchase.time, max (mevt.time) as last_stage_time
    from (select * from purchase_history) purchase
    join (select * from cart_history) mevt
    on purchase.s = mevt.s
    where purchase.time > mevt.time
    group by purchase.s, purchase.time
  ) last
  join (select * from events) action
  on last.s = action.s and last.last_stage_time = action.time
) list;

explain
select *
from (
  select last.*, action.st2, action.n
  from (
    select purchase.s, purchase.time, max (mevt.time) as last_stage_time
    from (select * from purchase_history) purchase
    join (select * from cart_history) mevt
    on purchase.s = mevt.s
    where purchase.time > mevt.time
    group by purchase.s, purchase.time
  ) last
  join (select * from events) action
  on last.s = action.s and last.last_stage_time = action.time
) list;

select *
from (
  select last.*, action.st2, action.n
  from (
    select purchase.s, purchase.time, max (mevt.time) as last_stage_time
    from (select * from purchase_history) purchase
    join (select * from cart_history) mevt
    on purchase.s = mevt.s
    where purchase.time > mevt.time
    group by purchase.s, purchase.time
  ) last
  join (select * from events) action
  on last.s = action.s and last.last_stage_time = action.time
) list;
set hive.cbo.enable=false;

-- SORT_QUERY_RESULTS

CREATE TABLE mytable(val1 INT, val2 INT, val3 INT);

EXPLAIN
SELECT *
FROM mytable src1, mytable src2
WHERE src1.val1=src2.val1
  AND src1.val2 between 2450816 and 2451500
  AND src2.val2 between 2450816 and 2451500;
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 RIGHT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
SELECT c.c1, c.c2, c.c3, c.c4


set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 RIGHT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 RIGHT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

SELECT dest1.* FROM dest1;
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
SELECT c.c1, c.c2, c.c3, c.c4



set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS TEXTFILE;

-- SORT_QUERY_RESULTS

EXPLAIN
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4;


SELECT dest1.* FROM dest1;
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 LEFT OUTER JOIN
 (
  FROM src src3 SELECT src3.key AS c5, src3.value AS c6 WHERE src3.key > 20 and src3.key < 25
 ) c
 ON (a.c1 = c.c5)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4, c.c5 AS c5, c.c6 AS c6
) c
SELECT c.c1, c.c2, c.c3, c.c4, c.c5, c.c6



set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING, c5 INT, c6 STRING) STORED AS TEXTFILE;

-- SORT_QUERY_RESULTS

EXPLAIN
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 LEFT OUTER JOIN
 (
  FROM src src3 SELECT src3.key AS c5, src3.value AS c6 WHERE src3.key > 20 and src3.key < 25
 ) c
 ON (a.c1 = c.c5)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4, c.c5 AS c5, c.c6 AS c6
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4, c.c5, c.c6;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 FULL OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 LEFT OUTER JOIN
 (
  FROM src src3 SELECT src3.key AS c5, src3.value AS c6 WHERE src3.key > 20 and src3.key < 25
 ) c
 ON (a.c1 = c.c5)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4, c.c5 AS c5, c.c6 AS c6
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4, c.c5, c.c6;

SELECT dest1.* FROM dest1;
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
SELECT c.c1, c.c2, c.c3, c.c4 where c.c3 IS NULL AND c.c1 IS NOT NULL

set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest1(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4 where c.c3 IS NULL AND c.c1 IS NOT NULL;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 LEFT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1 SELECT c.c1, c.c2, c.c3, c.c4 where c.c3 IS NULL AND c.c1 IS NOT NULL;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN EXTENDED
FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value where src1.ds = '2008-04-08' and src1.hr = '12';

FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value where src1.ds = '2008-04-08' and src1.hr = '12';

SELECT dest1.* FROM dest1;
EXPLAIN FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = b.key)
SELECT Y.*;
-- SORT_QUERY_RESULTS

CREATE TABLE join_1to1_1(key1 int, key2 int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in5.txt' INTO TABLE join_1to1_1;

CREATE TABLE join_1to1_2(key1 int, key2 int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in6.txt' INTO TABLE join_1to1_2;


set hive.outerjoin.supports.filters=false;

set hive.join.emit.interval=5;

SELECT * FROM join_1to1_1 a join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.value = 66 and b.value = 66;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2 and a.value = 66 and b.value = 66;

set hive.join.emit.interval=2;
SELECT * FROM join_1to1_1 a join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.value = 66 and b.value = 66;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2 and a.value = 66 and b.value = 66;

set hive.join.emit.interval=1;
SELECT * FROM join_1to1_1 a join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.value = 66 and b.value = 66;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2 and a.value = 66 and b.value = 66;



set hive.outerjoin.supports.filters=true;

set hive.join.emit.interval=5;

SELECT * FROM join_1to1_1 a join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.value = 66 and b.value = 66;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2 and a.value = 66 and b.value = 66;

set hive.join.emit.interval=2;
SELECT * FROM join_1to1_1 a join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.value = 66 and b.value = 66;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2 and a.value = 66 and b.value = 66;

set hive.join.emit.interval=1;
SELECT * FROM join_1to1_1 a join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.value = 66 and b.value = 66;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2;
SELECT * FROM join_1to1_1 a full outer join join_1to1_2 b on a.key1 = b.key1 and a.key2 = b.key2 and a.value = 66 and b.value = 66;

set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

CREATE TABLE orc_update_table (k1 INT, f1 STRING, op_code STRING)
CLUSTERED BY (k1) INTO 2 BUCKETS
STORED AS ORC TBLPROPERTIES("transactional"="true");

INSERT INTO TABLE orc_update_table VALUES (1, 'a', 'I');

CREATE TABLE orc_table (k1 INT, f1 STRING)
CLUSTERED BY (k1) SORTED BY (k1) INTO 2 BUCKETS
STORED AS ORC;

INSERT OVERWRITE TABLE orc_table VALUES (1, 'x');

set hive.cbo.enable=true;
SET hive.execution.engine=mr;
SET hive.auto.convert.join=false;
SET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
SET hive.conf.validation=false;
SET hive.doing.acid=false;

SELECT t1.*, t2.* FROM orc_table t1
JOIN orc_update_table t2 ON t1.k1=t2.k1 ORDER BY t1.k1;
set hive.mapred.mode=nonstrict;
explain select p1.p_name, p2.p_name
from part p1 , part p2;

explain select p1.p_name, p2.p_name, p3.p_name
from part p1 ,part p2 ,part p3
where p1.p_name = p2.p_name and p2.p_name = p3.p_name;

explain select p1.p_name, p2.p_name, p3.p_name
from part p1 , (select p_name from part) p2 ,part p3
where p1.p_name = p2.p_name and p2.p_name = p3.p_name;

explain select p1.p_name, p2.p_name, p3.p_name
from part p1 , part p2 , part p3
where p2.p_partkey + p1.p_partkey = p1.p_partkey and p3.p_name = p2.p_name;

explain select p1.p_name, p2.p_name, p3.p_name, p4.p_name
from part p1 , part p2 join part p3 on p2.p_name = p1.p_name join part p4
where p2.p_name = p3.p_name and p1.p_partkey = p4.p_partkey
            and p1.p_partkey = p2.p_partkey;

explain select p1.p_name, p2.p_name, p3.p_name, p4.p_name
from part p1 join part p2 on p2.p_name = p1.p_name , part p3  , part p4
where p2.p_name = p3.p_name and p1.p_partkey = p4.p_partkey
            and p1.p_partkey = p2.p_partkey;explain select *
from src s1 ,
src s2 on s1.key = s2.key;create table tinyA(a bigint, b bigint) stored as textfile;
create table tinyB(a bigint, bList array<int>) stored as textfile;

load data local inpath '../../data/files/tiny_a.txt' into table tinyA;
load data local inpath '../../data/files/tiny_b.txt' into table tinyB;

-- SORT_QUERY_RESULTS

select * from tinyA;
select * from tinyB;

select tinyB.a, tinyB.bList from tinyB full outer join tinyA on tinyB.a = tinyA.a;
-- SORT_QUERY_RESULTS

CREATE TABLE joinone(key1 int, key2 int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in5.txt' INTO TABLE joinone;

CREATE TABLE joinTwo(key1 int, key2 int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in6.txt' INTO TABLE joinTwo;

SELECT * FROM joinone JOIN joinTwo ON(joinone.key2=joinTwo.key2);
set hive.mapred.mode=nonstrict;
explain select *
from part p1 join part p2 join part p3 on p1.p_name = p2.p_name and p2.p_name = p3.p_name;

explain select *
from part p1 join part p2 join part p3 on p2.p_name = p1.p_name and p3.p_name = p2.p_name;

explain select *
from part p1 join part p2 join part p3 on p2.p_partkey + p1.p_partkey = p1.p_partkey and p3.p_name = p2.p_name;

explain select *
from part p1 join part p2 join part p3 on p2.p_partkey = 1 and p3.p_name = p2.p_name;
set hive.mapred.mode=nonstrict;
explain select *
from part p1 join part p2 join part p3 on p1.p_name = p2.p_name join part p4 on p2.p_name = p3.p_name and p1.p_name = p4.p_name;

explain select *
from part p1 join part p2 join part p3 on p2.p_name = p1.p_name join part p4 on p2.p_name = p3.p_name and p1.p_partkey = p4.p_partkey
            and p1.p_partkey = p2.p_partkey;
set hive.mapred.mode=nonstrict;
explain select *
from part p1 join part p2 join part p3
where p1.p_name = p2.p_name and p2.p_name = p3.p_name;

explain select *
from part p1 join part p2 join part p3
where p2.p_name = p1.p_name and p3.p_name = p2.p_name;

explain select *
from part p1 join part p2 join part p3
where p2.p_partkey + p1.p_partkey = p1.p_partkey and p3.p_name = p2.p_name;

explain select *
from part p1 join part p2 join part p3
where p2.p_partkey = 1 and p3.p_name = p2.p_name;
set hive.mapred.mode=nonstrict;
explain select *
from part p1 join part p2 join part p3 on p1.p_name = p2.p_name join part p4
where p2.p_name = p3.p_name and p1.p_name = p4.p_name;

explain select *
from part p1 join part p2 join part p3 on p2.p_name = p1.p_name join part p4
where p2.p_name = p3.p_name and p1.p_partkey = p4.p_partkey
            and p1.p_partkey = p2.p_partkey;
set hive.mapred.mode=nonstrict;
create table part2(
    p2_partkey INT,
    p2_name STRING,
    p2_mfgr STRING,
    p2_brand STRING,
    p2_type STRING,
    p2_size INT,
    p2_container STRING,
    p2_retailprice DOUBLE,
    p2_comment STRING
);

create table part3(
    p3_partkey INT,
    p3_name STRING,
    p3_mfgr STRING,
    p3_brand STRING,
    p3_type STRING,
    p3_size INT,
    p3_container STRING,
    p3_retailprice DOUBLE,
    p3_comment STRING
);

explain select *
from part p1 join part2 p2 join part3 p3 on p1.p_name = p2_name and p2_name = p3_name;

explain select *
from part p1 join part2 p2 join part3 p3 on p2_name = p1.p_name and p3_name = p2_name;

explain select *
from part p1 join part2 p2 join part3 p3 on p2_partkey + p_partkey = p1.p_partkey and p3_name = p2_name;

explain select *
from part p1 join part2 p2 join part3 p3 on p2_partkey = 1 and p3_name = p2_name;
set hive.mapred.mode=nonstrict;
create table part2(
    p2_partkey INT,
    p2_name STRING,
    p2_mfgr STRING,
    p2_brand STRING,
    p2_type STRING,
    p2_size INT,
    p2_container STRING,
    p2_retailprice DOUBLE,
    p2_comment STRING
);

create table part3(
    p3_partkey INT,
    p3_name STRING,
    p3_mfgr STRING,
    p3_brand STRING,
    p3_type STRING,
    p3_size INT,
    p3_container STRING,
    p3_retailprice DOUBLE,
    p3_comment STRING
);

explain select *
from part p1 join part2 p2 join part3 p3 on p1.p_name = p2_name join part p4 on p2_name = p3_name and p1.p_name = p4.p_name;

explain select *
from part p1 join part2 p2 join part3 p3 on p2_name = p1.p_name join part p4 on p2_name = p3_name and p1.p_partkey = p4.p_partkey
            and p1.p_partkey = p2_partkey;
set hive.mapred.mode=nonstrict;
create table part2(
    p2_partkey INT,
    p2_name STRING,
    p2_mfgr STRING,
    p2_brand STRING,
    p2_type STRING,
    p2_size INT,
    p2_container STRING,
    p2_retailprice DOUBLE,
    p2_comment STRING
);

create table part3(
    p3_partkey INT,
    p3_name STRING,
    p3_mfgr STRING,
    p3_brand STRING,
    p3_type STRING,
    p3_size INT,
    p3_container STRING,
    p3_retailprice DOUBLE,
    p3_comment STRING
);

explain select *
from part p1 join part2 p2 join part3 p3
where p1.p_name = p2_name and p2_name = p3_name;

explain select *
from part p1 join part2 p2 join part3 p3
where p2_name = p1.p_name and p3_name = p2_name;

explain select *
from part p1 join part2 p2 join part3 p3
where p2_partkey + p1.p_partkey = p1.p_partkey and p3_name = p2_name;

explain select *
from part p1 join part2 p2 join part3 p3
where p2_partkey = 1 and p3_name = p2_name;
set hive.mapred.mode=nonstrict;
create table part2(
    p2_partkey INT,
    p2_name STRING,
    p2_mfgr STRING,
    p2_brand STRING,
    p2_type STRING,
    p2_size INT,
    p2_container STRING,
    p2_retailprice DOUBLE,
    p2_comment STRING
);

create table part3(
    p3_partkey INT,
    p3_name STRING,
    p3_mfgr STRING,
    p3_brand STRING,
    p3_type STRING,
    p3_size INT,
    p3_container STRING,
    p3_retailprice DOUBLE,
    p3_comment STRING
);

explain select *
from part p1 join part2 p2 join part3 p3 on p1.p_name = p2_name join part p4
where p2_name = p3_name and p1.p_name = p4.p_name;

explain select *
from part p1 join part2 p2 join part3 p3 on p2_name = p1.p_name join part p4
where p2_name = p3_name and p1.p_partkey = p4.p_partkey
            and p1.p_partkey = p2_partkey;
-- outer join is not qualified for pushing down of where to join condition
CREATE TABLE ltable (index int, la int, lk1 string, lk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
CREATE TABLE rtable (ra int, rk1 string, rk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

insert into ltable values (1, null, 'CD5415192314304', '00071'), (2, null, 'CD5415192225530', '00071');
insert into rtable values (1, 'CD5415192314304', '00071'), (45, 'CD5415192314304', '00072');

set hive.auto.convert.join=false;
EXPLAIN SELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,'EMPTY')=COALESCE(r.ra,'EMPTY');
SELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,'EMPTY')=COALESCE(r.ra,'EMPTY');

set hive.auto.convert.join=true;
EXPLAIN SELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,'EMPTY')=COALESCE(r.ra,'EMPTY');
SELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,'EMPTY')=COALESCE(r.ra,'EMPTY');set hive.mapred.mode=nonstrict;


explain select s1.key, s2.key
from src s1, src s2
where key = s2.key
;set hive.mapred.mode=nonstrict;

explain select s1.key, s2.key
from src s1, src s2
where INPUT__FILE__NAME = s2.INPUT__FILE__NAME
;set hive.mapred.mode=nonstrict;
create table srcpart_empty(key int, value string) partitioned by (ds string);
create table src2_empty (key int, value string);

select /*+mapjoin(a)*/ a.key, b.value from srcpart_empty a join src b on a.key=b.key;
select /*+mapjoin(a)*/ a.key, b.value from src2_empty a join src b on a.key=b.key;

set hive.mapred.mode=nonstrict;
set hive.auto.convert.join = true;
select a.key, b.value from srcpart_empty a join src b on a.key=b.key;
select a.key, b.value from src2_empty a join src b on a.key=b.key;set hive.mapred.mode=nonstrict;
-- SORT_AND_HASH_QUERY_RESULTS

CREATE TABLE myinput1(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in3.txt' INTO TABLE myinput1;

SELECT * FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key and a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key=b.key and a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT * from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.value = c.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;
SELECT * from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT * from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.key = c.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;

SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

CREATE TABLE smb_input1(key int, value int) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE smb_input2(key int, value int) CLUSTERED BY (value) SORTED BY (value) INTO 2 BUCKETS;
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' into table smb_input1;
LOAD DATA LOCAL INPATH '../../data/files/in2.txt' into table smb_input1;
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' into table smb_input2;
LOAD DATA LOCAL INPATH '../../data/files/in2.txt' into table smb_input2;

SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
SET hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input2 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input2 a JOIN smb_input2 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key AND a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input2 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a JOIN smb_input2 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a JOIN smb_input2 b ON a.key = b.key AND a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input2 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a LEFT OUTER JOIN smb_input2 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input2 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input2 a RIGHT OUTER JOIN smb_input2 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SET hive.outerjoin.supports.filters = false;

SELECT * FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key and a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key=b.key and a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT * from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT * from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.value = c.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;
SELECT * from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT * from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.key = c.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;

SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input2 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input2 a JOIN smb_input2 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key AND a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input2 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a JOIN smb_input2 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a JOIN smb_input2 b ON a.key = b.key AND a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input2 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a LEFT OUTER JOIN smb_input2 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input2 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input2 a RIGHT OUTER JOIN smb_input2 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- HIVE-3411 Filter predicates on outer join overlapped on single alias is not handled properly

create table a as SELECT 100 as key, a.value as value FROM src LATERAL VIEW explode(array(40, 50, 60)) a as value limit 3;

-- overlap on a
explain extended select * from a left outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (a.key=c.key AND a.value=60 AND c.value=60);
select * from a left outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (a.key=c.key AND a.value=60 AND c.value=60);
select /*+ MAPJOIN(b,c)*/ * from a left outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (a.key=c.key AND a.value=60 AND c.value=60);

-- overlap on b
explain extended select * from a right outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (b.key=c.key AND b.value=60 AND c.value=60);
select * from a right outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (b.key=c.key AND b.value=60 AND c.value=60);
select /*+ MAPJOIN(a,c)*/ * from a right outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (b.key=c.key AND b.value=60 AND c.value=60);

-- overlap on b with two filters for each
explain extended select * from a right outer join a b on (a.key=b.key AND a.value=50 AND b.value=50 AND b.value>10) left outer join a c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60);
select * from a right outer join a b on (a.key=b.key AND a.value=50 AND b.value=50 AND b.value>10) left outer join a c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60);
select /*+ MAPJOIN(a,c)*/ * from a right outer join a b on (a.key=b.key AND a.value=50 AND b.value=50 AND b.value>10) left outer join a c on (b.key=c.key AND b.value=60 AND b.value>20 AND c.value=60);

-- overlap on a, b
explain extended select * from a full outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (b.key=c.key AND b.value=60 AND c.value=60) left outer join a d on (a.key=d.key AND a.value=40 AND d.value=40);
select * from a full outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (b.key=c.key AND b.value=60 AND c.value=60) left outer join a d on (a.key=d.key AND a.value=40 AND d.value=40);

-- triple overlap on a
explain extended select * from a left outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (a.key=c.key AND a.value=60 AND c.value=60) left outer join a d on (a.key=d.key AND a.value=40 AND d.value=40);
select * from a left outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (a.key=c.key AND a.value=60 AND c.value=60) left outer join a d on (a.key=d.key AND a.value=40 AND d.value=40);
select /*+ MAPJOIN(b,c, d)*/ * from a left outer join a b on (a.key=b.key AND a.value=50 AND b.value=50) left outer join a c on (a.key=c.key AND a.value=60 AND c.value=60) left outer join a d on (a.key=d.key AND a.value=40 AND d.value=40);
set hive.mapred.mode=nonstrict;
create table split    (id int, line_id int, orders string);
create table bar      (id int, line_id int, orders string);
create table foo      (id int, line_id int, orders string);
create table forecast (id int, line_id int, orders string);

set hive.auto.convert.join.noconditionaltask=false;

explain
SELECT foo.id, count(*) as factor from
 foo JOIN bar  ON (foo.id = bar.id and foo.line_id = bar.line_id)
 JOIN split    ON (foo.id = split.id and foo.line_id = split.line_id)
 JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id)
 WHERE foo.orders != 'blah'
 group by foo.id;

drop table split;
drop table bar;
drop table foo;
drop table forecast;

reset hive.auto.convert.join.noconditionaltask;
set hive.mapred.mode=nonstrict;




create table hive_foo (foo_id int, foo_name string, foo_a string, foo_b string,
foo_c string, foo_d string) row format delimited fields terminated by ','
stored as textfile;

create table hive_bar (bar_id int, bar_0 int, foo_id int, bar_1 int, bar_name
string, bar_a string, bar_b string, bar_c string, bar_d string) row format
delimited fields terminated by ',' stored as textfile;

create table hive_count (bar_id int, n int) row format delimited fields
terminated by ',' stored as textfile;

load data local inpath '../../data/files/hive_626_foo.txt' overwrite into table hive_foo;
load data local inpath '../../data/files/hive_626_bar.txt' overwrite into table hive_bar;
load data local inpath '../../data/files/hive_626_count.txt' overwrite into table hive_count;

explain
select hive_foo.foo_name, hive_bar.bar_name, n from hive_foo join hive_bar on hive_foo.foo_id =
hive_bar.foo_id join hive_count on hive_count.bar_id = hive_bar.bar_id;

select hive_foo.foo_name, hive_bar.bar_name, n from hive_foo join hive_bar on hive_foo.foo_id =
hive_bar.foo_id join hive_count on hive_count.bar_id = hive_bar.bar_id;





set hive.mapred.mode=nonstrict;
-- Test Joins with a variety of literals in the on clause

SELECT COUNT(*) FROM src a JOIN src b ON a.key = b.key AND a.key = 0L;

SELECT COUNT(*) FROM src a JOIN src b ON a.key = b.key AND a.key = 0S;

SELECT COUNT(*) FROM src a JOIN src b ON a.key = b.key AND a.key = 0Y;

SELECT COUNT(*) FROM src a JOIN src b ON a.key = b.key AND a.key = 0BD;
-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key STRING, value STRING, val2 STRING) STORED AS TEXTFILE;

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x,y) */ x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.key = z.key)
WHERE z.ds='2008-04-08' and z.hr=11;

INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x,y) */ x.key, z.value, y.value
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.key = z.key)
WHERE z.ds='2008-04-08' and z.hr=11;

select * from dest_j1;

CREATE TABLE src_copy(key int, value string);
CREATE TABLE src1_copy(key string, value string);
INSERT OVERWRITE TABLE src_copy select key, value from src;
INSERT OVERWRITE TABLE src1_copy select key, value from src1;

EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x,y) */ x.key, z.value, y.value
FROM src1_copy x JOIN src_copy y ON (x.key = y.key)
JOIN srcpart z ON (x.key = z.key)
WHERE z.ds='2008-04-08' and z.hr=11;

INSERT OVERWRITE TABLE dest_j1
SELECT /*+ MAPJOIN(x,y) */ x.key, z.value, y.value
FROM src1_copy x JOIN src_copy y ON (x.key = y.key)
JOIN srcpart z ON (x.key = z.key)
WHERE z.ds='2008-04-08' and z.hr=11;

select * from dest_j1;





set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;
-- SORT_QUERY_RESULTS
explain
select count(*) from srcpart a join srcpart b on a.key = b.key and a.hr = b.hr join srcpart c on a.hr = c.hr and a.key = c.key;
select count(*) from srcpart a join srcpart b on a.key = b.key and a.hr = b.hr join srcpart c on a.hr = c.hr and a.key = c.key;
set hive.mapred.mode=nonstrict;

explain select p1.p_size, p2.p_size
from part p1 left outer join part p2 on p1.p_partkey = p2.p_partkey
  right outer join part p3 on p2.p_partkey = p3.p_partkey and
              p1.p_size > 10
;

explain select p1.p_size, p2.p_size
from part p1 left outer join part p2 on p1.p_partkey = p2.p_partkey
  right outer join part p3 on p2.p_partkey = p3.p_partkey and
              p1.p_size > 10 and p1.p_size > p2.p_size + 10
;set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE myinput1(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' INTO TABLE myinput1;

SELECT * FROM myinput1 a JOIN myinput1 b;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.key = b.value;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.key = b.key;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.value = b.value;
SELECT * FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key=b.key;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key;
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key and a.value=b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key=b.key and a.value = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key = b.key;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.value = b.value and a.key=b.key;

SELECT * from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value);
SELECT * from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value) LEFT OUTER JOIN myinput1 c ON (b.value=c.value);
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.value = c.value;

SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.key;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key = b.key;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key = b.key;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key;
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value;

CREATE TABLE smb_input1(key int, value int) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE smb_input2(key int, value int) CLUSTERED BY (value) SORTED BY (value) INTO 2 BUCKETS;
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' into table smb_input1;
LOAD DATA LOCAL INPATH '../../data/files/in2.txt' into table smb_input1;
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' into table smb_input2;
LOAD DATA LOCAL INPATH '../../data/files/in2.txt' into table smb_input2;

SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
SET hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key AND a.value = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input1 b ON a.key = b.key;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key = b.key;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input1 b ON a.key = b.key;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input2 b ON a.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input2 b ON a.key = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input2 b ON a.key = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input2 b ON a.key = b.value;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input2 a JOIN smb_input2 b ON a.value = b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input2 a RIGHT OUTER JOIN smb_input2 b ON a.value = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a JOIN smb_input2 b ON a.value = b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a LEFT OUTER JOIN smb_input2 b ON a.value = b.value;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

CREATE TABLE myinput1(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in8.txt' INTO TABLE myinput1;

-- merging
explain select * from myinput1 a join myinput1 b on a.key<=>b.value;
-- SORT_QUERY_RESULTS
select * from myinput1 a join myinput1 b on a.key<=>b.value;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key=c.key;
select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key=c.key;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key<=>c.key;
select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key<=>c.key;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value=b.key join myinput1 c on a.key<=>c.key AND a.value=c.value;

select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value=b.key join myinput1 c on a.key<=>c.key AND a.value=c.value;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value<=>b.key join myinput1 c on a.key<=>c.key AND a.value<=>c.value;
select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value<=>b.key join myinput1 c on a.key<=>c.key AND a.value<=>c.value;

-- outer joins
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key<=>b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key<=>b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key<=>b.value;

-- map joins
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key<=>b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key<=>b.value;

CREATE TABLE smb_input(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in4.txt' into table smb_input;
LOAD DATA LOCAL INPATH '../../data/files/in5.txt' into table smb_input;


;

-- smbs
CREATE TABLE smb_input1(key int, value int) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE smb_input2(key int, value int) CLUSTERED BY (value) SORTED BY (value) INTO 2 BUCKETS;

from smb_input
insert overwrite table smb_input1 select *
insert overwrite table smb_input2 select *;

SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
SET hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key <=> b.key;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key <=> b.key AND a.value <=> b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input1 b ON a.key <=> b.key;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input1 b ON a.key <=> b.key;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input1 b ON a.key <=> b.key;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a JOIN smb_input2 b ON a.key <=> b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a JOIN smb_input2 b ON a.key <=> b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input1 a LEFT OUTER JOIN smb_input2 b ON a.key <=> b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input1 a RIGHT OUTER JOIN smb_input2 b ON a.key <=> b.value;

SELECT /*+ MAPJOIN(a) */ * FROM smb_input2 a JOIN smb_input2 b ON a.value <=> b.value;
SELECT /*+ MAPJOIN(a) */ * FROM smb_input2 a RIGHT OUTER JOIN smb_input2 b ON a.value <=> b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a JOIN smb_input2 b ON a.value <=> b.value;
SELECT /*+ MAPJOIN(b) */ * FROM smb_input2 a LEFT OUTER JOIN smb_input2 b ON a.value <=> b.value;

--HIVE-3315 join predicate transitive
explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.key is NULL;
select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.key is NULL;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

create table tbl1(c1 varchar(10), intcol int);
create table tbl2(c2 varchar(30));
insert into table tbl1 select repeat('t', 10), 11 from src limit 1;
insert into table tbl1 select repeat('s', 10), 22 from src limit 1;
insert into table tbl2 select concat(repeat('t', 10), 'ppp') from src limit 1;
insert into table tbl2 select repeat('s', 10) from src limit 1;

explain
select /*+ MAPJOIN(tbl2) */ c1,c2 from tbl1 join tbl2 on (c1 = c2) order by c1,c2;
select /*+ MAPJOIN(tbl2) */ c1,c2 from tbl1 join tbl2 on (c1 = c2) order by c1,c2;
set hive.mapred.mode=nonstrict;
explain
select srcpart.key, src1.value from
((srcpart inner join src on srcpart.key = src.key))
inner join src src1 on src1.value =srcpart.value;

explain
select srcpart.key, src1.value from
(srcpart inner join src on srcpart.key = src.key)
inner join src src1 on src1.value =srcpart.value;

explain
select srcpart.key, src1.value from
((srcpart inner join src on srcpart.key = src.key)
inner join src src1 on src1.value =srcpart.value);

explain
select srcpart.key, src1.value from
((srcpart inner join src on srcpart.key = src.key)
inner join src src1 on src1.value =srcpart.value)
inner join src src2 on src2.key = src1.key;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

create table join_rc1(key string, value string) stored as RCFile;
create table join_rc2(key string, value string) stored as RCFile;
insert overwrite table join_rc1 select * from src;
insert overwrite table join_rc2 select * from src;

explain
select join_rc1.key, join_rc2.value
FROM join_rc1 JOIN join_rc2 ON join_rc1.key = join_rc2.key;

select join_rc1.key, join_rc2.value
FROM join_rc1 JOIN join_rc2 ON join_rc1.key = join_rc2.key;



set hive.mapred.mode=nonstrict;




CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;
LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- SORT_QUERY_RESULTS

EXPLAIN FROM T1 a JOIN src c ON c.key+1=a.key
SELECT a.key, a.val, c.key;

EXPLAIN FROM T1 a JOIN src c ON c.key+1=a.key
SELECT /*+ STREAMTABLE(a) */ a.key, a.val, c.key;

FROM T1 a JOIN src c ON c.key+1=a.key
SELECT a.key, a.val, c.key;

FROM T1 a JOIN src c ON c.key+1=a.key
SELECT /*+ STREAMTABLE(a) */ a.key, a.val, c.key;

EXPLAIN FROM T1 a
  LEFT OUTER JOIN T2 b ON (b.key=a.key)
  RIGHT OUTER JOIN T3 c ON (c.val = a.val)
SELECT a.key, b.key, a.val, c.val;

EXPLAIN FROM T1 a
  LEFT OUTER JOIN T2 b ON (b.key=a.key)
  RIGHT OUTER JOIN T3 c ON (c.val = a.val)
SELECT /*+ STREAMTABLE(a) */ a.key, b.key, a.val, c.val;

FROM T1 a
  LEFT OUTER JOIN T2 b ON (b.key=a.key)
  RIGHT OUTER JOIN T3 c ON (c.val = a.val)
SELECT a.key, b.key, a.val, c.val;

FROM T1 a
  LEFT OUTER JOIN T2 b ON (b.key=a.key)
  RIGHT OUTER JOIN T3 c ON (c.val = a.val)
SELECT /*+ STREAMTABLE(a) */ a.key, b.key, a.val, c.val;

EXPLAIN FROM UNIQUEJOIN
  PRESERVE T1 a (a.key, a.val),
  PRESERVE T2 b (b.key, b.val),
  PRESERVE T3 c (c.key, c.val)
SELECT a.key, b.key, c.key;

EXPLAIN FROM UNIQUEJOIN
  PRESERVE T1 a (a.key, a.val),
  PRESERVE T2 b (b.key, b.val),
  PRESERVE T3 c (c.key, c.val)
SELECT /*+ STREAMTABLE(b) */ a.key, b.key, c.key;

FROM UNIQUEJOIN
  PRESERVE T1 a (a.key, a.val),
  PRESERVE T2 b (b.key, b.val),
  PRESERVE T3 c (c.key, c.val)
SELECT a.key, b.key, c.key;

FROM UNIQUEJOIN
  PRESERVE T1 a (a.key, a.val),
  PRESERVE T2 b (b.key, b.val),
  PRESERVE T3 c (c.key, c.val)
SELECT /*+ STREAMTABLE(b) */ a.key, b.key, c.key;









CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T4(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;
LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T4;

EXPLAIN
SELECT /*+ STREAMTABLE(a) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;

SELECT /*+ STREAMTABLE(a) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;


EXPLAIN
SELECT /*+ STREAMTABLE(a) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON a.val = c.val
          JOIN T4 d ON a.key + 1 = d.key + 1;


SELECT /*+ STREAMTABLE(a) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON a.val = c.val
          JOIN T4 d ON a.key + 1 = d.key + 1;











CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T4(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;
LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T4;

EXPLAIN
SELECT /*+ STREAMTABLE(a,c) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;

SELECT /*+ STREAMTABLE(a,c) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;


EXPLAIN
SELECT /*+ STREAMTABLE(a,c) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON a.val = c.val
          JOIN T4 d ON a.key + 1 = d.key + 1;


SELECT /*+ STREAMTABLE(a,c) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON a.val = c.val
          JOIN T4 d ON a.key + 1 = d.key + 1;






CREATE TABLE T1(key1 STRING, val1 STRING) STORED AS TEXTFILE;
CREATE TABLE T2(key2 STRING, val2 STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key3 STRING, val3 STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;
LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

set hive.auto.convert.join=true;

explain select /*+ STREAMTABLE(a) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
select /*+ STREAMTABLE(a) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;

explain select /*+ STREAMTABLE(b) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
select /*+ STREAMTABLE(b) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;

explain select /*+ STREAMTABLE(c) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
select /*+ STREAMTABLE(c) */ a.*, b.*, c.* from T1 a join T2 b on a.key1=b.key2 join T3 c on a.key1=c.key3;
set hive.mapred.mode=nonstrict;
create table fact(m1 int, m2 int, d1 int, d2 int);
create table dim1(f1 int, f2 int);
create table dim2(f3 int, f4 int);
create table dim3(f5 int, f6 int);
create table dim4(f7 int, f8 int);
create table dim5(f9 int, f10 int);
create table dim6(f11 int, f12 int);
create table dim7(f13 int, f14 int);

LOAD DATA LOCAL INPATH '../../data/files/fact-data.txt' INTO TABLE fact;
LOAD DATA LOCAL INPATH '../../data/files/dim-data.txt' INTO TABLE dim1;
LOAD DATA LOCAL INPATH '../../data/files/dim-data.txt' INTO TABLE dim2;
LOAD DATA LOCAL INPATH '../../data/files/dim-data.txt' INTO TABLE dim3;
LOAD DATA LOCAL INPATH '../../data/files/dim-data.txt' INTO TABLE dim4;
LOAD DATA LOCAL INPATH '../../data/files/dim-data.txt' INTO TABLE dim5;
LOAD DATA LOCAL INPATH '../../data/files/dim-data.txt' INTO TABLE dim6;
LOAD DATA LOCAL INPATH '../../data/files/dim-data.txt' INTO TABLE dim7;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=5000;

explain select m1, m2, f2 from fact join dim1 on fact.d1=dim1.f1;
select m1, m2, f2 from fact join dim1 on fact.d1=dim1.f1;

explain select m1, m2, f2, f4 from fact join dim1 on fact.d1=dim1.f1 join dim2 on fact.d2=dim2.f3;
select m1, m2, f2, f4 from fact join dim1 on fact.d1=dim1.f1 join dim2 on fact.d2=dim2.f3;

explain select m1, m2, f2, f4 from fact join dim1 on fact.d1= dim1.f1 join dim2 on dim1.f2 = dim2.f3;
select m1, m2, f2, f4 from fact join dim1 on fact.d1= dim1.f1 join dim2 on dim1.f2 = dim2.f3;

explain select m1, m2, f2, f4 from fact Left outer join dim1 on fact.d1= dim1.f1 Left outer join dim2 on dim1.f2 = dim2.f3;
select m1, m2, f2, f4 from fact Left outer join dim1 on fact.d1= dim1.f1 Left outer join dim2 on dim1.f2 = dim2.f3;

explain Select m1, m2, f2, f4, f6, f8, f10, f12, f14
 from fact
 Left outer join dim1 on  fact.d1= dim1.f1
 Left outer join dim2 on  dim1.f2 = dim2.f3
 Left outer Join dim3 on  fact.d2= dim3.f5
 Left outer Join dim4 on  dim3.f6= dim4.f7
 Left outer join dim5 on  dim4.f8= dim5.f9
 Left outer Join dim6 on  dim3.f6= dim6.f11
 Left outer Join dim7 on  dim6.f12 = dim7.f13;

Select m1, m2, f2, f4, f6, f8, f10, f12, f14
 from fact
 Left outer join dim1 on  fact.d1= dim1.f1
 Left outer join dim2 on  dim1.f2 = dim2.f3
 Left outer Join dim3 on  fact.d2= dim3.f5
 Left outer Join dim4 on  dim3.f6= dim4.f7
 Left outer join dim5 on  dim4.f8= dim5.f9
 Left outer Join dim6 on  dim3.f6= dim6.f11
 Left outer Join dim7 on  dim6.f12 = dim7.f13;

-- SORT_QUERY_RESULTS

DESCRIBE src_thrift;

EXPLAIN
SELECT s1.aint, s2.lintstring
FROM src_thrift s1
JOIN src_thrift s2
ON s1.aint = s2.aint;

SELECT s1.aint, s2.lintstring
FROM src_thrift s1
JOIN src_thrift s2
ON s1.aint = s2.aint;
set hive.mapred.mode=nonstrict;
-- see HIVE-4033 earlier a flag named hasVC was not initialized correctly in MapOperator.java, resulting in NPE for following query. order by and limit in the query is not relevant, problem would be evident even without those. They are there to keep .q.out file small and sorted.

-- SORT_QUERY_RESULTS

explain select t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value from src t1 join src t2 on t1.key = t2.key join src t3 on t2.value = t3.value order by t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value limit 3;

select t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value from src t1 join src t2 on t1.key = t2.key join src t3 on t2.value = t3.value order by t3.BLOCK__OFFSET__INSIDE__FILE,t3.key,t3.value limit 3;

explain
select t2.BLOCK__OFFSET__INSIDE__FILE
from src t1 join src t2 on t1.key = t2.key where t1.key < 100 order by t2.BLOCK__OFFSET__INSIDE__FILE;

select t2.BLOCK__OFFSET__INSIDE__FILE
from src t1 join src t2 on t1.key = t2.key where t1.key < 100 order by t2.BLOCK__OFFSET__INSIDE__FILE;
drop table invites;
drop table invites2;
create table invites (foo int, bar string) partitioned by (ds string);
create table invites2 (foo int, bar string) partitioned by (ds string);

set hive.mapred.mode=strict;

-- test join views: see HIVE-1989

create view v as select invites.bar, invites2.foo, invites2.ds from invites join invites2 on invites.ds=invites2.ds;

explain select * from v where ds='2011-09-01';

drop view v;
drop table invites;
drop table invites2;
add jar ${system:maven.local.repository}/org/apache/hive/hcatalog/hive-hcatalog-core/${system:hive.version}/hive-hcatalog-core-${system:hive.version}.jar;

drop table if exists json_serde1_1;
drop table if exists json_serde1_2;

create table json_serde1_1 (a array<string>,b map<string,int>)
  row format serde 'org.apache.hive.hcatalog.data.JsonSerDe';

insert into table json_serde1_1
  select array('aaa'),map('aaa',1) from src limit 2;

select * from json_serde1_1;

create table json_serde1_2 (
  a array<int>,
  b map<int,date>,
  c struct<c1:int, c2:string, c3:array<string>, c4:map<string, int>, c5:struct<c5_1:string, c5_2:int>>
) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe';

insert into table json_serde1_2
  select
    array(3, 2, 1),
    map(1, date '2001-01-01', 2, null),
    named_struct(
      'c1', 123456,
      'c2', 'hello',
      'c3', array('aa', 'bb', 'cc'),
      'c4', map('abc', 123, 'xyz', 456),
      'c5', named_struct('c5_1', 'bye', 'c5_2', 88))
  from src limit 2;

select * from json_serde1_2;

drop table json_serde1_1;
drop table json_serde1_2;

add jar ${system:maven.local.repository}/org/apache/hive/hcatalog/hive-hcatalog-core/${system:hive.version}/hive-hcatalog-core-${system:hive.version}.jar;

create table json_serde_qualified_types (
  c1 char(10),
  c2 varchar(20),
  c3 decimal(10, 5)
) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe';

describe json_serde_qualified_types;

drop table json_serde_qualified_types;

add jar ${system:maven.local.repository}/org/apache/hive/hcatalog/hive-hcatalog-core/${system:hive.version}/hive-hcatalog-core-${system:hive.version}.jar;

CREATE TABLE t1 (c1 int, c2 string, c3 timestamp)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
WITH SERDEPROPERTIES ('timestamp.formats'='yyyy-MM-dd\'T\'HH:mm:ss')
;
LOAD DATA LOCAL INPATH "../../data/files/tsformat.json" INTO TABLE t1;
select a.c1, a.c2, b.c3
from t1 a join t1 b on a.c1 = b.c1;

drop table t1;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

-- SORT_BEFORE_DIFF

create table test_user (`user` string, `group` string);
grant select on table test_user to user hive_test;

explain select `user` from test_user;

show grant user hive_test on table test_user;

drop table test_user;

create table test_user (role string, `group` string);
grant select on table test_user to user hive_test;

explain select role from test_user;

show grant user hive_test on table test_user;

drop table test_user;
set hive.support.sql11.reserved.keywords=false;
drop table varchar_udf_1;

create table varchar_udf_1 (c1 string, c2 string, c3 varchar(10), c4 varchar(20));
insert overwrite table varchar_udf_1
  select key, value, key, value from src where key = '238' limit 1;

select
  regexp(c2, 'val'),
  regexp(c4, 'val'),
  regexp(c2, 'val') = regexp(c4, 'val')
from varchar_udf_1 limit 1;

drop table varchar_udf_1;



CREATE TABLE tmp_pyang_lv (inputs string) STORED AS RCFILE;
INSERT OVERWRITE TABLE tmp_pyang_lv SELECT key FROM src;

EXPLAIN SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol SORT BY key ASC, myCol ASC LIMIT 1;
EXPLAIN SELECT myTable.* FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LIMIT 3;
EXPLAIN SELECT myTable.myCol, myTable2.myCol2 FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LATERAL VIEW explode(array('a', 'b', 'c')) myTable2 AS myCol2 LIMIT 9;
EXPLAIN SELECT myTable2.* FROM src LATERAL VIEW explode(array(array(1,2,3))) myTable AS myCol LATERAL VIEW explode(myTable.myCol) myTable2 AS myCol2 LIMIT 3;

-- Verify that * selects columns from both tables
SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol SORT BY key ASC, myCol ASC LIMIT 1;
-- TABLE.* should be supported
SELECT myTable.* FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LIMIT 3;
-- Multiple lateral views should result in a Cartesian product
SELECT myTable.myCol, myTable2.myCol2 FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LATERAL VIEW explode(array('a', 'b', 'c')) myTable2 AS myCol2 LIMIT 9;
-- Should be able to reference tables generated earlier
SELECT myTable2.* FROM src LATERAL VIEW explode(array(array(1,2,3))) myTable AS myCol LATERAL VIEW explode(myTable.myCol) myTable2 AS myCol2 LIMIT 3;

EXPLAIN
SELECT myCol from tmp_pyang_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;

SELECT myCol from tmp_PYANG_lv LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;

CREATE TABLE tmp_pyang_src_rcfile (key string, value array<string>) STORED AS RCFILE;
INSERT OVERWRITE TABLE tmp_pyang_src_rcfile SELECT key, array(value) FROM src ORDER BY key LIMIT 20;

SELECT key,value from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol;
SELECT myCol from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol;
SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol;

SELECT subq.key,subq.value
FROM (
SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
)subq;

SELECT subq.myCol
FROM (
SELECT * from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
)subq;

SELECT subq.key
FROM (
SELECT key, value from tmp_pyang_src_rcfile LATERAL VIEW explode(value) myTable AS myCol
)subq;

EXPLAIN SELECT value, myCol from (SELECT key, array(value[0]) AS value FROM tmp_pyang_src_rcfile GROUP BY value[0], key) a
LATERAL VIEW explode(value) myTable AS myCol;

SELECT value, myCol from (SELECT key, array(value[0]) AS value FROM tmp_pyang_src_rcfile GROUP BY value[0], key) a
LATERAL VIEW explode(value) myTable AS myCol;



-- Check alias count for LATERAL VIEW syntax:
-- explode returns a table with only 1 col - should be an error if query specifies >1 col aliases
SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol1, myCol2 LIMIT 3;--HIVE 3226
drop table array_valued_src;
create table array_valued_src (key string, value array<string>);
insert overwrite table array_valued_src select key, array(value) from src;

-- replace sel(*) to sel(exprs) for reflecting CP result properly
explain select count(val) from (select a.key as key, b.value as array_val from src a join array_valued_src b on a.key=b.key) i lateral view explode (array_val) c as val;
select count(val) from (select a.key as key, b.value as array_val from src a join array_valued_src b on a.key=b.key) i lateral view explode (array_val) c as val;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION explode2 AS 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2';

EXPLAIN SELECT col1, col2 FROM src LATERAL VIEW explode2(array(1,2,3)) myTable AS col1, col2 group by col1, col2 LIMIT 3;

SELECT col1, col2 FROM src LATERAL VIEW explode2(array(1,2,3)) myTable AS col1, col2 group by col1, col2 LIMIT 3;

DROP TEMPORARY FUNCTION explode2;add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION explode2 AS 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2';

SELECT col1, col2 FROM src LATERAL VIEW explode2(array(1,2,3)) myTable AS col1, col2 LIMIT 3;

DROP TEMPORARY FUNCTION explode2;SELECT src.key FROM src LATERAL VIEW explode(array(1,2,3)) AS myTable JOIN src b ON src.key;
set hive.fetch.task.conversion=more;

--HIVE-2608 Do not require AS a,b,c part in LATERAL VIEW
EXPLAIN SELECT myTab.* from src LATERAL VIEW explode(map('key1', 100, 'key2', 200)) myTab limit 2;
SELECT myTab.* from src LATERAL VIEW explode(map('key1', 100, 'key2', 200)) myTab limit 2;

EXPLAIN SELECT explode(map('key1', 100, 'key2', 200)) from src limit 2;
SELECT explode(map('key1', 100, 'key2', 200)) from src limit 2;

-- view
create view lv_noalias as SELECT myTab.* from src LATERAL VIEW explode(map('key1', 100, 'key2', 200)) myTab limit 2;

explain select * from lv_noalias a join lv_noalias b on a.key=b.key;
select * from lv_noalias a join lv_noalias b on a.key=b.key;CREATE TABLE lv_table( c1 STRING,  c2 ARRAY<INT>, c3 INT, c4 CHAR(1));
INSERT OVERWRITE TABLE lv_table SELECT 'abc  ', array(1,2,3), 100, 't' FROM src;

CREATE OR REPLACE VIEW lv_view AS SELECT * FROM lv_table;

EXPLAIN SELECT * FROM lv_view LATERAL VIEW explode(array(1,2,3)) myTable AS myCol SORT BY c1 ASC, myCol ASC LIMIT 1;
EXPLAIN SELECT myTable.* FROM lv_view LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LIMIT 3;
EXPLAIN SELECT myTable.myCol, myTable2.myCol2 FROM lv_view LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LATERAL VIEW explode(array('a', 'b', 'c')) myTable2 AS myCol2 LIMIT 9;
EXPLAIN SELECT myTable2.* FROM lv_view LATERAL VIEW explode(array(array(1,2,3))) myTable AS myCol LATERAL VIEW explode(myTable.myCol) myTable2 AS myCol2 LIMIT 3;

-- Verify that * selects columns from both tables
SELECT * FROM lv_view LATERAL VIEW explode(array(1,2,3)) myTable AS myCol SORT BY c1 ASC, myCol ASC LIMIT 1;
-- TABLE.* should be supported
SELECT myTable.* FROM lv_view LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LIMIT 3;
-- Multiple lateral views should result in a Cartesian product
SELECT myTable.myCol, myTable2.myCol2 FROM lv_view LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LATERAL VIEW explode(array('a', 'b', 'c')) myTable2 AS myCol2 LIMIT 9;
-- Should be able to reference tables generated earlier
SELECT myTable2.* FROM lv_view LATERAL VIEW explode(array(array(1,2,3))) myTable AS myCol LATERAL VIEW explode(myTable.myCol) myTable2 AS myCol2 LIMIT 3;

EXPLAIN
SELECT SIZE(c2),c3,TRIM(c1),c4,myCol from lv_view LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;

SELECT SIZE(c2),c3,TRIM(c1),c4,myCol from lv_view LATERAL VIEW explode(array(1,2,3)) myTab as myCol limit 3;

-- UDTF forwards nothing, OUTER LV add null for that
explain
select * from src LATERAL VIEW OUTER explode(array()) C AS a limit 10;
select * from src LATERAL VIEW OUTER explode(array()) C AS a limit 10;

-- backward compatible (UDTF forwards something for OUTER LV)
explain
select * from src LATERAL VIEW OUTER explode(array(4,5)) C AS a limit 10;
select * from src LATERAL VIEW OUTER explode(array(4,5)) C AS a limit 10;

create table array_valued as select key, if (key > 300, array(value, value), null) as value from src;

explain
select * from array_valued LATERAL VIEW OUTER explode(value) C AS a limit 10;
select * from array_valued LATERAL VIEW OUTER explode(value) C AS a limit 10;
set hive.optimize.ppd=true;

EXPLAIN SELECT value, myCol FROM (SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol) a WHERE key='0';
SELECT value, myCol FROM (SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol) a WHERE key='0';

EXPLAIN SELECT value, myCol FROM (SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol) a WHERE key='0' AND myCol=1;
SELECT value, myCol FROM (SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol) a WHERE key='0' AND myCol=1;

EXPLAIN SELECT value, myCol FROM (SELECT * FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol) a WHERE ds='2008-04-08' AND hr="12" LIMIT 12;
SELECT value, myCol FROM (SELECT * FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol) a WHERE ds='2008-04-08' AND hr="12" LIMIT 12;

EXPLAIN SELECT value, myCol FROM (SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LATERAL VIEW explode(array(1,2,3)) myTable2 AS myCol2) a WHERE key='0';
SELECT value, myCol FROM (SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol LATERAL VIEW explode(array(1,2,3)) myTable2 AS myCol2) a WHERE key='0';

-- HIVE-4293 Predicates following UDTF operator are removed by PPD
EXPLAIN SELECT value, myCol FROM (SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol WHERE myCol > 1) a WHERE key='0';
SELECT value, myCol FROM (SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS myCol WHERE myCol > 1) a WHERE key='0';set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;
set hive.stats.dbclass=fs;
-- Tests truncating a column from a list bucketing table

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

CREATE TABLE test_tab (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE;

ALTER TABLE test_tab SKEWED BY (key) ON ("484") STORED AS DIRECTORIES;

INSERT OVERWRITE TABLE test_tab PARTITION (part = '1') SELECT * FROM src;

describe formatted test_tab partition (part='1');
--1. testLagWithPTFWindowing
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_retailprice, sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part
partition by p_mfgr
order by p_name
);

-- 2. testLagWithWindowingNoPTF
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_retailprice, sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from part
;

-- 3. testJoinWithLag
select p1.p_mfgr, p1.p_name,
p1.p_size, p1.p_size - lag(p1.p_size,1,p1.p_size) over( distribute by p1.p_mfgr sort by p1.p_name) as deltaSz
from part p1 join part p2 on p1.p_partkey = p2.p_partkey
 ;

-- 4. testLagInSum
select  p_mfgr,p_name, p_size,
sum(p_size - lag(p_size,1)) over(distribute by p_mfgr  sort by p_name ) as deltaSum
from part
window w1 as (rows between 2 preceding and 2 following) ;

-- 5. testLagInSumOverWindow
select  p_mfgr,p_name, p_size,
sum(p_size - lag(p_size,1)) over w1 as deltaSum
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following) ;

set hive.cbo.enable=false;
-- 6. testRankInLead
-- disable cbo because of CALCITE-653

select p_mfgr, p_name, p_size, r1,
lead(r1,1,r1) over (distribute by p_mfgr sort by p_name) as deltaRank
from (
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr  sort by p_name) as r1
from part
) a;

set hive.cbo.enable=true;
-- 7. testLeadWithPTF
select p_mfgr, p_name,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
p_size, p_size - lead(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from noop(on part
partition by p_mfgr
order by p_name
)
;

-- 8. testOverNoPartitionMultipleAggregate
select p_name, p_retailprice,
lead(p_retailprice) over() as l1 ,
lag(p_retailprice) over() as l2
from part
where p_retailprice = 1173.15;

-- 1. testLeadUDAF
select p_mfgr, p_retailprice,
lead(p_retailprice) over (partition by p_mfgr order by p_name) as l1,
lead(p_retailprice,1) over (partition by p_mfgr order by p_name) as l2,
lead(p_retailprice,1,10) over (partition by p_mfgr order by p_name) as l3,
lead(p_retailprice,1, p_retailprice) over (partition by p_mfgr order by p_name) as l4,
p_retailprice - lead(p_retailprice,1,p_retailprice) over (partition by p_mfgr order by p_name)
from part;

-- 2.testLeadUDAFPartSz1
select p_mfgr, p_name, p_retailprice,
lead(p_retailprice,1) over (partition by p_mfgr, p_name ),
p_retailprice - lead(p_retailprice,1,p_retailprice) over (partition by p_mfgr, p_name)
from part;

-- 3.testLagUDAF
select p_mfgr, p_retailprice,
lag(p_retailprice,1) over (partition by p_mfgr order by p_name) as l1,
lag(p_retailprice) over (partition by p_mfgr order by p_name) as l2,
lag(p_retailprice,1, p_retailprice) over (partition by p_mfgr order by p_name) as l3,
lag(p_retailprice,1,10) over (partition by p_mfgr order by p_name) as l4,
p_retailprice - lag(p_retailprice,1,p_retailprice) over (partition by p_mfgr order by p_name)
from part;

-- 4.testLagUDAFPartSz1
select p_mfgr, p_name, p_retailprice,
lag(p_retailprice,1) over (partition by p_mfgr, p_name ),
p_retailprice - lag(p_retailprice,1,p_retailprice) over (partition by p_mfgr, p_name)
from part;

-- 5.testLeadLagUDAF
select p_mfgr, p_retailprice,
lead(p_retailprice,1) over (partition by p_mfgr order by p_name) as l1,
lead(p_retailprice,1, p_retailprice) over (partition by p_mfgr order by p_name) as l2,
p_retailprice - lead(p_retailprice,1,p_retailprice) over (partition by p_mfgr order by p_name),
lag(p_retailprice,1) over (partition by p_mfgr order by p_name) as l3,
lag(p_retailprice,1, p_retailprice) over (partition by p_mfgr order by p_name)  as l4
from part;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

drop table sales;
drop table things;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

CREATE TABLE sales (name STRING, id INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

CREATE TABLE things (id INT, name STRING) partitioned by (ds string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

load data local inpath '../../data/files/sales.txt' INTO TABLE sales;
load data local inpath '../../data/files/things.txt' INTO TABLE things partition(ds='2011-10-23');
load data local inpath '../../data/files/things2.txt' INTO TABLE things partition(ds='2011-10-24');

SELECT name,id FROM sales;

SELECT id,name FROM things;

SELECT name,id FROM sales LEFT SEMI JOIN things ON (sales.id = things.id);

drop table sales;
drop table things;
CREATE TABLE T1(key INT);
LOAD DATA LOCAL INPATH '../../data/files/leftsemijoin_mr_t1.txt' INTO TABLE T1;
CREATE TABLE T2(key INT);
LOAD DATA LOCAL INPATH '../../data/files/leftsemijoin_mr_t2.txt' INTO TABLE T2;

-- Run this query using TestMinimrCliDriver

SELECT * FROM T1;
SELECT * FROM T2;

set hive.auto.convert.join=false;
set mapred.reduce.tasks=2;

set hive.join.emit.interval=100;

SELECT T1.key FROM T1 LEFT SEMI JOIN (SELECT key FROM T2 SORT BY key) tmp ON (T1.key=tmp.key);

set hive.join.emit.interval=1;

SELECT T1.key FROM T1 LEFT SEMI JOIN (SELECT key FROM T2 SORT BY key) tmp ON (T1.key=tmp.key);
explain select * from src where key = '238' limit 0;
explain select src.key, count(src.value) from src group by src.key limit 0;
explain select * from ( select key from src limit 3) sq1 limit 0;

select * from src where key = '238' limit 0;
select src.key, count(src.value) from src group by src.key limit 0;
select * from ( select key from src limit 3) sq1 limit 0;
set hive.mapred.mode=nonstrict;
set hive.optimize.limittranspose=false;

explain
select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1;

select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1;


set hive.optimize.limittranspose=true;
set hive.optimize.limittranspose.reductionpercentage=0.0001f;
set hive.optimize.limittranspose.reductiontuples=10;

explain
select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1;

select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1;


set hive.optimize.limittranspose.reductionpercentage=0.1f;
set hive.optimize.limittranspose.reductiontuples=10;

explain
select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1;

select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 1;

select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 1;


set hive.optimize.limittranspose.reductionpercentage=1f;
set hive.optimize.limittranspose.reductiontuples=0;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 1;

select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 1;

explain
select *
from src src1 right outer join (
  select src2.key, src2.value
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
order by src2.key
limit 1;

select *
from src src1 right outer join (
  select src2.key, src2.value
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
order by src2.key
limit 1;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 0;

select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 0;


set hive.mapred.mode=nonstrict;
set hive.optimize.limittranspose=false;

explain
select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1 offset 1;

select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1 offset 1;


set hive.optimize.limittranspose=true;
set hive.optimize.limittranspose.reductionpercentage=0.0001f;
set hive.optimize.limittranspose.reductiontuples=10;

explain
select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1 offset 1;

select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1 offset 1;


set hive.optimize.limittranspose.reductionpercentage=0.1f;
set hive.optimize.limittranspose.reductiontuples=10;

explain
select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1 offset 1;

select *
from src src1 left outer join src src2
on src1.key = src2.key
limit 1 offset 1;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 1 offset 1;

select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 1 offset 1;


set hive.optimize.limittranspose.reductionpercentage=1f;
set hive.optimize.limittranspose.reductiontuples=0;

explain
select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 1 offset 1;

select *
from src src1 right outer join (
  select *
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
limit 1 offset 1;

explain
select *
from src src1 right outer join (
  select src2.key, src2.value
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
order by src2.key
limit 1 offset 1;

select *
from src src1 right outer join (
  select src2.key, src2.value
  from src src2 left outer join src src3
  on src2.value = src3.value) src2
on src1.key = src2.key
order by src2.key
limit 1 offset 1;
set hive.mapred.mode=nonstrict;
set hive.limit.query.max.table.partition=1;

explain select * from srcpart limit 1;
select * from srcpart limit 1;

explain select * from srcpart;
select * from srcpart;
set hive.mapred.mode=nonstrict;
set hive.limit.query.max.table.partition=1;

-- SORT_QUERY_RESULTS

explain select ds from srcpart where hr=11 and ds='2008-04-08';
select ds from srcpart where hr=11 and ds='2008-04-08';

explain select distinct hr from srcpart;
select distinct hr from srcpart;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.stats.autogather=true;
set hive.compute.query.using.stats=true;

create table part (c int) partitioned by (d string);
insert into table part partition (d)
select hr,ds from srcpart;

set hive.limit.query.max.table.partition=1;

explain select count(*) from part;
select count(*) from part;

set hive.compute.query.using.stats=false;

explain select count(*) from part;
select count(*) from part;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.limit.pushdown.memory.usage=0.3f;
set hive.optimize.reducededuplication.min.reducer=1;

-- SORT_QUERY_RESULTS

-- HIVE-3562 Some limit can be pushed down to map stage

explain
select key,value from src order by key limit 20;
select key,value from src order by key limit 20;

explain
select key,value from src order by key desc limit 20;
select key,value from src order by key desc limit 20;

explain
select value, sum(key + 1) as sum from src group by value order by value limit 20;
select value, sum(key + 1) as sum from src group by value order by value limit 20;

-- deduped RS
explain
select value,avg(key + 1) from src group by value order by value limit 20;
select value,avg(key + 1) from src group by value order by value limit 20;

-- distincts
explain
select distinct(cdouble) as dis from alltypesorc order by dis limit 20;
select distinct(cdouble) as dis from alltypesorc order by dis limit 20;

explain
select ctinyint, count(distinct(cdouble)) from alltypesorc group by ctinyint order by ctinyint limit 20;
select ctinyint, count(distinct(cdouble)) from alltypesorc group by ctinyint order by ctinyint limit 20;

explain
select ctinyint, count(cdouble) from (select ctinyint, cdouble from alltypesorc group by ctinyint, cdouble) t1 group by ctinyint order by ctinyint limit 20;
select ctinyint, count(cdouble) from (select ctinyint, cdouble from alltypesorc group by ctinyint, cdouble) t1 group by ctinyint order by ctinyint limit 20;

-- multi distinct
explain
select ctinyint, count(distinct(cstring1)), count(distinct(cstring2)) from alltypesorc group by ctinyint order by ctinyint limit 20;
select ctinyint, count(distinct(cstring1)), count(distinct(cstring2)) from alltypesorc group by ctinyint order by ctinyint limit 20;

-- limit zero
explain
select key,value from src order by key limit 0;
select key,value from src order by key limit 0;

-- 2MR (applied to last RS)
explain
select value, sum(key) as sum from src group by value order by sum limit 20;
select value, sum(key) as sum from src group by value order by sum limit 20;

-- subqueries
explain
select * from
(select key, count(1) from src group by key order by key limit 2) subq
join
(select key, count(1) from src group by key limit 3) subq2
on subq.key=subq2.key limit 4;

set hive.map.aggr=false;
-- map aggregation disabled
explain
select value, sum(key) as sum from src group by value order by value limit 20;
select value, sum(key) as sum from src group by value order by value limit 20;

set hive.limit.pushdown.memory.usage=0.00002f;

-- flush for order-by
explain
select key,value,value,value,value,value,value,value,value from src order by key limit 100;
select key,value,value,value,value,value,value,value,value from src order by key limit 100;

-- flush for group-by
explain
select sum(key) as sum from src group by concat(key,value,value,value,value,value,value,value,value,value) order by sum limit 100;
select sum(key) as sum from src group by concat(key,value,value,value,value,value,value,value,value,value) order by sum limit 100;
set hive.mapred.mode=nonstrict;
set hive.limit.pushdown.memory.usage=0.3f;

-- negative, RS + join
explain select * from src a join src b on a.key=b.key limit 20;

-- negative, RS + filter
explain select value, sum(key) as sum from src group by value having sum > 100 limit 20;

-- negative, RS + lateral view
explain select key, L.* from (select * from src order by key) a lateral view explode(array(value, value)) L as v limit 10;

-- negative, RS + forward + multi-groupby
CREATE TABLE dest_2(key STRING, c1 INT);
CREATE TABLE dest_3(key STRING, c1 INT);

EXPLAIN FROM src
INSERT OVERWRITE TABLE dest_2 SELECT value, sum(key) GROUP BY value
INSERT OVERWRITE TABLE dest_3 SELECT value, sum(key) GROUP BY value limit 20;
set hive.mapred.mode=nonstrict;


CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE;


EXPLAIN
INSERT OVERWRITE TABLE dest_l1
SELECT j.*
FROM (SELECT t1.key, p1.value
      FROM src1 t1
      LEFT OUTER JOIN src p1
      ON (t1.key = p1.key)
      UNION ALL
      SELECT t2.key, p2.value
      FROM src1 t2
      LEFT OUTER JOIN src p2
      ON (t2.key = p2.key)) j;

INSERT OVERWRITE TABLE dest_l1
SELECT j.*
FROM (SELECT t1.key, p1.value
      FROM src1 t1
      LEFT OUTER JOIN src p1
      ON (t1.key = p1.key)
      UNION ALL
      SELECT t2.key, p2.value
      FROM src1 t2
      LEFT OUTER JOIN src p2
      ON (t2.key = p2.key)) j;

set hive.mapred.mode=nonstrict;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.LineageLogger;

drop table if exists src2;
create table src2 as select key key2, value value2 from src1;

select * from src1 where key is not null and value is not null limit 3;
select * from src1 where key > 10 and value > 'val' order by key limit 5;

drop table if exists dest1;
create table dest1 as select * from src1;
insert into table dest1 select * from src2;

select key k, dest1.value from dest1;
select key from src1 union select key2 from src2 order by key;
select key k from src1 union select key2 from src2 order by k;

select key, count(1) a from dest1 group by key;
select key k, count(*) from dest1 group by key;
select key k, count(value) from dest1 group by key;
select value, max(length(key)) from dest1 group by value;
select value, max(length(key)) from dest1 group by value order by value limit 5;

select key, length(value) from dest1;
select length(value) + 3 from dest1;
select 5 from dest1;
select 3 * 5 from dest1;

drop table if exists dest2;
create table dest2 as select * from src1 JOIN src2 ON src1.key = src2.key2;
insert overwrite table dest2 select * from src1 JOIN src2 ON src1.key = src2.key2;
insert into table dest2 select * from src1 JOIN src2 ON src1.key = src2.key2;
insert into table dest2
  select * from src1 JOIN src2 ON length(src1.value) = length(src2.value2) + 1;

select * from src1 where length(key) > 2;
select * from src1 where length(key) > 2 and value > 'a';

drop table if exists dest3;
create table dest3 as
  select * from src1 JOIN src2 ON src1.key = src2.key2 WHERE length(key) > 1;
insert overwrite table dest2
  select * from src1 JOIN src2 ON src1.key = src2.key2 WHERE length(key) > 3;

drop table if exists dest_l1;
CREATE TABLE dest_l1(key INT, value STRING) STORED AS TEXTFILE;

INSERT OVERWRITE TABLE dest_l1
SELECT j.*
FROM (SELECT t1.key, p1.value
      FROM src1 t1
      LEFT OUTER JOIN src p1
      ON (t1.key = p1.key)
      UNION ALL
      SELECT t2.key, p2.value
      FROM src1 t2
      LEFT OUTER JOIN src p2
      ON (t2.key = p2.key)) j;

drop table if exists emp;
drop table if exists dept;
drop table if exists project;
drop table if exists tgt;
create table emp(emp_id int, name string, mgr_id int, dept_id int);
create table dept(dept_id int, dept_name string);
create table project(project_id int, project_name string);
create table tgt(dept_name string, name string,
  emp_id int, mgr_id int, proj_id int, proj_name string);

INSERT INTO TABLE tgt
SELECT emd.dept_name, emd.name, emd.emp_id, emd.mgr_id, p.project_id, p.project_name
FROM (
  SELECT d.dept_name, em.name, em.emp_id, em.mgr_id, em.dept_id
  FROM (
    SELECT e.name, e.dept_id, e.emp_id emp_id, m.emp_id mgr_id
    FROM emp e JOIN emp m ON e.emp_id = m.emp_id
    ) em
  JOIN dept d ON d.dept_id = em.dept_id
  ) emd JOIN project p ON emd.dept_id = p.project_id;

drop table if exists dest_l2;
create table dest_l2 (id int, c1 tinyint, c2 int, c3 bigint) stored as textfile;
insert into dest_l2 values(0, 1, 100, 10000);

select * from (
  select c1 + c2 x from dest_l2
  union all
  select sum(c3) y from (select c3 from dest_l2) v1) v2 order by x;

drop table if exists dest_l3;
create table dest_l3 (id int, c1 string, c2 string, c3 int) stored as textfile;
insert into dest_l3 values(0, "s1", "s2", 15);

select sum(a.c1) over (partition by a.c1 order by a.id)
from dest_l2 a
where a.c2 != 10
group by a.c1, a.c2, a.id
having count(a.c2) > 0;

select sum(a.c1), count(b.c1), b.c2, b.c3
from dest_l2 a join dest_l3 b on (a.id = b.id)
where a.c2 != 10 and b.c3 > 0
group by a.c1, a.c2, a.id, b.c1, b.c2, b.c3
having count(a.c2) > 0
order by b.c3 limit 5;

drop table if exists t;
create table t as
select distinct a.c2, a.c3 from dest_l2 a
inner join dest_l3 b on (a.id = b.id)
where a.id > 0 and b.c3 = 15;

SELECT substr(src1.key,1,1), count(DISTINCT substr(src1.value,5)),
concat(substr(src1.key,1,1),sum(substr(src1.value,5)))
from src1
GROUP BY substr(src1.key,1,1);

drop table if exists relations;
create table relations (identity char(32), type string,
  ep1_src_type string, ep1_type string, ep2_src_type string, ep2_type string,
  ep1_ids array<string>, ep2_ids array<string>);

drop table if exists rels_exploded;
create table rels_exploded (identity char(32), type string,
  ep1_src_type string, ep1_type string, ep2_src_type string, ep2_type string,
  ep1_id char(32), ep2_id char(32));

select identity, ep1_id from relations
  lateral view explode(ep1_ids) nav_rel as ep1_id;

insert into rels_exploded select identity, type,
  ep1_src_type, ep1_type, ep2_src_type, ep2_type, ep1_id, ep2_id
from relations lateral view explode(ep1_ids) rel1 as ep1_id
  lateral view explode (ep2_ids) rel2 as ep2_id;

set hive.mapred.mode=nonstrict;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.LineageLogger;
set hive.metastore.disallow.incompatible.col.type.changes=false;
drop table if exists d1;
create table d1(a int);

from (select a.ctinyint x, b.cstring1 y
from alltypesorc a join alltypesorc b on a.cint = b.cbigint) t
insert into table d1 select x + length(y);

drop table if exists d2;
create table d2(b varchar(128));

from (select a.ctinyint x, b.cstring1 y
from alltypesorc a join alltypesorc b on a.cint = b.cbigint) t
insert into table d1 select x where y is null
insert into table d2 select y where x > 0;

drop table if exists t;
create table t as
select * from
  (select * from
     (select key from src1 limit 1) v1) v2;

drop table if exists dest_l1;
create table dest_l1(a int, b varchar(128))
  partitioned by (ds string) clustered by (a) into 2 buckets;

insert into table dest_l1 partition (ds='today')
select cint, cast(cstring1 as varchar(128)) as cs
from alltypesorc
where cint is not null and cint < 0 order by cint, cs limit 5;

insert into table dest_l1 partition (ds='tomorrow')
select min(cint), cast(min(cstring1) as varchar(128)) as cs
from alltypesorc
where cint is not null and cboolean1 = true
group by csmallint
having min(cbigint) > 10;

select cint, rank() over(order by cint) from alltypesorc
where cint > 10 and cint < 10000 limit 10;

select a.ctinyint, a.cint, count(a.cdouble)
  over(partition by a.ctinyint order by a.cint desc
    rows between 1 preceding and 1 following)
from alltypesorc a inner join alltypesorc b on a.cint = b.cbigint
order by a.ctinyint, a.cint;

with v2 as
  (select cdouble, count(cint) over() a,
    sum(cint + cbigint) over(partition by cboolean1) b
    from (select * from alltypesorc) v1)
select cdouble, a, b, a + b, cdouble + a from v2
where cdouble is not null
order by cdouble, a, b limit 5;

select a.cbigint, a.ctinyint, b.cint, b.ctinyint
from
  (select ctinyint, cbigint from alltypesorc
   union all
   select ctinyint, cbigint from alltypesorc) a
  inner join
  alltypesorc b
  on (a.ctinyint = b.ctinyint)
where b.ctinyint < 100 and a.cbigint is not null and b.cint is not null
order by a.cbigint, a.ctinyint, b.cint, b.ctinyint limit 5;

select x.ctinyint, x.cint, c.cbigint-100, c.cstring1
from alltypesorc c
join (
   select a.ctinyint ctinyint, b.cint cint
   from (select * from alltypesorc a where cboolean1=false) a
   join alltypesorc b on (a.cint = b.cbigint - 224870380)
 ) x on (x.cint = c.cint)
where x.ctinyint > 10
and x.cint < 4.5
and x.ctinyint + length(c.cstring2) < 1000;

select c1, x2, x3
from (
  select c1, min(c2) x2, sum(c3) x3
  from (
    select c1, c2, c3
    from (
      select cint c1, ctinyint c2, min(cbigint) c3
      from alltypesorc
      where cint is not null
      group by cint, ctinyint
      order by cint, ctinyint
      limit 5
    ) x
  ) x2
  group by c1
) y
where x2 > 0
order by x2, c1 desc;

select key, value from src1
where key in (select key+18 from src1) order by key;

select * from src1 a
where exists
  (select cint from alltypesorc b
   where a.key = b.ctinyint + 300)
and key > 300;

select key, value from src1
where key not in (select key+18 from src1) order by key;

select * from src1 a
where not exists
  (select cint from alltypesorc b
   where a.key = b.ctinyint + 300)
and key > 300;

with t as (select key x, value y from src1 where key > '2')
select x, y from t where y > 'v' order by x, y limit 5;

from (select key x, value y from src1 where key > '2') t
select x, y where y > 'v' order by x, y limit 5;

drop view if exists dest_v1;
create view dest_v1 as
  select ctinyint, cint from alltypesorc where ctinyint is not null;

select * from dest_v1 order by ctinyint, cint limit 2;

alter view dest_v1 as select ctinyint from alltypesorc;

select t.ctinyint from (select * from dest_v1 where ctinyint is not null) t
where ctinyint > 10 order by ctinyint limit 2;

drop view if exists dest_v2;
create view dest_v2 (a, b) as select c1, x2
from (
  select c1, min(c2) x2
  from (
    select c1, c2, c3
    from (
      select cint c1, ctinyint c2, min(cfloat) c3
      from alltypesorc
      group by cint, ctinyint
      order by cint, ctinyint
      limit 1
    ) x
  ) x2
  group by c1
) y
order by x2,c1 desc;

drop view if exists dest_v3;
create view dest_v3 (a1, a2, a3, a4, a5, a6, a7) as
  select x.csmallint, x.cbigint bint1, x.ctinyint, c.cbigint bint2, x.cint, x.cfloat, c.cstring1
  from alltypesorc c
  join (
     select a.csmallint csmallint, a.ctinyint ctinyint, a.cstring2 cstring2,
           a.cint cint, a.cstring1 ctring1, b.cfloat cfloat, b.cbigint cbigint
     from ( select * from alltypesorc a where cboolean1=true ) a
     join alltypesorc b on (a.csmallint = b.cint)
   ) x on (x.ctinyint = c.cbigint)
  where x.csmallint=11
  and x.cint > 899
  and x.cfloat > 4.5
  and c.cstring1 < '7'
  and x.cint + x.cfloat + length(c.cstring1) < 1000;

alter view dest_v3 as
  select * from (
    select sum(a.ctinyint) over (partition by a.csmallint order by a.csmallint) a,
      count(b.cstring1) x, b.cboolean1
    from alltypesorc a join alltypesorc b on (a.cint = b.cint)
    where a.cboolean2 = true and b.cfloat > 0
    group by a.ctinyint, a.csmallint, b.cboolean1
    having count(a.cint) > 10
    order by a, x, b.cboolean1 limit 10) t;

select * from dest_v3 limit 2;

drop table if exists src_dp;
create table src_dp (first string, word string, year int, month int, day int);
drop table if exists dest_dp1;
create table dest_dp1 (first string, word string) partitioned by (year int);
drop table if exists dest_dp2;
create table dest_dp2 (first string, word string) partitioned by (y int, m int);
drop table if exists dest_dp3;
create table dest_dp3 (first string, word string) partitioned by (y int, m int, d int);

set hive.exec.dynamic.partition.mode=nonstrict;

insert into dest_dp1 partition (year) select first, word, year from src_dp;
insert into dest_dp2 partition (y, m) select first, word, year, month from src_dp;
insert into dest_dp2 partition (y=0, m) select first, word, month from src_dp where year=0;
insert into dest_dp3 partition (y=0, m, d) select first, word, month m, day d from src_dp where year=0;

drop table if exists src_dp1;
create table src_dp1 (f string, w string, m int);

from src_dp, src_dp1
insert into dest_dp1 partition (year) select first, word, year
insert into dest_dp2 partition (y, m) select first, word, year, month
insert into dest_dp3 partition (y=2, m, d) select first, word, month m, day d where year=2
insert into dest_dp2 partition (y=1, m) select f, w, m
insert into dest_dp1 partition (year=0) select f, w;

reset hive.metastore.disallow.incompatible.col.type.changes;
CREATE TABLE mytable (col1 STRING, col2 INT)
ROW FORMAT DELIMITED
LINES TERMINATED BY ',';
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- list bucketing DML : dynamic partition and 2 stage query plan.

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

-- create a skewed table
create table list_bucketing_dynamic_part (key String, value String)
partitioned by (ds String, hr String)
skewed by (key) on ("484")
stored as DIRECTORIES
;

-- list bucketing DML
explain extended
insert overwrite table list_bucketing_dynamic_part partition (ds='2008-04-08', hr) select key, value, hr from srcpart where ds='2008-04-08';
insert overwrite table list_bucketing_dynamic_part partition (ds='2008-04-08', hr) select key, value, hr from srcpart where ds='2008-04-08';

-- check DML result
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='11');
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='12');

select count(1) from srcpart where ds='2008-04-08';
select count(1) from list_bucketing_dynamic_part where ds='2008-04-08';

select key, value from srcpart where ds='2008-04-08' and hr='11' and key = "484";
set hive.optimize.listbucketing=true;
explain extended
select key, value from list_bucketing_dynamic_part where ds='2008-04-08' and hr='11' and key = "484";
select key, value from list_bucketing_dynamic_part where ds='2008-04-08' and hr='11' and key = "484";

-- clean up resources
drop table list_bucketing_dynamic_part;

set mapred.input.dir.recursive=true;

-- run this test case in minimr to ensure it works in cluster
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- list bucketing DML: static partition. multiple skewed columns.
-- ds=2008-04-08/hr=11/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME:
--  5263 000000_0
--  5263 000001_0
-- ds=2008-04-08/hr=11/key=103/value=val_103:
-- 99 000000_0
-- 99 000001_0
-- ds=2008-04-08/hr=11/key=484/value=val_484:
-- 87 000000_0
-- 87 000001_0

-- create a skewed table
create table list_bucketing_static_part (key String, value String)
    partitioned by (ds String, hr String)
    skewed by (key) on ('484','51','103')
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML without merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from src;

insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08', hr = '11')
select key, value from src;

-- check DML result
show partitions list_bucketing_static_part;
desc formatted list_bucketing_static_part partition (ds='2008-04-08', hr='11');
set hive.mapred.mode=nonstrict;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Ensure it works if skewed column is not the first column in the table columns

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- list bucketing DML: static partition. multiple skewed columns.

-- create a skewed table
create table list_bucketing_static_part (key String, value String)
    partitioned by (ds String, hr String)
    skewed by (value) on ('val_466','val_287','val_82')
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML without merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from src;

insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08', hr = '11')
select key, value from src;

-- check DML result
show partitions list_bucketing_static_part;
desc formatted list_bucketing_static_part partition (ds='2008-04-08', hr='11');

set hive.optimize.listbucketing=true;
explain extended
select key, value from list_bucketing_static_part where ds='2008-04-08' and hr='11' and value = "val_466";
select key, value from list_bucketing_static_part where ds='2008-04-08' and hr='11' and value = "val_466";

drop table list_bucketing_static_part;
set hive.mapred.mode=nonstrict;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Ensure it works if skewed column is not the first column in the table columns

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- test where the skewed values are more than 1 say columns no. 2 and 4 in a table with 5 columns
create table list_bucketing_mul_col (col1 String, col2 String, col3 String, col4 String, col5 string)
    partitioned by (ds String, hr String)
    skewed by (col2, col4) on (('466','val_466'),('287','val_287'),('82','val_82'))
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML
explain extended
insert overwrite table list_bucketing_mul_col partition (ds = '2008-04-08',  hr = '11')
select 1, key, 1, value, 1 from src;

insert overwrite table list_bucketing_mul_col partition (ds = '2008-04-08', hr = '11')
select 1, key, 1, value, 1 from src;

-- check DML result
show partitions list_bucketing_mul_col;
desc formatted list_bucketing_mul_col partition (ds='2008-04-08', hr='11');

set hive.optimize.listbucketing=true;
explain extended
select * from list_bucketing_mul_col
where ds='2008-04-08' and hr='11' and col2 = "466" and col4 = "val_466";
select * from list_bucketing_mul_col
where ds='2008-04-08' and hr='11' and col2 = "466" and col4 = "val_466";

explain extended
select * from list_bucketing_mul_col
where ds='2008-04-08' and hr='11' and col2 = "382" and col4 = "val_382";
select * from list_bucketing_mul_col
where ds='2008-04-08' and hr='11' and col2 = "382" and col4 = "val_382";

drop table list_bucketing_mul_col;
set hive.mapred.mode=nonstrict;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Ensure skewed value map has escaped directory name

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- test where the skewed values are more than 1 say columns no. 2 and 4 in a table with 5 columns
create table list_bucketing_mul_col (col1 String, col2 String, col3 String, col4 String, col5 string)
    partitioned by (ds String, hr String)
    skewed by (col2, col4) on (('466','val_466'),('287','val_287'),('82','val_82'))
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML
explain extended
insert overwrite table list_bucketing_mul_col partition (ds = '2008-04-08',  hr = '2013-01-23+18:00:99')
select 1, key, 1, value, 1 from src;

insert overwrite table list_bucketing_mul_col partition (ds = '2008-04-08', hr = '2013-01-23+18:00:99')
select 1, key, 1, value, 1 from src;

-- check DML result
show partitions list_bucketing_mul_col;
desc formatted list_bucketing_mul_col partition (ds='2008-04-08', hr='2013-01-23+18:00:99');

set hive.optimize.listbucketing=true;
explain extended
select * from list_bucketing_mul_col
where ds='2008-04-08' and hr='2013-01-23+18:00:99' and col2 = "466" and col4 = "val_466";
select * from list_bucketing_mul_col
where ds='2008-04-08' and hr='2013-01-23+18:00:99' and col2 = "466" and col4 = "val_466";

drop table list_bucketing_mul_col;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- list bucketing DML : unpartitioned table and 2 stage query plan.

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

-- create a skewed table
create table list_bucketing (key String, value String)
skewed by (key) on ("484")
stored as DIRECTORIES
;

-- list bucketing DML
explain extended
insert overwrite table list_bucketing select * from src;
insert overwrite table list_bucketing select * from src;

-- check DML result
desc formatted list_bucketing;

select count(1) from src;
select count(1) from list_bucketing;

select key, value from src where key = "484";
set hive.optimize.listbucketing=true;
explain extended
select key, value from list_bucketing where key = "484";
select key, value from list_bucketing where key = "484";

-- clean up resources
drop table list_bucketing;

set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.smallfiles.avgsize=200;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.stats.reliable=true;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- list bucketing DML: static partition. multiple skewed columns.
-- ds=2008-04-08/hr=11/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME:
--  5263 000000_0
--  5263 000001_0
-- ds=2008-04-08/hr=11/key=103/value=val_103:
-- 99 000000_0
-- 99 000001_0
-- ds=2008-04-08/hr=11/key=484/value=val_484:
-- 87 000000_0
-- 87 000001_0

-- create a skewed table
create table list_bucketing_static_part (key String, value String)
    partitioned by (ds String, hr String)
    skewed by (key, value) on (('484','val_484'),('51','val_14'),('103','val_103'))
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML without merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08', hr = '11')
select key, value from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_static_part;
desc formatted list_bucketing_static_part partition (ds='2008-04-08', hr='11');

select count(1) from srcpart where ds = '2008-04-08';
select count(*) from list_bucketing_static_part;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.optimize.listbucketing=true;
explain extended
select * from list_bucketing_static_part where ds = '2008-04-08' and  hr = '11' and key = '484' and value = 'val_484';
select * from list_bucketing_static_part where ds = '2008-04-08' and  hr = '11' and key = '484' and value = 'val_484';
select * from srcpart where ds = '2008-04-08' and key = '484' and value = 'val_484';

-- 51 and val_51 in the table so skewed data for 51 and val_14 should be none
-- but query should succeed for 51 or 51 and val_14
select * from srcpart where ds = '2008-04-08' and key = '51';
select * from list_bucketing_static_part where key = '51';
select * from srcpart where ds = '2008-04-08' and key = '51' and value = 'val_14';
select * from list_bucketing_static_part where key = '51' and value = 'val_14';

-- queries with < <= > >= should work for skewed test although we don't benefit from pruning
select count(1) from srcpart where ds = '2008-04-08' and key < '51';
select count(1) from list_bucketing_static_part where key < '51';
select count(1) from srcpart where ds = '2008-04-08' and key <= '51';
select count(1) from list_bucketing_static_part where key <= '51';
select count(1) from srcpart where ds = '2008-04-08' and key > '51';
select count(1) from list_bucketing_static_part where key > '51';
select count(1) from srcpart where ds = '2008-04-08' and key >= '51';
select count(1) from list_bucketing_static_part where key >= '51';

-- clean up
drop table list_bucketing_static_part;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- list bucketing DML : static partition and 2 stage query plan.

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

-- create a skewed table
create table list_bucketing_static_part (key String, value String) partitioned by (ds String, hr String) skewed by (key) on ("484") stored as DIRECTORIES;

-- list bucketing DML
explain extended
insert overwrite table list_bucketing_static_part partition (ds='2008-04-08', hr='11') select key, value from srcpart where ds='2008-04-08';
insert overwrite table list_bucketing_static_part partition (ds='2008-04-08', hr='11') select key, value from srcpart where ds='2008-04-08';

-- check DML result
desc formatted list_bucketing_static_part partition (ds='2008-04-08', hr='11');

select count(1) from srcpart where ds='2008-04-08';
select count(1) from list_bucketing_static_part where ds='2008-04-08';

select key, value from srcpart where ds='2008-04-08' and hr='11' and key = "484";
set hive.optimize.listbucketing=true;
explain extended
select key, value from list_bucketing_static_part where ds='2008-04-08' and hr='11' and key = "484";
select key, value from list_bucketing_static_part where ds='2008-04-08' and hr='11' and key = "484";
-- clean up resources
drop table list_bucketing_static_part;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.smallfiles.avgsize=200;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- list bucketing DML: static partition. multiple skewed columns. merge.
-- ds=2008-04-08/hr=11/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME:
--  5263 000000_0
--  5263 000001_0
-- ds=2008-04-08/hr=11/key=103/value=val_103:
-- 99 000000_0
-- 99 000001_0
-- after merge
-- 142 000000_0
-- ds=2008-04-08/hr=11/key=484/value=val_484:
-- 87 000000_0
-- 87 000001_0
-- after merge
-- 118 000001_0

-- create a skewed table
create table list_bucketing_static_part (key String, value String)
    partitioned by (ds String, hr String)
    skewed by (key, value) on (('484','val_484'),('51','val_14'),('103','val_103'))
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML without merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08', hr = '11')
select key, value from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_static_part;
desc formatted list_bucketing_static_part partition (ds='2008-04-08', hr='11');

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
-- list bucketing DML with merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_static_part;
desc formatted list_bucketing_static_part partition (ds='2008-04-08', hr='11');

select count(1) from srcpart where ds = '2008-04-08';
select count(*) from list_bucketing_static_part;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.optimize.listbucketing=true;
explain extended
select * from list_bucketing_static_part where ds = '2008-04-08' and  hr = '11' and key = '484' and value = 'val_484';
select * from list_bucketing_static_part where ds = '2008-04-08' and  hr = '11' and key = '484' and value = 'val_484';
select * from srcpart where ds = '2008-04-08' and key = '484' and value = 'val_484';

-- clean up
drop table list_bucketing_static_part;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- list bucketing DML: multiple skewed columns. 2 stages

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- create a skewed table
create table list_bucketing_dynamic_part (key String, value String)
partitioned by (ds String, hr String)
skewed by (key, value) on (('484','val_484'),('51','val_14'),('103','val_103'))
stored as DIRECTORIES;

-- list bucketing DML
explain extended
insert overwrite table list_bucketing_dynamic_part partition (ds='2008-04-08', hr) select key, value, hr from srcpart where ds='2008-04-08';
insert overwrite table list_bucketing_dynamic_part partition (ds='2008-04-08', hr) select key, value, hr from srcpart where ds='2008-04-08';

-- check DML result
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='11');
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='12');

select count(1) from srcpart where ds='2008-04-08';
select count(1) from list_bucketing_dynamic_part where ds='2008-04-08';

select key, value from srcpart where ds='2008-04-08' and key = "103" and value ="val_103";
set hive.optimize.listbucketing=true;
explain extended
select key, value, ds, hr from list_bucketing_dynamic_part where ds='2008-04-08' and key = "103" and value ="val_103";
select key, value, ds, hr from list_bucketing_dynamic_part where ds='2008-04-08' and key = "103" and value ="val_103";

-- clean up resources
drop table list_bucketing_dynamic_part;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.smallfiles.avgsize=200;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- list bucketing DML: dynamic partition. multiple skewed columns. merge.
-- The following explains merge example used in this test case
-- DML will generated 2 partitions
-- ds=2008-04-08/hr=a1
-- ds=2008-04-08/hr=b1
-- without merge, each partition has more files
-- ds=2008-04-08/hr=a1 has 2 files
-- ds=2008-04-08/hr=b1 has 6 files
-- with merge each partition has more files
-- ds=2008-04-08/hr=a1 has 1 files
-- ds=2008-04-08/hr=b1 has 4 files
-- The following shows file size and name in each directory
-- hr=a1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME:
-- without merge
-- 155 000000_0
-- 155 000001_0
-- with merge
-- 254 000000_0
-- hr=b1/key=103/value=val_103:
-- without merge
-- 99 000000_0
-- 99 000001_0
-- with merge
-- 142 000001_0
-- hr=b1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME:
-- without merge
-- 5181 000000_0
-- 5181 000001_0
-- with merge
-- 5181 000000_0
-- 5181 000001_0
-- hr=b1/key=484/value=val_484
-- without merge
-- 87 000000_0
-- 87 000001_0
-- with merge
-- 118 000002_0

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- create a skewed table
create table list_bucketing_dynamic_part (key String, value String)
    partitioned by (ds String, hr String)
    skewed by (key, value) on (('484','val_484'),('51','val_14'),('103','val_103'))
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML without merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_dynamic_part;
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='a1');
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='b1');

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
-- list bucketing DML with merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_dynamic_part;
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='a1');
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='b1');

select count(1) from srcpart where ds = '2008-04-08';
select count(*) from list_bucketing_dynamic_part;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.optimize.listbucketing=true;
explain extended
select * from list_bucketing_dynamic_part where key = '484' and value = 'val_484';
select * from list_bucketing_dynamic_part where key = '484' and value = 'val_484';
select * from srcpart where ds = '2008-04-08' and key = '484' and value = 'val_484';

-- clean up
drop table list_bucketing_dynamic_part;

set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.smallfiles.avgsize=200;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.rcfile.block.level=true;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS

-- list bucketing DML : dynamic partition (one level) , merge , one skewed column
-- DML without merge files mixed with small and big files:
-- ds=2008-04-08/hr=a1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/
-- 155 000000_0
-- ds=2008-04-08/hr=b1/key=484
-- 87 000000_0
-- 87 000001_0
-- ds=2008-04-08/hr=b1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/
-- 5201 000000_0
-- 5201 000001_0
-- DML with merge will merge small files

-- skewed table
CREATE TABLE list_bucketing_dynamic_part (key String, value STRING)
    PARTITIONED BY (ds string, hr string)
    skewed by (key) on ('484')
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML without merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_dynamic_part;
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='a1');
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='b1');

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
-- list bucketing DML with merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_dynamic_part;
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='a1');
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='b1');

select count(1) from srcpart where ds = '2008-04-08';
select count(*) from list_bucketing_dynamic_part;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
explain extended
select * from list_bucketing_dynamic_part where key = '484' and value = 'val_484';
select * from list_bucketing_dynamic_part where key = '484' and value = 'val_484';
select * from srcpart where ds = '2008-04-08' and key = '484' and value = 'val_484';

-- clean up
drop table list_bucketing_dynamic_part;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.smallfiles.avgsize=200;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- list bucketing alter table ... concatenate:
-- Use list bucketing DML to generate mutilple files in partitions by turning off merge
-- dynamic partition. multiple skewed columns. merge.
-- The following explains merge example used in this test case
-- DML will generated 2 partitions
-- ds=2008-04-08/hr=a1
-- ds=2008-04-08/hr=b1
-- without merge, each partition has more files
-- ds=2008-04-08/hr=a1 has 2 files
-- ds=2008-04-08/hr=b1 has 6 files
-- with merge each partition has more files
-- ds=2008-04-08/hr=a1 has 1 files
-- ds=2008-04-08/hr=b1 has 4 files
-- The following shows file size and name in each directory
-- hr=a1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME:
-- without merge
-- 155 000000_0
-- 155 000001_0
-- with merge
-- 254 000000_0
-- hr=b1/key=103/value=val_103:
-- without merge
-- 99 000000_0
-- 99 000001_0
-- with merge
-- 142 000001_0
-- hr=b1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME:
-- without merge
-- 5181 000000_0
-- 5181 000001_0
-- with merge
-- 5181 000000_0
-- 5181 000001_0
-- hr=b1/key=484/value=val_484
-- without merge
-- 87 000000_0
-- 87 000001_0
-- with merge
-- 118 000002_0

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- create a skewed table
create table list_bucketing_dynamic_part (key String, value String)
    partitioned by (ds String, hr String)
    skewed by (key, value) on (('484','val_484'),('51','val_14'),('103','val_103'))
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML without merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_dynamic_part;
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='a1');
desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='b1');

-- concatenate the partition and it will merge files
alter table list_bucketing_dynamic_part partition (ds='2008-04-08', hr='b1') concatenate;

desc formatted list_bucketing_dynamic_part partition (ds='2008-04-08', hr='b1');

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select count(1) from srcpart where ds = '2008-04-08';
select count(*) from list_bucketing_dynamic_part;
explain extended
select * from list_bucketing_dynamic_part where key = '484' and value = 'val_484';
select * from list_bucketing_dynamic_part where key = '484' and value = 'val_484';
select * from srcpart where ds = '2008-04-08' and key = '484' and value = 'val_484' order by hr;

-- clean up
drop table list_bucketing_dynamic_part;



set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.smallfiles.avgsize=200;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

-- list bucketing DML: static partition. multiple skewed columns. merge.
-- ds=2008-04-08/hr=11/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME:
--  5263 000000_0
--  5263 000001_0
-- ds=2008-04-08/hr=11/key=103:
-- 99 000000_0
-- 99 000001_0
-- after merge
-- 142 000000_0
-- ds=2008-04-08/hr=11/key=484:
-- 87 000000_0
-- 87 000001_0
-- after merge
-- 118 000001_0

-- create a skewed table
create table list_bucketing_static_part (key String, value String)
    partitioned by (ds String, hr String)
    skewed by (key) on ('484','103')
    stored as DIRECTORIES
    STORED AS RCFILE;

-- list bucketing DML without merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08', hr = '11')
select key, value from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_static_part;
desc formatted list_bucketing_static_part partition (ds='2008-04-08', hr='11');

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
-- list bucketing DML with merge. use bucketize to generate a few small files.
explain extended
insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from srcpart where ds = '2008-04-08';

insert overwrite table list_bucketing_static_part partition (ds = '2008-04-08',  hr = '11')
select key, value from srcpart where ds = '2008-04-08';

-- check DML result
show partitions list_bucketing_static_part;
desc formatted list_bucketing_static_part partition (ds='2008-04-08', hr='11');

select count(1) from srcpart where ds = '2008-04-08';
select count(*) from list_bucketing_static_part;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.optimize.listbucketing=true;
explain extended
select * from list_bucketing_static_part where ds = '2008-04-08' and  hr = '11' and key = '484' and value = 'val_484';
select * from list_bucketing_static_part where ds = '2008-04-08' and  hr = '11' and key = '484' and value = 'val_484';
select * from srcpart where ds = '2008-04-08' and key = '484' and value = 'val_484';

-- clean up
drop table list_bucketing_static_part;
set hive.mapred.mode=nonstrict;
set hive.optimize.listbucketing=true;
set mapred.input.dir.recursive=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

-- List bucketing query logic test case. We simulate the directory structure by DML here.
-- Test condition:
-- 1. where clause has multiple skewed columns
-- 2. where clause doesn't have non-skewed column
-- 3. where clause has one and operator
-- Test focus:
-- 1. basic list bucketing query work
-- Test result:
-- 1. pruner only pick up right directory
-- 2. query result is right

-- create a skewed table
create table fact_daily (key String, value String)
partitioned by (ds String, hr String)
skewed by (key, value) on (('484','val_484'),('238','val_238'))
stored as DIRECTORIES;

insert overwrite table fact_daily partition (ds = '1', hr = '4')
select key, value from src;

describe formatted fact_daily PARTITION (ds = '1', hr='4');

SELECT count(1) FROM fact_daily WHERE ds='1' and hr='4';

-- pruner only pick up skewed-value directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT key FROM fact_daily WHERE ( ds='1' and hr='4') and (key='484' and value= 'val_484');
-- List Bucketing Query
SELECT key FROM fact_daily WHERE ( ds='1' and hr='4') and (key='484' and value= 'val_484');

-- pruner only pick up skewed-value directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT key,value FROM fact_daily WHERE ( ds='1' and hr='4') and (key='238' and value= 'val_238');
-- List Bucketing Query
SELECT key,value FROM fact_daily WHERE ( ds='1' and hr='4') and (key='238' and value= 'val_238');

-- pruner only pick up default directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT key FROM fact_daily WHERE ( ds='1' and hr='4') and (value = "3");
-- List Bucketing Query
SELECT key FROM fact_daily WHERE ( ds='1' and hr='4') and (value = "3");

-- pruner only pick up default directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT key,value FROM fact_daily WHERE ( ds='1' and hr='4') and key = '495';
-- List Bucketing Query
SELECT key,value FROM fact_daily WHERE ( ds='1' and hr='4') and key = '369';
set hive.mapred.mode=nonstrict;
set hive.optimize.listbucketing=true;
set mapred.input.dir.recursive=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS

-- List bucketing query logic test case. We simulate the directory structure by DML here.
-- Test condition:
-- 1. where clause has multiple skewed columns and non-skewed columns
-- 3. where clause has a few operators
-- Test focus:
-- 1. basic list bucketing query work
-- Test result:
-- 1. pruner only pick up right directory
-- 2. query result is right

-- create a skewed table
create table fact_daily (key String, value String)
partitioned by (ds String, hr String)
skewed by (key, value) on (('484','val_484'),('238','val_238'))
stored as DIRECTORIES;

insert overwrite table fact_daily partition (ds = '1', hr = '4')
select key, value from src;

describe formatted fact_daily PARTITION (ds = '1', hr='4');

SELECT count(1) FROM fact_daily WHERE ds='1' and hr='4';

-- pruner only pick up default directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT key, value FROM fact_daily WHERE ds='1' and hr='4' and value= 'val_484';
-- List Bucketing Query
SELECT key, value FROM fact_daily WHERE ds='1' and hr='4' and value= 'val_484';

-- pruner only pick up default directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT key FROM fact_daily WHERE ds='1' and hr='4' and key= '406';
-- List Bucketing Query
SELECT key, value FROM fact_daily WHERE ds='1' and hr='4' and key= '406';

-- pruner only pick up skewed-value directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT key, value FROM fact_daily WHERE ds='1' and hr='4' and ( (key='484' and value ='val_484')  or (key='238' and value= 'val_238'));
-- List Bucketing Query
SELECT key, value FROM fact_daily WHERE ds='1' and hr='4' and ( (key='484' and value ='val_484')  or (key='238' and value= 'val_238'));

-- clean up
drop table fact_daily;
set hive.mapred.mode=nonstrict;
set hive.optimize.listbucketing=true;
set mapred.input.dir.recursive=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

-- List bucketing query logic test case. We simulate the directory structure by DML here.
-- Test condition:
-- 1. where clause has multiple skewed columns and non-skewed columns
-- 3. where clause has a few operators
-- Test focus:
-- 1. query works for on partition level.
--    A table can mix up non-skewed partition and skewed partition
--    Even for skewed partition, it can have different skewed information.
-- Test result:
-- 1. pruner only pick up right directory
-- 2. query result is right

-- create a skewed table
create table fact_daily (key String, value String)
partitioned by (ds String, hr String) ;

-- partition no skew
insert overwrite table fact_daily partition (ds = '1', hr = '1')
select key, value from src;
describe formatted fact_daily PARTITION (ds = '1', hr='1');

-- partition. skewed value is 484/238
alter table fact_daily skewed by (key, value) on (('484','val_484'),('238','val_238')) stored as DIRECTORIES;
insert overwrite table fact_daily partition (ds = '1', hr = '2')
select key, value from src;
describe formatted fact_daily PARTITION (ds = '1', hr='2');

-- another partition. skewed value is 327
alter table fact_daily skewed by (key, value) on (('327','val_327')) stored as DIRECTORIES;
insert overwrite table fact_daily partition (ds = '1', hr = '3')
select key, value from src;
describe formatted fact_daily PARTITION (ds = '1', hr='3');

-- query non-skewed partition
explain extended
select * from fact_daily where ds = '1' and  hr='1' and key='145';
select * from fact_daily where ds = '1' and  hr='1' and key='145';
explain extended
select count(*) from fact_daily where ds = '1' and  hr='1';
select count(*) from fact_daily where ds = '1' and  hr='1';

-- query skewed partition
explain extended
SELECT * FROM fact_daily WHERE ds='1' and hr='2' and (key='484' and value='val_484');
SELECT * FROM fact_daily WHERE ds='1' and hr='2' and (key='484' and value='val_484');

-- query another skewed partition
explain extended
SELECT * FROM fact_daily WHERE ds='1' and hr='3' and (key='327' and value='val_327');
SELECT * FROM fact_daily WHERE ds='1' and hr='3' and (key='327' and value='val_327');
set hive.mapred.mode=nonstrict;
set hive.optimize.listbucketing=true;
set mapred.input.dir.recursive=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS

-- List bucketing query logic test case.
-- Test condition:
-- 1. where clause has only one skewed column
-- 2. where clause doesn't have non-skewed column
-- 3. where clause has one and operator
-- Test result:
-- 1. pruner only pick up right directory
-- 2. query result is right

-- create 2 tables: fact_daily and fact_tz
-- fact_daily will be used for list bucketing query
-- fact_tz is a table used to prepare data and test directories
CREATE TABLE fact_daily(x int) PARTITIONED BY (ds STRING);
CREATE TABLE fact_tz(x int) PARTITIONED BY (ds STRING, hr STRING)
LOCATION '${hiveconf:hive.metastore.warehouse.dir}/fact_tz';

-- create /fact_tz/ds=1/hr=1 directory
INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='1')
SELECT key FROM src WHERE key=484;

-- create /fact_tz/ds=1/hr=2 directory
INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='2')
SELECT key+11 FROM src WHERE key=484;

dfs -lsr ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1;
dfs -mv ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/hr=1 ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/x=484;
dfs -mv ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/hr=2 ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME;
dfs -lsr ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1;

-- switch fact_daily to skewed table and point its location to /fact_tz/ds=1
alter table fact_daily skewed by (x) on (484);
ALTER TABLE fact_daily SET TBLPROPERTIES('EXTERNAL'='TRUE');
ALTER TABLE fact_daily ADD PARTITION (ds='1')
LOCATION '${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1';

-- set List Bucketing location map
alter table fact_daily PARTITION (ds = '1') set skewed location (484='${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/x=484','HIVE_DEFAULT_LIST_BUCKETING_KEY'='${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME');
describe formatted fact_daily PARTITION (ds = '1');

SELECT * FROM fact_daily WHERE ds='1';

-- pruner only pick up skewed-value directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT x FROM fact_daily WHERE ds='1' and x=484;
-- List Bucketing Query
SELECT x FROM fact_daily WHERE ds='1' and x=484;

-- pruner only pick up default directory since x equal to non-skewed value
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT x FROM fact_daily WHERE ds='1' and x=495;
-- List Bucketing Query
SELECT x FROM fact_daily WHERE ds='1' and x=495;
explain extended SELECT x FROM fact_daily WHERE ds='1' and x=1;
SELECT x FROM fact_daily WHERE ds='1' and x=1;
set hive.mapred.mode=nonstrict;
set hive.optimize.listbucketing=true;
set mapred.input.dir.recursive=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS

-- List bucketing query logic test case.
-- Test condition:
-- 1. where clause has only one skewed column
-- 2. where clause doesn't have non-skewed column
-- Test focus:
-- 1. list bucketing query logic works fine for subquery
-- Test result:
-- 1. pruner only pick up right directory
-- 2. query result is right

-- create 2 tables: fact_daily and fact_tz
-- fact_daily will be used for list bucketing query
-- fact_tz is a table used to prepare data and test directories
CREATE TABLE fact_daily(x int, y STRING) PARTITIONED BY (ds STRING);
CREATE TABLE fact_tz(x int, y STRING) PARTITIONED BY (ds STRING, hr STRING)
LOCATION '${hiveconf:hive.metastore.warehouse.dir}/fact_tz';

-- create /fact_tz/ds=1/hr=1 directory
INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='1')
SELECT key, value FROM src WHERE key=484;

-- create /fact_tz/ds=1/hr=2 directory
INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='2')
SELECT key+11, value FROM src WHERE key=484;

dfs -lsr ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1;
dfs -mv ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/hr=1 ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/x=484;
dfs -mv ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/hr=2 ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME;
dfs -lsr ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1;

-- switch fact_daily to skewed table and point its location to /fact_tz/ds=1
alter table fact_daily skewed by (x) on (484);
ALTER TABLE fact_daily SET TBLPROPERTIES('EXTERNAL'='TRUE');
ALTER TABLE fact_daily ADD PARTITION (ds='1')
LOCATION '${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1';

-- set List Bucketing location map
alter table fact_daily PARTITION (ds = '1') set skewed location (484='${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/x=484','HIVE_DEFAULT_LIST_BUCKETING_KEY'='${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME');
describe formatted fact_daily PARTITION (ds = '1');

SELECT * FROM fact_daily WHERE ds='1';

-- The first subquery
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended select x from (select x from fact_daily where ds = '1') subq where x = 484;
-- List Bucketing Query
select x from (select * from fact_daily where ds = '1') subq where x = 484;

-- The second subquery
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484;
-- List Bucketing Query
select x1, y1 from(select x as x1, y as y1 from fact_daily where ds ='1') subq where x1 = 484;


-- The third subquery
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended  select y, count(1) from fact_daily where ds ='1' and x = 484 group by y;
-- List Bucketing Query
select y, count(1) from fact_daily where ds ='1' and x = 484 group by y;

-- The fourth subquery
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended  select x, c from (select x, count(1) as c from fact_daily where ds = '1' group by x) subq where x = 484;;
-- List Bucketing Query
select x, c from (select x, count(1) as c from fact_daily where ds = '1' group by x) subq where x = 484;
set hive.mapred.mode=nonstrict;
set hive.optimize.listbucketing=true;
set mapred.input.dir.recursive=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- SORT_QUERY_RESULTS

-- List bucketing query logic test case.
-- Test condition:
-- 1. where clause has single skewed columns and non-skewed columns
-- 3. where clause has a few operators
-- Test focus:
-- 1. basic list bucketing query works for not (equal) case
-- Test result:
-- 1. pruner only pick up right directory
-- 2. query result is right

-- create 2 tables: fact_daily and fact_tz
-- fact_daily will be used for list bucketing query
-- fact_tz is a table used to prepare data and test directories
CREATE TABLE fact_daily(x int, y STRING, z STRING) PARTITIONED BY (ds STRING);
CREATE TABLE fact_tz(x int, y STRING, z STRING) PARTITIONED BY (ds STRING, hr STRING)
LOCATION '${hiveconf:hive.metastore.warehouse.dir}/fact_tz';

-- create /fact_tz/ds=1/hr=1 directory
INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='1')
SELECT key, value, value FROM src WHERE key=484;

-- create /fact_tz/ds=1/hr=2 directory
INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='2')
SELECT key, value, value FROM src WHERE key=278 or key=86;

-- create /fact_tz/ds=1/hr=3 directory
INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='3')
SELECT key, value, value FROM src WHERE key=238;

dfs -lsr ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1;
dfs -mv ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/hr=1 ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/x=484;
dfs -mv ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/hr=2 ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME;
dfs -mv ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/hr=3 ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/x=238;
dfs -lsr ${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1;

-- switch fact_daily to skewed table and point its location to /fact_tz/ds=1
alter table fact_daily skewed by (x) on (484,238);
ALTER TABLE fact_daily SET TBLPROPERTIES('EXTERNAL'='TRUE');
ALTER TABLE fact_daily ADD PARTITION (ds='1')
LOCATION '${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1';

-- set List Bucketing location map
alter table fact_daily PARTITION (ds = '1') set skewed location (484='${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/x=484',
238='${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/x=238',
'HIVE_DEFAULT_LIST_BUCKETING_KEY'='${hiveconf:hive.metastore.warehouse.dir}/fact_tz/ds=1/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME');
describe formatted fact_daily PARTITION (ds = '1');

SELECT * FROM fact_daily WHERE ds='1';

-- pruner  pick up right directory
-- explain plan shows which directory selected: Truncated Path -> Alias
explain extended SELECT x FROM fact_daily WHERE ds='1' and not (x = 86);
-- List Bucketing Query
SELECT x FROM fact_daily WHERE ds='1' and not (x = 86);
set hive.fetch.task.conversion=more;

EXPLAIN SELECT -1BD, 0BD, 1BD, 3.14BD, -3.14BD, 99999999999999999BD, 99999999999999999.9999999999999BD, 1E99BD FROM src LIMIT 1;

SELECT -1BD, 0BD, 1BD, 3.14BD, -3.14BD, 99999999999999999BD, 99999999999999999.9999999999999BD, 1E99BD FROM src LIMIT 1;
set hive.fetch.task.conversion=more;

EXPLAIN SELECT 3.14, -3.14, 3.14e8, 3.14e-8, -3.14e8, -3.14e-8, 3.14e+8, 3.14E8, 3.14E-8 FROM src LIMIT 1;
SELECT 3.14, -3.14, 3.14e8, 3.14e-8, -3.14e8, -3.14e-8, 3.14e+8, 3.14E8, 3.14E-8 FROM src LIMIT 1;

set hive.fetch.task.conversion=more;

EXPLAIN SELECT 100, 100Y, 100S, 100L FROM src LIMIT 1;

SELECT 100, 100Y, 100S, 100L FROM src LIMIT 1;
set hive.fetch.task.conversion=more;

EXPLAIN SELECT 'face''book', 'face' 'book', 'face'
                                            'book',
               "face""book", "face" "book", "face"
                                            "book",
               'face' 'bo' 'ok', 'face'"book",
               "face"'book', 'facebook' FROM src LIMIT 1;

SELECT 'face''book', 'face' 'book', 'face'
                                    'book',
       "face""book", "face" "book", "face"
                                    "book",
       'face' 'bo' 'ok', 'face'"book",
       "face"'book', 'facebook' FROM src LIMIT 1;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.stats.fetch.column.stats=true;
set hive.execution.mode=llap;
set hive.llap.execution.mode=auto;

-- simple query with multiple reduce stages
EXPLAIN SELECT key, count(value) as cnt FROM src GROUP BY key ORDER BY cnt;

create table src_orc stored as orc as select * from src;

EXPLAIN SELECT key, count(value) as cnt FROM src_orc GROUP BY key ORDER BY cnt;

set hive.llap.auto.enforce.stats=false;

EXPLAIN SELECT key, count(value) as cnt FROM src_orc GROUP BY key ORDER BY cnt;

set hive.llap.auto.enforce.stats=true;

analyze table src_orc compute statistics for columns;

EXPLAIN SELECT key, count(value) as cnt FROM src_orc GROUP BY key ORDER BY cnt;

EXPLAIN SELECT * from src_orc join src on (src_orc.key = src.key) order by src.value;

EXPLAIN SELECT * from src_orc s1 join src_orc s2 on (s1.key = s2.key) order by s2.value;

set hive.llap.auto.enforce.tree=false;

EXPLAIN SELECT * from src_orc join src on (src_orc.key = src.key) order by src.value;

set hive.llap.auto.enforce.tree=true;

set hive.llap.auto.max.input.size=10;

EXPLAIN SELECT * from src_orc s1 join src_orc s2 on (s1.key = s2.key) order by s2.value;

set hive.llap.auto.max.input.size=1000000000;
set hive.llap.auto.max.output.size=10;

EXPLAIN SELECT * from src_orc s1 join src_orc s2 on (s1.key = s2.key) order by s2.value;

set hive.llap.auto.max.output.size=1000000000;

set hive.llap.execution.mode=map;

EXPLAIN SELECT * from src_orc s1 join src_orc s2 on (s1.key = s2.key) order by s2.value;

set hive.llap.execution.mode=none;

EXPLAIN SELECT * from src_orc s1 join src_orc s2 on (s1.key = s2.key) order by s2.value;

set hive.llap.execution.mode=all;

EXPLAIN SELECT * from src_orc s1 join src_orc s2 on (s1.key = s2.key) order by s2.value;

CREATE TEMPORARY FUNCTION test_udf_get_java_string AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString';

set hive.llap.execution.mode=auto;

EXPLAIN SELECT sum(cast(key as int) + 1) from src_orc where cast(key as int) > 1;
EXPLAIN SELECT sum(cast(test_udf_get_java_string(cast(key as string)) as int) + 1) from src_orc where cast(key as int) > 1;
EXPLAIN SELECT sum(cast(key as int) + 1) from src_orc where cast(test_udf_get_java_string(cast(key as string)) as int) > 1;

set hive.execution.mode=container;SET hive.vectorized.execution.enabled=true;
set hive.mapred.mode=nonstrict;
SELECT   cfloat,
         cint,
         cdouble,
         cbigint,
         cstring1
FROM     alltypesorc
WHERE    (cbigint > -23)
           AND ((cdouble != 988888)
                OR (cint > -863.257))
ORDER BY cbigint, cfloat;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;

SET hive.llap.io.enabled=false;

SET hive.exec.orc.default.buffer.size=32768;
SET hive.exec.orc.default.row.index.stride=1000;
SET hive.optimize.index.filter=true;
set hive.fetch.task.conversion=none;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

DROP TABLE orc_llap;

CREATE TABLE orc_llap (
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE)
partitioned by (csmallint smallint)
clustered by (cint) into 2 buckets stored as orc;

insert into table orc_llap partition (csmallint = 1)
select cint, cbigint, cfloat, cdouble from alltypesorc order by cdouble asc limit 10;
insert into table orc_llap partition (csmallint = 2)
select cint, cbigint, cfloat, cdouble from alltypesorc order by cdouble asc limit 10;

alter table orc_llap SET TBLPROPERTIES ('transactional'='true');

insert into table orc_llap partition (csmallint = 3)
select cint, cbigint, cfloat, cdouble from alltypesorc order by cdouble desc limit 10;

SET hive.llap.io.enabled=true;

explain
select cint, csmallint, cbigint from orc_llap where cint is not null order
by csmallint, cint;
select cint, csmallint, cbigint from orc_llap where cint is not null order
by csmallint, cint;

insert into table orc_llap partition (csmallint = 1) values (1, 1, 1, 1);

update orc_llap set cbigint = 2 where cint = 1;

explain
select cint, csmallint, cbigint from orc_llap where cint is not null order
by csmallint, cint;
select cint, csmallint, cbigint from orc_llap where cint is not null order
by csmallint, cint;

DROP TABLE orc_llap;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;

SET hive.llap.io.enabled=false;

SET hive.exec.orc.default.buffer.size=32768;
SET hive.exec.orc.default.row.index.stride=1000;
SET hive.optimize.index.filter=true;

set hive.auto.convert.join=true;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=255;

DROP TABLE orc_llap_part;
DROP TABLE orc_llap_dim_part;

CREATE TABLE orc_llap_part(
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cchar1 CHAR(255),
    cvchar1 VARCHAR(255),
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN
) PARTITIONED BY (ctinyint TINYINT) STORED AS ORC;

CREATE TABLE orc_llap_dim_part(
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cchar1 CHAR(255),
    cvchar1 VARCHAR(255),
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN
) PARTITIONED BY (ctinyint TINYINT) STORED AS ORC;

INSERT OVERWRITE TABLE orc_llap_part PARTITION (ctinyint)
SELECT csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring1, cstring1, cboolean1, cboolean2, ctinyint FROM alltypesorc;

INSERT OVERWRITE TABLE orc_llap_dim_part PARTITION (ctinyint)
SELECT null, null, sum(cbigint) as cbigint, null, null, null, null, null, null, null, ctinyint FROM alltypesorc WHERE ctinyint > 10 AND ctinyint < 21 GROUP BY ctinyint;

drop table llap_temp_table;

set hive.cbo.enable=false;
SET hive.llap.io.enabled=true;
SET hive.vectorized.execution.enabled=true;

explain
SELECT oft.ctinyint, oft.cint, oft.cchar1, oft.cvchar1 FROM orc_llap_part oft
  INNER JOIN orc_llap_dim_part od ON oft.ctinyint = od.ctinyint;
create table llap_temp_table as
SELECT oft.ctinyint, oft.cint, oft.cchar1, oft.cvchar1 FROM orc_llap_part oft
  INNER JOIN orc_llap_dim_part od ON oft.ctinyint = od.ctinyint;
select sum(hash(*)) from llap_temp_table;
drop table llap_temp_table;


DROP TABLE orc_llap_part;
DROP TABLE orc_llap_dim_part;
SET hive.vectorized.execution.enabled=true;

SET hive.llap.io.enabled=false;

SET hive.exec.orc.default.row.index.stride=1000;
SET hive.optimize.index.filter=true;
set hive.auto.convert.join=false;

DROP TABLE orc_llap;

set hive.auto.convert.join=true;
SET hive.llap.io.enabled=false;

CREATE TABLE orc_llap(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cstring2 STRING,
    ctimestamp1 TIMESTAMP,
    ctimestamp2 TIMESTAMP,
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN)
    STORED AS ORC tblproperties ("orc.compress"="NONE");

insert into table orc_llap
select ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2
from alltypesorc;


SET hive.llap.io.enabled=true;

drop table llap_temp_table;
explain
select * from orc_llap where cint > 10 and cbigint is not null;
create table llap_temp_table as
select * from orc_llap where cint > 10 and cbigint is not null;
select sum(hash(*)) from llap_temp_table;

explain
select * from orc_llap where cint > 10 and cint < 5000000;
select * from orc_llap where cint > 10 and cint < 5000000;

DROP TABLE orc_llap;
drop table llap_temp_table;



create table hive_test_src ( col1 string ) stored as textfile ;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src ;

create table hive_test_dst ( col1 string ) partitioned by ( pcol1 string , pcol2 string) stored as sequencefile;
insert overwrite table hive_test_dst partition ( pcol1='test_part', pCol2='test_Part') select col1 from hive_test_src ;
select * from hive_test_dst where pcol1='test_part' and pcol2='test_Part';

insert overwrite table hive_test_dst partition ( pCol1='test_part', pcol2='test_Part') select col1 from hive_test_src ;
select * from hive_test_dst where pcol1='test_part' and pcol2='test_part';

select * from hive_test_dst where pcol1='test_part';
select * from hive_test_dst where pcol1='test_part' and pcol2='test_part';
select * from hive_test_dst where pcol1='test_Part';




create table hive_test ( col1 string ) partitioned by ( pcol1 string , pcol2 string) stored as textfile;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test partition (pcol1='part1',pcol2='part1') ;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test partition (pcol2='part2',pcol1='part2') ;
select * from hive_test where pcol1='part1' and pcol2='part1';
select * from hive_test where pcol1='part2' and pcol2='part2';



set hive.cli.errors.ignore=true;

ADD FILE ../../data/scripts/error_script;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20S, 0.23)
-- (this test is flaky so it is currently disabled for all Hadoop versions)

CREATE TABLE loadpart1(a STRING, b STRING) PARTITIONED BY (ds STRING);

INSERT OVERWRITE TABLE loadpart1 PARTITION (ds='2009-01-01')
SELECT TRANSFORM(src.key, src.value) USING 'error_script' AS (tkey, tvalue)
FROM src;

DESCRIBE loadpart1;
SHOW PARTITIONS loadpart1;

LOAD DATA LOCAL INPATH '../../data1/files/kv1.txt' INTO TABLE loadpart1 PARTITION(ds='2009-05-05');
SHOW PARTITIONS loadpart1;


CREATE TABLE mytable(key binary, value int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '9'
STORED AS TEXTFILE;
-- this query loads native binary data, stores in a table and then queries it. Note that string.txt contains binary data. Also uses transform clause and then length udf.

LOAD DATA LOCAL INPATH '../../data/files/string.txt' INTO TABLE mytable;

create table dest1 (key binary, value int);

insert overwrite table dest1 select transform(*) using 'cat' as key binary, value int from mytable;

select key, value, length (key) from dest1;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

show partitions srcpart;




create table if not exists nzhang_part1 like srcpart;
create table if not exists nzhang_part2 like srcpart;
describe extended nzhang_part1;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

explain
from srcpart
insert overwrite table nzhang_part1 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
insert overwrite table nzhang_part2 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08';

from srcpart
insert overwrite table nzhang_part1 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
insert overwrite table nzhang_part2 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08';


show partitions nzhang_part1;
show partitions nzhang_part2;

select * from nzhang_part1 where ds is not null and hr is not null;
select * from nzhang_part2 where ds is not null and hr is not null;



-- SORT_QUERY_RESULTS

show partitions srcpart;



create table if not exists nzhang_part10 like srcpart;
describe extended nzhang_part10;

set hive.merge.mapfiles=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

explain
from srcpart
insert overwrite table nzhang_part10 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08';

from srcpart
insert overwrite table nzhang_part10 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08';


show partitions nzhang_part10;

select * from nzhang_part10 where ds is not null and hr is not null;


show partitions srcpart;


create table if not exists nzhang_part like srcpart;
describe extended nzhang_part;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.exec.compress.output=true;
set hive.exec.dynamic.partition=true;

insert overwrite table nzhang_part partition (ds="2010-03-03", hr) select key, value, hr from srcpart where ds is not null and hr is not null;

select * from nzhang_part where ds = '2010-03-03' and hr = '11';
select * from nzhang_part where ds = '2010-03-03' and hr = '12';


show partitions srcpart;



create table if not exists nzhang_part12 like srcpart;
describe extended nzhang_part12;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.exec.dynamic.partition=true;


insert overwrite table nzhang_part12 partition (ds="2010-03-03", hr) select key, value, cast(hr*2 as int) from srcpart where ds is not null and hr is not null;

show partitions nzhang_part12;

select * from nzhang_part12 where ds is not null and hr is not null;


set hive.mapred.mode=nonstrict;
show partitions srcpart;



create table if not exists nzhang_part13 like srcpart;
describe extended nzhang_part13;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.exec.dynamic.partition=true;

explain
insert overwrite table nzhang_part13 partition (ds="2010-03-03", hr)
select * from (
   select key, value, '22'
   from src
   where key < 20
   union all
   select key, value, '33'
   from src
   where key > 20 and key < 40) s;

insert overwrite table nzhang_part13 partition (ds="2010-03-03", hr)
select * from (
   select key, value, '22'
   from src
   where key < 20
   union all
   select key, value, '33'
   from src
   where key > 20 and key < 40) s;

show partitions nzhang_part13;

select * from nzhang_part13 where ds is not null and hr is not null;


-- EXCLUDE_OS_WINDOWS
-- excluded on windows because of difference in file name encoding logic

-- SORT_QUERY_RESULTS

create table if not exists nzhang_part14 (key string)
  partitioned by (value string);

describe extended nzhang_part14;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

explain
insert overwrite table nzhang_part14 partition(value)
select key, value from (
  select * from (select 'k1' as key, cast(null as string) as value from src limit 2)a
  union all
  select * from (select 'k2' as key, '' as value from src limit 2)b
  union all
  select * from (select 'k3' as key, ' ' as value from src limit 2)c
) T;

insert overwrite table nzhang_part14 partition(value)
select key, value from (
  select * from (select 'k1' as key, cast(null as string) as value from src limit 2)a
  union all
  select * from (select 'k2' as key, '' as value from src limit 2)b
  union all
  select * from (select 'k3' as key, ' ' as value from src limit 2)c
) T;


show partitions nzhang_part14;

select * from nzhang_part14 where value <> 'a';


-- INCLUDE_OS_WINDOWS
-- included only on  windows because of difference in file name encoding logic

-- SORT_QUERY_RESULTS

create table if not exists nzhang_part14 (key string)
  partitioned by (value string);

describe extended nzhang_part14;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

explain
insert overwrite table nzhang_part14 partition(value)
select key, value from (
  select * from (select 'k1' as key, cast(null as string) as value from src limit 2)a
  union all
  select * from (select 'k2' as key, '' as value from src limit 2)b
  union all
  select * from (select 'k3' as key, ' ' as value from src limit 2)c
) T;

insert overwrite table nzhang_part14 partition(value)
select key, value from (
  select * from (select 'k1' as key, cast(null as string) as value from src limit 2)a
  union all
  select * from (select 'k2' as key, '' as value from src limit 2)b
  union all
  select * from (select 'k3' as key, ' ' as value from src limit 2)c
) T;


show partitions nzhang_part14;

select * from nzhang_part14 where value <> 'a';



set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table if not exists load_dyn_part15_test (key string)
  partitioned by (part_key string);

show partitions load_dyn_part15_test;

INSERT OVERWRITE TABLE load_dyn_part15_test PARTITION(part_key)
SELECT key, part_key FROM src LATERAL VIEW explode(array("1","{2","3]")) myTable AS part_key;

show partitions load_dyn_part15_test;set hive.explain.user=false;
-- SORT_QUERY_RESULTS

create table if not exists nzhang_part_bucket (key string, value string)
  partitioned by (ds string, hr string)
  clustered by (key) into 10 buckets;

describe extended nzhang_part_bucket;

set hive.merge.mapfiles=false;

set hive.exec.dynamic.partition=true;

explain
insert overwrite table nzhang_part_bucket partition (ds='2010-03-23', hr) select key, value, hr from srcpart where ds is not null and hr is not null;

insert overwrite table nzhang_part_bucket partition (ds='2010-03-23', hr) select key, value, hr from srcpart where ds is not null and hr is not null;

show partitions nzhang_part_bucket;

select * from nzhang_part_bucket where ds='2010-03-23' and hr='11';
select * from nzhang_part_bucket where ds='2010-03-23' and hr='12';



set hive.explain.user=false;
-- SORT_QUERY_RESULTS

show partitions srcpart;



create table if not exists nzhang_part3 like srcpart;
describe extended nzhang_part3;

set hive.merge.mapfiles=false;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

explain
insert overwrite table nzhang_part3 partition (ds, hr) select key, value, ds, hr from srcpart where ds is not null and hr is not null;

insert overwrite table nzhang_part3 partition (ds, hr) select key, value, ds, hr from srcpart where ds is not null and hr is not null;

select * from nzhang_part3 where ds is not null and hr is not null;


-- SORT_QUERY_RESULTS

show partitions srcpart;



create table if not exists nzhang_part4 like srcpart;
describe extended nzhang_part4;

set hive.merge.mapfiles=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table nzhang_part4 partition (ds='2008-04-08', hr='existing_value') select key, value from src;

explain
insert overwrite table nzhang_part4 partition (ds, hr) select key, value, ds, hr from srcpart where ds is not null and hr is not null;

insert overwrite table nzhang_part4 partition (ds, hr) select key, value, ds, hr from srcpart where ds is not null and hr is not null;

show partitions nzhang_part4;
select * from nzhang_part4 where ds='2008-04-08' and hr is not null;

select * from nzhang_part4 where ds is not null and hr is not null;




create table if not exists nzhang_part5 (key string) partitioned by (value string);
describe extended nzhang_part5;

set hive.merge.mapfiles=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions=2000;
set hive.exec.max.dynamic.partitions.pernode=2000;

explain
insert overwrite table nzhang_part5 partition (value) select key, value from src;

insert overwrite table nzhang_part5 partition (value) select key, value from src;

show partitions nzhang_part5;

select * from nzhang_part5 where value='val_0';
select * from nzhang_part5 where value='val_2';


show partitions srcpart;



create table if not exists nzhang_part6 like srcpart;
describe extended nzhang_part6;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.exec.dynamic.partition=true;

insert overwrite table nzhang_part6 partition (ds="2010-03-03", hr) select key, value, hr from srcpart where ds is not null and hr is not null;

select * from nzhang_part6 where ds = '2010-03-03' and hr = '11';
select * from nzhang_part6 where ds = '2010-03-03' and hr = '12';

show partitions srcpart;



create table if not exists nzhang_part7 like srcpart;
describe extended nzhang_part7;


insert overwrite table nzhang_part7 partition (ds='2010-03-03', hr='12') select key, value from srcpart where ds = '2008-04-08' and hr = '12';

show partitions nzhang_part7;

select * from nzhang_part7 where ds is not null and hr is not null;

-- SORT_QUERY_RESULTS

show partitions srcpart;



create table if not exists nzhang_part8 like srcpart;
describe extended nzhang_part8;

set hive.merge.mapfiles=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

explain extended
from srcpart
insert overwrite table nzhang_part8 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
insert overwrite table nzhang_part8 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08';

from srcpart
insert overwrite table nzhang_part8 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
insert overwrite table nzhang_part8 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08';

show partitions nzhang_part8;

select * from nzhang_part8 where ds is not null and hr is not null;

-- SORT_QUERY_RESULTS

show partitions srcpart;



create table if not exists nzhang_part9 like srcpart;
describe extended nzhang_part9;

set hive.merge.mapfiles=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

explain
from srcpart
insert overwrite table nzhang_part9 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08';

from srcpart
insert overwrite table nzhang_part9 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08';


show partitions nzhang_part9;

select * from nzhang_part9 where ds is not null and hr is not null;

set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table hive_test_src ( col1 string ) partitioned by (pcol1 string) stored as textfile;
alter table hive_test_src add partition (pcol1 = 'test_part');
set hive.security.authorization.enabled=true;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src partition (pcol1 = 'test_part');
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table hive_test_src ( col1 string ) partitioned by (pcol1 string) stored as textfile;
alter table hive_test_src add partition (pcol1 = 'test_part');
set hive.security.authorization.enabled=true;
grant Update on table hive_test_src to user hive_test_user;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src partition (pcol1 = 'test_part');
-- test for loading into tables with the file with space in the name


CREATE TABLE load_file_with_space_in_the_name(name STRING, age INT);
LOAD DATA LOCAL INPATH '../../data/files/person age.txt' INTO TABLE load_file_with_space_in_the_name;
LOAD DATA LOCAL INPATH '../../data/files/person+age.txt' INTO TABLE load_file_with_space_in_the_name;

create table load_overwrite (key string, value string) stored as textfile location 'file:${system:test.tmp.dir}/load_overwrite';
create table load_overwrite2 (key string, value string) stored as textfile location 'file:${system:test.tmp.dir}/load2_overwrite2';

load data local inpath '../../data/files/kv1.txt' into table load_overwrite;
load data local inpath '../../data/files/kv2.txt' into table load_overwrite;
load data local inpath '../../data/files/kv3.txt' into table load_overwrite;

show table extended like load_overwrite;
desc extended load_overwrite;
select count(*) from load_overwrite;

load data inpath '${system:test.tmp.dir}/load_overwrite/kv*.txt' overwrite into table load_overwrite2;

show table extended like load_overwrite2;
desc extended load_overwrite2;
select count(*) from load_overwrite2;

load data inpath '${system:test.tmp.dir}/load2_*' overwrite into table load_overwrite;
show table extended like load_overwrite;
select count(*) from load_overwrite;
-- HIVE-3300 [jira] LOAD DATA INPATH fails if a hdfs file with same name is added to table
-- 'loader' table is used only for uploading kv1.txt to HDFS (!hdfs -put is not working on minMRDriver)

create table result (key string, value string);
create table loader (key string, value string);

load data local inpath '../../data/files/kv1.txt' into table loader;

load data inpath '/build/ql/test/data/warehouse/loader/kv1.txt' into table result;
show table extended like result;

load data local inpath '../../data/files/kv1.txt' into table loader;

load data inpath '/build/ql/test/data/warehouse/loader/kv1.txt' into table result;
show table extended like result;

load data local inpath '../../data/files/kv1.txt' into table loader;

load data inpath '/build/ql/test/data/warehouse/loader/kv1.txt' into table result;
show table extended like result;
--HIVE 6209

drop table target;
drop table temp;

create table target (key string, value string) stored as textfile location 'file:${system:test.tmp.dir}/target';
create table temp (key string, value string) stored as textfile location 'file:${system:test.tmp.dir}/temp';

set fs.pfile.impl.disable.cache=false;

load data local inpath '../../data/files/kv1.txt' into table temp;
load data inpath '${system:test.tmp.dir}/temp/kv1.txt' overwrite into table target;
select count(*) from target;

load data local inpath '../../data/files/kv2.txt' into table temp;
load data inpath '${system:test.tmp.dir}/temp/kv2.txt' overwrite into table target;
select count(*) from target;

drop table target;
drop table temp;dfs ${system:test.dfs.mkdir} hdfs:///tmp/test_load_hdfs_file_with_space_in_the_name/;

dfs -copyFromLocal ../../data/files hdfs:///tmp/test_load_hdfs_file_with_space_in_the_name/.;

CREATE TABLE load_file_with_space_in_the_name(name STRING, age INT);
LOAD DATA INPATH 'hdfs:///tmp/test_load_hdfs_file_with_space_in_the_name/files/person age.txt' INTO TABLE load_file_with_space_in_the_name;
LOAD DATA INPATH 'hdfs:///tmp/test_load_hdfs_file_with_space_in_the_name/files/person+age.txt' INTO TABLE load_file_with_space_in_the_name;

dfs -rmr hdfs:///tmp/test_load_hdfs_file_with_space_in_the_name;


create table load_local (id INT);

load data local inpath '../../data/files/ext_test/' into table load_local;

select * from load_local;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table hive_test_src ( col1 string ) stored as textfile;
set hive.security.authorization.enabled=true;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src ;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table hive_test_src ( col1 string ) stored as textfile;
set hive.security.authorization.enabled=true;
grant Update on table hive_test_src to user hive_test_user;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src ;
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/non_hdfs_path;
dfs -touchz ${system:test.tmp.dir}/non_hdfs_path/1.txt;
dfs -chmod 555 ${system:test.tmp.dir}/non_hdfs_path/1.txt;

create table t1(i int);
load data inpath 'pfile:${system:test.tmp.dir}/non_hdfs_path/' overwrite into table t1;

CREATE TABLE non_native2(key int, value string)
STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler';

LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE non_native2;
set hive.default.fileformat=ORC;
create table orc_staging (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);
create table orc_test (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_staging;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_staging/;

load data inpath '${hiveconf:hive.metastore.warehouse.dir}/orc_staging/orc_split_elim.orc' into table orc_test;
load data local inpath '../../data/files/orc_split_elim.orc' into table orc_test;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_test/;
set hive.default.fileformat=ORC;
create table orc_test (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);

load data local inpath '../../data/files/kv1.txt' into table orc_test;
create table text_test (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);
load data local inpath '../../data/files/kv1.txt' into table text_test;

set hive.default.fileformat=ORC;
create table orc_test (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);
load data inpath '${hiveconf:hive.metastore.warehouse.dir}/text_test/kv1.txt' into table orc_test;
create table text_test (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);
load data local inpath '../../data/files/kv1.txt' into table text_test;

set hive.default.fileformat=ORC;
create table orc_test (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);
load data inpath '${hiveconf:hive.metastore.warehouse.dir}/text_test/' into table orc_test;
set hive.default.fileformat=ORC;
create table orc_staging (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);
create table orc_test (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) partitioned by (ds string);

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_staging;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_staging/;

load data inpath '${hiveconf:hive.metastore.warehouse.dir}/orc_staging/orc_split_elim.orc' into table orc_test partition (ds='10');
load data local inpath '../../data/files/orc_split_elim.orc' into table orc_test partition (ds='10');
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_test/ds=10/;

alter table orc_test add partition(ds='11');
load data local inpath '../../data/files/kv1.txt' into table orc_test partition(ds='11');
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_test/ds=11/;
set hive.default.fileformat=ORC;
create table orc_staging (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp);
create table orc_test (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) partitioned by (ds string);

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_staging;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_staging/;

load data inpath '${hiveconf:hive.metastore.warehouse.dir}/orc_staging/orc_split_elim.orc' into table orc_test partition (ds='10');
load data local inpath '../../data/files/orc_split_elim.orc' into table orc_test partition (ds='10');
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_test/ds=10/;

load data local inpath '../../data/files/orc_split_elim.orc' overwrite into table orc_staging;
load data inpath '${hiveconf:hive.metastore.warehouse.dir}/orc_staging/' overwrite into table orc_test partition (ds='10');
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_test/ds=10/;
create table load_overwrite like src;

insert overwrite table load_overwrite select * from src;
show table extended like load_overwrite;
select count(*) from load_overwrite;


load data local inpath '../../data/files/kv1.txt' into table load_overwrite;
show table extended like load_overwrite;
select count(*) from load_overwrite;


load data local inpath '../../data/files/kv1.txt' overwrite into table load_overwrite;
show table extended like load_overwrite;
select count(*) from load_overwrite;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table hive_test_src ( col1 string ) partitioned by (pcol1 string) stored as textfile;
set hive.security.authorization.enabled=true;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src partition (pcol1 = 'test_part');
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
create table hive_test_src ( col1 string ) partitioned by (pcol1 string) stored as textfile;
set hive.security.authorization.enabled=true;
grant Update on table hive_test_src to user hive_test_user;
load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src partition (pcol1 = 'test_part');
create table hive_test_src ( col1 string ) partitioned by (pcol1 string) stored as textfile;
load data local inpath '../../data/files/test.dat' into table hive_test_src;
-- Load data can't work with table with stored as directories
CREATE TABLE  if not exists stored_as_dirs_multiple (col1 STRING, col2 int, col3 STRING)
SKEWED BY (col1, col2) ON (('s1',1), ('s3',3), ('s13',13), ('s78',78))  stored as DIRECTORIES;

LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE stored_as_dirs_multiple;
DROP VIEW xxx11;
CREATE VIEW xxx11 AS SELECT * FROM src;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE xxx11;
-- test for loading into tables with the correct file format
-- test for loading into partitions with the correct file format


CREATE TABLE load_wrong_fileformat_T1(name STRING) STORED AS SEQUENCEFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE load_wrong_fileformat_T1;
-- test for loading into tables with the correct file format
-- test for loading into partitions with the correct file format


CREATE TABLE T1(name STRING) STORED AS RCFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.seq' INTO TABLE T1;-- test for loading into tables with the correct file format
-- test for loading into partitions with the correct file format


CREATE TABLE T1(name STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.seq' INTO TABLE T1;
CREATE TABLE loadpart1(a STRING, b STRING) PARTITIONED BY (ds STRING,ds1 STRING);
LOAD DATA LOCAL INPATH '../../data1/files/kv1.txt' INTO TABLE loadpart1 PARTITION(ds='2009-05-05');
set hive.exec.mode.local.auto=true;
set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.VerifySessionStateLocalErrorsHook;

FROM src SELECT TRANSFORM(key, value) USING 'python ../../data/scripts/cat_error.py' AS (key, value);
drop table tstsrc;
create table tstsrc like src;
insert overwrite table tstsrc select key, value from src;

SHOW LOCKS;
SHOW LOCKS tstsrc;

LOCK TABLE tstsrc shared;
SHOW LOCKS;
SHOW LOCKS tstsrc;
SHOW LOCKS tstsrc extended;

UNLOCK TABLE tstsrc;
SHOW LOCKS;
SHOW LOCKS extended;
SHOW LOCKS tstsrc;
lock TABLE tstsrc SHARED;
SHOW LOCKS;
SHOW LOCKS extended;
SHOW LOCKS tstsrc;
LOCK TABLE tstsrc SHARED;
SHOW LOCKS;
SHOW LOCKS extended;
SHOW LOCKS tstsrc;
UNLOCK TABLE tstsrc;
SHOW LOCKS;
SHOW LOCKS extended;
SHOW LOCKS tstsrc;
drop table tstsrc;
drop table tstsrc;
create table tstsrc like src;
insert overwrite table tstsrc select key, value from src;

drop table tstsrcpart;
create table tstsrcpart like srcpart;

insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='11')
select key, value from srcpart where ds='2008-04-08' and hr='11';

LOCK TABLE tstsrc SHARED;
LOCK TABLE tstsrcpart SHARED;
LOCK TABLE tstsrcpart PARTITION(ds='2008-04-08', hr='11') EXCLUSIVE;
SHOW LOCKS;
SHOW LOCKS tstsrcpart;
SHOW LOCKS tstsrcpart PARTITION(ds='2008-04-08', hr='11');
SHOW LOCKS tstsrcpart PARTITION(ds='2008-04-08', hr='11') extended;

UNLOCK TABLE tstsrc;
SHOW LOCKS;
SHOW LOCKS tstsrcpart;
SHOW LOCKS tstsrcpart PARTITION(ds='2008-04-08', hr='11');

UNLOCK TABLE tstsrcpart;
SHOW LOCKS;
SHOW LOCKS tstsrcpart;
SHOW LOCKS tstsrcpart PARTITION(ds='2008-04-08', hr='11');

UNLOCK TABLE tstsrcpart PARTITION(ds='2008-04-08', hr='11');
SHOW LOCKS;
SHOW LOCKS tstsrcpart;
SHOW LOCKS tstsrcpart PARTITION(ds='2008-04-08', hr='11');


drop table tstsrc;
drop table tstsrcpart;
drop table tstsrcpart;
create table tstsrcpart like srcpart;

from srcpart
insert overwrite table tstsrcpart partition (ds='2008-04-08',hr='11')
select key, value where ds='2008-04-08' and hr='11';

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;


from srcpart
insert overwrite table tstsrcpart partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08';

from srcpart
insert overwrite table tstsrcpart partition (ds ='2008-04-08', hr) select key, value, hr where ds = '2008-04-08';


SHOW LOCKS;
SHOW LOCKS tstsrcpart;

drop table tstsrcpart;

drop table tst1;
create table tst1 (key string, value string) partitioned by (a string, b string, c string, d string);


from srcpart
insert overwrite table tst1 partition (a='1', b='2', c, d) select key, value, ds, hr where ds = '2008-04-08';


drop table tst1;
set hive.lock.mapred.only.operation=true;
drop table tstsrcpart;
create table tstsrcpart like srcpart;

from srcpart
insert overwrite table tstsrcpart partition (ds='2008-04-08',hr='11')
select key, value where ds='2008-04-08' and hr='11';

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;


from srcpart
insert overwrite table tstsrcpart partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08';

from srcpart
insert overwrite table tstsrcpart partition (ds ='2008-04-08', hr) select key, value, hr where ds = '2008-04-08';


SHOW LOCKS;
SHOW LOCKS tstsrcpart;

drop table tstsrcpart;

drop table tst1;
create table tst1 (key string, value string) partitioned by (a string, b string, c string, d string);


from srcpart
insert overwrite table tst1 partition (a='1', b='2', c, d) select key, value, ds, hr where ds = '2008-04-08';


drop table tst1;
drop table tstsrc;
create table tstsrc like src;
insert overwrite table tstsrc select key, value from src;

set hive.lock.numretries=0;
set hive.unlock.numretries=0;

LOCK TABLE tstsrc SHARED;
LOCK TABLE tstsrc SHARED;
LOCK TABLE tstsrc EXCLUSIVE;
drop table tstsrc;
create table tstsrc like src;
insert overwrite table tstsrc select key, value from src;

set hive.unlock.numretries=0;
UNLOCK TABLE tstsrc;
drop table tstsrcpart;
create table tstsrcpart like srcpart;

insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='11')
select key, value from srcpart where ds='2008-04-08' and hr='11';

set hive.lock.numretries=0;
set hive.unlock.numretries=0;
UNLOCK TABLE tstsrcpart PARTITION(ds='2008-04-08', hr='11');
drop table tstsrcpart;
create table tstsrcpart like srcpart;

insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='11')
select key, value from srcpart where ds='2008-04-08' and hr='11';

set hive.lock.numretries=0;
set hive.unlock.numretries=0;

LOCK TABLE tstsrcpart PARTITION(ds='2008-04-08', hr='11') EXCLUSIVE;
SHOW LOCKS tstsrcpart PARTITION(ds='2008-04-08', hr='12');

drop table tstsrcpart;
show locks tstsrcpart extended;create database lockneg1;
use lockneg1;

create table tstsrcpart like default.srcpart;

insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='11')
select key, value from default.srcpart where ds='2008-04-08' and hr='11';

lock database lockneg1 shared;
show locks database lockneg1;
select count(1) from tstsrcpart where ds='2008-04-08' and hr='11';

unlock database lockneg1;
show locks database lockneg1;
lock database lockneg1 exclusive;
show locks database lockneg1;
select count(1) from tstsrcpart where ds='2008-04-08' and hr='11';
set hive.lock.numretries=0;

create database lockneg4;

lock database lockneg4 exclusive;
lock database lockneg4 shared;
set hive.lock.numretries=0;

create database lockneg9;

lock database lockneg9 shared;
show locks database lockneg9;

drop database lockneg9;
set hive.lock.numretries=0;

create database lockneg2;
use lockneg2;

create table tstsrcpart like default.srcpart;

insert overwrite table tstsrcpart partition (ds='2008-04-08', hr='11')
select key, value from default.srcpart where ds='2008-04-08' and hr='11';

lock database lockneg2 shared;
show locks;

lock database lockneg2 exclusive;
show locks;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;

-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
 FROM
  src a
 LEFT OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  src a
 LEFT OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

EXPLAIN EXTENDED
 FROM
  srcpart a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key AND a.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  srcpart a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key AND a.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;


EXPLAIN EXTENDED
 FROM
  src a
 LEFT OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';

 FROM
  src a
 LEFT OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';

EXPLAIN EXTENDED
 FROM
  srcpart a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND a.ds = '2008-04-08';

 FROM
  srcpart a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND a.ds = '2008-04-08';

set hive.explain.user=false;
-- SORT_QUERY_RESULTS

drop table sour1;
drop table sour2;
drop table expod1;
drop table expod2;

set hive.auto.convert.join=true;

create table sour1(id int, av1 string, av2 string, av3 string) row format delimited fields terminated by ',';
create table sour2(id int, bv1 string, bv2 string, bv3 string) row format delimited fields terminated by ',';

load data local inpath '../../data/files/sour1.txt' into table sour1;
load data local inpath '../../data/files//sour2.txt' into table sour2;

create table expod1(aid int, av array<string>);
create table expod2(bid int, bv array<string>);

insert overwrite table expod1 select id, array(av1,av2,av3) from sour1;
insert overwrite table expod2 select id, array(bv1,bv2,bv3) from sour2;

explain with sub1 as
(select aid, avalue from expod1 lateral view explode(av) avs as avalue ),
sub2 as
(select bid, bvalue from expod2 lateral view explode(bv) bvs as bvalue)
select sub1.aid, sub1.avalue, sub2.bvalue
from sub1,sub2
where sub1.aid=sub2.bid;

with sub1 as
(select aid, avalue from expod1 lateral view explode(av) avs as avalue ),
sub2 as
(select bid, bvalue from expod2 lateral view explode(bv) bvs as bvalue)
select sub1.aid, sub1.avalue, sub2.bvalue
from sub1,sub2
where sub1.aid=sub2.bid;

set hive.fetch.task.conversion=more;

CREATE TEMPORARY MACRO SIGMOID (x DOUBLE) 1.0 / (1.0 + EXP(-x));
SELECT SIGMOID(2) FROM src LIMIT 1;
EXPLAIN SELECT SIGMOID(2) FROM src LIMIT 1;
EXPLAIN EXTENDED SELECT SIGMOID(2) FROM src LIMIT 1;
DROP TEMPORARY MACRO SIGMOID;

CREATE TEMPORARY MACRO FIXED_NUMBER() 1;
SELECT FIXED_NUMBER() + 1 FROM src LIMIT 1;
EXPLAIN SELECT FIXED_NUMBER() + 1 FROM src LIMIT 1;
EXPLAIN EXTENDED SELECT FIXED_NUMBER() + 1 FROM src LIMIT 1;
DROP TEMPORARY MACRO FIXED_NUMBER;

set macrotest=1;
CREATE TEMPORARY MACRO CONF_TEST() "${hiveconf:macrotest}";
SELECT CONF_TEST() FROM src LIMIT 1;
DROP TEMPORARY MACRO CONF_TEST;

CREATE TEMPORARY MACRO SIMPLE_ADD (x INT, y INT) x + y;
CREATE TEMPORARY MACRO SIMPLE_ADD (x INT, y INT) x + y;
SELECT SIMPLE_ADD(1, 9) FROM src LIMIT 1;
EXPLAIN SELECT SIMPLE_ADD(1, 9) FROM src LIMIT 1;
EXPLAIN EXTENDED SELECT SIMPLE_ADD(1, 9) FROM src LIMIT 1;
DROP TEMPORARY MACRO SIMPLE_ADD;
DROP TEMPORARY MACRO SIMPLE_ADD;


set hive.mapred.mode=nonstrict;
drop table macro_testing;
CREATE TABLE macro_testing(a int, b int, c int);

insert into table macro_testing values (1,2,3);
insert into table macro_testing values (4,5,6);

create temporary macro math_square(x int) x*x;
create temporary macro math_add(x int) x+x;

select math_square(a), math_square(b),factorial(a), factorial(b), math_add(a), math_add(b),int(c) from macro_testing order by int(c);
CREATE TEMPORARY MACRO DOUBLE (x DOUBLE) 1.0 / (1.0 + EXP(-x));
CREATE TEMPORARY MACRO BAD_MACRO (x INT, y INT) x;
SELECT  /*+ MAPJOIN(b) */ sum(a.key) as sum_a
    FROM srcpart a
    JOIN src b ON a.key = b.key where a.ds is not null;

set hive.outerjoin.supports.filters=true;

-- const filter on outer join
EXPLAIN
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND true limit 10;
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND true limit 10;

-- func filter on outer join
EXPLAIN
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND b.key * 10 < '1000' limit 10;
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND b.key * 10 < '1000' limit 10;

-- field filter on outer join
EXPLAIN
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN
    (select key, named_struct('key', key, 'value', value) as kv from src) b on a.key=b.key AND b.kv.key > 200 limit 10;
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN
    (select key, named_struct('key', key, 'value', value) as kv from src) b on a.key=b.key AND b.kv.key > 200 limit 10;

set hive.outerjoin.supports.filters=false;

EXPLAIN
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND true limit 10;
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND true limit 10;

EXPLAIN
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND b.key * 10 < '1000' limit 10;
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN src b on a.key=b.key AND b.key * 10 < '1000' limit 10;

EXPLAIN
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN
    (select key, named_struct('key', key, 'value', value) as kv from src) b on a.key=b.key AND b.kv.key > 200 limit 10;
SELECT /*+ MAPJOIN(a) */ * FROM src a RIGHT OUTER JOIN
    (select key, named_struct('key', key, 'value', value) as kv from src) b on a.key=b.key AND b.kv.key > 200 limit 10;

set hive.auto.convert.join=true;
set hive.auto.convert.join.use.nonstaged=false;

add jar ${system:maven.local.repository}/org/apache/hive/hcatalog/hive-hcatalog-core/${system:hive.version}/hive-hcatalog-core-${system:hive.version}.jar;

CREATE TABLE t1 (a string, b string)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
;
LOAD DATA LOCAL INPATH "../../data/files/sample.json" INTO TABLE t1;
select * from src join t1 on src.key =t1.a;
drop table t1;
set hive.auto.convert.join=false;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;

-- SORT_QUERY_RESULTS

CREATE TABLE over1k(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k;

CREATE TABLE t1(dec decimal(4,2)) STORED AS ORC;
INSERT INTO TABLE t1 select dec from over1k;
CREATE TABLE t2(dec decimal(4,0)) STORED AS ORC;
INSERT INTO TABLE t2 select dec from over1k;

explain
select t1.dec, t2.dec from t1 join t2 on (t1.dec=t2.dec) order by t1.dec;

set hive.mapjoin.optimized.hashtable=false;

select t1.dec, t2.dec from t1 join t2 on (t1.dec=t2.dec) order by t1.dec;

set hive.mapjoin.optimized.hashtable=true;

select t1.dec, t2.dec from t1 join t2 on (t1.dec=t2.dec) order by t1.dec;
set hive.map.aggr = true;
set hive.groupby.skewindata = true;
explain
FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.value;

FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.value as value order by value limit 10;

set hive.map.aggr = true;
set hive.groupby.skewindata = false;
explain
FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.value;

FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.value as value order by value limit 10;


set hive.map.aggr = false;
set hive.groupby.skewindata = true;
explain
FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.value;

FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.value as value order by value limit 10;


set hive.map.aggr = false;
set hive.groupby.skewindata = false;
explain
FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.value;

FROM srcpart c
JOIN srcpart d
ON ( c.key=d.key AND c.ds='2008-04-08' AND d.ds='2008-04-08')
SELECT /*+ MAPJOIN(d) */ DISTINCT c.value as value order by value limit 10;


set hive.auto.convert.join = false;

-- SORT_QUERY_RESULTS

--HIVE-2101 mapjoin sometimes gives wrong results if there is a filter in the on condition

SELECT * FROM src1
  RIGHT OUTER JOIN src1 src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10)
  JOIN src src3 ON (src2.key = src3.key AND src3.key < 300)
  SORT BY src1.key, src2.key, src3.key;

explain
SELECT /*+ mapjoin(src1, src2) */ * FROM src1
  RIGHT OUTER JOIN src1 src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10)
  JOIN src src3 ON (src2.key = src3.key AND src3.key < 300)
  SORT BY src1.key, src2.key, src3.key;

SELECT /*+ mapjoin(src1, src2) */ * FROM src1
  RIGHT OUTER JOIN src1 src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10)
  JOIN src src3 ON (src2.key = src3.key AND src3.key < 300)
  SORT BY src1.key, src2.key, src3.key;

set hive.auto.convert.join = true;

explain
SELECT * FROM src1
  RIGHT OUTER JOIN src1 src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10)
  JOIN src src3 ON (src2.key = src3.key AND src3.key < 300)
  SORT BY src1.key, src2.key, src3.key;

SELECT * FROM src1
  RIGHT OUTER JOIN src1 src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10)
  JOIN src src3 ON (src2.key = src3.key AND src3.key < 300)
  SORT BY src1.key, src2.key, src3.key;
set hive.exec.post.hooks = org.apache.hadoop.hive.ql.hooks.MapJoinCounterHook,org.apache.hadoop.hive.ql.hooks.PrintCompletedTasksHook;

drop table dest1;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

set hive.auto.convert.join = true;

INSERT OVERWRITE TABLE dest1
SELECT /*+ MAPJOIN(x) */ x.key, count(1) FROM src1 x JOIN src y ON (x.key = y.key) group by x.key;


FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key = src3.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;



set hive.mapjoin.localtask.max.memory.usage = 0.0001;
set hive.mapjoin.check.memory.rows = 2;
set hive.auto.convert.join.noconditionaltask = false;


FROM srcpart src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src2.value
where (src1.ds = '2008-04-08' or src1.ds = '2008-04-09' )and (src1.hr = '12' or src1.hr = '11');


FROM src src1 JOIN src src2 ON (src1.key = src2.key) JOIN src src3 ON (src1.key + src2.key = src3.key)
INSERT OVERWRITE TABLE dest1 SELECT src1.key, src3.value;




set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;
set hive.metastore.aggregate.stats.cache.enabled=false;

-- Since the inputs are small, it should be automatically converted to mapjoin

-- SORT_QUERY_RESULTS

explain extended select srcpart.key from srcpart join src on (srcpart.value=src.value) join src1 on (srcpart.key=src1.key);

explain
select srcpart.key from srcpart join src on (srcpart.value=src.value) join src1 on (srcpart.key=src1.key) where srcpart.value > 'val_450';

explain
select count(*) from srcpart join src on (srcpart.value=src.value) join src src1 on (srcpart.key=src1.key) group by ds;

set hive.mapjoin.optimized.hashtable=false;

select srcpart.key from srcpart join src on (srcpart.value=src.value) join src1 on (srcpart.key=src1.key) where srcpart.value > 'val_450';

select count(*) from srcpart join src on (srcpart.value=src.value) join src src1 on (srcpart.key=src1.key) group by ds;

set hive.mapjoin.optimized.hashtable=true;

select srcpart.key from srcpart join src on (srcpart.value=src.value) join src1 on (srcpart.key=src1.key) where srcpart.value > 'val_450';

select count(*) from srcpart join src on (srcpart.value=src.value) join src src1 on (srcpart.key=src1.key) group by ds;
set hive.mapred.mode=nonstrict;

set hive.auto.convert.join = true;

-- SORT_QUERY_RESULTS

create table src0 like src;
insert into table src0 select * from src where src.key < 10;

set hive.mapjoin.check.memory.rows=1;

explain
select src1.key as k1, src1.value as v1, src2.key, src2.value
from src0 src1 inner join src0 src2 on src1.key = src2.key;

select src1.key as k1, src1.value as v1, src2.key, src2.value
from src0 src1 inner join src0 src2 on src1.key = src2.key;

drop table src0;set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- SORT_QUERY_RESULTS
-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN
SELECT subq.key1, z.value
FROM
(SELECT x.key as key1, x.value as value1, y.key as key2, y.value as value2
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);

SELECT subq.key1, z.value
FROM
(SELECT x.key as key1, x.value as value1, y.key as key2, y.value as value2
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);

EXPLAIN
SELECT subq.key1, z.value
FROM
(SELECT x.key as key1, x.value as value1, y.key as key2, y.value as value2
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);

SELECT subq.key1, z.value
FROM
(SELECT x.key as key1, x.value as value1, y.key as key2, y.value as value2
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);
set hive.mapred.mode=nonstrict;
drop table x;
drop table y;
drop table z;

CREATE TABLE x (name STRING, id INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

CREATE TABLE y (id INT, name STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

CREATE TABLE z (id INT, name STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

load data local inpath '../../data/files/x.txt' INTO TABLE x;
load data local inpath '../../data/files/y.txt' INTO TABLE y;
load data local inpath '../../data/files/z.txt' INTO TABLE z;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

EXPLAIN
SELECT subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
FROM
(SELECT x.id as key1, x.name as value1, y.id as key2, y.name as value2
 FROM y JOIN x ON (x.id = y.id)) subq
 JOIN z ON (subq.key1 = z.id);

SELECT subq.key1, subq.value1, subq.key2, subq.value2, z.id, z.name
FROM
(SELECT x.id as key1, x.name as value1, y.id as key2, y.name as value2
 FROM y JOIN x ON (x.id = y.id)) subq
 JOIN z ON (subq.key1 = z.id);

drop table x;
drop table y;
drop table z;
set hive.auto.convert.join = false;

-- SORT_QUERY_RESULTS

--HIVE-2101 mapjoin sometimes gives wrong results if there is a filter in the on condition

create table dest_1 (key STRING, value STRING) stored as textfile;
insert overwrite table dest_1 select * from src1 order by src1.value limit 8;
insert into table dest_1 select "333444","555666" from src1 limit 1;

create table dest_2 (key STRING, value STRING) stored as textfile;

insert into table dest_2 select * from dest_1;

SELECT * FROM src1
  RIGHT OUTER JOIN dest_1 src2 ON (src1.key = src2.key)
  JOIN dest_2 src3 ON (src2.key = src3.key)
  SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT /*+ mapjoin(src1, src2) */ * FROM src1
  RIGHT OUTER JOIN dest_1 src2 ON (src1.key = src2.key)
  JOIN dest_2 src3 ON (src2.key = src3.key)
  SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT /*+ mapjoin(src1, src2) */ * FROM src1
  RIGHT OUTER JOIN dest_1 src2 ON (src1.key = src2.key)
  JOIN dest_2 src3 ON (src2.key = src3.key)
  SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT /*+ mapjoin(src1, src2) */ * FROM src1
  RIGHT OUTER JOIN dest_1 src2 ON (src1.key = src2.key)
  JOIN dest_2 src3 ON (src1.key = src3.key)
  SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

set hive.auto.convert.join = true;

SELECT * FROM src1
  LEFT OUTER JOIN dest_1 src2 ON (src1.key = src2.key)
  JOIN dest_2 src3 ON (src1.key = src3.key)
  SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src1
  LEFT OUTER JOIN dest_1 src2 ON (src1.key = src2.key)
  JOIN dest_2 src3 ON (src2.key = src3.key)
  SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

explain
SELECT * FROM src1
  RIGHT OUTER JOIN dest_1 src2 ON (src1.key = src2.key)
  JOIN dest_2 src3 ON (src2.key = src3.key)
  SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;

SELECT * FROM src1
  RIGHT OUTER JOIN dest_1 src2 ON (src1.key = src2.key)
  JOIN dest_2 src3 ON (src2.key = src3.key)
  SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;
set hive.explain.user=false;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
DISTRIBUTE BY tvalue, tkey
SORT BY ten, one;


FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
DISTRIBUTE BY tvalue, tkey
SORT BY ten, one;


SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
DISTRIBUTE BY tvalue, tkey;


FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
DISTRIBUTE BY tvalue, tkey;

SELECT * FROM (SELECT dest1.* FROM dest1 DISTRIBUTE BY key SORT BY key, ten, one, value) T ORDER BY key;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
SORT BY tvalue, tkey;


FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
SORT BY tvalue, tkey;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
DISTRIBUTE BY tvalue, tkey
SORT BY ten DESC, one ASC;


FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
DISTRIBUTE BY tvalue, tkey
SORT BY ten DESC, one ASC;


SELECT dest1.* FROM dest1;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
SELECT src.key as c1, CAST(src.key / 10 AS INT) as c2, CAST(src.key % 10 AS INT) as c3, src.value as c4
DISTRIBUTE BY c4, c1
SORT BY c2 DESC, c3 ASC;


FROM src
INSERT OVERWRITE TABLE dest1
SELECT src.key as c1, CAST(src.key / 10 AS INT) as c2, CAST(src.key % 10 AS INT) as c3, src.value as c4
DISTRIBUTE BY c4, c1
SORT BY c2 DESC, c3 ASC;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
SELECT src.key, CAST(src.key / 10 AS INT) as c2, CAST(src.key % 10 AS INT) as c3, src.value
DISTRIBUTE BY value, key
SORT BY c2 DESC, c3 ASC;


FROM src
INSERT OVERWRITE TABLE dest1
SELECT src.key, CAST(src.key / 10 AS INT) as c2, CAST(src.key % 10 AS INT) as c3, src.value
DISTRIBUTE BY value, key
SORT BY c2 DESC, c3 ASC;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(k STRING, v STRING, key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
MAP src.*, src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (k, v, tkey, ten, one, tvalue)
SORT BY tvalue, tkey;


FROM src
INSERT OVERWRITE TABLE dest1
MAP src.*, src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (k, v, tkey, ten, one, tvalue)
SORT BY tvalue, tkey;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(k STRING, v STRING, key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1
MAP src.*, src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (k, v, tkey, ten, one, tvalue)
DISTRIBUTE BY rand(3)
SORT BY tvalue, tkey;


FROM src
INSERT OVERWRITE TABLE dest1
MAP src.*, src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (k, v, tkey, ten, one, tvalue)
DISTRIBUTE BY rand(3)
SORT BY tvalue, tkey;

SELECT dest1.* FROM dest1;
set hive.exec.mode.local.auto=false;
set hive.exec.job.debug.capture.stacktraces=true;
set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.VerifySessionStateStackTracesHook;

FROM src SELECT TRANSFORM(key, value) USING 'script_does_not_exist' AS (key, value);

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Hadoop 0.23 changes the getTaskDiagnostics behavior
-- The Error Code of hive failure MapReduce job changes
-- In Hadoop 0.20
-- Hive failure MapReduce job gets 20000 as Error Code
-- In Hadoop 0.23
-- Hive failure MapReduce job gets 2 as Error Code
set hive.exec.mode.local.auto=false;
set hive.exec.job.debug.capture.stacktraces=true;
set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.VerifySessionStateStackTracesHook;

FROM src SELECT TRANSFORM(key, value) USING 'script_does_not_exist' AS (key, value);

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Hadoop 0.23 changes the getTaskDiagnostics behavior
-- The Error Code of hive failure MapReduce job changes
-- In Hadoop 0.20
-- Hive failure MapReduce job gets 20000 as Error Code
-- In Hadoop 0.23
-- Hive failure MapReduce job gets 2 as Error Code
set hive.exec.mode.local.auto=false;
set hive.exec.job.debug.capture.stacktraces=false;
set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.VerifySessionStateStackTracesHook;

FROM src SELECT TRANSFORM(key, value) USING 'script_does_not_exist' AS (key, value);

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Hadoop 0.23 changes the getTaskDiagnostics behavior
-- The Error Code of hive failure MapReduce job changes
-- In Hadoop 0.20
-- Hive failure MapReduce job gets 20000 as Error Code
-- In Hadoop 0.23
-- Hive failure MapReduce job gets 2 as Error Code
set hive.exec.mode.local.auto=false;
set hive.exec.job.debug.capture.stacktraces=false;
set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.VerifySessionStateStackTracesHook;

FROM src SELECT TRANSFORM(key, value) USING 'script_does_not_exist' AS (key, value);

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Hadoop 0.23 changes the getTaskDiagnostics behavior
-- The Error Code of hive failure MapReduce job changes
-- In Hadoop 0.20
-- Hive failure MapReduce job gets 20000 as Error Code
-- In Hadoop 0.23
-- Hive failure MapReduce job gets 2 as Error Code
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;

-- SORT_QUERY_RESULTS

create table dest1(key int, val int);

explain
insert overwrite table dest1
select key, count(1) from src group by key;

insert overwrite table dest1
select key, count(1) from src group by key;

select * from dest1;

drop table dest1;

create table test_src(key string, value string) partitioned by (ds string);
create table dest1(key string);

insert overwrite table test_src partition(ds='101') select * from src;
insert overwrite table test_src partition(ds='102') select * from src;

explain
insert overwrite table dest1 select key from test_src;
insert overwrite table dest1 select key from test_src;

set hive.merge.smallfiles.avgsize=16;
explain
insert overwrite table dest1 select key from test_src;
insert overwrite table dest1 select key from test_src;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;

-- SORT_QUERY_RESULTS

create table test1(key int, val int);

explain
insert overwrite table test1
select key, count(1) from src group by key;

insert overwrite table test1
select key, count(1) from src group by key;

select * from test1;

drop table test1;


create table test_src(key string, value string) partitioned by (ds string);
create table test1(key string);

insert overwrite table test_src partition(ds='101') select * from src;
insert overwrite table test_src partition(ds='102') select * from src;

explain
insert overwrite table test1 select key from test_src;
insert overwrite table test1 select key from test_src;

set hive.merge.smallfiles.avgsize=16;
explain
insert overwrite table test1 select key from test_src;
insert overwrite table test1 select key from test_src;
set hive.mapred.mode=nonstrict;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

-- SORT_QUERY_RESULTS

create table merge_src as
select key, value from srcpart where ds is not null;

create table merge_src_part (key string, value string) partitioned by (ds string);
insert overwrite table merge_src_part partition(ds) select key, value, ds from srcpart where ds is not null;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain extended
create table merge_src2 as
select key, value from merge_src;

create table merge_src2 as
select key, value from merge_src;

select * from merge_src2;
describe formatted merge_src2;

create table merge_src_part2 like merge_src_part;


explain extended
insert overwrite table merge_src_part2 partition(ds)
select key, value, ds from merge_src_part
where ds is not null;

insert overwrite table merge_src_part2 partition(ds)
select key, value, ds from merge_src_part
where ds is not null;

show partitions merge_src_part2;

select * from merge_src_part2 where ds is not null;

drop table merge_src_part2;

create table merge_src_part2 like merge_src_part;

explain extended
from (select * from merge_src_part where ds is not null distribute by ds) s
insert overwrite table merge_src_part2 partition(ds)
select key, value, ds;

from (select * from merge_src_part where ds is not null distribute by ds) s
insert overwrite table merge_src_part2 partition(ds)
select key, value, ds;

show partitions merge_src_part2;

select * from merge_src_part2 where ds is not null;
set hive.mapred.mode=nonstrict;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

-- SORT_QUERY_RESULTS

create table nzhang_part like srcpart;

explain
insert overwrite table nzhang_part partition (ds='2010-08-15', hr) select key, value, hr from srcpart where ds='2008-04-08';

insert overwrite table nzhang_part partition (ds='2010-08-15', hr) select key, value, hr from srcpart where ds='2008-04-08';

select * from nzhang_part;

explain
insert overwrite table nzhang_part partition (ds='2010-08-15', hr=11) select key, value from srcpart where ds='2008-04-08';

insert overwrite table nzhang_part partition (ds='2010-08-15', hr=11) select key, value from srcpart where ds='2008-04-08';

select * from nzhang_part;

explain
insert overwrite table nzhang_part partition (ds='2010-08-15', hr)
select * from (
    select key, value, hr from srcpart where ds='2008-04-08'
    union all
    select '1' as key, '1' as value, 'file,' as hr from src limit 1) s;

insert overwrite table nzhang_part partition (ds='2010-08-15', hr)
select * from (
    select key, value, hr from srcpart where ds='2008-04-08'
    union all
    select '1' as key, '1' as value, 'file,' as hr from src limit 1) s;

show partitions nzhang_part;

select * from nzhang_part where hr = 'file,';


set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.join.emit.interval=100000;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.tez.dynamic.partition.pruning=true;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

explain
select * from src a join src1 b on a.key = b.key;

select * from src a join src1 b on a.key = b.key;


CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS ORCFILE;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin_part;

CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS ORCFILE;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

explain
select count(*)
from tab a join tab_part b on a.key = b.key;

select * from tab a join tab_part b on a.key = b.key;

set hive.join.emit.interval=2;

select * from tab a join tab_part b on a.key = b.key;

explain
select count(*)
from tab a left outer join tab_part b on a.key = b.key;

select count(*)
from tab a left outer join tab_part b on a.key = b.key;

explain
select count (*)
from tab a right outer join tab_part b on a.key = b.key;

select count (*)
from tab a right outer join tab_part b on a.key = b.key;

explain
select count(*)
from tab a full outer join tab_part b on a.key = b.key;

select count(*)
from tab a full outer join tab_part b on a.key = b.key;

explain select count(*) from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value;
select count(*) from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value;

explain select count(*) from tab a join tab_part b on a.value = b.value;
select count(*) from tab a join tab_part b on a.value = b.value;

explain
select count(*) from (select s1.key as key, s1.value as value from tab s1 join tab s3 on s1.key=s3.key
UNION  ALL
select s2.key as key, s2.value as value from tab s2
) a join tab_part b on (a.key = b.key);

explain select count(*) from tab a join tab_part b on a.value = b.value;
select count(*) from tab a join tab_part b on a.value = b.value;

explain select count(*) from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value;
select count(*) from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value;

explain
select count(*) from (select s1.key as key, s1.value as value from tab s1 join tab s3 on s1.key=s3.key
UNION  ALL
select s2.key as key, s2.value as value from tab s2
) a join tab_part b on (a.key = b.key);

explain
select count(*) from
(select rt1.id from
(select t1.key as id, t1.value as od from tab t1 order by id, od) rt1) vt1
join
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
where vt1.id=vt2.id;

select count(*) from
(select rt1.id from
(select t1.key as id, t1.value as od from tab t1 order by id, od) rt1) vt1
join
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
where vt1.id=vt2.id;

set mapred.reduce.tasks=3;
select * from (select * from tab where tab.key = 0)a full outer join (select * from tab_part where tab_part.key = 98)b on a.key = b.key;
select * from (select * from tab where tab.key = 0)a right outer join (select * from tab_part where tab_part.key = 98)b on a.key = b.key;

select * from
(select * from tab where tab.key = 0)a
full outer join
(select * from tab_part where tab_part.key = 98)b join tab_part c on a.key = b.key and b.key = c.key;

select * from
(select * from tab where tab.key = 0)a
full outer join
(select * from tab_part where tab_part.key = 98)b on a.key = b.key join tab_part c on b.key = c.key;

select * from
(select * from tab where tab.key = 0)a
join
(select * from tab_part where tab_part.key = 98)b full outer join tab_part c on a.key = b.key and b.key = c.key;

select * from
(select * from tab where tab.key = 0)a
join
(select * from tab_part where tab_part.key = 98)b on a.key = b.key full outer join tab_part c on b.key = c.key;

set hive.cbo.enable = false;

select * from
(select * from tab where tab.key = 0)a
full outer join
(select * from tab_part where tab_part.key = 98)b join tab_part c on a.key = b.key and b.key = c.key;

select * from
(select * from tab where tab.key = 0)a
join
(select * from tab_part where tab_part.key = 98)b full outer join tab_part c on a.key = b.key and b.key = c.key;
set hive.mapred.mode=nonstrict;
create table a (val1 int, val2 int);
create table b (val1 int, val2 int);
create table c (val1 int, val2 int);
create table d (val1 int, val2 int);
create table e (val1 int, val2 int);

explain select * from a join b on a.val1=b.val1 join c on a.val1=c.val1 join d on a.val1=d.val1 join e on a.val2=e.val2;

--HIVE-3070 filter on outer join condition removed while merging join tree
explain select * from src a join src b on a.key=b.key left outer join src c on b.key=c.key and b.key<10;
set hive.mapred.mode=nonstrict;
-- HIVE-3464

create table a (key string, value string);

-- (a-b-c-d)
explain
select * from a join a b on (a.key=b.key) left outer join a c on (b.key=c.key) left outer join a d on (a.key=d.key);

explain
select * from a join a b on (a.key=b.key) left outer join a c on (b.key=c.key) right outer join a d on (a.key=d.key);

explain
select * from a join a b on (a.key=b.key) right outer join a c on (b.key=c.key) left outer join a d on (a.key=d.key);

explain
select * from a join a b on (a.key=b.key) right outer join a c on (b.key=c.key) right outer join a d on (a.key=d.key);

-- ((a-b-d)-c) (reordered)
explain
select * from a join a b on (a.key=b.key) left outer join a c on (b.value=c.key) left outer join a d on (a.key=d.key);

explain
select * from a join a b on (a.key=b.key) right outer join a c on (b.value=c.key) right outer join a d on (a.key=d.key);

explain
select * from a join a b on (a.key=b.key) full outer join a c on (b.value=c.key) full outer join a d on (a.key=d.key);

-- (((a-b)-c)-d)
explain
select * from a join a b on (a.key=b.key) left outer join a c on (b.value=c.key) right outer join a d on (a.key=d.key);

explain
select * from a join a b on (a.key=b.key) left outer join a c on (b.value=c.key) full outer join a d on (a.key=d.key);

explain
select * from a join a b on (a.key=b.key) right outer join a c on (b.value=c.key) left outer join a d on (a.key=d.key);

explain
select * from a join a b on (a.key=b.key) right outer join a c on (b.value=c.key) full outer join a d on (a.key=d.key);

-- ((a-b)-c-d)
explain
select * from a join a b on (a.key=b.key) left outer join a c on (b.value=c.key) left outer join a d on (c.key=d.key);
set hive.auto.convert.join=false;
set hive.cbo.enable=false;

select
  a.key, b.value, c.value
from
  src a,
  src1 b,
  src1 c
where
  a.key = b.key and a.key = c.key
  and b.key != '' and b.value != ''
  and a.value > 'wal_6789'
  and c.value > 'wal_6789'
;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

-- SORT_QUERY_RESULTS

create table srcpart_merge_dp like srcpart;

create table merge_dynamic_part like srcpart;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);

set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.smallfiles.avgsize=1000000000;
set hive.optimize.sort.dynamic.partition=false;
explain
insert overwrite table merge_dynamic_part partition (ds='2008-04-08', hr) select key, value, hr from srcpart_merge_dp where ds='2008-04-08';
insert overwrite table merge_dynamic_part partition (ds='2008-04-08', hr) select key, value, hr from srcpart_merge_dp where ds='2008-04-08';

select * from merge_dynamic_part;
show table extended like `merge_dynamic_part`;


set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1000000000;
explain
insert overwrite table merge_dynamic_part partition (ds='2008-04-08', hr=11) select key, value from srcpart_merge_dp where ds='2008-04-08';
insert overwrite table merge_dynamic_part partition (ds='2008-04-08', hr=11) select key, value from srcpart_merge_dp where ds='2008-04-08';

select * from merge_dynamic_part;
show table extended like `merge_dynamic_part`;

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1000000000;
explain
insert overwrite table merge_dynamic_part partition (ds, hr) select key, value, ds, hr from srcpart_merge_dp where ds='2008-04-08' and hr=11;
insert overwrite table merge_dynamic_part partition (ds, hr) select key, value, ds, hr from srcpart_merge_dp where ds='2008-04-08' and hr=11;;

select * from merge_dynamic_part;
show table extended like `merge_dynamic_part`;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table srcpart_merge_dp like srcpart;

create table merge_dynamic_part like srcpart;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket0.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);
load data local inpath '../../data/files/srcbucket1.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);


set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=3000;
set hive.exec.compress.output=false;
set hive.optimize.sort.dynamic.partition=false;

explain
insert overwrite table merge_dynamic_part partition (ds='2008-04-08', hr) select key, value, hr from srcpart_merge_dp where ds='2008-04-08';
insert overwrite table merge_dynamic_part partition (ds='2008-04-08', hr) select key, value, hr from srcpart_merge_dp where ds='2008-04-08';

show table extended like `merge_dynamic_part`;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

-- SORT_QUERY_RESULTS

create table srcpart_merge_dp like srcpart;

create table merge_dynamic_part like srcpart;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);

load data local inpath '../../data/files/kv1.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-09', hr=11);
load data local inpath '../../data/files/kv2.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-09', hr=11);
load data local inpath '../../data/files/kv1.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-09', hr=12);
load data local inpath '../../data/files/kv2.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-09', hr=12);

show partitions srcpart_merge_dp;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=3000;
set hive.exec.compress.output=false;

explain
insert overwrite table merge_dynamic_part partition (ds, hr) select key, value, ds, hr from srcpart_merge_dp where ds>='2008-04-08';

insert overwrite table merge_dynamic_part partition (ds, hr) select key, value, ds, hr from srcpart_merge_dp where ds>='2008-04-08';

select ds, hr, count(1) from merge_dynamic_part where ds>='2008-04-08' group by ds, hr;

show table extended like `merge_dynamic_part`;
set hive.mapred.mode=nonstrict;
-- this test verifies that the block merge task that can follow a query to generate dynamic
-- partitions does not produce incorrect results by dropping partitions

create table srcpart_merge_dp like srcpart;

create table srcpart_merge_dp_rc like srcpart;
alter table srcpart_merge_dp_rc set fileformat RCFILE;

create table merge_dynamic_part like srcpart;
alter table merge_dynamic_part set fileformat RCFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);

insert overwrite table srcpart_merge_dp_rc partition (ds = '2008-04-08', hr)
select key, value, hr from srcpart_merge_dp where ds = '2008-04-08';

set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=10000000000000;
set hive.exec.compress.output=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

explain
insert overwrite table merge_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 2 == 0, 'a1', 'b1') as hr from srcpart_merge_dp_rc where ds = '2008-04-08';

insert overwrite table merge_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 2 == 0, 'a1', 'b1') as hr from srcpart_merge_dp_rc where ds = '2008-04-08';

show partitions merge_dynamic_part;

select count(*) from merge_dynamic_part;
set hive.mapred.mode=nonstrict;
-- this is to test the case where some dynamic partitions are merged and some are moved

create table srcpart_merge_dp like srcpart;

create table srcpart_merge_dp_rc like srcpart;
alter table srcpart_merge_dp_rc set fileformat RCFILE;

create table merge_dynamic_part like srcpart;
alter table merge_dynamic_part set fileformat RCFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=11);

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcpart_merge_dp partition(ds='2008-04-08', hr=12);

insert overwrite table srcpart_merge_dp_rc partition (ds = '2008-04-08', hr)
select key, value, hr from srcpart_merge_dp where ds = '2008-04-08';

set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=200;
set hive.exec.compress.output=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

explain
insert overwrite table merge_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') as hr from srcpart_merge_dp_rc where ds = '2008-04-08';

insert overwrite table merge_dynamic_part partition (ds = '2008-04-08', hr)
select key, value, if(key % 100 == 0, 'a1', 'b1') as hr from srcpart_merge_dp_rc where ds = '2008-04-08';

show partitions merge_dynamic_part;

select count(*) from merge_dynamic_part;
drop table if exists test_join_1;
drop table if exists test_join_2;

create table test_join_1(a string, b string);
create table test_join_2(a string, b string);

explain
select * from
(
    SELECT a a, b b
    FROM test_join_1
)t1

join

(
    SELECT a a, b b
    FROM test_join_1
)t2
    on  t1.a = t2.a
    and t1.a = t2.b

join

(
    select a from test_join_2
)t3 on t1.a = t3.a;

drop table test_join_1;
drop table test_join_2;
create table src2 like src;
CREATE INDEX src_index_merge_test ON TABLE src2(key) as 'COMPACT' WITH DEFERRED REBUILD;
alter table src2 concatenate;
create table srcpart2 (key int, value string) partitioned by (ds string);
insert overwrite table srcpart2 partition (ds='2011') select * from src;
alter table srcpart2 concatenate;



create table srcpart2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets stored as RCFILE;
insert overwrite table srcpart2 partition (ds='2011') select * from src;
alter table srcpart2 partition (ds = '2011') concatenate;
set hive.mapred.mode=nonstrict;
CREATE TABLE TEST1(A INT, B DOUBLE) partitioned by (ds string);
explain extended select max(ds) from TEST1;
select max(ds) from TEST1;

alter table TEST1 add partition (ds='1');
explain extended select max(ds) from TEST1;
select max(ds) from TEST1;

explain extended select count(distinct ds) from TEST1;
select count(distinct ds) from TEST1;

explain extended select count(ds) from TEST1;
select count(ds) from TEST1;

alter table TEST1 add partition (ds='2');
explain extended
select count(*) from TEST1 a2 join (select max(ds) m from TEST1) b on a2.ds=b.m;
select count(*) from TEST1 a2 join (select max(ds) m from TEST1) b on a2.ds=b.m;


CREATE TABLE TEST2(A INT, B DOUBLE) partitioned by (ds string, hr string);
alter table TEST2 add partition (ds='1', hr='1');
alter table TEST2 add partition (ds='1', hr='2');
alter table TEST2 add partition (ds='1', hr='3');

explain extended select ds, count(distinct hr) from TEST2 group by ds;
select ds, count(distinct hr) from TEST2 group by ds;

explain extended select ds, count(hr) from TEST2 group by ds;
select ds, count(hr) from TEST2 group by ds;

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

explain extended select max(ds) from TEST1;
select max(ds) from TEST1;

select distinct ds from srcpart;
select min(ds),max(ds) from srcpart;

-- HIVE-3594 URI encoding for temporary path
alter table TEST2 add partition (ds='01:10:10', hr='01');
alter table TEST2 add partition (ds='01:10:20', hr='02');

explain extended select ds, count(distinct hr) from TEST2 group by ds;
select ds, count(distinct hr) from TEST2 group by ds;
set hive.mapred.mode=nonstrict;
select key from(
select '1' as key from srcpart where ds="2008-04-09"
UNION all
SELECT key from srcpart where ds="2008-04-09" and hr="11"
) tab group by key;

select key from(
SELECT '1' as key from src
UNION all
SELECT key as key from src
) tab group by key;

select max(key) from(
SELECT '1' as key from src
UNION all
SELECT key as key from src
) tab group by key;

select key from(
SELECT '1' as key from src
UNION all
SELECT '2' as key from src
) tab group by key;


select key from(
SELECT '1' as key from src
UNION all
SELECT key as key from src
UNION all
SELECT '2' as key from src
UNION all
SELECT key as key from src
) tab group by key;

select k from (select * from (SELECT '1' as k from src limit 0)a union all select key as k from src limit 1)tab;

select k from (select * from (SELECT '1' as k from src limit 1)a union all select key as k from src limit 0)tab;

select max(ds) from srcpart;

select count(ds) from srcpart;


create table tmp_meta_export_listener_drop_test (foo string);
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/data/exports/HIVE-3427;
set hive.metastore.pre.event.listeners=org.apache.hadoop.hive.ql.parse.MetaDataExportListener;
set hive.metadata.export.location=${system:test.tmp.dir}/data/exports/HIVE-3427;
set hive.metadata.move.exported.metadata.to.trash=false;
drop table tmp_meta_export_listener_drop_test;
dfs -rmr ${system:test.tmp.dir}/data/exports/HIVE-3427;
set hive.metastore.pre.event.listeners=;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.compute.query.using.stats=true;
set hive.stats.autogather=true;
create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal,
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

create table stats_tbl(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal,
           bin binary);

create table stats_tbl_part(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal,
           bin binary) partitioned by (dt string);


insert overwrite table stats_tbl select * from over10k;

insert into table stats_tbl_part partition (dt='2010') select * from over10k where t>0 and t<30;
insert into table stats_tbl_part partition (dt='2011') select * from over10k where t>30 and t<60;
insert into table stats_tbl_part partition (dt='2012') select * from over10k where t>60;

explain
select count(*), sum(1), sum(0.2), count(1), count(s), count(bo), count(bin), count(si), max(i), min(b) from stats_tbl;
explain
select count(*), sum(1), sum(0.2), count(1), count(s), count(bo), count(bin), count(si), max(i), min(b) from stats_tbl_part;

explain
select count(*), '1' as one, sum(1), sum(0.2), 2 as two, count(1), count(s), 3+4.0 as three, count(bo), count(bin), count(si), max(i), min(b) from stats_tbl;
explain
select count(*), '1' as one, sum(1), sum(0.2), 2 as two, count(1), count(s), 3+4.0 as three, count(bo), count(bin), count(si), max(i), min(b) from stats_tbl_part;

analyze table stats_tbl compute statistics for columns t,si,i,b,f,d,bo,s,bin;
analyze table stats_tbl_part partition(dt='2010') compute statistics for columns t,si,i,b,f,d,bo,s,bin;
analyze table stats_tbl_part partition(dt='2011') compute statistics for columns t,si,i,b,f,d,bo,s,bin;
analyze table stats_tbl_part partition(dt='2012') compute statistics for columns t,si,i,b,f,d,bo,s,bin;

explain
select count(*), sum(1), sum(0.2), count(1), count(s), count(bo), count(bin), count(si) from stats_tbl;
select count(*), sum(1), sum(0.2), count(1), count(s), count(bo), count(bin), count(si) from stats_tbl;
explain
select min(i), max(i), min(b), max(b), min(f), max(f), min(d), max(d) from stats_tbl;
select min(i), max(i), min(b), max(b), min(f), max(f), min(d), max(d) from stats_tbl;

explain
select min(i), '1' as one, max(i), min(b), max(b), min(f), max(f), 3+4.0 as three, min(d), max(d) from stats_tbl;
select min(i), '1' as one, max(i), min(b), max(b), min(f), max(f), 3+4.0 as three, min(d), max(d) from stats_tbl;



explain
select count(*), sum(1), sum(0.2), count(1), count(s), count(bo), count(bin), count(si) from stats_tbl_part;
select count(*), sum(1), sum(0.2), count(1), count(s), count(bo), count(bin), count(si) from stats_tbl_part;
explain
select min(i), max(i), min(b), max(b), min(f), max(f), min(d), max(d) from stats_tbl_part;
select min(i), max(i), min(b), max(b), min(f), max(f), min(d), max(d) from stats_tbl_part;

explain
select min(i), '1' as one, max(i), min(b), max(b), min(f), max(f), 3+4.0 as three, min(d), max(d) from stats_tbl_part;
select min(i), '1' as one, max(i), min(b), max(b), min(f), max(f), 3+4.0 as three, min(d), max(d) from stats_tbl_part;

explain select count(ts) from stats_tbl_part;

explain select count('1') from stats_tbl group by '1';
select count('1') from stats_tbl group by '1';

explain select count('1') from stats_tbl_part group by '1';
select count('1') from stats_tbl_part group by '1';

drop table stats_tbl;
drop table stats_tbl_part;

set hive.compute.query.using.stats=false;
set hive.stats.dbclass=fs;
set hive.compute.query.using.stats=true;
set hive.explain.user=false;
create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal,
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

create table stats_tbl_part(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal,
           bin binary) partitioned by (dt int);


from over10k
insert overwrite table stats_tbl_part partition (dt=2010) select t,si,i,b,f,d,bo,s,ts,dec,bin where t>0 and t<30
insert overwrite table stats_tbl_part partition (dt=2014) select t,si,i,b,f,d,bo,s,ts,dec,bin where t > 30 and t<60;

analyze table stats_tbl_part partition(dt) compute statistics;
analyze table stats_tbl_part partition(dt=2010) compute statistics for columns t,si,i,b,f,d,bo,s,bin;
analyze table stats_tbl_part partition(dt=2014) compute statistics for columns t,si,i,b,f,d,bo,s,bin;

explain
select count(*), count(1), sum(1), count(s), count(bo), count(bin), count(si), max(i), min(b), max(f), min(d) from stats_tbl_part where dt = 2010;
select count(*), count(1), sum(1), count(s), count(bo), count(bin), count(si), max(i), min(b), max(f), min(d) from stats_tbl_part where dt = 2010;
explain
select count(*), count(1), sum(1), sum(2), count(s), count(bo), count(bin), count(si), max(i), min(b), max(f), min(d) from stats_tbl_part where dt > 2010;
select count(*), count(1), sum(1), sum(2), count(s), count(bo), count(bin), count(si), max(i), min(b), max(f), min(d) from stats_tbl_part where dt > 2010;

select count(*) from stats_tbl_part;
select count(*)/2 from stats_tbl_part;
drop table stats_tbl_part;
set hive.compute.query.using.stats=false;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

-- SORT_QUERY_RESULTS

create table nzhang_t1 like srcpart;
create table nzhang_t2 like srcpart;

FROM srcpart
INSERT OVERWRITE TABLE nzhang_t1 PARTITION (ds, hr)
SELECT key, value, ds, hr
WHERE ds = '2008-04-08' AND hr = '11'
INSERT OVERWRITE TABLE nzhang_t2 PARTITION (ds, hr)
SELECT key, value, ds, hr
WHERE ds = '2008-04-08' and hr = '12'
GROUP BY key, value, ds, hr;

show partitions nzhang_t1;
show partitions nzhang_t2;

select * from nzhang_t1;
select * from nzhang_t2;


set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.exec.script.allow.partial.consumption = false;
-- Tests exception in ScriptOperator.close() by passing to the operator a small amount of data
SELECT TRANSFORM(*) USING 'true' AS a, b FROM (SELECT TRANSFORM(*) USING 'echo' AS a, b FROM src LIMIT 1) tmp;set hive.ddl.output.format=json;

CREATE TABLE IF NOT EXISTS jsontable (key INT, value STRING) COMMENT 'json table' STORED AS TEXTFILE;

ALTER TABLE jsontable ADD COLUMNS (name STRING COMMENT 'a new column');

ALTER TABLE jsontable RENAME TO jsontable2;

SHOW TABLE EXTENDED LIKE jsontable2;

DROP TABLE jsontable2;

set hive.ddl.output.format=text;

create table mismatch_columns(key string, value string);

insert overwrite table mismatch_columns select key from srcpart where ds is not null;
FROM src
INSERT TABLE dest1 SELECT '1234', src.value WHERE src.key < 100
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join.noconditionaltask.size=60000000;
set hive.log.trace.id=mrrTest;

-- simple query with multiple reduce stages
-- SORT_QUERY_RESULTS

EXPLAIN SELECT key, count(value) as cnt FROM src GROUP BY key ORDER BY cnt;
SELECT key, count(value) as cnt FROM src GROUP BY key ORDER BY cnt;

set hive.auto.convert.join=false;
-- join query with multiple reduce stages;
EXPLAIN SELECT s2.key, count(distinct s2.value) as cnt FROM src s1 join src s2 on (s1.key = s2.key) GROUP BY s2.key ORDER BY cnt,s2.key;
SELECT s2.key, count(distinct s2.value) as cnt FROM src s1 join src s2 on (s1.key = s2.key) GROUP BY s2.key ORDER BY cnt,s2.key;

set hive.auto.convert.join=true;
-- same query with broadcast join
EXPLAIN SELECT s2.key, count(distinct s2.value) as cnt FROM src s1 join src s2 on (s1.key = s2.key) GROUP BY s2.key ORDER BY cnt,s2.key;
SELECT s2.key, count(distinct s2.value) as cnt FROM src s1 join src s2 on (s1.key = s2.key) GROUP BY s2.key ORDER BY cnt,s2.key;

set hive.auto.convert.join=false;
-- query with multiple branches in the task dag
EXPLAIN
SELECT *
FROM
  (SELECT key, count(value) as cnt
  FROM src GROUP BY key ORDER BY cnt) s1
  JOIN
  (SELECT key, count(value) as cnt
  FROM src GROUP BY key ORDER BY cnt) s2
  JOIN
  (SELECT key, count(value) as cnt
  FROM src GROUP BY key ORDER BY cnt) s3
  ON (s1.key = s2.key and s1.key = s3.key)
WHERE
  s1.cnt > 1
ORDER BY s1.key;

SELECT *
FROM
  (SELECT key, count(value) as cnt
  FROM src GROUP BY key ORDER BY cnt) s1
  JOIN
  (SELECT key, count(value) as cnt
  FROM src GROUP BY key ORDER BY cnt) s2
  JOIN
  (SELECT key, count(value) as cnt
  FROM src GROUP BY key ORDER BY cnt) s3
  ON (s1.key = s2.key and s1.key = s3.key)
WHERE
  s1.cnt > 1
ORDER BY s1.key;

set hive.log.trace.id=Test2;
set hive.auto.convert.join=true;
-- query with broadcast join in the reduce stage
EXPLAIN
SELECT *
FROM
  (SELECT key, count(value) as cnt FROM src GROUP BY key) s1
  JOIN src ON (s1.key = src.key) order by s1.key;

SELECT *
FROM
  (SELECT key, count(value) as cnt FROM src GROUP BY key) s1
  JOIN src ON (s1.key = src.key) order by s1.key;
CREATE TABLE TBL(C1 INT, C2 INT, C3 INT, C4 INT);

CREATE TABLE DEST1(d1 INT, d2 INT) STORED AS TEXTFILE;
CREATE TABLE DEST2(d1 INT, d2 INT, d3 INT) STORED AS TEXTFILE;
CREATE TABLE DEST3(d1 INT, d2 INT, d3 INT, d4 INT) STORED AS TEXTFILE;
CREATE TABLE DEST4(d1 INT, d2 INT, d3 INT, d4 INT) STORED AS TEXTFILE;

EXPLAIN
FROM TBL
INSERT OVERWRITE TABLE DEST1 SELECT TBL.C1, COUNT(TBL.C2) GROUP BY TBL.C1
INSERT OVERWRITE TABLE DEST2 SELECT TBL.C1, TBL.C2, COUNT(TBL.C3) GROUP BY TBL.C1, TBL.C2;

EXPLAIN
FROM TBL
INSERT OVERWRITE TABLE DEST1 SELECT TBL.C1, COUNT(TBL.C2) GROUP BY TBL.C1
INSERT OVERWRITE TABLE DEST2 SELECT TBL.C1, TBL.C2, COUNT(TBL.C3) GROUP BY TBL.C2, TBL.C1;

EXPLAIN
FROM TBL
INSERT OVERWRITE TABLE DEST3 SELECT TBL.C1, TBL.C2, TBL.C3, COUNT(TBL.C4) GROUP BY TBL.C1, TBL.C2, TBL.C3
INSERT OVERWRITE TABLE DEST2 SELECT TBL.C1, TBL.C2, COUNT(TBL.C3) GROUP BY TBL.C1, TBL.C2;

EXPLAIN
FROM TBL
INSERT OVERWRITE TABLE DEST3 SELECT TBL.C1, TBL.C2, TBL.C3, COUNT(TBL.C4) GROUP BY TBL.C1, TBL.C2, TBL.C3
INSERT OVERWRITE TABLE DEST4 SELECT TBL.C1, TBL.C2, TBL.C3, COUNT(TBL.C4) GROUP BY TBL.C1, TBL.C3, TBL.C2;


EXPLAIN
FROM TBL
INSERT OVERWRITE TABLE DEST3 SELECT TBL.C1, TBL.C2, TBL.C3, COUNT(TBL.C4) GROUP BY TBL.C1, TBL.C2, TBL.C3
INSERT OVERWRITE TABLE DEST2 SELECT TBL.C1, TBL.C2, COUNT(TBL.C3) GROUP BY TBL.C1, TBL.C2
INSERT OVERWRITE TABLE DEST1 SELECT TBL.C1, COUNT(TBL.C2) GROUP BY TBL.C1;
set hive.mapred.mode=nonstrict;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter,org.apache.hadoop.hive.ql.hooks.PrintCompletedTasksHook;

create table smallTbl1(key string, value string);
insert overwrite table smallTbl1 select * from src where key < 10;

create table smallTbl2(key string, value string);
insert overwrite table smallTbl2 select * from src where key < 10;

create table smallTbl3(key string, value string);
insert overwrite table smallTbl3 select * from src where key < 10;

create table smallTbl4(key string, value string);
insert overwrite table smallTbl4 select * from src where key < 10;

create table bigTbl(key string, value string);
insert overwrite table bigTbl
select * from
(
 select * from src
   union all
 select * from src
   union all
 select * from src
   union all
 select * from src
   union all
 select * from src
   union all
 select * from src
   union all
 select * from src
   union all
 select * from src
   union all
 select * from src
   union all
 select * from src
) subq;

set hive.auto.convert.join=true;

explain
select count(*) FROM
(select bigTbl.key as key, bigTbl.value as value1,
 bigTbl.value as value2 FROM bigTbl JOIN smallTbl1
 on (bigTbl.key = smallTbl1.key)
) firstjoin
JOIN
smallTbl2 on (firstjoin.value1 = smallTbl2.value);

select count(*) FROM
(select bigTbl.key as key, bigTbl.value as value1,
 bigTbl.value as value2 FROM bigTbl JOIN smallTbl1
 on (bigTbl.key = smallTbl1.key)
) firstjoin
JOIN
smallTbl2 on (firstjoin.value1 = smallTbl2.value);

set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Now run a query with two-way join, which should be converted into a
-- map-join followed by groupby - two MR jobs overall
explain
select count(*) FROM
(select bigTbl.key as key, bigTbl.value as value1,
 bigTbl.value as value2 FROM bigTbl JOIN smallTbl1
 on (bigTbl.key = smallTbl1.key)
) firstjoin
JOIN
smallTbl2 on (firstjoin.value1 = smallTbl2.value);

select count(*) FROM
(select bigTbl.key as key, bigTbl.value as value1,
 bigTbl.value as value2 FROM bigTbl JOIN smallTbl1
 on (bigTbl.key = smallTbl1.key)
) firstjoin
JOIN
smallTbl2 on (firstjoin.value1 = smallTbl2.value);

-- Now run a query with two-way join, which should first be converted into a
-- map-join followed by groupby and then finally into a single MR job.

explain
select count(*) FROM
(select bigTbl.key as key, bigTbl.value as value1,
 bigTbl.value as value2 FROM bigTbl JOIN smallTbl1
 on (bigTbl.key = smallTbl1.key)
) firstjoin
JOIN
smallTbl2 on (firstjoin.value1 = smallTbl2.value)
group by smallTbl2.key;

select count(*) FROM
(select bigTbl.key as key, bigTbl.value as value1,
 bigTbl.value as value2 FROM bigTbl JOIN smallTbl1
 on (bigTbl.key = smallTbl1.key)
) firstjoin
JOIN
smallTbl2 on (firstjoin.value1 = smallTbl2.value)
group by smallTbl2.key;

drop table bigTbl;

create table bigTbl(key1 string, key2 string, value string);
insert overwrite table bigTbl
select * from
(
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
   union all
 select key as key1, key as key2, value from src
) subq;

set hive.auto.convert.join.noconditionaltask=false;
-- First disable noconditionaltask
EXPLAIN
SELECT SUM(HASH(join3.key1)),
       SUM(HASH(join3.key2)),
       SUM(HASH(join3.key3)),
       SUM(HASH(join3.key4)),
       SUM(HASH(join3.key5)),
       SUM(HASH(smallTbl4.key)),
       SUM(HASH(join3.value1)),
       SUM(HASH(join3.value2))
FROM (SELECT join2.key1 as key1,
             join2.key2 as key2,
             join2.key3 as key3,
             join2.key4 as key4,
             smallTbl3.key as key5,
             join2.value1 as value1,
             join2.value2 as value2
      FROM (SELECT join1.key1 as key1,
                   join1.key2 as key2,
                   join1.key3 as key3,
                   smallTbl2.key as key4,
                   join1.value1 as value1,
                   join1.value2 as value2
            FROM (SELECT bigTbl.key1 as key1,
                         bigTbl.key2 as key2,
                         smallTbl1.key as key3,
                         bigTbl.value as value1,
                         bigTbl.value as value2
                  FROM bigTbl
                  JOIN smallTbl1 ON (bigTbl.key1 = smallTbl1.key)) join1
            JOIN smallTbl2 ON (join1.value1 = smallTbl2.value)) join2
      JOIN smallTbl3 ON (join2.key2 = smallTbl3.key)) join3
JOIN smallTbl4 ON (join3.key3 = smallTbl4.key);

SELECT SUM(HASH(join3.key1)),
       SUM(HASH(join3.key2)),
       SUM(HASH(join3.key3)),
       SUM(HASH(join3.key4)),
       SUM(HASH(join3.key5)),
       SUM(HASH(smallTbl4.key)),
       SUM(HASH(join3.value1)),
       SUM(HASH(join3.value2))
FROM (SELECT join2.key1 as key1,
             join2.key2 as key2,
             join2.key3 as key3,
             join2.key4 as key4,
             smallTbl3.key as key5,
             join2.value1 as value1,
             join2.value2 as value2
      FROM (SELECT join1.key1 as key1,
                   join1.key2 as key2,
                   join1.key3 as key3,
                   smallTbl2.key as key4,
                   join1.value1 as value1,
                   join1.value2 as value2
            FROM (SELECT bigTbl.key1 as key1,
                         bigTbl.key2 as key2,
                         smallTbl1.key as key3,
                         bigTbl.value as value1,
                         bigTbl.value as value2
                  FROM bigTbl
                  JOIN smallTbl1 ON (bigTbl.key1 = smallTbl1.key)) join1
            JOIN smallTbl2 ON (join1.value1 = smallTbl2.value)) join2
      JOIN smallTbl3 ON (join2.key2 = smallTbl3.key)) join3
JOIN smallTbl4 ON (join3.key3 = smallTbl4.key);

set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;
-- Enable noconditionaltask and set the size of hive.auto.convert.join.noconditionaltask.size
-- to 10000, which is large enough to fit all four small tables (smallTbl1 to smallTbl4).
-- We will use a single MR job to evaluate this query.
EXPLAIN
SELECT SUM(HASH(join3.key1)),
       SUM(HASH(join3.key2)),
       SUM(HASH(join3.key3)),
       SUM(HASH(join3.key4)),
       SUM(HASH(join3.key5)),
       SUM(HASH(smallTbl4.key)),
       SUM(HASH(join3.value1)),
       SUM(HASH(join3.value2))
FROM (SELECT join2.key1 as key1,
             join2.key2 as key2,
             join2.key3 as key3,
             join2.key4 as key4,
             smallTbl3.key as key5,
             join2.value1 as value1,
             join2.value2 as value2
      FROM (SELECT join1.key1 as key1,
                   join1.key2 as key2,
                   join1.key3 as key3,
                   smallTbl2.key as key4,
                   join1.value1 as value1,
                   join1.value2 as value2
            FROM (SELECT bigTbl.key1 as key1,
                         bigTbl.key2 as key2,
                         smallTbl1.key as key3,
                         bigTbl.value as value1,
                         bigTbl.value as value2
                  FROM bigTbl
                  JOIN smallTbl1 ON (bigTbl.key1 = smallTbl1.key)) join1
            JOIN smallTbl2 ON (join1.value1 = smallTbl2.value)) join2
      JOIN smallTbl3 ON (join2.key2 = smallTbl3.key)) join3
JOIN smallTbl4 ON (join3.key3 = smallTbl4.key);

SELECT SUM(HASH(join3.key1)),
       SUM(HASH(join3.key2)),
       SUM(HASH(join3.key3)),
       SUM(HASH(join3.key4)),
       SUM(HASH(join3.key5)),
       SUM(HASH(smallTbl4.key)),
       SUM(HASH(join3.value1)),
       SUM(HASH(join3.value2))
FROM (SELECT join2.key1 as key1,
             join2.key2 as key2,
             join2.key3 as key3,
             join2.key4 as key4,
             smallTbl3.key as key5,
             join2.value1 as value1,
             join2.value2 as value2
      FROM (SELECT join1.key1 as key1,
                   join1.key2 as key2,
                   join1.key3 as key3,
                   smallTbl2.key as key4,
                   join1.value1 as value1,
                   join1.value2 as value2
            FROM (SELECT bigTbl.key1 as key1,
                         bigTbl.key2 as key2,
                         smallTbl1.key as key3,
                         bigTbl.value as value1,
                         bigTbl.value as value2
                  FROM bigTbl
                  JOIN smallTbl1 ON (bigTbl.key1 = smallTbl1.key)) join1
            JOIN smallTbl2 ON (join1.value1 = smallTbl2.value)) join2
      JOIN smallTbl3 ON (join2.key2 = smallTbl3.key)) join3
JOIN smallTbl4 ON (join3.key3 = smallTbl4.key);

set hive.auto.convert.join.noconditionaltask.size=200;
-- Enable noconditionaltask and set the size of hive.auto.convert.join.noconditionaltask.size
-- to 200, which is large enough to fit two small tables. We will have two jobs to evaluate this
-- query. The first job is a Map-only job to evaluate join1 and join2.
-- The second job will evaluate the rest of this query.
EXPLAIN
SELECT SUM(HASH(join3.key1)),
       SUM(HASH(join3.key2)),
       SUM(HASH(join3.key3)),
       SUM(HASH(join3.key4)),
       SUM(HASH(join3.key5)),
       SUM(HASH(smallTbl4.key)),
       SUM(HASH(join3.value1)),
       SUM(HASH(join3.value2))
FROM (SELECT join2.key1 as key1,
             join2.key2 as key2,
             join2.key3 as key3,
             join2.key4 as key4,
             smallTbl3.key as key5,
             join2.value1 as value1,
             join2.value2 as value2
      FROM (SELECT join1.key1 as key1,
                   join1.key2 as key2,
                   join1.key3 as key3,
                   smallTbl2.key as key4,
                   join1.value1 as value1,
                   join1.value2 as value2
            FROM (SELECT bigTbl.key1 as key1,
                         bigTbl.key2 as key2,
                         smallTbl1.key as key3,
                         bigTbl.value as value1,
                         bigTbl.value as value2
                  FROM bigTbl
                  JOIN smallTbl1 ON (bigTbl.key1 = smallTbl1.key)) join1
            JOIN smallTbl2 ON (join1.value1 = smallTbl2.value)) join2
      JOIN smallTbl3 ON (join2.key2 = smallTbl3.key)) join3
JOIN smallTbl4 ON (join3.key3 = smallTbl4.key);

SELECT SUM(HASH(join3.key1)),
       SUM(HASH(join3.key2)),
       SUM(HASH(join3.key3)),
       SUM(HASH(join3.key4)),
       SUM(HASH(join3.key5)),
       SUM(HASH(smallTbl4.key)),
       SUM(HASH(join3.value1)),
       SUM(HASH(join3.value2))
FROM (SELECT join2.key1 as key1,
             join2.key2 as key2,
             join2.key3 as key3,
             join2.key4 as key4,
             smallTbl3.key as key5,
             join2.value1 as value1,
             join2.value2 as value2
      FROM (SELECT join1.key1 as key1,
                   join1.key2 as key2,
                   join1.key3 as key3,
                   smallTbl2.key as key4,
                   join1.value1 as value1,
                   join1.value2 as value2
            FROM (SELECT bigTbl.key1 as key1,
                         bigTbl.key2 as key2,
                         smallTbl1.key as key3,
                         bigTbl.value as value1,
                         bigTbl.value as value2
                  FROM bigTbl
                  JOIN smallTbl1 ON (bigTbl.key1 = smallTbl1.key)) join1
            JOIN smallTbl2 ON (join1.value1 = smallTbl2.value)) join2
      JOIN smallTbl3 ON (join2.key2 = smallTbl3.key)) join3
JOIN smallTbl4 ON (join3.key3 = smallTbl4.key);

set hive.auto.convert.join.noconditionaltask.size=0;
-- Enable noconditionaltask and but set the size of hive.auto.convert.join.noconditionaltask.size
-- to 0. The plan will be the same as the one with a disabled nonconditionaltask.
EXPLAIN
SELECT SUM(HASH(join3.key1)),
       SUM(HASH(join3.key2)),
       SUM(HASH(join3.key3)),
       SUM(HASH(join3.key4)),
       SUM(HASH(join3.key5)),
       SUM(HASH(smallTbl4.key)),
       SUM(HASH(join3.value1)),
       SUM(HASH(join3.value2))
FROM (SELECT join2.key1 as key1,
             join2.key2 as key2,
             join2.key3 as key3,
             join2.key4 as key4,
             smallTbl3.key as key5,
             join2.value1 as value1,
             join2.value2 as value2
      FROM (SELECT join1.key1 as key1,
                   join1.key2 as key2,
                   join1.key3 as key3,
                   smallTbl2.key as key4,
                   join1.value1 as value1,
                   join1.value2 as value2
            FROM (SELECT bigTbl.key1 as key1,
                         bigTbl.key2 as key2,
                         smallTbl1.key as key3,
                         bigTbl.value as value1,
                         bigTbl.value as value2
                  FROM bigTbl
                  JOIN smallTbl1 ON (bigTbl.key1 = smallTbl1.key)) join1
            JOIN smallTbl2 ON (join1.value1 = smallTbl2.value)) join2
      JOIN smallTbl3 ON (join2.key2 = smallTbl3.key)) join3
JOIN smallTbl4 ON (join3.key3 = smallTbl4.key);

SELECT SUM(HASH(join3.key1)),
       SUM(HASH(join3.key2)),
       SUM(HASH(join3.key3)),
       SUM(HASH(join3.key4)),
       SUM(HASH(join3.key5)),
       SUM(HASH(smallTbl4.key)),
       SUM(HASH(join3.value1)),
       SUM(HASH(join3.value2))
FROM (SELECT join2.key1 as key1,
             join2.key2 as key2,
             join2.key3 as key3,
             join2.key4 as key4,
             smallTbl3.key as key5,
             join2.value1 as value1,
             join2.value2 as value2
      FROM (SELECT join1.key1 as key1,
                   join1.key2 as key2,
                   join1.key3 as key3,
                   smallTbl2.key as key4,
                   join1.value1 as value1,
                   join1.value2 as value2
            FROM (SELECT bigTbl.key1 as key1,
                         bigTbl.key2 as key2,
                         smallTbl1.key as key3,
                         bigTbl.value as value1,
                         bigTbl.value as value2
                  FROM bigTbl
                  JOIN smallTbl1 ON (bigTbl.key1 = smallTbl1.key)) join1
            JOIN smallTbl2 ON (join1.value1 = smallTbl2.value)) join2
      JOIN smallTbl3 ON (join2.key2 = smallTbl3.key)) join3
JOIN smallTbl4 ON (join3.key3 = smallTbl4.key);
set hive.mapred.mode=nonstrict;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter,org.apache.hadoop.hive.ql.hooks.PrintCompletedTasksHook;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=6000;

-- we will generate one MR job.
EXPLAIN
SELECT tmp.key
FROM (SELECT x1.key AS key FROM src x1 JOIN src1 y1 ON (x1.key = y1.key)
      UNION ALL
      SELECT x2.key AS key FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)) tmp
ORDER BY tmp.key;

SELECT tmp.key
FROM (SELECT x1.key AS key FROM src x1 JOIN src1 y1 ON (x1.key = y1.key)
      UNION ALL
      SELECT x2.key AS key FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)) tmp
ORDER BY tmp.key;

set hive.auto.convert.join.noconditionaltask.size=400;
-- Check if the total size of local tables will be
-- larger than the limit that
-- we set through hive.auto.convert.join.noconditionaltask.size (right now, it is
-- 400 bytes). If so, do not merge.
-- For this query, we will merge the MapJoin of x2 and y2 into the MR job
-- for UNION ALL and ORDER BY. But, the MapJoin of x1 and y2 will not be merged
-- into that MR job.
EXPLAIN
SELECT tmp.key
FROM (SELECT x1.key AS key FROM src x1 JOIN src1 y1 ON (x1.key = y1.key)
      UNION ALL
      SELECT x2.key AS key FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)) tmp
ORDER BY tmp.key;

SELECT tmp.key
FROM (SELECT x1.key AS key FROM src x1 JOIN src1 y1 ON (x1.key = y1.key)
      UNION ALL
      SELECT x2.key AS key FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)) tmp
ORDER BY tmp.key;

set hive.auto.convert.join.noconditionaltask.size=6000;
-- We will use two jobs.
-- We will generate one MR job for GROUP BY
-- on x1, one MR job for both the MapJoin of x2 and y2, the UNION ALL, and the
-- ORDER BY.
EXPLAIN
SELECT tmp.key
FROM (SELECT x1.key AS key FROM src1 x1 GROUP BY x1.key
      UNION ALL
      SELECT x2.key AS key FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)) tmp
ORDER BY tmp.key;

SELECT tmp.key
FROM (SELECT x1.key AS key FROM src1 x1 GROUP BY x1.key
      UNION ALL
      SELECT x2.key AS key FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)) tmp
ORDER BY tmp.key;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is disabled,
-- we will use 5 jobs.
-- We will generate one MR job to evaluate the sub-query tmp1,
-- one MR job to evaluate the sub-query tmp2,
-- one MR job for the Join of tmp1 and tmp2,
-- one MR job for aggregation on the result of the Join of tmp1 and tmp2,
-- and one MR job for the ORDER BY.
EXPLAIN
SELECT tmp1.key as key, count(*) as cnt
FROM (SELECT x1.key AS key
      FROM src x1 JOIN src1 y1 ON (x1.key = y1.key)
      GROUP BY x1.key) tmp1
JOIN (SELECT x2.key AS key
      FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)
      GROUP BY x2.key) tmp2
ON (tmp1.key = tmp2.key)
GROUP BY tmp1.key
ORDER BY key, cnt;

SELECT tmp1.key as key, count(*) as cnt
FROM (SELECT x1.key AS key
      FROM src x1 JOIN src1 y1 ON (x1.key = y1.key)
      GROUP BY x1.key) tmp1
JOIN (SELECT x2.key AS key
      FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)
      GROUP BY x2.key) tmp2
ON (tmp1.key = tmp2.key)
GROUP BY tmp1.key
ORDER BY key, cnt;

set hive.optimize.correlation=true;
-- When Correlation Optimizer is enabled,
-- we will use two jobs. This first MR job will evaluate sub-queries of tmp1, tmp2,
-- the Join of tmp1 and tmp2, and the aggregation on the result of the Join of
-- tmp1 and tmp2. The second job will do the ORDER BY.
EXPLAIN
SELECT tmp1.key as key, count(*) as cnt
FROM (SELECT x1.key AS key
      FROM src x1 JOIN src1 y1 ON (x1.key = y1.key)
      GROUP BY x1.key) tmp1
JOIN (SELECT x2.key AS key
      FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)
      GROUP BY x2.key) tmp2
ON (tmp1.key = tmp2.key)
GROUP BY tmp1.key
ORDER BY key, cnt;

SELECT tmp1.key as key, count(*) as cnt
FROM (SELECT x1.key AS key
      FROM src x1 JOIN src1 y1 ON (x1.key = y1.key)
      GROUP BY x1.key) tmp1
JOIN (SELECT x2.key AS key
      FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)
      GROUP BY x2.key) tmp2
ON (tmp1.key = tmp2.key)
GROUP BY tmp1.key
ORDER BY key, cnt;

set hive.optimize.correlation=false;
-- When Correlation Optimizer is disabled,
-- we will use five jobs.
-- We will generate one MR job to evaluate the sub-query tmp1,
-- one MR job to evaluate the sub-query tmp2,
-- one MR job for the Join of tmp1 and tmp2,
-- one MR job for aggregation on the result of the Join of tmp1 and tmp2,
-- and one MR job for the ORDER BY.
EXPLAIN
SELECT tmp1.key as key, count(*) as cnt
FROM (SELECT x1.key AS key
      FROM src1 x1
      GROUP BY x1.key) tmp1
JOIN (SELECT x2.key AS key
      FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)
      GROUP BY x2.key) tmp2
ON (tmp1.key = tmp2.key)
GROUP BY tmp1.key
ORDER BY key, cnt;

SELECT tmp1.key as key, count(*) as cnt
FROM (SELECT x1.key AS key
      FROM src1 x1
      GROUP BY x1.key) tmp1
JOIN (SELECT x2.key AS key
      FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)
      GROUP BY x2.key) tmp2
ON (tmp1.key = tmp2.key)
GROUP BY tmp1.key
ORDER BY key, cnt;

set hive.optimize.correlation=true;
-- When Correlation Optimizer is enabled,
-- we will use two job. This first MR job will evaluate sub-queries of tmp1, tmp2,
-- the Join of tmp1 and tmp2, and the aggregation on the result of the Join of
-- tmp1 and tmp2. The second job will do the ORDER BY.
EXPLAIN
SELECT tmp1.key as key, count(*) as cnt
FROM (SELECT x1.key AS key
      FROM src1 x1
      GROUP BY x1.key) tmp1
JOIN (SELECT x2.key AS key
      FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)
      GROUP BY x2.key) tmp2
ON (tmp1.key = tmp2.key)
GROUP BY tmp1.key
ORDER BY key, cnt;

SELECT tmp1.key as key, count(*) as cnt
FROM (SELECT x1.key AS key
      FROM src1 x1
      GROUP BY x1.key) tmp1
JOIN (SELECT x2.key AS key
      FROM src x2 JOIN src1 y2 ON (x2.key = y2.key)
      GROUP BY x2.key) tmp2
ON (tmp1.key = tmp2.key)
GROUP BY tmp1.key
ORDER BY key, cnt;

-- Check if we can correctly handle partitioned table.
CREATE TABLE part_table(key string, value string) PARTITIONED BY (partitionId int);
INSERT OVERWRITE TABLE part_table PARTITION (partitionId=1)
  SELECT key, value FROM src ORDER BY key, value LIMIT 100;
INSERT OVERWRITE TABLE part_table PARTITION (partitionId=2)
  SELECT key, value FROM src1 ORDER BY key, value;

EXPLAIN
SELECT count(*)
FROM part_table x JOIN src1 y ON (x.key = y.key);

SELECT count(*)
FROM part_table x JOIN src1 y ON (x.key = y.key);

set hive.auto.convert.join.noconditionaltask.size=10000000;
set hive.optimize.correlation=false;
-- HIVE-5891 Alias conflict when merging multiple mapjoin tasks into their common
-- child mapred task
EXPLAIN
SELECT * FROM (
  SELECT c.key FROM
    (SELECT a.key FROM src a JOIN src b ON a.key=b.key GROUP BY a.key) tmp
    JOIN src c ON tmp.key=c.key
  UNION ALL
  SELECT c.key FROM
    (SELECT a.key FROM src a JOIN src b ON a.key=b.key GROUP BY a.key) tmp
    JOIN src c ON tmp.key=c.key
) x;

SELECT * FROM (
  SELECT c.key FROM
    (SELECT a.key FROM src a JOIN src b ON a.key=b.key GROUP BY a.key) tmp
    JOIN src c ON tmp.key=c.key
  UNION ALL
  SELECT c.key FROM
    (SELECT a.key FROM src a JOIN src b ON a.key=b.key GROUP BY a.key) tmp
    JOIN src c ON tmp.key=c.key
) x;

set hive.mapred.mode=nonstrict;
drop table emps;

create table emps (empno int, deptno int, empname string);

insert into table emps values (1,2,"11"),(1,2,"11"),(3,4,"33"),(1,3,"11"),(2,5,"22"),(2,5,"22");

select * from emps;

select * from emps where (int(empno+deptno/2), int(deptno/3)) in ((2,0),(3,2));

select * from emps where (int(empno+deptno/2), int(deptno/3)) not in ((2,0),(3,2));

select * from emps where (empno,deptno) in ((1,2),(3,2));

select * from emps where (empno,deptno) not in ((1,2),(3,2));

select * from emps where (empno,deptno) in ((1,2),(1,3));

select * from emps where (empno,deptno) not in ((1,2),(1,3));

explain
select * from emps where (empno+1,deptno) in ((1,2),(3,2));

explain
select * from emps where (empno+1,deptno) not in ((1,2),(3,2));

select * from emps where empno in (1,2);

select * from emps where empno in (1,2) and deptno > 2;

select * from emps where (empno) in (1,2) and deptno > 2;

select * from emps where ((empno) in (1,2) and deptno > 2);

explain select * from emps where ((empno*2)|1,deptno) in ((empno+1,2),(empno+2,2));

select * from emps where ((empno*2)|1,deptno) in ((empno+1,2),(empno+2,2));

select (empno*2)|1,substr(empname,1,1) from emps;

select * from emps where ((empno*2)|1,substr(empname,1,1)) in ((empno+1,'2'),(empno+2,'2'));

select * from emps where ((empno*2)|1,substr(empname,1,1)) not in ((empno+1,'2'),(empno+2,'2'));

select * from emps where ((empno*2)|1,substr(empname,1,1)) in ((empno+1,'2'),(empno+3,'2'));

select * from emps where ((empno*2)|1,substr(empname,1,1)) not in ((empno+1,'2'),(empno+3,'2'));


select sum(empno), empname from emps where ((empno*2)|1,substr(empname,1,1)) in ((empno+1,'2'),(empno+3,'2'))
group by empname;

select * from emps where ((empno*2)|1,substr(empname,1,1)) in ((empno+1,'2'),(empno+3,'2'))
union
select * from emps where (empno,deptno) in ((1,2),(3,2));

drop view v;

create view v as
select * from(
select * from emps where ((empno*2)|1,substr(empname,1,1)) in ((empno+1,'2'),(empno+3,'2'))
union
select * from emps where (empno,deptno) in ((1,2),(3,2)))subq order by empno desc;

select * from v;

select subq.e1 from
(select (empno*2)|1 as e1, substr(empname,1,1) as n1 from emps)subq
join
(select empno as e2 from emps where ((empno*2)|1,substr(empname,1,1)) in ((empno+1,'2'),(empno+3,'2')))subq2
on e1=e2+1;
-- SORT_QUERY_RESULTS

create table src_multi1 like src;
create table src_multi2 like src;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.stats.dbclass=fs;
explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;


set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;



set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

select * from src_multi1;
select * from src_multi2;


set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

select * from src_multi1;
select * from src_multi2;


set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

select * from src_multi1;
select * from src_multi2;




set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

explain
from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;

explain
from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;



set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/hive_test/multiins_local/temp;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;


explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;
-- SORT_QUERY_RESULTS

--HIVE-3699 Multiple insert overwrite into multiple tables query stores same results in all tables
create table e1 (key string, count int);
create table e2 (key string, count int);

explain FROM src
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(*) WHERE key>450 GROUP BY key
INSERT OVERWRITE TABLE e2
    SELECT key, COUNT(*) WHERE key>500 GROUP BY key;

FROM src
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(*) WHERE key>450 GROUP BY key
INSERT OVERWRITE TABLE e2
    SELECT key, COUNT(*) WHERE key>500 GROUP BY key;

select * from e1;
select * from e2;

explain FROM src
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(*) WHERE key>450 GROUP BY key
INSERT OVERWRITE TABLE e2
    SELECT key, COUNT(*) GROUP BY key;

FROM src
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(*) WHERE key>450 GROUP BY key
INSERT OVERWRITE TABLE e2
    SELECT key, COUNT(*) GROUP BY key;

select * from e1;
select * from e2;
set hive.mapred.mode=nonstrict;
--HIVE-3699 Multiple insert overwrite into multiple tables query stores same results in all tables
create table e1 (count int);
create table e2 (percentile double);
set hive.stats.dbclass=fs;
explain
FROM (select key, cast(key as double) as value from src order by key) a
INSERT OVERWRITE TABLE e1
    SELECT COUNT(*)
INSERT OVERWRITE TABLE e2
    SELECT percentile_approx(value, 0.5);

FROM (select key, cast(key as double) as value from src order by key) a
INSERT OVERWRITE TABLE e1
    SELECT COUNT(*)
INSERT OVERWRITE TABLE e2
    SELECT percentile_approx(value, 0.5);

select * from e1;
select * from e2;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
create table e1 (key string, keyD double);
create table e2 (key string, keyD double, value string);
create table e3 (key string, keyD double);
set hive.stats.dbclass=fs;
explain
FROM (select key, cast(key as double) as keyD, value from src order by key) a
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(distinct value) group by key
INSERT OVERWRITE TABLE e2
    SELECT key, sum(keyD), value group by key, value;

explain
FROM (select key, cast(key as double) as keyD, value from src order by key) a
INSERT OVERWRITE TABLE e2
    SELECT key, sum(keyD), value group by key, value
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(distinct value) group by key;

FROM (select key, cast(key as double) as keyD, value from src order by key) a
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(distinct value) group by key
INSERT OVERWRITE TABLE e2
    SELECT key, sum(keyD), value group by key, value;

select * from e1;
select * from e2;

FROM (select key, cast(key as double) as keyD, value from src order by key) a
INSERT OVERWRITE TABLE e2
    SELECT key, sum(keyD), value group by key, value
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(distinct value) group by key;

select * from e1;
select * from e2;

explain
from src
insert overwrite table e1
select key, count(distinct value) group by key
insert overwrite table e3
select value, count(distinct key) group by value;


explain
FROM (select key, cast(key as double) as keyD, value from src order by key) a
INSERT OVERWRITE TABLE e1
    SELECT key, COUNT(distinct value) group by key
INSERT OVERWRITE TABLE e2
    SELECT key, sum(keyD), value group by key, value
INSERT overwrite table e3
    SELECT key, COUNT(distinct keyD) group by key, keyD, value;
set hive.stats.dbclass=fs;
-- SORT_QUERY_RESULTS

create table src_10 as select * from src limit 10;

create table src_lv1 (key string, value string);
create table src_lv2 (key string, value string);
create table src_lv3 (key string, value string);

-- 2LV
-- TS[0]-LVF[1]-SEL[2]-LVJ[5]-SEL[11]-FS[12]
--             -SEL[3]-UDTF[4]-LVJ[5]
--      -LVF[6]-SEL[7]-LVJ[10]-SEL[13]-FS[14]
--             -SEL[8]-UDTF[9]-LVJ[10]
explain
from src_10
insert overwrite table src_lv1 select key, C lateral view explode(array(key+1, key+2)) A as C
insert overwrite table src_lv2 select key, C lateral view explode(array(key+3, key+4)) A as C;

from src_10
insert overwrite table src_lv1 select key, C lateral view explode(array(key+1, key+2)) A as C
insert overwrite table src_lv2 select key, C lateral view explode(array(key+3, key+4)) A as C;

select * from src_lv1;
select * from src_lv2;

-- 2(LV+GBY)
-- TS[0]-LVF[1]-SEL[2]-LVJ[5]-SEL[11]-GBY[12]-RS[13]-GBY[14]-SEL[15]-FS[16]
--             -SEL[3]-UDTF[4]-LVJ[5]
--      -LVF[6]-SEL[7]-LVJ[10]-SEL[17]-GBY[18]-RS[19]-GBY[20]-SEL[21]-FS[22]
--             -SEL[8]-UDTF[9]-LVJ[10]
explain
from src_10
insert overwrite table src_lv1 select key, sum(C) lateral view explode(array(key+1, key+2)) A as C group by key
insert overwrite table src_lv2 select key, sum(C) lateral view explode(array(key+3, key+4)) A as C group by key;

from src_10
insert overwrite table src_lv1 select key, sum(C) lateral view explode(array(key+1, key+2)) A as C group by key
insert overwrite table src_lv2 select key, sum(C) lateral view explode(array(key+3, key+4)) A as C group by key;

select * from src_lv1;
select * from src_lv2;

-- (LV+GBY) + RS:2GBY
-- TS[0]-LVF[1]-SEL[2]-LVJ[5]-SEL[6]-GBY[7]-RS[8]-GBY[9]-SEL[10]-FS[11]
--             -SEL[3]-UDTF[4]-LVJ[5]
--      -FIL[12]-SEL[13]-RS[14]-FOR[15]-FIL[16]-GBY[17]-SEL[18]-FS[19]
--                                     -FIL[20]-GBY[21]-SEL[22]-FS[23]
explain
from src_10
insert overwrite table src_lv1 select key, sum(C) lateral view explode(array(key+1, key+2)) A as C group by key
insert overwrite table src_lv2 select key, count(value) where key > 200 group by key
insert overwrite table src_lv3 select key, count(value) where key < 200 group by key;

from src_10
insert overwrite table src_lv1 select key, sum(C) lateral view explode(array(key+1, key+2)) A as C group by key
insert overwrite table src_lv2 select key, count(value) where key > 200 group by key
insert overwrite table src_lv3 select key, count(value) where key < 200 group by key;

select * from src_lv1;
select * from src_lv2;
select * from src_lv3;

-- todo: shared distinct columns (should work with hive.optimize.multigroupby.common.distincts)
-- 2(LV+GBY) + RS:2GBY
-- TS[0]-LVF[1]-SEL[2]-LVJ[5]-SEL[11]-GBY[12]-RS[13]-GBY[14]-SEL[15]-FS[16]
--             -SEL[3]-UDTF[4]-LVJ[5]
--      -LVF[6]-SEL[7]-LVJ[10]-SEL[17]-GBY[18]-RS[19]-GBY[20]-SEL[21]-FS[22]
--             -SEL[8]-UDTF[9]-LVJ[10]
--      -SEL[23]-GBY[24]-RS[25]-GBY[26]-SEL[27]-FS[28]
explain
from src_10
insert overwrite table src_lv1 select C, sum(distinct key) lateral view explode(array(key+1, key+2)) A as C group by C
insert overwrite table src_lv2 select C, sum(distinct key) lateral view explode(array(key+3, key+4)) A as C group by C
insert overwrite table src_lv3 select value, sum(distinct key) group by value;

from src_10
insert overwrite table src_lv1 select C, sum(distinct key) lateral view explode(array(key+1, key+2)) A as C group by C
insert overwrite table src_lv2 select C, sum(distinct key) lateral view explode(array(key+3, key+4)) A as C group by C
insert overwrite table src_lv3 select value, sum(distinct key) group by value;

select * from src_lv1;
select * from src_lv2;
select * from src_lv3;

create table src_lv4 (key string, value string);

-- Common distincts optimization works across non-lateral view queries, but not across lateral view multi inserts
explain
from src_10
insert overwrite table src_lv1 select key, sum(distinct C) lateral view explode(array(key+1, key+2)) A as C group by key
insert overwrite table src_lv2 select key, sum(distinct C) lateral view explode(array(key+3, key+4)) A as C group by key
insert overwrite table src_lv3 select value, sum(distinct key) where key > 200 group by value
insert overwrite table src_lv4 select value, sum(distinct key) where key < 200 group by value;

from src_10
insert overwrite table src_lv1 select key, sum(distinct C) lateral view explode(array(key+1, key+2)) A as C group by key
insert overwrite table src_lv2 select key, sum(distinct C) lateral view explode(array(key+3, key+4)) A as C group by key
insert overwrite table src_lv3 select value, sum(distinct key) where key > 200 group by value
insert overwrite table src_lv4 select value, sum(distinct key) where key < 200 group by value;

select * from src_lv1;
select * from src_lv2;
select * from src_lv3;
select * from src_lv4;
set hive.mapred.mode=nonstrict;
create table src_multi1 like src;
create table src_multi2 like src;
create table src_multi3 like src;
set hive.stats.dbclass=fs;
-- Testing the case where a map work contains both shuffling (ReduceSinkOperator)
-- and inserting to output table (FileSinkOperator).

explain
from src
insert overwrite table src_multi1 select key, count(1) group by key order by key
insert overwrite table src_multi2 select value, count(1) group by value order by value
insert overwrite table src_multi3 select * where key < 10;

from src
insert overwrite table src_multi1 select key, count(1) group by key order by key
insert overwrite table src_multi2 select value, count(1) group by value order by value
insert overwrite table src_multi3 select * where key < 10;

select * from src_multi1;
select * from src_multi2;
select * from src_multi3;
set hive.multi.insert.move.tasks.share.dependencies=true;
set hive.stats.dbclass=fs;
-- SORT_QUERY_RESULTS

create table src_multi1 like src;
create table src_multi2 like src;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;


set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;



set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

select * from src_multi1;
select * from src_multi2;


set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

select * from src_multi1;
select * from src_multi2;


set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

from src
insert overwrite table src_multi1 select * where key < 10 group by key, value
insert overwrite table src_multi2 select * where key > 10 and key < 20 group by key, value;

select * from src_multi1;
select * from src_multi2;




set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

explain
from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;

explain
from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

from (select * from src  union all select * from src) s
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20;

select * from src_multi1;
select * from src_multi2;



set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/hive_test/multiins_local/temp;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;


explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/0' select * where key = 0
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key = 2
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/4' select * where key = 4;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

from src
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

select * from src_multi1;
select * from src_multi2;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

select * from src_multi1;
select * from src_multi2;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=false;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

select * from src_multi1;
select * from src_multi2;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

explain
from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

from src
insert overwrite table src_multi1 select * where key < 10
insert overwrite table src_multi2 select * where key > 10 and key < 20
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/1' select * where key < 10 group by key, value cluster by key
insert overwrite local directory '${system:test.tmp.dir}/hive_test/multiins_local/2' select * where key > 10 and key < 20 group by key, value cluster by value;

select * from src_multi1;
select * from src_multi2;

dfs -ls ${system:test.tmp.dir}/hive_test/multiins_local;
dfs -rmr ${system:test.tmp.dir}/hive_test/multiins_local;
set hive.mapred.mode=nonstrict;
drop table if exists src2;
drop table if exists src_multi1;
drop table if exists src_multi1;
set hive.stats.dbclass=fs;
CREATE TABLE src2 as SELECT * FROM src;

create table src_multi1 like src;
create table src_multi2 like src;

explain
from (select * from src1 where key < 10 union all select * from src2 where key > 100) s
insert overwrite table src_multi1 select key, value where key < 150 order by key
insert overwrite table src_multi2 select key, value where key > 400 order by value;

from (select * from src1 where key < 10 union all select * from src2 where key > 100) s
insert overwrite table src_multi1 select key, value where key < 150 order by key
insert overwrite table src_multi2 select key, value where key > 400 order by value;

select * from src_multi1;
select * from src_multi2;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;

-- SORT_QUERY_RESULTS

CREATE TABLE src11 as SELECT * FROM src;
CREATE TABLE src12 as SELECT * FROM src;
CREATE TABLE src13 as SELECT * FROM src;
CREATE TABLE src14 as SELECT * FROM src;


EXPLAIN SELECT * FROM
src11 a JOIN
src12 b ON (a.key = b.key) JOIN
(SELECT * FROM (SELECT * FROM src13 UNION ALL SELECT * FROM src14)a )c ON c.value = b.value;
set hive.semantic.analyzer.hook=org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1;

drop table tbl_sahook;
create table tbl_sahook (c string);
desc extended tbl_sahook;
drop table tbl_sahook;

set hive.semantic.analyzer.hook=org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1,org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook;

drop table tbl_sahooks;
create table tbl_sahooks (c string);
desc extended tbl_sahooks;
drop table tbl_sahooks;

set hive.semantic.analyzer.hook=org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook,org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1;

drop table tbl_sahooks;
create table tbl_sahooks (c string);
desc extended tbl_sahooks;
drop table tbl_sahooks;

set hive.semantic.analyzer.hook=org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1,org.apache.hadoop.hive.ql.metadata.DummySemanticAnalyzerHook1;

drop table tbl_sahooks;
create table tbl_sahooks (c string);
desc extended tbl_sahooks;

set hive.semantic.analyzer.hook=;
drop table tbl_sahooks;

CREATE TABLE pokes (foo INT, bar STRING);
create table pokes2(foo INT, bar STRING);

create table jssarma_nilzma_bad as select a.val, a.filename, a.offset from (select hash(foo) as val, INPUT__FILE__NAME as filename, BLOCK__OFFSET__INSIDE__FILE as  offset from pokes) a join pokes2 b on (a.val = b.foo);

drop table jssarma_nilzma_bad;

drop table pokes;
drop table pokes2;
CREATE TABLE pokes (foo INT, bar STRING);
create table pokes2(foo INT, bar STRING);

create table jssarma_nilzma_bad as select a.val, a.filename, a.offset from (select hash(foo) as val, INPUT__FILE__NAME as filename, BLOCK__OFFSET__INSIDE__FILE as  offset from pokes) a join pokes2 b on (a.val = b.foo);

drop table jssarma_nilzma_bad;

drop table pokes;
drop table pokes2;
CREATE TABLE pokes (foo INT, bar STRING);
create table pokes2(foo INT, bar STRING);

create table jssarma_nilzma_bad as select a.val, a.filename, a.offset from (select hash(foo) as val, INPUT__FILE__NAME as filename, BLOCK__OFFSET__INSIDE__FILE as  offset from pokes) a join pokes2 b on (a.val = b.foo);

drop table jssarma_nilzma_bad;

drop table pokes;
drop table pokes2;

create table nestedcomplex (
simple_int int,
max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>,
max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>,
max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:bigint>>>>>>>>>>>>>>>>>>>>>>>,
simple_string string)
ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
   'hive.serialization.extend.nesting.levels'='true',
   'line.delim'='\n'
)
;

describe nestedcomplex;
describe extended nestedcomplex;


load data local inpath '../../data/files/nested_complex.txt' overwrite into table nestedcomplex;

select * from nestedcomplex sort by simple_int;
create table nestedcomplex_additional (
simple_int int,
max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>>>,
max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>>>>>,
max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:int>>>>>>>>>>>>>>>>>>>>>>>>>>,
simple_string string)
ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
   'hive.serialization.extend.additional.nesting.levels'='true',
   'line.delim'='\n'
)
;

describe nestedcomplex_additional;
describe extended nestedcomplex_additional;


load data local inpath '../../data/files/nestedcomplex_additional.txt' overwrite into table nestedcomplex_additional;

select * from nestedcomplex_additional sort by simple_int;

create table nestedcomplex (
simple_int int,
max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>,
max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>,
max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:bigint>>>>>>>>>>>>>>>>>>>>>>>,
simple_string string)

;


-- This should fail in as extended nesting levels are not enabled using the serdeproperty hive.serialization.extend.nesting.levels
load data local inpath '../../data/files/nested_complex.txt' overwrite into table nestedcomplex;

select * from nestedcomplex sort by simple_int;
set hive.mapred.mode=nonstrict;
add file ../../data/scripts/newline.py;
set hive.transform.escape.input=true;

-- SORT_QUERY_RESULTS

create table tmp_tmp(key string, value string) stored as rcfile;
insert overwrite table tmp_tmp
SELECT TRANSFORM(key, value) USING
'python newline.py' AS key, value FROM src limit 6;

select * from tmp_tmp;

drop table tmp_tmp;

add file ../../data/scripts/escapednewline.py;
add file ../../data/scripts/escapedtab.py;
add file ../../data/scripts/doubleescapedtab.py;
add file ../../data/scripts/escapedcarriagereturn.py;

create table tmp_tmp(key string, value string) stored as rcfile;
insert overwrite table tmp_tmp
SELECT TRANSFORM(key, value) USING
'python escapednewline.py' AS key, value FROM src limit 5;

select * from tmp_tmp;

SELECT TRANSFORM(key, value) USING
'cat' AS (key, value) FROM tmp_tmp;

insert overwrite table tmp_tmp
SELECT TRANSFORM(key, value) USING
'python escapedcarriagereturn.py' AS key, value FROM src limit 5;

select * from tmp_tmp;

SELECT TRANSFORM(key, value) USING
'cat' AS (key, value) FROM tmp_tmp;

insert overwrite table tmp_tmp
SELECT TRANSFORM(key, value) USING
'python escapedtab.py' AS key, value FROM src limit 5;

select * from tmp_tmp;

SELECT TRANSFORM(key, value) USING
'cat' AS (key, value) FROM tmp_tmp;

insert overwrite table tmp_tmp
SELECT TRANSFORM(key, value) USING
'python doubleescapedtab.py' AS key, value FROM src limit 5;

select * from tmp_tmp;

SELECT TRANSFORM(key, value) USING
'cat' AS (key, value) FROM tmp_tmp;

SELECT key FROM (SELECT TRANSFORM ('a\tb', 'c') USING 'cat' AS (key, value) FROM src limit 1)a ORDER BY key ASC;

SELECT value FROM (SELECT TRANSFORM ('a\tb', 'c') USING 'cat' AS (key, value) FROM src limit 1)a ORDER BY value ASC;
set hive.mapred.mode=nonstrict;
EXPLAIN
SELECT c1 FROM (select value as c1, key as c2 from src) x where c2 < 100;

SELECT c1 FROM (select value as c1, key as c2 from src) x where c2 < 100;

-- was negative/ambiguous_table_col.q

drop table ambiguous;
create table ambiguous (key string, value string);

FROM src key
INSERT OVERWRITE TABLE ambiguous SELECT key.key, key.value WHERE key.value < 'val_100';

drop table ambiguous;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

-- negative, references twice for result of funcion
explain select nkey, nkey + 1 from (select key + 1 as nkey, value from src) a;

set hive.auto.convert.join=false;
-- This test query is introduced for HIVE-4968.
-- First, we do not convert the join to MapJoin.
EXPLAIN
SELECT tmp4.key as key, tmp4.value as value, tmp4.count as count
FROM (SELECT tmp2.key as key, tmp2.value as value, tmp3.count as count
      FROM (SELECT *
            FROM (SELECT key, value
                  FROM src1) tmp1 ) tmp2
      JOIN (SELECT count(*) as count
            FROM src1) tmp3
      ) tmp4;

SELECT tmp4.key as key, tmp4.value as value, tmp4.count as count
FROM (SELECT tmp2.key as key, tmp2.value as value, tmp3.count as count
      FROM (SELECT *
            FROM (SELECT key, value
                  FROM src1) tmp1 ) tmp2
      JOIN (SELECT count(*) as count
            FROM src1) tmp3
      ) tmp4;

set hive.auto.convert.join=true;
-- Then, we convert the join to MapJoin.
EXPLAIN
SELECT tmp4.key as key, tmp4.value as value, tmp4.count as count
FROM (SELECT tmp2.key as key, tmp2.value as value, tmp3.count as count
      FROM (SELECT *
            FROM (SELECT key, value
                  FROM src1) tmp1 ) tmp2
      JOIN (SELECT count(*) as count
            FROM src1) tmp3
      ) tmp4;

SELECT tmp4.key as key, tmp4.value as value, tmp4.count as count
FROM (SELECT tmp2.key as key, tmp2.value as value, tmp3.count as count
      FROM (SELECT *
            FROM (SELECT key, value
                  FROM src1) tmp1 ) tmp2
      JOIN (SELECT count(*) as count
            FROM src1) tmp3
      ) tmp4;
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.value WHERE src.key < 100 group by src.key
EXPLAIN SELECT key, count(1) FROM src;
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=minimal;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;

-- backward compatible (minimal)
explain select * from src limit 10;
select * from src limit 10;

explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10;
select * from srcpart where ds='2008-04-08' AND hr='11' limit 10;

explain select key from src limit 10;
select key from src limit 10;

-- negative, filter on non-partition column
explain select * from srcpart where key > 100 limit 10;
select * from srcpart where key > 100 limit 10;

-- negative, table sampling
explain select * from src TABLESAMPLE (0.25 PERCENT) limit 10;
select * from src TABLESAMPLE (0.25 PERCENT) limit 10;

set hive.fetch.task.conversion=more;

-- backward compatible (more)
explain select * from src limit 10;
select * from src limit 10;

explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10;
select * from srcpart where ds='2008-04-08' AND hr='11' limit 10;

-- select expression
explain select cast(key as int) * 10, upper(value) from src limit 10;
select cast(key as int) * 10, upper(value) from src limit 10;

-- filter on non-partition column
explain select key from src where key < 100 limit 10;
select key from src where key < 100 limit 10;

-- select expr for partitioned table
explain select key from srcpart where ds='2008-04-08' AND hr='11' limit 10;
select key from srcpart where ds='2008-04-08' AND hr='11' limit 10;

-- virtual columns
explain select *, BLOCK__OFFSET__INSIDE__FILE from src where key < 10 limit 10;
select *, BLOCK__OFFSET__INSIDE__FILE from src where key < 100 limit 10;

-- virtual columns on partitioned table
explain select *, BLOCK__OFFSET__INSIDE__FILE from srcpart where key < 10 limit 30;
select *, BLOCK__OFFSET__INSIDE__FILE from srcpart where key < 10 limit 30;

-- bucket sampling
explain select *, BLOCK__OFFSET__INSIDE__FILE from src TABLESAMPLE (BUCKET 1 OUT OF 40 ON key);
select *, BLOCK__OFFSET__INSIDE__FILE from src TABLESAMPLE (BUCKET 1 OUT OF 40 ON key);
explain select *, BLOCK__OFFSET__INSIDE__FILE from srcpart TABLESAMPLE (BUCKET 1 OUT OF 40 ON key);
select *, BLOCK__OFFSET__INSIDE__FILE from srcpart TABLESAMPLE (BUCKET 1 OUT OF 40 ON key);

-- split sampling
explain select * from src TABLESAMPLE (0.25 PERCENT);
select * from src TABLESAMPLE (0.25 PERCENT);
explain select *, BLOCK__OFFSET__INSIDE__FILE from srcpart TABLESAMPLE (0.25 PERCENT);
select *, BLOCK__OFFSET__INSIDE__FILE from srcpart TABLESAMPLE (0.25 PERCENT);

-- sub query
explain
select key, value from (select value key,key value from src where key > 200) a where value < 250 limit 20;
select key, value from (select value key,key value from src where key > 200) a where value < 250 limit 20;

-- lateral view
explain
select key,X from srcpart lateral view explode(array(key,value)) L as x where (ds='2008-04-08' AND hr='11') limit 20;
select key,X from srcpart lateral view explode(array(key,value)) L as x where (ds='2008-04-08' AND hr='11') limit 20;

-- non deterministic func
explain select key, value, BLOCK__OFFSET__INSIDE__FILE from srcpart where ds="2008-04-09" AND rand() > 1;
select key, value, BLOCK__OFFSET__INSIDE__FILE from srcpart where ds="2008-04-09" AND rand() > 1;

-- negative, groupby
explain select key, count(value) from src group by key;

-- negative, distinct
explain select distinct key, value from src;

-- negative, CTAS
explain create table srcx as select distinct key, value from src;

-- negative, analyze
explain analyze table src compute statistics;

-- negative, join
explain select * from src join src src2 on src.key=src2.key;
set hive.fetch.task.conversion=more;
set hive.mapred.mode=nonstrict;
explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10;
explain select cast(key as int) * 10, upper(value) from src limit 10;

set hive.fetch.task.conversion.threshold=10000;

explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10;
explain select cast(key as int) * 10, upper(value) from src limit 10;
-- Scans without limit (should be Fetch task now)
explain select concat(key, value)  from src;

set hive.fetch.task.conversion.threshold=100;

-- from HIVE-7397, limit + partition pruning filter
explain select * from srcpart where ds='2008-04-08' AND hr='11' limit 10;
explain select cast(key as int) * 10, upper(value) from src limit 10;
-- Scans without limit (should not be Fetch task now)
explain select concat(key, value)  from src;
-- Simple Scans without limit (will be  Fetch task now)
explain select key, value  from src;
explain select key  from src;
explain select *    from src;
explain select key,1 from src;
explain select cast(key as char(20)),1 from src;
CREATE TABLE `table`(`string` string) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/docurl.txt' INTO TABLE `table`;

SELECT `table`, count(1)
FROM
(
  FROM `table`
  SELECT TRANSFORM (`table`.`string`)
  USING 'java -cp ../util/target/classes/ org.apache.hadoop.hive.scripts.extracturl' AS (`table`, count)
) subq
GROUP BY `table`;
DROP TABLE `insert`;

CREATE TABLE `insert` (key INT, `as` STRING);

EXPLAIN INSERT INTO TABLE `insert` SELECT * FROM src LIMIT 100;
INSERT INTO TABLE `insert` SELECT * FROM src LIMIT 100;
SELECT SUM(HASH(hash)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (hash) FROM `insert`
) t;

EXPLAIN INSERT INTO TABLE `insert` SELECT * FROM src LIMIT 100;
INSERT INTO TABLE `insert` SELECT * FROM src LIMIT 100;
SELECT SUM(HASH(sum)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (sum) FROM `insert`
) t;

SELECT COUNT(*) FROM `insert`;

EXPLAIN INSERT OVERWRITE TABLE `insert` SELECT * FROM src LIMIT 10;
INSERT OVERWRITE TABLE `insert` SELECT * FROM src LIMIT 10;
SELECT SUM(HASH(add)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (add) FROM `insert`
) t;


DROP TABLE `insert`;
select concat("Абвгде", "谢谢") from src limit 1;
create table non_ascii_literal2 as
select "谢谢" as col1, "Абвгде" as col2;

select * from non_ascii_literal2
where col2 = "Абвгде";

create temporary function mylastval as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue';

select  p_mfgr,p_name, p_size,
sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2,
first_value(p_size) over w1  as f,
last_value(p_size, false) over w1  as l,
mylastval(p_size, false) over w1  as m
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);


CREATE TABLE nopart_insert(a STRING, b STRING) PARTITIONED BY (ds STRING);

INSERT OVERWRITE TABLE nopart_insert
SELECT TRANSFORM(src.key, src.value) USING '../../data/scripts/error_script' AS (tkey, tvalue)
FROM src;


CREATE TABLE nopart_load(a STRING, b STRING) PARTITIONED BY (ds STRING);

load data local inpath '../../data/files/kv1.txt' overwrite into table nopart_load ;

set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(dummy STRING, key INT, value DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', key, count(1) WHERE src.key < 100 group by key;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', key, count(1) WHERE src.key < 100 group by key;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(dummy STRING, key INT, value DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.key, count(1) WHERE key < 100 group by src.key;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.key, count(1) WHERE key < 100 group by src.key;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(c string, key INT, value DOUBLE) STORED AS TEXTFILE;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.key, sum(src.value) WHERE src.key < 100 group by key;EXPLAIN
SELECT key from src JOIN src1 on src1.key=src.key;

SELECT key from src JOIN src1 on src1.key=src.key;
set hive.mapred.mode=nonstrict;
set hive.exec.pre.hooks=;

EXPLAIN
SELECT *  FROM src src1 JOIN src src2 WHERE src1.key < 10 and src2.key < 10 SORT BY src1.key, src1.value, src2.key, src2.value;

SELECT *  FROM src src1 JOIN src src2 WHERE src1.key < 10 and src2.key < 10 SORT BY src1.key, src1.value, src2.key, src2.value;
SELECT percentile(3.5, 0.99) FROM src;
-- base table with null data
DROP TABLE IF EXISTS base_tab;
CREATE TABLE base_tab(a STRING, b STRING, c STRING, d STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/null.txt' INTO TABLE base_tab;
DESCRIBE EXTENDED base_tab;

-- table with non-default null format
DROP TABLE IF EXISTS null_tab1;
EXPLAIN CREATE TABLE null_tab1(a STRING, b STRING) ROW FORMAT DELIMITED NULL DEFINED AS 'fooNull';
CREATE TABLE null_tab1(a STRING, b STRING) ROW FORMAT DELIMITED NULL DEFINED AS 'fooNull';
DESCRIBE EXTENDED null_tab1;
SHOW CREATE TABLE null_tab1;

-- load null data from another table and verify that the null is stored in the expected format
INSERT OVERWRITE TABLE null_tab1 SELECT a,b FROM base_tab;
dfs -cat ${system:test.warehouse.dir}/null_tab1/*;
SELECT * FROM null_tab1;
-- alter the null format and verify that the old null format is no longer in effect
ALTER TABLE null_tab1 SET SERDEPROPERTIES ( 'serialization.null.format'='foo');
SELECT * FROM null_tab1;


DROP TABLE null_tab1;
DROP TABLE base_tab;
-- base table with null data
DROP TABLE IF EXISTS base_tab;
CREATE TABLE base_tab(a STRING, b STRING, c STRING, d STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/null.txt' INTO TABLE base_tab;
DESCRIBE EXTENDED base_tab;

-- table with non-default null format
DROP TABLE IF EXISTS null_tab3;
EXPLAIN CREATE TABLE null_tab3 ROW FORMAT DELIMITED NULL DEFINED AS 'fooNull'
   AS SELECT a, b FROM base_tab;
CREATE TABLE null_tab3 ROW FORMAT DELIMITED NULL DEFINED AS 'fooNull'
   AS SELECT a, b FROM base_tab;
DESCRIBE EXTENDED null_tab3;
SHOW CREATE TABLE null_tab3;

dfs -cat ${system:test.warehouse.dir}/null_tab3/*;
SELECT * FROM null_tab3;
-- alter the null format and verify that the old null format is no longer in effect
ALTER TABLE null_tab3 SET SERDEPROPERTIES ( 'serialization.null.format'='foo');
SELECT * FROM null_tab3;


DROP TABLE null_tab3;
DROP TABLE base_tab;
-- base table with null data
DROP TABLE IF EXISTS base_tab;
CREATE TABLE base_tab(a STRING, b STRING, c STRING, d STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/null.txt' INTO TABLE base_tab;
DESCRIBE EXTENDED base_tab;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/hive_test/nullformat/tmp;
dfs -rmr ${system:test.tmp.dir}/hive_test/nullformat/*;
INSERT OVERWRITE LOCAL DIRECTORY '${system:test.tmp.dir}/hive_test/nullformat'
   ROW FORMAT DELIMITED NULL DEFINED AS 'fooNull' SELECT a,b FROM base_tab;
dfs -cat ${system:test.tmp.dir}/hive_test/nullformat/000000_0;

-- load the exported data back into a table with same null format and verify null values
DROP TABLE IF EXISTS null_tab2;
CREATE TABLE null_tab2(a STRING, b STRING) ROW FORMAT DELIMITED NULL DEFINED AS 'fooNull';
LOAD DATA LOCAL INPATH '${system:test.tmp.dir}/hive_test/nullformat/000000_0' INTO TABLE null_tab2;
SELECT * FROM null_tab2;


dfs -rmr ${system:test.tmp.dir}/hive_test/nullformat;
DROP TABLE base_tab;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;

explain
select count(1) from src x where x.key > 9999;

select count(1) from src x where x.key > 9999;

set hive.map.aggr=true;
set hive.groupby.skewindata=false;

explain
select count(1) from src x where x.key > 9999;

select count(1) from src x where x.key > 9999;

set hive.map.aggr=false;
set hive.groupby.skewindata=true;

explain
select count(1) from src x where x.key > 9999;

select count(1) from src x where x.key > 9999;

set hive.map.aggr=false;
set hive.groupby.skewindata=false;

explain
select count(1) from src x where x.key > 9999;

select count(1) from src x where x.key > 9999;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;

explain
select x.key, count(1) from src x where x.key > 9999 group by x.key;

select x.key, count(1) from src x where x.key > 9999 group by x.key;

set hive.map.aggr=true;
set hive.groupby.skewindata=false;

explain
select x.key, count(1) from src x where x.key > 9999 group by x.key;

select x.key, count(1) from src x where x.key > 9999 group by x.key;

set hive.map.aggr=false;
set hive.groupby.skewindata=true;

explain
select x.key, count(1) from src x where x.key > 9999 group by x.key;

select x.key, count(1) from src x where x.key > 9999 group by x.key;

set hive.map.aggr=false;
set hive.groupby.skewindata=false;

explain
select x.key, count(1) from src x where x.key > 9999 group by x.key;

select x.key, count(1) from src x where x.key > 9999 group by x.key;
set hive.mapred.mode=nonstrict;
CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE tstparttbl PARTITION (ds='2008-04-09');
LOAD DATA LOCAL INPATH '../../data/files/nullfile.txt' INTO TABLE tstparttbl PARTITION (ds='2008-04-08');
explain
select count(1) from tstparttbl;
select count(1) from tstparttbl;

CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/nullfile.txt' INTO TABLE tstparttbl2 PARTITION (ds='2008-04-09');
LOAD DATA LOCAL INPATH '../../data/files/nullfile.txt' INTO TABLE tstparttbl2 PARTITION (ds='2008-04-08');
explain
select count(1) from tstparttbl2;
select count(1) from tstparttbl2;
DROP TABLE tstparttbl;
CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE tstparttbl PARTITION (ds='2008-04-09');
LOAD DATA LOCAL INPATH '../../data/files/nullfile.txt' INTO TABLE tstparttbl PARTITION (ds='2008-04-08');
explain
select count(1) from tstparttbl;
select count(1) from tstparttbl;

DROP TABLE tstparttbl2;
CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/nullfile.txt' INTO TABLE tstparttbl2 PARTITION (ds='2008-04-09');
LOAD DATA LOCAL INPATH '../../data/files/nullfile.txt' INTO TABLE tstparttbl2 PARTITION (ds='2008-04-08');
explain
select count(1) from tstparttbl2;
select count(1) from tstparttbl2;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=true;

explain
select count(1), count(distinct x.value) from src x where x.key = 9999;

select count(1), count(distinct x.value) from src x where x.key = 9999;

set hive.map.aggr=true;
set hive.groupby.skewindata=false;

explain
select count(1), count(distinct x.value) from src x where x.key = 9999;

select count(1), count(distinct x.value) from src x where x.key = 9999;

set hive.map.aggr=false;
set hive.groupby.skewindata=true;

explain
select count(1), count(distinct x.value) from src x where x.key = 9999;

select count(1), count(distinct x.value) from src x where x.key = 9999;

set hive.map.aggr=false;
set hive.groupby.skewindata=false;

explain
select count(1), count(distinct x.value) from src x where x.key = 9999;

select count(1), count(distinct x.value) from src x where x.key = 9999;
set hive.mapred.mode=nonstrict;
set hive.map.aggr=true;
set hive.groupby.skewindata=false;

explain
select count(1), count(distinct x.value), count(distinct substr(x.value, 5)) from src x where x.key = 9999;

select count(1), count(distinct x.value), count(distinct substr(x.value, 5)) from src x where x.key = 9999;

set hive.map.aggr=false;
set hive.groupby.skewindata=false;

explain
select count(1), count(distinct x.value), count(distinct substr(x.value, 5)) from src x where x.key = 9999;

select count(1), count(distinct x.value), count(distinct substr(x.value, 5)) from src x where x.key = 9999;

CREATE TABLE tstparttbl(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE tstparttbl PARTITION (ds='2009-04-09');


CREATE TABLE tstparttbl2(KEY STRING, VALUE STRING) PARTITIONED BY(ds string) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE tstparttbl2 PARTITION (ds='2009-04-09');

explain
select u.* from
(
  select key, value from tstparttbl x where x.ds='2009-04-05'
    union all
  select key, value from tstparttbl2 y where y.ds='2009-04-09'
)u;

select u.* from
(
  select key, value from tstparttbl x where x.ds='2009-04-05'
    union all
  select key, value from tstparttbl2 y where y.ds='2009-04-09'
)u;




create table tstnullinut(a string, b string);
select x.* from tstnullinut x;
select x.a, count(1) from tstnullinut x group by x.a;



create table nulltbl(key int) partitioned by (ds string);
select key from nulltbl where ds='101';

select count(1) from nulltbl where ds='101';


create table map_txt (
  id int,
  content map<int,string>
)
row format delimited
null defined as '\\N'
stored as textfile
;

LOAD DATA LOCAL INPATH '../../data/files/mapNull.txt' INTO TABLE map_txt;

select * from map_txt;

select id, map_keys(content) from map_txt;

CREATE TABLE nullscript(KEY STRING, VALUE STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE nullscript;
LOAD DATA LOCAL INPATH '../../data/files/nullfile.txt' INTO TABLE nullscript;
explain
select transform(key) using 'cat' as key1 from nullscript;
select transform(key) using 'cat' as key1 from nullscript;



EXPLAIN SELECT ARRAY(NULL, 0),
               ARRAY(NULL, ARRAY()),
               ARRAY(NULL, MAP()),
               ARRAY(NULL, STRUCT(0))
        FROM src tablesample (1 rows);

SELECT ARRAY(NULL, 0),
       ARRAY(NULL, ARRAY()),
       ARRAY(NULL, MAP()),
       ARRAY(NULL, STRUCT(0))
FROM src tablesample (1 rows);




create table temp_null(a int) stored as textfile;
load data local inpath '../../data/files/test.dat' overwrite into table temp_null;

select null, null from temp_null;

create table tt(a int, b string);
insert overwrite table tt select null, null from temp_null;
select * from tt;

create table tt_b(a int, b string) row format serde "org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe";
insert overwrite table tt_b select null, null from temp_null;
select * from tt_b;

insert overwrite directory "target/warehouse/null_columns.out" select null, null from temp_null;
dfs -cat ${system:test.warehouse.dir}/null_columns.out/*;


create table temp_null2 (key string, value string) partitioned by (ds string);
insert overwrite table temp_null2 partition(ds='2010-04-01') select '1',NULL from src limit 1;
select * from temp_null2 where ds is not null;





set hive.fetch.task.conversion=more;

EXPLAIN SELECT null + 7, 1.0 - null, null + null,
               CAST(21 AS BIGINT) % CAST(5 AS TINYINT),
               CAST(21 AS BIGINT) % CAST(21 AS BIGINT),
               9 % "3" FROM src LIMIT 1;

SELECT null + 7, 1.0 - null, null + null,
       CAST(21 AS BIGINT) % CAST(5 AS TINYINT),
       CAST(21 AS BIGINT) % CAST(21 AS BIGINT),
       9 % "3" FROM src LIMIT 1;

create table over1k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over1k' into table over1k;

-- Integers
select nvl(t, true) from over1k limit 5;
EXPLAIN
SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 10,10;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 10,10;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 0,10;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 1,10;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 300,100;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 100 OFFSET 300;

set hive.cbo.enable=false;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 10,10;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 0,10;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 1,10;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 300,100;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key ORDER BY src.key LIMIT 100 OFFSET 300;

set hive.limit.optimize.enable=true;
set hive.limit.row.max.size=12;
set hive.mapred.mode=nonstrict;

EXPLAIN EXTENDED
SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 400,10;

SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 400,10;

EXPLAIN EXTENDED
SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,10;

SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,10;

EXPLAIN EXTENDED
SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,20;

SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,20;

EXPLAIN EXTENDED
SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,600;

SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,600;

set hive.cbo.enable=false;

EXPLAIN EXTENDED
SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 400,10;

SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 400,10;

EXPLAIN EXTENDED
SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,10;

SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,10;

EXPLAIN EXTENDED
SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,20;

SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,20;

EXPLAIN EXTENDED
SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,600;

SELECT srcpart.key, substr(srcpart.value,5), ds, hr FROM srcpart LIMIT 490,600;set hive.explain.user=false;
set hive.limit.pushdown.memory.usage=0.3f;
set hive.optimize.reducededuplication.min.reducer=1;

explain
select key,value from src order by key limit 10,20;
select key,value from src order by key limit 10,20;

explain
select key,value from src order by key desc limit 10,20;
select key,value from src order by key desc limit 10,20;

explain
select value, sum(key + 1) as sum from src group by value order by value limit 10,20;
select value, sum(key + 1) as sum from src group by value order by value limit 10,20;

-- deduped RS
explain
select value,avg(key + 1) from src group by value order by value limit 10,20;
select value,avg(key + 1) from src group by value order by value limit 10,20;

-- distincts
explain
select distinct(cdouble) as dis from alltypesorc order by dis limit 10,20;
select distinct(cdouble) as dis from alltypesorc order by dis limit 10,20;

explain
select ctinyint, count(distinct(cdouble)) from alltypesorc group by ctinyint order by ctinyint limit 10,20;
select ctinyint, count(distinct(cdouble)) from alltypesorc group by ctinyint order by ctinyint limit 10,20;

explain
select ctinyint, count(cdouble) from (select ctinyint, cdouble from alltypesorc group by ctinyint, cdouble) t1 group by ctinyint order by ctinyint limit 10,20;
select ctinyint, count(cdouble) from (select ctinyint, cdouble from alltypesorc group by ctinyint, cdouble) t1 group by ctinyint order by ctinyint limit 10,20;

-- multi distinct
explain
select ctinyint, count(distinct(cstring1)), count(distinct(cstring2)) from alltypesorc group by ctinyint order by ctinyint limit 10,20;
select ctinyint, count(distinct(cstring1)), count(distinct(cstring2)) from alltypesorc group by ctinyint order by ctinyint limit 10,20;

-- limit zero
explain
select key,value from src order by key limit 0,0;
select key,value from src order by key limit 0,0;

-- 2MR (applied to last RS)
explain
select value, sum(key) as sum from src group by value order by sum limit 10,20;
select value, sum(key) as sum from src group by value order by sum limit 10,20;

set hive.map.aggr=false;
-- map aggregation disabled
explain
select value, sum(key) as sum from src group by value order by value limit 10,20;
select value, sum(key) as sum from src group by value order by value limit 10,20;

set hive.limit.pushdown.memory.usage=0.00002f;

-- flush for order-by
explain
select key,value,value,value,value,value,value,value,value from src order by key limit 30,70;
select key,value,value,value,value,value,value,value,value from src order by key limit 30,70;

-- flush for group-by
explain
select sum(key) as sum from src group by concat(key,value,value,value,value,value,value,value,value,value) order by sum limit 30,70;
select sum(key) as sum from src group by concat(key,value,value,value,value,value,value,value,value,value) order by sum limit 30,70;

-- subqueries
explain
select * from
(select key, count(1) from src group by key order by key limit 10,20) subq
join
(select key, count(1) from src group by key limit 20,20) subq2
on subq.key=subq2.key limit 3,5;
select * from
(select key, count(1) from src group by key order by key limit 10,20) subq
join
(select key, count(1) from src group by key order by key limit 20,20) subq2
on subq.key=subq2.key limit 3,5;

set hive.fetch.task.conversion=more;

select 1.0 < 2.0 from src limit 1;
select 2.0 < 2.0 from src limit 1;
select 2.0 > 1.0 from src limit 1;
select 2.0 > 2.0 from src limit 1;

select 'NaN' < 2.0 from src limit 1;
select 1.0 < 'NaN' from src limit 1;
select 1.0 > 'NaN' from src limit 1;
select 'NaN' > 2.0 from src limit 1;
select 'NaN' > 'NaN' from src limit 1;
select 'NaN' < 'NaN' from src limit 1;

select 'NaN' = 2.0 from src limit 1;
select 1.0 = 'NaN' from src limit 1;
select 'NaN' = 2.0 from src limit 1;
select 'NaN' = 'NaN' from src limit 1;

select 'NaN' <> 2.0 from src limit 1;
select 1.0 <> 'NaN' from src limit 1;
select 'NaN' <> 2.0 from src limit 1;
select 'NaN' <> 'NaN' from src limit 1;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=false;
-- SORT_QUERY_RESULTS

-- Disable CBO here, because it messes with the cases specifically crafted for the optimizer.
-- Instead, we could improve the optimizer to recognize more cases, e.g. filter before join.

explain extended
select key from src where false;
select key from src where false;

explain extended
select count(key) from srcpart where 1=2 group by key;
select count(key) from srcpart where 1=2 group by key;

explain extended
select * from (select key from src where false) a left outer join (select key from srcpart limit 0) b on a.key=b.key;
select * from (select key from src where false) a left outer join (select key from srcpart limit 0) b on a.key=b.key;

explain extended
select count(key) from src where false union all select count(key) from srcpart ;
select count(key) from src where false union all select count(key) from srcpart ;

explain extended
select * from (select key from src where false) a left outer join (select value from srcpart limit 0) b ;
select * from (select key from src where false) a left outer join (select value from srcpart limit 0) b ;

explain extended
select * from (select key from src union all select src.key from src left outer join srcpart on src.key = srcpart.key) a  where false;
select * from (select key from src union all select src.key from src left outer join srcpart on src.key = srcpart.key) a  where false;

explain extended
select * from src s1, src s2 where false and s1.value = s2.value;
select * from src s1, src s2 where false and s1.value = s2.value;

explain extended
select count(1) from src where null = 1;
select count(1) from src where null = 1;
set hive.mapred.mode=nonstrict;
EXPLAIN SELECT * FROM src a LEFT OUTER JOIN src b on (a.key=b.key);
EXPLAIN SELECT * FROM src a LEFT JOIN src b on (a.key=b.key);

EXPLAIN SELECT * FROM src a RIGHT OUTER JOIN src b on (a.key=b.key);
EXPLAIN SELECT * FROM src a RIGHT JOIN src b on (a.key=b.key);

EXPLAIN SELECT * FROM src a FULL OUTER JOIN src b on (a.key=b.key);
EXPLAIN SELECT * FROM src a FULL JOIN src b on (a.key=b.key);
set hive.mapred.mode=nonstrict;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;

CREATE TABLE orc_create_people_staging (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp,
  state string);

LOAD DATA LOCAL INPATH '../../data/files/orc_create_people.txt' OVERWRITE INTO TABLE orc_create_people_staging;

set hive.exec.dynamic.partition.mode=nonstrict;

set hive.stats.autogather=false;
-- non-partitioned table
-- partial scan gather
CREATE TABLE orc_create_people (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp,
  state string)
STORED AS orc;

INSERT OVERWRITE TABLE orc_create_people SELECT * FROM orc_create_people_staging ORDER BY id;

set hive.stats.autogather = true;
analyze table orc_create_people compute statistics;
desc formatted orc_create_people;

analyze table orc_create_people compute statistics partialscan;
desc formatted orc_create_people;

analyze table orc_create_people compute statistics noscan;
desc formatted orc_create_people;

drop table orc_create_people;

-- auto stats gather
CREATE TABLE orc_create_people (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp,
  state string)
STORED AS orc;

INSERT OVERWRITE TABLE orc_create_people SELECT * FROM orc_create_people_staging ORDER BY id;

desc formatted orc_create_people;

drop table orc_create_people;

set hive.stats.autogather=false;
-- partitioned table
-- partial scan gather
CREATE TABLE orc_create_people (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp)
PARTITIONED BY (state string)
STORED AS orc;

INSERT OVERWRITE TABLE orc_create_people PARTITION (state)
  SELECT * FROM orc_create_people_staging ORDER BY id;

set hive.stats.autogather = true;
analyze table orc_create_people partition(state) compute statistics;
desc formatted orc_create_people partition(state="Ca");
desc formatted orc_create_people partition(state="Or");

analyze table orc_create_people partition(state) compute statistics partialscan;
desc formatted orc_create_people partition(state="Ca");
desc formatted orc_create_people partition(state="Or");

analyze table orc_create_people partition(state) compute statistics noscan;
desc formatted orc_create_people partition(state="Ca");
desc formatted orc_create_people partition(state="Or");

drop table orc_create_people;

-- auto stats gather
CREATE TABLE orc_create_people (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp)
PARTITIONED BY (state string)
STORED AS orc;

INSERT OVERWRITE TABLE orc_create_people PARTITION (state)
  SELECT * FROM orc_create_people_staging ORDER BY id;

desc formatted orc_create_people partition(state="Ca");
desc formatted orc_create_people partition(state="Or");

drop table orc_create_people;

set hive.stats.autogather=false;
-- partitioned and bucketed table
-- partial scan gather
CREATE TABLE orc_create_people (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp)
PARTITIONED BY (state string)
clustered by (first_name)
sorted by (last_name)
into 4 buckets
STORED AS orc;

INSERT OVERWRITE TABLE orc_create_people PARTITION (state)
  SELECT * FROM orc_create_people_staging ORDER BY id;

set hive.stats.autogather = true;
analyze table orc_create_people partition(state) compute statistics;
desc formatted orc_create_people partition(state="Ca");
desc formatted orc_create_people partition(state="Or");

analyze table orc_create_people partition(state) compute statistics partialscan;
desc formatted orc_create_people partition(state="Ca");
desc formatted orc_create_people partition(state="Or");

analyze table orc_create_people partition(state) compute statistics noscan;
desc formatted orc_create_people partition(state="Ca");
desc formatted orc_create_people partition(state="Or");

drop table orc_create_people;

-- auto stats gather
CREATE TABLE orc_create_people (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp)
PARTITIONED BY (state string)
clustered by (first_name)
sorted by (last_name)
into 4 buckets
STORED AS orc;

INSERT OVERWRITE TABLE orc_create_people PARTITION (state)
  SELECT * FROM orc_create_people_staging ORDER BY id;

desc formatted orc_create_people partition(state="Ca");
desc formatted orc_create_people partition(state="Or");

drop table orc_create_people;

set hive.stats.autogather=false;
-- create table with partitions containing text and ORC files.
-- ORC files implements StatsProvidingRecordReader but text files does not.
-- So the partition containing text file should not have statistics.
CREATE TABLE orc_create_people (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp)
PARTITIONED BY (state string)
STORED AS orc;

INSERT OVERWRITE TABLE orc_create_people PARTITION (state)
  SELECT * FROM orc_create_people_staging ORDER BY id;

set hive.stats.autogather = true;
analyze table orc_create_people partition(state) compute statistics;
desc formatted orc_create_people partition(state="Ca");

analyze table orc_create_people partition(state) compute statistics partialscan;
desc formatted orc_create_people partition(state="Ca");

analyze table orc_create_people partition(state) compute statistics noscan;
desc formatted orc_create_people partition(state="Ca");

drop table orc_create_people;
SET hive.exec.schema.evolution=true;
create table src_orc (key tinyint, val string) stored as orc;
alter table src_orc set fileformat textfile;
SET hive.exec.schema.evolution=false;
create table src_orc (key tinyint, val string)  clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc set fileformat textfile;
SET hive.exec.schema.evolution=true;
create table src_orc (key tinyint, val string) stored as orc;
alter table src_orc set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
SET hive.exec.schema.evolution=false;
create table src_orc (key tinyint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

DROP TABLE orc_create;
DROP TABLE orc_create_complex;
DROP TABLE orc_create_staging;
DROP TABLE orc_create_people_staging;
DROP TABLE orc_create_people;

CREATE TABLE orc_create_staging (
  str STRING,
  mp  MAP<STRING,STRING>,
  lst ARRAY<STRING>,
  strct STRUCT<A:STRING,B:STRING>
) ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '|'
    COLLECTION ITEMS TERMINATED BY ','
    MAP KEYS TERMINATED BY ':';

DESCRIBE FORMATTED orc_create_staging;

CREATE TABLE orc_create (key INT, value STRING)
   PARTITIONED BY (ds string)
   STORED AS ORC;

DESCRIBE FORMATTED orc_create;

DROP TABLE orc_create;

CREATE TABLE orc_create (key INT, value STRING)
   PARTITIONED BY (ds string);

DESCRIBE FORMATTED orc_create;

ALTER TABLE orc_create SET FILEFORMAT ORC;

DESCRIBE FORMATTED orc_create;

DROP TABLE orc_create;

set hive.default.fileformat=orc;

CREATE TABLE orc_create (key INT, value STRING)
   PARTITIONED BY (ds string);

set hive.default.fileformat=TextFile;

DESCRIBE FORMATTED orc_create;

CREATE TABLE orc_create_complex (
  str STRING,
  mp  MAP<STRING,STRING>,
  lst ARRAY<STRING>,
  strct STRUCT<A:STRING,B:STRING>
) STORED AS ORC;

DESCRIBE FORMATTED orc_create_complex;

LOAD DATA LOCAL INPATH '../../data/files/orc_create.txt' OVERWRITE INTO TABLE orc_create_staging;

SELECT * from orc_create_staging;

INSERT OVERWRITE TABLE orc_create_complex SELECT * FROM orc_create_staging;

SELECT * from orc_create_complex;
SELECT str from orc_create_complex;
SELECT mp from orc_create_complex;
SELECT lst from orc_create_complex;
SELECT strct from orc_create_complex;

CREATE TABLE orc_create_people_staging (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp,
  state string);

LOAD DATA LOCAL INPATH '../../data/files/orc_create_people.txt'
  OVERWRITE INTO TABLE orc_create_people_staging;

CREATE TABLE orc_create_people (
  id int,
  first_name string,
  last_name string,
  address string,
  salary decimal,
  start_date timestamp)
PARTITIONED BY (state string)
STORED AS orc;

set hive.exec.dynamic.partition.mode=nonstrict;

INSERT OVERWRITE TABLE orc_create_people PARTITION (state)
  SELECT * FROM orc_create_people_staging;

SET hive.optimize.index.filter=true;
-- test predicate push down with partition pruning
SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca';

-- test predicate push down
SELECT COUNT(*) FROM orc_create_people where id = 50;
SELECT COUNT(*) FROM orc_create_people where id between 10 and 20;
SELECT COUNT(*) FROM orc_create_people where id > 10 and id < 100;
SELECT COUNT(*) FROM orc_create_people where (id + 1) = 20;
SELECT COUNT(*) FROM orc_create_people where (id + 10) < 200;
SELECT COUNT(*) FROM orc_create_people where id < 30  or first_name = "Rafael";
SELECT COUNT(*) FROM orc_create_people
   where length(substr(first_name, 1, 2)) <= 2 and last_name like '%';
SELECT COUNT(*) FROM orc_create_people where salary = 200.00;
SELECT COUNT(*) FROM orc_create_people WHERE start_date IS NULL;
SELECT COUNT(*) FROM orc_create_people WHERE YEAR(start_date) = 2014;

-- test predicate push down with partition pruning
SELECT COUNT(*) FROM orc_create_people where salary = 200.00 and state = 'Ca';

-- test predicate push down with no column projection
SELECT id, first_name, last_name, address
  FROM orc_create_people WHERE id > 90;

DROP TABLE orc_create;
DROP TABLE orc_create_complex;
DROP TABLE orc_create_staging;
DROP TABLE orc_create_people_staging;
DROP TABLE orc_create_people;
set hive.mapred.mode=nonstrict;
set mapred.max.split.size=100;
set mapred.min.split.size=1;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

DROP TABLE orc_createas1a;
DROP TABLE orc_createas1b;
DROP TABLE orc_createas1c;

CREATE TABLE orc_createas1a (key INT, value STRING)
    PARTITIONED BY (ds string);
INSERT OVERWRITE TABLE orc_createas1a PARTITION (ds='1')
    SELECT * FROM src;
INSERT OVERWRITE TABLE orc_createas1a PARTITION (ds='2')
    SELECT * FROM src;

EXPLAIN CREATE TABLE orc_createas1b
    STORED AS ORC AS
    SELECT * FROM src;

CREATE TABLE orc_createas1b
    STORED AS ORC AS
    SELECT * FROM src;

EXPLAIN SELECT * FROM orc_createas1b ORDER BY key LIMIT 5;

SELECT * FROM orc_createas1b ORDER BY key LIMIT 5;

EXPLAIN
    CREATE TABLE orc_createas1c
    STORED AS ORC AS
        SELECT key, value, PMOD(HASH(key), 50) as part
        FROM orc_createas1a;
CREATE TABLE orc_createas1c
    STORED AS ORC AS
        SELECT key, value, PMOD(HASH(key), 50) as part
        FROM orc_createas1a;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM orc_createas1a
) t;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM orc_createas1c
) t;

DROP TABLE orc_createas1a;
DROP TABLE orc_createas1b;
DROP TABLE orc_createas1c;
set hive.exec.orc.dictionary.key.size.threshold=-1;

-- Set the threshold to -1 to guarantee dictionary encoding is turned off
-- Tests that the data can be read back correctly when a string column is stored
-- without dictionary encoding

CREATE TABLE test_orc (key STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';

-- should be single split
INSERT OVERWRITE TABLE test_orc SELECT key FROM src TABLESAMPLE (10 ROWS);

-- Test reading the column back

SELECT * FROM test_orc;

ALTER TABLE test_orc SET SERDEPROPERTIES ('orc.stripe.size' = '1');

CREATE TABLE src_thousand(key STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1kv2.cogroup.txt'
     INTO TABLE src_thousand;

set hive.exec.orc.dictionary.key.size.threshold=0.5;

-- Add data to the table in such a way that alternate stripes encode the column
-- differently.  Setting orc.stripe.size = 1 guarantees the stripes each have
-- 5000 rows.  The first stripe will have 5 * 630 distinct rows and thus be
-- above the cutoff of 50% and will be direct encoded. The second stripe
-- will have 5 * 1 distinct rows and thus be under the cutoff and will be
-- dictionary encoded. The final stripe will have 630 out of 1000 and be
-- direct encoded.

INSERT OVERWRITE TABLE test_orc
SELECT key FROM (
SELECT CONCAT("a", key) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("b", key) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("c", key) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("d", key) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("e", key) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("f", 1) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("g", 1) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("h", 1) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("i", 1) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("j", 1) AS key FROM src_thousand
UNION ALL
SELECT CONCAT("k", key) AS key FROM src_thousand
) a ORDER BY key LIMIT 11000;

SELECT SUM(HASH(key)) FROM test_orc;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE test_orc (key STRING)
PARTITIONED BY (part STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- Create a table with one column write to a partition, then add an additional column and write
-- to another partition
-- This can produce unexpected results with CombineHiveInputFormat

INSERT OVERWRITE TABLE test_orc PARTITION (part = '1') SELECT key FROM src tablesample (5 rows);

ALTER TABLE test_orc ADD COLUMNS (cnt INT);

INSERT OVERWRITE TABLE test_orc PARTITION (part = '2') SELECT key, count(*) FROM src GROUP BY key LIMIT 5;

SELECT * FROM test_orc;
-- Create a table with one column, write to it, then add an additional column
-- This can break reads

-- SORT_QUERY_RESULTS

CREATE TABLE test_orc (key STRING)
STORED AS ORC;

INSERT OVERWRITE TABLE test_orc SELECT key FROM src LIMIT 5;

ALTER TABLE test_orc ADD COLUMNS (value STRING);

SELECT * FROM test_orc;
CREATE TABLE test_orc (key STRING, cnt INT)
CLUSTERED BY (key) INTO 3 BUCKETS
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';


set hive.exec.reducers.max = 1;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- Creates a table bucketed into 3 buckets, but only one contains data, specifically bucket 1,
-- buckets 0 and 2 are empty, so this tests reading from and empty file followed by a file
-- containing data and a file containing data followed by an empty file.
-- This can produce unexpected results with CombineHiveInputFormat

INSERT OVERWRITE TABLE test_orc SELECT one, COUNT(*) FROM (SELECT 1 AS one FROM src) a GROUP BY one;

SELECT count(*) FROM test_orc;
-- SORT_QUERY_RESULTS

CREATE TABLE test_orc (key STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';

INSERT OVERWRITE TABLE test_orc SELECT '' FROM src tablesample (10 rows);

-- Test reading a column which is just empty strings

SELECT * FROM test_orc;

INSERT OVERWRITE TABLE test_orc SELECT IF (key % 3 = 0, key, '') FROM src tablesample (10 rows);

-- Test reading a column which has some empty strings

SELECT * FROM test_orc;
CREATE TABLE test_orc (key STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';

ALTER TABLE test_orc SET SERDEPROPERTIES ('orc.row.index.stride' = '1000');

-- nulls.txt is a file containing a non-null string row followed by 1000 null string rows
-- this produces the effect that the number of non-null rows between the last and second
-- to last index stride are the same (there's only two index strides)

CREATE TABLE src_null(a STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/nulls.txt' INTO TABLE src_null;

INSERT OVERWRITE TABLE test_orc SELECT a FROM src_null;

SELECT * FROM test_orc LIMIT 5;
set hive.mapred.mode=nonstrict;
CREATE TABLE staging(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE staging;

CREATE TABLE orc_ppd(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS ORC tblproperties("orc.row.index.stride" = "1000", "orc.bloom.filter.columns"="*");

insert overwrite table orc_ppd select * from staging;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump;

select * from orc_ppd limit 1;

alter table orc_ppd set tblproperties("orc.bloom.filter.fpp"="0.01");

insert overwrite table orc_ppd select * from staging;

select * from orc_ppd limit 1;

CREATE TABLE orc_ppd_part(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
PARTITIONED BY (ds string, hr int) STORED AS ORC tblproperties("orc.row.index.stride" = "1000", "orc.bloom.filter.columns"="*");

insert overwrite table orc_ppd_part partition(ds = "2015", hr = 10) select * from staging;

select * from orc_ppd_part limit 1;
set hive.mapred.mode=nonstrict;
set hive.metastore.disallow.incompatible.col.type.changes=false;
create table if not exists alltypes (
 bo boolean,
 ti tinyint,
 si smallint,
 i int,
 bi bigint,
 f float,
 d double,
 de decimal(10,3),
 ts timestamp,
 da date,
 s string,
 c char(5),
 vc varchar(5),
 m map<string, string>,
 l array<int>,
 st struct<c1:int, c2:string>
) row format delimited fields terminated by '|'
collection items terminated by ','
map keys terminated by ':' stored as textfile;

create table if not exists alltypes_orc (
 bo boolean,
 ti tinyint,
 si smallint,
 i int,
 bi bigint,
 f float,
 d double,
 de decimal(10,3),
 ts timestamp,
 da date,
 s string,
 c char(5),
 vc varchar(5),
 m map<string, string>,
 l array<int>,
 st struct<c1:int, c2:string>
) stored as orc;

load data local inpath '../../data/files/alltypes2.txt' overwrite into table alltypes;

insert overwrite table alltypes_orc select * from alltypes;

select * from alltypes_orc;

alter table alltypes_orc change si si int;
select * from alltypes_orc;

alter table alltypes_orc change si si bigint;
alter table alltypes_orc change i i bigint;
select * from alltypes_orc;

set hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;

explain select ti, si, i, bi from alltypes_orc;
select ti, si, i, bi from alltypes_orc;

set hive.exec.dynamic.partition.mode=nonstrict;
create table src_part_orc (key int, value string) partitioned by (ds string) stored as orc;
insert overwrite table src_part_orc partition(ds) select key, value, ds from srcpart where ds is not null;

select * from src_part_orc limit 10;

alter table src_part_orc change key key bigint;
select * from src_part_orc limit 10;
reset hive.metastore.disallow.incompatible.col.type.changes;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;

SET hive.llap.io.enabled=false;

SET hive.exec.orc.default.buffer.size=32768;
SET hive.exec.orc.default.row.index.stride=1000;
SET hive.optimize.index.filter=true;
set hive.auto.convert.join=false;

DROP TABLE cross_numbers;
DROP TABLE orc_llap;
DROP TABLE orc_llap_small;

CREATE TABLE orc_llap(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cstring2 STRING,
    ctimestamp1 TIMESTAMP,
    ctimestamp2 TIMESTAMP,
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN)
    STORED AS ORC;

CREATE TABLE orc_llap_small(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT)
    STORED AS ORC;


create table cross_numbers(i int);

insert into table cross_numbers
select distinct csmallint
from alltypesorc where csmallint > 0 order by csmallint limit 10;

insert into table orc_llap
select ctinyint + i, csmallint + i, cint + i, cbigint + i, cfloat + i, cdouble + i, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2
from alltypesorc cross join cross_numbers;

insert into table orc_llap_small
select ctinyint, csmallint, cint from alltypesorc where ctinyint is not null and cint is not null limit 15;


SET hive.llap.io.enabled=true;
set hive.auto.convert.join=true;

-- Cross join with no projection - do it on small table
explain
select count(1) from orc_llap_small y join orc_llap_small x;
select count(1) from orc_llap_small y join orc_llap_small x;

-- All row groups selected, no projection
select count(*) from orc_llap_small;

-- All row groups pruned
select count(*) from orc_llap_small where cint < 60000000;

-- Hash cannot be vectorized, so run hash as the last step on a temp table
drop table llap_temp_table;
explain
select cint, csmallint, cbigint from orc_llap where cint > 10 and cbigint is not null;
create table llap_temp_table as
select cint, csmallint, cbigint from orc_llap where cint > 10 and cbigint is not null;
select sum(hash(*)) from llap_temp_table;

drop table llap_temp_table;
explain
select * from orc_llap where cint > 10 and cbigint is not null;
create table llap_temp_table as
select * from orc_llap where cint > 10 and cbigint is not null;
select sum(hash(*)) from llap_temp_table;

drop table llap_temp_table;
explain
select cstring2 from orc_llap where cint > 5 and cint < 10;
create table llap_temp_table as
select cstring2 from orc_llap where cint > 5 and cint < 10;
select sum(hash(*)) from llap_temp_table;


drop table llap_temp_table;
explain
select cstring1, cstring2, count(*) from orc_llap group by cstring1, cstring2;
create table llap_temp_table as
select cstring1, cstring2, count(*) from orc_llap group by cstring1, cstring2;
select sum(hash(*)) from llap_temp_table;

drop table llap_temp_table;
explain
select o1.cstring1, o2.cstring2 from orc_llap o1 inner join orc_llap o2 on o1.csmallint = o2.csmallint where o1.cbigint is not null and o2.cbigint is not null;
create table llap_temp_table as
select o1.cstring1, o2.cstring2 from orc_llap o1 inner join orc_llap o2 on o1.csmallint = o2.csmallint where o1.cbigint is not null and o2.cbigint is not null;
select sum(hash(*)) from llap_temp_table;

-- multi-stripe test
insert into table orc_llap
select ctinyint + i, csmallint + i, cint + i, cbigint + i, cfloat + i, cdouble + i, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2
from alltypesorc cross join cross_numbers;

alter table orc_llap concatenate;

drop table llap_temp_table;
explain
select cint, csmallint, cbigint from orc_llap where cint > 10 and cbigint is not null;
create table llap_temp_table as
select cint, csmallint, cbigint from orc_llap where cint > 10 and cbigint is not null;
select sum(hash(*)) from llap_temp_table;

drop table llap_temp_table;
explain
select * from orc_llap where cint > 10 and cbigint is not null;
create table llap_temp_table as
select * from orc_llap where cint > 10 and cbigint is not null;
select sum(hash(*)) from llap_temp_table;

drop table llap_temp_table;
explain
select cstring2 from orc_llap where cint > 5 and cint < 10;
create table llap_temp_table as
select cstring2 from orc_llap where cint > 5 and cint < 10;
select sum(hash(*)) from llap_temp_table;

drop table llap_temp_table;
explain
select cstring1, cstring2, count(*) from orc_llap group by cstring1, cstring2;
create table llap_temp_table as
select cstring1, cstring2, count(*) from orc_llap group by cstring1, cstring2;
select sum(hash(*)) from llap_temp_table;

drop table llap_temp_table;
explain
select o1.cstring1, o2.cstring2 from orc_llap o1 inner join orc_llap o2 on o1.csmallint = o2.csmallint where o1.cbigint is not null and o2.cbigint is not null;
create table llap_temp_table as
select o1.cstring1, o2.cstring2 from orc_llap o1 inner join orc_llap o2 on o1.csmallint = o2.csmallint where o1.cbigint is not null and o2.cbigint is not null;
select sum(hash(*)) from llap_temp_table;

drop table llap_temp_table;


DROP TABLE cross_numbers;
DROP TABLE orc_llap;
DROP TABLE orc_llap_small;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.merge.orcfile.stripe.level=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.optimize.sort.dynamic.partition=false;
set mapred.min.split.size=1000;
set mapred.max.split.size=2000;
set tez.grouping.min-size=1000;
set tez.grouping.max-size=2000;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;

-- SORT_QUERY_RESULTS

DROP TABLE orcfile_merge1;
DROP TABLE orcfile_merge1b;
DROP TABLE orcfile_merge1c;

CREATE TABLE orcfile_merge1 (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC;
CREATE TABLE orcfile_merge1b (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC;
CREATE TABLE orcfile_merge1c (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC;

-- merge disabled
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1 PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1 PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1/ds=1/part=0/;

set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;
-- auto-merge slow way
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1b PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1b PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1b/ds=1/part=0/;

set hive.merge.orcfile.stripe.level=true;
-- auto-merge fast way
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1c PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1c PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1c/ds=1/part=0/;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
-- Verify
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1 WHERE ds='1'
) t;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1b WHERE ds='1'
) t;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1c WHERE ds='1'
) t;

select count(*) from orcfile_merge1;
select count(*) from orcfile_merge1b;
select count(*) from orcfile_merge1c;

DROP TABLE orcfile_merge1;
DROP TABLE orcfile_merge1b;
DROP TABLE orcfile_merge1c;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.merge.orcfile.stripe.level=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.optimize.sort.dynamic.partition=false;
set mapred.min.split.size=1000;
set mapred.max.split.size=2000;
set tez.am.grouping.split-count=2;
set tez.grouping.split-count=2;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- SORT_QUERY_RESULTS

DROP TABLE orcfile_merge1;
DROP TABLE orcfile_merge1b;
DROP TABLE orcfile_merge1c;

CREATE TABLE orcfile_merge1 (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC tblproperties("orc.compress"="SNAPPY","orc.compress.size"="4096");
CREATE TABLE orcfile_merge1b (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC tblproperties("orc.compress"="SNAPPY","orc.compress.size"="4096");
CREATE TABLE orcfile_merge1c (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC tblproperties("orc.compress"="SNAPPY","orc.compress.size"="4096");

-- merge disabled
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1 PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1 PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1/ds=1/part=0/;

set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
-- auto-merge slow way
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1b PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1b PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1b/ds=1/part=0/;

set hive.merge.orcfile.stripe.level=true;
-- auto-merge fast way
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1c PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1c PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1c/ds=1/part=0/;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
-- Verify
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1 WHERE ds='1'
) t;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1b WHERE ds='1'
) t;

select count(*) from orcfile_merge1;
select count(*) from orcfile_merge1b;

set tez.am.grouping.split-count=1;
set tez.grouping.split-count=1;
-- concatenate
explain ALTER TABLE  orcfile_merge1 PARTITION (ds='1', part='0') CONCATENATE;
ALTER TABLE  orcfile_merge1 PARTITION (ds='1', part='0') CONCATENATE;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1/ds=1/part=0/;

-- Verify
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1c WHERE ds='1'
) t;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1 WHERE ds='1'
) t;

select count(*) from orcfile_merge1;
select count(*) from orcfile_merge1c;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump;
select * from orcfile_merge1 where ds='1' and part='0' limit 1;
select * from orcfile_merge1c where ds='1' and part='0' limit 1;

DROP TABLE orcfile_merge1;
DROP TABLE orcfile_merge1b;
DROP TABLE orcfile_merge1c;
DROP TABLE orcfile_merge1;
DROP TABLE orc_split_elim;

create table orc_split_elim (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_split_elim;
load data local inpath '../../data/files/orc_split_elim.orc' into table orc_split_elim;

create table orcfile_merge1 (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc tblproperties("orc.compress.size"="4096");

insert overwrite table orcfile_merge1 select * from orc_split_elim;
insert into table orcfile_merge1 select * from orc_split_elim;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1/;

set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.orcfile.stripe.level=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set tez.am.grouping.split-count=1;
set tez.grouping.split-count=1;
set hive.exec.orc.default.buffer.size=120;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump;
select * from orcfile_merge1 limit 1;
SET hive.exec.post.hooks=;

-- concatenate
ALTER TABLE  orcfile_merge1 CONCATENATE;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1/;

select count(*) from orc_split_elim;
-- will have double the number of rows
select count(*) from orcfile_merge1;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump;
select * from orcfile_merge1 limit 1;
SET hive.exec.post.hooks=;

DROP TABLE orc_split_elim;
DROP TABLE orcfile_merge1;
CREATE TABLE `alltypesorc3xcols`(
  `atinyint` tinyint,
  `asmallint` smallint,
  `aint` int,
  `abigint` bigint,
  `afloat` float,
  `adouble` double,
  `astring1` string,
  `astring2` string,
  `atimestamp1` timestamp,
  `atimestamp2` timestamp,
  `aboolean1` boolean,
  `aboolean2` boolean,
  `btinyint` tinyint,
  `bsmallint` smallint,
  `bint` int,
  `bbigint` bigint,
  `bfloat` float,
  `bdouble` double,
  `bstring1` string,
  `bstring2` string,
  `btimestamp1` timestamp,
  `btimestamp2` timestamp,
  `bboolean1` boolean,
  `bboolean2` boolean,
  `ctinyint` tinyint,
  `csmallint` smallint,
  `cint` int,
  `cbigint` bigint,
  `cfloat` float,
  `cdouble` double,
  `cstring1` string,
  `cstring2` string,
  `ctimestamp1` timestamp,
  `ctimestamp2` timestamp,
  `cboolean1` boolean,
  `cboolean2` boolean) stored as ORC;

load data local inpath '../../data/files/alltypesorc3xcols' into table alltypesorc3xcols;
load data local inpath '../../data/files/alltypesorc3xcols' into table alltypesorc3xcols;

select count(*) from alltypesorc3xcols;
select sum(hash(*)) from alltypesorc3xcols;

alter table alltypesorc3xcols concatenate;

select count(*) from alltypesorc3xcols;
select sum(hash(*)) from alltypesorc3xcols;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecOrcFileDump;
select * from alltypesorc3xcols limit 1;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.merge.orcfile.stripe.level=true;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.merge.sparkfiles=true;

DROP TABLE orcfile_merge2a;

CREATE TABLE orcfile_merge2a (key INT, value STRING)
    PARTITIONED BY (one string, two string, three string)
    STORED AS ORC;

EXPLAIN INSERT OVERWRITE TABLE orcfile_merge2a PARTITION (one='1', two, three)
    SELECT key, value, PMOD(HASH(key), 10) as two,
        PMOD(HASH(value), 10) as three
    FROM src;

INSERT OVERWRITE TABLE orcfile_merge2a PARTITION (one='1', two, three)
    SELECT key, value, PMOD(HASH(key), 10) as two,
        PMOD(HASH(value), 10) as three
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge2a/one=1/two=0/three=2/;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge2a
) t;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value, '1', PMOD(HASH(key), 10),
        PMOD(HASH(value), 10)) USING 'tr \t _' AS (c)
    FROM src
) t;

DROP TABLE orcfile_merge2a;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.merge.orcfile.stripe.level=true;
set hive.merge.sparkfiles=true;

DROP TABLE orcfile_merge3a;
DROP TABLE orcfile_merge3b;

CREATE TABLE orcfile_merge3a (key int, value string)
    PARTITIONED BY (ds string) STORED AS TEXTFILE;
CREATE TABLE orcfile_merge3b (key int, value string) STORED AS ORC;

INSERT OVERWRITE TABLE orcfile_merge3a PARTITION (ds='1')
    SELECT * FROM src;
INSERT OVERWRITE TABLE orcfile_merge3a PARTITION (ds='2')
    SELECT * FROM src;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
EXPLAIN INSERT OVERWRITE TABLE orcfile_merge3b
    SELECT key, value FROM orcfile_merge3a;

INSERT OVERWRITE TABLE orcfile_merge3b
    SELECT key, value FROM orcfile_merge3a;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge3b/;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM orcfile_merge3a
) t;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM orcfile_merge3b
) t;

DROP TABLE orcfile_merge3a;
DROP TABLE orcfile_merge3b;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.merge.orcfile.stripe.level=true;

DROP TABLE orcfile_merge3a;
DROP TABLE orcfile_merge3b;

CREATE TABLE orcfile_merge3a (key int, value string)
    PARTITIONED BY (ds string) STORED AS ORC;
CREATE TABLE orcfile_merge3b (key int, value string) STORED AS TEXTFILE;

set hive.merge.mapfiles=false;
set hive.merge.sparkfiles=false;
INSERT OVERWRITE TABLE orcfile_merge3a PARTITION (ds='1')
    SELECT * FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge3a/ds=1/;

set hive.merge.mapfiles=true;
set hive.merge.sparkfiles=true;
INSERT OVERWRITE TABLE orcfile_merge3a PARTITION (ds='1')
    SELECT * FROM src;

INSERT OVERWRITE TABLE orcfile_merge3a PARTITION (ds='2')
    SELECT * FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge3a/ds=1/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge3a/ds=2/;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
EXPLAIN INSERT OVERWRITE TABLE orcfile_merge3b
    SELECT key, value FROM orcfile_merge3a;
INSERT OVERWRITE TABLE orcfile_merge3b
    SELECT key, value FROM orcfile_merge3a;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM orcfile_merge3a
) t;

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapred.max.split.size=100;
set mapref.min.split.size=1;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM orcfile_merge3b
) t;

DROP TABLE orcfile_merge3a;
DROP TABLE orcfile_merge3b;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

create table orc_merge5 (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;
create table orc_merge5b (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_merge5;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=50000;
SET hive.optimize.index.filter=true;
set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.compute.splits.in.am=true;
set tez.grouping.min-size=1000;
set tez.grouping.max-size=50000;
set hive.merge.sparkfiles=false;

-- 3 mappers
explain insert overwrite table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert overwrite table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;

-- 3 files total
analyze table orc_merge5b compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5b/;
select * from orc_merge5b;

set hive.merge.orcfile.stripe.level=true;
set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;

-- 3 mappers
explain insert overwrite table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert overwrite table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;

-- 1 file after merging
analyze table orc_merge5b compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5b/;
select * from orc_merge5b;

set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;

insert overwrite table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
analyze table orc_merge5b compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5b/;
select * from orc_merge5b;

set hive.merge.orcfile.stripe.level=true;
explain alter table orc_merge5b concatenate;
alter table orc_merge5b concatenate;

-- 1 file after merging
analyze table orc_merge5b compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5b/;
select * from orc_merge5b;

set hive.explain.user=false;
-- SORT_QUERY_RESULTS

-- orc file merge tests for static partitions
create table orc_merge5 (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;
create table orc_merge5a (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) partitioned by (year string, hour int) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_merge5;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=50000;
SET hive.optimize.index.filter=true;
set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.compute.splits.in.am=true;
set tez.grouping.min-size=1000;
set tez.grouping.max-size=50000;
set hive.merge.sparkfiles=false;

-- 3 mappers
explain insert overwrite table orc_merge5a partition (year="2000",hour=24) select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert overwrite table orc_merge5a partition (year="2000",hour=24) select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert overwrite table orc_merge5a partition (year="2001",hour=24) select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;

-- 3 files total
analyze table orc_merge5a partition(year="2000",hour=24) compute statistics noscan;
analyze table orc_merge5a partition(year="2001",hour=24) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/year=2000/hour=24/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/year=2001/hour=24/;
show partitions orc_merge5a;
select * from orc_merge5a;

set hive.merge.orcfile.stripe.level=true;
set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;

-- 3 mappers
explain insert overwrite table orc_merge5a partition (year="2000",hour=24) select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert overwrite table orc_merge5a partition (year="2000",hour=24) select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert overwrite table orc_merge5a partition (year="2001",hour=24) select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;

-- 1 file after merging
analyze table orc_merge5a partition(year="2000",hour=24) compute statistics noscan;
analyze table orc_merge5a partition(year="2001",hour=24) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/year=2000/hour=24/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/year=2001/hour=24/;
show partitions orc_merge5a;
select * from orc_merge5a;

set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;

insert overwrite table orc_merge5a partition (year="2000",hour=24) select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert overwrite table orc_merge5a partition (year="2001",hour=24) select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
analyze table orc_merge5a partition(year="2000",hour=24) compute statistics noscan;
analyze table orc_merge5a partition(year="2001",hour=24) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/year=2000/hour=24/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/year=2001/hour=24/;
show partitions orc_merge5a;
select * from orc_merge5a;

set hive.merge.orcfile.stripe.level=true;
explain alter table orc_merge5a partition(year="2000",hour=24) concatenate;
alter table orc_merge5a partition(year="2000",hour=24) concatenate;
alter table orc_merge5a partition(year="2001",hour=24) concatenate;

-- 1 file after merging
analyze table orc_merge5a partition(year="2000",hour=24) compute statistics noscan;
analyze table orc_merge5a partition(year="2001",hour=24) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/year=2000/hour=24/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/year=2001/hour=24/;
show partitions orc_merge5a;
select * from orc_merge5a;

set hive.explain.user=false;
-- SORT_QUERY_RESULTS

-- orc merge file tests for dynamic partition case

create table orc_merge5 (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;
create table orc_merge5a (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) partitioned by (st double) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_merge5;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=50000;
SET hive.optimize.index.filter=true;
set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.compute.splits.in.am=true;
set tez.grouping.min-size=1000;
set tez.grouping.max-size=50000;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.optimize.sort.dynamic.partition=false;
set hive.merge.sparkfiles=false;

-- 3 mappers
explain insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;
insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;
insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;

-- 3 files total
analyze table orc_merge5a partition(st=80.0) compute statistics noscan;
analyze table orc_merge5a partition(st=0.8) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=80.0/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=0.8/;
show partitions orc_merge5a;
select * from orc_merge5a where userid<=13;

set hive.merge.orcfile.stripe.level=true;
set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;

-- 3 mappers
explain insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;
insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;
insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;

-- 1 file after merging
analyze table orc_merge5a partition(st=80.0) compute statistics noscan;
analyze table orc_merge5a partition(st=0.8) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=80.0/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=0.8/;
show partitions orc_merge5a;
select * from orc_merge5a where userid<=13;

set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;

insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;
insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;
analyze table orc_merge5a partition(st=80.0) compute statistics noscan;
analyze table orc_merge5a partition(st=0.8) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=80.0/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=0.8/;
show partitions orc_merge5a;
select * from orc_merge5a where userid<=13;

set hive.merge.orcfile.stripe.level=true;
explain alter table orc_merge5a partition(st=80.0) concatenate;
alter table orc_merge5a partition(st=80.0) concatenate;
alter table orc_merge5a partition(st=0.8) concatenate;

-- 1 file after merging
analyze table orc_merge5a partition(st=80.0) compute statistics noscan;
analyze table orc_merge5a partition(st=0.8) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=80.0/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=0.8/;
show partitions orc_merge5a;
select * from orc_merge5a where userid<=13;

create table if not exists alltypes (
 bo boolean,
 ti tinyint,
 si smallint,
 i int,
 bi bigint,
 f float,
 d double,
 de decimal(10,3),
 ts timestamp,
 da date,
 s string,
 c char(5),
 vc varchar(5),
 m map<string, string>,
 l array<int>,
 st struct<c1:int, c2:string>
) row format delimited fields terminated by '|'
collection items terminated by ','
map keys terminated by ':' stored as textfile;

create table alltypes_orc like alltypes;
alter table alltypes_orc set fileformat orc;

load data local inpath '../../data/files/alltypes2.txt' overwrite into table alltypes;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;

insert overwrite table alltypes_orc select * from alltypes;
insert into table alltypes_orc select * from alltypes;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/alltypes_orc/;

set hive.merge.orcfile.stripe.level=true;
set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;

alter table alltypes_orc concatenate;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/alltypes_orc/;
create table ts_merge (
userid bigint,
string1 string,
subtype double,
decimal1 decimal(38,18),
ts timestamp
) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' overwrite into table ts_merge;
load data local inpath '../../data/files/orc_split_elim.orc' into table ts_merge;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/ts_merge/;

set hive.merge.orcfile.stripe.level=true;
set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;

select count(*) from ts_merge;
alter table ts_merge concatenate;
select count(*) from ts_merge;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/ts_merge/;

-- incompatible merge test (stripe statistics missing)

create table a_merge like alltypesorc;

insert overwrite table a_merge select * from alltypesorc;
load data local inpath '../../data/files/alltypesorc' into table a_merge;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/a_merge/;

select count(*) from a_merge;
alter table a_merge concatenate;
select count(*) from a_merge;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/a_merge/;

insert into table a_merge select * from alltypesorc;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/a_merge/;

select count(*) from a_merge;
alter table a_merge concatenate;
select count(*) from a_merge;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/a_merge/;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.merge.orcfile.stripe.level=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.optimize.sort.dynamic.partition=false;
set mapred.min.split.size=1000;
set mapred.max.split.size=2000;
set tez.grouping.min-size=1000;
set tez.grouping.max-size=2000;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;

set hive.metastore.warehouse.dir=pfile://${system:test.tmp.dir}/orc_merge_diff_fs;

-- SORT_QUERY_RESULTS

DROP TABLE orcfile_merge1;
DROP TABLE orcfile_merge1b;
DROP TABLE orcfile_merge1c;

CREATE TABLE orcfile_merge1 (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC;
CREATE TABLE orcfile_merge1b (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC;
CREATE TABLE orcfile_merge1c (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS ORC;

-- merge disabled
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1 PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1 PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1/ds=1/part=0/;

set hive.merge.tezfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.sparkfiles=true;
-- auto-merge slow way
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1b PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1b PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1b/ds=1/part=0/;

set hive.merge.orcfile.stripe.level=true;
-- auto-merge fast way
EXPLAIN
    INSERT OVERWRITE TABLE orcfile_merge1c PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 2) as part
        FROM src;

INSERT OVERWRITE TABLE orcfile_merge1c PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 2) as part
    FROM src;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orcfile_merge1c/ds=1/part=0/;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
-- Verify
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1 WHERE ds='1'
) t;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1b WHERE ds='1'
) t;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM orcfile_merge1c WHERE ds='1'
) t;

select count(*) from orcfile_merge1;
select count(*) from orcfile_merge1b;
select count(*) from orcfile_merge1c;

DROP TABLE orcfile_merge1;
DROP TABLE orcfile_merge1b;
DROP TABLE orcfile_merge1c;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

create table orc_merge5 (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;
create table orc_merge5b (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_merge5;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.merge.orcfile.stripe.level=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;

-- 3 mappers
explain insert overwrite table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
set hive.exec.orc.write.format=0.12;
insert overwrite table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert into table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert into table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
set hive.exec.orc.write.format=0.11;
insert into table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert into table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;
insert into table orc_merge5b select userid,string1,subtype,decimal1,ts from orc_merge5 where userid<=13;

-- 5 files total
analyze table orc_merge5b compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5b/;
select * from orc_merge5b;

set hive.merge.orcfile.stripe.level=true;
alter table orc_merge5b concatenate;

-- 3 file after merging - all 0.12 format files will be merged and 0.11 files will be left behind
analyze table orc_merge5b compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5b/;
select * from orc_merge5b;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

-- orc merge file tests for dynamic partition case

create table orc_merge5 (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;
create table orc_merge5a (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) partitioned by (st double) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_merge5;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=50000;
SET hive.optimize.index.filter=true;
set hive.merge.orcfile.stripe.level=false;
set hive.merge.tezfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.compute.splits.in.am=true;
set tez.am.grouping.min-size=1000;
set tez.am.grouping.max-size=50000;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.optimize.sort.dynamic.partition=false;
set hive.merge.sparkfiles=false;

explain insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5;
set hive.exec.orc.default.row.index.stride=1000;
insert overwrite table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5 order by userid;
insert into table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5 order by userid;
set hive.exec.orc.default.row.index.stride=2000;
insert into table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5 order by userid;
insert into table orc_merge5a partition (st) select userid,string1,subtype,decimal1,ts,subtype from orc_merge5 order by userid;

analyze table orc_merge5a partition(st=80.0) compute statistics noscan;
analyze table orc_merge5a partition(st=0.8) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=80.0/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=0.8/;
show partitions orc_merge5a;
select * from orc_merge5a where userid<=13;

set hive.merge.orcfile.stripe.level=true;
explain alter table orc_merge5a partition(st=80.0) concatenate;
alter table orc_merge5a partition(st=80.0) concatenate;
alter table orc_merge5a partition(st=0.8) concatenate;

analyze table orc_merge5a partition(st=80.0) compute statistics noscan;
analyze table orc_merge5a partition(st=0.8) compute statistics noscan;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=80.0/;
dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/orc_merge5a/st=0.8/;
show partitions orc_merge5a;
select * from orc_merge5a where userid<=13;

create table concat_incompat like alltypesorc;

load data local inpath '../../data/files/alltypesorc' into table concat_incompat;
load data local inpath '../../data/files/alltypesorc' into table concat_incompat;
load data local inpath '../../data/files/alltypesorc' into table concat_incompat;
load data local inpath '../../data/files/alltypesorc' into table concat_incompat;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/concat_incompat/;
select count(*) from concat_incompat;

ALTER TABLE concat_incompat CONCATENATE;

dfs -ls ${hiveconf:hive.metastore.warehouse.dir}/concat_incompat/;
select count(*) from concat_incompat;
create table if not exists alltypes (
 bo boolean,
 ti tinyint,
 si smallint,
 i int,
 bi bigint,
 f float,
 d double,
 de decimal(10,3),
 ts timestamp,
 da date,
 s string,
 c char(5),
 vc varchar(5),
 m map<string, string>,
 l array<int>,
 st struct<c1:int, c2:string>
) row format delimited fields terminated by '|'
collection items terminated by ','
map keys terminated by ':' stored as textfile;

create table alltypes_orc like alltypes;
alter table alltypes_orc set fileformat orc;

load data local inpath '../../data/files/alltypes2.txt' overwrite into table alltypes;

insert overwrite table alltypes_orc select * from alltypes;

select min(bo), max(bo), min(ti), max(ti), min(si), max(si), min(i), max(i), min(bi), max(bi), min(f), max(f), min(d), max(d), min(de), max(de), min(ts), max(ts), min(da), max(da), min(s), max(s), min(c), max(c), min(vc), max(vc) from alltypes;

select min(bo), max(bo), min(ti), max(ti), min(si), max(si), min(i), max(i), min(bi), max(bi), min(f), max(f), min(d), max(d), min(de), max(de), min(ts), max(ts), min(da), max(da), min(s), max(s), min(c), max(c), min(vc), max(vc) from alltypes_orc;

create table listtable(l array<string>);
create table listtable_orc(l array<string>) stored as orc;

insert overwrite table listtable select array(null) from src;
insert overwrite table listtable_orc select * from listtable;

select size(l) from listtable_orc limit 10;

set hive.mapred.mode=nonstrict;
SET hive.fetch.task.conversion=none;
SET hive.optimize.index.filter=true;
SET hive.cbo.enable=false;

CREATE TABLE staging(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE staging;
LOAD DATA LOCAL INPATH '../../data/files/over1k' INTO TABLE staging;

CREATE TABLE orc_ppd_staging(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           c char(50),
           v varchar(50),
           da date,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS ORC tblproperties("orc.row.index.stride" = "1000", "orc.bloom.filter.columns"="*");

insert overwrite table orc_ppd_staging select t, si, i, b, f, d, bo, s, cast(s as char(50)), cast(s as varchar(50)), cast(ts as date), ts, dec, bin from staging order by t, s;

-- just to introduce a gap in min/max range for bloom filters. The dataset has contiguous values
-- which makes it hard to test bloom filters
insert into orc_ppd_staging select -10,-321,-65680,-4294967430,-97.94,-13.07,true,"aaa","aaa","aaa","1990-03-11","1990-03-11 10:11:58.703308",-71.54,"aaa" from staging limit 1;
insert into orc_ppd_staging select 127,331,65690,4294967440,107.94,23.07,true,"zzz","zzz","zzz","2023-03-11","2023-03-11 10:11:58.703308",71.54,"zzz" from staging limit 1;

CREATE TABLE orc_ppd(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           c char(50),
           v varchar(50),
           da date,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS ORC tblproperties("orc.row.index.stride" = "1000", "orc.bloom.filter.columns"="*");

insert overwrite table orc_ppd select t, si, i, b, f, d, bo, s, cast(s as char(50)), cast(s as varchar(50)), cast(ts as date), ts, dec, bin from orc_ppd_staging order by t, s;

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter;

-- Row group statistics for column t:
-- Entry 0: count: 994 hasNull: true min: -10 max: 54 sum: 26014 positions: 0,0,0,0,0,0,0
-- Entry 1: count: 1000 hasNull: false min: 54 max: 118 sum: 86812 positions: 0,2,124,0,0,116,11
-- Entry 2: count: 100 hasNull: false min: 118 max: 127 sum: 12151 positions: 0,4,119,0,0,244,19

-- INPUT_RECORDS: 2100 (all row groups)
select count(*) from orc_ppd;

-- INPUT_RECORDS: 0 (no row groups)
select count(*) from orc_ppd where t > 127;

-- INPUT_RECORDS: 1000 (1 row group)
select count(*) from orc_ppd where t = 55;
select count(*) from orc_ppd where t <=> 50;
select count(*) from orc_ppd where t <=> 100;

-- INPUT_RECORDS: 2000 (2 row groups)
select count(*) from orc_ppd where t = "54";

-- INPUT_RECORDS: 1000 (1 row group)
select count(*) from orc_ppd where t = -10.0;

-- INPUT_RECORDS: 1000 (1 row group)
select count(*) from orc_ppd where t = cast(53 as float);
select count(*) from orc_ppd where t = cast(53 as double);

-- INPUT_RECORDS: 2000 (2 row groups)
select count(*) from orc_ppd where t < 100;

-- INPUT_RECORDS: 1000 (1 row group)
select count(*) from orc_ppd where t < 100 and t > 98;

-- INPUT_RECORDS: 2000 (2 row groups)
select count(*) from orc_ppd where t <= 100;

-- INPUT_RECORDS: 1000 (1 row group)
select count(*) from orc_ppd where t is null;

-- INPUT_RECORDS: 1100 (2 row groups)
select count(*) from orc_ppd where t in (5, 120);

-- INPUT_RECORDS: 1000 (1 row group)
select count(*) from orc_ppd where t between 60 and 80;

-- bloom filter tests
-- INPUT_RECORDS: 0
select count(*) from orc_ppd where t = -100;
select count(*) from orc_ppd where t <=> -100;
select count(*) from orc_ppd where t = 125;
select count(*) from orc_ppd where t IN (-100, 125, 200);

-- Row group statistics for column s:
-- Entry 0: count: 1000 hasNull: false min:  max: zach young sum: 12907 positions: 0,0,0
-- Entry 1: count: 1000 hasNull: false min: alice allen max: zach zipper sum: 12704 positions: 0,1611,191
-- Entry 2: count: 100 hasNull: false min: bob davidson max: zzz sum: 1281 positions: 0,3246,373

-- INPUT_RECORDS: 0 (no row groups)
select count(*) from orc_ppd where s > "zzz";

-- INPUT_RECORDS: 1000 (1 row group)
select count(*) from orc_ppd where s = "zach young";
select count(*) from orc_ppd where s <=> "zach zipper";
select count(*) from orc_ppd where s <=> "";

-- INPUT_RECORDS: 0
select count(*) from orc_ppd where s is null;

-- INPUT_RECORDS: 2100
select count(*) from orc_ppd where s is not null;

-- INPUT_RECORDS: 0
select count(*) from orc_ppd where s = cast("zach young" as char(50));

-- INPUT_RECORDS: 1000 (1 row group)
select count(*) from orc_ppd where s = cast("zach young" as char(10));
select count(*) from orc_ppd where s = cast("zach young" as varchar(10));
select count(*) from orc_ppd where s = cast("zach young" as varchar(50));

-- INPUT_RECORDS: 2000 (2 row groups)
select count(*) from orc_ppd where s < "b";

-- INPUT_RECORDS: 2000 (2 row groups)
select count(*) from orc_ppd where s > "alice" and s < "bob";

-- INPUT_RECORDS: 2000 (2 row groups)
select count(*) from orc_ppd where s in ("alice allen", "");

-- INPUT_RECORDS: 2000 (2 row groups)
select count(*) from orc_ppd where s between "" and "alice allen";

-- INPUT_RECORDS: 100 (1 row group)
select count(*) from orc_ppd where s between "zz" and "zzz";

-- INPUT_RECORDS: 1100 (2 row groups)
select count(*) from orc_ppd where s between "zach zipper" and "zzz";

-- bloom filter tests
-- INPUT_RECORDS: 0
select count(*) from orc_ppd where s = "hello world";
select count(*) from orc_ppd where s <=> "apache hive";
select count(*) from orc_ppd where s IN ("a", "z");

-- INPUT_RECORDS: 100
select count(*) from orc_ppd where s = "sarah ovid";

-- INPUT_RECORDS: 1100
select count(*) from orc_ppd where s = "wendy king";

-- INPUT_RECORDS: 1000
select count(*) from orc_ppd where s = "wendy king" and t < 0;

-- INPUT_RECORDS: 100
select count(*) from orc_ppd where s = "wendy king" and t > 100;SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), b boolean) stored as orc tblproperties("orc.stripe.size"="16777216");

insert overwrite table newtypesorc select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, true from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, false from src src2) uniontbl;

set hive.optimize.index.filter=false;

-- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select sum(hash(*)) from newtypesorc where b=true;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where b=false;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where b!=true;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where b!=false;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where b<true;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where b<false;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where b<=true;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where b<=false;

set hive.mapred.mode=nonstrict;
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties("orc.stripe.size"="16777216");

insert overwrite table newtypesorc select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("1970-02-20" as date) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("1970-02-27" as date) from src src2) uniontbl;

set hive.optimize.index.filter=false;

-- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select sum(hash(*)) from newtypesorc where c="apple";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c="apple";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c!="apple";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c!="apple";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c<"hello";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c<"hello";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c<="hello";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c<="hello";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c="apple ";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c="apple ";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c in ("apple", "carrot");

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c in ("apple", "carrot");

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c in ("apple", "hello");

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c in ("apple", "hello");

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c in ("carrot");

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c in ("carrot");

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c between "apple" and "carrot";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c between "apple" and "carrot";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c between "apple" and "zombie";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c between "apple" and "zombie";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where c between "carrot" and "carrot1";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where c between "carrot" and "carrot1";

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties("orc.stripe.size"="16777216");

insert overwrite table newtypesorc select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("1970-02-20" as date) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("1970-02-27" as date) from src src2) uniontbl;

-- date data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select sum(hash(*)) from newtypesorc where da='1970-02-20';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da='1970-02-20';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da= date '1970-02-20';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da!='1970-02-20';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da!='1970-02-20';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da<'1970-02-27';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da<'1970-02-27';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da<'1970-02-29';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da<'1970-02-29';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da<'1970-02-15';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da<'1970-02-15';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da<='1970-02-20';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da<='1970-02-20';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da<='1970-02-27';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da<='1970-02-27';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19';
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties("orc.stripe.size"="16777216");

insert overwrite table newtypesorc select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("1970-02-20" as date) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("1970-02-27" as date) from src src2) uniontbl;

-- decimal data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select sum(hash(*)) from newtypesorc where d=0.22;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d=0.22;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d='0.22';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d='0.22';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d=cast('0.22' as float);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d=cast('0.22' as float);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d!=0.22;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d!=0.22;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d!='0.22';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d!='0.22';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<11.22;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<11.22;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<'11.22';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<'11.22';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<cast('11.22' as float);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<cast('11.22' as float);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<1;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<1;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<=11.22;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<=11.22;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<='11.22';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<='11.22';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<=cast('11.22' as decimal);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<=cast('11.22' as decimal);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<=11.22BD;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<=11.22BD;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d<=12;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d<=12;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0');

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0');

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22');

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22');

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0');

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0');

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d between 0 and 1;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d between 0 and 1;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d between 0 and 1000;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d between 0 and 1000;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d between 0 and '2.0';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d between 0 and '2.0';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10));
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.optimize.index.filter=true;
set hive.cbo.enable=false;

create table test_acid( i int, ts timestamp)
                      clustered by (i) into 2 buckets
                      stored as orc
                      tblproperties ('transactional'='true');
insert into table test_acid values (1, '2014-09-14 12:34:30');
delete from test_acid where ts = '2014-15-16 17:18:19.20';
select i,ts from test_acid where ts = '2014-15-16 17:18:19.20';
select i,ts from test_acid where ts <= '2014-09-14 12:34:30';
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypesorc(c char(10), v varchar(10), d date, ts timestamp) stored as orc tblproperties("orc.stripe.size"="16777216");

insert overwrite table newtypesorc select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), null, null from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), null, null from src src2) uniontbl;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts is null;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts is null;

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where d is null;

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where d is null;

set hive.cbo.enable=false;

create table orc_test( col1 varchar(15), col2 char(10)) stored as orc;
create table text_test( col1 varchar(15), col2 char(10));

insert into orc_test values ('val1', '1');
insert overwrite table text_test select * from orc_test;

explain select * from text_test where col2='1';
select * from text_test where col2='1';

set hive.optimize.index.filter=false;
select * from orc_test where col2='1';

set hive.optimize.index.filter=true;
select * from orc_test where col2='1';

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), ts timestamp) stored as orc tblproperties("orc.stripe.size"="16777216");

insert overwrite table newtypesorc select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("2011-01-01 01:01:01" as timestamp) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("2011-01-20 01:01:01" as timestamp) from src src2) uniontbl;

-- timestamp data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select sum(hash(*)) from newtypesorc where cast(ts as string)='2011-01-01 01:01:01';

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where cast(ts as string)='2011-01-01 01:01:01';

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts=cast('2011-01-01 01:01:01' as varchar(20));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts=cast('2011-01-01 01:01:01' as varchar(20));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts!=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts!=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts<cast('2011-01-20 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts<cast('2011-01-20 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts<cast('2011-01-22 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts<cast('2011-01-22 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts<cast('2010-10-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts<cast('2010-10-01 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts<=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts<=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts<=cast('2011-01-20 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts<=cast('2011-01-20 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts in (cast('2011-01-02 01:01:01' as timestamp), cast('2011-01-20 01:01:01' as timestamp));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts in (cast('2011-01-02 01:01:01' as timestamp), cast('2011-01-20 01:01:01' as timestamp));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts in (cast('2011-01-01 01:01:01' as timestamp), cast('2011-01-20 01:01:01' as timestamp));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts in (cast('2011-01-01 01:01:01' as timestamp), cast('2011-01-20 01:01:01' as timestamp));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts in (cast('2011-01-02 01:01:01' as timestamp), cast('2011-01-08 01:01:01' as timestamp));

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts in (cast('2011-01-02 01:01:01' as timestamp), cast('2011-01-08 01:01:01' as timestamp));

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2011-01-08 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2011-01-08 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2011-01-25 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2011-01-25 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2010-11-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2010-11-01 01:01:01' as timestamp);
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties("orc.stripe.size"="16777216");

insert overwrite table newtypesorc select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("1970-02-20" as date) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("1970-02-27" as date) from src src2) uniontbl;

set hive.optimize.index.filter=false;

-- varchar data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select sum(hash(*)) from newtypesorc where v="bee";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v="bee";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v!="bee";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v!="bee";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v<"world";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v<"world";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v<="world";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v<="world";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v="bee   ";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v="bee   ";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v in ("bee", "orange");

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v in ("bee", "orange");

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v in ("bee", "world");

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v in ("bee", "world");

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v in ("orange");

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v in ("orange");

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v between "bee" and "orange";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v between "bee" and "orange";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v between "bee" and "zombie";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v between "bee" and "zombie";

set hive.optimize.index.filter=false;
select sum(hash(*)) from newtypesorc where v between "orange" and "pine";

set hive.optimize.index.filter=true;
select sum(hash(*)) from newtypesorc where v between "orange" and "pine";

set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE orc_pred(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS ORC;

ALTER TABLE orc_pred SET SERDEPROPERTIES ('orc.row.index.stride' = '1000');

CREATE TABLE staging(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE staging;

INSERT INTO TABLE orc_pred select * from staging;

-- no predicate case. the explain plan should not have filter expression in table scan operator

SELECT SUM(HASH(t)) FROM orc_pred;

SET hive.optimize.index.filter=true;
SELECT SUM(HASH(t)) FROM orc_pred;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT SUM(HASH(t)) FROM orc_pred;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT SUM(HASH(t)) FROM orc_pred;
SET hive.optimize.index.filter=false;

-- all the following queries have predicates which are pushed down to table scan operator if
-- hive.optimize.index.filter is set to true. the explain plan should show filter expression
-- in table scan operator.

SELECT * FROM orc_pred WHERE t<2 limit 1;
SET hive.optimize.index.filter=true;
SELECT * FROM orc_pred WHERE t<2 limit 1;
SET hive.optimize.index.filter=false;

SELECT * FROM orc_pred WHERE t>2 limit 1;
SET hive.optimize.index.filter=true;
SELECT * FROM orc_pred WHERE t>2 limit 1;
SET hive.optimize.index.filter=false;

SELECT SUM(HASH(t)) FROM orc_pred
  WHERE t IS NOT NULL
  AND t < 0
  AND t > -2;

SET hive.optimize.index.filter=true;
SELECT SUM(HASH(t)) FROM orc_pred
  WHERE t IS NOT NULL
  AND t < 0
  AND t > -2;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT SUM(HASH(t)) FROM orc_pred
  WHERE t IS NOT NULL
  AND t < 0
  AND t > -2;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT SUM(HASH(t)) FROM orc_pred
  WHERE t IS NOT NULL
  AND t < 0
  AND t > -2;
SET hive.optimize.index.filter=false;

SELECT t, s FROM orc_pred
  WHERE t <=> -1
  AND s IS NOT NULL
  AND s LIKE 'bob%'
  ;

SET hive.optimize.index.filter=true;
SELECT t, s FROM orc_pred
  WHERE t <=> -1
  AND s IS NOT NULL
  AND s LIKE 'bob%'
  ;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT t, s FROM orc_pred
  WHERE t <=> -1
  AND s IS NOT NULL
  AND s LIKE 'bob%'
  ;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT t, s FROM orc_pred
  WHERE t <=> -1
  AND s IS NOT NULL
  AND s LIKE 'bob%'
  ;
SET hive.optimize.index.filter=false;

SELECT t, s FROM orc_pred
  WHERE s IS NOT NULL
  AND s LIKE 'bob%'
  AND t NOT IN (-1,-2,-3)
  AND t BETWEEN 25 AND 30
  SORT BY t,s;

set hive.optimize.index.filter=true;
SELECT t, s FROM orc_pred
  WHERE s IS NOT NULL
  AND s LIKE 'bob%'
  AND t NOT IN (-1,-2,-3)
  AND t BETWEEN 25 AND 30
  SORT BY t,s;
set hive.optimize.index.filter=false;

EXPLAIN SELECT t, s FROM orc_pred
  WHERE s IS NOT NULL
  AND s LIKE 'bob%'
  AND t NOT IN (-1,-2,-3)
  AND t BETWEEN 25 AND 30
  SORT BY t,s;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT t, s FROM orc_pred
  WHERE s IS NOT NULL
  AND s LIKE 'bob%'
  AND t NOT IN (-1,-2,-3)
  AND t BETWEEN 25 AND 30
  SORT BY t,s;
SET hive.optimize.index.filter=false;

SELECT t, si, d, s FROM orc_pred
  WHERE d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  ORDER BY s DESC
  LIMIT 3;

SET hive.optimize.index.filter=true;
SELECT t, si, d, s FROM orc_pred
  WHERE d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  ORDER BY s DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT t, si, d, s FROM orc_pred
  WHERE d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  ORDER BY s DESC
  LIMIT 3;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT t, si, d, s FROM orc_pred
  WHERE d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  ORDER BY s DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;

SELECT t, si, d, s FROM orc_pred
  WHERE t > 10
  AND t <> 101
  AND d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  SORT BY s DESC
  LIMIT 3;

SET hive.optimize.index.filter=true;
SELECT t, si, d, s FROM orc_pred
  WHERE t > 10
  AND t <> 101
  AND d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  SORT BY s DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT t, si, d, s FROM orc_pred
  WHERE t > 10
  AND t <> 101
  AND d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  SORT BY s DESC
  LIMIT 3;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT t, si, d, s FROM orc_pred
  WHERE t > 10
  AND t <> 101
  AND d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  SORT BY s DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;
SET hive.exec.schema.evolution=false;
set hive.fetch.task.conversion=more;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition.mode=nonstrict;

CREATE TABLE orc_partitioned(a INT, b STRING) partitioned by (ds string) STORED AS ORC;
insert into table orc_partitioned partition (ds = 'today') select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10;
insert into table orc_partitioned partition (ds = 'tomorrow') select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10;

-- Use the old change the SERDE trick to avoid ORC DDL checks... and remove a column on the end.
ALTER TABLE orc_partitioned SET SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';
ALTER TABLE orc_partitioned REPLACE COLUMNS (cint int);
ALTER TABLE orc_partitioned SET SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde';

SELECT * FROM orc_partitioned WHERE ds = 'today';
SELECT * FROM orc_partitioned WHERE ds = 'tomorrow';

SET hive.exec.schema.evolution=true;
create table src_orc (key tinyint, val string) stored as orc;
alter table src_orc change key k tinyint first;
SET hive.exec.schema.evolution=false;
create table src_orc (key tinyint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc change key k tinyint first;
SET hive.exec.schema.evolution=true;
create table src_orc (key tinyint, val string) stored as orc;
alter table src_orc change key k tinyint after val;
SET hive.exec.schema.evolution=false;
create table src_orc (key tinyint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc change key k tinyint after val;
SET hive.exec.schema.evolution=true;
create table src_orc (key tinyint, val string) stored as orc;
alter table src_orc replace columns (k int);
SET hive.exec.schema.evolution=false;
create table src_orc (key tinyint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc replace columns (k int);
SET hive.exec.schema.evolution=true;
create table src_orc (key tinyint, val string) stored as orc;
alter table src_orc replace columns (k smallint, val string);
SET hive.exec.schema.evolution=false;
create table src_orc (key tinyint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc replace columns (k smallint, val string);
SET hive.exec.schema.evolution=true;
create table src_orc (key smallint, val string) stored as orc;
alter table src_orc replace columns (k int, val string, z smallint);
alter table src_orc replace columns (k int, val string, z tinyint);
SET hive.exec.schema.evolution=false;
create table src_orc (key smallint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc replace columns (k int, val string, z smallint);
alter table src_orc replace columns (k int, val string, z tinyint);
set hive.fetch.task.conversion=none;
create table src_orc (key smallint, val string) stored as orc;
create table src_orc2 (key smallint, val string) stored as orc;

-- integer type widening
insert overwrite table src_orc select * from src;
select sum(hash(*)) from src_orc;

alter table src_orc change key key smallint;
select sum(hash(*)) from src_orc;

alter table src_orc change key key int;
select sum(hash(*)) from src_orc;

alter table src_orc change key key bigint;
select sum(hash(*)) from src_orc;

-- replace columns for adding columns and type widening
insert overwrite table src_orc2 select * from src;
select sum(hash(*)) from src_orc2;

alter table src_orc2 replace columns (k smallint, v string);
select sum(hash(*)) from src_orc2;

alter table src_orc2 replace columns (k int, v string);
select sum(hash(*)) from src_orc2;

alter table src_orc2 replace columns (k bigint, v string);
select sum(hash(*)) from src_orc2;

alter table src_orc2 replace columns (k bigint, v string, z int);
select sum(hash(*)) from src_orc2;

alter table src_orc2 replace columns (k bigint, v string, z bigint);
select sum(hash(*)) from src_orc2;

alter table src_orc2 replace columns (k bigint, v string, z bigint, y float);
select sum(hash(*)) from src_orc2;

-- SORT_QUERY_RESULTS

create table orc_split_elim (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) stored as orc;

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_split_elim;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;
SET hive.optimize.index.filter=false;

-- The above table will have 5 splits with the followings stats
--  Stripe 1:
--    Column 0: count: 5000
--    Column 1: count: 5000 min: 2 max: 100 sum: 499902
--    Column 2: count: 5000 min: foo max: zebra sum: 24998
--    Column 3: count: 5000 min: 0.8 max: 8.0 sum: 39992.8
--    Column 4: count: 5000 min: 0 max: 1.2 sum: 1.2
--    Column 5: count: 5000
--  Stripe 2:
--    Column 0: count: 5000
--    Column 1: count: 5000 min: 13 max: 100 sum: 499913
--    Column 2: count: 5000 min: bar max: zebra sum: 24998
--    Column 3: count: 5000 min: 8.0 max: 80.0 sum: 40072.0
--    Column 4: count: 5000 min: 0 max: 2.2 sum: 2.2
--    Column 5: count: 5000
--  Stripe 3:
--    Column 0: count: 5000
--    Column 1: count: 5000 min: 29 max: 100 sum: 499929
--    Column 2: count: 5000 min: cat max: zebra sum: 24998
--    Column 3: count: 5000 min: 8.0 max: 8.0 sum: 40000.0
--    Column 4: count: 5000 min: 0 max: 3.3 sum: 3.3
--    Column 5: count: 5000
--  Stripe 4:
--    Column 0: count: 5000
--    Column 1: count: 5000 min: 70 max: 100 sum: 499970
--    Column 2: count: 5000 min: dog max: zebra sum: 24998
--    Column 3: count: 5000 min: 1.8 max: 8.0 sum: 39993.8
--    Column 4: count: 5000 min: 0 max: 4.4 sum: 4.4
--    Column 5: count: 5000
--  Stripe 5:
--    Column 0: count: 5000
--    Column 1: count: 5000 min: 5 max: 100 sum: 499905
--    Column 2: count: 5000 min: eat max: zebra sum: 24998
--    Column 3: count: 5000 min: 0.8 max: 8.0 sum: 39992.8
--    Column 4: count: 5000 min: 0 max: 5.5 sum: 5.5
--    Column 5: count: 5000

-- 5 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=0;

SET hive.optimize.index.filter=true;
-- 0 mapper
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=0;
SET hive.optimize.index.filter=false;

-- 5 mappers. count should be 0
select count(*) from orc_split_elim where userid<=0;

SET hive.optimize.index.filter=true;
-- 0 mapper
select count(*) from orc_split_elim where userid<=0;
SET hive.optimize.index.filter=false;

-- 5 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=2;

SET hive.optimize.index.filter=true;
-- 1 mapper
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=2;
SET hive.optimize.index.filter=false;

-- 5 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=5;

SET hive.optimize.index.filter=true;
-- 2 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=5;
SET hive.optimize.index.filter=false;

-- 5 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=13;

SET hive.optimize.index.filter=true;
-- 3 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=13;
SET hive.optimize.index.filter=false;

-- 5 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=29;

SET hive.optimize.index.filter=true;
-- 4 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=29;
SET hive.optimize.index.filter=false;

-- 5 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=70;

SET hive.optimize.index.filter=true;
-- 5 mappers
select userid,string1,subtype,decimal1,ts from orc_split_elim where userid<=70;
SET hive.optimize.index.filter=false;

-- partitioned table
create table orc_split_elim_part (userid bigint, string1 string, subtype double, decimal1 decimal, ts timestamp) partitioned by (country string, year int) stored as orc;

alter table orc_split_elim_part add partition(country='us', year=2000);
alter table orc_split_elim_part add partition(country='us', year=2001);

load data local inpath '../../data/files/orc_split_elim.orc' into table orc_split_elim_part partition(country='us', year=2000);
load data local inpath '../../data/files/orc_split_elim.orc' into table orc_split_elim_part partition(country='us', year=2001);

-- 10 mapper - no split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=2 and country='us';

SET hive.optimize.index.filter=true;
-- 2 mapper - split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=2 and country='us';
SET hive.optimize.index.filter=false;

-- 10 mapper - no split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=2 and country='us' and (year=2000 or year=2001);

SET hive.optimize.index.filter=true;
-- 2 mapper - split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=2 and country='us' and (year=2000 or year=2001);
SET hive.optimize.index.filter=false;

-- 10 mapper - no split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=2 and country='us' and year=2000;

SET hive.optimize.index.filter=true;
-- 1 mapper - split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=2 and country='us' and year=2000;
SET hive.optimize.index.filter=false;

-- 10 mapper - no split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=5 and country='us';

SET hive.optimize.index.filter=true;
-- 4 mapper - split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=5 and country='us';
SET hive.optimize.index.filter=false;

-- 10 mapper - no split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=5 and country='us' and (year=2000 or year=2001);

SET hive.optimize.index.filter=true;
-- 4 mapper - split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=5 and country='us' and (year=2000 or year=2001);
SET hive.optimize.index.filter=false;

-- 10 mapper - no split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=5 and country='us' and year=2000;

SET hive.optimize.index.filter=true;
-- 2 mapper - split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=5 and country='us' and year=2000;
SET hive.optimize.index.filter=false;

-- 0 mapper - no split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=70 and country='in';
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=70 and country='us' and year=2002;

SET hive.optimize.index.filter=true;
-- 0 mapper - split elimination
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=70 and country='in';
select userid,string1,subtype,decimal1,ts from orc_split_elim_part where userid<=70 and country='us' and year=2002;
SET hive.optimize.index.filter=false;
SET hive.exec.schema.evolution=true;
create table src_orc (key tinyint, val string) stored as orc;
alter table src_orc change key key float;
SET hive.exec.schema.evolution=false;
create table src_orc (key tinyint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc change key key float;
SET hive.exec.schema.evolution=true;
create table src_orc (key smallint, val string) stored as orc;
desc src_orc;
alter table src_orc change key key smallint;
desc src_orc;
alter table src_orc change key key int;
desc src_orc;
alter table src_orc change key key bigint;
desc src_orc;
alter table src_orc change val val char(100);
SET hive.exec.schema.evolution=false;
create table src_orc (key smallint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
desc src_orc;
alter table src_orc change key key smallint;
desc src_orc;
alter table src_orc change key key int;
desc src_orc;
alter table src_orc change key key bigint;
desc src_orc;
alter table src_orc change val val char(100);
SET hive.exec.schema.evolution=true;
create table src_orc (key tinyint, val string) stored as orc;
alter table src_orc change key key smallint;
SET hive.exec.schema.evolution=false;
create table src_orc (key tinyint, val string) clustered by (key) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
alter table src_orc change key key smallint;
set hive.mapred.mode=nonstrict;
-- create table with 1000 rows
create table srcorc(key string, value string) stored as textfile;
insert overwrite table srcorc select * from src;
insert into table srcorc select * from src;

-- load table with each row group having 1000 rows and stripe 1 & 2 having 5000 & 2000 rows respectively
create table if not exists vectororc
(s1 string,
s2 string,
d double,
s3 string)
stored as ORC tblproperties("orc.row.index.stride"="1000", "orc.stripe.size"="100000", "orc.compress.size"="10000");

-- insert creates separate orc files
insert overwrite table vectororc select "apple", "a", rand(1), "zoo" from srcorc;
insert into table vectororc select null, "b", rand(2), "zoo" from srcorc;
insert into table vectororc select null, "c", rand(3), "zoo" from srcorc;
insert into table vectororc select "apple", "d", rand(4), "zoo" from srcorc;
insert into table vectororc select null, "e", rand(5), "z" from srcorc;
insert into table vectororc select "apple", "f", rand(6), "z" from srcorc;
insert into table vectororc select null, "g", rand(7), "zoo" from srcorc;

-- since vectororc table has multiple orc file we will load them into a single file using another table
create table if not exists testorc
(s1 string,
s2 string,
d double,
s3 string)
stored as ORC tblproperties("orc.row.index.stride"="1000", "orc.stripe.size"="100000", "orc.compress.size"="10000");
insert overwrite table testorc select * from vectororc order by s2;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.optimize.index.filter=true;

set hive.vectorized.execution.enabled=false;
-- row group (1,4) from stripe 1 and row group (1) from stripe 2
-- PPD ONLY
select count(*),int(sum(d)) from testorc where s1 is not null;
set hive.vectorized.execution.enabled=true;
-- VECTORIZATION + PPD
select count(*),int(sum(d)) from testorc where s1 is not null;

set hive.vectorized.execution.enabled=false;
-- row group (2,3,5) from stripe 1 and row group (2) from stripe 2
-- PPD ONLY
select count(*),int(sum(d)) from testorc where s2 in ("b", "c", "e", "g");
set hive.vectorized.execution.enabled=true;
-- VECTORIZATION + PPD
select count(*),int(sum(d)) from testorc where s2 in ("b", "c", "e", "g");

set hive.vectorized.execution.enabled=false;
-- last row group of stripe 1 and first row group of stripe 2
-- PPD ONLY
select count(*),int(sum(d)) from testorc where s3="z";
set hive.vectorized.execution.enabled=true;
-- VECTORIZATION + PPD
select count(*),int(sum(d)) from testorc where s3="z";

set hive.vectorized.execution.enabled=false;
-- first row group of stripe 1 and last row group of stripe 2
-- PPD ONLY
select count(*),int(sum(d)) from testorc where s2="a" or s2="g";
set hive.vectorized.execution.enabled=true;
-- VECTORIZATION + PPD
select count(*),int(sum(d)) from testorc where s2="a" or s2="g";

drop table srcorc;
drop table vectororc;
drop table testorc;
set hive.mapred.mode=nonstrict;
drop table if exists test_txt;
drop table if exists test_orc;
create table test_txt(
	c1 varchar(64),
	c2 int,
	c3 int,
	c4 char(4),
	c5 decimal(16, 10),
	c6 boolean,
	c7 float,
	c8 int,
	c9 varchar(64),
	c10 string,
	c11 int,
	c12 boolean,
	c13 int,
	c14 string,
	c15 boolean,
	c16 int,
	c17 varchar(64),
	c18 int,
	c19 int,
	c20 string,
	c21 string,
	c22 decimal(16, 10),
	c23 int,
	c24 char(4),
	c25 varchar(64),
	c26 boolean,
	c27 string,
	c28 varchar(64),
	c29 boolean,
	c30 decimal(16, 10),
	c31 varchar(64),
	c32 varchar(64),
	c33 varchar(64),
	c34 decimal(16, 10),
	c35 char(4),
	c36 string,
	c37 int,
	c38 float,
	c39 float,
	c40 varchar(64),
	c41 int,
	c42 int,
	c43 varchar(64),
	c44 char(4),
	c45 int,
	c46 int,
	c47 int,
	c48 boolean,
	c49 int,
	c50 float,
	c51 char(4),
	c52 float,
	c53 int,
	c54 int,
	c55 decimal(16, 10),
	c56 float,
	c57 string,
	c58 varchar(64),
	c59 int,
	c60 boolean,
	c61 varchar(64),
	c62 decimal(16, 10),
	c63 int,
	c64 float,
	c65 char(4),
	c66 boolean,
	c67 decimal(16, 10),
	c68 int,
	c69 decimal(16, 10),
	c70 char(4),
	c71 decimal(16, 10),
	c72 string,
	c73 string,
	c74 int,
	c75 int,
	c76 float,
	c77 char(4),
	c78 varchar(64),
	c79 decimal(16, 10),
	c80 int,
	c81 int,
	c82 decimal(16, 10),
	c83 int,
	c84 varchar(64),
	c85 decimal(16, 10),
	c86 int,
	c87 float,
	c88 decimal(16, 10),
	c89 char(4),
	c90 decimal(16, 10),
	c91 int,
	c92 string,
	c93 int,
	c94 int,
	c95 int,
	c96 int,
	c97 int,
	c98 decimal(16, 10),
	c99 float,
	c100 boolean,
	c101 varchar(64),
	c102 int,
	c103 float,
	c104 varchar(64),
	c105 decimal(16, 10),
	c106 decimal(16, 10),
	c107 char(4),
	c108 char(4),
	c109 decimal(16, 10),
	c110 float,
	c111 float,
	c112 decimal(16, 10),
	c113 string,
	c114 varchar(64),
	c115 varchar(64),
	c116 float,
	c117 int,
	c118 int,
	c119 int,
	c120 int,
	c121 char(4),
	c122 int,
	c123 float,
	c124 varchar(64),
	c125 string,
	c126 string,
	c127 int,
	c128 varchar(64),
	c129 varchar(64),
	c130 float,
	c131 char(4),
	c132 varchar(64),
	c133 varchar(64),
	c134 decimal(16, 10),
	c135 varchar(64),
	c136 char(4),
	c137 int,
	c138 decimal(16, 10),
	c139 char(4),
	c140 decimal(16, 10),
	c141 string,
	c142 char(4),
	c143 boolean,
	c144 varchar(64),
	c145 varchar(64),
	c146 int,
	c147 decimal(16, 10),
	c148 varchar(64),
	c149 float,
	c150 int,
	c151 string,
	c152 string,
	c153 varchar(64),
	c154 int,
	c155 varchar(64),
	c156 int,
	c157 float,
	c158 decimal(16, 10),
	c159 varchar(64),
	c160 int,
	c161 int,
	c162 decimal(16, 10),
	c163 varchar(64),
	c164 float,
	c165 char(4),
	c166 float,
	c167 char(4),
	c168 float,
	c169 varchar(64),
	c170 char(4),
	c171 int,
	c172 int,
	c173 char(4),
	c174 int,
	c175 char(4),
	c176 int,
	c177 int,
	c178 boolean,
	c179 char(4),
	c180 string,
	c181 int,
	c182 string,
	c183 int,
	c184 int,
	c185 float,
	c186 varchar(64),
	c187 int,
	c188 int,
	c189 int,
	c190 decimal(16, 10),
	c191 char(4),
	c192 char(4),
	c193 varchar(64),
	c194 float,
	c195 boolean,
	c196 varchar(64),
	c197 varchar(64),
	c198 float,
	c199 char(4),
	c200 decimal(16, 10),
	c201 char(4),
	c202 int,
	c203 int,
	c204 string,
	c205 varchar(64),
	c206 char(4),
	c207 int,
	c208 int,
	c209 char(4),
	c210 varchar(64),
	c211 float,
	c212 int,
	c213 int,
	c214 char(4),
	c215 int,
	c216 float,
	c217 decimal(16, 10),
	c218 int,
	c219 varchar(64),
	c220 int,
	c221 varchar(64),
	c222 varchar(64),
	c223 int,
	c224 varchar(64),
	c225 int,
	c226 float,
	c227 int,
	c228 char(4),
	c229 char(4),
	c230 char(4),
	c231 int,
	c232 float,
	c233 int,
	c234 int,
	c235 varchar(64),
	c236 float,
	c237 varchar(64),
	c238 string,
	c239 varchar(64),
	c240 int,
	c241 int,
	c242 int,
	c243 char(4),
	c244 float,
	c245 int,
	c246 char(4),
	c247 float,
	c248 varchar(64),
	c249 varchar(64),
	c250 char(4),
	c251 int,
	c252 float,
	c253 int,
	c254 decimal(16, 10),
	c255 char(4),
	c256 int,
	c257 int,
	c258 string,
	c259 float,
	c260 varchar(64),
	c261 varchar(64),
	c262 int,
	c263 string,
	c264 varchar(64),
	c265 int,
	c266 boolean,
	c267 varchar(64),
	c268 varchar(64),
	c269 varchar(64),
	c270 int,
	c271 varchar(64),
	c272 int,
	c273 char(4),
	c274 int,
	c275 int,
	c276 char(4),
	c277 char(4),
	c278 decimal(16, 10),
	c279 varchar(64),
	c280 float,
	c281 string,
	c282 string,
	c283 int,
	c284 char(4),
	c285 float,
	c286 boolean,
	c287 char(4),
	c288 varchar(64),
	c289 int,
	c290 int,
	c291 int,
	c292 varchar(64),
	c293 varchar(64),
	c294 float,
	c295 int,
	c296 decimal(16, 10),
	c297 float,
	c298 float,
	c299 char(4),
	c300 decimal(16, 10),
	c301 char(4),
	c302 varchar(64),
	c303 varchar(64),
	c304 char(4),
	c305 int,
	c306 boolean,
	c307 float,
	c308 float,
	c309 int,
	c310 char(4),
	c311 char(4),
	c312 char(4),
	c313 char(4),
	c314 int,
	c315 int,
	c316 char(4),
	c317 char(4),
	c318 decimal(16, 10),
	c319 float,
	c320 float,
	c321 varchar(64),
	c322 string,
	c323 varchar(64),
	c324 varchar(64),
	c325 float,
	c326 varchar(64),
	c327 varchar(64),
	c328 float,
	c329 char(4),
	c330 int,
	c331 decimal(16, 10),
	c332 int,
	c333 varchar(64),
	c334 decimal(16, 10),
	c335 int,
	c336 varchar(64),
	c337 int,
	c338 varchar(64),
	c339 int,
	c340 int,
	c341 int,
	c342 char(4),
	c343 varchar(64),
	c344 decimal(16, 10),
	c345 int,
	c346 char(4),
	c347 int,
	c348 varchar(64),
	c349 int,
	c350 string,
	c351 char(4),
	c352 int,
	c353 int,
	c354 int,
	c355 int,
	c356 int,
	c357 float,
	c358 varchar(64),
	c359 boolean,
	c360 varchar(64),
	c361 int,
	c362 float,
	c363 char(4),
	c364 varchar(64),
	c365 int,
	c366 int,
	c367 decimal(16, 10),
	c368 decimal(16, 10),
	c369 int,
	c370 decimal(16, 10),
	c371 char(4),
	c372 char(4),
	c373 char(4),
	c374 string,
	c375 float,
	c376 boolean,
	c377 float,
	c378 varchar(64),
	c379 char(4),
	c380 varchar(64),
	c381 varchar(64),
	c382 char(4),
	c383 int,
	c384 char(4),
	c385 boolean,
	c386 float,
	c387 varchar(64),
	c388 string,
	c389 decimal(16, 10),
	c390 decimal(16, 10),
	c391 float,
	c392 boolean,
	c393 float,
	c394 int,
	c395 varchar(64),
	c396 decimal(16, 10),
	c397 decimal(16, 10),
	c398 varchar(64),
	c399 boolean,
	c400 float,
	c401 int,
	c402 int,
	c403 char(4),
	c404 float,
	c405 string,
	c406 varchar(64),
	c407 decimal(16, 10),
	c408 int,
	c409 varchar(64),
	c410 int,
	c411 int,
	c412 char(4),
	c413 float,
	c414 int,
	c415 char(4),
	c416 int,
	c417 int,
	c418 float,
	c419 int,
	c420 int,
	c421 int,
	c422 float,
	c423 varchar(64),
	c424 char(4),
	c425 varchar(64),
	c426 float,
	c427 int,
	c428 varchar(64),
	c429 float,
	c430 int,
	c431 char(4),
	c432 decimal(16, 10),
	c433 varchar(64),
	c434 int,
	c435 string,
	c436 int,
	c437 int,
	c438 float,
	c439 char(4),
	c440 int,
	c441 int,
	c442 varchar(64),
	c443 int,
	c444 int,
	c445 float,
	c446 int,
	c447 boolean,
	c448 int,
	c449 int,
	c450 char(4),
	c451 int,
	c452 boolean,
	c453 varchar(64),
	c454 char(4),
	c455 varchar(64),
	c456 int,
	c457 int,
	c458 varchar(64),
	c459 float,
	c460 boolean,
	c461 varchar(64),
	c462 char(4),
	c463 varchar(64),
	c464 int,
	c465 string,
	c466 varchar(64),
	c467 boolean,
	c468 string,
	c469 float,
	c470 float,
	c471 int,
	c472 varchar(64),
	c473 char(4),
	c474 decimal(16, 10),
	c475 char(4),
	c476 float,
	c477 char(4),
	c478 float,
	c479 int,
	c480 int,
	c481 int,
	c482 decimal(16, 10),
	c483 decimal(16, 10),
	c484 int,
	c485 int,
	c486 float,
	c487 char(4),
	c488 char(4),
	c489 varchar(64),
	c490 int,
	c491 varchar(64),
	c492 int,
	c493 decimal(16, 10),
	c494 int,
	c495 int,
	c496 int,
	c497 varchar(64),
	c498 decimal(16, 10),
	c499 float,
	c500 char(4),
	c501 decimal(16, 10),
	c502 char(4),
	c503 decimal(16, 10),
	c504 string,
	c505 boolean,
	c506 boolean,
	c507 char(4),
	c508 char(4),
	c509 int,
	c510 float,
	c511 decimal(16, 10),
	c512 float,
	c513 char(4),
	c514 varchar(64),
	c515 string,
	c516 char(4),
	c517 float,
	c518 int,
	c519 int,
	c520 decimal(16, 10),
	c521 int,
	c522 decimal(16, 10),
	c523 int,
	c524 boolean,
	c525 int,
	c526 int,
	c527 char(4),
	c528 varchar(64),
	c529 float,
	c530 decimal(16, 10),
	c531 char(4),
	c532 string,
	c533 char(4),
	c534 boolean,
	c535 boolean,
	c536 int,
	c537 float,
	c538 int,
	c539 int,
	c540 int,
	c541 decimal(16, 10),
	c542 int,
	c543 float,
	c544 float,
	c545 float,
	c546 boolean,
	c547 float,
	c548 float,
	c549 int,
	c550 int,
	c551 int,
	c552 int,
	c553 int,
	c554 float,
	c555 char(4),
	c556 char(4),
	c557 int,
	c558 int,
	c559 decimal(16, 10),
	c560 varchar(64),
	c561 int,
	c562 decimal(16, 10),
	c563 decimal(16, 10),
	c564 string,
	c565 decimal(16, 10),
	c566 int,
	c567 boolean,
	c568 char(4),
	c569 float,
	c570 string,
	c571 decimal(16, 10),
	c572 int,
	c573 float,
	c574 boolean,
	c575 float,
	c576 float,
	c577 varchar(64),
	c578 varchar(64),
	c579 int,
	c580 char(4),
	c581 varchar(64),
	c582 int,
	c583 decimal(16, 10),
	c584 char(4),
	c585 decimal(16, 10),
	c586 int,
	c587 varchar(64),
	c588 float,
	c589 int,
	c590 int,
	c591 char(4),
	c592 float,
	c593 varchar(64),
	c594 float,
	c595 varchar(64),
	c596 int,
	c597 int,
	c598 char(4),
	c599 varchar(64),
	c600 int,
	c601 int,
	c602 int,
	c603 string,
	c604 int,
	c605 float,
	c606 char(4),
	c607 boolean,
	c608 int,
	c609 int,
	c610 varchar(64),
	c611 float,
	c612 varchar(64),
	c613 int,
	c614 boolean,
	c615 int,
	c616 int,
	c617 float,
	c618 char(4),
	c619 decimal(16, 10),
	c620 varchar(64),
	c621 varchar(64),
	c622 int,
	c623 char(4),
	c624 int,
	c625 varchar(64),
	c626 int,
	c627 int,
	c628 char(4),
	c629 varchar(64),
	c630 char(4),
	c631 decimal(16, 10),
	c632 varchar(64),
	c633 varchar(64),
	c634 varchar(64),
	c635 float,
	c636 int,
	c637 varchar(64),
	c638 string,
	c639 int,
	c640 int,
	c641 int,
	c642 float,
	c643 char(4),
	c644 int,
	c645 varchar(64),
	c646 float,
	c647 varchar(64),
	c648 char(4),
	c649 decimal(16, 10),
	c650 char(4),
	c651 varchar(64),
	c652 varchar(64),
	c653 int,
	c654 decimal(16, 10),
	c655 float,
	c656 varchar(64),
	c657 int,
	c658 char(4),
	c659 float,
	c660 int,
	c661 int,
	c662 string,
	c663 int,
	c664 int,
	c665 char(4),
	c666 char(4),
	c667 float,
	c668 int,
	c669 char(4),
	c670 int,
	c671 int,
	c672 varchar(64),
	c673 int,
	c674 string,
	c675 varchar(64),
	c676 float,
	c677 varchar(64),
	c678 char(4),
	c679 varchar(64),
	c680 decimal(16, 10),
	c681 int,
	c682 int,
	c683 string,
	c684 varchar(64),
	c685 int,
	c686 string,
	c687 char(4),
	c688 varchar(64),
	c689 varchar(64),
	c690 string,
	c691 float,
	c692 int,
	c693 float,
	c694 int,
	c695 float,
	c696 char(4),
	c697 float,
	c698 int,
	c699 float,
	c700 varchar(64),
	c701 varchar(64),
	c702 int,
	c703 float,
	c704 char(4),
	c705 float,
	c706 float,
	c707 decimal(16, 10),
	c708 varchar(64),
	c709 char(4),
	c710 boolean,
	c711 varchar(64),
	c712 int,
	c713 boolean,
	c714 float,
	c715 int,
	c716 int,
	c717 int,
	c718 int,
	c719 char(4),
	c720 int,
	c721 varchar(64),
	c722 int,
	c723 int,
	c724 int,
	c725 varchar(64),
	c726 varchar(64),
	c727 int,
	c728 decimal(16, 10),
	c729 char(4),
	c730 int,
	c731 string,
	c732 decimal(16, 10),
	c733 boolean,
	c734 varchar(64),
	c735 char(4),
	c736 int,
	c737 int,
	c738 string,
	c739 decimal(16, 10),
	c740 varchar(64),
	c741 int,
	c742 varchar(64),
	c743 string,
	c744 char(4),
	c745 int,
	c746 char(4),
	c747 varchar(64),
	c748 int,
	c749 int,
	c750 int,
	c751 char(4),
	c752 boolean,
	c753 varchar(64),
	c754 float,
	c755 char(4),
	c756 char(4),
	c757 int,
	c758 int,
	c759 float,
	c760 varchar(64),
	c761 float,
	c762 boolean,
	c763 int,
	c764 int,
	c765 int,
	c766 int,
	c767 decimal(16, 10),
	c768 int,
	c769 int,
	c770 char(4),
	c771 varchar(64),
	c772 char(4),
	c773 decimal(16, 10),
	c774 varchar(64),
	c775 char(4),
	c776 float,
	c777 string,
	c778 int,
	c779 float,
	c780 char(4),
	c781 char(4),
	c782 char(4),
	c783 char(4),
	c784 int,
	c785 decimal(16, 10),
	c786 decimal(16, 10),
	c787 varchar(64),
	c788 int,
	c789 char(4),
	c790 char(4),
	c791 varchar(64),
	c792 int,
	c793 decimal(16, 10),
	c794 int,
	c795 int,
	c796 int,
	c797 int,
	c798 int,
	c799 int,
	c800 int,
	c801 varchar(64),
	c802 decimal(16, 10),
	c803 float,
	c804 int,
	c805 int,
	c806 int,
	c807 int,
	c808 char(4),
	c809 int,
	c810 float,
	c811 boolean,
	c812 decimal(16, 10),
	c813 int,
	c814 decimal(16, 10),
	c815 varchar(64),
	c816 varchar(64),
	c817 float,
	c818 float,
	c819 int,
	c820 varchar(64),
	c821 boolean,
	c822 int,
	c823 varchar(64),
	c824 varchar(64),
	c825 decimal(16, 10),
	c826 char(4),
	c827 varchar(64),
	c828 char(4),
	c829 float,
	c830 decimal(16, 10),
	c831 varchar(64),
	c832 varchar(64),
	c833 int,
	c834 float,
	c835 varchar(64),
	c836 int,
	c837 string,
	c838 char(4),
	c839 int,
	c840 int,
	c841 char(4),
	c842 int,
	c843 float,
	c844 int,
	c845 int,
	c846 boolean,
	c847 float,
	c848 decimal(16, 10),
	c849 int,
	c850 int,
	c851 int,
	c852 int,
	c853 int,
	c854 char(4),
	c855 varchar(64),
	c856 int,
	c857 int,
	c858 decimal(16, 10),
	c859 decimal(16, 10),
	c860 boolean,
	c861 int,
	c862 int,
	c863 float,
	c864 string,
	c865 int,
	c866 decimal(16, 10),
	c867 varchar(64),
	c868 char(4),
	c869 float,
	c870 string,
	c871 varchar(64),
	c872 char(4),
	c873 int,
	c874 int,
	c875 int,
	c876 decimal(16, 10),
	c877 float,
	c878 float,
	c879 decimal(16, 10),
	c880 decimal(16, 10),
	c881 int,
	c882 int,
	c883 boolean,
	c884 float,
	c885 char(4),
	c886 char(4),
	c887 decimal(16, 10),
	c888 float,
	c889 decimal(16, 10),
	c890 int,
	c891 varchar(64),
	c892 float,
	c893 int,
	c894 int,
	c895 float,
	c896 char(4),
	c897 boolean,
	c898 int,
	c899 int,
	c900 float,
	c901 int,
	c902 int,
	c903 int,
	c904 char(4),
	c905 decimal(16, 10),
	c906 int,
	c907 char(4),
	c908 int,
	c909 decimal(16, 10),
	c910 char(4),
	c911 decimal(16, 10),
	c912 varchar(64),
	c913 char(4),
	c914 boolean,
	c915 string,
	c916 varchar(64),
	c917 char(4),
	c918 int,
	c919 float,
	c920 char(4),
	c921 int,
	c922 char(4),
	c923 decimal(16, 10),
	c924 int,
	c925 float,
	c926 boolean,
	c927 int,
	c928 decimal(16, 10),
	c929 int,
	c930 decimal(16, 10),
	c931 int,
	c932 int,
	c933 varchar(64),
	c934 varchar(64),
	c935 int,
	c936 varchar(64),
	c937 string,
	c938 char(4),
	c939 decimal(16, 10),
	c940 int,
	c941 char(4),
	c942 int,
	c943 int,
	c944 varchar(64),
	c945 float,
	c946 int,
	c947 decimal(16, 10),
	c948 decimal(16, 10),
	c949 char(4),
	c950 int,
	c951 int,
	c952 varchar(64),
	c953 float,
	c954 boolean,
	c955 float,
	c956 varchar(64),
	c957 char(4),
	c958 char(4),
	c959 decimal(16, 10),
	c960 int,
	c961 int,
	c962 int,
	c963 float,
	c964 varchar(64),
	c965 char(4),
	c966 float,
	c967 char(4),
	c968 decimal(16, 10),
	c969 int,
	c970 varchar(64),
	c971 int,
	c972 varchar(64),
	c973 char(4),
	c974 int,
	c975 varchar(64),
	c976 decimal(16, 10),
	c977 boolean,
	c978 int,
	c979 int,
	c980 char(4),
	c981 decimal(16, 10),
	c982 int,
	c983 decimal(16, 10),
	c984 varchar(64),
	c985 int,
	c986 char(4),
	c987 decimal(16, 10),
	c988 string,
	c989 int,
	c990 float,
	c991 int,
	c992 int,
	c993 char(4),
	c994 decimal(16, 10),
	c995 boolean,
	c996 int,
	c997 int,
	c998 varchar(64),
	c999 char(4),
	c1000 int,
	c1001 boolean,
	c1002 varchar(64),
	c1003 varchar(64),
	c1004 float,
	c1005 float,
	c1006 int,
	c1007 decimal(16, 10),
	c1008 float,
	c1009 varchar(64),
	c1010 boolean,
	c1011 char(4),
	c1012 float,
	c1013 char(4),
	c1014 varchar(64),
	c1015 char(4),
	c1016 decimal(16, 10),
	c1017 char(4),
	c1018 float,
	c1019 varchar(64),
	c1020 int,
	c1021 float,
	c1022 decimal(16, 10),
	c1023 varchar(64),
	c1024 char(4),
	c1025 float,
	c1026 float,
	c1027 int,
	c1028 int,
	c1029 char(4),
	c1030 decimal(16, 10),
	c1031 char(4),
	c1032 varchar(64),
	c1033 varchar(64),
	c1034 char(4),
	c1035 varchar(64),
	c1036 decimal(16, 10),
	c1037 int,
	c1038 float,
	c1039 float,
	c1040 varchar(64),
	c1041 varchar(64),
	c1042 float,
	c1043 int,
	c1044 boolean,
	c1045 string,
	c1046 string,
	c1047 char(4),
	c1048 float,
	c1049 varchar(64),
	c1050 varchar(64),
	c1051 float,
	c1052 int,
	c1053 char(4),
	c1054 float,
	c1055 decimal(16, 10),
	c1056 string,
	c1057 char(4),
	c1058 varchar(64),
	c1059 char(4),
	c1060 int,
	c1061 float,
	c1062 string,
	c1063 varchar(64),
	c1064 int,
	c1065 varchar(64),
	c1066 char(4),
	c1067 char(4),
	c1068 string,
	c1069 string,
	c1070 varchar(64),
	c1071 char(4),
	c1072 int,
	c1073 float,
	c1074 varchar(64),
	c1075 float,
	c1076 char(4),
	c1077 decimal(16, 10),
	c1078 char(4),
	c1079 int,
	c1080 char(4),
	c1081 int,
	c1082 float,
	c1083 int,
	c1084 float,
	c1085 int,
	c1086 boolean,
	c1087 float,
	c1088 float,
	c1089 decimal(16, 10),
	c1090 float,
	c1091 decimal(16, 10),
	c1092 char(4),
	c1093 int,
	c1094 float,
	c1095 decimal(16, 10),
	c1096 varchar(64),
	c1097 decimal(16, 10),
	c1098 varchar(64),
	c1099 varchar(64),
	c1100 boolean,
	c1101 decimal(16, 10),
	c1102 char(4),
	c1103 char(4),
	c1104 decimal(16, 10),
	c1105 decimal(16, 10),
	c1106 boolean,
	c1107 varchar(64),
	c1108 char(4),
	c1109 float,
	c1110 decimal(16, 10),
	c1111 decimal(16, 10),
	c1112 int,
	c1113 int,
	c1114 float,
	c1115 string,
	c1116 char(4),
	c1117 float,
	c1118 varchar(64),
	c1119 int,
	c1120 varchar(64),
	c1121 int,
	c1122 varchar(64),
	c1123 int,
	c1124 int,
	c1125 char(4),
	c1126 int,
	c1127 varchar(64),
	c1128 int,
	c1129 char(4),
	c1130 boolean,
	c1131 varchar(64),
	c1132 float,
	c1133 char(4),
	c1134 float,
	c1135 int,
	c1136 float,
	c1137 string,
	c1138 int,
	c1139 int,
	c1140 float,
	c1141 char(4),
	c1142 float,
	c1143 int,
	c1144 decimal(16, 10),
	c1145 int,
	c1146 float,
	c1147 boolean,
	c1148 int,
	c1149 boolean,
	c1150 float,
	c1151 varchar(64),
	c1152 decimal(16, 10),
	c1153 string,
	c1154 int,
	c1155 string,
	c1156 string,
	c1157 varchar(64),
	c1158 varchar(64),
	c1159 decimal(16, 10),
	c1160 char(4),
	c1161 char(4),
	c1162 int,
	c1163 int,
	c1164 float,
	c1165 int,
	c1166 int,
	c1167 varchar(64),
	c1168 int,
	c1169 varchar(64),
	c1170 float,
	c1171 char(4),
	c1172 varchar(64),
	c1173 int,
	c1174 int,
	c1175 char(4),
	c1176 int,
	c1177 char(4),
	c1178 float,
	c1179 int,
	c1180 int,
	c1181 varchar(64),
	c1182 boolean,
	c1183 char(4),
	c1184 char(4),
	c1185 int,
	c1186 boolean,
	c1187 float,
	c1188 float,
	c1189 decimal(16, 10),
	c1190 varchar(64),
	c1191 float,
	c1192 int,
	c1193 varchar(64),
	c1194 float,
	c1195 decimal(16, 10),
	c1196 char(4),
	c1197 int,
	c1198 int,
	c1199 string,
	c1200 int,
	c1201 char(4),
	c1202 int,
	c1203 varchar(64),
	c1204 int,
	c1205 varchar(64),
	c1206 char(4),
	c1207 int,
	c1208 float,
	c1209 int,
	c1210 decimal(16, 10),
	c1211 decimal(16, 10),
	c1212 decimal(16, 10),
	c1213 int,
	c1214 int,
	c1215 varchar(64),
	c1216 boolean,
	c1217 decimal(16, 10),
	c1218 float,
	c1219 int,
	c1220 int,
	c1221 varchar(64),
	c1222 char(4),
	c1223 varchar(64),
	c1224 string,
	c1225 varchar(64),
	c1226 varchar(64),
	c1227 int,
	c1228 float,
	c1229 int,
	c1230 char(4),
	c1231 varchar(64),
	c1232 int,
	c1233 int,
	c1234 int,
	c1235 string,
	c1236 char(4),
	c1237 float,
	c1238 int,
	c1239 int,
	c1240 decimal(16, 10),
	c1241 char(4),
	c1242 string,
	c1243 int,
	c1244 int,
	c1245 int,
	c1246 float,
	c1247 char(4),
	c1248 varchar(64),
	c1249 int,
	c1250 int,
	c1251 int,
	c1252 varchar(64),
	c1253 float,
	c1254 decimal(16, 10),
	c1255 boolean,
	c1256 int,
	c1257 int,
	c1258 int,
	c1259 int,
	c1260 float,
	c1261 float,
	c1262 decimal(16, 10),
	c1263 varchar(64),
	c1264 int,
	c1265 varchar(64),
	c1266 float,
	c1267 char(4),
	c1268 float,
	c1269 float,
	c1270 decimal(16, 10),
	c1271 int,
	c1272 varchar(64),
	c1273 string,
	c1274 int,
	c1275 char(4),
	c1276 char(4),
	c1277 boolean,
	c1278 float,
	c1279 int,
	c1280 int,
	c1281 char(4),
	c1282 decimal(16, 10),
	c1283 boolean,
	c1284 decimal(16, 10),
	c1285 boolean,
	c1286 varchar(64),
	c1287 char(4),
	c1288 char(4),
	c1289 varchar(64),
	c1290 int,
	c1291 char(4),
	c1292 varchar(64),
	c1293 float,
	c1294 int,
	c1295 int,
	c1296 float,
	c1297 int,
	c1298 char(4),
	c1299 varchar(64),
	c1300 char(4),
	c1301 char(4),
	c1302 char(4),
	c1303 decimal(16, 10),
	c1304 varchar(64),
	c1305 varchar(64),
	c1306 char(4),
	c1307 varchar(64),
	c1308 int,
	c1309 decimal(16, 10),
	c1310 varchar(64),
	c1311 boolean,
	c1312 int,
	c1313 float,
	c1314 float,
	c1315 char(4),
	c1316 varchar(64),
	c1317 float,
	c1318 varchar(64),
	c1319 int,
	c1320 varchar(64),
	c1321 int,
	c1322 char(4),
	c1323 float,
	c1324 float,
	c1325 varchar(64),
	c1326 decimal(16, 10),
	c1327 varchar(64),
	c1328 varchar(64),
	c1329 char(4),
	c1330 char(4),
	c1331 float,
	c1332 int,
	c1333 char(4),
	c1334 int,
	c1335 varchar(64),
	c1336 int,
	c1337 char(4),
	c1338 decimal(16, 10),
	c1339 varchar(64),
	c1340 int,
	c1341 varchar(64),
	c1342 decimal(16, 10),
	c1343 char(4),
	c1344 int,
	c1345 float,
	c1346 float,
	c1347 int,
	c1348 char(4),
	c1349 varchar(64),
	c1350 int,
	c1351 char(4),
	c1352 int,
	c1353 int,
	c1354 float,
	c1355 varchar(64),
	c1356 int,
	c1357 float,
	c1358 varchar(64),
	c1359 varchar(64),
	c1360 decimal(16, 10),
	c1361 varchar(64),
	c1362 int,
	c1363 int,
	c1364 int,
	c1365 char(4),
	c1366 int,
	c1367 varchar(64),
	c1368 int,
	c1369 char(4),
	c1370 string,
	c1371 varchar(64),
	c1372 varchar(64),
	c1373 int,
	c1374 char(4),
	c1375 decimal(16, 10),
	c1376 float,
	c1377 char(4),
	c1378 float,
	c1379 float,
	c1380 int,
	c1381 decimal(16, 10),
	c1382 int,
	c1383 int,
	c1384 int,
	c1385 boolean,
	c1386 char(4),
	c1387 varchar(64),
	c1388 int,
	c1389 varchar(64),
	c1390 float,
	c1391 decimal(16, 10),
	c1392 string,
	c1393 varchar(64),
	c1394 int,
	c1395 decimal(16, 10),
	c1396 int,
	c1397 int,
	c1398 varchar(64),
	c1399 float,
	c1400 string,
	c1401 boolean,
	c1402 boolean,
	c1403 int,
	c1404 float,
	c1405 varchar(64),
	c1406 decimal(16, 10),
	c1407 decimal(16, 10),
	c1408 varchar(64),
	c1409 char(4),
	c1410 int,
	c1411 int,
	c1412 int,
	c1413 varchar(64),
	c1414 int,
	c1415 decimal(16, 10),
	c1416 int,
	c1417 varchar(64),
	c1418 varchar(64),
	c1419 int,
	c1420 char(4),
	c1421 string,
	c1422 decimal(16, 10),
	c1423 int,
	c1424 int,
	c1425 char(4),
	c1426 int,
	c1427 int,
	c1428 int,
	c1429 float,
	c1430 char(4),
	c1431 int,
	c1432 decimal(16, 10),
	c1433 int,
	c1434 float,
	c1435 int,
	c1436 decimal(16, 10),
	c1437 boolean,
	c1438 float,
	c1439 char(4),
	c1440 varchar(64),
	c1441 float,
	c1442 int,
	c1443 char(4),
	c1444 int,
	c1445 int,
	c1446 int,
	c1447 boolean,
	c1448 int,
	c1449 int,
	c1450 float,
	c1451 varchar(64),
	c1452 float,
	c1453 string,
	c1454 int,
	c1455 char(4),
	c1456 char(4),
	c1457 float,
	c1458 float,
	c1459 float,
	c1460 decimal(16, 10),
	c1461 float,
	c1462 int,
	c1463 boolean,
	c1464 decimal(16, 10),
	c1465 int,
	c1466 string,
	c1467 varchar(64),
	c1468 char(4),
	c1469 varchar(64),
	c1470 int,
	c1471 varchar(64),
	c1472 varchar(64),
	c1473 char(4),
	c1474 decimal(16, 10),
	c1475 int,
	c1476 int,
	c1477 int,
	c1478 float,
	c1479 string,
	c1480 varchar(64),
	c1481 float,
	c1482 int,
	c1483 char(4),
	c1484 int,
	c1485 float,
	c1486 float,
	c1487 float,
	c1488 varchar(64),
	c1489 varchar(64),
	c1490 int,
	c1491 char(4),
	c1492 varchar(64),
	c1493 char(4),
	c1494 int,
	c1495 char(4),
	c1496 varchar(64),
	c1497 char(4),
	c1498 decimal(16, 10),
	c1499 int,
	c1500 char(4),
	c1501 int,
	c1502 int,
	c1503 int,
	c1504 varchar(64),
	c1505 char(4),
	c1506 int,
	c1507 int,
	c1508 varchar(64),
	c1509 boolean,
	c1510 char(4),
	c1511 int,
	c1512 string,
	c1513 char(4),
	c1514 int,
	c1515 int,
	c1516 string,
	c1517 float,
	c1518 float,
	c1519 int,
	c1520 boolean,
	c1521 int,
	c1522 varchar(64),
	c1523 varchar(64),
	c1524 varchar(64),
	c1525 float,
	c1526 varchar(64),
	c1527 char(4),
	c1528 varchar(64),
	c1529 varchar(64),
	c1530 int,
	c1531 char(4),
	c1532 int,
	c1533 int,
	c1534 varchar(64),
	c1535 int,
	c1536 string,
	c1537 int,
	c1538 string,
	c1539 char(4),
	c1540 float,
	c1541 string,
	c1542 int,
	c1543 varchar(64),
	c1544 int,
	c1545 string,
	c1546 decimal(16, 10),
	c1547 varchar(64),
	c1548 char(4),
	c1549 decimal(16, 10),
	c1550 char(4),
	c1551 char(4),
	c1552 float,
	c1553 varchar(64),
	c1554 varchar(64),
	c1555 float,
	c1556 char(4),
	c1557 int,
	c1558 boolean,
	c1559 decimal(16, 10),
	c1560 string,
	c1561 string,
	c1562 varchar(64),
	c1563 float,
	c1564 float,
	c1565 int,
	c1566 boolean,
	c1567 int,
	c1568 char(4),
	c1569 varchar(64),
	c1570 boolean,
	c1571 int,
	c1572 int,
	c1573 int,
	c1574 int,
	c1575 float,
	c1576 float,
	c1577 decimal(16, 10),
	c1578 decimal(16, 10),
	c1579 float,
	c1580 float,
	c1581 int,
	c1582 decimal(16, 10),
	c1583 decimal(16, 10),
	c1584 int,
	c1585 float,
	c1586 decimal(16, 10),
	c1587 float,
	c1588 int,
	c1589 float,
	c1590 char(4),
	c1591 char(4),
	c1592 float,
	c1593 string,
	c1594 decimal(16, 10),
	c1595 int,
	c1596 varchar(64),
	c1597 boolean,
	c1598 float,
	c1599 int,
	c1600 float,
	c1601 float,
	c1602 char(4),
	c1603 boolean,
	c1604 float,
	c1605 char(4),
	c1606 int,
	c1607 string,
	c1608 int,
	c1609 int,
	c1610 char(4),
	c1611 string,
	c1612 char(4),
	c1613 float,
	c1614 int,
	c1615 int,
	c1616 varchar(64),
	c1617 varchar(64),
	c1618 varchar(64),
	c1619 int,
	c1620 float,
	c1621 varchar(64),
	c1622 float,
	c1623 int,
	c1624 float,
	c1625 varchar(64),
	c1626 int,
	c1627 int,
	c1628 varchar(64),
	c1629 int,
	c1630 decimal(16, 10),
	c1631 float,
	c1632 char(4),
	c1633 char(4),
	c1634 boolean,
	c1635 decimal(16, 10),
	c1636 decimal(16, 10),
	c1637 int,
	c1638 varchar(64),
	c1639 varchar(64),
	c1640 int,
	c1641 float,
	c1642 float,
	c1643 float,
	c1644 varchar(64),
	c1645 char(4),
	c1646 varchar(64),
	c1647 varchar(64),
	c1648 int,
	c1649 int,
	c1650 int,
	c1651 string,
	c1652 int,
	c1653 char(4),
	c1654 decimal(16, 10),
	c1655 float,
	c1656 string,
	c1657 char(4),
	c1658 float,
	c1659 int,
	c1660 decimal(16, 10),
	c1661 char(4),
	c1662 float,
	c1663 float,
	c1664 char(4),
	c1665 decimal(16, 10),
	c1666 char(4),
	c1667 string,
	c1668 char(4),
	c1669 varchar(64),
	c1670 decimal(16, 10),
	c1671 decimal(16, 10),
	c1672 int,
	c1673 int,
	c1674 int,
	c1675 int,
	c1676 char(4),
	c1677 char(4),
	c1678 varchar(64),
	c1679 varchar(64),
	c1680 decimal(16, 10),
	c1681 decimal(16, 10),
	c1682 varchar(64),
	c1683 int,
	c1684 int,
	c1685 varchar(64),
	c1686 string,
	c1687 varchar(64),
	c1688 int,
	c1689 int,
	c1690 varchar(64),
	c1691 string,
	c1692 decimal(16, 10),
	c1693 char(4),
	c1694 string,
	c1695 string,
	c1696 varchar(64),
	c1697 varchar(64),
	c1698 char(4),
	c1699 float,
	c1700 char(4),
	c1701 varchar(64),
	c1702 varchar(64),
	c1703 int,
	c1704 varchar(64),
	c1705 float,
	c1706 decimal(16, 10),
	c1707 char(4),
	c1708 boolean,
	c1709 string,
	c1710 varchar(64),
	c1711 float,
	c1712 int,
	c1713 varchar(64),
	c1714 varchar(64),
	c1715 int,
	c1716 varchar(64),
	c1717 int,
	c1718 int,
	c1719 varchar(64),
	c1720 int,
	c1721 varchar(64),
	c1722 string,
	c1723 char(4),
	c1724 char(4),
	c1725 float,
	c1726 boolean,
	c1727 int,
	c1728 float,
	c1729 varchar(64),
	c1730 varchar(64),
	c1731 int,
	c1732 varchar(64),
	c1733 int,
	c1734 int,
	c1735 varchar(64),
	c1736 char(4),
	c1737 char(4),
	c1738 boolean,
	c1739 varchar(64),
	c1740 string,
	c1741 char(4),
	c1742 varchar(64),
	c1743 char(4),
	c1744 decimal(16, 10),
	c1745 int,
	c1746 float,
	c1747 float,
	c1748 int,
	c1749 char(4),
	c1750 boolean,
	c1751 varchar(64),
	c1752 int,
	c1753 int,
	c1754 int,
	c1755 float,
	c1756 varchar(64),
	c1757 varchar(64),
	c1758 int,
	c1759 int,
	c1760 decimal(16, 10),
	c1761 int,
	c1762 int,
	c1763 int,
	c1764 char(4),
	c1765 decimal(16, 10),
	c1766 char(4),
	c1767 char(4),
	c1768 decimal(16, 10),
	c1769 string,
	c1770 varchar(64),
	c1771 varchar(64),
	c1772 float,
	c1773 int,
	c1774 int,
	c1775 varchar(64),
	c1776 decimal(16, 10),
	c1777 float,
	c1778 varchar(64),
	c1779 varchar(64),
	c1780 float,
	c1781 varchar(64),
	c1782 float,
	c1783 varchar(64),
	c1784 int,
	c1785 char(4),
	c1786 boolean,
	c1787 decimal(16, 10),
	c1788 int,
	c1789 int,
	c1790 int,
	c1791 float,
	c1792 varchar(64),
	c1793 varchar(64),
	c1794 decimal(16, 10),
	c1795 int,
	c1796 int,
	c1797 varchar(64),
	c1798 char(4),
	c1799 int,
	c1800 decimal(16, 10),
	c1801 int,
	c1802 int,
	c1803 int,
	c1804 float,
	c1805 float,
	c1806 char(4),
	c1807 varchar(64),
	c1808 int,
	c1809 string,
	c1810 char(4),
	c1811 float,
	c1812 int,
	c1813 varchar(64),
	c1814 varchar(64),
	c1815 int,
	c1816 char(4),
	c1817 boolean,
	c1818 char(4),
	c1819 string,
	c1820 int,
	c1821 float,
	c1822 string,
	c1823 varchar(64),
	c1824 varchar(64),
	c1825 char(4),
	c1826 int,
	c1827 varchar(64),
	c1828 varchar(64),
	c1829 char(4),
	c1830 decimal(16, 10),
	c1831 int,
	c1832 decimal(16, 10),
	c1833 int,
	c1834 decimal(16, 10),
	c1835 boolean,
	c1836 int,
	c1837 float,
	c1838 decimal(16, 10),
	c1839 decimal(16, 10),
	c1840 char(4),
	c1841 boolean,
	c1842 varchar(64),
	c1843 varchar(64),
	c1844 int,
	c1845 varchar(64),
	c1846 varchar(64),
	c1847 varchar(64),
	c1848 char(4),
	c1849 string,
	c1850 char(4),
	c1851 float,
	c1852 float,
	c1853 int,
	c1854 varchar(64),
	c1855 float,
	c1856 varchar(64),
	c1857 char(4),
	c1858 float,
	c1859 int,
	c1860 float,
	c1861 int,
	c1862 string,
	c1863 char(4),
	c1864 decimal(16, 10),
	c1865 string,
	c1866 varchar(64),
	c1867 char(4),
	c1868 float,
	c1869 varchar(64),
	c1870 varchar(64),
	c1871 char(4),
	c1872 int,
	c1873 float,
	c1874 int,
	c1875 float,
	c1876 int,
	c1877 float,
	c1878 decimal(16, 10),
	c1879 boolean,
	c1880 varchar(64),
	c1881 varchar(64),
	c1882 float,
	c1883 int,
	c1884 string,
	c1885 varchar(64),
	c1886 char(4),
	c1887 char(4),
	c1888 int,
	c1889 char(4),
	c1890 char(4),
	c1891 decimal(16, 10),
	c1892 int,
	c1893 char(4),
	c1894 float,
	c1895 string,
	c1896 boolean,
	c1897 int,
	c1898 varchar(64),
	c1899 char(4),
	c1900 float,
	c1901 float,
	c1902 float,
	c1903 float,
	c1904 float,
	c1905 char(4),
	c1906 char(4),
	c1907 char(4),
	c1908 decimal(16, 10),
	c1909 float,
	c1910 int,
	c1911 char(4),
	c1912 boolean,
	c1913 int,
	c1914 int,
	c1915 varchar(64),
	c1916 int,
	c1917 varchar(64),
	c1918 int,
	c1919 string,
	c1920 decimal(16, 10),
	c1921 float,
	c1922 float,
	c1923 int,
	c1924 int,
	c1925 float,
	c1926 decimal(16, 10),
	c1927 string,
	c1928 float,
	c1929 float,
	c1930 int,
	c1931 varchar(64),
	c1932 float,
	c1933 varchar(64),
	c1934 int,
	c1935 int,
	c1936 boolean,
	c1937 float,
	c1938 float,
	c1939 char(4),
	c1940 varchar(64),
	c1941 int,
	c1942 char(4),
	c1943 varchar(64),
	c1944 boolean,
	c1945 int,
	c1946 int,
	c1947 float,
	c1948 int,
	c1949 varchar(64),
	c1950 decimal(16, 10),
	c1951 string,
	c1952 varchar(64),
	c1953 int,
	c1954 varchar(64),
	c1955 float,
	c1956 decimal(16, 10),
	c1957 int,
	c1958 int,
	c1959 varchar(64),
	c1960 char(4),
	c1961 int,
	c1962 char(4),
	c1963 varchar(64),
	c1964 int,
	c1965 float,
	c1966 char(4),
	c1967 float,
	c1968 boolean,
	c1969 decimal(16, 10),
	c1970 float,
	c1971 varchar(64),
	c1972 float,
	c1973 int,
	c1974 float,
	c1975 int,
	c1976 char(4),
	c1977 varchar(64),
	c1978 string,
	c1979 int,
	c1980 varchar(64),
	c1981 char(4),
	c1982 float,
	c1983 int,
	c1984 float,
	c1985 boolean,
	c1986 int,
	c1987 boolean,
	c1988 int,
	c1989 varchar(64),
	c1990 int,
	c1991 varchar(64),
	c1992 char(4),
	c1993 boolean,
	c1994 varchar(64),
	c1995 boolean,
	c1996 int,
	c1997 string,
	c1998 varchar(64),
	c1999 int,
	c2000 boolean
) row format delimited fields terminated by ',';
load data local inpath '../../data/files/2000_cols_data.csv' overwrite into table test_txt;
create table test_orc like test_txt;
alter table test_orc set fileformat orc;
insert into table test_orc select * from test_txt;

select c1, c2, c1999 from test_txt order by c1;
select c1, c2, c1999 from test_orc order by c1;

EXPLAIN
SELECT x.* FROM SRC x ORDER BY key limit 10;

SELECT x.* FROM SRC x ORDER BY key limit 10;

EXPLAIN
SELECT x.* FROM SRC x ORDER BY key desc limit 10;

SELECT x.* FROM SRC x ORDER BY key desc limit 10;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;

EXPLAIN
SELECT subq.key, subq.value FROM
(SELECT x.* FROM SRC x ORDER BY key limit 10) subq
where subq.key < 10;

SELECT subq.key, subq.value FROM
(SELECT x.* FROM SRC x ORDER BY key limit 10) subq
where subq.key < 10;
CREATE TABLE dest1(key INT, ten INT, one INT, value STRING) STORED AS TEXTFILE;

FROM src
INSERT OVERWRITE TABLE dest1
MAP src.key, CAST(src.key / 10 AS INT), CAST(src.key % 10 AS INT), src.value
USING 'cat' AS (tkey, ten, one, tvalue)
ORDER BY tvalue, tkey
SORT BY ten, one;
set hive.groupby.orderby.position.alias=true;

-- invalid position alias in order by
SELECT src.key, src.value FROM src ORDER BY 0;
set hive.groupby.orderby.position.alias=true;

-- position alias is not supported when SELECT *
SELECT src.* FROM src ORDER BY 1;
select t1.p_name, t2.p_name
from (select * from part order by p_size limit 10) t1 join part t2 on t1.p_partkey = t2.p_partkey and t1.p_size = t2.p_size
where t1.p_partkey < 100000;

set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;

-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

EXPLAIN EXTENDED
 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

EXPLAIN EXTENDED
 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';

 FROM
  src a
 FULL OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyOverriddenConfigsHook;
set some.hive.config.doesnt.exit=abc;

select count(*) from src;
set hive.explain.user=false;
set mapred.job.name='test_parallel';
set hive.exec.parallel=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- SORT_QUERY_RESULTS

create table if not exists src_a like src;
create table if not exists src_b like src;

explain
from (select key, value from src group by key, value) s
insert overwrite table src_a select s.key, s.value group by s.key, s.value
insert overwrite table src_b select s.key, s.value group by s.key, s.value;

from (select key, value from src group by key, value) s
insert overwrite table src_a select s.key, s.value group by s.key, s.value
insert overwrite table src_b select s.key, s.value group by s.key, s.value;

select * from src_a;
select * from src_b;


set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

from (select key, value from src group by key, value) s
insert overwrite table src_a select s.key, s.value group by s.key, s.value
insert overwrite table src_b select s.key, s.value group by s.key, s.value;

select * from src_a;
select * from src_b;
set hive.mapred.mode=nonstrict;
set mapreduce.job.reduces=4;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2;

EXPLAIN FORMATTED
SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2;

SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2;

set hive.mapred.mode=nonstrict;
set mapreduce.job.reduces=4;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- SORT_QUERY_RESULTS

CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

SELECT dest_j1.* FROM dest_j1;
set hive.mapred.mode=nonstrict;
create table src5 (key string, value string);
load data local inpath '../../data/files/kv5.txt' into table src5;
load data local inpath '../../data/files/kv5.txt' into table src5;

set mapred.reduce.tasks = 4;
set hive.optimize.sampling.orderby=true;
set hive.optimize.sampling.orderby.percent=0.66f;

explain
create table total_ordered as select * from src5 order by key, value;
create table total_ordered as select * from src5 order by key, value;

desc formatted total_ordered;
select * from total_ordered;

set hive.optimize.sampling.orderby.percent=0.0001f;
-- rolling back to single task in case that the number of sample is not enough

drop table total_ordered;
create table total_ordered as select * from src5 order by key, value;

desc formatted total_ordered;
select * from total_ordered;
set hive.mapred.mode=nonstrict;
SELECT key, value FROM src CLUSTER BY key, value;
SELECT key, value FROM src ORDER BY key ASC, value ASC;
SELECT key, value FROM src SORT BY key, value;
SELECT * FROM (SELECT key, value FROM src DISTRIBUTE BY key, value)t ORDER BY key, value;


SELECT key, value FROM src CLUSTER BY (key, value);
SELECT key, value FROM src ORDER BY key ASC, value ASC;
SELECT key, value FROM src SORT BY (key, value);
SELECT * FROM (SELECT key, value FROM src DISTRIBUTE BY (key, value))t ORDER BY key, value;
DROP TABLE parquet_array_null_element_staging;
DROP TABLE parquet_array_null_element;

CREATE TABLE parquet_array_null_element_staging (
    id int,
    lstint ARRAY<INT>,
    lststr ARRAY<STRING>,
    mp  MAP<STRING,STRING>
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':'
NULL DEFINED AS '';

CREATE TABLE parquet_array_null_element (
    id int,
    lstint ARRAY<INT>,
    lststr ARRAY<STRING>,
    mp  MAP<STRING,STRING>
) STORED AS PARQUET;

DESCRIBE FORMATTED parquet_array_null_element;

LOAD DATA LOCAL INPATH '../../data/files/parquet_array_null_element.txt' OVERWRITE INTO TABLE parquet_array_null_element_staging;

SELECT * FROM parquet_array_null_element_staging;

INSERT OVERWRITE TABLE parquet_array_null_element SELECT * FROM parquet_array_null_element_staging;

SELECT lstint from parquet_array_null_element;
SELECT lststr from parquet_array_null_element;
SELECT mp from parquet_array_null_element;
SELECT * FROM parquet_array_null_element;
-- this test creates a Parquet table with an array of multi-field structs

CREATE TABLE parquet_array_of_multi_field_structs (
    locations ARRAY<STRUCT<latitude: DOUBLE, longitude: DOUBLE>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/MultiFieldGroupInList.parquet'
OVERWRITE INTO TABLE parquet_array_of_multi_field_structs;

SELECT * FROM parquet_array_of_multi_field_structs;

DROP TABLE parquet_array_of_multi_field_structs;

-- maps use the same writable structure, so validate that the data can be read
-- as a map instead of an array of structs

CREATE TABLE parquet_map_view_of_multi_field_structs (
    locations MAP<DOUBLE, DOUBLE>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/MultiFieldGroupInList.parquet'
OVERWRITE INTO TABLE parquet_map_view_of_multi_field_structs;

SELECT * FROM parquet_map_view_of_multi_field_structs;

DROP TABLE parquet_map_view_of_multi_field_structs;
-- this test creates a Parquet table with an array of optional structs

CREATE TABLE parquet_array_of_optional_elements (
    locations ARRAY<STRUCT<latitude: DOUBLE, longitude: DOUBLE>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/NewOptionalGroupInList.parquet'
OVERWRITE INTO TABLE parquet_array_of_optional_elements;

SELECT * FROM parquet_array_of_optional_elements;

DROP TABLE parquet_array_of_optional_elements;
-- this test creates a Parquet table with an array of structs

CREATE TABLE parquet_array_of_required_elements (
    locations ARRAY<STRUCT<latitude: DOUBLE, longitude: DOUBLE>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/NewRequiredGroupInList.parquet'
OVERWRITE INTO TABLE parquet_array_of_required_elements;

SELECT * FROM parquet_array_of_required_elements;

DROP TABLE parquet_array_of_required_elements;
-- this test creates a Parquet table with an array of single-field structs
-- that has an ambiguous Parquet schema that is assumed to be a list of bigints
-- This is verifies compliance with the spec for this case.

CREATE TABLE parquet_ambiguous_array_of_single_field_structs (
    single_element_groups ARRAY<BIGINT>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/SingleFieldGroupInList.parquet'
OVERWRITE INTO TABLE parquet_ambiguous_array_of_single_field_structs;

SELECT * FROM parquet_ambiguous_array_of_single_field_structs;

DROP TABLE parquet_ambiguous_array_of_single_field_structs;
-- this test creates a Parquet table with an array of structs

CREATE TABLE parquet_array_of_structs (
    locations ARRAY<STRUCT<latitude: DOUBLE, longitude: DOUBLE>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/HiveRequiredGroupInList.parquet'
OVERWRITE INTO TABLE parquet_array_of_structs;

SELECT * FROM parquet_array_of_structs;

DROP TABLE parquet_array_of_structs;
-- this test creates a Parquet table from a structure with an unannotated
-- repeated structure of (x,y) structs

CREATE TABLE parquet_array_of_unannotated_groups (
    list_of_points ARRAY<STRUCT<x: FLOAT, y: FLOAT>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/UnannotatedListOfGroups.parquet'
OVERWRITE INTO TABLE parquet_array_of_unannotated_groups;

SELECT * FROM parquet_array_of_unannotated_groups;

DROP TABLE parquet_array_of_unannotated_groups;
-- this test creates a Parquet table from a structure with an unannotated
-- repeated structure of int32s

CREATE TABLE parquet_array_of_unannotated_ints (
    list_of_ints ARRAY<INT>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/UnannotatedListOfPrimitives.parquet'
OVERWRITE INTO TABLE parquet_array_of_unannotated_ints;

SELECT * FROM parquet_array_of_unannotated_ints;

DROP TABLE parquet_array_of_unannotated_ints;
-- this test creates a Parquet table with an array of structs

CREATE TABLE parquet_avro_array_of_primitives (
    list_of_ints ARRAY<INT>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/AvroPrimitiveInList.parquet'
OVERWRITE INTO TABLE parquet_avro_array_of_primitives;

SELECT * FROM parquet_avro_array_of_primitives;

DROP TABLE parquet_avro_array_of_primitives;
-- this test creates a Parquet table with an array of single-field structs
-- as written by parquet-avro

CREATE TABLE parquet_avro_array_of_single_field_structs (
    single_element_groups ARRAY<STRUCT<count: BIGINT>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/AvroSingleFieldGroupInList.parquet'
OVERWRITE INTO TABLE parquet_avro_array_of_single_field_structs;

SELECT * FROM parquet_avro_array_of_single_field_structs;

DROP TABLE parquet_avro_array_of_single_field_structs;
set parquet.column.index.access=true;

DROP TABLE IF EXISTS parquet_columnar_access_stage;
DROP TABLE IF EXISTS parquet_columnar_access;
DROP TABLE IF EXISTS parquet_columnar_renamed;

CREATE TABLE parquet_columnar_access_stage (
    s string,
    i int,
    f float
  ) ROW FORMAT DELIMITED
  FIELDS TERMINATED BY '|';

CREATE TABLE parquet_columnar_access (
    s string,
    x int,
    y int,
    f float,
    address struct<intVals:int,strVals:string>
  ) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/parquet_columnar.txt' OVERWRITE INTO TABLE parquet_columnar_access_stage;

INSERT OVERWRITE TABLE parquet_columnar_access SELECT s, i, (i + 1), f, named_struct('intVals',
i,'strVals',s) FROM parquet_columnar_access_stage;
SELECT * FROM parquet_columnar_access;

ALTER TABLE parquet_columnar_access REPLACE COLUMNS (s1 string, x1 int, y1 int, f1 float);

SELECT * FROM parquet_columnar_access;

ALTER TABLE parquet_columnar_access REPLACE COLUMNS (s1 string, x1 bigint, y1 int, f1 double);

SELECT * FROM parquet_columnar_access;
DROP TABLE parquet_create_staging;
DROP TABLE parquet_create;

CREATE TABLE parquet_create_staging (
    id int,
    str string,
    mp  MAP<STRING,STRING>,
    lst ARRAY<STRING>,
    strct STRUCT<A:STRING,B:STRING>
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

CREATE TABLE parquet_create (
    id int,
    str string,
    mp  MAP<STRING,STRING>,
    lst ARRAY<STRING>,
    strct STRUCT<A:STRING,B:STRING>
) STORED AS PARQUET;

DESCRIBE FORMATTED parquet_create;

LOAD DATA LOCAL INPATH '../../data/files/parquet_create.txt' OVERWRITE INTO TABLE parquet_create_staging;

SELECT * FROM parquet_create_staging;

INSERT OVERWRITE TABLE parquet_create SELECT * FROM parquet_create_staging;

SELECT id, count(0) FROM parquet_create group by id;
SELECT str from parquet_create;
SELECT mp from parquet_create;
SELECT lst from parquet_create;
SELECT strct from parquet_create;
set hive.mapred.mode=nonstrict;
drop table staging;
drop table parquet_ctas;
drop table parquet_ctas_advanced;
drop table parquet_ctas_alias;
drop table parquet_ctas_mixed;

create table staging (key int, value string) stored as textfile;
insert into table staging select * from src order by key limit 10;

create table parquet_ctas stored as parquet as select * from staging;
describe parquet_ctas;
select * from parquet_ctas;

create table parquet_ctas_advanced stored as parquet as select key+1,concat(value,"value") from staging;
describe parquet_ctas_advanced;
select * from parquet_ctas_advanced;

create table parquet_ctas_alias stored as parquet as select key+1 as mykey,concat(value,"value") as myvalue from staging;
describe parquet_ctas_alias;
select * from parquet_ctas_alias;

create table parquet_ctas_mixed stored as parquet as select key,key+1,concat(value,"value") as myvalue from staging;
describe parquet_ctas_mixed;
select * from parquet_ctas_mixed;set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS dec;

CREATE TABLE dec(name string, value decimal(8,4));

LOAD DATA LOCAL INPATH '../../data/files/dec.txt' INTO TABLE dec;

DROP TABLE IF EXISTS parq_dec;

CREATE TABLE parq_dec(name string, value decimal(5,2)) STORED AS PARQUET;

DESC parq_dec;

INSERT OVERWRITE TABLE parq_dec SELECT name, value FROM dec;

SELECT * FROM parq_dec;

SELECT value, count(*) FROM parq_dec GROUP BY value ORDER BY value;

TRUNCATE TABLE parq_dec;

INSERT OVERWRITE TABLE parq_dec SELECT name, NULL FROM dec;

SELECT * FROM parq_dec;

DROP TABLE IF EXISTS parq_dec1;

CREATE TABLE parq_dec1(name string, value decimal(4,1)) STORED AS PARQUET;

DESC parq_dec1;

LOAD DATA LOCAL INPATH '../../data/files/dec.parq' INTO TABLE parq_dec1;

SELECT VALUE FROM parq_dec1;

DROP TABLE dec;
DROP TABLE parq_dec;
DROP TABLE parq_dec1;
DROP TABLE IF EXISTS dec_comp;

CREATE TABLE dec_comp(arr ARRAY<decimal(5,2)>, m MAP<String, decimal(5,2)>, s STRUCT<i:int, d:decimal(5,2)>)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' COLLECTION ITEMS TERMINATED BY ','  MAP KEYS TERMINATED by ':';

LOAD DATA LOCAL INPATH '../../data/files/dec_comp.txt' INTO TABLE dec_comp;

SELECT * FROM dec_comp;

DROP TABLE IF EXISTS parq_dec_comp;

CREATE TABLE parq_dec_comp(arr ARRAY<decimal(5,2)>, m MAP<String, decimal(5,2)>, s STRUCT<i:int, d:decimal(5,2)>)
STORED AS PARQUET;

DESC parq_dec_comp;

INSERT OVERWRITE TABLE parq_dec_comp SELECT * FROM dec_comp;

SELECT * FROM parq_dec_comp;

DROP TABLE dec_comp;
DROP TABLE parq_dec_comp;
create table timetest_parquet(t timestamp) stored as parquet;

load data local inpath '../../data/files/parquet_external_time.parq' into table timetest_parquet;

select * from timetest_parquet;set hive.mapred.mode=nonstrict;

drop table if exists staging;
drop table if exists parquet_jointable1;
drop table if exists parquet_jointable2;
drop table if exists parquet_jointable1_bucketed_sorted;
drop table if exists parquet_jointable2_bucketed_sorted;

create table staging (key int, value string) stored as textfile;
insert into table staging select distinct key, value from src order by key limit 2;

create table parquet_jointable1 stored as parquet as select * from staging;

create table parquet_jointable2 stored as parquet as select key,key+1,concat(value,"value") as myvalue from staging;

-- SORT_QUERY_RESULTS

-- MR join

explain select p2.myvalue from parquet_jointable1 p1 join parquet_jointable2 p2 on p1.key=p2.key;
select p2.myvalue from parquet_jointable1 p1 join parquet_jointable2 p2 on p1.key=p2.key;

set hive.auto.convert.join=true;

-- The two tables involved in the join have differing number of columns(table1-2,table2-3). In case of Map and SMB join,
-- when the second table is loaded, the column indices in hive.io.file.readcolumn.ids refer to columns of both the first and the second table
-- and hence the parquet schema/types passed to ParquetInputSplit should contain only the column indexes belonging to second/current table

-- Map join

explain select p2.myvalue from parquet_jointable1 p1 join parquet_jointable2 p2 on p1.key=p2.key;
select p2.myvalue from parquet_jointable1 p1 join parquet_jointable2 p2 on p1.key=p2.key;

set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;
set hive.auto.convert.sortmerge.join=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- SMB join

create table parquet_jointable1_bucketed_sorted (key int,value string) clustered by (key) sorted by (key ASC) INTO 1 BUCKETS stored as parquet;
insert overwrite table parquet_jointable1_bucketed_sorted select key,concat(value,"value1") as value from staging cluster by key;
create table parquet_jointable2_bucketed_sorted (key int,value1 string, value2 string) clustered by (key) sorted by (key ASC) INTO 1 BUCKETS stored as parquet;
insert overwrite table parquet_jointable2_bucketed_sorted select key,concat(value,"value2-1") as value1,concat(value,"value2-2") as value2 from staging cluster by key;
explain select p1.value,p2.value2 from parquet_jointable1_bucketed_sorted p1 join parquet_jointable2_bucketed_sorted p2 on p1.key=p2.key;
select p1.value,p2.value2 from parquet_jointable1_bucketed_sorted p1 join parquet_jointable2_bucketed_sorted p2 on p1.key=p2.key;
set hive.optimize.index.filter = true;
set hive.auto.convert.join=false;

CREATE TABLE tbl1(id INT) STORED AS PARQUET;
INSERT INTO tbl1 VALUES(1), (2);

CREATE TABLE tbl2(id INT, value STRING) STORED AS PARQUET;
INSERT INTO tbl2 VALUES(1, 'value1');
INSERT INTO tbl2 VALUES(1, 'value2');

select tbl1.id, t1.value, t2.value
FROM tbl1
JOIN (SELECT * FROM tbl2 WHERE value='value1') t1 ON tbl1.id=t1.id
JOIN (SELECT * FROM tbl2 WHERE value='value2') t2 ON tbl1.id=t2.id;
-- This test attempts to write a parquet table from an avro table that contains map null values
-- JAVA_VERSION_SPECIFIC_OUTPUT

DROP TABLE IF EXISTS avro_table;
DROP TABLE IF EXISTS parquet_table;

CREATE TABLE avro_table (avreau_col_1 map<string,string>) STORED AS AVRO;
LOAD DATA LOCAL INPATH '../../data/files/map_null_val.avro' OVERWRITE INTO TABLE avro_table;

CREATE TABLE parquet_table STORED AS PARQUET AS SELECT * FROM avro_table;
SELECT * FROM parquet_table;

DROP TABLE avro_table;
DROP TABLE parquet_table;
-- this test reads and writes a parquet file with a map of arrays of ints
-- validates PARQUET-26 is fixed

CREATE TABLE parquet_map_of_arrays_of_ints (
    examples MAP<STRING, ARRAY<INT>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/StringMapOfOptionalIntArray.parquet'
OVERWRITE INTO TABLE parquet_map_of_arrays_of_ints;

CREATE TABLE parquet_map_of_arrays_of_ints_copy STORED AS PARQUET AS SELECT * FROM parquet_map_of_arrays_of_ints;

SELECT * FROM parquet_map_of_arrays_of_ints_copy;

DROP TABLE parquet_map_of_arrays_of_ints;
DROP TABLE parquet_map_of_arrays_of_ints_copy;
-- this test reads and writes a parquet file with a map of maps

CREATE TABLE parquet_map_of_maps (
    map_of_maps MAP<STRING, MAP<STRING, INT>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/NestedMap.parquet'
OVERWRITE INTO TABLE parquet_map_of_maps;

CREATE TABLE parquet_map_of_maps_copy STORED AS PARQUET AS SELECT * FROM parquet_map_of_maps;

SELECT * FROM parquet_map_of_maps_copy;

DROP TABLE parquet_map_of_maps;
DROP TABLE parquet_map_of_maps_copy;
DROP TABLE parquet_mixed_case;

CREATE TABLE parquet_mixed_case (
  lowerCase string,
  UPPERcase string,
  stats bigint,
  moreuppercase string,
  MORELOWERCASE string
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/parquet_mixed_case' OVERWRITE INTO TABLE parquet_mixed_case;

SELECT lowercase, "|", uppercase, "|", stats, "|", moreuppercase, "|", morelowercase FROM parquet_mixed_case;
set hive.mapred.mode=nonstrict;
DROP TABLE if exists parquet_mixed_partition_formats;

CREATE TABLE parquet_mixed_partition_formats (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary string,
  m1 map<string, varchar(3)>,
  l1 array<int>,
  st1 struct<c1:int, c2:char(1)>,
  d date)
PARTITIONED BY (dateint int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

---- partition dateint=20140330 is stored as TEXTFILE
LOAD DATA LOCAL INPATH '../../data/files/parquet_types.txt' OVERWRITE INTO TABLE parquet_mixed_partition_formats PARTITION (dateint=20140330);

SELECT * FROM parquet_mixed_partition_formats;

DESCRIBE FORMATTED parquet_mixed_partition_formats PARTITION (dateint=20140330);

---change table serde and file format to PARQUET----

ALTER TABLE parquet_mixed_partition_formats
     SET FILEFORMAT
     INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
     OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
     SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe';

DESCRIBE FORMATTED parquet_mixed_partition_formats;
DESCRIBE FORMATTED parquet_mixed_partition_formats PARTITION (dateint=20140330);

SELECT * FROM parquet_mixed_partition_formats;
set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hcatalog/hive-hcatalog-core/${system:hive.version}/hive-hcatalog-core-${system:hive.version}.jar;

CREATE TABLE parquet_table_json_partition (
id bigint COMMENT 'from deserializer',
address struct<country:bigint,state:bigint> COMMENT 'from deserializer',
reports array<bigint> COMMENT 'from deserializer')
PARTITIONED BY (
ts string)
ROW FORMAT SERDE
'org.apache.hive.hcatalog.data.JsonSerDe'
STORED AS INPUTFORMAT
'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat';

LOAD DATA LOCAL INPATH '../../data/files/sample2.json' INTO TABLE parquet_table_json_partition PARTITION(ts='20150101');

SELECT * FROM parquet_table_json_partition LIMIT 100;

ALTER TABLE parquet_table_json_partition
  SET FILEFORMAT INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
                 OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
                 SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe';

SELECT * FROM parquet_table_json_partition LIMIT 100;

CREATE TABLE new_table AS SELECT * FROM parquet_table_json_partition LIMIT 100;

SELECT * FROM new_table;


-- start with the original nestedcomplex test

create table nestedcomplex (
simple_int int,
max_nested_array  array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<int>>>>>>>>>>>>>>>>>>>>>>>,
max_nested_map    array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<map<string,string>>>>>>>>>>>>>>>>>>>>>>,
max_nested_struct array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<array<struct<s:string, i:bigint>>>>>>>>>>>>>>>>>>>>>>>,
simple_string string)
ROW FORMAT SERDE
   'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES (
   'hive.serialization.extend.nesting.levels'='true',
   'line.delim'='\n'
)
;

describe nestedcomplex;
describe extended nestedcomplex;

load data local inpath '../../data/files/nested_complex.txt' overwrite into table nestedcomplex;

-- and load the table into Parquet

CREATE TABLE parquet_nested_complex STORED AS PARQUET AS SELECT * FROM nestedcomplex;

SELECT * FROM parquet_nested_complex SORT BY simple_int;

DROP TABLE nestedcomplex;
DROP TABLE parquet_nested_complex;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

-- SORT_QUERY_RESULTS

DROP TABLE parquet_partitioned_staging;
DROP TABLE parquet_partitioned;

CREATE TABLE parquet_partitioned_staging (
    id int,
    str string,
    part string
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|';

CREATE TABLE parquet_partitioned (
    id int,
    str string
) PARTITIONED BY (part string)
STORED AS PARQUET;

DESCRIBE FORMATTED parquet_partitioned;

LOAD DATA LOCAL INPATH '../../data/files/parquet_partitioned.txt' OVERWRITE INTO TABLE parquet_partitioned_staging;

SELECT * FROM parquet_partitioned_staging;

INSERT OVERWRITE TABLE parquet_partitioned PARTITION (part) SELECT * FROM parquet_partitioned_staging;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SELECT * FROM parquet_partitioned;
SELECT part, COUNT(0) FROM parquet_partitioned GROUP BY part;

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
SELECT * FROM parquet_partitioned;
SELECT part, COUNT(0) FROM parquet_partitioned GROUP BY part;
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypestbl(c char(10), v varchar(10), d decimal(5,3), b boolean) stored as parquet;

insert overwrite table newtypestbl select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, true from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, false from src src2 limit 10) uniontbl;

SET hive.optimize.ppd=true;
SET hive.optimize.index.filter=true;
select * from newtypestbl where b=true;
select * from newtypestbl where b!=true;
select * from newtypestbl where b<true;
select * from newtypestbl where b>true;
select * from newtypestbl where b<=true sort by c;

select * from newtypestbl where b=false;
select * from newtypestbl where b!=false;
select * from newtypestbl where b<false;
select * from newtypestbl where b>false;
select * from newtypestbl where b<=false;


SET hive.optimize.index.filter=false;
select * from newtypestbl where b=true;
select * from newtypestbl where b!=true;
select * from newtypestbl where b<true;
select * from newtypestbl where b>true;
select * from newtypestbl where b<=true sort by c;

select * from newtypestbl where b=false;
select * from newtypestbl where b!=false;
select * from newtypestbl where b<false;
select * from newtypestbl where b>false;
select * from newtypestbl where b<=false;SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.ppd=true;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypestbl(c char(10), v varchar(10), d decimal(5,3), da date) stored as parquet;

insert overwrite table newtypestbl select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("1970-02-20" as date) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("1970-02-27" as date) from src src2 limit 10) uniontbl;

set hive.optimize.index.filter=false;

-- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select * from newtypestbl where c="apple";

set hive.optimize.index.filter=true;
select * from newtypestbl where c="apple";

set hive.optimize.index.filter=false;
select * from newtypestbl where c!="apple";

set hive.optimize.index.filter=true;
select * from newtypestbl where c!="apple";

set hive.optimize.index.filter=false;
select * from newtypestbl where c<"hello";

set hive.optimize.index.filter=true;
select * from newtypestbl where c<"hello";

set hive.optimize.index.filter=false;
select * from newtypestbl where c<="hello" sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where c<="hello" sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where c="apple ";

set hive.optimize.index.filter=true;
select * from newtypestbl where c="apple ";

set hive.optimize.index.filter=false;
select * from newtypestbl where c in ("apple", "carrot");

set hive.optimize.index.filter=true;
select * from newtypestbl where c in ("apple", "carrot");

set hive.optimize.index.filter=false;
select * from newtypestbl where c in ("apple", "hello") sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where c in ("apple", "hello") sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where c in ("carrot");

set hive.optimize.index.filter=true;
select * from newtypestbl where c in ("carrot");

set hive.optimize.index.filter=false;
select * from newtypestbl where c between "apple" and "carrot";

set hive.optimize.index.filter=true;
select * from newtypestbl where c between "apple" and "carrot";

set hive.optimize.index.filter=false;
select * from newtypestbl where c between "apple" and "zombie" sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where c between "apple" and "zombie" sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where c between "carrot" and "carrot1";

set hive.optimize.index.filter=true;
select * from newtypestbl where c between "carrot" and "carrot1";SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.ppd=true;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypestbl(c char(10), v varchar(10), d decimal(5,3), da date) stored as parquet;

insert overwrite table newtypestbl select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("1970-02-20" as date) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("1970-02-27" as date) from src src2 limit 10) uniontbl;

-- date data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select * from newtypestbl where da='1970-02-20';

set hive.optimize.index.filter=true;
select * from newtypestbl where da='1970-02-20';

set hive.optimize.index.filter=true;
select * from newtypestbl where da= date '1970-02-20';

set hive.optimize.index.filter=false;
select * from newtypestbl where da=cast('1970-02-20' as date);

set hive.optimize.index.filter=true;
select * from newtypestbl where da=cast('1970-02-20' as date);

set hive.optimize.index.filter=false;
select * from newtypestbl where da=cast('1970-02-20' as varchar(20));

set hive.optimize.index.filter=true;
select * from newtypestbl where da=cast('1970-02-20' as varchar(20));

set hive.optimize.index.filter=false;
select * from newtypestbl where da!='1970-02-20';

set hive.optimize.index.filter=true;
select * from newtypestbl where da!='1970-02-20';

set hive.optimize.index.filter=false;
select * from newtypestbl where da<'1970-02-27';

set hive.optimize.index.filter=true;
select * from newtypestbl where da<'1970-02-27';

set hive.optimize.index.filter=false;
select * from newtypestbl where da<'1970-02-29' sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where da<'1970-02-29' sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where da<'1970-02-15';

set hive.optimize.index.filter=true;
select * from newtypestbl where da<'1970-02-15';

set hive.optimize.index.filter=false;
select * from newtypestbl where da<='1970-02-20';

set hive.optimize.index.filter=true;
select * from newtypestbl where da<='1970-02-20';

set hive.optimize.index.filter=false;
select * from newtypestbl where da<='1970-02-27' sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where da<='1970-02-27' sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where da in (cast('1970-02-21' as date), cast('1970-02-27' as date));

set hive.optimize.index.filter=true;
select * from newtypestbl where da in (cast('1970-02-21' as date), cast('1970-02-27' as date));

set hive.optimize.index.filter=false;
select * from newtypestbl where da in (cast('1970-02-20' as date), cast('1970-02-27' as date)) sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where da in (cast('1970-02-20' as date), cast('1970-02-27' as date)) sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where da in (cast('1970-02-21' as date), cast('1970-02-22' as date));

set hive.optimize.index.filter=true;
select * from newtypestbl where da in (cast('1970-02-21' as date), cast('1970-02-22' as date));

set hive.optimize.index.filter=false;
select * from newtypestbl where da between '1970-02-19' and '1970-02-22';

set hive.optimize.index.filter=true;
select * from newtypestbl where da between '1970-02-19' and '1970-02-22';

set hive.optimize.index.filter=false;
select * from newtypestbl where da between '1970-02-19' and '1970-02-28' sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where da between '1970-02-19' and '1970-02-28' sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where da between '1970-02-18' and '1970-02-19';

set hive.optimize.index.filter=true;
select * from newtypestbl where da between '1970-02-18' and '1970-02-19';
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypestbl(c char(10), v varchar(10), d decimal(5,3), da date) stored as parquet;

insert overwrite table newtypestbl select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("1970-02-20" as date) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("1970-02-27" as date) from src src2 limit 10) uniontbl;

-- decimal data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select * from newtypestbl where d=0.22;

set hive.optimize.index.filter=true;
select * from newtypestbl where d=0.22;

set hive.optimize.index.filter=false;
select * from newtypestbl where d='0.22';

set hive.optimize.index.filter=true;
select * from newtypestbl where d='0.22';

set hive.optimize.index.filter=false;
select * from newtypestbl where d=cast('0.22' as float);

set hive.optimize.index.filter=true;
select * from newtypestbl where d=cast('0.22' as float);

set hive.optimize.index.filter=false;
select * from newtypestbl where d!=0.22;

set hive.optimize.index.filter=true;
select * from newtypestbl where d!=0.22;

set hive.optimize.index.filter=false;
select * from newtypestbl where d!='0.22';

set hive.optimize.index.filter=true;
select * from newtypestbl where d!='0.22';

set hive.optimize.index.filter=false;
select * from newtypestbl where d!=cast('0.22' as float);

set hive.optimize.index.filter=true;
select * from newtypestbl where d!=cast('0.22' as float);

set hive.optimize.index.filter=false;
select * from newtypestbl where d<11.22;

set hive.optimize.index.filter=true;
select * from newtypestbl where d<11.22;

set hive.optimize.index.filter=false;
select * from newtypestbl where d<'11.22';

set hive.optimize.index.filter=true;
select * from newtypestbl where d<'11.22';

set hive.optimize.index.filter=false;
select * from newtypestbl where d<cast('11.22' as float);

set hive.optimize.index.filter=true;
select * from newtypestbl where d<cast('11.22' as float);

set hive.optimize.index.filter=false;
select * from newtypestbl where d<1;

set hive.optimize.index.filter=true;
select * from newtypestbl where d<1;

set hive.optimize.index.filter=false;
select * from newtypestbl where d<=11.22 sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where d<=11.22 sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where d<='11.22' sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where d<='11.22' sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where d<=cast('11.22' as float) sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where d<=cast('11.22' as float) sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where d<=cast('11.22' as decimal);

set hive.optimize.index.filter=true;
select * from newtypestbl where d<=cast('11.22' as decimal);

set hive.optimize.index.filter=false;
select * from newtypestbl where d<=11.22BD sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where d<=11.22BD sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where d<=12 sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where d<=12 sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where d in ('0.22', '1.0');

set hive.optimize.index.filter=true;
select * from newtypestbl where d in ('0.22', '1.0');

set hive.optimize.index.filter=false;
select * from newtypestbl where d in ('0.22', '11.22') sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where d in ('0.22', '11.22') sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where d in ('0.9', '1.0');

set hive.optimize.index.filter=true;
select * from newtypestbl where d in ('0.9', '1.0');

set hive.optimize.index.filter=false;
select * from newtypestbl where d in ('0.9', 0.22);

set hive.optimize.index.filter=true;
select * from newtypestbl where d in ('0.9', 0.22);

set hive.optimize.index.filter=false;
select * from newtypestbl where d in ('0.9', 0.22, cast('11.22' as float)) sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where d in ('0.9', 0.22, cast('11.22' as float)) sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where d between 0 and 1;

set hive.optimize.index.filter=true;
select * from newtypestbl where d between 0 and 1;

set hive.optimize.index.filter=false;
select * from newtypestbl where d between 0 and 1000 sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where d between 0 and 1000 sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where d between 0 and '2.0';

set hive.optimize.index.filter=true;
select * from newtypestbl where d between 0 and '2.0';

set hive.optimize.index.filter=false;
select * from newtypestbl where d between 0 and cast(3 as float);

set hive.optimize.index.filter=true;
select * from newtypestbl where d between 0 and cast(3 as float);

set hive.optimize.index.filter=false;
select * from newtypestbl where d between 1 and cast(30 as char(10));

set hive.optimize.index.filter=true;
select * from newtypestbl where d between 1 and cast(30 as char(10));
SET hive.optimize.index.filter=true;
SET hive.optimize.ppd=true;

-- Test predicate with partitioned columns
CREATE TABLE part1 (id int, content string) PARTITIONED BY (p string) STORED AS PARQUET;
ALTER TABLE part1 ADD PARTITION (p='p1');
INSERT INTO TABLE part1 PARTITION (p='p1') VALUES (1, 'a'), (2, 'b');
SELECT * FROM part1 WHERE p='p1';
DROP TABLE part1 PURGE;SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.ppd=true;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypestbl(c char(10), v varchar(10), d decimal(5,3), ts timestamp) stored as parquet;

insert overwrite table newtypestbl select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("2011-01-01 01:01:01" as timestamp) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("2011-01-20 01:01:01" as timestamp) from src src2 limit 10) uniontbl;

-- timestamp data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select * from newtypestbl where cast(ts as string)='2011-01-01 01:01:01';

set hive.optimize.index.filter=true;
select * from newtypestbl where cast(ts as string)='2011-01-01 01:01:01';

set hive.optimize.index.filter=false;
select * from newtypestbl where ts=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select * from newtypestbl where ts=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select * from newtypestbl where ts=cast('2011-01-01 01:01:01' as varchar(20));

set hive.optimize.index.filter=true;
select * from newtypestbl where ts=cast('2011-01-01 01:01:01' as varchar(20));

set hive.optimize.index.filter=false;
select * from newtypestbl where ts!=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select * from newtypestbl where ts!=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select * from newtypestbl where ts<cast('2011-01-20 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select * from newtypestbl where ts<cast('2011-01-20 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select * from newtypestbl where ts<cast('2011-01-22 01:01:01' as timestamp) sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where ts<cast('2011-01-22 01:01:01' as timestamp) sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where ts<cast('2010-10-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select * from newtypestbl where ts<cast('2010-10-01 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select * from newtypestbl where ts<=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select * from newtypestbl where ts<=cast('2011-01-01 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select * from newtypestbl where ts<=cast('2011-01-20 01:01:01' as timestamp) sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where ts<=cast('2011-01-20 01:01:01' as timestamp) sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where ts in (cast('2011-01-02 01:01:01' as timestamp), cast('2011-01-20 01:01:01' as timestamp));

set hive.optimize.index.filter=true;
select * from newtypestbl where ts in (cast('2011-01-02 01:01:01' as timestamp), cast('2011-01-20 01:01:01' as timestamp));

set hive.optimize.index.filter=false;
select * from newtypestbl where ts in (cast('2011-01-01 01:01:01' as timestamp), cast('2011-01-20 01:01:01' as timestamp)) sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where ts in (cast('2011-01-01 01:01:01' as timestamp), cast('2011-01-20 01:01:01' as timestamp)) sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where ts in (cast('2011-01-02 01:01:01' as timestamp), cast('2011-01-08 01:01:01' as timestamp));

set hive.optimize.index.filter=true;
select * from newtypestbl where ts in (cast('2011-01-02 01:01:01' as timestamp), cast('2011-01-08 01:01:01' as timestamp));

set hive.optimize.index.filter=false;
select * from newtypestbl where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2011-01-08 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select * from newtypestbl where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2011-01-08 01:01:01' as timestamp);

set hive.optimize.index.filter=false;
select * from newtypestbl where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2011-01-25 01:01:01' as timestamp) sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2011-01-25 01:01:01' as timestamp) sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2010-11-01 01:01:01' as timestamp);

set hive.optimize.index.filter=true;
select * from newtypestbl where ts between cast('2010-10-01 01:01:01' as timestamp) and cast('2010-11-01 01:01:01' as timestamp);
SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.ppd=true;
SET mapred.min.split.size=1000;
SET mapred.max.split.size=5000;

create table newtypestbl(c char(10), v varchar(10), d decimal(5,3), da date) stored as parquet;

insert overwrite table newtypestbl select * from (select cast("apple" as char(10)), cast("bee" as varchar(10)), 0.22, cast("1970-02-20" as date) from src src1 union all select cast("hello" as char(10)), cast("world" as varchar(10)), 11.22, cast("1970-02-27" as date) from src src2 limit 10) uniontbl;

set hive.optimize.index.filter=false;

-- varchar data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)
select * from newtypestbl where v="bee";

set hive.optimize.index.filter=true;
select * from newtypestbl where v="bee";

set hive.optimize.index.filter=false;
select * from newtypestbl where v!="bee";

set hive.optimize.index.filter=true;
select * from newtypestbl where v!="bee";

set hive.optimize.index.filter=false;
select * from newtypestbl where v<"world";

set hive.optimize.index.filter=true;
select * from newtypestbl where v<"world";

set hive.optimize.index.filter=false;
select * from newtypestbl where v<="world" sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where v<="world" sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where v="bee   ";

set hive.optimize.index.filter=true;
select * from newtypestbl where v="bee   ";

set hive.optimize.index.filter=false;
select * from newtypestbl where v in ("bee", "orange");

set hive.optimize.index.filter=true;
select * from newtypestbl where v in ("bee", "orange");

set hive.optimize.index.filter=false;
select * from newtypestbl where v in ("bee", "world") sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where v in ("bee", "world") sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where v in ("orange");

set hive.optimize.index.filter=true;
select * from newtypestbl where v in ("orange");

set hive.optimize.index.filter=false;
select * from newtypestbl where v between "bee" and "orange";

set hive.optimize.index.filter=true;
select * from newtypestbl where v between "bee" and "orange";

set hive.optimize.index.filter=false;
select * from newtypestbl where v between "bee" and "zombie" sort by c;

set hive.optimize.index.filter=true;
select * from newtypestbl where v between "bee" and "zombie" sort by c;

set hive.optimize.index.filter=false;
select * from newtypestbl where v between "orange" and "pine";

set hive.optimize.index.filter=true;
select * from newtypestbl where v between "orange" and "pine";set hive.mapred.mode=nonstrict;
SET hive.optimize.ppd=true;

-- SORT_QUERY_RESULTS
CREATE TABLE tbl_pred(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS PARQUET;

CREATE TABLE staging(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE staging;

INSERT INTO TABLE tbl_pred select * from staging;

-- no predicate case. the explain plan should not have filter expression in table scan operator

SELECT SUM(HASH(t)) FROM tbl_pred;

SET hive.optimize.index.filter=true;
SELECT SUM(HASH(t)) FROM tbl_pred;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT SUM(HASH(t)) FROM tbl_pred;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT SUM(HASH(t)) FROM tbl_pred;
SET hive.optimize.index.filter=false;

-- all the following queries have predicates which are pushed down to table scan operator if
-- hive.optimize.index.filter is set to true. the explain plan should show filter expression
-- in table scan operator.

SELECT * FROM tbl_pred WHERE t<2 limit 1;
SET hive.optimize.index.filter=true;
SELECT * FROM tbl_pred WHERE t<2 limit 1;
SET hive.optimize.index.filter=false;

SELECT * FROM tbl_pred WHERE t>2 limit 1;
SET hive.optimize.index.filter=true;
SELECT * FROM tbl_pred WHERE t>2 limit 1;
SET hive.optimize.index.filter=false;

SELECT * FROM tbl_pred
  WHERE t IS NOT NULL
  AND t < 0
  AND t > -2
  LIMIT 10;

SET hive.optimize.index.filter=true;
SELECT * FROM tbl_pred
  WHERE t IS NOT NULL
  AND t < 0
  AND t > -2
  LIMIT 10;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT * FROM tbl_pred
  WHERE t IS NOT NULL
  AND t < 0
  AND t > -2
  LIMIT 10;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT * FROM tbl_pred
  WHERE t IS NOT NULL
  AND t < 0
  AND t > -2
  LIMIT 10;
SET hive.optimize.index.filter=false;

SELECT t, s FROM tbl_pred
  WHERE t <=> -1
  AND s IS NOT NULL
  AND s LIKE 'bob%'
  ;

SET hive.optimize.index.filter=true;
SELECT t, s FROM tbl_pred
  WHERE t <=> -1
  AND s IS NOT NULL
  AND s LIKE 'bob%'
  ;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT t, s FROM tbl_pred
  WHERE t <=> -1
  AND s IS NOT NULL
  AND s LIKE 'bob%'
  ;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT t, s FROM tbl_pred
  WHERE t <=> -1
  AND s IS NOT NULL
  AND s LIKE 'bob%'
  ;
SET hive.optimize.index.filter=false;

SELECT t, s FROM tbl_pred
  WHERE s IS NOT NULL
  AND s LIKE 'bob%'
  AND t NOT IN (-1,-2,-3)
  AND t BETWEEN 25 AND 30
  SORT BY t,s;

set hive.optimize.index.filter=true;
SELECT t, s FROM tbl_pred
  WHERE s IS NOT NULL
  AND s LIKE 'bob%'
  AND t NOT IN (-1,-2,-3)
  AND t BETWEEN 25 AND 30
  SORT BY t,s;
set hive.optimize.index.filter=false;

EXPLAIN SELECT t, s FROM tbl_pred
  WHERE s IS NOT NULL
  AND s LIKE 'bob%'
  AND t NOT IN (-1,-2,-3)
  AND t BETWEEN 25 AND 30
  SORT BY t,s;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT t, s FROM tbl_pred
  WHERE s IS NOT NULL
  AND s LIKE 'bob%'
  AND t NOT IN (-1,-2,-3)
  AND t BETWEEN 25 AND 30
  SORT BY t,s;
SET hive.optimize.index.filter=false;

SELECT t, si, d, s FROM tbl_pred
  WHERE d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  ORDER BY s DESC
  LIMIT 3;

SET hive.optimize.index.filter=true;
SELECT t, si, d, s FROM tbl_pred
  WHERE d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  ORDER BY s DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT t, si, d, s FROM tbl_pred
  WHERE d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  ORDER BY s DESC
  LIMIT 3;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT t, si, d, s FROM tbl_pred
  WHERE d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  ORDER BY s DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;

SELECT t, si, d, s FROM tbl_pred
  WHERE t > 10
  AND t <> 101
  AND d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  SORT BY s DESC
  LIMIT 3;

SET hive.optimize.index.filter=true;
SELECT t, si, d, s FROM tbl_pred
  WHERE t > 10
  AND t <> 101
  AND d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  SORT BY s DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;

SET hive.optimize.index.filter=true;
SELECT f, i, b FROM tbl_pred
  WHERE f IS NOT NULL
  AND f < 123.2
  AND f > 1.92
  AND f >= 9.99
  AND f BETWEEN 1.92 AND 123.2
  AND i IS NOT NULL
  AND i < 67627
  AND i > 60627
  AND i >= 60626
  AND i BETWEEN 60626 AND 67627
  AND b IS NOT NULL
  AND b < 4294967861
  AND b > 4294967261
  AND b >= 4294967260
  AND b BETWEEN 4294967261 AND 4294967861
  SORT BY f DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;

EXPLAIN SELECT t, si, d, s FROM tbl_pred
  WHERE t > 10
  AND t <> 101
  AND d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  SORT BY s DESC
  LIMIT 3;

SET hive.optimize.index.filter=true;
EXPLAIN SELECT t, si, d, s FROM tbl_pred
  WHERE t > 10
  AND t <> 101
  AND d >= ROUND(9.99)
  AND d < 12
  AND t IS NOT NULL
  AND s LIKE '%son'
  AND s NOT LIKE '%car%'
  AND t > 0
  AND si BETWEEN 300 AND 400
  SORT BY s DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;


SET hive.optimize.index.filter=true;
EXPLAIN SELECT f, i, b FROM tbl_pred
  WHERE f IS NOT NULL
  AND f < 123.2
  AND f > 1.92
  AND f >= 9.99
  AND f BETWEEN 1.92 AND 123.2
  AND i IS NOT NULL
  AND i < 67627
  AND i > 60627
  AND i >= 60626
  AND i BETWEEN 60626 AND 67627
  AND b IS NOT NULL
  AND b < 4294967861
  AND b > 4294967261
  AND b >= 4294967260
  AND b BETWEEN 4294967261 AND 4294967861
  SORT BY f DESC
  LIMIT 3;
SET hive.optimize.index.filter=false;-- This test makes sure that parquet can read older parquet files written by Hive <= 0.12
-- alltypesparquet is a files written by older version of Hive

CREATE TABLE alltypesparquet (
    bo1 boolean,
    ti1 tinyint,
    si1 smallint,
    i1 int,
    bi1 bigint,
    f1 float,
    d1 double,
    s1 string,
    m1 map<string,string>,
    l1 array<int>,
    st1 struct<c1:int,c2:string>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/alltypesparquet' OVERWRITE INTO TABLE alltypesparquet;

SELECT * FROM alltypesparquet;set hive.mapred.mode=nonstrict;
-- Some tables might have extra columns and struct elements on the schema than the on Parquet schema;
-- This is called 'schema evolution' as the Parquet file is not ready yet for such new columns;
-- Hive should support this schema, and return NULL values instead;

DROP TABLE NewStructField;
DROP TABLE NewStructFieldTable;

CREATE TABLE NewStructField(a struct<a1:map<string,string>, a2:struct<e1:int>>) STORED AS PARQUET;

INSERT OVERWRITE TABLE NewStructField SELECT named_struct('a1', map('k1','v1'), 'a2', named_struct('e1',5)) FROM srcpart LIMIT 5;

DESCRIBE NewStructField;
SELECT * FROM NewStructField;
set hive.metastore.disallow.incompatible.col.type.changes=false;
-- Adds new fields to the struct types
ALTER TABLE NewStructField REPLACE COLUMNS (a struct<a1:map<string,string>, a2:struct<e1:int,e2:string>, a3:int>, b int);
reset hive.metastore.disallow.incompatible.col.type.changes;
DESCRIBE NewStructField;
SELECT * FROM NewStructField;

-- Makes sure that new parquet tables contain the new struct field
CREATE TABLE NewStructFieldTable STORED AS PARQUET AS SELECT * FROM NewStructField;
DESCRIBE NewStructFieldTable;
SELECT * FROM NewStructFieldTable;

DROP TABLE NewStructField;
DROP TABLE NewStructFieldTable;
set hive.mapred.mode=nonstrict;
DROP TABLE if exists parquet_mixed_fileformat;

CREATE TABLE parquet_mixed_fileformat (
    id int,
    str string,
    part string
) PARTITIONED BY (dateint int)
 ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|';

---- partition dateint=20140330 is stored as TEXTFILE

LOAD DATA LOCAL INPATH '../../data/files/parquet_partitioned.txt' OVERWRITE INTO TABLE parquet_mixed_fileformat PARTITION (dateint=20140330);

SELECT * FROM parquet_mixed_fileformat;

DESCRIBE FORMATTED parquet_mixed_fileformat PARTITION (dateint=20140330);

---change table serde and file format to PARQUET----

ALTER TABLE parquet_mixed_fileformat set SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe';
ALTER TABLE parquet_mixed_fileformat
     SET FILEFORMAT
     INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
     OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe';

DESCRIBE FORMATTED parquet_mixed_fileformat;
DESCRIBE FORMATTED parquet_mixed_fileformat PARTITION (dateint=20140330);

SELECT * FROM parquet_mixed_fileformat;
-- Sometimes, the user wants to create a table from just a portion of the file schema;
-- This test makes sure that this scenario works;

DROP TABLE test;

-- Current file schema is: (id int, name string, address struct<number:int,street:string,zip:string>);
-- Creates a table from just a portion of the file schema, including struct elements (test lower/upper case as well)
CREATE TABLE test (Name string, address struct<Zip:string,Street:string>) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/HiveGroup.parquet' OVERWRITE INTO TABLE test;
SELECT * FROM test;

DROP TABLE test;-- this test creates a Parquet table with an array of structs

CREATE TABLE parquet_thrift_array_of_primitives (
    list_of_ints ARRAY<INT>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/ThriftPrimitiveInList.parquet'
OVERWRITE INTO TABLE parquet_thrift_array_of_primitives;

SELECT * FROM parquet_thrift_array_of_primitives;

DROP TABLE parquet_thrift_array_of_primitives;
-- this test creates a Parquet table with an array of single-field structs
-- as written by parquet-thrift

CREATE TABLE parquet_thrift_array_of_single_field_structs (
    single_element_groups ARRAY<STRUCT<count: BIGINT>>
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/ThriftSingleFieldGroupInList.parquet'
OVERWRITE INTO TABLE parquet_thrift_array_of_single_field_structs;

SELECT * FROM parquet_thrift_array_of_single_field_structs;

DROP TABLE parquet_thrift_array_of_single_field_structs;
set hive.mapred.mode=nonstrict;
DROP TABLE parquet_types_staging;
DROP TABLE parquet_types;

CREATE TABLE parquet_types_staging (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary string,
  m1 map<string, varchar(3)>,
  l1 array<int>,
  st1 struct<c1:int, c2:char(1)>,
  d date
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

CREATE TABLE parquet_types (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary binary,
  m1 map<string, varchar(3)>,
  l1 array<int>,
  st1 struct<c1:int, c2:char(1)>,
  d date
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/parquet_types.txt' OVERWRITE INTO TABLE parquet_types_staging;

SELECT * FROM parquet_types_staging;

INSERT OVERWRITE TABLE parquet_types
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
unhex(cbinary), m1, l1, st1, d FROM parquet_types_staging;

SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), m1, l1, st1, d FROM parquet_types;

SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar) FROM parquet_types;

-- test types in group by

SELECT ctinyint,
  MAX(cint),
  MIN(csmallint),
  COUNT(cstring1),
  ROUND(AVG(cfloat), 5),
  ROUND(STDDEV_POP(cdouble),5)
FROM parquet_types
GROUP BY ctinyint
ORDER BY ctinyint
;

SELECT cfloat, count(*) FROM parquet_types GROUP BY cfloat ORDER BY cfloat;

SELECT cchar, count(*) FROM parquet_types GROUP BY cchar ORDER BY cchar;

SELECT cvarchar, count(*) FROM parquet_types GROUP BY cvarchar ORDER BY cvarchar;

SELECT cstring1, count(*) FROM parquet_types GROUP BY cstring1 ORDER BY cstring1;

SELECT t, count(*) FROM parquet_types GROUP BY t ORDER BY t;

SELECT hex(cbinary), count(*) FROM parquet_types GROUP BY cbinary;DROP TABLE parquet_type_promotion_staging;
DROP TABLE parquet_type_promotion;

SET hive.metastore.disallow.incompatible.col.type.changes=false;

CREATE TABLE parquet_type_promotion_staging (
  cint int,
  clong bigint,
  cfloat float,
  cdouble double,
  m1 map<string, int>,
  l1 array<int>,
  st1 struct<c1:int, c2:int>,
  fm1 map<string, float>,
  fl1 array<float>,
  fst1 struct<c1:float, c2:float>
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

LOAD DATA LOCAL INPATH '../../data/files/parquet_type_promotion.txt' OVERWRITE INTO TABLE parquet_type_promotion_staging;

SELECT * FROM parquet_type_promotion_staging;

CREATE TABLE parquet_type_promotion (
  cint int,
  clong bigint,
  cfloat float,
  cdouble double,
  m1 map<string, int>,
  l1 array<int>,
  st1 struct<c1:int, c2:int>,
  fm1 map<string, float>,
  fl1 array<float>,
  fst1 struct<c1:float, c2:float>

) STORED AS PARQUET;

INSERT OVERWRITE TABLE parquet_type_promotion
   SELECT * FROM parquet_type_promotion_staging;

SELECT * FROM parquet_type_promotion;

ALTER TABLE  parquet_type_promotion REPLACE COLUMNS(
  cint bigint,
  clong bigint,
  cfloat double,
  cdouble double,
  m1 map<string, bigint>,
  l1 array<bigint>,
  st1 struct<c1:int, c2:bigint>,
  fm1 map<string, double>,
  fl1 array<double>,
  fst1 struct<c1:double, c2:float>
);

SELECT * FROM parquet_type_promotion;

-- This test covers the case where array<struct<f1,f2,..>> data
-- can be retrieved useing map<f1,f2>.
-- This also test if there are more than 2 fields in array_of_struct
-- which can be read as  map as well.

DROP TABLE arrays_of_struct_to_map;
CREATE TABLE arrays_of_struct_to_map (locations1 array<struct<c1:int,c2:int>>, locations2 array<struct<f1:int,
f2:int,f3:int>>) STORED AS PARQUET;
INSERT INTO TABLE arrays_of_struct_to_map select array(named_struct("c1",1,"c2",2)), array(named_struct("f1",
77,"f2",88,"f3",99)) FROM parquet_type_promotion LIMIT 1;
SELECT * FROM arrays_of_struct_to_map;
ALTER TABLE arrays_of_struct_to_map REPLACE COLUMNS (locations1 array<struct<c1:int,c2:int,c3:int>>, locations2
array<struct<f1:int,f2:int,f3:int>>);
SELECT * FROM arrays_of_struct_to_map;
ALTER TABLE arrays_of_struct_to_map REPLACE COLUMNS (locations1 map<int,bigint>, locations2 map<bigint,int>);
SELECT * FROM arrays_of_struct_to_map;
CREATE TABLE text_tbl (a STRUCT<b:STRUCT<c:INT>>)
STORED AS TEXTFILE;

-- This inserts one NULL row
INSERT OVERWRITE TABLE text_tbl
SELECT IF(false, named_struct("b", named_struct("c", 1)), NULL)
FROM src LIMIT 1;

-- We test that parquet is written with a level 0 definition
CREATE TABLE parq_tbl
STORED AS PARQUET
AS SELECT * FROM text_tbl;

SELECT * FROM text_tbl;
SELECT * FROM parq_tbl;

DROP TABLE text_tbl;
DROP TABLE parq_tbl;
create table test1(col1 string) partitioned by (partitionId int);
insert overwrite table test1 partition (partitionId=1)
  select key from src tablesample (10 rows);

 FROM (
 FROM test1
 SELECT partitionId, 111 as col2, 222 as col3, 333 as col4
 WHERE partitionId = 1
 DISTRIBUTE BY partitionId
 SORT BY partitionId
 ) b

SELECT TRANSFORM(
 b.partitionId,b.col2,b.col3,b.col4
 )

 USING 'cat' as (a,b,c,d);set hive.mapred.mode=nonstrict;
CREATE TABLE empty (c INT) PARTITIONED BY (p INT);
SELECT MAX(c) FROM empty;
SELECT MAX(p) FROM empty;

ALTER TABLE empty ADD PARTITION (p=1);
SELECT MAX(p) FROM empty;

set hive.ddl.output.format=json;

CREATE TABLE add_part_test (key STRING, value STRING) PARTITIONED BY (ds STRING);
SHOW PARTITIONS add_part_test;

ALTER TABLE add_part_test ADD PARTITION (ds='2010-01-01');
SHOW PARTITIONS add_part_test;

ALTER TABLE add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-01');
SHOW PARTITIONS add_part_test;

ALTER TABLE add_part_test ADD IF NOT EXISTS PARTITION (ds='2010-01-02');
SHOW PARTITIONS add_part_test;

SHOW TABLE EXTENDED LIKE add_part_test PARTITION (ds='2010-01-02');

ALTER TABLE add_part_test DROP PARTITION (ds='2010-01-02');

DROP TABLE add_part_test;

set hive.ddl.output.format=text;
set hive.mapred.mode=nonstrict;
-- create testing table.
create table part_boolexpr(key int, value string) partitioned by (dt int, ts string);

-- both the below queries should return 0 rows
select count(*) from part_boolexpr where key = 'abc';
select * from part_boolexpr where dt = 'abc';
explain select count(1) from srcpart where true;
explain select count(1) from srcpart where false;
explain select count(1) from srcpart where true and hr='11';
explain select count(1) from srcpart where true or hr='11';
explain select count(1) from srcpart where false or hr='11';
explain select count(1) from srcpart where false and hr='11';set hive.mapred.mode=nonstrict;
drop table partition_char_1;

create table partition_char_1 (key string, value char(20)) partitioned by (dt char(10), region int);

insert overwrite table partition_char_1 partition(dt='2000-01-01', region=1)
  select * from src tablesample (10 rows);

select * from partition_char_1 limit 1;

drop table partition_char_1;
drop table if exists partcoltypenum;
create table partcoltypenum (key int, value string) partitioned by (tint tinyint, sint smallint, bint bigint);

-- add partition
alter table partcoltypenum add partition(tint=100Y, sint=20000S, bint=300000000000L);

-- describe partition
describe formatted partcoltypenum partition (tint=100, sint=20000S, bint='300000000000');

-- change partition file format
alter table partcoltypenum partition(tint=100, sint=20000S, bint='300000000000') set fileformat rcfile;
describe formatted partcoltypenum partition (tint=100Y, sint=20000S, bint=300000000000L);

-- change partition clusterby, sortby and bucket
alter table partcoltypenum partition(tint='100', sint=20000, bint=300000000000L) clustered by (key) sorted by (key desc) into 4 buckets;
describe formatted partcoltypenum partition (tint=100Y, sint=20000S, bint=300000000000L);

-- rename partition
alter table partcoltypenum partition(tint=100, sint=20000, bint=300000000000) rename to partition (tint=110Y, sint=22000S, bint=330000000000L);
describe formatted partcoltypenum partition (tint=110Y, sint=22000, bint='330000000000');

-- insert partition
insert into partcoltypenum partition (tint=110Y, sint=22000S, bint=330000000000L) select key, value from src limit 10;
insert into partcoltypenum partition (tint=110, sint=22000, bint=330000000000) select key, value from src limit 20;

-- select partition
select count(1) from partcoltypenum where tint=110Y and sint=22000S and bint=330000000000L;
select count(1) from partcoltypenum where tint=110Y and sint=22000 and bint='330000000000';

-- analyze partition statistics and columns statistics
analyze table partcoltypenum partition (tint=110Y, sint=22000S, bint=330000000000L) compute statistics;
describe extended partcoltypenum partition (tint=110Y, sint=22000S, bint=330000000000L);

analyze table partcoltypenum partition (tint=110Y, sint=22000S, bint=330000000000L) compute statistics for columns;
describe formatted partcoltypenum partition (tint=110Y, sint=22000S, bint=330000000000L) key;
describe formatted partcoltypenum partition (tint=110Y, sint=22000S, bint=330000000000L) value;

-- change table column type for partition
alter table partcoltypenum change key key decimal(10,0);
alter table partcoltypenum partition (tint=110Y, sint=22000S, bint=330000000000L) change key key decimal(10,0);
describe formatted partcoltypenum partition (tint=110Y, sint=22000S, bint=330000000000L);

-- change partititon column type
alter table partcoltypenum partition column (tint decimal(3,0));
describe formatted partcoltypenum partition (tint=110BD, sint=22000S, bint=330000000000L);

-- show partition
show partitions partcoltypenum partition (tint=110BD, sint=22000S, bint=330000000000L);

-- drop partition
alter table partcoltypenum drop partition (tint=110BD, sint=22000S, bint=330000000000L);
show partitions partcoltypenum;

-- change partition file location
insert into partcoltypenum partition (tint=100BD, sint=20000S, bint=300000000000L) select key, value from src limit 10;
describe formatted partcoltypenum partition (tint=100BD, sint=20000S, bint=300000000000L);
alter table partcoltypenum partition(tint=100BD, sint=20000S, bint=300000000000L) set location "file:/test/test/tint=1/sint=2/bint=3";
describe formatted partcoltypenum partition (tint=100BD, sint=20000S, bint=300000000000L);

drop table partcoltypenum;

drop table if exists partcoltypeothers;
create table partcoltypeothers (key int, value string) partitioned by (decpart decimal(6,2), datepart date);

set hive.typecheck.on.insert=false;
insert into partcoltypeothers partition (decpart = 1000.01BD, datepart = date '2015-4-13') select key, value from src limit 10;
show partitions partcoltypeothers;

set hive.typecheck.on.insert=true;
alter table partcoltypeothers partition(decpart = '1000.01BD', datepart = date '2015-4-13') rename to partition (decpart = 1000.01BD, datepart = date '2015-4-13');
show partitions partcoltypeothers;

drop table partcoltypeothers;


set hive.mapred.mode=nonstrict;
drop table partition_date_1;

create table partition_date_1 (key string, value string) partitioned by (dt date, region string);

insert overwrite table partition_date_1 partition(dt='2000-01-01', region= '1')
  select * from src tablesample (10 rows);
insert overwrite table partition_date_1 partition(dt='2000-01-01', region= '2')
  select * from src tablesample (5 rows);
insert overwrite table partition_date_1 partition(dt='2013-12-10', region= '2020-20-20')
  select * from src tablesample (5 rows);
insert overwrite table partition_date_1 partition(dt='2013-08-08', region= '1')
  select * from src tablesample (20 rows);
insert overwrite table partition_date_1 partition(dt='2013-08-08', region= '10')
  select * from src tablesample (11 rows);


select distinct dt from partition_date_1;
select * from partition_date_1 where dt = '2000-01-01' and region = '2' order by key,value;

-- 15
select count(*) from partition_date_1 where dt = date '2000-01-01';
-- 15.  Also try with string value in predicate
select count(*) from partition_date_1 where dt = '2000-01-01';
-- 5
select count(*) from partition_date_1 where dt = date '2000-01-01' and region = '2';
-- 11
select count(*) from partition_date_1 where dt = date '2013-08-08' and region = '10';
-- 30
select count(*) from partition_date_1 where region = '1';
-- 0
select count(*) from partition_date_1 where dt = date '2000-01-01' and region = '3';
-- 0
select count(*) from partition_date_1 where dt = date '1999-01-01';

-- Try other comparison operations

-- 20
select count(*) from partition_date_1 where dt > date '2000-01-01' and region = '1';
-- 10
select count(*) from partition_date_1 where dt < date '2000-01-02' and region = '1';
-- 20
select count(*) from partition_date_1 where dt >= date '2000-01-02' and region = '1';
-- 10
select count(*) from partition_date_1 where dt <= date '2000-01-01' and region = '1';
-- 20
select count(*) from partition_date_1 where dt <> date '2000-01-01' and region = '1';
-- 10
select count(*) from partition_date_1 where dt between date '1999-12-30' and date '2000-01-03' and region = '1';


-- Try a string key with date-like strings

-- 5
select count(*) from partition_date_1 where region = '2020-20-20';
-- 5
select count(*) from partition_date_1 where region > '2010-01-01';

drop table partition_date_1;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

drop table partition_date2_1;

create table partition_date2_1 (key string, value string) partitioned by (dt date, region int);

-- test date literal syntax
from (select * from src tablesample (1 rows)) x
insert overwrite table partition_date2_1 partition(dt=date '2000-01-01', region=1) select *
insert overwrite table partition_date2_1 partition(dt=date '2000-01-01', region=2) select *
insert overwrite table partition_date2_1 partition(dt=date '1999-01-01', region=2) select *;

select distinct dt from partition_date2_1;
select * from partition_date2_1;

-- insert overwrite
insert overwrite table partition_date2_1 partition(dt=date '2000-01-01', region=2)
  select 'changed_key', 'changed_value' from src tablesample (2 rows);
select * from partition_date2_1;

-- truncate
truncate table partition_date2_1 partition(dt=date '2000-01-01', region=2);
select distinct dt from partition_date2_1;
select * from partition_date2_1;

-- alter table add partition
alter table partition_date2_1 add partition (dt=date '1980-01-02', region=3);
select distinct dt from partition_date2_1;
select * from partition_date2_1;

-- alter table drop
alter table partition_date2_1 drop partition (dt=date '1999-01-01', region=2);
select distinct dt from partition_date2_1;
select * from partition_date2_1;

-- alter table set serde
alter table partition_date2_1 partition(dt=date '1980-01-02', region=3)
  set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';

-- alter table set fileformat
alter table partition_date2_1 partition(dt=date '1980-01-02', region=3)
  set fileformat rcfile;
describe extended partition_date2_1  partition(dt=date '1980-01-02', region=3);

insert overwrite table partition_date2_1 partition(dt=date '1980-01-02', region=3)
  select * from src tablesample (2 rows);
select * from partition_date2_1 order by key,value,dt,region;

-- alter table set location
alter table partition_date2_1 partition(dt=date '1980-01-02', region=3)
  set location "file:///tmp/partition_date2_1";
describe extended partition_date2_1 partition(dt=date '1980-01-02', region=3);

-- alter table touch
alter table partition_date2_1 touch partition(dt=date '1980-01-02', region=3);

drop table partition_date2_1;
create table sc as select *
from (select '2011-01-11', '2011-01-11+14:18:26' from src tablesample (1 rows)
      union all
      select '2011-01-11', '2011-01-11+15:18:26' from src tablesample (1 rows)
      union all
      select '2011-01-11', '2011-01-11+16:18:26' from src tablesample (1 rows) ) s;

create table sc_part (key string) partitioned by (ts string) stored as rcfile;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

set hive.decode.partition.name=false;
insert overwrite table sc_part partition(ts) select * from sc;
show partitions sc_part;
select count(*) from sc_part where ts is not null;

set hive.decode.partition.name=true;
insert overwrite table sc_part partition(ts) select * from sc;
show partitions sc_part;
select count(*) from sc_part where ts is not null;
set hive.mapred.mode=nonstrict;
create table partition_test_multilevel (key string, value string) partitioned by (level1 string, level2 string, level3 string);

insert overwrite table partition_test_multilevel partition(level1='1111', level2='111', level3='11') select key, value from srcpart tablesample (11 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='111', level3='22') select key, value from srcpart tablesample (12 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='111', level3='33') select key, value from srcpart tablesample (13 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='111', level3='44') select key, value from srcpart tablesample (14 rows);

insert overwrite table partition_test_multilevel partition(level1='1111', level2='222', level3='11') select key, value from srcpart tablesample (15 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='222', level3='22') select key, value from srcpart tablesample (16 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='222', level3='33') select key, value from srcpart tablesample (17 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='222', level3='44') select key, value from srcpart tablesample (18 rows);

insert overwrite table partition_test_multilevel partition(level1='1111', level2='333', level3='11') select key, value from srcpart tablesample (19 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='333', level3='22') select key, value from srcpart tablesample (20 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='333', level3='33') select key, value from srcpart tablesample (21 rows);
insert overwrite table partition_test_multilevel partition(level1='1111', level2='333', level3='44') select key, value from srcpart tablesample (22 rows);

insert overwrite table partition_test_multilevel partition(level1='2222', level2='111', level3='11') select key, value from srcpart tablesample (11 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='111', level3='22') select key, value from srcpart tablesample (12 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='111', level3='33') select key, value from srcpart tablesample (13 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='111', level3='44') select key, value from srcpart tablesample (14 rows);

insert overwrite table partition_test_multilevel partition(level1='2222', level2='222', level3='11') select key, value from srcpart tablesample (15 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='222', level3='22') select key, value from srcpart tablesample (16 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='222', level3='33') select key, value from srcpart tablesample (17 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='222', level3='44') select key, value from srcpart tablesample (18 rows);

insert overwrite table partition_test_multilevel partition(level1='2222', level2='333', level3='11') select key, value from srcpart tablesample (19 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='333', level3='22') select key, value from srcpart tablesample (20 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='333', level3='33') select key, value from srcpart tablesample (21 rows);
insert overwrite table partition_test_multilevel partition(level1='2222', level2='333', level3='44') select key, value from srcpart tablesample (22 rows);

set metaconf:hive.metastore.try.direct.sql=false;

-- beginning level partition in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level1 = '2222' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level1 >= '2222' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level1 !='2222' group by level1, level2, level3;

-- middle level partition in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level2 = '222' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level2 <= '222' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level2 != '222' group by level1, level2, level3;

-- ending level partition in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level3 = '22' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level3 >= '22' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level3 != '22' group by level1, level2, level3;

-- two different levels of partitions in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level2 >= '222' and level3 = '33' group by level1, level2, level3;

select level1, level2, level3, count(*) from partition_test_multilevel where level1 <= '1111' and level3 >= '33' group by level1, level2, level3;


-- all levels of partitions in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level1 = '2222' and level2 >= '222' and level3 <= '33' group by level1, level2, level3;

-- between
select level1, level2, level3, count(*) from partition_test_multilevel where (level1 = '2222') and (level2 between '222' and '333') and (level3 between '11' and '33') group by level1, level2, level3;

explain select level1, level2, level3, count(*) from partition_test_multilevel where (level1 = '2222') and (level2 between '222' and '333') and (level3 between '11' and '33') group by level1, level2, level3;

set metaconf:hive.metastore.try.direct.sql=true;

-- beginning level partition in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level1 = '2222' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level1 >= '2222' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level1 !='2222' group by level1, level2, level3;

-- middle level partition in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level2 = '222' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level2 <= '222' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level2 != '222' group by level1, level2, level3;

-- ending level partition in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level3 = '22' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level3 >= '22' group by level1, level2, level3;
select level1, level2, level3, count(*) from partition_test_multilevel where level3 != '22' group by level1, level2, level3;

-- two different levels of partitions in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level2 >= '222' and level3 = '33' group by level1, level2, level3;

select level1, level2, level3, count(*) from partition_test_multilevel where level1 <= '1111' and level3 >= '33' group by level1, level2, level3;


-- all levels of partitions in predicate
select level1, level2, level3, count(*) from partition_test_multilevel where level1 = '2222' and level2 >= '222' and level3 <= '33' group by level1, level2, level3;

-- between
select level1, level2, level3, count(*) from partition_test_multilevel where (level1 = '2222') and (level2 between '222' and '333') and (level3 between '11' and '33') group by level1, level2, level3;

explain select level1, level2, level3, count(*) from partition_test_multilevel where (level1 = '2222') and (level2 between '222' and '333') and (level3 between '11' and '33') group by level1, level2, level3;

create table partition_schema1(key string, value string) partitioned by (dt string);

insert overwrite table partition_schema1 partition(dt='100') select * from src1;
desc partition_schema1 partition(dt='100');

alter table partition_schema1 add columns (x string);

desc partition_schema1;
desc partition_schema1 partition (dt='100');


create table src_part_serde (key int, value string) partitioned by (ds string) stored as sequencefile;
insert overwrite table src_part_serde partition (ds='2011') select * from src;
alter table src_part_serde set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' with SERDEPROPERTIES ('serialization.format'='\t');
select key, value from src_part_serde where ds='2011' order by key, value limit 20;
create table sc as select *
from (select '2011-01-11', '2011-01-11+14:18:26' from src tablesample (1 rows)
      union all
      select '2011-01-11', '2011-01-11+15:18:26' from src tablesample (1 rows)
      union all
      select '2011-01-11', '2011-01-11+16:18:26' from src tablesample (1 rows) ) s;

create table sc_part (key string) partitioned by (ts string) stored as rcfile;

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table sc_part partition(ts) select * from sc;
show partitions sc_part;
select count(*) from sc_part where ts is not null;

insert overwrite table sc_part partition(ts) select * from sc;
show partitions sc_part;
select count(*) from sc_part where ts is not null;
set hive.mapred.mode=nonstrict;
-- Exclude test on Windows due to space character being escaped in Hive paths on Windows.
-- EXCLUDE_OS_WINDOWS
drop table partition_timestamp_1;

create table partition_timestamp_1 (key string, value string) partitioned by (dt timestamp, region string);

insert overwrite table partition_timestamp_1 partition(dt='2000-01-01 01:00:00', region= '1')
  select * from src tablesample (10 rows);
insert overwrite table partition_timestamp_1 partition(dt='2000-01-01 02:00:00', region= '2')
  select * from src tablesample (5 rows);
insert overwrite table partition_timestamp_1 partition(dt='2001-01-01 01:00:00', region= '2020-20-20')
  select * from src tablesample (5 rows);
insert overwrite table partition_timestamp_1 partition(dt='2001-01-01 02:00:00', region= '1')
  select * from src tablesample (20 rows);
insert overwrite table partition_timestamp_1 partition(dt='2001-01-01 03:00:00', region= '10')
  select * from src tablesample (11 rows);

select distinct dt from partition_timestamp_1;
select * from partition_timestamp_1 where dt = '2000-01-01 01:00:00' and region = '2' order by key,value;

-- 10
select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 01:00:00';
-- 10.  Also try with string value in predicate
select count(*) from partition_timestamp_1 where dt = '2000-01-01 01:00:00';
-- 5
select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 02:00:00' and region = '2';
-- 11
select count(*) from partition_timestamp_1 where dt = timestamp '2001-01-01 03:00:00' and region = '10';
-- 30
select count(*) from partition_timestamp_1 where region = '1';
-- 0
select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 01:00:00' and region = '3';
-- 0
select count(*) from partition_timestamp_1 where dt = timestamp '1999-01-01 01:00:00';

-- Try other comparison operations

-- 20
select count(*) from partition_timestamp_1 where dt > timestamp '2000-01-01 01:00:00' and region = '1';
-- 10
select count(*) from partition_timestamp_1 where dt < timestamp '2000-01-02 01:00:00' and region = '1';
-- 20
select count(*) from partition_timestamp_1 where dt >= timestamp '2000-01-02 01:00:00' and region = '1';
-- 10
select count(*) from partition_timestamp_1 where dt <= timestamp '2000-01-01 01:00:00' and region = '1';
-- 20
select count(*) from partition_timestamp_1 where dt <> timestamp '2000-01-01 01:00:00' and region = '1';
-- 10
select count(*) from partition_timestamp_1 where dt between timestamp '1999-12-30 12:00:00' and timestamp '2000-01-03 12:00:00' and region = '1';


-- Try a string key with timestamp-like strings

-- 5
select count(*) from partition_timestamp_1 where region = '2020-20-20';
-- 5
select count(*) from partition_timestamp_1 where region > '2010-01-01';

drop table partition_timestamp_1;
set hive.mapred.mode=nonstrict;
-- Exclude test on Windows due to space character being escaped in Hive paths on Windows.
-- EXCLUDE_OS_WINDOWS
drop table partition_timestamp2_1;

create table partition_timestamp2_1 (key string, value string) partitioned by (dt timestamp, region int);

-- test timestamp literal syntax
from (select * from src tablesample (1 rows)) x
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '2000-01-01 00:00:00', region=1) select *
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '2000-01-01 01:00:00', region=1) select *
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '1999-01-01 00:00:00', region=2) select *
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '1999-01-01 01:00:00', region=2) select *;

select distinct dt from partition_timestamp2_1;
select * from partition_timestamp2_1;

-- insert overwrite
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '2000-01-01 00:00:00', region=1)
  select 'changed_key', 'changed_value' from src tablesample (2 rows);
select * from partition_timestamp2_1;

-- truncate
truncate table partition_timestamp2_1 partition(dt=timestamp '2000-01-01 00:00:00', region=1);
select distinct dt from partition_timestamp2_1;
select * from partition_timestamp2_1;

-- alter table add partition
alter table partition_timestamp2_1 add partition (dt=timestamp '1980-01-02 00:00:00', region=3);
select distinct dt from partition_timestamp2_1;
select * from partition_timestamp2_1;

-- alter table drop
alter table partition_timestamp2_1 drop partition (dt=timestamp '1999-01-01 01:00:00', region=2);
select distinct dt from partition_timestamp2_1;
select * from partition_timestamp2_1;

-- alter table set serde
alter table partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3)
  set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';

-- alter table set fileformat
alter table partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3)
  set fileformat rcfile;
describe extended partition_timestamp2_1  partition(dt=timestamp '1980-01-02 00:00:00', region=3);

insert overwrite table partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3)
  select * from src tablesample (2 rows);
select * from partition_timestamp2_1 order by key,value,dt,region;

-- alter table set location
alter table partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3)
  set location "file:///tmp/partition_timestamp2_1";
describe extended partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3);

-- alter table touch
alter table partition_timestamp2_1 touch partition(dt=timestamp '1980-01-02 00:00:00', region=3);

drop table partition_timestamp2_1;
-- Windows-specific due to space character being escaped in Hive paths on Windows.
-- INCLUDE_OS_WINDOWS
drop table partition_timestamp2_1;

create table partition_timestamp2_1 (key string, value string) partitioned by (dt timestamp, region int);

-- test timestamp literal syntax
from (select * from src tablesample (1 rows)) x
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '2000-01-01 00:00:00', region=1) select *
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '2000-01-01 01:00:00', region=1) select *
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '1999-01-01 00:00:00', region=2) select *
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '1999-01-01 01:00:00', region=2) select *;

select distinct dt from partition_timestamp2_1;
select * from partition_timestamp2_1;

-- insert overwrite
insert overwrite table partition_timestamp2_1 partition(dt=timestamp '2000-01-01 00:00:00', region=1)
  select 'changed_key', 'changed_value' from src tablesample (2 rows);
select * from partition_timestamp2_1;

-- truncate
truncate table partition_timestamp2_1 partition(dt=timestamp '2000-01-01 00:00:00', region=1);
select distinct dt from partition_timestamp2_1;
select * from partition_timestamp2_1;

-- alter table add partition
alter table partition_timestamp2_1 add partition (dt=timestamp '1980-01-02 00:00:00', region=3);
select distinct dt from partition_timestamp2_1;
select * from partition_timestamp2_1;

-- alter table drop
alter table partition_timestamp2_1 drop partition (dt=timestamp '1999-01-01 01:00:00', region=2);
select distinct dt from partition_timestamp2_1;
select * from partition_timestamp2_1;

-- alter table set serde
alter table partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3)
  set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';

-- alter table set fileformat
alter table partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3)
  set fileformat rcfile;
describe extended partition_timestamp2_1  partition(dt=timestamp '1980-01-02 00:00:00', region=3);

insert overwrite table partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3)
  select * from src tablesample (2 rows);
select * from partition_timestamp2_1 order by key,value,dt,region;

-- alter table set location
alter table partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3)
  set location "file:///tmp/partition_timestamp2_1";
describe extended partition_timestamp2_1 partition(dt=timestamp '1980-01-02 00:00:00', region=3);

-- alter table touch
alter table partition_timestamp2_1 touch partition(dt=timestamp '1980-01-02 00:00:00', region=3);

drop table partition_timestamp2_1;
-- Windows-specific test due to space character being escaped in Hive paths on Windows.
-- INCLUDE_OS_WINDOWS
drop table partition_timestamp_1;

create table partition_timestamp_1 (key string, value string) partitioned by (dt timestamp, region string);

insert overwrite table partition_timestamp_1 partition(dt='2000-01-01 01:00:00', region= '1')
  select * from src tablesample (10 rows);
insert overwrite table partition_timestamp_1 partition(dt='2000-01-01 02:00:00', region= '2')
  select * from src tablesample (5 rows);
insert overwrite table partition_timestamp_1 partition(dt='2001-01-01 01:00:00', region= '2020-20-20')
  select * from src tablesample (5 rows);
insert overwrite table partition_timestamp_1 partition(dt='2001-01-01 02:00:00', region= '1')
  select * from src tablesample (20 rows);
insert overwrite table partition_timestamp_1 partition(dt='2001-01-01 03:00:00', region= '10')
  select * from src tablesample (11 rows);

select distinct dt from partition_timestamp_1;
select * from partition_timestamp_1 where dt = '2000-01-01 01:00:00' and region = '2' order by key,value;

-- 10
select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 01:00:00';
-- 10.  Also try with string value in predicate
select count(*) from partition_timestamp_1 where dt = '2000-01-01 01:00:00';
-- 5
select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 02:00:00' and region = '2';
-- 11
select count(*) from partition_timestamp_1 where dt = timestamp '2001-01-01 03:00:00' and region = '10';
-- 30
select count(*) from partition_timestamp_1 where region = '1';
-- 0
select count(*) from partition_timestamp_1 where dt = timestamp '2000-01-01 01:00:00' and region = '3';
-- 0
select count(*) from partition_timestamp_1 where dt = timestamp '1999-01-01 01:00:00';

-- Try other comparison operations

-- 20
select count(*) from partition_timestamp_1 where dt > timestamp '2000-01-01 01:00:00' and region = '1';
-- 10
select count(*) from partition_timestamp_1 where dt < timestamp '2000-01-02 01:00:00' and region = '1';
-- 20
select count(*) from partition_timestamp_1 where dt >= timestamp '2000-01-02 01:00:00' and region = '1';
-- 10
select count(*) from partition_timestamp_1 where dt <= timestamp '2000-01-01 01:00:00' and region = '1';
-- 20
select count(*) from partition_timestamp_1 where dt <> timestamp '2000-01-01 01:00:00' and region = '1';
-- 10
select count(*) from partition_timestamp_1 where dt between timestamp '1999-12-30 12:00:00' and timestamp '2000-01-03 12:00:00' and region = '1';


-- Try a string key with timestamp-like strings

-- 5
select count(*) from partition_timestamp_1 where region = '2020-20-20';
-- 5
select count(*) from partition_timestamp_1 where region > '2010-01-01';

drop table partition_timestamp_1;
set hive.mapred.mode=nonstrict;
set hive.typecheck.on.insert = true;

-- begin part(string, string) pass(string, int)
CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day string) stored as textfile;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day=2);

select * from tab1;
drop table tab1;

-- begin part(string, int) pass(string, string)
CREATE TABLE tab1 (id1 int,id2 string) PARTITIONED BY(month string,day int) stored as textfile;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='2');

select * from tab1;
drop table tab1;

-- begin part(string, date) pass(string, date)
create table tab1 (id1 int, id2 string) PARTITIONED BY(month string,day date) stored as textfile;
alter table tab1 add partition (month='June', day='2008-01-01');
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' overwrite into table tab1 PARTITION(month='June', day='2008-01-01');

select id1, id2, day from tab1 where day='2008-01-01';
drop table tab1;

-- Test partition column type is considered as the type given in table def
-- and not as 'string'
CREATE TABLE datePartTbl(col1 string) PARTITIONED BY (date_prt date);

-- Add test partitions and some sample data
INSERT OVERWRITE TABLE datePartTbl PARTITION(date_prt='2014-08-09')
  SELECT 'col1-2014-08-09' FROM src LIMIT 1;

INSERT OVERWRITE TABLE datePartTbl PARTITION(date_prt='2014-08-10')
  SELECT 'col1-2014-08-10' FROM src LIMIT 1;

-- Query where 'date_prt' value is restricted to given values in IN operator.
SELECT * FROM datePartTbl WHERE date_prt IN (CAST('2014-08-09' AS DATE), CAST('2014-08-08' AS DATE));

DROP TABLE datePartTbl;
set hive.mapred.mode=nonstrict;
drop table partition_varchar_1;

create table partition_varchar_1 (key string, value varchar(20)) partitioned by (dt varchar(10), region int);

insert overwrite table partition_varchar_1 partition(dt='2000-01-01', region=1)
  select * from src tablesample (10 rows);
insert overwrite table partition_varchar_1 partition(dt='2000-01-01', region=2)
  select * from src tablesample (5 rows);
insert overwrite table partition_varchar_1 partition(dt='2013-08-08', region=1)
  select * from src tablesample (20 rows);
insert overwrite table partition_varchar_1 partition(dt='2013-08-08', region=10)
  select * from src tablesample (11 rows);

select distinct dt from partition_varchar_1;
select * from partition_varchar_1 where dt = '2000-01-01' and region = 2 order by key,value;

-- 15
select count(*) from partition_varchar_1 where dt = '2000-01-01';
-- 5
select count(*) from partition_varchar_1 where dt = '2000-01-01' and region = 2;
-- 11
select count(*) from partition_varchar_1 where dt = '2013-08-08' and region = 10;
-- 30
select count(*) from partition_varchar_1 where region = 1;
-- 0
select count(*) from partition_varchar_1 where dt = '2000-01-01' and region = 3;
-- 0
select count(*) from partition_varchar_1 where dt = '1999-01-01';

-- Try other comparison operations

-- 20
select count(*) from partition_varchar_1 where dt > '2000-01-01' and region = 1;
-- 10
select count(*) from partition_varchar_1 where dt < '2000-01-02' and region = 1;
-- 20
select count(*) from partition_varchar_1 where dt >= '2000-01-02' and region = 1;
-- 10
select count(*) from partition_varchar_1 where dt <= '2000-01-01' and region = 1;
-- 20
select count(*) from partition_varchar_1 where dt <> '2000-01-01' and region = 1;

drop table partition_varchar_1;
drop table partition_varchar_2;

create table partition_varchar_2 (key string, value varchar(20)) partitioned by (dt varchar(15), region int);

insert overwrite table partition_varchar_2 partition(dt='2000-01-01', region=1)
  select * from src order by key limit 1;

select * from partition_varchar_2 where cast(dt as varchar(10)) = '2000-01-01';

drop table partition_varchar_2;
set hive.mapred.mode=nonstrict;


create table partition_vs_table(key string, value string) partitioned by (ds string);

insert overwrite table partition_vs_table partition(ds='100') select key, value from src;

alter table partition_vs_table add columns (newcol string);

insert overwrite table partition_vs_table partition(ds='101') select key, value, key from src;

select key, value, newcol from partition_vs_table
order by key, value, newcol;

set hive.mapred.mode=nonstrict;


create table partition_test_partitioned(key string, value string) partitioned by (dt string);

insert overwrite table partition_test_partitioned partition(dt=100) select * from src1;
show table extended like partition_test_partitioned;
show table extended like partition_test_partitioned partition(dt=100);
select key from partition_test_partitioned where dt=100;
select key from partition_test_partitioned;

alter table partition_test_partitioned set fileformat rcfile;
insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;
show table extended like partition_test_partitioned;
show table extended like partition_test_partitioned partition(dt=100);
show table extended like partition_test_partitioned partition(dt=101);
select key from partition_test_partitioned where dt=100;
select key from partition_test_partitioned where dt=101;
select key from partition_test_partitioned;

alter table partition_test_partitioned set fileformat Sequencefile;
insert overwrite table partition_test_partitioned partition(dt=102) select * from src1;
show table extended like partition_test_partitioned;
show table extended like partition_test_partitioned partition(dt=100);
show table extended like partition_test_partitioned partition(dt=101);
show table extended like partition_test_partitioned partition(dt=102);
select key from partition_test_partitioned where dt=100;
select key from partition_test_partitioned where dt=101;
select key from partition_test_partitioned where dt=102;
select key from partition_test_partitioned;

select key from partition_test_partitioned where dt >=100 and dt <= 102;

set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- This tests that the schema can be changed for binary serde data
create table prt(key string, value string) partitioned by (dt string);
insert overwrite table prt partition(dt='1') select * from src where key = 238;

select * from prt where dt is not null;
select key+key, value from prt where dt is not null;

alter table prt add columns (value2 string);

select key+key, value from prt where dt is not null;
select * from prt where dt is not null;
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- This tests that the schema can be changed for binary serde data
create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile;
alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
insert overwrite table partition_test_partitioned partition(dt='1') select * from src where key = 238;

select * from partition_test_partitioned where dt is not null;
select key+key, value from partition_test_partitioned where dt is not null;
set hive.metastore.disallow.incompatible.col.type.changes=false;
alter table partition_test_partitioned change key key int;
reset hive.metastore.disallow.incompatible.col.type.changes;
select key+key, value from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;

alter table partition_test_partitioned add columns (value2 string);

select key+key, value from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- This tests that the schema can be changed for binary serde data
create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile;
alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
insert overwrite table partition_test_partitioned partition(dt='1') select * from src where key = 238;

select * from partition_test_partitioned where dt is not null;
select key+key, value from partition_test_partitioned where dt is not null;
set hive.metastore.disallow.incompatible.col.type.changes=false;
alter table partition_test_partitioned change key key int;
reset hive.metastore.disallow.incompatible.col.type.changes;
select key+key, value from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;

insert overwrite table partition_test_partitioned partition(dt='2') select * from src where key = 97;

alter table partition_test_partitioned add columns (value2 string);

select key+key, value from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;

insert overwrite table partition_test_partitioned partition(dt='3') select key, value, value from src where key = 200;

select key+key, value, value2 from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;
set hive.mapred.mode=nonstrict;
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- This tests that the schema can be changed for partitioned tables for binary serde data for joins
create table T1(key string, value string) partitioned by (dt string) stored as rcfile;
alter table T1 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
insert overwrite table T1 partition (dt='1') select * from src where key = 238 or key = 97;
set hive.metastore.disallow.incompatible.col.type.changes=false;
alter table T1 change key key int;

insert overwrite table T1 partition (dt='2') select * from src where key = 238 or key = 97;

alter table T1 change key key string;

create table T2(key string, value string) partitioned by (dt string) stored as rcfile;
insert overwrite table T2 partition (dt='1') select * from src where key = 238 or key = 97;

select /* + MAPJOIN(a) */ count(*) FROM T1 a JOIN T2 b ON a.key = b.key;
select count(*) FROM T1 a JOIN T2 b ON a.key = b.key;
reset hive.metastore.disallow.incompatible.col.type.changes;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 1;

CREATE TABLE tbl1(key int, value string) PARTITIONED by (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS rcfile;
CREATE TABLE tbl2(key int, value string) PARTITIONED by (ds string)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS rcfile;

alter table tbl1 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
alter table tbl2 set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';

insert overwrite table tbl1 partition (ds='1') select * from src where key < 10;
insert overwrite table tbl2 partition (ds='1') select * from src where key < 10;

alter table tbl1 change key key int;
insert overwrite table tbl1 partition (ds='2') select * from src where key < 10;

alter table tbl1 change key key string;

-- The subquery itself is being map-joined. Multiple partitions of tbl1 with different schemas are being read for tbl2
select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

set hive.optimize.bucketmapjoin = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should
-- be converted to a bucketized mapside join. Multiple partitions of tbl1 with different schemas are being read for each
-- bucket of tbl2
select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

set hive.optimize.bucketmapjoin.sortedmerge = true;

-- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join. Multiple partitions of tbl1 with different schemas are being read for a
-- given file of tbl2
select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

-- Since the join key is modified by the sub-query, neither sort-merge join not bucketized map-side
-- join should be performed.  Multiple partitions of tbl1 with different schemas are being read for tbl2
select /*+mapjoin(subq1)*/ count(*) from
  (select a.key+1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key+1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- This tests that the schema can be changed for binary serde data
create table partition_test_partitioned(key string, value string)
partitioned by (dt string) stored as rcfile;
insert overwrite table partition_test_partitioned partition(dt='1')
select * from src where key = 238;

select * from partition_test_partitioned where dt is not null;
select key+key, value from partition_test_partitioned where dt is not null;
set hive.metastore.disallow.incompatible.col.type.changes=false;
alter table partition_test_partitioned change key key int;
reset hive.metastore.disallow.incompatible.col.type.changes;
select key+key, value from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;

alter table partition_test_partitioned add columns (value2 string);

select key+key, value from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;

insert overwrite table partition_test_partitioned partition(dt='2')
select key, value, value from src where key = 86;

select key+key, value, value2, dt from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- This tests that the schema can be changed for binary serde data
create table partition_test_partitioned(key string, value string)
partitioned by (dt string) stored as textfile;
insert overwrite table partition_test_partitioned partition(dt='1')
select * from src where key = 238;

select * from partition_test_partitioned where dt is not null;
select key+key, value from partition_test_partitioned where dt is not null;
set hive.metastore.disallow.incompatible.col.type.changes=false;
alter table partition_test_partitioned change key key int;
reset hive.metastore.disallow.incompatible.col.type.changes;
select key+key, value from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;

alter table partition_test_partitioned add columns (value2 string);

select key+key, value from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;

insert overwrite table partition_test_partitioned partition(dt='2')
select key, value, value from src where key = 86;

select key+key, value, value2, dt from partition_test_partitioned where dt is not null;
select * from partition_test_partitioned where dt is not null;
set hive.mapred.mode=nonstrict;
-- HIVE-5199, HIVE-5285 : CustomSerDe(1, 2, 3) are used here.
-- The final results should be all NULL columns deserialized using
-- CustomSerDe(1, 2, 3) irrespective of the inserted values

DROP TABLE PW17;
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-custom-serde/${system:hive.version}/hive-it-custom-serde-${system:hive.version}.jar;
CREATE TABLE PW17(`USER` STRING, COMPLEXDT ARRAY<INT>) PARTITIONED BY (YEAR STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe1';
LOAD DATA LOCAL INPATH '../../data/files/pw17.txt' INTO TABLE PW17 PARTITION (YEAR='1');
ALTER TABLE PW17 PARTITION(YEAR='1') SET SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe2';
ALTER TABLE PW17 SET SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe1';
-- Without the fix HIVE-5199, will throw cast exception via FetchOperator
SELECT * FROM PW17;

-- Test for non-parititioned table.
DROP TABLE PW17_2;
CREATE TABLE PW17_2(`USER` STRING, COMPLEXDT ARRAY<INT>) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe1';
LOAD DATA LOCAL INPATH '../../data/files/pw17.txt' INTO TABLE PW17_2;
-- Without the fix HIVE-5199, will throw cast exception via MapOperator
SELECT COUNT(*) FROM PW17_2;

DROP TABLE PW17_3;
CREATE TABLE PW17_3(`USER` STRING, COMPLEXDT ARRAY<ARRAY<INT> >) PARTITIONED BY (YEAR STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe3';
LOAD DATA LOCAL INPATH '../../data/files/pw17.txt' INTO TABLE PW17_3 PARTITION (YEAR='1');
ALTER TABLE PW17_3 PARTITION(YEAR='1') SET SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe2';
ALTER TABLE PW17_3 SET SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe3';
-- Without the fix HIVE-5285, will throw cast exception via FetchOperator
SELECT * FROM PW17;

DROP TABLE PW17_4;
CREATE TABLE PW17_4(`USER` STRING, COMPLEXDT ARRAY<ARRAY<INT> >) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe3';
LOAD DATA LOCAL INPATH '../../data/files/pw17.txt' INTO TABLE PW17_4;
-- Without the fix HIVE-5285, will throw cast exception via MapOperator
SELECT COUNT(*) FROM PW17_4;

set hive.mapred.mode=nonstrict;
-- HIVE-5202 : Tests for SettableUnionObjectInspectors
-- CustomSerDe(4,5) are used here.
-- The final results should be all NULL columns deserialized using
-- CustomSerDe(4, 5) irrespective of the inserted values

DROP TABLE PW18;
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-custom-serde/${system:hive.version}/hive-it-custom-serde-${system:hive.version}.jar;
CREATE TABLE PW18(`USER` STRING, COMPLEXDT UNIONTYPE<INT, DOUBLE>) PARTITIONED BY (YEAR STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe5';
LOAD DATA LOCAL INPATH '../../data/files/pw17.txt' INTO TABLE PW18 PARTITION (YEAR='1');
ALTER TABLE PW18 PARTITION(YEAR='1') SET SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe4';
-- Without the fix HIVE-5202, will throw unsupported data type exception.
SELECT * FROM PW18;

-- Test for non-parititioned table.
DROP TABLE PW18_2;
CREATE TABLE PW18_2(`USER` STRING, COMPLEXDT UNIONTYPE<INT, DOUBLE>) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.CustomSerDe5';
LOAD DATA LOCAL INPATH '../../data/files/pw17.txt' INTO TABLE PW18_2;
-- Without the fix HIVE-5202, will throw unsupported data type exception
SELECT COUNT(*) FROM PW18_2;
set hive.mapred.mode=nonstrict;
-- SORT_BEFORE_DIFF

create table partition_test_partitioned(key string, value string) partitioned by (dt string);

insert overwrite table partition_test_partitioned partition(dt=100) select * from src1;
alter table partition_test_partitioned set fileformat rcfile;
insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;
alter table partition_test_partitioned set fileformat Sequencefile;
insert overwrite table partition_test_partitioned partition(dt=102) select * from src1;

set hive.fetch.task.conversion=minimal;
explain select *, BLOCK__OFFSET__INSIDE__FILE from partition_test_partitioned where dt >=100 and dt <= 102;
select * from partition_test_partitioned where dt >=100 and dt <= 102;

set hive.fetch.task.conversion=more;
explain select *, BLOCK__OFFSET__INSIDE__FILE from partition_test_partitioned where dt >=100 and dt <= 102;
select * from partition_test_partitioned where dt >=100 and dt <= 102;



create table partition_test_partitioned(key string, value string) partitioned by (dt string);

alter table partition_test_partitioned set fileformat rcfile;
insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;
show table extended like partition_test_partitioned partition(dt=101);

alter table partition_test_partitioned set fileformat Sequencefile;
insert overwrite table partition_test_partitioned partition(dt=102) select * from src1;
show table extended like partition_test_partitioned partition(dt=102);
select key from partition_test_partitioned where dt=102;

insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;
show table extended like partition_test_partitioned partition(dt=101);
select key from partition_test_partitioned where dt=101;


create table partition_test_partitioned(key string, value string) partitioned by (dt string);
alter table partition_test_partitioned set fileformat sequencefile;
insert overwrite table partition_test_partitioned partition(dt='1') select * from src1;
alter table partition_test_partitioned partition (dt='1') set fileformat sequencefile;

alter table partition_test_partitioned add partition (dt='2');
alter table partition_test_partitioned drop partition (dt='2');

set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

create table partition_test_partitioned(key string, value string) partitioned by (dt string);

alter table partition_test_partitioned set fileformat rcfile;
insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;
alter table partition_test_partitioned set fileformat Sequencefile;
insert overwrite table partition_test_partitioned partition(dt=102) select * from src1;

select dt, count(1) from partition_test_partitioned where dt is not null group by dt;

insert overwrite table partition_test_partitioned partition(dt=103) select * from src1;

select dt, count(1) from partition_test_partitioned where dt is not null group by dt;
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

create table partition_test_partitioned(key string, value string) partitioned by (dt string);

alter table partition_test_partitioned set fileformat rcfile;
insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;
alter table partition_test_partitioned set fileformat Sequencefile;

insert overwrite table partition_test_partitioned partition(dt=102) select * from src1;

select count(1) from
(select key, value from partition_test_partitioned where dt=101 and key < 100
 union all
select key, value from partition_test_partitioned where dt=101 and key < 20)s;

select count(1) from
(select key, value from partition_test_partitioned where dt=101 and key < 100
 union all
select key, value from partition_test_partitioned where dt=102 and key < 20)s;
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

create table partition_test_partitioned(key string, value string) partitioned by (dt string);

alter table partition_test_partitioned set fileformat rcfile;
insert overwrite table partition_test_partitioned partition(dt=101) select * from src1;

select count(1) from partition_test_partitioned  a join partition_test_partitioned  b on a.key = b.key
where a.dt = '101' and b.dt = '101';

select count(1) from partition_test_partitioned  a join partition_test_partitioned  b on a.key = b.key
where a.dt = '101' and b.dt = '101' and a.key < 100;set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- This tests that a query can span multiple partitions which can not only have different file formats, but
-- also different serdes
create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile;
insert overwrite table partition_test_partitioned partition(dt='1') select * from src;
alter table partition_test_partitioned set fileformat sequencefile;
insert overwrite table partition_test_partitioned partition(dt='2') select * from src;
alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';
insert overwrite table partition_test_partitioned partition(dt='3') select * from src;

select * from partition_test_partitioned where dt is not null order by key, value, dt limit 20;
select key+key as key, value, dt from partition_test_partitioned where dt is not null order by key, value, dt limit 20;
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

-- This tests that a query can span multiple partitions which can not only have different file formats, but
-- also different serdes
create table partition_test_partitioned(key string, value string) partitioned by (dt string) stored as rcfile;
insert overwrite table partition_test_partitioned partition(dt='1') select * from src;
alter table partition_test_partitioned set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';
insert overwrite table partition_test_partitioned partition(dt='2') select * from src;

select * from partition_test_partitioned where dt is not null order by key, value, dt limit 20;
select key+key as key, value, dt from partition_test_partitioned where dt is not null order by key, value, dt limit 20;

create table t (a string) partitioned by (b map<string,string>);
set hive.metastore.partition.inherit.table.properties=a,b;
-- The property needs to be unset at the end of the test till HIVE-3109/HIVE-3112 is fixed

create table mytbl (c1 tinyint) partitioned by (c2 string) tblproperties ('a'='myval','b'='yourval','c'='noval');
alter table mytbl add partition (c2 = 'v1');
describe formatted mytbl partition (c2='v1');

set hive.metastore.partition.inherit.table.properties=;
set hive.metastore.partition.inherit.table.properties="";
create table mytbl (c1 tinyint) partitioned by (c2 string) tblproperties ('a'='myval','b'='yourval','c'='noval');
alter table mytbl add partition (c2 = 'v1');
describe formatted mytbl partition (c2='v1');
set hive.metastore.partition.inherit.table.properties=key1,*;
-- The property needs to be unset at the end of the test till HIVE-3109/HIVE-3112 is fixed

create table mytbl (c1 tinyint) partitioned by (c2 string) tblproperties ('a'='myval','b'='yourval','c'='noval');
alter table mytbl add partition (c2 = 'v1');
describe formatted mytbl partition (c2='v1');

set hive.metastore.partition.inherit.table.properties=;
set hive.mapred.mode=nonstrict;
drop table pcr_t1;
drop table pcr_t2;
drop table pcr_t3;

create table pcr_t1 (key int, value string) partitioned by (ds string);

insert overwrite table pcr_t1 partition (ds='2000-04-08') select * from src where key < 20 order by key;
insert overwrite table pcr_t1 partition (ds='2000-04-09') select * from src where key < 20 order by key;
insert overwrite table pcr_t1 partition (ds='2000-04-10') select * from src where key < 20 order by key;

explain extended select key, value, ds from pcr_t1 where ds<='2000-04-09' and key<5 order by key, ds;
select key, value, ds from pcr_t1 where ds<='2000-04-09' and key<5 order by key, ds;

explain extended select key, value from pcr_t1 where ds<='2000-04-09' or key<5 order by key;
select key, value from pcr_t1 where ds<='2000-04-09' or key<5 order by key;

explain extended select key, value, ds from pcr_t1 where ds<='2000-04-09' and key<5 and value != 'val_2' order by key, ds;
select key, value, ds from pcr_t1 where ds<='2000-04-09' and key<5 and value != 'val_2' order by key, ds;


explain extended
select key, value, ds from pcr_t1
where (ds < '2000-04-09' and key < 5) or (ds > '2000-04-09' and value == 'val_5') order by key, ds;

select key, value, ds from pcr_t1
where (ds < '2000-04-09' and key < 5) or (ds > '2000-04-09' and value == 'val_5') order by key, ds;


explain extended
select key, value, ds from pcr_t1
where (ds < '2000-04-10' and key < 5) or (ds > '2000-04-08' and value == 'val_5') order by key, ds;

select key, value, ds from pcr_t1
where (ds < '2000-04-10' and key < 5) or (ds > '2000-04-08' and value == 'val_5') order by key, ds;


explain extended
select key, value, ds from pcr_t1
where (ds < '2000-04-10' or key < 5) and (ds > '2000-04-08' or value == 'val_5') order by key, ds;

select key, value, ds from pcr_t1
where (ds < '2000-04-10' or key < 5) and (ds > '2000-04-08' or value == 'val_5') order by key, ds;


explain extended select key, value from pcr_t1 where (ds='2000-04-08' or ds='2000-04-09') and key=14 order by key, value;
select key, value from pcr_t1 where (ds='2000-04-08' or ds='2000-04-09') and key=14 order by key, value;

explain extended select key, value from pcr_t1 where ds='2000-04-08' or ds='2000-04-09' order by key, value;
select key, value from pcr_t1 where ds='2000-04-08' or ds='2000-04-09' order by key, value;

explain extended select key, value from pcr_t1 where ds>='2000-04-08' or ds<'2000-04-10' order by key, value;
select key, value from pcr_t1 where ds>='2000-04-08' or ds<'2000-04-10' order by key, value;

explain extended select key, value, ds from pcr_t1 where (ds='2000-04-08' and key=1) or (ds='2000-04-09' and key=2) order by key, value, ds;
select key, value, ds from pcr_t1 where (ds='2000-04-08' and key=1) or (ds='2000-04-09' and key=2) order by key, value, ds;

explain extended select * from pcr_t1 t1 join pcr_t1 t2 on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-08' order by t1.key;
select * from pcr_t1 t1 join pcr_t1 t2 on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-08' order by t1.key;

explain extended select * from pcr_t1 t1 join pcr_t1 t2 on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-09' order by t1.key;
select * from pcr_t1 t1 join pcr_t1 t2 on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-09' order by t1.key;

insert overwrite table pcr_t1 partition (ds='2000-04-11') select * from src where key < 20 order by key;

explain extended select key, value, ds from pcr_t1 where (ds>'2000-04-08' and ds<'2000-04-11') or (ds>='2000-04-08' and ds<='2000-04-11' and key=2) order by key, value, ds;
select key, value, ds from pcr_t1 where (ds>'2000-04-08' and ds<'2000-04-11') or (ds>='2000-04-08' and ds<='2000-04-11' and key=2) order by key, value, ds;

explain extended select key, value, ds from pcr_t1 where (ds>'2000-04-08' and ds<'2000-04-11') or (ds<='2000-04-09' and key=2) order by key, value, ds;
select key, value, ds from pcr_t1 where (ds>'2000-04-08' and ds<'2000-04-11') or (ds<='2000-04-09' and key=2) order by key, value, ds;

create table pcr_t2 (key int, value string);
create table pcr_t3 (key int, value string);

explain extended
from pcr_t1
insert overwrite table pcr_t2 select key, value where ds='2000-04-08'
insert overwrite table pcr_t3 select key, value where ds='2000-04-08';

from pcr_t1
insert overwrite table pcr_t2 select key, value where ds='2000-04-08'
insert overwrite table pcr_t3 select key, value where ds='2000-04-08';

explain extended
from pcr_t1
insert overwrite table pcr_t2 select key, value where ds='2000-04-08' and key=2
insert overwrite table pcr_t3 select key, value where ds='2000-04-08' and key=3;

from pcr_t1
insert overwrite table pcr_t2 select key, value where ds='2000-04-08' and key=2
insert overwrite table pcr_t3 select key, value where ds='2000-04-08' and key=3;


explain extended select key, value from srcpart where ds='2008-04-08' and hr=11 order by key limit 10;
select key, value from srcpart where ds='2008-04-04' and hr=11 order by key limit 10;

explain extended select key, value, ds, hr from srcpart where ds='2008-04-08' and (hr='11' or hr='12') and key=11 order by key, ds, hr;
select key, value, ds, hr from srcpart where ds='2008-04-08' and (hr='11' or hr='12') and key=11 order by key, ds, hr;

explain extended select key, value, ds, hr from srcpart where hr='11' and key=11 order by key, ds, hr;
select key, value, ds, hr from srcpart where hr='11' and key=11 order by key, ds, hr;

drop table pcr_t1;
drop table pcr_t2;
drop table pcr_t3;


-- Test cases when a non-boolean ds expression has same and different values for all possible ds values:
drop table pcr_foo;
create table pcr_foo (key int, value string) partitioned by (ds int);
insert overwrite table pcr_foo partition (ds=3) select * from src where key < 10 order by key;
insert overwrite table pcr_foo partition (ds=5) select * from src where key < 10 order by key;
insert overwrite table pcr_foo partition (ds=7) select * from src where key < 10 order by key;

-- the condition is 'true' for all the 3 partitions (ds=3,5,7):
select key, value, ds from pcr_foo where (ds % 2 == 1);

-- the condition is 'true' for partitions (ds=3,5) but 'false' of partition ds=7:
select key, value, ds from pcr_foo where (ds / 3 < 2);

drop table pcr_foo;



-- Cover org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.FieldExprProcessor.
-- Create a table with a struct data:
create table ab(strct struct<a:int, b:string>)
row format delimited
  fields terminated by '\t'
  collection items terminated by '\001';
load data local inpath '../../data/files/kv1.txt'
overwrite into table ab;

-- Create partitioned table with struct data:
drop table foo_field;
create table foo_field (s struct<a:int,b:string>) partitioned by (ds int);
insert overwrite table foo_field partition (ds=5) select strct from ab where strct.a < 10 limit 2;
insert overwrite table foo_field partition (ds=7) select strct from ab where strct.a > 190 limit 2;
select s,ds from foo_field where ((ds + s.a) > 0) order by ds,s;

drop table foo_field;
explain select key,value from srcpart where cast(hr as double)  = cast(11 as double);
explain select key,value from srcpart where hr  = cast(11 as double);
explain select key,value from srcpart where cast(hr as double)  = 11 ;
set hive.mapred.mode=nonstrict;
drop table pcs_t1;
drop table pcs_t2;

create table pcs_t1 (key int, value string) partitioned by (ds string);
insert overwrite table pcs_t1 partition (ds='2000-04-08') select * from src where key < 20 order by key;
insert overwrite table pcs_t1 partition (ds='2000-04-09') select * from src where key < 20 order by key;
insert overwrite table pcs_t1 partition (ds='2000-04-10') select * from src where key < 20 order by key;

analyze table pcs_t1 partition(ds) compute statistics;
analyze table pcs_t1 partition(ds) compute statistics for columns;

set hive.optimize.point.lookup = true;
set hive.optimize.point.lookup.min = 1;

explain extended select key, value, ds from pcs_t1 where (ds='2000-04-08' and key=1) or (ds='2000-04-09' and key=2) order by key, value, ds;
select key, value, ds from pcs_t1 where (ds='2000-04-08' and key=1) or (ds='2000-04-09' and key=2) order by key, value, ds;

set hive.optimize.point.lookup = false;
set hive.optimize.partition.columns.separate=true;
set hive.optimize.ppd=true;

explain extended select ds from pcs_t1 where struct(ds, key) in (struct('2000-04-08',1), struct('2000-04-09',2));
select ds from pcs_t1 where struct(ds, key) in (struct('2000-04-08',1), struct('2000-04-09',2));

explain extended select ds from pcs_t1 where struct(ds, key+2) in (struct('2000-04-08',3), struct('2000-04-09',4));
select ds from pcs_t1 where struct(ds, key+2) in (struct('2000-04-08',3), struct('2000-04-09',4));

explain extended select /*+ MAPJOIN(pcs_t1) */ a.ds, b.key from pcs_t1 a join pcs_t1 b  on a.ds=b.ds where struct(a.ds, a.key, b.ds) in (struct('2000-04-08',1, '2000-04-09'), struct('2000-04-09',2, '2000-04-08'));

select /*+ MAPJOIN(pcs_t1) */ a.ds, b.key from pcs_t1 a join pcs_t1 b  on a.ds=b.ds where struct(a.ds, a.key, b.ds) in (struct('2000-04-08',1, '2000-04-09'), struct('2000-04-09',2, '2000-04-08'));

explain extended select ds from pcs_t1 where struct(ds, key+key) in (struct('2000-04-08',1), struct('2000-04-09',2));
select ds from pcs_t1 where struct(ds, key+key) in (struct('2000-04-08',1), struct('2000-04-09',2));

explain select lag(key) over (partition by key) as c1
from pcs_t1 where struct(ds, key) in (struct('2000-04-08',1), struct('2000-04-09',2));
select lag(key) over (partition by key) as c1
from pcs_t1 where struct(ds, key) in (struct('2000-04-08',1), struct('2000-04-09',2));

EXPLAIN EXTENDED
SELECT * FROM (
  SELECT X.* FROM pcs_t1 X WHERE struct(X.ds, X.key) in (struct('2000-04-08',1), struct('2000-04-09',2))
  UNION ALL
  SELECT Y.* FROM pcs_t1 Y WHERE struct(Y.ds, Y.key) in (struct('2000-04-08',1), struct('2000-04-09',2))
) A
WHERE A.ds = '2008-04-08'
SORT BY A.key, A.value, A.ds;

SELECT * FROM (
  SELECT X.* FROM pcs_t1 X WHERE struct(X.ds, X.key) in (struct('2000-04-08',1), struct('2000-04-09',2))
  UNION ALL
  SELECT Y.* FROM pcs_t1 Y WHERE struct(Y.ds, Y.key) in (struct('2000-04-08',1), struct('2000-04-09',2))
) A
WHERE A.ds = '2008-04-08'
SORT BY A.key, A.value, A.ds;

explain extended select ds from pcs_t1 where struct(case when ds='2000-04-08' then 10 else 20 end) in (struct(10),struct(11));
select ds from pcs_t1 where struct(case when ds='2000-04-08' then 10 else 20 end) in (struct(10),struct(11));

explain extended select ds from pcs_t1 where struct(ds, key, rand(100)) in (struct('2000-04-08',1,0.2), struct('2000-04-09',2,0.3));

explain extended select ds from pcs_t1 where struct(ds='2000-04-08' or key = 2, key) in (struct(true,2), struct(false,3));
select ds from pcs_t1 where struct(ds='2000-04-08' or key = 2, key) in (struct(true,2), struct(false,3));

explain extended select ds from pcs_t1 where key = 3 or (struct(ds='2000-04-08' or key = 2, key) in (struct(true,2), struct(false,3)) and key+5 > 0);
select ds from pcs_t1 where key = 3 or (struct(ds='2000-04-08' or key = 2, key) in (struct(true,2), struct(false,3)) and key+5 > 0);-- explain plan json:  the query gets the formatted json output of the query plan of the hive query

-- JAVA_VERSION_SPECIFIC_OUTPUT

EXPLAIN FORMATTED SELECT count(1) FROM src;
explain
SELECT key
FROM src
WHERE
   ((key = '0'
   AND value = '8') OR (key = '1'
   AND value = '5') OR (key = '2'
   AND value = '6') OR (key = '3'
   AND value = '8') OR (key = '4'
   AND value = '1') OR (key = '5'
   AND value = '6') OR (key = '6'
   AND value = '1') OR (key = '7'
   AND value = '1') OR (key = '8'
   AND value = '1') OR (key = '9'
   AND value = '1') OR (key = '10'
   AND value = '3'))
;


set hive.optimize.point.lookup.min=3;
set hive.optimize.partition.columns.separate=false;
explain
SELECT key
FROM src
WHERE
   ((key = '0'
   AND value = '8') OR (key = '1'
   AND value = '5') OR (key = '2'
   AND value = '6') OR (key = '3'
   AND value = '8') OR (key = '4'
   AND value = '1') OR (key = '5'
   AND value = '6') OR (key = '6'
   AND value = '1') OR (key = '7'
   AND value = '1') OR (key = '8'
   AND value = '1') OR (key = '9'
   AND value = '1') OR (key = '10'
   AND value = '3'))
;

set hive.optimize.partition.columns.separate=true;
explain
SELECT key
FROM src
WHERE
   ((key = '0'
   AND value = '8') OR (key = '1'
   AND value = '5') OR (key = '2'
   AND value = '6') OR (key = '3'
   AND value = '8') OR (key = '4'
   AND value = '1') OR (key = '5'
   AND value = '6') OR (key = '6'
   AND value = '1') OR (key = '7'
   AND value = '1') OR (key = '8'
   AND value = '1') OR (key = '9'
   AND value = '1') OR (key = '10'
   AND value = '3'))
;
set hive.mapred.mode=nonstrict;
drop table pcr_t1;
drop table pcr_t2;
drop table pcr_t3;

create table pcr_t1 (key int, value string) partitioned by (ds string);
insert overwrite table pcr_t1 partition (ds='2000-04-08') select * from src where key < 20 order by key;
insert overwrite table pcr_t1 partition (ds='2000-04-09') select * from src where key < 20 order by key;
insert overwrite table pcr_t1 partition (ds='2000-04-10') select * from src where key < 20 order by key;

create table pcr_t2 (ds string, key int, value string);
from pcr_t1
insert overwrite table pcr_t2 select ds, key, value where ds='2000-04-08';
from pcr_t1
insert overwrite table pcr_t2 select ds, key, value where ds='2000-04-08' and key=2;

set hive.optimize.point.lookup.min=2;
set hive.optimize.partition.columns.separate=true;

explain extended
select key, value, ds
from pcr_t1
where (ds='2000-04-08' and key=1) or (ds='2000-04-09' and key=2)
order by key, value, ds;

explain extended
select *
from pcr_t1 t1 join pcr_t1 t2
on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-08'
order by t1.key;

explain extended
select *
from pcr_t1 t1 join pcr_t1 t2
on t1.key=t2.key and t1.ds='2000-04-08' and t2.ds='2000-04-09'
order by t1.key;

explain extended
select *
from pcr_t1 t1 join pcr_t2 t2
where (t1.ds='2000-04-08' and t2.key=1) or (t1.ds='2000-04-09' and t2.key=2)
order by t2.key, t2.value, t1.ds;

explain extended
select *
from pcr_t1 t1 join pcr_t2 t2
where (t2.ds='2000-04-08' and t1.key=1) or (t2.ds='2000-04-09' and t1.key=2)
order by t1.key, t1.value, t2.ds;

drop table pcr_t1;
drop table pcr_t2;
drop table pcr_t3;set hive.mapred.mode=nonstrict;
drop table pcr_t1;

create table pcr_t1 (key int, value string) partitioned by (ds1 string, ds2 string);
insert overwrite table pcr_t1 partition (ds1='2000-04-08', ds2='2001-04-08') select * from src where key < 20 order by key;
insert overwrite table pcr_t1 partition (ds1='2000-04-09', ds2='2001-04-09') select * from src where key < 20 order by key;
insert overwrite table pcr_t1 partition (ds1='2000-04-10', ds2='2001-04-10') select * from src where key < 20 order by key;

set hive.optimize.point.lookup.min=2;
set hive.optimize.partition.columns.separate=true;

explain extended
select key, value, ds1, ds2
from pcr_t1
where (ds1='2000-04-08' and key=1) or (ds1='2000-04-09' and key=2)
order by key, value, ds1, ds2;

explain extended
select key, value, ds1, ds2
from pcr_t1
where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-08' and key=2)
order by key, value, ds1, ds2;

explain extended
select *
from pcr_t1 t1 join pcr_t1 t2
on t1.key=t2.key and t1.ds1='2000-04-08' and t2.ds2='2001-04-08'
order by t1.key;

explain extended
select *
from pcr_t1 t1 join pcr_t1 t2
on t1.key=t2.key and t1.ds1='2000-04-08' and t2.ds1='2000-04-09'
order by t1.key;

explain extended
select *
from pcr_t1 t1 join pcr_t1 t2
where (t1.ds1='2000-04-08' and t2.key=1) or (t1.ds1='2000-04-09' and t2.key=2)
order by t2.key, t2.value, t1.ds1;

drop table pcr_t1;
set hive.mapred.mode=nonstrict;
drop table pcr_t1;

create table pcr_t1 (key int, value string) partitioned by (ds1 string, ds2 string);
insert overwrite table pcr_t1 partition (ds1='2000-04-08', ds2='2001-04-08') select * from src where key < 20 order by key;
insert overwrite table pcr_t1 partition (ds1='2000-04-09', ds2='2001-04-09') select * from src where key < 20 order by key;
insert overwrite table pcr_t1 partition (ds1='2000-04-10', ds2='2001-04-10') select * from src where key < 20 order by key;

set hive.optimize.point.lookup=false;
set hive.optimize.partition.columns.separate=false;

explain extended
select key, value, ds1, ds2
from pcr_t1
where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-09' and key=2)
order by key, value, ds1, ds2;

set hive.optimize.point.lookup=true;
set hive.optimize.point.lookup.min=0;
set hive.optimize.partition.columns.separate=true;

explain extended
select key, value, ds1, ds2
from pcr_t1
where (ds1='2000-04-08' and ds2='2001-04-08' and key=1) or (ds1='2000-04-09' and ds2='2001-04-09' and key=2)
order by key, value, ds1, ds2;

drop table pcr_t1;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

EXPLAIN
SELECT src.key as c3 from src where src.key > '2';

SELECT src.key as c3 from src where src.key > '2';

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT src.key as c3 from src where src.key > '2';

SELECT src.key as c3 from src where src.key > '2';
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;

explain
select b.key,b.cc
from (
  select a.*
  from (
    select key, count(value) as cc
    from srcpart a
    where a.ds = '2008-04-08' and a.hr = '11'
    group by key
  )a
  distribute by a.key
  sort by a.key,a.cc desc) b
where b.cc>1;

select b.key,b.cc
from (
  select a.*
  from (
    select key, count(value) as cc
    from srcpart a
    where a.ds = '2008-04-08' and a.hr = '11'
    group by key
  )a
  distribute by a.key
  sort by a.key,a.cc desc) b
where b.cc>1;

EXPLAIN
SELECT user_id
FROM (
  SELECT
  CAST(key AS INT) AS user_id
  ,CASE WHEN (value LIKE 'aaa%' OR value LIKE 'vvv%')
  THEN 1
  ELSE 0 END AS tag_student
  FROM srcpart
) sub
WHERE sub.tag_student > 0;

EXPLAIN
SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key)  where x.key = 20 CLUSTER BY v1;

set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

explain
select b.key,b.cc
from (
  select a.*
  from (
    select key, count(value) as cc
    from srcpart a
    where a.ds = '2008-04-08' and a.hr = '11'
    group by key
  )a
  distribute by a.key
  sort by a.key,a.cc desc) b
where b.cc>1;

select b.key,b.cc
from (
  select a.*
  from (
    select key, count(value) as cc
    from srcpart a
    where a.ds = '2008-04-08' and a.hr = '11'
    group by key
  )a
  distribute by a.key
  sort by a.key,a.cc desc) b
where b.cc>1;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

EXPLAIN
SELECT * FROM SRC x where x.key = 10 CLUSTER BY x.key;
SELECT * FROM SRC x where x.key = 10 CLUSTER BY x.key;

EXPLAIN
SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key)  where x.key = 20 CLUSTER BY v1;;
SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY v1;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT * FROM SRC x where x.key = 10 CLUSTER BY x.key;
SELECT * FROM SRC x where x.key = 10 CLUSTER BY x.key;

EXPLAIN
SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key)  where x.key = 20 CLUSTER BY v1;;
SELECT x.key, x.value as v1, y.key  FROM SRC x JOIN SRC y ON (x.key = y.key) where x.key = 20 CLUSTER BY v1;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

CREATE TABLE ppd_constant_expr(c1 STRING, c2 INT, c3 DOUBLE) STORED AS TEXTFILE;

EXPLAIN
FROM src1
INSERT OVERWRITE TABLE ppd_constant_expr SELECT 4 + NULL, src1.key - NULL, NULL + NULL;

FROM src1
INSERT OVERWRITE TABLE ppd_constant_expr SELECT 4 + NULL, src1.key - NULL, NULL + NULL;

SELECT ppd_constant_expr.* FROM ppd_constant_expr;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
FROM src1
INSERT OVERWRITE TABLE ppd_constant_expr SELECT 4 + NULL, src1.key - NULL, NULL + NULL;

FROM src1
INSERT OVERWRITE TABLE ppd_constant_expr SELECT 4 + NULL, src1.key - NULL, NULL + NULL;

SELECT ppd_constant_expr.* FROM ppd_constant_expr;
-- Test that the partition pruner does not fail when there is a constant expression in the filter

EXPLAIN SELECT COUNT(*) FROM srcpart WHERE ds = '2008-04-08' and 'a' = 'a';

SELECT COUNT(*) FROM srcpart WHERE ds = '2008-04-08' and 'a' = 'a';
set hive.support.sql11.reserved.keywords=false;
-- We need the above setting for backward compatibility because 'user' is a keyword in SQL2011
-- ppd leaves invalid expr in field expr
CREATE TABLE test_issue (fileid int, infos ARRAY<STRUCT<user:INT>>, test_c STRUCT<user_c:STRUCT<age:INT>>);
CREATE VIEW v_test_issue AS SELECT fileid, i.user, test_c.user_c.age FROM test_issue LATERAL VIEW explode(infos) info AS i;

-- dummy data
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE test_issue;

SELECT * FROM v_test_issue WHERE age = 25;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

EXPLAIN
SELECT src1.c1
FROM
(SELECT src.value as c1, count(src.key) as c2 from src where src.value > 'val_10' group by src.value) src1
WHERE src1.c1 > 'val_200' and (src1.c2 > 30 or src1.c1 < 'val_400');

SELECT src1.c1
FROM
(SELECT src.value as c1, count(src.key) as c2 from src where src.value > 'val_10' group by src.value) src1
WHERE src1.c1 > 'val_200' and (src1.c2 > 30 or src1.c1 < 'val_400');

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT src1.c1
FROM
(SELECT src.value as c1, count(src.key) as c2 from src where src.value > 'val_10' group by src.value) src1
WHERE src1.c1 > 'val_200' and (src1.c2 > 30 or src1.c1 < 'val_400');

SELECT src1.c1
FROM
(SELECT src.value as c1, count(src.key) as c2 from src where src.value > 'val_10' group by src.value) src1
WHERE src1.c1 > 'val_200' and (src1.c2 > 30 or src1.c1 < 'val_400');
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

EXPLAIN
SELECT max(src1.c1), src1.c2
FROM
(SELECT src.value AS c1, count(src.key) AS c2 FROM src WHERE src.value > 'val_10' GROUP BY src.value) src1
WHERE src1.c1 > 'val_200' AND (src1.c2 > 30 OR src1.c1 < 'val_400')
GROUP BY src1.c2;

SELECT max(src1.c1), src1.c2
FROM
(SELECT src.value AS c1, count(src.key) AS c2 FROM src WHERE src.value > 'val_10' GROUP BY src.value) src1
WHERE src1.c1 > 'val_200' AND (src1.c2 > 30 OR src1.c1 < 'val_400')
GROUP BY src1.c2;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT max(src1.c1), src1.c2
FROM
(SELECT src.value AS c1, count(src.key) AS c2 FROM src WHERE src.value > 'val_10' GROUP BY src.value) src1
WHERE src1.c1 > 'val_200' AND (src1.c2 > 30 OR src1.c1 < 'val_400')
GROUP BY src1.c2;

SELECT max(src1.c1), src1.c2
FROM
(SELECT src.value AS c1, count(src.key) AS c2 FROM src WHERE src.value > 'val_10' GROUP BY src.value) src1
WHERE src1.c1 > 'val_200' AND (src1.c2 > 30 OR src1.c1 < 'val_400')
GROUP BY src1.c2;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.c1, count(1)
FROM
(SELECT src.key AS c1, src.value AS c2 from src where src.key > '1' ) src1
JOIN
(SELECT src.key AS c3, src.value AS c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
WHERE src1.c1 > '20' AND (src1.c2 < 'val_50' OR src1.c1 > '2') AND (src2.c3 > '50' OR src1.c1 < '50') AND (src2.c3 <> '4')
GROUP BY src1.c1;
SELECT src1.c1, count(1)
FROM
(SELECT src.key AS c1, src.value AS c2 from src where src.key > '1' ) src1
JOIN
(SELECT src.key AS c3, src.value AS c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
WHERE src1.c1 > '20' AND (src1.c2 < 'val_50' OR src1.c1 > '2') AND (src2.c3 > '50' OR src1.c1 < '50') AND (src2.c3 <> '4')
GROUP BY src1.c1;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT src1.c1, count(1)
FROM
(SELECT src.key AS c1, src.value AS c2 from src where src.key > '1' ) src1
JOIN
(SELECT src.key AS c3, src.value AS c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
WHERE src1.c1 > '20' AND (src1.c2 < 'val_50' OR src1.c1 > '2') AND (src2.c3 > '50' OR src1.c1 < '50') AND (src2.c3 <> '4')
GROUP BY src1.c1;
SELECT src1.c1, count(1)
FROM
(SELECT src.key AS c1, src.value AS c2 from src where src.key > '1' ) src1
JOIN
(SELECT src.key AS c3, src.value AS c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
WHERE src1.c1 > '20' AND (src1.c2 < 'val_50' OR src1.c1 > '2') AND (src2.c3 > '50' OR src1.c1 < '50') AND (src2.c3 <> '4')
GROUP BY src1.c1;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key > '1' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
WHERE src1.c1 > '20' and (src1.c2 < 'val_50' or src1.c1 > '2') and (src2.c3 > '50' or src1.c1 < '50') and (src2.c3 <> '4');

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key > '1' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
WHERE src1.c1 > '20' and (src1.c2 < 'val_50' or src1.c1 > '2') and (src2.c3 > '50' or src1.c1 < '50') and (src2.c3 <> '4');

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key > '1' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
WHERE src1.c1 > '20' and (src1.c2 < 'val_50' or src1.c1 > '2') and (src2.c3 > '50' or src1.c1 < '50') and (src2.c3 <> '4');

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key > '1' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
WHERE src1.c1 > '20' and (src1.c2 < 'val_50' or src1.c1 > '2') and (src2.c3 > '50' or src1.c1 < '50') and (src2.c3 <> '4');
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key <> '302' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key <> '305' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
JOIN
(SELECT src.key as c5, src.value as c6 from src where src.key <> '306' ) src3
ON src1.c2 = src3.c6
WHERE src1.c1 <> '311' and (src1.c2 <> 'val_50' or src1.c1 > '1') and (src2.c3 <> '10' or src1.c1 <> '10') and (src2.c3 <> '14') and (sqrt(src3.c5) <> 13);

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key <> '302' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key <> '305' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
JOIN
(SELECT src.key as c5, src.value as c6 from src where src.key <> '306' ) src3
ON src1.c2 = src3.c6
WHERE src1.c1 <> '311' and (src1.c2 <> 'val_50' or src1.c1 > '1') and (src2.c3 <> '10' or src1.c1 <> '10') and (src2.c3 <> '14') and (sqrt(src3.c5) <> 13);

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key <> '302' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key <> '305' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
JOIN
(SELECT src.key as c5, src.value as c6 from src where src.key <> '306' ) src3
ON src1.c2 = src3.c6
WHERE src1.c1 <> '311' and (src1.c2 <> 'val_50' or src1.c1 > '1') and (src2.c3 <> '10' or src1.c1 <> '10') and (src2.c3 <> '14') and (sqrt(src3.c5) <> 13);

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key <> '302' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key <> '305' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
JOIN
(SELECT src.key as c5, src.value as c6 from src where src.key <> '306' ) src3
ON src1.c2 = src3.c6
WHERE src1.c1 <> '311' and (src1.c2 <> 'val_50' or src1.c1 > '1') and (src2.c3 <> '10' or src1.c1 <> '10') and (src2.c3 <> '14') and (sqrt(src3.c5) <> 13);
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key <> '11' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key <> '12' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
JOIN
(SELECT src.key as c5, src.value as c6 from src where src.key <> '13' ) src3
ON src1.c1 = src3.c5
WHERE src1.c1 > '0' and (src1.c2 <> 'val_500' or src1.c1 > '1') and (src2.c3 > '10' or src1.c1 <> '10') and (src2.c3 <> '4') and (src3.c5 <> '1');

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key <> '11' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key <> '12' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
JOIN
(SELECT src.key as c5, src.value as c6 from src where src.key <> '13' ) src3
ON src1.c1 = src3.c5
WHERE src1.c1 > '0' and (src1.c2 <> 'val_500' or src1.c1 > '1') and (src2.c3 > '10' or src1.c1 <> '10') and (src2.c3 <> '4') and (src3.c5 <> '1');

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key <> '11' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key <> '12' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
JOIN
(SELECT src.key as c5, src.value as c6 from src where src.key <> '13' ) src3
ON src1.c1 = src3.c5
WHERE src1.c1 > '0' and (src1.c2 <> 'val_500' or src1.c1 > '1') and (src2.c3 > '10' or src1.c1 <> '10') and (src2.c3 <> '4') and (src3.c5 <> '1');

SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src where src.key <> '11' ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key <> '12' ) src2
ON src1.c1 = src2.c3 AND src1.c1 < '400'
JOIN
(SELECT src.key as c5, src.value as c6 from src where src.key <> '13' ) src3
ON src1.c1 = src3.c5
WHERE src1.c1 > '0' and (src1.c2 <> 'val_500' or src1.c1 > '1') and (src2.c3 > '10' or src1.c1 <> '10') and (src2.c3 <> '4') and (src3.c5 <> '1');
create table dual(a string);

set hive.optimize.ppd=true;
drop table if exists test_tbl ;

create table test_tbl (id string,name string);

insert into table test_tbl
select 'a','b' from dual;

explain
select t2.*
from
(select id,name from (select id,name from test_tbl) t1 sort by id) t2
join test_tbl t3 on (t2.id=t3.id )
where t2.name='c' and t3.id='a';

select t2.*
from
(select id,name from (select id,name from test_tbl) t1 sort by id) t2
join test_tbl t3 on (t2.id=t3.id )
where t2.name='c' and t3.id='a';set hive.mapred.mode=nonstrict;
create table t1 (id1 string, id2 string);
create table t2 (id string, d int);

from src tablesample (1 rows)
  insert into table t1 select 'a','a'
  insert into table t2 select 'a',2;

explain
select a.*,b.d d1,c.d d2 from
  t1 a join t2 b on (a.id1 = b.id)
       join t2 c on (a.id2 = b.id) where b.d <= 1 and c.d <= 1;

explain
select * from (
select a.*,b.d d1,c.d d2 from
  t1 a join t2 b on (a.id1 = b.id)
       join t2 c on (a.id2 = b.id) where b.d <= 1 and c.d <= 1
) z where d1 > 1 or d2 > 1;

select * from (
select a.*,b.d d1,c.d d2 from
  t1 a join t2 b on (a.id1 = b.id)
       join t2 c on (a.id2 = b.id) where b.d <= 1 and c.d <= 1
) z where d1 > 1 or d2 > 1;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

explain extended select a.key, b.k2, b.k3
from src a
join (
select key,
min(key) as k,
min(key)+1 as k1,
min(key)+2 as k2,
min(key)+3 as k3
from src
group by key
) b
on a.key=b.key and b.k1 < 5;

select a.key, b.k2, b.k3
from src a
join (
select key,
min(key) as k,
min(key)+1 as k1,
min(key)+2 as k2,
min(key)+3 as k3
from src
group by key
) b
on a.key=b.key and b.k1 < 5;

set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;

explain extended select a.key, b.k2, b.k3
from src a
join (
select key,
min(key) as k,
min(key)+1 as k1,
min(key)+2 as k2,
min(key)+3 as k3
from src
group by key
) b
on a.key=b.key and b.k1 < 5;

select a.key, b.k2, b.k3
from src a
join (
select key,
min(key) as k,
min(key)+1 as k1,
min(key)+2 as k2,
min(key)+3 as k3
from src
group by key
) b
on a.key=b.key and b.k1 < 5;

set hive.optimize.ppd=false;
set hive.ppd.remove.duplicatefilters=false;

explain extended select a.key, b.k2, b.k3
from src a
join (
select key,
min(key) as k,
min(key)+1 as k1,
min(key)+2 as k2,
min(key)+3 as k3
from src
group by key
) b
on a.key=b.key and b.k1 < 5;

select a.key, b.k2, b.k3
from src a
join (
select key,
min(key) as k,
min(key)+1 as k1,
min(key)+2 as k2,
min(key)+3 as k3
from src
group by key
) b
on a.key=b.key and b.k1 < 5;

set hive.optimize.ppd=faluse;
set hive.ppd.remove.duplicatefilters=true;

explain extended select a.key, b.k2, b.k3
from src a
join (
select key,
min(key) as k,
min(key)+1 as k1,
min(key)+2 as k2,
min(key)+3 as k3
from src
group by key
) b
on a.key=b.key and b.k1 < 5;

select a.key, b.k2, b.k3
from src a
join (
select key,
min(key) as k,
min(key)+1 as k1,
min(key)+2 as k2,
min(key)+3 as k3
from src
group by key
) b
on a.key=b.key and b.k1 < 5;

CREATE TABLE hbase_ppd_keyrange(key int, value string)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key#binary,cf:string");

INSERT OVERWRITE TABLE hbase_ppd_keyrange
SELECT *
FROM src;

explain select * from hbase_ppd_keyrange where key > 8 and key < 21;
select * from hbase_ppd_keyrange where key > 8 and key < 21;

explain select * from hbase_ppd_keyrange where key > 8 and key <= 17;
select * from hbase_ppd_keyrange where key > 8 and key <= 17;


explain select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%';
select * from hbase_ppd_keyrange where key > 8 and key <= 17 and value like '%11%';

explain select * from hbase_ppd_keyrange where key >= 9 and key < 17 and key = 11;
select * from hbase_ppd_keyrange where key >=9  and key < 17 and key = 11;

drop table  hbase_ppd_keyrange;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

CREATE TABLE mi1(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE mi2(key INT, value STRING) STORED AS TEXTFILE;
CREATE TABLE mi3(key INT) PARTITIONED BY(ds STRING, hr STRING) STORED AS TEXTFILE;
CREATE TABLE mi4(value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src a JOIN src b ON (a.key = b.key)
INSERT OVERWRITE TABLE mi1 SELECT a.* WHERE a.key < 100
INSERT OVERWRITE TABLE mi2 SELECT a.key, a.value WHERE a.key >= 100 and a.key < 200
INSERT OVERWRITE TABLE mi3 PARTITION(ds='2008-04-08', hr='12') SELECT a.key WHERE a.key >= 200 and a.key < 300
INSERT OVERWRITE DIRECTORY 'target/warehouse/mi4.out' SELECT a.value WHERE a.key >= 300;

FROM src a JOIN src b ON (a.key = b.key)
INSERT OVERWRITE TABLE mi1 SELECT a.* WHERE a.key < 100
INSERT OVERWRITE TABLE mi2 SELECT a.key, a.value WHERE a.key >= 100 and a.key < 200
INSERT OVERWRITE TABLE mi3 PARTITION(ds='2008-04-08', hr='12') SELECT a.key WHERE a.key >= 200 and a.key < 300
INSERT OVERWRITE DIRECTORY 'target/warehouse/mi4.out' SELECT a.value WHERE a.key >= 300;

SELECT mi1.* FROM mi1;
SELECT mi2.* FROM mi2;
SELECT mi3.* FROM mi3;
LOAD DATA INPATH '${system:test.warehouse.dir}/mi4.out' OVERWRITE INTO TABLE mi4;
SELECT mi4.* FROM mi4;


set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
FROM src a JOIN src b ON (a.key = b.key)
INSERT OVERWRITE TABLE mi1 SELECT a.* WHERE a.key < 100
INSERT OVERWRITE TABLE mi2 SELECT a.key, a.value WHERE a.key >= 100 and a.key < 200
INSERT OVERWRITE TABLE mi3 PARTITION(ds='2008-04-08', hr='12') SELECT a.key WHERE a.key >= 200 and a.key < 300
INSERT OVERWRITE DIRECTORY 'target/warehouse/mi4.out' SELECT a.value WHERE a.key >= 300;

FROM src a JOIN src b ON (a.key = b.key)
INSERT OVERWRITE TABLE mi1 SELECT a.* WHERE a.key < 100
INSERT OVERWRITE TABLE mi2 SELECT a.key, a.value WHERE a.key >= 100 and a.key < 200
INSERT OVERWRITE TABLE mi3 PARTITION(ds='2008-04-08', hr='12') SELECT a.key WHERE a.key >= 200 and a.key < 300
INSERT OVERWRITE DIRECTORY 'target/warehouse/mi4.out' SELECT a.value WHERE a.key >= 300;

SELECT mi1.* FROM mi1;
SELECT mi2.* FROM mi2;
SELECT mi3.* FROM mi3;
LOAD DATA INPATH '${system:test.warehouse.dir}/mi4.out' OVERWRITE INTO TABLE mi4;
SELECT mi4.* FROM mi4;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

EXPLAIN
 FROM
  src a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  src a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
 FROM
  src a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  src a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

EXPLAIN
 FROM
  src a
 RIGHT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25';

 FROM
  src a
 RIGHT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25';

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
 FROM
  src a
 RIGHT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25';

 FROM
  src a
 RIGHT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25';
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

EXPLAIN
 FROM
  src a
 FULL OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25';

 FROM
  src a
 FULL OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25';

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
 FROM
  src a
 FULL OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25';

 FROM
  src a
 FULL OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25';
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

-- SORT_QUERY_RESULTS

EXPLAIN
 FROM
  src a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 RIGHT OUTER JOIN
  src c
 ON (a.key = c.key)
 SELECT a.key, a.value, b.key, b.value, c.key
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25' AND sqrt(c.key) <> 13 ;

 FROM
  src a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 RIGHT OUTER JOIN
  src c
 ON (a.key = c.key)
 SELECT a.key, a.value, b.key, b.value, c.key
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25' AND sqrt(c.key) <> 13 ;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
 FROM
  src a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 RIGHT OUTER JOIN
  src c
 ON (a.key = c.key)
 SELECT a.key, a.value, b.key, b.value, c.key
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25' AND sqrt(c.key) <> 13 ;

 FROM
  src a
 LEFT OUTER JOIN
  src b
 ON (a.key = b.key)
 RIGHT OUTER JOIN
  src c
 ON (a.key = c.key)
 SELECT a.key, a.value, b.key, b.value, c.key
 WHERE a.key > '10' AND a.key < '20' AND b.key > '15' AND b.key < '25' AND sqrt(c.key) <> 13 ;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;

create table t1 (id int, key string, value string);
create table t2 (id int, key string, value string);
create table t3 (id int, key string, value string);
create table t4 (id int, key string, value string);

explain select * from t1 full outer join t2 on t1.id=t2.id join t3 on t2.id=t3.id where t3.id=20;
explain select * from t1 join t2 on (t1.id=t2.id) left outer join t3 on (t2.id=t3.id) where t2.id=20;
explain select * from t1 join t2 on (t1.id=t2.id) left outer join t3 on (t1.id=t3.id) where t2.id=20;

drop table t1;
drop table t2;
drop table t3;
drop table t4;set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3
WHERE rand() > 0.5;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT src1.c1, src2.c4
FROM
(SELECT src.key as c1, src.value as c2 from src ) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src where src.key > '2' ) src2
ON src1.c1 = src2.c3
WHERE rand() > 0.5;
set hive.mapred.mode=nonstrict;
drop table pokes;
drop table pokes2;
create table pokes (foo int, bar int, blah int);
create table pokes2 (foo int, bar int, blah int);

-- Q1: predicate should not be pushed on the right side of a left outer join
explain
SELECT a.foo as foo1, b.foo as foo2, b.bar
FROM pokes a LEFT OUTER JOIN pokes2 b
ON a.foo=b.foo
WHERE b.bar=3;

-- Q2: predicate should not be pushed on the right side of a left outer join
explain
SELECT * FROM
    (SELECT a.foo as foo1, b.foo as foo2, b.bar
    FROM pokes a LEFT OUTER JOIN pokes2 b
    ON a.foo=b.foo) a
WHERE a.bar=3;

-- Q3: predicate should be pushed
explain
SELECT * FROM
    (SELECT a.foo as foo1, b.foo as foo2, a.bar
    FROM pokes a JOIN pokes2 b
    ON a.foo=b.foo) a
WHERE a.bar=3;

-- Q4: here, the filter c.bar should be created under the first join but above the second
explain select c.foo, d.bar from (select c.foo, b.bar, c.blah from pokes c left outer join pokes b on c.foo=b.foo) c left outer join pokes d where d.foo=1 and c.bar=2;

drop table pokes;
drop table pokes2;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;
set hive.entity.capture.transform=true;

-- SORT_QUERY_RESULTS

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'cat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

-- test described in HIVE-4598

EXPLAIN
FROM (
    FROM ( SELECT * FROM src ) mapout REDUCE * USING 'cat' AS x,y
) reduced
insert overwrite local directory '/tmp/a' select * where x='a' or x='b'
insert overwrite local directory '/tmp/b' select * where x='c' or x='d';
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

EXPLAIN
SELECT *
FROM srcpart a JOIN srcpart b
ON a.key = b.key
WHERE a.ds = '2008-04-08' AND
      b.ds = '2008-04-08' AND
      CASE a.key
        WHEN '27' THEN TRUE
        WHEN '38' THEN FALSE
        ELSE NULL
       END
ORDER BY a.key, a.value, a.ds, a.hr, b.key, b.value, b.ds, b.hr;

SELECT *
FROM srcpart a JOIN srcpart b
ON a.key = b.key
WHERE a.ds = '2008-04-08' AND
      b.ds = '2008-04-08' AND
      CASE a.key
        WHEN '27' THEN TRUE
        WHEN '38' THEN FALSE
        ELSE NULL
       END
ORDER BY a.key, a.value, a.ds, a.hr, b.key, b.value, b.ds, b.hr;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT *
FROM srcpart a JOIN srcpart b
ON a.key = b.key
WHERE a.ds = '2008-04-08' AND
      b.ds = '2008-04-08' AND
      CASE a.key
        WHEN '27' THEN TRUE
        WHEN '38' THEN FALSE
        ELSE NULL
       END
ORDER BY a.key, a.value, a.ds, a.hr, b.key, b.value, b.ds, b.hr;

SELECT *
FROM srcpart a JOIN srcpart b
ON a.key = b.key
WHERE a.ds = '2008-04-08' AND
      b.ds = '2008-04-08' AND
      CASE a.key
        WHEN '27' THEN TRUE
        WHEN '38' THEN FALSE
        ELSE NULL
       END
ORDER BY a.key, a.value, a.ds, a.hr, b.key, b.value, b.ds, b.hr;

set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

EXPLAIN
SELECT key, randum123
FROM (SELECT *, cast(rand() as double) AS randum123 FROM src WHERE key = 100) a
WHERE randum123 <=0.1;

EXPLAIN
SELECT * FROM
(
SELECT key, randum123
FROM (SELECT *, cast(rand() as double) AS randum123 FROM src WHERE key = 100) a
WHERE randum123 <=0.1)s WHERE s.randum123>0.1 LIMIT 20;

EXPLAIN
SELECT key,randum123, h4
FROM (SELECT *, cast(rand() as double) AS randum123, hex(4) AS h4 FROM src WHERE key = 100) a
WHERE a.h4 <= 3;

EXPLAIN
SELECT key,randum123, v10
FROM (SELECT *, cast(rand() as double) AS randum123, value*10 AS v10 FROM src WHERE key = 100) a
WHERE a.v10 <= 200;

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
SELECT key, randum123
FROM (SELECT *, cast(rand() as double) AS randum123 FROM src WHERE key = 100) a
WHERE randum123 <=0.1;

EXPLAIN
SELECT * FROM
(
SELECT key, randum123
FROM (SELECT *, cast(rand() as double) AS randum123 FROM src WHERE key = 100) a
WHERE randum123 <=0.1)s WHERE s.randum123>0.1 LIMIT 20;

EXPLAIN
SELECT key,randum123, h4
FROM (SELECT *, cast(rand() as double) AS randum123, hex(4) AS h4 FROM src WHERE key = 100) a
WHERE a.h4 <= 3;

EXPLAIN
SELECT key,randum123, v10
FROM (SELECT *, cast(rand() as double) AS randum123, value*10 AS v10 FROM src WHERE key = 100) a
WHERE a.v10 <= 200;
explain
SELECT value from (
  select explode(array(key, value)) as (value) from (
    select * FROM src WHERE key > 400
  ) A
) B WHERE value < 450;

SELECT value from (
  select explode(array(key, value)) as (value) from (
    select * FROM src WHERE key > 400
  ) A
) B WHERE value < 450;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=false;

EXPLAIN
FROM (
  FROM src select src.key, src.value WHERE src.key < '100'
    UNION ALL
  FROM src SELECT src.* WHERE src.key > '150'
) unioned_query
SELECT unioned_query.*
  WHERE key > '4' and value > 'val_4';

FROM (
  FROM src select src.key, src.value WHERE src.key < '100'
    UNION ALL
  FROM src SELECT src.* WHERE src.key > '150'
) unioned_query
SELECT unioned_query.*
  WHERE key > '4' and value > 'val_4';

set hive.ppd.remove.duplicatefilters=true;

EXPLAIN
FROM (
  FROM src select src.key, src.value WHERE src.key < '100'
    UNION ALL
  FROM src SELECT src.* WHERE src.key > '150'
) unioned_query
SELECT unioned_query.*
  WHERE key > '4' and value > 'val_4';

FROM (
  FROM src select src.key, src.value WHERE src.key < '100'
    UNION ALL
  FROM src SELECT src.* WHERE src.key > '150'
) unioned_query
SELECT unioned_query.*
  WHERE key > '4' and value > 'val_4';
set hive.mapred.mode=nonstrict;
-- test predicate pushdown on a view with a union

drop view v;

create table t1_new (key string, value string) partitioned by (ds string);

insert overwrite table t1_new partition (ds = '2011-10-15')
select 'key1', 'value1' from src tablesample (1 rows);

insert overwrite table t1_new partition (ds = '2011-10-16')
select 'key2', 'value2' from src tablesample (1 rows);

create table t1_old (keymap string, value string) partitioned by (ds string);

insert overwrite table t1_old partition (ds = '2011-10-13')
select 'keymap3', 'value3' from src tablesample (1 rows);

insert overwrite table t1_old partition (ds = '2011-10-14')
select 'keymap4', 'value4' from src tablesample (1 rows);

create table t1_mapping (key string, keymap string) partitioned by (ds string);

insert overwrite table t1_mapping partition (ds = '2011-10-13')
select 'key3', 'keymap3' from src tablesample (1 rows);

insert overwrite table t1_mapping partition (ds = '2011-10-14')
select 'key4', 'keymap4' from src tablesample (1 rows);


create view t1 partitioned on (ds) as
select * from
(
select key, value, ds from t1_new
union all
select key, value, t1_old.ds from t1_old join t1_mapping
on t1_old.keymap = t1_mapping.keymap and
   t1_old.ds = t1_mapping.ds
) subq;

explain extended
select * from t1 where ds = '2011-10-13';

select * from t1 where ds = '2011-10-13';

select * from t1 where ds = '2011-10-14';

explain extended
select * from t1 where ds = '2011-10-15';

select * from t1 where ds = '2011-10-15';
select * from t1 where ds = '2011-10-16';set hive.mapred.mode=nonstrict;
--HIVE-3926 PPD on virtual column of partitioned table is not working

explain extended
select * from srcpart where BLOCK__OFFSET__INSIDE__FILE<100;
select * from srcpart where BLOCK__OFFSET__INSIDE__FILE<100;

explain extended
select b.* from src a join
  (select *,BLOCK__OFFSET__INSIDE__FILE from srcpart where BLOCK__OFFSET__INSIDE__FILE<100) b
    on a.key=b.key AND b.BLOCK__OFFSET__INSIDE__FILE<50 order by ds,hr,BLOCK__OFFSET__INSIDE__FILE;

select b.* from src a join
  (select *,BLOCK__OFFSET__INSIDE__FILE from srcpart where BLOCK__OFFSET__INSIDE__FILE<100) b
    on a.key=b.key AND b.BLOCK__OFFSET__INSIDE__FILE<50 order by ds,hr,BLOCK__OFFSET__INSIDE__FILE;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;

EXPLAIN EXTENDED
SELECT
  CAST(key AS INT) AS user_id, value
  FROM srcpart
  WHERE ds='2008-04-08' and
  ( CASE WHEN (value LIKE 'aaa%' OR value LIKE 'vvv%')
   THEN 1
   ELSE 0  end ) > 0
;

SELECT
  CAST(key AS INT) AS user_id, value
  FROM srcpart
  WHERE ds='2008-04-08' and
  ( CASE WHEN (value LIKE 'aaa%' OR value LIKE 'vvv%')
   THEN 1
   ELSE 0  end ) > 0
;

set hive.optimize.ppd=false;

EXPLAIN EXTENDED
SELECT
  CAST(key AS INT) AS user_id, value
  FROM srcpart
  WHERE ds='2008-04-08' and
  ( CASE WHEN (value LIKE 'aaa%' OR value LIKE 'vvv%')
   THEN 1
   ELSE 0  end ) > 0
;

SELECT
  CAST(key AS INT) AS user_id, value
  FROM srcpart
  WHERE ds='2008-04-08' and
  ( CASE WHEN (value LIKE 'aaa%' OR value LIKE 'vvv%')
   THEN 1
   ELSE 0  end ) > 0
;
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

create table ppr_test (key string) partitioned by (ds string);

alter table ppr_test add partition (ds = '1234');
alter table ppr_test add partition (ds = '1224');
alter table ppr_test add partition (ds = '1214');
alter table ppr_test add partition (ds = '12+4');
alter table ppr_test add partition (ds = '12.4');
alter table ppr_test add partition (ds = '12:4');
alter table ppr_test add partition (ds = '12%4');
alter table ppr_test add partition (ds = '12*4');

insert overwrite table ppr_test partition(ds = '1234') select * from (select '1234' from src tablesample (1 rows) union all select 'abcd' from src tablesample (1 rows)) s;
insert overwrite table ppr_test partition(ds = '1224') select * from (select '1224' from src tablesample (1 rows) union all select 'abcd' from src tablesample (1 rows)) s;
insert overwrite table ppr_test partition(ds = '1214') select * from (select '1214' from src tablesample (1 rows) union all select 'abcd' from src tablesample (1 rows)) s;
insert overwrite table ppr_test partition(ds = '12+4') select * from (select '12+4' from src tablesample (1 rows) union all select 'abcd' from src tablesample (1 rows)) s;
insert overwrite table ppr_test partition(ds = '12.4') select * from (select '12.4' from src tablesample (1 rows) union all select 'abcd' from src tablesample (1 rows)) s;
insert overwrite table ppr_test partition(ds = '12:4') select * from (select '12:4' from src tablesample (1 rows) union all select 'abcd' from src tablesample (1 rows)) s;
insert overwrite table ppr_test partition(ds = '12%4') select * from (select '12%4' from src tablesample (1 rows) union all select 'abcd' from src tablesample (1 rows)) s;
insert overwrite table ppr_test partition(ds = '12*4') select * from (select '12*4' from src tablesample (1 rows) union all select 'abcd' from src tablesample (1 rows)) s;


select * from ppr_test where ds = '1234' order by key;
select * from ppr_test where ds = '1224' order by key;
select * from ppr_test where ds = '1214' order by key;
select * from ppr_test where ds = '12.4' order by key;
select * from ppr_test where ds = '12+4' order by key;
select * from ppr_test where ds = '12:4' order by key;
select * from ppr_test where ds = '12%4' order by key;
select * from ppr_test where ds = '12*4' order by key;
select * from ppr_test where ds = '12.*4' order by key;

select * from ppr_test where ds = '1234' and key = '1234';
select * from ppr_test where ds = '1224' and key = '1224';
select * from ppr_test where ds = '1214' and key = '1214';
select * from ppr_test where ds = '12.4' and key = '12.4';
select * from ppr_test where ds = '12+4' and key = '12+4';
select * from ppr_test where ds = '12:4' and key = '12:4';
select * from ppr_test where ds = '12%4' and key = '12%4';
select * from ppr_test where ds = '12*4' and key = '12*4';


set hive.fetch.task.conversion=more;

create table ppr_test (key string) partitioned by (ds string);

insert overwrite table ppr_test partition(ds='2') select '2' from src tablesample (1 rows);
insert overwrite table ppr_test partition(ds='22') select '22' from src tablesample (1 rows);

select * from ppr_test where ds = '2';
select * from ppr_test where ds = '22';


create table ppr_test2 (key string) partitioned by (ds string, s string);
insert overwrite table ppr_test2 partition(ds='1', s='2') select '1' from src tablesample (1 rows);
insert overwrite table ppr_test2 partition(ds='2', s='1') select '2' from src tablesample (1 rows);

select * from ppr_test2 where s = '1';
select * from ppr_test2 where ds = '1';


create table ppr_test3 (key string) partitioned by (col string, ol string, l string);
insert overwrite table ppr_test3 partition(col='1', ol='2', l = '3') select '1' from src tablesample (1 rows);
insert overwrite table ppr_test3 partition(col='1', ol='1', l = '2') select '2' from src tablesample (1 rows);
insert overwrite table ppr_test3 partition(col='1', ol='2', l = '1') select '3' from src tablesample (1 rows);

select * from ppr_test3 where l = '1';
select * from ppr_test3 where l = '2';
select * from ppr_test3 where ol = '1';
select * from ppr_test3 where ol = '2';
select * from ppr_test3 where col = '1';
select * from ppr_test3 where ol = '2' and l = '1';
select * from ppr_test3 where col='1' and ol = '2' and l = '1';
set hive.mapred.mode=nonstrict;

explain select * from srcpart where key < 10;
select * from srcpart where key < 10;

explain select * from srcpart;
select * from srcpart;

explain select key from srcpart;
select key from srcpart;
set hive.mapred.mode=nonstrict;

set hive.cli.print.header=true;

SELECT src1.key as k1, src1.value as v1,
       src2.key as k2, src2.value as v2 FROM
  (SELECT * FROM src WHERE src.key < 10) src1
    JOIN
  (SELECT * FROM src WHERE src.key < 10) src2
  SORT BY k1, v1, k2, v2
  LIMIT 10;

SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key LIMIT 10;

use default;
set hive.heartbeat.interval=5;


CREATE TABLE PROGRESS_1(key int, value string) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv6.txt' INTO TABLE PROGRESS_1;

select count(1) from PROGRESS_1 t1 join PROGRESS_1 t2 on t1.key=t2.key;


set hive.explain.user=false;
-- SORT_QUERY_RESULTS

--1. test1
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
  partition by p_mfgr
  order by p_name
  );

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
  partition by p_mfgr
  order by p_name
  );

-- 2. testJoinWithNoop
explain
select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part p1 join part p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
;

select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part p1 join part p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
;

-- 3. testOnlyPTF
explain
select p_mfgr, p_name, p_size
from noop(on part
partition by p_mfgr
order by p_name);

select p_mfgr, p_name, p_size
from noop(on part
partition by p_mfgr
order by p_name);

-- 4. testPTFAlias
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
  partition by p_mfgr
  order by p_name
  ) abc;

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
  partition by p_mfgr
  order by p_name
  ) abc;

-- 5. testPTFAndWhereWithWindowing
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part
          partition by p_mfgr
          order by p_name
          )
;

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part
          partition by p_mfgr
          order by p_name
          )
;

-- 6. testSWQAndPTFAndGBy
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part
          partition by p_mfgr
          order by p_name
          )
group by p_mfgr, p_name, p_size
;

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part
          partition by p_mfgr
          order by p_name
          )
group by p_mfgr, p_name, p_size
;

-- 7. testJoin
explain
select abc.*
from noop(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey;

select abc.*
from noop(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey;

-- 8. testJoinRight
explain
select abc.*
from part p1 join noop(on part
partition by p_mfgr
order by p_name
) abc on abc.p_partkey = p1.p_partkey;

select abc.*
from part p1 join noop(on part
partition by p_mfgr
order by p_name
) abc on abc.p_partkey = p1.p_partkey;

-- 9. testNoopWithMap
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part
partition by p_mfgr
order by p_name, p_size desc);

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part
partition by p_mfgr
order by p_name, p_size desc);

-- 10. testNoopWithMapWithWindowing
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopwithmap(on part
  partition by p_mfgr
  order by p_name);

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopwithmap(on part
  partition by p_mfgr
  order by p_name);

-- 11. testHavingWithWindowingPTFNoGBY
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
partition by p_mfgr
order by p_name)
;

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part
partition by p_mfgr
order by p_name)
;

-- 12. testFunctionChain
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on noopwithmap(on noop(on part
partition by p_mfgr
order by p_mfgr DESC, p_name
)));

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on noopwithmap(on noop(on part
partition by p_mfgr
order by p_mfgr DESC, p_name
)));

-- 13. testPTFAndWindowingInSubQ
explain
select p_mfgr, p_name,
sub1.cd, sub1.s1
from (select p_mfgr, p_name,
count(p_size) over (partition by p_mfgr order by p_name) as cd,
p_retailprice,
sum(p_retailprice) over w1  as s1
from noop(on part
partition by p_mfgr
order by p_name)
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following)
) sub1 ;

select p_mfgr, p_name,
sub1.cd, sub1.s1
from (select p_mfgr, p_name,
count(p_size) over (partition by p_mfgr order by p_name) as cd,
p_retailprice,
sum(p_retailprice) over w1  as s1
from noop(on part
partition by p_mfgr
order by p_name)
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following)
) sub1 ;

-- 14. testPTFJoinWithWindowingWithCount
explain
select abc.p_mfgr, abc.p_name,
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr,
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noop(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey
;

select abc.p_mfgr, abc.p_name,
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr,
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noop(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey
;

-- 15. testDistinctInSelectWithPTF
explain
select DISTINCT p_mfgr, p_name, p_size
from noop(on part
partition by p_mfgr
order by p_name);

select DISTINCT p_mfgr, p_name, p_size
from noop(on part
partition by p_mfgr
order by p_name);


-- 16. testViewAsTableInputToPTF
create view IF NOT EXISTS mfgr_price_view as
select p_mfgr, p_brand,
sum(p_retailprice) as s
from part
group by p_mfgr, p_brand;

explain
select p_mfgr, p_brand, s,
sum(s) over w1  as s1
from noop(on mfgr_price_view
partition by p_mfgr
order by p_mfgr)
window w1 as ( partition by p_mfgr order by p_brand rows between 2 preceding and current row);

select p_mfgr, p_brand, s,
sum(s) over w1  as s1
from noop(on mfgr_price_view
partition by p_mfgr
order by p_mfgr)
window w1 as ( partition by p_mfgr order by p_brand rows between 2 preceding and current row);

-- 17. testMultipleInserts2SWQsWithPTF
CREATE TABLE part_4(
p_mfgr STRING,
p_name STRING,
p_size INT,
r INT,
dr INT,
s DOUBLE);

CREATE TABLE part_5(
p_mfgr STRING,
p_name STRING,
p_size INT,
s2 INT,
r INT,
dr INT,
cud DOUBLE,
fv1 INT);

explain
from noop(on part
partition by p_mfgr
order by p_name)
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size,
rank() over (distribute by p_mfgr sort by p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr,
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud,
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

from noop(on part
partition by p_mfgr
order by p_name)
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size,
rank() over (distribute by p_mfgr sort by p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr,
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud,
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

select * from part_4;

select * from part_5;

-- 18. testMulti2OperatorsFunctionChainWithMap
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noopwithmap(on
          noop(on
              noop(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr,p_name
        order by p_mfgr,p_name) ;

select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noopwithmap(on
          noop(on
              noop(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr,p_name
        order by p_mfgr,p_name) ;

-- 19. testMulti3OperatorsFunctionChain
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noop(on
          noop(on
              noop(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr
        order by p_mfgr ) ;

select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noop(on
          noop(on
              noop(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr
        order by p_mfgr ) ;

-- 20. testMultiOperatorChainWithNoWindowing
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1
from noop(on
        noop(on
          noop(on
              noop(on part
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr));

select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1
from noop(on
        noop(on
          noop(on
              noop(on part
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr));


-- 21. testMultiOperatorChainEndsWithNoopMap
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noopwithmap(on
        noop(on
          noop(on
              noop(on part
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr)
          partition by p_mfgr,p_name
          order by p_mfgr,p_name);

select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noopwithmap(on
        noop(on
          noop(on
              noop(on part
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr)
          partition by p_mfgr,p_name
          order by p_mfgr,p_name);

-- 22. testMultiOperatorChainWithDiffPartitionForWindow1
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row)  as s2
from noop(on
        noopwithmap(on
              noop(on part
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          partition by p_mfgr
          order by p_mfgr
          ));

select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row)  as s2
from noop(on
        noopwithmap(on
              noop(on part
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          partition by p_mfgr
          order by p_mfgr
          ));

-- 23. testMultiOperatorChainWithDiffPartitionForWindow2
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmap(on
        noop(on
              noop(on part
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          ));

select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmap(on
        noop(on
              noop(on part
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          ));

set hive.mapred.mode=nonstrict;
create table tlb1 (id int, fkey int, val string);
create table tlb2 (fid int, name string);
insert into table tlb1 values(100,1,'abc');
insert into table tlb1 values(200,1,'efg');
insert into table tlb2 values(1, 'key1');

explain
select ddd.id, ddd.fkey, aaa.name
from (
    select id, fkey,
    row_number() over (partition by id, fkey) as rnum
    from tlb1 group by id, fkey
 ) ddd
inner join tlb2 aaa on aaa.fid = ddd.fkey;

select ddd.id, ddd.fkey, aaa.name
from (
    select id, fkey,
    row_number() over (partition by id, fkey) as rnum
    from tlb1 group by id, fkey
 ) ddd
inner join tlb2 aaa on aaa.fid = ddd.fkey;

explain
select ddd.id, ddd.fkey, aaa.name, ddd.rnum
from (
    select id, fkey,
    row_number() over (partition by id, fkey) as rnum
    from tlb1 group by id, fkey
 ) ddd
inner join tlb2 aaa on aaa.fid = ddd.fkey;

select ddd.id, ddd.fkey, aaa.name, ddd.rnum
from (
    select id, fkey,
    row_number() over (partition by id, fkey) as rnum
    from tlb1 group by id, fkey
 ) ddd
inner join tlb2 aaa on aaa.fid = ddd.fkey;


set hive.optimize.ppd=false;

explain
select ddd.id, ddd.fkey, aaa.name
from (
    select id, fkey,
    row_number() over (partition by id, fkey) as rnum
    from tlb1 group by id, fkey
 ) ddd
inner join tlb2 aaa on aaa.fid = ddd.fkey;

select ddd.id, ddd.fkey, aaa.name
from (
    select id, fkey,
    row_number() over (partition by id, fkey) as rnum
    from tlb1 group by id, fkey
 ) ddd
inner join tlb2 aaa on aaa.fid = ddd.fkey;


-- SORT_QUERY_RESULTS

-- 1. aggregate functions with decimal type

select p_mfgr, p_retailprice,
lead(p_retailprice) over (partition by p_mfgr ORDER BY p_name) as c1,
lag(p_retailprice) over (partition by p_mfgr ORDER BY p_name) as c2,
first_value(p_retailprice) over (partition by p_mfgr ORDER BY p_name) as c3,
last_value(p_retailprice) over (partition by p_mfgr ORDER BY p_name) as c4
from part;

-- 2. ranking functions with decimal type

select p_mfgr, p_retailprice,
row_number() over (PARTITION BY p_mfgr ORDER BY p_retailprice) as c1,
rank() over (PARTITION BY p_mfgr ORDER BY p_retailprice) as c2,
dense_rank() over (PARTITION BY p_mfgr ORDER BY p_retailprice) as c3,
percent_rank() over (PARTITION BY p_mfgr ORDER BY p_retailprice) as c4,
cume_dist() over (PARTITION BY p_mfgr ORDER BY p_retailprice) as c5,
ntile(5) over (PARTITION BY p_mfgr ORDER BY p_retailprice) as c6
from part;

-- 3. order by decimal

select p_mfgr, p_retailprice,
lag(p_retailprice) over (partition by p_mfgr ORDER BY p_retailprice desc) as c1
from part;

-- 4. partition by decimal

select p_mfgr, p_retailprice,
lag(p_retailprice) over (partition by p_retailprice) as c1
from part;

-- SORT_QUERY_RESULTS

-- 1. testNoPTFNoWindowing
select p_mfgr, p_name, p_size
from part
distribute by p_mfgr
sort by p_name ;

-- 2. testUDAFsNoWindowingNoPTFNoGBY
select p_mfgr,p_name, p_retailprice,
sum(p_retailprice) over(partition by p_mfgr order by p_name) as s,
min(p_retailprice) over(partition by p_mfgr order by p_name) as mi,
max(p_retailprice) over(partition by p_mfgr order by p_name) as ma,
avg(p_retailprice) over(partition by p_mfgr order by p_name) as av
from part
;

-- 3. testConstExprInSelect
select 'tst1' as key, count(1) as value from part;
set hive.explain.user=false;
DROP TABLE flights_tiny;

create table flights_tiny (
ORIGIN_CITY_NAME string,
DEST_CITY_NAME string,
YEAR int,
MONTH int,
DAY_OF_MONTH int,
ARR_DELAY float,
FL_NUM string
);

LOAD DATA LOCAL INPATH '../../data/files/flights_tiny.txt' OVERWRITE INTO TABLE flights_tiny;

-- SORT_QUERY_RESULTS

-- 1. basic Matchpath test
explain
select origin_city_name, fl_num, year, month, day_of_month, sz, tpath
from matchpath(on
        flights_tiny
        distribute by fl_num
        sort by year, month, day_of_month
      arg1('LATE.LATE+'),
      arg2('LATE'), arg3(arr_delay > 15),
    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')
   );

select origin_city_name, fl_num, year, month, day_of_month, sz, tpath
from matchpath(on
        flights_tiny
        distribute by fl_num
        sort by year, month, day_of_month
      arg1('LATE.LATE+'),
      arg2('LATE'), arg3(arr_delay > 15),
    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')
   );

-- 2. Matchpath on 1 partition
explain
select origin_city_name, fl_num, year, month, day_of_month, sz, tpath
from matchpath(on
        flights_tiny
        sort by fl_num, year, month, day_of_month
      arg1('LATE.LATE+'),
      arg2('LATE'), arg3(arr_delay > 15),
    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')
   )
where fl_num = 1142;

select origin_city_name, fl_num, year, month, day_of_month, sz, tpath
from matchpath(on
        flights_tiny
        sort by fl_num, year, month, day_of_month
      arg1('LATE.LATE+'),
      arg2('LATE'), arg3(arr_delay > 15),
    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')
   )
where fl_num = 1142;

-- 3. empty partition.
explain
select origin_city_name, fl_num, year, month, day_of_month, sz, tpath
from matchpath(on
        (select * from flights_tiny where fl_num = -1142) flights_tiny
        sort by fl_num, year, month, day_of_month
      arg1('LATE.LATE+'),
      arg2('LATE'), arg3(arr_delay > 15),
    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')
   );


select origin_city_name, fl_num, year, month, day_of_month, sz, tpath
from matchpath(on
        (select * from flights_tiny where fl_num = -1142) flights_tiny
        sort by fl_num, year, month, day_of_month
      arg1('LATE.LATE+'),
      arg2('LATE'), arg3(arr_delay > 15),
    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')
   );

-- testAggrFuncsWithNoGBYNoPartDef
select p_mfgr,
sum(p_retailprice) as s1
from part;-- testAmbiguousWindowDefn
select p_mfgr, p_name, p_size,
sum(p_size) over (w1) as s1,
sum(p_size) over (w2) as s2,
sum(p_size) over (w3) as s3
from part
distribute by p_mfgr
sort by p_mfgr
window w1 as (rows between 2 preceding and 2 following),
       w2 as (rows between unbounded preceding and current row),
       w3 as w3;

-- testPartitonBySortBy
select p_mfgr, p_name, p_size,
sum(p_retailprice) over (distribute by p_mfgr order by p_mfgr) as s1
from part
;
-- testDuplicateWindowAlias
select p_mfgr, p_name, p_size,
sum(p_size) over (w1) as s1,
sum(p_size) over (w2) as s2
from part
window w1 as (partition by p_mfgr order by p_mfgr rows between 2 preceding and 2 following),
       w2 as w1,
       w2 as (rows between unbounded preceding and current row);
-- testHavingLeadWithNoGBYNoWindowing
select  p_mfgr,p_name, p_size
from part
having lead(p_size, 1) over() <= p_size
distribute by p_mfgr
sort by p_name;
-- testHavingLeadWithPTF
select  p_mfgr,p_name, p_size
from noop(on part
partition by p_mfgr
order by p_name)
having lead(p_size, 1) over() <= p_size
distribute by p_mfgr
sort by p_name;
set hive.mapred.mode=nonstrict;
-- testInvalidValueBoundary

select  p_mfgr,p_name, p_size,
sum(p_size) over (w1) as s ,
dense_rank() over(w1) as dr
from part
window w1 as (partition by p_mfgr order by p_complex range between  2 preceding and current row);
-- testJoinWithAmbigousAlias
select abc.*
from noop(on part
partition by p_mfgr
order by p_name
) abc join part on abc.p_partkey = p1.p_partkey;
-- testNoWindowDefn
select p_mfgr, p_name, p_size,
sum(p_size) over (w1) as s1,
sum(p_size) over (w2) as s2
from part
distribute by p_mfgr
sort by p_mfgr
window w1 as (rows between 2 preceding and 2 following);

-- testPartitonBySortBy
select p_mfgr, p_name, p_size,
sum(p_retailprice) over (partition by p_mfgr sort by p_mfgr) as s1
from part
;
-- testWhereWithRankCond
select  p_mfgr,p_name, p_size,
rank() over() as r
from part
where r < 4
distribute by p_mfgr
sort by p_mfgr;
DROP TABLE part_rc;

CREATE TABLE part_rc(
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
)  STORED AS RCFILE ;

LOAD DATA LOCAL INPATH '../../data/files/part.rc' overwrite into table part_rc;

-- SORT_QUERY_RESULTS

-- testWindowingPTFWithPartRC
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice)  over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_rc
partition by p_mfgr
order by p_name);
DROP TABLE flights_tiny;

create table flights_tiny (
ORIGIN_CITY_NAME string,
DEST_CITY_NAME string,
YEAR int,
MONTH int,
DAY_OF_MONTH int,
ARR_DELAY float,
FL_NUM string
);

LOAD DATA LOCAL INPATH '../../data/files/flights_tiny.txt' OVERWRITE INTO TABLE flights_tiny;

create temporary function matchpathtest as 'org.apache.hadoop.hive.ql.udf.ptf.MatchPath$MatchPathResolver';

-- SORT_QUERY_RESULTS

-- 1. basic Matchpath test
select origin_city_name, fl_num, year, month, day_of_month, sz, tpath
from matchpathtest(on
        flights_tiny
        distribute by fl_num
        sort by year, month, day_of_month
      arg1('LATE.LATE+'),
      arg2('LATE'), arg3(arr_delay > 15),
    arg4('origin_city_name, fl_num, year, month, day_of_month, size(tpath) as sz, tpath[0].day_of_month as tpath')
   );

drop temporary function matchpathtest;
DROP TABLE part_seq;

CREATE TABLE part_seq(
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
) STORED AS SEQUENCEFILE ;

LOAD DATA LOCAL INPATH '../../data/files/part.seq' overwrite into table part_seq;

-- SORT_QUERY_RESULTS

-- testWindowingPTFWithPartSeqFile
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on part_seq
partition by p_mfgr
order by p_name);
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

--1. test1
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopstreaming(on part
  partition by p_mfgr
  order by p_name
  );

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopstreaming(on part
  partition by p_mfgr
  order by p_name
  );

-- 2. testJoinWithNoop
explain
select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noopstreaming (on (select p1.* from part p1 join part p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
;

select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noopstreaming (on (select p1.* from part p1 join part p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
;

-- 7. testJoin
explain
select abc.*
from noopstreaming(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey;

select abc.*
from noopstreaming(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey;

-- 9. testNoopWithMap
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmapstreaming(on part
partition by p_mfgr
order by p_name, p_size desc);

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmapstreaming(on part
partition by p_mfgr
order by p_name, p_size desc);

-- 10. testNoopWithMapWithWindowing
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopwithmapstreaming(on part
  partition by p_mfgr
  order by p_name);

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopwithmapstreaming(on part
  partition by p_mfgr
  order by p_name);

-- 12. testFunctionChain
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noopstreaming(on noopwithmapstreaming(on noopstreaming(on part
partition by p_mfgr
order by p_mfgr, p_name
)));

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noopstreaming(on noopwithmapstreaming(on noopstreaming(on part
partition by p_mfgr
order by p_mfgr, p_name
)));

-- 12.1 testFunctionChain
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noopstreaming(on noopwithmap(on noopstreaming(on part
partition by p_mfgr
order by p_mfgr, p_name
)));

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noopstreaming(on noopwithmap(on noopstreaming(on part
partition by p_mfgr
order by p_mfgr, p_name
)));

-- 12.2 testFunctionChain
explain
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on noopwithmapstreaming(on noopstreaming(on part
partition by p_mfgr
order by p_mfgr, p_name
)));

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on noopwithmapstreaming(on noopstreaming(on part
partition by p_mfgr
order by p_mfgr, p_name
)));

-- 14. testPTFJoinWithWindowingWithCount
explain
select abc.p_mfgr, abc.p_name,
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr,
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noopstreaming(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey
;

select abc.p_mfgr, abc.p_name,
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr,
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noopstreaming(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey
;

-- 18. testMulti2OperatorsFunctionChainWithMap
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noopstreaming(on
        noopwithmap(on
          noop(on
              noopstreaming(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr,p_name
        order by p_mfgr,p_name) ;

select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noopstreaming(on
        noopwithmap(on
          noop(on
              noopstreaming(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr,p_name
        order by p_mfgr,p_name) ;

-- 19. testMulti3OperatorsFunctionChain
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noopstreaming(on
          noop(on
              noopstreaming(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr
        order by p_mfgr ) ;

select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noopstreaming(on
          noop(on
              noopstreaming(on part
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr
        order by p_mfgr ) ;

-- 23. testMultiOperatorChainWithDiffPartitionForWindow2
explain
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmapstreaming(on
        noop(on
              noopstreaming(on part
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          ));

select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmapstreaming(on
        noop(on
              noopstreaming(on part
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          ));
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (rows unbounded following) as s1
     from part distribute by p_mfgr sort by p_name;

select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (range unbounded following) as s1
     from part distribute by p_mfgr sort by p_name;

set hive.mapred.mode=nonstrict;
drop table push_or;

create table push_or (key int, value string) partitioned by (ds string);

insert overwrite table push_or partition (ds='2000-04-08') select * from src where key < 20 order by key;
insert overwrite table push_or partition (ds='2000-04-09') select * from src where key < 20 order by key;

explain extended select key, value, ds from push_or where ds='2000-04-09' or key=5 order by key, ds;
select key, value, ds from push_or where ds='2000-04-09' or key=5 order by key, ds;

set hive.stats.dbclass=fs;
set hive.stats.autogather=true;
set hive.cbo.enable=false;

DROP TABLE IF EXISTS lineitem_ix;
CREATE TABLE lineitem_ix (L_ORDERKEY      INT,
                                L_PARTKEY       INT,
                                L_SUPPKEY       INT,
                                L_LINENUMBER    INT,
                                L_QUANTITY      DOUBLE,
                                L_EXTENDEDPRICE DOUBLE,
                                L_DISCOUNT      DOUBLE,
                                L_TAX           DOUBLE,
                                L_RETURNFLAG    STRING,
                                L_LINESTATUS    STRING,
                                l_shipdate      STRING,
                                L_COMMITDATE    STRING,
                                L_RECEIPTDATE   STRING,
                                L_SHIPINSTRUCT  STRING,
                                L_SHIPMODE      STRING,
                                L_COMMENT       STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|';

LOAD DATA LOCAL INPATH '../../data/files/lineitem.txt' OVERWRITE INTO TABLE lineitem_ix;

CREATE INDEX lineitem_ix_lshipdate_idx ON TABLE lineitem_ix(l_shipdate) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(l_shipdate)");
ALTER INDEX lineitem_ix_lshipdate_idx ON lineitem_ix REBUILD;

explain select l_shipdate, count(l_shipdate)
from lineitem_ix
group by l_shipdate;

select l_shipdate, count(l_shipdate)
from lineitem_ix
group by l_shipdate
order by l_shipdate;

set hive.optimize.index.groupby=true;

explain select l_shipdate, count(l_shipdate)
from lineitem_ix
group by l_shipdate;

select l_shipdate, count(l_shipdate)
from lineitem_ix
group by l_shipdate
order by l_shipdate;

set hive.optimize.index.groupby=false;


explain select year(l_shipdate) as year,
        month(l_shipdate) as month,
        count(l_shipdate) as monthly_shipments
from lineitem_ix
group by year(l_shipdate), month(l_shipdate)
order by year, month;

select year(l_shipdate) as year,
        month(l_shipdate) as month,
        count(l_shipdate) as monthly_shipments
from lineitem_ix
group by year(l_shipdate), month(l_shipdate)
order by year, month;

set hive.optimize.index.groupby=true;

explain select year(l_shipdate) as year,
        month(l_shipdate) as month,
        count(l_shipdate) as monthly_shipments
from lineitem_ix
group by year(l_shipdate), month(l_shipdate)
order by year, month;

select year(l_shipdate) as year,
        month(l_shipdate) as month,
        count(l_shipdate) as monthly_shipments
from lineitem_ix
group by year(l_shipdate), month(l_shipdate)
order by year, month;

explain select lastyear.month,
        thisyear.month,
        (thisyear.monthly_shipments - lastyear.monthly_shipments) /
lastyear.monthly_shipments as monthly_shipments_delta
   from (select year(l_shipdate) as year,
                month(l_shipdate) as month,
                count(l_shipdate) as monthly_shipments
           from lineitem_ix
          where year(l_shipdate) = 1997
          group by year(l_shipdate), month(l_shipdate)
        )  lastyear join
        (select year(l_shipdate) as year,
                month(l_shipdate) as month,
                count(l_shipdate) as monthly_shipments
           from lineitem_ix
          where year(l_shipdate) = 1998
          group by year(l_shipdate), month(l_shipdate)
        )  thisyear
  on lastyear.month = thisyear.month;

explain  select l_shipdate, cnt
from (select l_shipdate, count(l_shipdate) as cnt from lineitem_ix group by l_shipdate
union all
select l_shipdate, l_orderkey as cnt
from lineitem_ix) dummy;

CREATE TABLE tbl(key int, value int);
CREATE INDEX tbl_key_idx ON TABLE tbl(key) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(key)");
ALTER INDEX tbl_key_idx ON tbl REBUILD;

EXPLAIN select key, count(key) from tbl where key = 1 group by key;
EXPLAIN select key, count(key) from tbl group by key;

EXPLAIN select count(1) from tbl;
EXPLAIN select count(key) from tbl;

EXPLAIN select key FROM tbl GROUP BY key;
EXPLAIN select key FROM tbl GROUP BY value, key;
EXPLAIN select key FROM tbl WHERE key = 3 GROUP BY key;
EXPLAIN select key FROM tbl WHERE value = 2 GROUP BY key;
EXPLAIN select key FROM tbl GROUP BY key, substr(key,2,3);

EXPLAIN select key, value FROM tbl GROUP BY value, key;
EXPLAIN select key, value FROM tbl WHERE value = 1 GROUP BY key, value;

EXPLAIN select DISTINCT key FROM tbl;
EXPLAIN select DISTINCT key FROM tbl;
EXPLAIN select DISTINCT key FROM tbl;
EXPLAIN select DISTINCT key, value FROM tbl;
EXPLAIN select DISTINCT key, value FROM tbl WHERE value = 2;
EXPLAIN select DISTINCT key, value FROM tbl WHERE value = 2 AND key = 3;
EXPLAIN select DISTINCT key, value FROM tbl WHERE value = key;
EXPLAIN select DISTINCT key, substr(value,2,3) FROM tbl WHERE value = key;
EXPLAIN select DISTINCT key, substr(value,2,3) FROM tbl;

EXPLAIN select * FROM (select DISTINCT key, value FROM tbl) v1 WHERE v1.value = 2;

DROP TABLE tbl;

CREATE TABLE tblpart (key int, value string) PARTITIONED BY (ds string, hr int);
INSERT OVERWRITE TABLE tblpart PARTITION (ds='2008-04-08', hr=11) SELECT key, value FROM srcpart WHERE ds = '2008-04-08' AND hr = 11;
INSERT OVERWRITE TABLE tblpart PARTITION (ds='2008-04-08', hr=12) SELECT key, value FROM srcpart WHERE ds = '2008-04-08' AND hr = 12;
INSERT OVERWRITE TABLE tblpart PARTITION (ds='2008-04-09', hr=11) SELECT key, value FROM srcpart WHERE ds = '2008-04-09' AND hr = 11;
INSERT OVERWRITE TABLE tblpart PARTITION (ds='2008-04-09', hr=12) SELECT key, value FROM srcpart WHERE ds = '2008-04-09' AND hr = 12;

CREATE INDEX tbl_part_index ON TABLE tblpart(key) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(key)");

ALTER INDEX tbl_part_index ON tblpart PARTITION (ds='2008-04-08', hr=11) REBUILD;
EXPLAIN SELECT key, count(key) FROM tblpart WHERE ds='2008-04-09' AND hr=12 AND key < 10 GROUP BY key;

ALTER INDEX tbl_part_index ON tblpart PARTITION (ds='2008-04-08', hr=12) REBUILD;
ALTER INDEX tbl_part_index ON tblpart PARTITION (ds='2008-04-09', hr=11) REBUILD;
ALTER INDEX tbl_part_index ON tblpart PARTITION (ds='2008-04-09', hr=12) REBUILD;
EXPLAIN SELECT key, count(key) FROM tblpart WHERE ds='2008-04-09' AND hr=12 AND key < 10 GROUP BY key;

DROP INDEX tbl_part_index on tblpart;
DROP TABLE tblpart;

CREATE TABLE tbl(key int, value int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';
LOAD DATA LOCAL INPATH '../../data/files/tbl.txt' OVERWRITE INTO TABLE tbl;

CREATE INDEX tbl_key_idx ON TABLE tbl(key) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(key)");
ALTER INDEX tbl_key_idx ON tbl REBUILD;

set hive.optimize.index.groupby=false;
explain select key, count(key) from tbl group by key order by key;
select key, count(key) from tbl group by key order by key;
set hive.optimize.index.groupby=true;
explain select key, count(key) from tbl group by key order by key;
select key, count(key) from tbl group by key order by key;
DROP TABLE tbl;

reset hive.cbo.enable;
set hive.stats.dbclass=fs;
set hive.stats.autogather=true;
set hive.cbo.enable=true;

DROP TABLE IF EXISTS lineitem_ix;
CREATE TABLE lineitem_ix (L_ORDERKEY      INT,
                                L_PARTKEY       INT,
                                L_SUPPKEY       INT,
                                L_LINENUMBER    INT,
                                L_QUANTITY      DOUBLE,
                                L_EXTENDEDPRICE DOUBLE,
                                L_DISCOUNT      DOUBLE,
                                L_TAX           DOUBLE,
                                L_RETURNFLAG    STRING,
                                L_LINESTATUS    STRING,
                                l_shipdate      STRING,
                                L_COMMITDATE    STRING,
                                L_RECEIPTDATE   STRING,
                                L_SHIPINSTRUCT  STRING,
                                L_SHIPMODE      STRING,
                                L_COMMENT       STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|';

LOAD DATA LOCAL INPATH '../../data/files/lineitem.txt' OVERWRITE INTO TABLE lineitem_ix;

CREATE INDEX lineitem_ix_lshipdate_idx ON TABLE lineitem_ix(l_shipdate) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(l_shipdate)");
ALTER INDEX lineitem_ix_lshipdate_idx ON lineitem_ix REBUILD;

explain select l_shipdate, count(l_shipdate)
from lineitem_ix
group by l_shipdate;

select l_shipdate, count(l_shipdate)
from lineitem_ix
group by l_shipdate
order by l_shipdate;

set hive.optimize.index.groupby=true;

explain select l_shipdate, count(l_shipdate)
from lineitem_ix
group by l_shipdate;

select l_shipdate, count(l_shipdate)
from lineitem_ix
group by l_shipdate
order by l_shipdate;

set hive.optimize.index.groupby=false;


explain select year(l_shipdate) as year,
        month(l_shipdate) as month,
        count(l_shipdate) as monthly_shipments
from lineitem_ix
group by year(l_shipdate), month(l_shipdate)
order by year, month;

select year(l_shipdate) as year,
        month(l_shipdate) as month,
        count(l_shipdate) as monthly_shipments
from lineitem_ix
group by year(l_shipdate), month(l_shipdate)
order by year, month;

set hive.optimize.index.groupby=true;

explain select year(l_shipdate) as year,
        month(l_shipdate) as month,
        count(l_shipdate) as monthly_shipments
from lineitem_ix
group by year(l_shipdate), month(l_shipdate)
order by year, month;

select year(l_shipdate) as year,
        month(l_shipdate) as month,
        count(l_shipdate) as monthly_shipments
from lineitem_ix
group by year(l_shipdate), month(l_shipdate)
order by year, month;

explain select lastyear.month,
        thisyear.month,
        (thisyear.monthly_shipments - lastyear.monthly_shipments) /
lastyear.monthly_shipments as monthly_shipments_delta
   from (select year(l_shipdate) as year,
                month(l_shipdate) as month,
                count(l_shipdate) as monthly_shipments
           from lineitem_ix
          where year(l_shipdate) = 1997
          group by year(l_shipdate), month(l_shipdate)
        )  lastyear join
        (select year(l_shipdate) as year,
                month(l_shipdate) as month,
                count(l_shipdate) as monthly_shipments
           from lineitem_ix
          where year(l_shipdate) = 1998
          group by year(l_shipdate), month(l_shipdate)
        )  thisyear
  on lastyear.month = thisyear.month;

explain  select l_shipdate, cnt
from (select l_shipdate, count(l_shipdate) as cnt from lineitem_ix group by l_shipdate
union all
select l_shipdate, l_orderkey as cnt
from lineitem_ix) dummy;

CREATE TABLE tbl(key int, value int);
CREATE INDEX tbl_key_idx ON TABLE tbl(key) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(key)");
ALTER INDEX tbl_key_idx ON tbl REBUILD;

EXPLAIN select key, count(key) from tbl where key = 1 group by key;
EXPLAIN select key, count(key) from tbl group by key;

EXPLAIN select count(1) from tbl;
EXPLAIN select count(key) from tbl;

EXPLAIN select key FROM tbl GROUP BY key;
EXPLAIN select key FROM tbl GROUP BY value, key;
EXPLAIN select key FROM tbl WHERE key = 3 GROUP BY key;
EXPLAIN select key FROM tbl WHERE value = 2 GROUP BY key;
EXPLAIN select key FROM tbl GROUP BY key, substr(key,2,3);

EXPLAIN select key, value FROM tbl GROUP BY value, key;
EXPLAIN select key, value FROM tbl WHERE value = 1 GROUP BY key, value;

EXPLAIN select DISTINCT key FROM tbl;
EXPLAIN select DISTINCT key FROM tbl;
EXPLAIN select DISTINCT key FROM tbl;
EXPLAIN select DISTINCT key, value FROM tbl;
EXPLAIN select DISTINCT key, value FROM tbl WHERE value = 2;
EXPLAIN select DISTINCT key, value FROM tbl WHERE value = 2 AND key = 3;
EXPLAIN select DISTINCT key, value FROM tbl WHERE value = key;
EXPLAIN select DISTINCT key, substr(value,2,3) FROM tbl WHERE value = key;
EXPLAIN select DISTINCT key, substr(value,2,3) FROM tbl;

EXPLAIN select * FROM (select DISTINCT key, value FROM tbl) v1 WHERE v1.value = 2;

DROP TABLE tbl;

CREATE TABLE tblpart (key int, value string) PARTITIONED BY (ds string, hr int);
INSERT OVERWRITE TABLE tblpart PARTITION (ds='2008-04-08', hr=11) SELECT key, value FROM srcpart WHERE ds = '2008-04-08' AND hr = 11;
INSERT OVERWRITE TABLE tblpart PARTITION (ds='2008-04-08', hr=12) SELECT key, value FROM srcpart WHERE ds = '2008-04-08' AND hr = 12;
INSERT OVERWRITE TABLE tblpart PARTITION (ds='2008-04-09', hr=11) SELECT key, value FROM srcpart WHERE ds = '2008-04-09' AND hr = 11;
INSERT OVERWRITE TABLE tblpart PARTITION (ds='2008-04-09', hr=12) SELECT key, value FROM srcpart WHERE ds = '2008-04-09' AND hr = 12;

CREATE INDEX tbl_part_index ON TABLE tblpart(key) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(key)");

ALTER INDEX tbl_part_index ON tblpart PARTITION (ds='2008-04-08', hr=11) REBUILD;
EXPLAIN SELECT key, count(key) FROM tblpart WHERE ds='2008-04-09' AND hr=12 AND key < 10 GROUP BY key;

ALTER INDEX tbl_part_index ON tblpart PARTITION (ds='2008-04-08', hr=12) REBUILD;
ALTER INDEX tbl_part_index ON tblpart PARTITION (ds='2008-04-09', hr=11) REBUILD;
ALTER INDEX tbl_part_index ON tblpart PARTITION (ds='2008-04-09', hr=12) REBUILD;
EXPLAIN SELECT key, count(key) FROM tblpart WHERE ds='2008-04-09' AND hr=12 AND key < 10 GROUP BY key;

DROP INDEX tbl_part_index on tblpart;
DROP TABLE tblpart;

CREATE TABLE tbl(key int, value int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';
LOAD DATA LOCAL INPATH '../../data/files/tbl.txt' OVERWRITE INTO TABLE tbl;

CREATE INDEX tbl_key_idx ON TABLE tbl(key) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(key)");
ALTER INDEX tbl_key_idx ON tbl REBUILD;

set hive.optimize.index.groupby=false;
explain select key, count(key) from tbl group by key order by key;
select key, count(key) from tbl group by key order by key;
set hive.optimize.index.groupby=true;
explain select key, count(key) from tbl group by key order by key;
select key, count(key) from tbl group by key order by key;
DROP TABLE tbl;set hive.stats.dbclass=fs;
set hive.stats.autogather=true;
set hive.cbo.enable=true;
set hive.optimize.index.groupby=true;

DROP TABLE IF EXISTS lineitem_ix;
DROP INDEX IF EXISTS lineitem_ix_L_ORDERKEY_idx on lineitem_ix;
DROP INDEX IF EXISTS lineitem_ix_L_PARTKEY_idx on lineitem_ix;


CREATE TABLE lineitem_ix (L_ORDERKEY      INT,
                                L_PARTKEY       INT,
                                L_SUPPKEY       INT,
                                L_LINENUMBER    INT,
                                L_QUANTITY      DOUBLE,
                                L_EXTENDEDPRICE DOUBLE,
                                L_DISCOUNT      DOUBLE,
                                L_TAX           DOUBLE,
                                L_RETURNFLAG    STRING,
                                L_LINESTATUS    STRING,
                                l_shipdate      STRING,
                                L_COMMITDATE    STRING,
                                L_RECEIPTDATE   STRING,
                                L_SHIPINSTRUCT  STRING,
                                L_SHIPMODE      STRING,
                                L_COMMENT       STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|';

LOAD DATA LOCAL INPATH '../../data/files/lineitem.txt' OVERWRITE INTO TABLE lineitem_ix;

CREATE INDEX lineitem_ix_L_ORDERKEY_idx ON TABLE lineitem_ix(L_ORDERKEY) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(L_ORDERKEY)");
ALTER INDEX lineitem_ix_L_ORDERKEY_idx ON lineitem_ix REBUILD;

CREATE INDEX lineitem_ix_L_PARTKEY_idx ON TABLE lineitem_ix(L_PARTKEY) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(L_PARTKEY)");
ALTER INDEX lineitem_ix_L_PARTKEY_idx ON lineitem_ix REBUILD;

explain
select count(1)
from lineitem_ix;

select count(1)
from lineitem_ix;

explain
select count(L_ORDERKEY)
from lineitem_ix;

select count(L_ORDERKEY)
from lineitem_ix;

explain select L_ORDERKEY+L_PARTKEY as keysum,
count(L_ORDERKEY), count(L_PARTKEY)
from lineitem_ix
group by L_ORDERKEY, L_PARTKEY;

select L_ORDERKEY+L_PARTKEY as keysum,
count(L_ORDERKEY), count(L_PARTKEY)
from lineitem_ix
group by L_ORDERKEY, L_PARTKEY;

explain
select L_ORDERKEY, count(L_ORDERKEY)
from  lineitem_ix
where L_ORDERKEY = 7
group by L_ORDERKEY;

select L_ORDERKEY, count(L_ORDERKEY)
from  lineitem_ix
where L_ORDERKEY = 7
group by L_ORDERKEY;

explain
select L_ORDERKEY, count(1)
from lineitem_ix
group by L_ORDERKEY;

select L_ORDERKEY, count(1)
from lineitem_ix
group by L_ORDERKEY;

explain
select count(L_ORDERKEY+1)
from lineitem_ix;

select count(L_ORDERKEY+1)
from lineitem_ix;

explain
select L_ORDERKEY, count(L_ORDERKEY+1)
from lineitem_ix
group by L_ORDERKEY;

select L_ORDERKEY, count(L_ORDERKEY+1)
from lineitem_ix
group by L_ORDERKEY;

explain
select L_ORDERKEY, count(L_ORDERKEY+1+L_ORDERKEY+2)
from lineitem_ix
group by L_ORDERKEY;

select L_ORDERKEY, count(L_ORDERKEY+1+L_ORDERKEY+2)
from lineitem_ix
group by L_ORDERKEY;

explain
select L_ORDERKEY, count(1+L_ORDERKEY+2)
from lineitem_ix
group by L_ORDERKEY;

select L_ORDERKEY, count(1+L_ORDERKEY+2)
from lineitem_ix
group by L_ORDERKEY;


explain
select L_ORDERKEY as a, count(1) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY;

select L_ORDERKEY as a, count(1) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY;

explain
select L_ORDERKEY, count(keysum), sum(keysum)
from
(select L_ORDERKEY, L_ORDERKEY+L_PARTKEY as keysum from lineitem_ix) tabA
group by L_ORDERKEY;

select L_ORDERKEY, count(keysum), sum(keysum)
from
(select L_ORDERKEY, L_ORDERKEY+L_PARTKEY as keysum from lineitem_ix) tabA
group by L_ORDERKEY;


explain
select L_ORDERKEY, count(L_ORDERKEY), sum(L_ORDERKEY)
from lineitem_ix
group by L_ORDERKEY;

select L_ORDERKEY, count(L_ORDERKEY), sum(L_ORDERKEY)
from lineitem_ix
group by L_ORDERKEY;

explain
select colA, count(colA)
from (select L_ORDERKEY as colA from lineitem_ix) tabA
group by colA;

select colA, count(colA)
from (select L_ORDERKEY as colA from lineitem_ix) tabA
group by colA;

explain
select keysum, count(keysum)
from
(select L_ORDERKEY+L_PARTKEY as keysum from lineitem_ix) tabA
group by keysum;

select keysum, count(keysum)
from
(select L_ORDERKEY+L_PARTKEY as keysum from lineitem_ix) tabA
group by keysum;

explain
select keysum, count(keysum)
from
(select L_ORDERKEY+1 as keysum from lineitem_ix) tabA
group by keysum;

select keysum, count(keysum)
from
(select L_ORDERKEY+1 as keysum from lineitem_ix) tabA
group by keysum;


explain
select keysum, count(1)
from
(select L_ORDERKEY+1 as keysum from lineitem_ix) tabA
group by keysum;

select keysum, count(1)
from
(select L_ORDERKEY+1 as keysum from lineitem_ix) tabA
group by keysum;


explain
select keysum, count(keysum)
from
(select L_ORDERKEY+1 as keysum from lineitem_ix where L_ORDERKEY = 7) tabA
group by keysum;

select keysum, count(keysum)
from
(select L_ORDERKEY+1 as keysum from lineitem_ix where L_ORDERKEY = 7) tabA
group by keysum;


explain
select ckeysum, count(ckeysum)
from
(select keysum, count(keysum) as ckeysum
from
	(select L_ORDERKEY+1 as keysum from lineitem_ix where L_ORDERKEY = 7) tabA
group by keysum) tabB
group by ckeysum;

select ckeysum, count(ckeysum)
from
(select keysum, count(keysum) as ckeysum
from
	(select L_ORDERKEY+1 as keysum from lineitem_ix where L_ORDERKEY = 7) tabA
group by keysum) tabB
group by ckeysum;

explain
select keysum, count(keysum) as ckeysum
from
(select L_ORDERKEY, count(L_ORDERKEY) as keysum
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY)tabA
group by keysum;

select keysum, count(keysum) as ckeysum
from
(select L_ORDERKEY, count(L_ORDERKEY) as keysum
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY)tabA
group by keysum;


DROP INDEX IF EXISTS src_key_idx on src;
CREATE INDEX src_key_idx ON TABLE src(key) AS 'org.apache.hadoop.hive.ql.index.AggregateIndexHandler' WITH DEFERRED REBUILD IDXPROPERTIES("AGGREGATES"="count(key)");
ALTER INDEX src_key_idx ON src REBUILD;

explain
select tabA.a, tabA.b, tabB.a, tabB.b
from
(select L_ORDERKEY as a, count(L_ORDERKEY) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY) tabA
join
(select key as a, count(key) as b
from src
group by key
) tabB
on (tabA.b=tabB.b);

select tabA.a, tabA.b, tabB.a, tabB.b
from
(select L_ORDERKEY as a, count(L_ORDERKEY) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY) tabA
join
(select key as a, count(key) as b
from src
group by key
) tabB
on (tabA.b=tabB.b);


explain
select tabA.a, tabA.b, tabB.a, tabB.b
from
(select L_ORDERKEY as a, count(L_ORDERKEY) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY) tabA
join
(select key as a, count(key) as b
from src
group by key
) tabB
on (tabA.b=tabB.b and tabB.a < '2');

select tabA.a, tabA.b, tabB.a, tabB.b
from
(select L_ORDERKEY as a, count(L_ORDERKEY) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY) tabA
join
(select key as a, count(key) as b
from src
group by key
) tabB
on (tabA.b=tabB.b and tabB.a < '2');

EXPLAIN
select L_ORDERKEY FROM lineitem_ix GROUP BY L_ORDERKEY, L_ORDERKEY+1;

select L_ORDERKEY FROM lineitem_ix GROUP BY L_ORDERKEY, L_ORDERKEY+1;

EXPLAIN
select L_ORDERKEY, L_ORDERKEY+1, count(L_ORDERKEY) FROM lineitem_ix GROUP BY L_ORDERKEY, L_ORDERKEY+1;

select L_ORDERKEY, L_ORDERKEY+1, count(L_ORDERKEY) FROM lineitem_ix GROUP BY L_ORDERKEY, L_ORDERKEY+1;

EXPLAIN
select L_ORDERKEY+2, count(L_ORDERKEY) FROM lineitem_ix GROUP BY L_ORDERKEY+2;

select L_ORDERKEY+2, count(L_ORDERKEY) FROM lineitem_ix GROUP BY L_ORDERKEY+2;

--with cbo on, the following query can use idx

explain
select b, count(b) as ckeysum
from
(
select L_ORDERKEY as a, count(L_ORDERKEY) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY
union all
select L_PARTKEY as a, count(L_PARTKEY) as b
from lineitem_ix
where L_PARTKEY < 10
group by L_PARTKEY
) tabA
group by b;

select b, count(b) as ckeysum
from
(
select L_ORDERKEY as a, count(L_ORDERKEY) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY
union all
select L_PARTKEY as a, count(L_PARTKEY) as b
from lineitem_ix
where L_PARTKEY < 10
group by L_PARTKEY
) tabA
group by b;

--with cbo on, the following query can not use idx because AggFunc is empty here

explain
select a, count(a) as ckeysum
from
(
select L_ORDERKEY as a, count(L_ORDERKEY) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY
union all
select L_PARTKEY as a, count(L_PARTKEY) as b
from lineitem_ix
where L_PARTKEY < 10
group by L_PARTKEY
) tabA
group by a;

select a, count(a) as ckeysum
from
(
select L_ORDERKEY as a, count(L_ORDERKEY) as b
from lineitem_ix
where L_ORDERKEY < 7
group by L_ORDERKEY
union all
select L_PARTKEY as a, count(L_PARTKEY) as b
from lineitem_ix
where L_PARTKEY < 10
group by L_PARTKEY
) tabA
group by a;

explain
select a, count(a)
from (
select case L_ORDERKEY when null then 1 else 1 END as a
from lineitem_ix)tab
group by a;

select a, count(a)
from (
select case L_ORDERKEY when null then 1 else 1 END as a
from lineitem_ix)tab
group by a;

set hive.cbo.enable=false;
set hive.exec.post.hooks = org.apache.hadoop.hive.ql.hooks.CheckQueryPropertiesHook;

select * from src a join src b on a.key = b.key limit 0;
select * from src group by src.key, src.value limit 0;
select * from src order by src.key limit 0;
select * from src sort by src.key limit 0;
select a.key, sum(b.value) from src a join src b on a.key = b.key group by a.key limit 0;
select transform(*) using 'cat' from src limit 0;
select * from src distribute by src.key limit 0;
select * from src cluster by src.key limit 0;

select key, sum(value) from (select a.key as key, b.value as value from src a join src b on a.key = b.key) c group by key limit 0;
select * from src a join src b on a.key = b.key order by a.key limit 0;
select * from src a join src b on a.key = b.key distribute by a.key sort by a.key, b.value limit 0;

create table nzhang_test1 stored as sequencefile as select 'key1' as key, 'value
1

http://asdf' value from src limit 1;

select * from nzhang_test1;
select count(*) from nzhang_test1;

explain
select * from nzhang_test1 where key='key1';

select * from nzhang_test1 where key='key1';

set hive.query.result.fileformat=SequenceFile;

select * from nzhang_test1;

select count(*) from nzhang_test1;

explain
select * from nzhang_test1 where key='key1';

select * from nzhang_test1 where key='key1';
from src
select transform('aa\;') using 'cat' as a  limit 1;

from src
select transform('bb') using 'cat' as b limit 1; from src
select transform('cc') using 'cat' as c limit 1;set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(`location` INT, `type` STRING) PARTITIONED BY(`table` STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src
INSERT OVERWRITE TABLE dest1 PARTITION(`table`='2008-04-08') SELECT src.key as `partition`, src.value as `from` WHERE src.key >= 200 and src.key < 300;

EXPLAIN
SELECT `int`.`location`, `int`.`type`, `int`.`table` FROM dest1 `int` WHERE `int`.`table` = '2008-04-08';

FROM src
INSERT OVERWRITE TABLE dest1 PARTITION(`table`='2008-04-08') SELECT src.key as `partition`, src.value as `from` WHERE src.key >= 200 and src.key < 300;

SELECT `int`.`location`, `int`.`type`, `int`.`table` FROM dest1 `int` WHERE `int`.`table` = '2008-04-08';
set hive.fetch.task.conversion=more;

EXPLAIN
SELECT
    'abc',        "abc",
    'abc\'',      "abc\"",
    'abc\\',      "abc\\",
    'abc\\\'',    "abc\\\"",
    'abc\\\\',    "abc\\\\",
    'abc\\\\\'',  "abc\\\\\"",
    'abc\\\\\\',  "abc\\\\\\",
    'abc""""\\',  "abc''''\\",
    'mysql_%\\_\%', 'mysql\\\_\\\\\%',
    "awk '{print NR\"\\t\"$0}'",
    'tab\ttab',   "tab\ttab"
FROM src
LIMIT 1;

SELECT
    'abc',        "abc",
    'abc\'',      "abc\"",
    'abc\\',      "abc\\",
    'abc\\\'',    "abc\\\"",
    'abc\\\\',    "abc\\\\",
    'abc\\\\\'',  "abc\\\\\"",
    'abc\\\\\\',  "abc\\\\\\",
    'abc""""\\',  "abc''''\\",
    'mysql_%\\_\%', 'mysql\\\_\\\\\%',
    "awk '{print NR\"\\t\"$0}'",
    'tab\ttab',   "tab\ttab"
FROM src
LIMIT 1;

set hive.support.quoted.identifiers=column;

create table src_b3(`x+1` string, `!@#$%^&*()_q` string) ;

alter table src_b3
clustered by (`!@#$%^&*()_q`) sorted by (`!@#$%^&*()_q`) into 2 buckets
;


-- alter partition
create table src_p3(`x+1` string, `y&y` string) partitioned by (`!@#$%^&*()_q` string);

insert overwrite table src_p3 partition(`!@#$%^&*()_q`='a') select * from src;
show partitions src_p3;

alter table src_p3 add if not exists partition(`!@#$%^&*()_q`='b');
show partitions src_p3;

alter table src_p3 partition(`!@#$%^&*()_q`='b') rename to partition(`!@#$%^&*()_q`='c');
show partitions src_p3;set hive.mapred.mode=nonstrict;

set hive.support.quoted.identifiers=column;

-- basic
create table t1(`x+1` string, `y&y` string, `!@#$%^&*()_q` string);
describe t1;
select `x+1`, `y&y`, `!@#$%^&*()_q` from t1;
explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1;
explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1 where `!@#$%^&*()_q` = '1';
explain select `x+1`, `y&y`, `!@#$%^&*()_q` from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&y`, `!@#$%^&*()_q` having `!@#$%^&*()_q` = '1';
explain select `x+1`, `y&y`, `!@#$%^&*()_q`, rank() over(partition by `!@#$%^&*()_q` order by  `y&y`)
from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&y`, `!@#$%^&*()_q` having `!@#$%^&*()_q` = '1';

-- case insensitive
explain select `X+1`, `Y&y`, `!@#$%^&*()_Q`, rank() over(partition by `!@#$%^&*()_q` order by  `y&y`)
from t1 where `!@#$%^&*()_q` = '1' group by `x+1`, `y&Y`, `!@#$%^&*()_q` having `!@#$%^&*()_Q` = '1';


-- escaped back ticks
create table t4(`x+1``` string, `y&y` string);
describe t4;
insert into table t4 select * from src;
select `x+1```, `y&y`, rank() over(partition by `x+1``` order by  `y&y`)
from t4 where `x+1``` = '10' group by `x+1```, `y&y` having `x+1``` = '10';

-- view
create view v1 as
select `x+1```, `y&y`
from t4 where `x+1``` < '200';

select `x+1```, `y&y`, rank() over(partition by `x+1``` order by  `y&y`)
from v1
group by `x+1```, `y&y`
;set hive.mapred.mode=nonstrict;

set hive.support.quoted.identifiers=column;


create table src_p(`x+1` string, `y&y` string) partitioned by (`!@#$%^&*()_q` string);
insert overwrite table src_p partition(`!@#$%^&*()_q`='a') select * from src;

show partitions src_p;

explain select `x+1`, `y&y`, `!@#$%^&*()_q`
from src_p where `!@#$%^&*()_q` = 'a' and `x+1`='10'
group by `x+1`, `y&y`, `!@#$%^&*()_q` having `!@#$%^&*()_q` = 'a'
;

set hive.exec.dynamic.partition.mode=nonstrict
;

create table src_p2(`x+1` string) partitioned by (`!@#$%^&*()_q` string);

insert overwrite table src_p2 partition(`!@#$%^&*()_q`)
select key, value as `!@#$%^&*()_q` from src where key < '200'
;

show partitions src_p2;set hive.mapred.mode=nonstrict;

set hive.support.quoted.identifiers=column;

set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(`!@#$%^&*()_q` string, `y&y` string)
SKEWED BY (`!@#$%^&*()_q`) ON ((2)) STORED AS TEXTFILE
;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(`!@#$%^&*()_q` string, `y&y` string)
SKEWED BY (`!@#$%^&*()_q`) ON ((2)) STORED AS TEXTFILE
;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T2;

-- a simple join query with skew on both the tables on the join key
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a. `!@#$%^&*()_q`  = b. `!@#$%^&*()_q`
;


set hive.support.quoted.identifiers=column;


;

create table src_b(`x+1` string, `!@#$%^&*()_q` string)
clustered by (`!@#$%^&*()_q`) sorted by (`!@#$%^&*()_q`) into 2 buckets
;

insert overwrite table src_b
select * from src
;

create table src_b2(`x+1` string, `!@#$%^&*()_q` string)
clustered by (`!@#$%^&*()_q`) sorted by (`!@#$%^&*()_q`) into 2 buckets
;

insert overwrite table src_b2
select * from src
;

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

set hive.auto.convert.sortmerge.join.to.mapjoin=false;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

select a.`x+1`, a.`!@#$%^&*()_q`, b.`x+1`, b.`!@#$%^&*()_q`
from src_b a join src_b2 b on a.`!@#$%^&*()_q` = b.`!@#$%^&*()_q`
where a.`x+1` < '11'
;ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;

CREATE TABLE xyz(KEY STRING, VALUE STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.TestSerDe'
STORED AS TEXTFILE
TBLPROPERTIES('columns'='valid_colname,invalid.colname')
;

describe xyz;FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234", src.value WHERE src.key < 100
-- scanning un-partitioned data
explain extended select * from src where rand(1) < 0.1;
select * from src where rand(1) < 0.1;
-- scanning partitioned data

create table tmptable(key string, value string, hr string, ds string);

explain extended
insert overwrite table tmptable
select a.* from srcpart a where rand(1) < 0.1 and a.ds = '2008-04-08';


insert overwrite table tmptable
select a.* from srcpart a where rand(1) < 0.1 and a.ds = '2008-04-08';

select * from tmptable x sort by x.key,x.value,x.ds,x.hr;

set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
-- complex predicates in the where clause

explain extended select a.* from srcpart a where rand(1) < 0.1 and a.ds = '2008-04-08' and not(key > 50 or key < 10) and a.hr like '%2';
select a.* from srcpart a where rand(1) < 0.1 and a.ds = '2008-04-08' and not(key > 50 or key < 10) and a.hr like '%2';

-- without rand for comparison
explain extended select a.* from srcpart a where a.ds = '2008-04-08' and not(key > 50 or key < 10) and a.hr like '%2';
select a.* from srcpart a where a.ds = '2008-04-08' and not(key > 50 or key < 10) and a.hr like '%2';
set hive.map.aggr.hash.percentmemory = 0.3;
set hive.mapred.local.mem = 256;

add file ../../data/scripts/dumpdata_script.py;

CREATE table columnTable_Bigdata (key STRING, value STRING)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileOutputFormat';

FROM (FROM src MAP src.key,src.value USING 'python dumpdata_script.py' AS (key,value) WHERE src.key = 10) subq
INSERT OVERWRITE TABLE columnTable_Bigdata SELECT subq.key, subq.value;

describe columnTable_Bigdata;
select count(columnTable_Bigdata.key) from columnTable_Bigdata;


set hive.mapred.mode=nonstrict;

CREATE table columnTable (key STRING, value STRING)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileOutputFormat';

FROM src
INSERT OVERWRITE TABLE columnTable SELECT src.key, src.value LIMIT 10;
describe columnTable;

SELECT columnTable.* FROM columnTable ORDER BY key ASC, value ASC;


set hive.mapred.mode=nonstrict;
set hive.merge.rcfile.block.level=true;
set mapred.max.split.size=100;
set mapred.min.split.size=1;

DROP TABLE rcfile_createas1a;
DROP TABLE rcfile_createas1b;

CREATE TABLE rcfile_createas1a (key INT, value STRING)
    PARTITIONED BY (ds string);
INSERT OVERWRITE TABLE rcfile_createas1a PARTITION (ds='1')
    SELECT * FROM src;
INSERT OVERWRITE TABLE rcfile_createas1a PARTITION (ds='2')
    SELECT * FROM src;

EXPLAIN
    CREATE TABLE rcfile_createas1b
    STORED AS RCFILE AS
        SELECT key, value, PMOD(HASH(key), 50) as part
        FROM rcfile_createas1a;
CREATE TABLE rcfile_createas1b
    STORED AS RCFILE AS
        SELECT key, value, PMOD(HASH(key), 50) as part
        FROM rcfile_createas1a;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM rcfile_createas1a
) t;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM rcfile_createas1b
) t;

DROP TABLE rcfile_createas1a;
DROP TABLE rcfile_createas1b;
SET hive.default.fileformat = RCFile;

CREATE TABLE rcfile_default_format (key STRING);
DESCRIBE FORMATTED rcfile_default_format;

CREATE TABLE rcfile_default_format_ctas AS SELECT key,value FROM src;
DESCRIBE FORMATTED rcfile_default_format_ctas;

CREATE TABLE rcfile_default_format_txtfile (key STRING) STORED AS TEXTFILE;
INSERT OVERWRITE TABLE rcfile_default_format_txtfile SELECT key from src;
DESCRIBE FORMATTED rcfile_default_format_txtfile;

SET hive.default.fileformat = TextFile;
CREATE TABLE textfile_default_format_ctas AS SELECT key,value FROM rcfile_default_format_ctas;
DESCRIBE FORMATTED textfile_default_format_ctas;

SET hive.default.fileformat = RCFile;
SET hive.default.rcfile.serde = org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe;
CREATE TABLE rcfile_default_format_ctas_default_serde AS SELECT key,value FROM rcfile_default_format_ctas;
DESCRIBE FORMATTED rcfile_default_format_ctas_default_serde;

CREATE TABLE rcfile_default_format_default_serde (key STRING);
DESCRIBE FORMATTED rcfile_default_format_default_serde;

SET hive.default.fileformat = TextFile;
CREATE TABLE rcfile_ctas_default_serde STORED AS rcfile AS SELECT key,value FROM rcfile_default_format_ctas;
DESCRIBE FORMATTED rcfile_ctas_default_serde;

CREATE TABLE rcfile_default_serde (key STRING) STORED AS rcfile;
DESCRIBE FORMATTED rcfile_default_serde;

set hive.mapred.mode=nonstrict;

CREATE table rcfileTableLazyDecompress (key STRING, value STRING) STORED AS RCFile;

FROM src
INSERT OVERWRITE TABLE rcfileTableLazyDecompress SELECT src.key, src.value LIMIT 10;

SELECT key, value FROM rcfileTableLazyDecompress where key > 238 ORDER BY key ASC, value ASC;

SELECT key, value FROM rcfileTableLazyDecompress where key > 238 and key < 400 ORDER BY key ASC, value ASC;

SELECT key, count(1) FROM rcfileTableLazyDecompress where key > 238 group by key ORDER BY key ASC;

set mapred.output.compress=true;
set hive.exec.compress.output=true;

FROM src
INSERT OVERWRITE TABLE rcfileTableLazyDecompress SELECT src.key, src.value LIMIT 10;

SELECT key, value FROM rcfileTableLazyDecompress where key > 238 ORDER BY key ASC, value ASC;

SELECT key, value FROM rcfileTableLazyDecompress where key > 238 and key < 400 ORDER BY key ASC, value ASC;

SELECT key, count(1) FROM rcfileTableLazyDecompress where key > 238 group by key ORDER BY key ASC;

set mapred.output.compress=false;
set hive.exec.compress.output=false;

set hive.merge.rcfile.block.level=false;
set hive.exec.dynamic.partition=true;
set mapred.max.split.size=100;
set mapref.min.split.size=1;

DROP TABLE rcfile_merge1;
DROP TABLE rcfile_merge1b;

CREATE TABLE rcfile_merge1 (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS RCFILE;
CREATE TABLE rcfile_merge1b (key INT, value STRING)
    PARTITIONED BY (ds STRING, part STRING) STORED AS RCFILE;

-- Use non block-level merge
EXPLAIN
    INSERT OVERWRITE TABLE rcfile_merge1 PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 100) as part
        FROM src;
INSERT OVERWRITE TABLE rcfile_merge1 PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 100) as part
    FROM src;

set hive.merge.rcfile.block.level=true;
EXPLAIN
    INSERT OVERWRITE TABLE rcfile_merge1b PARTITION (ds='1', part)
        SELECT key, value, PMOD(HASH(key), 100) as part
        FROM src;
INSERT OVERWRITE TABLE rcfile_merge1b PARTITION (ds='1', part)
    SELECT key, value, PMOD(HASH(key), 100) as part
    FROM src;

-- Verify
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM rcfile_merge1 WHERE ds='1'
) t;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM rcfile_merge1b WHERE ds='1'
) t;

DROP TABLE rcfile_merge1;
DROP TABLE rcfile_merge1b;
set hive.mapred.mode=nonstrict;
set hive.merge.rcfile.block.level=true;
set hive.exec.dynamic.partition=true;
set mapred.max.split.size=100;
set mapred.min.split.size=1;

DROP TABLE rcfile_merge2a;

CREATE TABLE rcfile_merge2a (key INT, value STRING)
    PARTITIONED BY (one string, two string, three string)
    STORED AS RCFILE;

EXPLAIN INSERT OVERWRITE TABLE rcfile_merge2a PARTITION (one='1', two, three)
    SELECT key, value, PMOD(HASH(key), 10) as two,
        PMOD(HASH(value), 10) as three
    FROM src;
INSERT OVERWRITE TABLE rcfile_merge2a PARTITION (one='1', two, three)
    SELECT key, value, PMOD(HASH(key), 10) as two,
        PMOD(HASH(value), 10) as three
    FROM src;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(*) USING 'tr \t _' AS (c)
    FROM rcfile_merge2a
) t;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value, '1', PMOD(HASH(key), 10),
        PMOD(HASH(value), 10)) USING 'tr \t _' AS (c)
    FROM src
) t;

DROP TABLE rcfile_merge2a;

set hive.mapred.mode=nonstrict;
set hive.merge.rcfile.block.level=true;
set mapred.max.split.size=100;
set mapred.min.split.size=1;

DROP TABLE rcfile_merge3a;
DROP TABLE rcfile_merge3b;

CREATE TABLE rcfile_merge3a (key int, value string)
    PARTITIONED BY (ds string) STORED AS TEXTFILE;
CREATE TABLE rcfile_merge3b (key int, value string) STORED AS RCFILE;

INSERT OVERWRITE TABLE rcfile_merge3a PARTITION (ds='1')
    SELECT * FROM src;
INSERT OVERWRITE TABLE rcfile_merge3a PARTITION (ds='2')
    SELECT * FROM src;

EXPLAIN INSERT OVERWRITE TABLE rcfile_merge3b
    SELECT key, value FROM rcfile_merge3a;
INSERT OVERWRITE TABLE rcfile_merge3b
    SELECT key, value FROM rcfile_merge3a;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM rcfile_merge3a
) t;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM rcfile_merge3b
) t;

DROP TABLE rcfile_merge3a;
DROP TABLE rcfile_merge3b;
set hive.mapred.mode=nonstrict;
set hive.merge.rcfile.block.level=true;
set mapred.max.split.size=100;
set mapred.min.split.size=1;

DROP TABLE rcfile_merge3a;
DROP TABLE rcfile_merge3b;

CREATE TABLE rcfile_merge3a (key int, value string)
    PARTITIONED BY (ds string) STORED AS RCFILE;
CREATE TABLE rcfile_merge3b (key int, value string) STORED AS TEXTFILE;

INSERT OVERWRITE TABLE rcfile_merge3a PARTITION (ds='1')
    SELECT * FROM src;
INSERT OVERWRITE TABLE rcfile_merge3a PARTITION (ds='2')
    SELECT * FROM src;

EXPLAIN INSERT OVERWRITE TABLE rcfile_merge3b
    SELECT key, value FROM rcfile_merge3a;
INSERT OVERWRITE TABLE rcfile_merge3b
    SELECT key, value FROM rcfile_merge3a;

SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM rcfile_merge3a
) t;
SELECT SUM(HASH(c)) FROM (
    SELECT TRANSFORM(key, value) USING 'tr \t _' AS (c)
    FROM rcfile_merge3b
) t;

DROP TABLE rcfile_merge3a;
DROP TABLE rcfile_merge3b;
set hive.mapred.mode=nonstrict;
CREATE TABLE src1_rc(key STRING, value STRING) STORED AS RCFILE;
INSERT OVERWRITE TABLE src1_rc SELECT * FROM src1;
SELECT * FROM src1_rc;


CREATE TABLE dest1_rc(c1 INT, c2 STRING, c3 INT, c4 STRING) STORED AS RCFILE;

EXPLAIN
FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 RIGHT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1_rc SELECT c.c1, c.c2, c.c3, c.c4;

FROM (
 FROM
  (
  FROM src src1 SELECT src1.key AS c1, src1.value AS c2 WHERE src1.key > 10 and src1.key < 20
  ) a
 RIGHT OUTER JOIN
 (
  FROM src src2 SELECT src2.key AS c3, src2.value AS c4 WHERE src2.key > 15 and src2.key < 25
 ) b
 ON (a.c1 = b.c3)
 SELECT a.c1 AS c1, a.c2 AS c2, b.c3 AS c3, b.c4 AS c4
) c
INSERT OVERWRITE TABLE dest1_rc SELECT c.c1, c.c2, c.c3, c.c4;

SELECT dest1_rc.* FROM dest1_rc;



set hive.mapred.mode=nonstrict;
CREATE TABLE test_src(key int, value string) stored as RCFILE;
set hive.io.rcfile.record.interval=5;
set hive.io.rcfile.record.buffer.size=100;
set hive.exec.compress.output=true;
INSERT OVERWRITE table test_src SELECT * FROM src;

set hive.io.rcfile.tolerate.corruptions=true;
SELECT key, value FROM test_src order by key;
set hive.mapred.mode=nonstrict;

CREATE table rcfile_unionTable (b STRING, c STRING)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
STORED AS RCFILE;

FROM src
INSERT OVERWRITE TABLE rcfile_unionTable SELECT src.key, src.value LIMIT 10;

SELECT * FROM (
SELECT b AS cola FROM rcfile_unionTable
UNION ALL
SELECT c AS cola FROM rcfile_unionTable) s ORDER BY cola ASC;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

CREATE TABLE fact_daily(x int) PARTITIONED BY (ds STRING);
CREATE TABLE fact_tz(x int) PARTITIONED BY (ds STRING, hr STRING)
LOCATION 'pfile:${system:test.tmp.dir}/fact_tz';

INSERT OVERWRITE TABLE fact_tz PARTITION (ds='1', hr='1')
SELECT key+11 FROM src WHERE key=484;

ALTER TABLE fact_daily SET TBLPROPERTIES('EXTERNAL'='TRUE');
ALTER TABLE fact_daily ADD PARTITION (ds='1')
LOCATION 'pfile:${system:test.tmp.dir}/fact_tz/ds=1';

set mapred.input.dir.recursive=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

SELECT * FROM fact_daily WHERE ds='1';

SELECT count(1) FROM fact_daily WHERE ds='1';
-- Can't have recursive views

drop table t;
drop view r0;
drop view r1;
drop view r2;
drop view r3;
create table t (id int);
create view r0 as select * from t;
create view r1 as select * from r0;
create view r2 as select * from r1;
create view r3 as select * from r2;
drop view r0;
alter view r3 rename to r0;
select * from r0;select p_name
from (select p_name from part distribute by 1 sort by 1) p
distribute by 1 sort by 1
;set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
;
set hive.exec.reducers.max = 1;
set hive.exec.script.trust = true;
set hive.optimize.reducededuplication = true;
set hive.optimize.reducededuplication.min.reducer = 1;


CREATE TABLE bucket5_1(key string, value string) CLUSTERED BY (key) INTO 2 BUCKETS;
explain extended
insert overwrite table bucket5_1
select * from src cluster by key;

insert overwrite table bucket5_1
select * from src cluster by key;

select sum(hash(key)),sum(hash(value)) from bucket5_1;
select sum(hash(key)),sum(hash(value)) from src;


create table complex_tbl_1(aid string, bid string, t int, ctime string, etime bigint, l string, et string) partitioned by (ds string);


create table complex_tbl_2(aet string, aes string) partitioned by (ds string);

explain extended
insert overwrite table complex_tbl_1 partition (ds='2010-03-29')
select s2.* from
(
 select TRANSFORM (aid,bid,t,ctime,etime,l,et)
 USING 'cat'
 AS (aid string, bid string, t int, ctime string, etime bigint, l string, et string)
 from
  (
   select transform(aet,aes)
   using 'cat'
   as (aid string, bid string, t int, ctime string, etime bigint, l string, et string)
   from complex_tbl_2 where ds ='2010-03-29' cluster by bid
)s
)s2;





create table t1( key_int1 int, key_int2 int, key_string1 string, key_string2 string);

set hive.optimize.reducededuplication=false;

set hive.map.aggr=false;
select Q1.key_int1, sum(Q1.key_int1) from (select * from t1 cluster by key_int1) Q1 group by Q1.key_int1;

drop table t1;
set hive.optimize.reducededuplication=true;
set hive.auto.convert.join=true;
explain select * from (select * from src cluster by key) a join src b on a.key = b.key limit 1;
set hive.mapred.mode=nonstrict;
set hive.optimize.reducededuplication=true;
set hive.optimize.reducededuplication.min.reducer=1;
set hive.map.aggr=true;

-- HIVE-2340 deduplicate RS followed by RS
-- hive.optimize.reducededuplication : wherther using this optimization
-- hive.optimize.reducededuplication.min.reducer : number of reducer of deduped RS should be this at least

-- RS-mGBY-RS-rGBY
explain select key, sum(key) from (select * from src distribute by key sort by key, value) Q1 group by key;
explain select key, sum(key), lower(value) from (select * from src order by key) Q1 group by key, lower(value);
explain select key, sum(key), (X + 1) from (select key, (value + 1) as X from src order by key) Q1 group by key, (X + 1);
-- mGBY-RS-rGBY-RS
explain select key, sum(key) as value from src group by key order by key;
-- RS-JOIN-mGBY-RS-rGBY
explain select src.key, sum(src.key) FROM src JOIN src1 ON src.key = src1.key group by src.key, src.value;
-- RS-JOIN-RS
explain select src.key, src.value FROM src JOIN src1 ON src.key = src1.key order by src.key, src.value;
-- mGBY-RS-rGBY-mGBY-RS-rGBY
explain from (select key, value from src group by key, value) s select s.key group by s.key;
explain select key, count(distinct value) from (select key, value from src group by key, value) t group by key;

select key, sum(key) from (select * from src distribute by key sort by key, value) Q1 group by key;
select key, sum(key), lower(value) from (select * from src order by key) Q1 group by key, lower(value);
select key, sum(key), (X + 1) from (select key, (value + 1) as X from src order by key) Q1 group by key, (X + 1);
select key, sum(key) as value from src group by key order by key;
select src.key, sum(src.key) FROM src JOIN src1 ON src.key = src1.key group by src.key, src.value;
select src.key, src.value FROM src JOIN src1 ON src.key = src1.key order by src.key, src.value;
from (select key, value from src group by key, value) s select s.key group by s.key;
select key, count(distinct value) from (select key, value from src group by key, value) t group by key;

set hive.map.aggr=false;

-- RS-RS-GBY
explain select key, sum(key) from (select * from src distribute by key sort by key, value) Q1 group by key;
explain select key, sum(key), lower(value) from (select * from src order by key) Q1 group by key, lower(value);
explain select key, sum(key), (X + 1) from (select key, (value + 1) as X from src order by key) Q1 group by key, (X + 1);
-- RS-GBY-RS
explain select key, sum(key) as value from src group by key order by key;
-- RS-JOIN-RS-GBY
explain select src.key, sum(src.key) FROM src JOIN src1 ON src.key = src1.key group by src.key, src.value;
-- RS-JOIN-RS
explain select src.key, src.value FROM src JOIN src1 ON src.key = src1.key order by src.key, src.value;
-- RS-GBY-RS-GBY
explain from (select key, value from src group by key, value) s select s.key group by s.key;
explain select key, count(distinct value) from (select key, value from src group by key, value) t group by key;

select key, sum(key) from (select * from src distribute by key sort by key, value) Q1 group by key;
select key, sum(key), lower(value) from (select * from src order by key) Q1 group by key, lower(value);
select key, sum(key), (X + 1) from (select key, (value + 1) as X from src order by key) Q1 group by key, (X + 1);
select key, sum(key) as value from src group by key order by key;
select src.key, sum(src.key) FROM src JOIN src1 ON src.key = src1.key group by src.key, src.value;
select src.key, src.value FROM src JOIN src1 ON src.key = src1.key order by src.key, src.value;
from (select key, value from src group by key, value) s select s.key group by s.key;
select key, count(distinct value) from (select key, value from src group by key, value) t group by key;
EXPLAIN EXTENDED
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value, 1+2, 3+4)
         USING 'cat'
  CLUSTER BY key
) tmap
SELECT tmap.key, regexp_extract(tmap.value, 'val_(\\d+\\t\\d+)',1) WHERE tmap.key < 100;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value, 1+2, 3+4)
         USING 'cat'
  CLUSTER BY key
) tmap
SELECT tmap.key, regexp_extract(tmap.value, 'val_(\\d+\\t\\d+)',1) WHERE tmap.key < 100;

EXPLAIN EXTENDED
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value, 1+2, 3+4)
         USING 'cat'
  CLUSTER BY key
) tmap
SELECT tmap.key, regexp_extract(tmap.value, 'val_(\\d+\\t\\d+)') WHERE tmap.key < 100;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value, 1+2, 3+4)
         USING 'cat'
  CLUSTER BY key
) tmap
SELECT tmap.key, regexp_extract(tmap.value, 'val_(\\d+\\t\\d+)') WHERE tmap.key < 100;
set hive.mapred.mode=nonstrict;
set hive.support.quoted.identifiers=none;

EXPLAIN
SELECT * FROM srcpart;

EXPLAIN
SELECT `..` FROM srcpart;

EXPLAIN
SELECT srcpart.`..` FROM srcpart;

EXPLAIN
SELECT `..` FROM srcpart a JOIN srcpart b
ON a.key = b.key AND a.value = b.value;

EXPLAIN
SELECT b.`..` FROM srcpart a JOIN srcpart b
ON a.key = b.key AND a.hr = b.hr AND a.ds = b.ds AND a.key = 103
ORDER BY ds, hr;

SELECT b.`..` FROM srcpart a JOIN srcpart b
ON a.key = b.key AND a.hr = b.hr AND a.ds = b.ds AND a.key = 103
ORDER BY ds, hr;

EXPLAIN
SELECT `.e.` FROM srcpart;

EXPLAIN
SELECT `d.*` FROM srcpart;

EXPLAIN
SELECT `(ds)?+.+` FROM srcpart;

EXPLAIN
SELECT `(ds|hr)?+.+` FROM srcpart ORDER BY key, value LIMIT 10;

SELECT `(ds|hr)?+.+` FROM srcpart ORDER BY key, value LIMIT 10;
set hive.support.quoted.identifiers=none;
EXPLAIN
SELECT `+++` FROM srcpart;
set hive.support.quoted.identifiers=none;
EXPLAIN
SELECT `.a.` FROM srcpart;
set hive.support.quoted.identifiers=none;
EXPLAIN
SELECT `..`, count(1) FROM srcpart GROUP BY `..`;
set hive.mapred.mode=nonstrict;
dfs -put ../../data/scripts/newline.py /newline.py;
add file hdfs:///newline.py;
set hive.transform.escape.input=true;

create table tmp_tmp(key string, value string) stored as rcfile;
insert overwrite table tmp_tmp
SELECT TRANSFORM(key, value) USING
'python newline.py' AS key, value FROM src limit 6;

select * from tmp_tmp ORDER BY key ASC, value ASC;

dfs -rmr /newline.py;
drop table tmp_tmp;
CREATE TABLE kv_rename_test(a int, b int, c int);
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a a STRING;
DESCRIBE kv_rename_test;
set hive.metastore.disallow.incompatible.col.type.changes=false;
ALTER TABLE kv_rename_test CHANGE a a1 INT;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a1 a2 INT FIRST;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a2 a INT AFTER b;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a a1 INT COMMENT 'test comment1';
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a1 a2 INT COMMENT 'test comment2' FIRST;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE COLUMN a2 a INT AFTER b;
DESCRIBE kv_rename_test;

DROP TABLE kv_rename_test;
SHOW TABLES;

-- Using non-default Database
CREATE DATABASE kv_rename_test_db;
USE kv_rename_test_db;

CREATE TABLE kv_rename_test(a int, b int, c int);
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a a STRING;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a a1 INT;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a1 a2 INT FIRST;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a2 a INT AFTER b;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a a1 INT COMMENT 'test comment1';
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE a1 a2 INT COMMENT 'test comment2' FIRST;
DESCRIBE kv_rename_test;

ALTER TABLE kv_rename_test CHANGE COLUMN a2 a INT AFTER b;
DESCRIBE kv_rename_test;
reset hive.metastore.disallow.incompatible.col.type.changes;
DROP TABLE kv_rename_test;
SHOW TABLES;
set hive.mapred.mode=nonstrict;

dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/ex_table;

CREATE EXTERNAL TABLE ex_table ( key INT, value STRING)
    PARTITIONED BY (part STRING)
    STORED AS textfile
	LOCATION 'file:${system:test.tmp.dir}/ex_table';

INSERT OVERWRITE TABLE ex_table PARTITION (part='part1')
SELECT key, value FROM src WHERE key < 10;

SHOW PARTITIONS ex_table;
SELECT * from ex_table where part='part1' ORDER BY key;

dfs -ls ${system:test.tmp.dir}/ex_table/part=part1;
dfs -cat ${system:test.tmp.dir}/ex_table/part=part1/000000_0;

ALTER TABLE ex_table PARTITION (part='part1') RENAME TO PARTITION (part='part2');

SHOW PARTITIONS ex_table;
SELECT * from ex_table where part='part2' ORDER BY key;

dfs -ls ${system:test.tmp.dir}/ex_table/part=part1;
dfs -cat ${system:test.tmp.dir}/ex_table/part=part1/000000_0;
-- This test verifies that if the tables location changes, renaming a partition will not change
-- the partition location accordingly

CREATE TABLE rename_partition_table (key STRING, value STRING) PARTITIONED BY (part STRING)
STORED AS RCFILE
LOCATION 'pfile:${system:test.tmp.dir}/rename_partition_table';

INSERT OVERWRITE TABLE rename_partition_table PARTITION (part = '1') SELECT * FROM src;

ALTER TABLE rename_partition_table SET LOCATION 'file:${system:test.tmp.dir}/rename_partition_table';

ALTER TABLE rename_partition_table PARTITION (part = '1') RENAME TO PARTITION (part = '2');

SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyPartitionIsNotSubdirectoryOfTableHook;

SELECT count(*) FROM rename_partition_table where part = '2';

SET hive.exec.post.hooks=;

DROP TABLE rename_partition_table;
-- This test verifies that if the tables location changes, renaming a table will not change
-- the table location scheme

CREATE TABLE rename_partition_table (key STRING, value STRING) PARTITIONED BY (part STRING)
STORED AS RCFILE
LOCATION 'pfile:${system:test.tmp.dir}/rename_partition_table';

INSERT OVERWRITE TABLE rename_partition_table PARTITION (part = '1') SELECT * FROM src;

ALTER TABLE rename_partition_table SET LOCATION 'file:${system:test.tmp.dir}/rename_partition_table';

set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.VerifyOutputTableLocationSchemeIsFileHook;

-- If the metastore attempts to change the scheme of the table back to the default pfile, it will get
-- an exception related to the source and destination file systems not matching

ALTER TABLE rename_partition_table RENAME TO rename_partition_table_renamed;
DROP TABLE IF EXISTS repairtable;

CREATE TABLE repairtable(col STRING) PARTITIONED BY (p1 STRING, p2 STRING);

MSCK TABLE repairtable;

dfs ${system:test.dfs.mkdir} ${system:test.warehouse.dir}/repairtable/p1=a/p2=a;
dfs ${system:test.dfs.mkdir} ${system:test.warehouse.dir}/repairtable/p1=b/p2=a;
dfs -touchz ${system:test.warehouse.dir}/repairtable/p1=b/p2=a/datafile;

MSCK TABLE default.repairtable;

MSCK REPAIR TABLE default.repairtable;

MSCK TABLE repairtable;

DROP TABLE default.repairtable;
set hive.test.mode=true;
set hive.test.mode.prefix=;

create table repl_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile
	tblproperties("repl.last.id"="43");

load data local inpath "../../data/files/test.dat"
	into table repl_employee partition (emp_country="us", emp_state="ca");

show partitions repl_employee;
show table extended like repl_employee;

drop table repl_employee for replication('33');

-- drop 33 => table does not get dropped, but ca will be

show partitions repl_employee;
show table extended like repl_employee;

load data local inpath "../../data/files/test.dat"
	into table repl_employee partition (emp_country="us", emp_state="ak");

show partitions repl_employee;

drop table repl_employee for replication('');

-- drop '' => ptns would be dropped, but not tables

show partitions repl_employee;
show table extended like repl_employee;

drop table repl_employee for replication('49');

-- table and ptns should have been dropped, so next create can succeed

create table repl_employee ( emp_id int comment "employee id")
	comment "employee table"
	partitioned by (emp_country string comment "two char iso code", emp_state string comment "free text")
	stored as textfile;

-- created table without a repl.last.id

load data local inpath "../../data/files/test.dat"
	into table repl_employee partition (emp_country="us", emp_state="ca");
load data local inpath "../../data/files/test.dat"
	into table repl_employee partition (emp_country="us", emp_state="ak");
load data local inpath "../../data/files/test.dat"
	into table repl_employee partition (emp_country="us", emp_state="wa");

show partitions repl_employee;
show table extended like repl_employee;

alter table repl_employee drop partition (emp_country="us", emp_state="ca");
alter table repl_employee drop partition (emp_country="us", emp_state="wa") for replication('59');

-- should have dropped ca, wa

show partitions repl_employee;
show table extended like repl_employee;

alter table repl_employee set tblproperties ("repl.last.id" = "42");

alter table repl_employee drop partition (emp_country="us", emp_state="ak");

-- should have dropped ak

show partitions repl_employee;
show table extended like repl_employee;

drop table repl_employee;

-- should drop the whole table, and this can be verified by trying to create another table with the same name

create table repl_employee( a string);

show table extended like repl_employee;

drop table repl_employee;



set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=managed_t,ext_t,managed_t_imported,managed_t_r_imported,ext_t_imported,ext_t_r_imported;

drop table if exists managed_t;
drop table if exists ext_t;
drop table if exists managed_t_imported;
drop table if exists managed_t_r_imported;
drop table if exists ext_t_imported;
drop table if exists ext_t_r_imported;

create table managed_t (emp_id int comment "employee id")
        partitioned by (emp_country string, emp_state string)
        stored as textfile;
load data local inpath "../../data/files/test.dat"
        into table managed_t partition (emp_country="us",emp_state="ca");

create external table ext_t (emp_id int comment "employee id")
        partitioned by (emp_country string, emp_state string)
        stored as textfile
        tblproperties("EXTERNAL"="true");
load data local inpath "../../data/files/test.dat"
        into table ext_t partition (emp_country="us",emp_state="ca");

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/managed_t/temp;
dfs -rmr target/tmp/ql/test/data/exports/managed_t;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/managed_t_r/temp;
dfs -rmr target/tmp/ql/test/data/exports/managed_t_r;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/ext_t/temp;
dfs -rmr target/tmp/ql/test/data/exports/ext_t;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/ext_t_r/temp;
dfs -rmr target/tmp/ql/test/data/exports/ext_t_r;

-- verifying difference between normal export of a external table
-- and a replication export of an ext table
-- the replication export will have squashed the "EXTERNAL" flag
-- this is because the destination of all replication exports are
-- managed tables. The managed tables should be similar except
-- for the repl.last.id values

export table managed_t to 'ql/test/data/exports/managed_t';
export table managed_t to 'ql/test/data/exports/managed_t_r' for replication('managed_t_r');
export table ext_t to 'ql/test/data/exports/ext_t';
export table ext_t to 'ql/test/data/exports/ext_t_r' for replication('ext_t_r');

drop table ext_t;
drop table managed_t;

import table managed_t_imported from 'ql/test/data/exports/managed_t';
describe extended managed_t_imported;
show table extended like managed_t_imported;
show create table managed_t_imported;
select * from managed_t_imported;

-- should have repl.last.id
import table managed_t_r_imported from 'ql/test/data/exports/managed_t_r';
describe extended managed_t_r_imported;
show table extended like managed_t_r_imported;
show create table managed_t_r_imported;
select * from managed_t_r_imported;

import table ext_t_imported from 'ql/test/data/exports/ext_t';
describe extended ext_t_imported;
show table extended like ext_t_imported;
show create table ext_t_imported;
select * from ext_t_imported;

-- should have repl.last.id
-- also - importing an external table replication export would turn the new table into a managed table
import table ext_t_r_imported from 'ql/test/data/exports/ext_t_r';
describe extended ext_t_imported;
show table extended like ext_t_r_imported;
show create table ext_t_r_imported;
select * from ext_t_r_imported;

drop table managed_t_imported;
drop table managed_t_r_imported;
drop table ext_t_imported;
drop table ext_t_r_imported;
set hive.mapred.mode=nonstrict;
set hive.test.mode=true;
set hive.test.mode.prefix=;
set hive.test.mode.nosamplelist=replsrc,repldst,repldst_md;

drop table if exists replsrc;
drop table if exists repldst;
drop table if exists repldst_md;

create table replsrc (emp_id int comment "employee id")
        partitioned by (emp_country string, emp_state string)
        stored as textfile;
load data local inpath "../../data/files/test.dat"
        into table replsrc partition (emp_country="us",emp_state="ca");

dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/repldst/temp;
dfs -rmr target/tmp/ql/test/data/exports/repldst;
dfs ${system:test.dfs.mkdir} target/tmp/ql/test/data/exports/repldst_md/temp;
dfs -rmr target/tmp/ql/test/data/exports/repldst_md;

export table replsrc to 'ql/test/data/exports/repldst' for replication('repldst');
export table replsrc to 'ql/test/data/exports/repldst_md' for metadata replication('repldst md-only');

drop table replsrc;

import table repldst from 'ql/test/data/exports/repldst';
describe extended repldst;
show table extended like repldst;
show create table repldst;
select * from repldst;

-- should be similar, except that select will return no results
import table repldst_md from 'ql/test/data/exports/repldst_md';
describe extended repldst_md;
show table extended like repldst_md;
show create table repldst_md;
select * from repldst_md;

drop table repldst;
drop table repldst_md;

set hive.skewjoin.key;
set hive.skewjoin.mapjoin.min.split;
set hive.skewjoin.key=300000;
set hive.skewjoin.mapjoin.min.split=256000000;
set hive.skewjoin.key;
set hive.skewjoin.mapjoin.min.split;

reset;

set hive.skewjoin.key;
set hive.skewjoin.mapjoin.min.split;
set hive.mapred.mode=nonstrict;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/test_root_dir_external_table;

insert overwrite directory "hdfs:///tmp/test_root_dir_external_table" select key from src where (key < 20) order by key;

dfs -cp /tmp/test_root_dir_external_table/000000_0 /000000_0;
dfs -rmr hdfs:///tmp/test_root_dir_external_table;

create external table roottable (key string) row format delimited fields terminated by '\\t' stored as textfile location 'hdfs:///';
select count(*) from roottable;

dfs -rmr /000000_0;set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;

-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
 FROM
  src a
 RIGHT OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  src a
 RIGHT OUTER JOIN
  srcpart b
 ON (a.key = b.key AND b.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

EXPLAIN EXTENDED
 FROM
  srcpart a
 RIGHT OUTER JOIN
  src b
 ON (a.key = b.key AND a.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;

 FROM
  srcpart a
 RIGHT OUTER JOIN
  src b
 ON (a.key = b.key AND a.ds = '2008-04-08')
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25;


EXPLAIN EXTENDED
 FROM
  src a
 RIGHT OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';

 FROM
  src a
 RIGHT OUTER JOIN
  srcpart b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND b.ds = '2008-04-08';

EXPLAIN EXTENDED
 FROM
  srcpart a
 RIGHT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND a.ds = '2008-04-08';

 FROM
  srcpart a
 RIGHT OUTER JOIN
  src b
 ON (a.key = b.key)
 SELECT a.key, a.value, b.key, b.value
 WHERE a.key > 10 AND a.key < 20 AND b.key > 15 AND b.key < 25 AND a.ds = '2008-04-08';

set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin = true;
set hive.skewjoin.key = 4;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=50;

-- This is mainly intended for spark, to test runtime skew join together with map join

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

EXPLAIN
SELECT COUNT(*) FROM
  (SELECT src1.key,src1.value FROM src src1 JOIN src src2 ON src1.key=src2.key) a
JOIN
  (SELECT src.key,src.value FROM src JOIN T1 ON src.key=T1.key) b
ON a.key=b.key;

SELECT COUNT(*) FROM
  (SELECT src1.key,src1.value FROM src src1 JOIN src src2 ON src1.key=src2.key) a
JOIN
  (SELECT src.key,src.value FROM src JOIN T1 ON src.key=T1.key) b
ON a.key=b.key;
explain extended SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 5 OUT OF 4 on key) s-- no input pruning, no sample filter
SELECT s.*
FROM srcpart TABLESAMPLE (BUCKET 1 OUT OF 1 ON rand()) s
WHERE s.ds='2008-04-08' and s.hr='11'

CREATE TABLE dest1(key INT, value STRING, dt STRING, hr STRING) STORED AS TEXTFILE;

-- no input pruning, no sample filter
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcpart TABLESAMPLE (BUCKET 1 OUT OF 1 ON rand()) s
WHERE s.ds='2008-04-08' and s.hr='11';

INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcpart TABLESAMPLE (BUCKET 1 OUT OF 1 ON rand()) s
WHERE s.ds='2008-04-08' and s.hr='11';

SELECT dest1.* FROM dest1;

select count(1) from srcbucket;
set hive.mapred.mode=nonstrict;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

set hive.exec.reducers.max=4;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set hive.default.fileformat=RCFILE;
set hive.exec.pre.hooks = org.apache.hadoop.hive.ql.hooks.PreExecutePrinter,org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables,org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook$PreExec;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.17, 0.18, 0.19)

create table srcpartbucket (key string, value string) partitioned by (ds string, hr string) clustered by (key) into 4 buckets;

insert overwrite table srcpartbucket partition(ds, hr) select * from srcpart where ds is not null and key < 10;

explain extended
select ds, count(1) from srcpartbucket tablesample (bucket 1 out of 4 on key) where ds is not null group by ds ORDER BY ds ASC;

select ds, count(1) from srcpartbucket tablesample (bucket 1 out of 4 on key) where ds is not null group by ds ORDER BY ds ASC;

select ds, count(1) from srcpartbucket tablesample (bucket 1 out of 2 on key) where ds is not null group by ds ORDER BY ds ASC;

select * from srcpartbucket where ds is not null ORDER BY key ASC, value ASC, ds ASC, hr ASC;


-- input pruning, no sample filter
-- default table sample columns
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2) s
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

-- input pruning, no sample filter
-- default table sample columns
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2) s;

INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2) s;

SELECT dest1.* FROM dest1;
-- sample columns not same as bucket columns
-- no input pruning, sample filter
INSERT OVERWRITE TABLE dest1 SELECT s.* -- here's another test
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key, value) s
-- SORT_QUERY_RESULTS

-- no input pruning, sample filter
EXPLAIN
SELECT s.key
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 on key) s;

SELECT s.key
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 on key) s SORT BY key;

-- bucket column is the same as table sample
-- No need for sample filter
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

-- bucket column is the same as table sample
-- No need for sample filter
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s;

INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s;

SELECT dest1.* FROM dest1;
-- no input pruning, sample filter
INSERT OVERWRITE TABLE dest1 SELECT s.* -- here's another test
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 on key) s
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

-- SORT_QUERY_RESULTS

-- no input pruning, sample filter
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest1 SELECT s.* -- here's another test
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 on key) s;

INSERT OVERWRITE TABLE dest1 SELECT s.* -- here's another test
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 on key) s;

SELECT dest1.* FROM dest1 SORT BY key, value;
-- both input pruning and sample filter
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 4 on key) s
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

-- both input pruning and sample filter
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 4 on key) s;

INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 4 on key) s;

SELECT dest1.* FROM dest1;

EXPLAIN EXTENDED SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 4 OUT OF 4 on key) s
ORDER BY key, value;
SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 4 OUT OF 4 on key) s
ORDER BY key, value;

EXPLAIN EXTENDED SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
ORDER BY key, value;
SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
ORDER BY key, value;

EXPLAIN EXTENDED SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 3 on key) s
ORDER BY key, value;
SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 3 on key) s
ORDER BY key, value;

EXPLAIN EXTENDED SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 2 OUT OF 3 on key) s
ORDER BY key, value;
SELECT s.* FROM srcbucket TABLESAMPLE (BUCKET 2 OUT OF 3 on key) s
ORDER BY key, value;

EXPLAIN EXTENDED SELECT s.* FROM srcbucket2 TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
ORDER BY key, value;
SELECT s.* FROM srcbucket2 TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
ORDER BY key, value;

EXPLAIN EXTENDED SELECT s.* FROM srcbucket2 TABLESAMPLE (BUCKET 2 OUT OF 4 on key) s
ORDER BY key, value;
SELECT s.* FROM srcbucket2 TABLESAMPLE (BUCKET 2 OUT OF 4 on key) s
ORDER BY key, value;

CREATE TABLE empty_bucket (key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
EXPLAIN EXTENDED SELECT s.* FROM empty_bucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
ORDER BY key, value;
SELECT s.* FROM empty_bucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) s
ORDER BY key, value;



-- both input pruning and sample filter
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 4 on key) s
WHERE s.key > 100
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

-- both input pruning and sample filter
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 4 on key) s
WHERE s.key > 100;

INSERT OVERWRITE TABLE dest1 SELECT s.*
FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 4 on key) s
WHERE s.key > 100;

SELECT dest1.* FROM dest1;
set hive.mapred.mode=nonstrict;
-- sampling with join and alias
-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
SELECT s.*
FROM srcpart TABLESAMPLE (BUCKET 1 OUT OF 1 ON key) s
JOIN srcpart TABLESAMPLE (BUCKET 1 OUT OF 10 ON key) t
WHERE t.key = s.key and t.value = s.value and s.ds='2008-04-08' and s.hr='11';

SELECT s.key, s.value
FROM srcpart TABLESAMPLE (BUCKET 1 OUT OF 1 ON key) s
JOIN srcpart TABLESAMPLE (BUCKET 1 OUT OF 10 ON key) t
WHERE t.key = s.key and t.value = s.value and s.ds='2008-04-08' and s.hr='11';

EXPLAIN
SELECT * FROM src TABLESAMPLE(100 ROWS) a JOIN src1 TABLESAMPLE(10 ROWS) b ON a.key=b.key;

SELECT * FROM src TABLESAMPLE(100 ROWS) a JOIN src1 TABLESAMPLE(10 ROWS) b ON a.key=b.key;

EXPLAIN
SELECT * FROM src TABLESAMPLE(100 ROWS) a, src1 TABLESAMPLE(10 ROWS) b WHERE a.key=b.key;

SELECT * FROM src TABLESAMPLE(100 ROWS) a, src1 TABLESAMPLE(10 ROWS) b WHERE a.key=b.key;EXPLAIN EXTENDED
SELECT s.*
FROM (SELECT a.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) a) s;

SELECT s.*
FROM (SELECT a.* FROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 2 on key) a) s;
set hive.mapred.mode=nonstrict;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapred.max.split.size=300;
set mapred.min.split.size=300;
set mapred.min.split.size.per.node=300;
set mapred.min.split.size.per.rack=300;
set hive.exec.mode.local.auto=true;
set hive.merge.smallfiles.avgsize=1;

-- EXCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)

-- create file inputs
create table sih_i_part (key int, value string) partitioned by (p string);
insert overwrite table sih_i_part partition (p='1') select key, value from src;
insert overwrite table sih_i_part partition (p='2') select key+10000, value from src;
insert overwrite table sih_i_part partition (p='3') select key+20000, value from src;
create table sih_src as select key, value from sih_i_part order by key, value;
create table sih_src2 as select key, value from sih_src order by key, value;

set hive.exec.post.hooks = org.apache.hadoop.hive.ql.hooks.VerifyIsLocalModeHook;
set mapreduce.framework.name=yarn;
set mapreduce.jobtracker.address=localhost:58;
set hive.sample.seednumber=7;

-- Relaxing hive.exec.mode.local.auto.input.files.max=1.
-- Hadoop20 will not generate more splits than there are files (one).
-- Hadoop23 generate splits correctly (four), hence the max needs to be adjusted to ensure running in local mode.
-- Default value is hive.exec.mode.local.auto.input.files.max=4 which produces expected behavior on Hadoop23.
-- hive.sample.seednumber is required because Hadoop23 generates multiple splits and tablesample is non-repeatable without it.

-- sample split, running locally limited by num tasks
select count(1) from sih_src tablesample(1 percent);

-- sample two tables
select count(1) from sih_src tablesample(1 percent) a join sih_src2 tablesample(1 percent) b on a.key = b.key;

set hive.exec.mode.local.auto.inputbytes.max=1000;

-- sample split, running locally limited by max bytes
select count(1) from sih_src tablesample(1 percent);
USE default;

set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
set mapred.max.split.size=300;
set mapred.min.split.size=300;
set mapred.min.split.size.per.node=300;
set mapred.min.split.size.per.rack=300;
set hive.exec.mode.local.auto=true;
set hive.merge.smallfiles.avgsize=1;

-- INCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)
-- This test sets mapred.max.split.size=300 and hive.merge.smallfiles.avgsize=1
-- in an attempt to force the generation of multiple splits and multiple output files.
-- However, Hadoop 0.20 is incapable of generating splits smaller than the block size
-- when using CombineFileInputFormat, so only one split is generated. This has a
-- significant impact on the results of the TABLESAMPLE(x PERCENT). This issue was
-- fixed in MAPREDUCE-2046 which is included in 0.22.

-- create file inputs
create table sih_i_part (key int, value string) partitioned by (p string);
insert overwrite table sih_i_part partition (p='1') select key, value from src;
insert overwrite table sih_i_part partition (p='2') select key+10000, value from src;
insert overwrite table sih_i_part partition (p='3') select key+20000, value from src;
create table sih_src as select key, value from sih_i_part order by key, value;
create table sih_src2 as select key, value from sih_src order by key, value;

set hive.exec.post.hooks = org.apache.hadoop.hive.ql.hooks.VerifyIsLocalModeHook ;
set mapred.job.tracker=localhost:58;
set hive.exec.mode.local.auto.input.files.max=1;

-- Sample split, running locally limited by num tasks
select count(1) from sih_src tablesample(1 percent);

-- sample two tables
select count(1) from sih_src tablesample(1 percent)a join sih_src2 tablesample(1 percent)b on a.key = b.key;

set hive.exec.mode.local.auto.inputbytes.max=1000;
set hive.exec.mode.local.auto.input.files.max=4;

-- sample split, running locally limited by max bytes
select count(1) from sih_src tablesample(1 percent);
set hive.mapred.mode=nonstrict;
set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.exec.schema.evolution=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, ACID Vectorized, MapWork, Partitioned
-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned1 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned1 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned2(a smallint, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned2 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned2 change column a a int;

insert into table partitioned2 partition(part=2) values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table partitioned2 partition(part=1) values(5000, 'new'),(90000, 'new');

select part,a,b from partitioned2;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DYNAMIC INSERT
---
CREATE TABLE partitioned3(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned3 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned3 add columns(c int, d string);

insert into table partitioned3 partition(part) values(1, 'new', 10, 'ten', 2),(2, 'new', 20, 'twenty', 2), (3, 'new', 30, 'thirty', 2),(4, 'new', 40, 'forty', 2),
    (5, 'new', 100, 'hundred', 1),(6, 'new', 200, 'two hundred', 1);

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... DYNAMIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned4(a smallint, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned4 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned4 change column a a int;

insert into table partitioned4 partition(part) values(72909, 'new', 2),(200, 'new', 2), (32768, 'new', 2),(40000, 'new', 2),
    (5000, 'new', 1),(90000, 'new', 1);

select part,a,b from partitioned4;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... UPDATE New Columns
---
CREATE TABLE partitioned5(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned5 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned5 add columns(c int, d string);

insert into table partitioned5 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned5 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select part,a,b,c,d from partitioned5;

-- UPDATE New Columns
update partitioned5 set c=99;

select part,a,b,c,d from partitioned5;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DELETE where old column
---
CREATE TABLE partitioned6(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned6 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned6 add columns(c int, d string);

insert into table partitioned6 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned6 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select part,a,b,c,d from partitioned6;

-- DELETE where old column
delete from partitioned6 where a = 2 or a = 4 or a = 6;

select part,a,b,c,d from partitioned6;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DELETE where new column
---
CREATE TABLE partitioned7(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned7 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned7 add columns(c int, d string);

insert into table partitioned7 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned7 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select part,a,b,c,d from partitioned7;

-- DELETE where new column
delete from partitioned7 where a = 1 or c = 30 or c == 100;

select part,a,b,c,d from partitioned7;


DROP TABLE partitioned1;
DROP TABLE partitioned2;
DROP TABLE partitioned3;
DROP TABLE partitioned4;
DROP TABLE partitioned5;
DROP TABLE partitioned6;
DROP TABLE partitioned7;set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.exec.schema.evolution=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, ACID Vectorized, MapWork, Table
-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE table1(a INT, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table1 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select a,b from table1;
select a,b,c from table1;
select a,b,c,d from table1;
select a,c,d from table1;
select a,d from table1;
select c from table1;
select d from table1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table2(a smallint, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table2 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table table2 change column a a int;

insert into table table2 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table table2 values(5000, 'new'),(90000, 'new');

select a,b from table2;



--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... UPDATE New Columns
---
CREATE TABLE table5(a INT, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table5 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table5 add columns(c int, d string);

insert into table table5 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table5 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select a,b,c,d from table5;

-- UPDATE New Columns
update table5 set c=99;

select a,b,c,d from table5;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DELETE where old column
---
CREATE TABLE table6(a INT, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table6 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table6 add columns(c int, d string);

insert into table table6 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table6 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select a,b,c,d from table6;

-- DELETE where old column
delete from table6 where a = 2 or a = 4 or a = 6;

select a,b,c,d from table6;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DELETE where new column
---
CREATE TABLE table7(a INT, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table7 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table7 add columns(c int, d string);

insert into table table7 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table7 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select a,b,c,d from table7;

-- DELETE where new column
delete from table7 where a = 1 or c = 30 or c == 100;

select a,b,c,d from table7;


DROP TABLE table1;
DROP TABLE table2;
DROP TABLE table5;
DROP TABLE table6;
DROP TABLE table7;set hive.mapred.mode=nonstrict;
set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.exec.schema.evolution=false;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Partitioned
-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned1 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned1 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned2(a smallint, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned2 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned2 change column a a int;

insert into table partitioned2 partition(part=2) values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table partitioned2 partition(part=1) values(5000, 'new'),(90000, 'new');

select part,a,b from partitioned2;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DYNAMIC INSERT
---
CREATE TABLE partitioned3(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned3 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned3 add columns(c int, d string);

insert into table partitioned3 partition(part) values(1, 'new', 10, 'ten', 2),(2, 'new', 20, 'twenty', 2), (3, 'new', 30, 'thirty', 2),(4, 'new', 40, 'forty', 2),
    (5, 'new', 100, 'hundred', 1),(6, 'new', 200, 'two hundred', 1);

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... DYNAMIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned4(a smallint, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned4 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned4 change column a a int;

insert into table partitioned4 partition(part) values(72909, 'new', 2),(200, 'new', 2), (32768, 'new', 2),(40000, 'new', 2),
    (5000, 'new', 1),(90000, 'new', 1);

select part,a,b from partitioned4;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... UPDATE New Columns
---
CREATE TABLE partitioned5(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned5 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned5 add columns(c int, d string);

insert into table partitioned5 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned5 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select part,a,b,c,d from partitioned5;

-- UPDATE New Columns
update partitioned5 set c=99;

select part,a,b,c,d from partitioned5;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DELETE where old column
---
CREATE TABLE partitioned6(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned6 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned6 add columns(c int, d string);

insert into table partitioned6 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned6 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select part,a,b,c,d from partitioned6;

-- DELETE where old column
delete from partitioned6 where a = 2 or a = 4 or a = 6;

select part,a,b,c,d from partitioned6;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DELETE where new column
---
CREATE TABLE partitioned7(a INT, b STRING) PARTITIONED BY(part INT) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table partitioned7 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned7 add columns(c int, d string);

insert into table partitioned7 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned7 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select part,a,b,c,d from partitioned7;

-- DELETE where new column
delete from partitioned7 where a = 1 or c = 30 or c == 100;

select part,a,b,c,d from partitioned7;


DROP TABLE partitioned1;
DROP TABLE partitioned2;
DROP TABLE partitioned3;
DROP TABLE partitioned4;
DROP TABLE partitioned5;
DROP TABLE partitioned6;
DROP TABLE partitioned7;set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.exec.schema.evolution=false;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, ACID Non-Vectorized, MapWork, Table
-- *IMPORTANT NOTE* We set hive.exec.schema.evolution=false above since schema evolution is always used for ACID.
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE table1(a INT, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table1 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select a,b from table1;
select a,b,c from table1;
select a,b,c,d from table1;
select a,c,d from table1;
select a,d from table1;
select c from table1;
select d from table1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table2(a smallint, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table2 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table table2 change column a a int;

insert into table table2 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table table2 values(5000, 'new'),(90000, 'new');

select a,b from table2;



--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... UPDATE New Columns
---
CREATE TABLE table5(a INT, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table5 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table5 add columns(c int, d string);

insert into table table5 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table5 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select a,b,c,d from table5;

-- UPDATE New Columns
update table5 set c=99;

select a,b,c,d from table5;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DELETE where old column
---
CREATE TABLE table6(a INT, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table6 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table6 add columns(c int, d string);

insert into table table6 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table6 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select a,b,c,d from table6;

-- DELETE where old column
delete from table6 where a = 2 or a = 4 or a = 6;

select a,b,c,d from table6;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DELETE where new column
---
CREATE TABLE table7(a INT, b STRING) clustered by (a) into 2 buckets STORED AS ORC TBLPROPERTIES ('transactional'='true');

insert into table table7 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table7 add columns(c int, d string);

insert into table table7 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table7 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

select a,b,c,d from table7;

-- DELETE where new column
delete from table7 where a = 1 or c = 30 or c == 100;

select a,b,c,d from table7;


DROP TABLE table1;
DROP TABLE table2;
DROP TABLE table5;
DROP TABLE table6;
DROP TABLE table7;set hive.mapred.mode=nonstrict;
set hive.cli.print.header=true;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=more;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, Non-Vectorized, FetchWork, Partitioned
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned1 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned1 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned2(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned2 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned2 change column a a int;

insert into table partitioned2 partition(part=2) values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table partitioned2 partition(part=1) values(5000, 'new'),(90000, 'new');

select part,a,b from partitioned2;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DYNAMIC INSERT
---
CREATE TABLE partitioned3(a INT, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned3 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned3 add columns(c int, d string);

insert into table partitioned3 partition(part) values(1, 'new', 10, 'ten', 2),(2, 'new', 20, 'twenty', 2), (3, 'new', 30, 'thirty', 2),(4, 'new', 40, 'forty', 2),
    (5, 'new', 100, 'hundred', 1),(6, 'new', 200, 'two hundred', 1);

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... DYNAMIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned4(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned4 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned4 change column a a int;

insert into table partitioned4 partition(part) values(72909, 'new', 2),(200, 'new', 2), (32768, 'new', 2),(40000, 'new', 2),
    (5000, 'new', 1),(90000, 'new', 1);

select part,a,b from partitioned4;


DROP TABLE partitioned1;
DROP TABLE partitioned2;
DROP TABLE partitioned3;
DROP TABLE partitioned4;set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=more;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, Non-Vectorized, FetchWork, Table
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE table1(a INT, b STRING) STORED AS ORC;

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table1 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select a,b from table1;
select a,b,c from table1;
select a,b,c,d from table1;
select a,c,d from table1;
select a,d from table1;
select c from table1;
select d from table1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table2(a smallint, b STRING) STORED AS ORC;

insert into table table2 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table table2 change column a a int;

insert into table table2 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table table2 values(5000, 'new'),(90000, 'new');

select a,b from table2;


DROP TABLE table1;
DROP TABLE table2;
set hive.mapred.mode=nonstrict;
set hive.cli.print.header=true;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, Non-Vectorized, FetchWork, Partitioned
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned1 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned1 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned2(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned2 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned2 change column a a int;

insert into table partitioned2 partition(part=2) values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table partitioned2 partition(part=1) values(5000, 'new'),(90000, 'new');

select part,a,b from partitioned2;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DYNAMIC INSERT
---
CREATE TABLE partitioned3(a INT, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned3 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned3 add columns(c int, d string);

insert into table partitioned3 partition(part) values(1, 'new', 10, 'ten', 2),(2, 'new', 20, 'twenty', 2), (3, 'new', 30, 'thirty', 2),(4, 'new', 40, 'forty', 2),
    (5, 'new', 100, 'hundred', 1),(6, 'new', 200, 'two hundred', 1);

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... DYNAMIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned4(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned4 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned4 change column a a int;

insert into table partitioned4 partition(part) values(72909, 'new', 2),(200, 'new', 2), (32768, 'new', 2),(40000, 'new', 2),
    (5000, 'new', 1),(90000, 'new', 1);

select part,a,b from partitioned4;


DROP TABLE partitioned1;
DROP TABLE partitioned2;
DROP TABLE partitioned3;
DROP TABLE partitioned4;set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, Non-Vectorized, MapWork, Table
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE table1(a INT, b STRING) STORED AS ORC;

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table1 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select a,b from table1;
select a,b,c from table1;
select a,b,c,d from table1;
select a,c,d from table1;
select a,d from table1;
select c from table1;
select d from table1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table2(a smallint, b STRING) STORED AS ORC;

insert into table table2 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table table2 change column a a int;

insert into table table2 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table table2 values(5000, 'new'),(90000, 'new');

select a,b from table2;


DROP TABLE table1;
DROP TABLE table2;set hive.mapred.mode=nonstrict;
set hive.cli.print.header=true;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=more;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, Vectorized, MapWork, Partitioned
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned1 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned1 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned2(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned2 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned2 change column a a int;

insert into table partitioned2 partition(part=2) values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table partitioned2 partition(part=1) values(5000, 'new'),(90000, 'new');

select part,a,b from partitioned2;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DYNAMIC INSERT
---
CREATE TABLE partitioned3(a INT, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned3 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned3 add columns(c int, d string);

insert into table partitioned3 partition(part) values(1, 'new', 10, 'ten', 2),(2, 'new', 20, 'twenty', 2), (3, 'new', 30, 'thirty', 2),(4, 'new', 40, 'forty', 2),
    (5, 'new', 100, 'hundred', 1),(6, 'new', 200, 'two hundred', 1);

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... DYNAMIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned4(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS ORC;

insert into table partitioned4 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned4 change column a a int;

insert into table partitioned4 partition(part) values(72909, 'new', 2),(200, 'new', 2), (32768, 'new', 2),(40000, 'new', 2),
    (5000, 'new', 1),(90000, 'new', 1);

select part,a,b from partitioned4;


DROP TABLE partitioned1;
DROP TABLE partitioned2;
DROP TABLE partitioned3;
DROP TABLE partitioned4;set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, Vectorized, MapWork, Table
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE table1(a INT, b STRING) STORED AS ORC;

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table1 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select a,b from table1;
select a,b,c from table1;
select a,b,c,d from table1;
select a,c,d from table1;
select a,d from table1;
select c from table1;
select d from table1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table2(a smallint, b STRING) STORED AS ORC;

insert into table table2 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table table2 change column a a int;

insert into table table2 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table table2 values(5000, 'new'),(90000, 'new');

select a,b from table2;


DROP TABLE table1;
DROP TABLE table2;set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: TEXT, Non-Vectorized, MapWork, Table
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE table1(a INT, b STRING) STORED AS TEXTFILE;

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table1 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select a,b from table1;
select a,b,c from table1;
select a,b,c,d from table1;
select a,c,d from table1;
select a,d from table1;
select c from table1;
select d from table1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table2(a smallint, b STRING) STORED AS TEXTFILE;

insert into table table2 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table table2 change column a a int;

insert into table table2 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table table2 values(5000, 'new'),(90000, 'new');

select a,b from table2;


DROP TABLE table1;
DROP TABLE table2;set hive.cli.print.header=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: TEXT, Non-Vectorized, MapWork, Table
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE table1(a INT, b STRING) STORED AS TEXTFILE;

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table table1 values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select a,b from table1;
select a,b,c from table1;
select a,b,c,d from table1;
select a,c,d from table1;
select a,d from table1;
select c from table1;
select d from table1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table2(a smallint, b STRING) STORED AS TEXTFILE;

insert into table table2 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table table2 change column a a int;

insert into table table2 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table table2 values(5000, 'new'),(90000, 'new');

select a,b from table2;


DROP TABLE table1;
DROP TABLE table2;set hive.mapred.mode=nonstrict;
set hive.cli.print.header=true;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=more;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: TEXT, Non-Vectorized, FetchWork, Partitioned
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned1 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned1 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned2(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned2 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned2 change column a a int;

insert into table partitioned2 partition(part=2) values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table partitioned2 partition(part=1) values(5000, 'new'),(90000, 'new');

select part,a,b from partitioned2;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DYNAMIC INSERT
---
CREATE TABLE partitioned3(a INT, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned3 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned3 add columns(c int, d string);

insert into table partitioned3 partition(part) values(1, 'new', 10, 'ten', 2),(2, 'new', 20, 'twenty', 2), (3, 'new', 30, 'thirty', 2),(4, 'new', 40, 'forty', 2),
    (5, 'new', 100, 'hundred', 1),(6, 'new', 200, 'two hundred', 1);

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... DYNAMIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned4(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned4 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned4 change column a a int;

insert into table partitioned4 partition(part) values(72909, 'new', 2),(200, 'new', 2), (32768, 'new', 2),(40000, 'new', 2),
    (5000, 'new', 1),(90000, 'new', 1);

select part,a,b from partitioned4;


DROP TABLE partitioned1;
DROP TABLE partitioned2;
DROP TABLE partitioned3;
DROP TABLE partitioned4;set hive.cli.print.header=true;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=more;

-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, Non-Vectorized, MapWork, Table
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS
---
CREATE TABLE table1(a INT, b STRING) STORED AS TEXTFILE;

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

select a,b from table1;

-- ADD COLUMNS
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

select a,b,c,d from table1;

-- ADD COLUMNS
alter table table1 add columns(e string);

insert into table table1 values(5, 'new', 100, 'hundred', 'another1'),(6, 'new', 200, 'two hundred', 'another2');

select a,b,c,d,e from table1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table3(a smallint, b STRING) STORED AS TEXTFILE;

insert into table table3 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

select a,b from table3;

-- ADD COLUMNS ... RESTRICT
alter table table3 change column a a int;

insert into table table3 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

select a,b from table3;

-- ADD COLUMNS ... RESTRICT
alter table table3 add columns(e string);

insert into table table3 values(5000, 'new', 'another5'),(90000, 'new', 'another6');

select a,b from table3;


-- ADD COLUMNS ... RESTRICT
alter table table3 change column a a int;

select a,b from table3;


DROP TABLE table1;
DROP TABLE table2;
DROP TABLE table3;set hive.mapred.mode=nonstrict;
set hive.cli.print.header=true;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=none;
set hive.exec.dynamic.partition.mode=nonstrict;


-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: TEXT, Non-Vectorized, MapWork, Partitioned
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... STATIC INSERT
---
CREATE TABLE partitioned1(a INT, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned1 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned1 add columns(c int, d string);

insert into table partitioned1 partition(part=2) values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

insert into table partitioned1 partition(part=1) values(5, 'new', 100, 'hundred'),(6, 'new', 200, 'two hundred');

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;

--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... STATIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned2(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned2 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned2 change column a a int;

insert into table partitioned2 partition(part=2) values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

insert into table partitioned2 partition(part=1) values(5000, 'new'),(90000, 'new');

select part,a,b from partitioned2;


--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS ... DYNAMIC INSERT
---
CREATE TABLE partitioned3(a INT, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned3 partition(part=1) values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

-- Table-Non-Cascade ADD COLUMNS ...
alter table partitioned3 add columns(c int, d string);

insert into table partitioned3 partition(part) values(1, 'new', 10, 'ten', 2),(2, 'new', 20, 'twenty', 2), (3, 'new', 30, 'thirty', 2),(4, 'new', 40, 'forty', 2),
    (5, 'new', 100, 'hundred', 1),(6, 'new', 200, 'two hundred', 1);

-- SELECT permutation columns to make sure NULL defaulting works right
select part,a,b from partitioned1;
select part,a,b,c from partitioned1;
select part,a,b,c,d from partitioned1;
select part,a,c,d from partitioned1;
select part,a,d from partitioned1;
select part,c from partitioned1;
select part,d from partitioned1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN ... DYNAMIC INSERT
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE partitioned4(a smallint, b STRING) PARTITIONED BY(part INT) STORED AS TEXTFILE;

insert into table partitioned4 partition(part=1) values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

-- Table-Non-Cascade CHANGE COLUMNS ...
alter table partitioned4 change column a a int;

insert into table partitioned4 partition(part) values(72909, 'new', 2),(200, 'new', 2), (32768, 'new', 2),(40000, 'new', 2),
    (5000, 'new', 1),(90000, 'new', 1);

select part,a,b from partitioned4;


DROP TABLE partitioned1;
DROP TABLE partitioned2;
DROP TABLE partitioned3;
DROP TABLE partitioned4;set hive.cli.print.header=true;
SET hive.exec.schema.evolution=true;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;

-- SORT_QUERY_RESULTS
--
-- FILE VARIATION: ORC, Non-Vectorized, MapWork, Table
--
--
-- SECTION VARIATION: ALTER TABLE ADD COLUMNS
---
CREATE TABLE table1(a INT, b STRING) STORED AS TEXTFILE;

insert into table table1 values(1, 'original'),(2, 'original'), (3, 'original'),(4, 'original');

select a,b from table1;

-- ADD COLUMNS
alter table table1 add columns(c int, d string);

insert into table table1 values(1, 'new', 10, 'ten'),(2, 'new', 20, 'twenty'), (3, 'new', 30, 'thirty'),(4, 'new', 40, 'forty');

select a,b,c,d from table1;

-- ADD COLUMNS
alter table table1 add columns(e string);

insert into table table1 values(5, 'new', 100, 'hundred', 'another1'),(6, 'new', 200, 'two hundred', 'another2');

select a,b,c,d,e from table1;


--
-- SECTION VARIATION: ALTER TABLE CHANGE COLUMN
-- smallint = (2-byte signed integer, from -32,768 to 32,767)
--
CREATE TABLE table3(a smallint, b STRING) STORED AS TEXTFILE;

insert into table table3 values(1000, 'original'),(6737, 'original'), ('3', 'original'),('4', 'original');

select a,b from table3;

-- ADD COLUMNS ... RESTRICT
alter table table3 change column a a int;

insert into table table3 values(72909, 'new'),(200, 'new'), (32768, 'new'),(40000, 'new');

select a,b from table3;

-- ADD COLUMNS ... RESTRICT
alter table table3 add columns(e string);

insert into table table3 values(5000, 'new', 'another5'),(90000, 'new', 'another6');

select a,b from table3;


-- ADD COLUMNS ... RESTRICT
alter table table3 change column a a int;

select a,b from table3;


DROP TABLE table1;
DROP TABLE table2;
DROP TABLE table3;set hive.mapred.mode=nonstrict;
dfs ${system:test.dfs.mkdir} file:///tmp/test;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/test;

create external table dynPart (key string) partitioned by (value string) row format delimited fields terminated by '\\t' stored as textfile;
insert overwrite local directory "/tmp/test" select key from src where (key = 10) order by key;
insert overwrite directory "/tmp/test" select key from src where (key = 20) order by key;
alter table dynPart add partition (value='0') location 'file:///tmp/test';
alter table dynPart add partition (value='1') location 'hdfs:///tmp/test';
select count(*) from dynPart;
select key from dynPart;
select key from src where (key = 10) order by key;
select key from src where (key = 20) order by key;

dfs -rmr file:///tmp/test;
dfs -rmr hdfs:///tmp/test;
set hive.mapred.mode=nonstrict;
dfs ${system:test.dfs.mkdir} file:///tmp/test;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/test;

create external table dynPart (key string) partitioned by (value string, value2 string) row format delimited fields terminated by '\\t' stored as textfile;
insert overwrite local directory "/tmp/test" select key from src where (key = 10) order by key;
insert overwrite directory "/tmp/test" select key from src where (key = 20) order by key;
alter table dynPart add partition (value='0', value2='clusterA') location 'file:///tmp/test';
alter table dynPart add partition (value='0', value2='clusterB') location 'hdfs:///tmp/test';
select value2, key from dynPart where value='0';

dfs -rmr file:///tmp/test;
dfs -rmr hdfs:///tmp/test;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- SORT_QUERY_RESULTS

-- EXCLUDE_OS_WINDOWS

-- NO_SESSION_REUSE

CREATE TABLE dest1(key INT, value STRING);

ADD FILE ../../ql/src/test/scripts/testgrep;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'testgrep' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue;

SELECT dest1.* FROM dest1;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
-- INCLUDE_OS_WINDOWS

CREATE TABLE dest1(key INT, value STRING);

ADD FILE ../../ql/src/test/scripts/testgrep_win.bat;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value)
         USING 'testgrep_win.bat' AS (tkey, tvalue)
  CLUSTER BY tkey
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tmap.tkey, tmap.tvalue;

SELECT dest1.* FROM dest1;
set hive.exec.script.allow.partial.consumption = false;
-- Tests exception in ScriptOperator.processOp() by passing extra data needed to fill pipe buffer
SELECT TRANSFORM(key, value, key, value, key, value, key, value, key, value, key, value, key, value, key, value, key, value, key, value, key, value, key, value) USING 'true' as a,b,c,d FROM src;
set hive.exec.script.allow.partial.consumption = true;
-- Test to ensure that a script with a bad error code still fails even with partial consumption
SELECT TRANSFORM(*) USING 'false' AS a, b FROM (SELECT TRANSFORM(*) USING 'echo' AS a, b FROM src LIMIT 1) tmp;
-- Verifies that script operator ID environment variables have unique values
-- in each instance of the script operator.
SELECT count(1) FROM
( SELECT * FROM (SELECT TRANSFORM('echo $HIVE_SCRIPT_OPERATOR_ID') USING 'sh' AS key FROM src order by key LIMIT 1)x UNION ALL
  SELECT * FROM (SELECT TRANSFORM('echo $HIVE_SCRIPT_OPERATOR_ID') USING 'sh' AS key FROM src order by key LIMIT 1)y ) a GROUP BY key;
set hive.script.operator.id.env.var = MY_ID;
-- Same test as script_env_var1, but test setting the variable name
SELECT count(1) FROM
( SELECT * FROM (SELECT TRANSFORM('echo $MY_ID') USING 'sh' AS key FROM src LIMIT 1)a UNION ALL
  SELECT * FROM (SELECT TRANSFORM('echo $MY_ID') USING 'sh' AS key FROM src LIMIT 1)b ) a GROUP BY key;
EXPLAIN
SELECT TRANSFORM(src.key, src.value) USING '../../data/scripts/error_script' AS (tkey, tvalue)
FROM src;

SELECT TRANSFORM(src.key, src.value) USING '../../data/scripts/error_script' AS (tkey, tvalue)
FROM src;

set hive.explain.user=false;
set hive.exec.script.allow.partial.consumption = true;
-- Tests exception in ScriptOperator.close() by passing to the operator a small amount of data
EXPLAIN SELECT TRANSFORM(*) USING 'true' AS a, b, c FROM (SELECT * FROM src LIMIT 1) tmp;
-- Tests exception in ScriptOperator.processOp() by passing extra data needed to fill pipe buffer
EXPLAIN SELECT TRANSFORM(key, value, key, value, key, value, key, value, key, value, key, value) USING 'head -n 1' as a,b,c,d FROM src;

SELECT TRANSFORM(*) USING 'true' AS a, b, c FROM (SELECT * FROM src LIMIT 1) tmp;
SELECT TRANSFORM(key, value, key, value, key, value, key, value, key, value, key, value) USING 'head -n 1' as a,b,c,d FROM src;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

explain select distinct src.* from src;

-- SORT_QUERY_RESULTS

select distinct src.* from src;

select distinct * from src;

explain select distinct * from src where key < '3';

select distinct * from src where key < '3';

from src a select distinct a.* where a.key = '238';

explain
SELECT distinct * from (
select * from src1
union all
select * from src )subq;

SELECT distinct * from (
select * from src1
union all
select * from src )subq;

drop view if exists sdi;

explain create view sdi as select distinct * from src order by key limit 2;

create view sdi as select distinct * from src order by key limit 2;

describe extended sdi;

describe formatted sdi;

select * from sdi;

select distinct * from src union all select distinct * from src1;

select distinct * from src join src1 on src.key=src1.key;

SELECT distinct *
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

select * from (select distinct * from src)src1
join
(select distinct * from src)src2
on src1.key=src2.key;

select distinct * from (select distinct * from src)src1;

explain select distinct src.* from src;

select distinct src.* from src;

select distinct * from src;

explain select distinct * from src where key < '3';

select distinct * from src where key < '3';

from src a select distinct a.* where a.key = '238';

explain
SELECT distinct * from (
select * from src1
union all
select * from src )subq;

SELECT distinct * from (
select * from src1
union all
select * from src )subq;

drop view if exists sdi;

explain create view sdi as select distinct * from src order by key limit 2;

create view sdi as select distinct * from src order by key limit 2;

describe extended sdi;

describe formatted sdi;

select * from sdi;

select distinct * from src union all select distinct * from src1;

select distinct * from src join src1 on src.key=src1.key;

SELECT distinct *
FROM src1 x JOIN src y ON (x.key = y.key)
JOIN srcpart z ON (x.value = z.value and z.ds='2008-04-08' and z.hr=11);

select * from (select distinct * from src)src1
join
(select distinct * from src)src2
on src1.key=src2.key;

select distinct * from (select distinct * from src)src1;
-- Duplicate column name: key

drop view if exists v;
create view v as select distinct * from src join src1 on src.key=src1.key;-- SELECT DISTINCT and GROUP BY can not be in the same query. Error encountered near token ‘key’

select distinct * from src group by key;drop table if exists datetest;
create table datetest(dValue date, iValue int);
insert into datetest values('2000-03-22', 1);
insert into datetest values('2001-03-22', 2);
insert into datetest values('2002-03-22', 3);
insert into datetest values('2003-03-22', 4);
SELECT * FROM datetest WHERE dValue IN ('2000-03-22','2001-03-22');
drop table datetest;

EXPLAIn
SELECT a, b FROM (
  SELECT key a, value b
  FROM src
) src1
ORDER BY a LIMIT 1;

SELECT a, b FROM (
  SELECT key a, value b
  FROM src
) src1
ORDER BY a LIMIT 1;
-- Check that charSetLiteral syntax conformance
-- Check that a sane error message with correct line/column numbers is emitted with helpful context tokens.
select _c17, count(1) from tmp_tl_foo group by _c17
set hive.explain.user=false;
explain
select 'a', 100;
select 'a', 100;

--evaluation
explain
select 1 + 1;
select 1 + 1;

-- explode (not possible for lateral view)
explain
select explode(array('a', 'b'));
select explode(array('a', 'b'));

set hive.fetch.task.conversion=more;

explain
select 'a', 100;
select 'a', 100;

explain
select 1 + 1;
select 1 + 1;

explain
select explode(array('a', 'b'));
select explode(array('a', 'b'));

-- subquery
explain
select 2 + 3,x from (select 1 + 2 x) X;
select 2 + 3,x from (select 1 + 2 x) X;

set hive.mapred.mode=nonstrict;
set hive.cbo.enable=true;

-- SORT_BEFORE_DIFF

drop table srclimit;
create table srclimit as select * from src limit 10;

select cast(value as binary), value from srclimit;

select cast(value as binary), value from srclimit order by value;

select cast(value as binary), value from srclimit order by value limit 5;

select cast(value as binary), value, key from srclimit order by value limit 5;

select *, key, value from srclimit;

select * from (select *, key, value from srclimit) t;

drop table srclimit;
-- Check SELECT * syntax.
-- Check that there should not be any identifier after STAR.
select *abcdef from src;set hive.entity.capture.transform=true;

EXPLAIN
SELECT /*+MAPJOIN(a)*/
TRANSFORM(a.key, a.value) USING 'cat' AS (tkey, tvalue)
FROM src a join src b
on a.key = b.key;


SELECT /*+MAPJOIN(a)*/
TRANSFORM(a.key, a.value) USING 'cat' AS (tkey, tvalue)
FROM src a join src b
on a.key = b.key;


EXPLAIN
SELECT /*+STREAMTABLE(a)*/
TRANSFORM(a.key, a.value) USING 'cat' AS (tkey, tvalue)
FROM src a join src b
on a.key = b.key;


SELECT /*+STREAMTABLE(a)*/
TRANSFORM(a.key, a.value) USING 'cat' AS (tkey, tvalue)
FROM src a join src b
on a.key = b.key;
-- Check alias count for SELECT UDTF() syntax:
-- explode returns a table with only 1 col - should be an error if query specifies >1 col aliases
SELECT explode(array(1,2,3)) AS (myCol1, myCol2) LIMIT 3;set hive.mapred.mode=nonstrict;
CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING);

INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')
SELECT src.key, src.value FROM src WHERE key < '200';

INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')
SELECT src.key, src.value FROM src WHERE key > '200';

SELECT count(*) FROM npe_test;

EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15;

SELECT count(*) FROM npe_test WHERE ds > 2012-11-31 AND ds < 2012-12-15;

DROP TABLE npe_test;
set hive.mapred.mode=nonstrict;
CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING);

INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')
SELECT src.key, src.value FROM src WHERE key < '200';

INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')
SELECT src.key, src.value FROM src WHERE key > '200';

SELECT count(*) FROM npe_test;

EXPLAIN SELECT * FROM npe_test WHERE NOT ds < 2012-11-31;

SELECT count(*) FROM npe_test WHERE NOT ds < 2012-11-31;

DROP TABLE npe_test;
set hive.mapred.mode=nonstrict;
CREATE TABLE npe_test (key STRING, value STRING) PARTITIONED BY (ds STRING);

INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-11')
SELECT src.key, src.value FROM src WHERE key < '200';

INSERT OVERWRITE TABLE npe_test PARTITION(ds='2012-12-12')
SELECT src.key, src.value FROM src WHERE key > '200';

SELECT count(*) FROM npe_test;

EXPLAIN SELECT * FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15;

SELECT count(*) FROM npe_test WHERE ds > 2012-11-31 OR ds < 2012-12-15;

DROP TABLE npe_test;
-- comment
-- comment;
-- comment
SELECT COUNT(1) FROM src;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

create table t1 as select cast(key as int) key, value from src where key <= 10;

select * from t1 sort by key;

create table t2 as select cast(2*key as int) key, value from t1;

select * from t2 sort by key;

create table t3 as select * from (select * from t1 union all select * from t2) b;
select * from t3 sort by key, value;

create table t4 (key int, value string);
select * from t4;

explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;

explain select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;
select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;

explain select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;
select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;

explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;
select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;

explain select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;

explain select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;
select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;

explain select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;
select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;

explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;
select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;

explain select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;
select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;

explain select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;

explain select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;
select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;

explain select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;
select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;

explain select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;
select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t1 a right outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t1 a right outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
-- reference rhs of semijoin in select-clause
select b.value from src a left semi join src b on (b.key = a.key and b.key = '100');
CREATE TABLE table_1 (boolean_col_1 BOOLEAN, float_col_2 FLOAT, bigint_col_3 BIGINT, varchar0111_col_4 VARCHAR(111), bigint_col_5 BIGINT, float_col_6 FLOAT, boolean_col_7 BOOLEAN, decimal0101_col_8 DECIMAL(1, 1), decimal0904_col_9 DECIMAL(9, 4), char0112_col_10 CHAR(112), double_col_11 DOUBLE, boolean_col_12 BOOLEAN, double_col_13 DOUBLE, varchar0142_col_14 VARCHAR(142), timestamp_col_15 TIMESTAMP, decimal0502_col_16 DECIMAL(5, 2), smallint_col_25 SMALLINT, decimal3222_col_18 DECIMAL(32, 22), boolean_col_19 BOOLEAN, decimal2012_col_20 DECIMAL(20, 12), char0204_col_21 CHAR(204), double_col_61 DOUBLE, timestamp_col_23 TIMESTAMP, int_col_24 INT, float_col_25 FLOAT, smallint_col_26 SMALLINT, double_col_27 DOUBLE, char0180_col_28 CHAR(180), decimal1503_col_29 DECIMAL(15, 3), timestamp_col_30 TIMESTAMP, smallint_col_31 SMALLINT, decimal2020_col_32 DECIMAL(20, 20), timestamp_col_33 TIMESTAMP, boolean_col_34 BOOLEAN, decimal3025_col_35 DECIMAL(30, 25), decimal3117_col_36 DECIMAL(31, 17), timestamp_col_37 TIMESTAMP, varchar0146_col_38 VARCHAR(146), boolean_col_39 BOOLEAN, double_col_40 DOUBLE, float_col_41 FLOAT, timestamp_col_42 TIMESTAMP, double_col_43 DOUBLE, boolean_col_44 BOOLEAN, timestamp_col_45 TIMESTAMP, tinyint_col_8 TINYINT, int_col_47 INT, decimal0401_col_48 DECIMAL(4, 1), varchar0064_col_49 VARCHAR(64), string_col_50 STRING, double_col_51 DOUBLE, string_col_52 STRING, boolean_col_53 BOOLEAN, int_col_54 INT, boolean_col_55 BOOLEAN, string_col_56 STRING, double_col_57 DOUBLE, varchar0131_col_58 VARCHAR(131), boolean_col_59 BOOLEAN, bigint_col_22 BIGINT, char0184_col_61 CHAR(184), varchar0173_col_62 VARCHAR(173), timestamp_col_63 TIMESTAMP, decimal1709_col_26 DECIMAL(20, 5), timestamp_col_65 TIMESTAMP, timestamp_col_66 TIMESTAMP, timestamp_col_67 TIMESTAMP, boolean_col_68 BOOLEAN, decimal1208_col_20 DECIMAL(33, 11), decimal1605_col_70 DECIMAL(16, 5), varchar0010_col_71 VARCHAR(10), tinyint_col_72 TINYINT, timestamp_col_10 TIMESTAMP, decimal2714_col_74 DECIMAL(27, 14), double_col_75 DOUBLE, boolean_col_76 BOOLEAN, double_col_77 DOUBLE, string_col_78 STRING, boolean_col_79 BOOLEAN, boolean_col_80 BOOLEAN, decimal0803_col_81 DECIMAL(8, 3), decimal1303_col_82 DECIMAL(13, 3), tinyint_col_83 TINYINT, decimal3424_col_84 DECIMAL(34, 24), float_col_85 FLOAT, boolean_col_86 BOOLEAN, char0233_col_87 CHAR(233));

CREATE TABLE table_18 (timestamp_col_1 TIMESTAMP, double_col_2 DOUBLE, boolean_col_3 BOOLEAN, timestamp_col_4 TIMESTAMP, decimal2103_col_5 DECIMAL(21, 3), char0221_col_6 CHAR(221), tinyint_col_7 TINYINT, float_col_8 FLOAT, int_col_2 INT, timestamp_col_10 TIMESTAMP, char0228_col_11 CHAR(228), timestamp_col_12 TIMESTAMP, double_col_13 DOUBLE, tinyint_col_6 TINYINT, tinyint_col_33 TINYINT, smallint_col_38 SMALLINT, boolean_col_17 BOOLEAN, double_col_18 DOUBLE, boolean_col_19 BOOLEAN, bigint_col_20 BIGINT, decimal0504_col_37 DECIMAL(37, 34), boolean_col_22 BOOLEAN, double_col_23 DOUBLE, timestamp_col_24 TIMESTAMP, varchar0076_col_25 VARCHAR(76), timestamp_col_18 TIMESTAMP, boolean_col_27 BOOLEAN, decimal1611_col_22 DECIMAL(37, 5), boolean_col_29 BOOLEAN);

set hive.cbo.enable = false;

explain
SELECT
COALESCE(498, LEAD(COALESCE(-973, -684, 515)) OVER (PARTITION BY (t2.int_col_2 + t1.smallint_col_25) ORDER BY (t2.int_col_2 + t1.smallint_col_25), FLOOR(t1.double_col_61) DESC), 524) AS int_col,
(t2.int_col_2) + (t1.smallint_col_25) AS int_col_1,
FLOOR(t1.double_col_61) AS float_col,
COALESCE(SUM(COALESCE(62, -380, -435)) OVER (PARTITION BY (t2.int_col_2 + t1.smallint_col_25) ORDER BY (t2.int_col_2 + t1.smallint_col_25) DESC, FLOOR(t1.double_col_61) DESC ROWS BETWEEN UNBOUNDED PRECEDING AND 48 FOLLOWING), 704) AS int_col_2
FROM table_1 t1
INNER JOIN table_18 t2 ON (((t2.tinyint_col_6) = (t1.bigint_col_22)) AND ((t2.decimal0504_col_37) = (t1.decimal1709_col_26))) AND ((t2.tinyint_col_33) = (t1.tinyint_col_8))
WHERE
(t2.smallint_col_38) IN (SELECT
COALESCE(-92, -994) AS int_col
FROM table_1 tt1
INNER JOIN table_18 tt2 ON (tt2.decimal1611_col_22) = (tt1.decimal1208_col_20)
WHERE
(t1.timestamp_col_10) = (tt2.timestamp_col_18));
-- rhs table reference in the where clause
select a.value from src a left semi join src b on a.key = b.key where b.value = 'val_18';
create table t1 as select cast(key as int) key, value from src;

create table t2 as select cast(key as int) key, value from src;

set hive.cbo.enable=false;

explain
select count(1)
from
  (select key
  from t1
  where key = 0) t1
left semi join
  (select key
  from t2
  where key = 0) t2
on 1 = 1;

select count(1)
from
  (select key
  from t1
  where key = 0) t1
left semi join
  (select key
  from t2
  where key = 0) t2
on 1 = 1;
-- rhs table reference in group by
select * from src a left semi join src b on a.key = b.key group by b.value;
set hive.mapred.mode=nonstrict;

CREATE TABLE table_1 (int_col_1 INT, decimal3003_col_2 DECIMAL(30, 3), timestamp_col_3 TIMESTAMP, decimal0101_col_4 DECIMAL(1, 1), double_col_5 DOUBLE, boolean_col_6 BOOLEAN, timestamp_col_7 TIMESTAMP, varchar0098_col_8 VARCHAR(98), int_col_9 INT, timestamp_col_10 TIMESTAMP, decimal0903_col_11 DECIMAL(9, 3), int_col_12 INT, bigint_col_13 BIGINT, boolean_col_14 BOOLEAN, char0254_col_15 CHAR(254), boolean_col_16 BOOLEAN, smallint_col_17 SMALLINT, float_col_18 FLOAT, decimal2608_col_19 DECIMAL(26, 8), varchar0216_col_20 VARCHAR(216), string_col_21 STRING, timestamp_col_22 TIMESTAMP, double_col_23 DOUBLE, smallint_col_24 SMALLINT, float_col_25 FLOAT, decimal2016_col_26 DECIMAL(20, 16), string_col_27 STRING, decimal0202_col_28 DECIMAL(2, 2), boolean_col_29 BOOLEAN, decimal2020_col_30 DECIMAL(20, 20), float_col_31 FLOAT, boolean_col_32 BOOLEAN, varchar0148_col_33 VARCHAR(148), decimal2121_col_34 DECIMAL(21, 21), timestamp_col_35 TIMESTAMP, float_col_36 FLOAT, float_col_37 FLOAT, string_col_38 STRING, decimal3420_col_39 DECIMAL(34, 20), smallint_col_40 SMALLINT, decimal1408_col_41 DECIMAL(14, 8), string_col_42 STRING, decimal0902_col_43 DECIMAL(9, 2), varchar0204_col_44 VARCHAR(204), float_col_45 FLOAT, tinyint_col_46 TINYINT, double_col_47 DOUBLE, timestamp_col_48 TIMESTAMP, double_col_49 DOUBLE, timestamp_col_50 TIMESTAMP, decimal0704_col_51 DECIMAL(7, 4), int_col_52 INT, double_col_53 DOUBLE, int_col_54 INT, timestamp_col_55 TIMESTAMP, decimal0505_col_56 DECIMAL(5, 5), char0155_col_57 CHAR(155), double_col_58 DOUBLE, timestamp_col_59 TIMESTAMP, double_col_60 DOUBLE, float_col_61 FLOAT, char0249_col_62 CHAR(249), float_col_63 FLOAT, smallint_col_64 SMALLINT, decimal1309_col_65 DECIMAL(13, 9), timestamp_col_66 TIMESTAMP, boolean_col_67 BOOLEAN, tinyint_col_68 TINYINT, tinyint_col_69 TINYINT, double_col_70 DOUBLE, bigint_col_71 BIGINT, boolean_col_72 BOOLEAN, float_col_73 FLOAT, char0222_col_74 CHAR(222), boolean_col_75 BOOLEAN, string_col_76 STRING, decimal2612_col_77 DECIMAL(26, 12), bigint_col_78 BIGINT, char0128_col_79 CHAR(128), tinyint_col_80 TINYINT, boolean_col_81 BOOLEAN, int_col_82 INT, boolean_col_83 BOOLEAN, decimal2622_col_84 DECIMAL(26, 22), boolean_col_85 BOOLEAN, boolean_col_86 BOOLEAN, decimal0907_col_87 DECIMAL(9, 7))
STORED AS orc;
CREATE TABLE table_18 (float_col_1 FLOAT, double_col_2 DOUBLE, decimal2518_col_3 DECIMAL(25, 18), boolean_col_4 BOOLEAN, bigint_col_5 BIGINT, boolean_col_6 BOOLEAN, boolean_col_7 BOOLEAN, char0035_col_8 CHAR(35), decimal2709_col_9 DECIMAL(27, 9), timestamp_col_10 TIMESTAMP, bigint_col_11 BIGINT, decimal3604_col_12 DECIMAL(36, 4), string_col_13 STRING, timestamp_col_14 TIMESTAMP, timestamp_col_15 TIMESTAMP, decimal1911_col_16 DECIMAL(19, 11), boolean_col_17 BOOLEAN, tinyint_col_18 TINYINT, timestamp_col_19 TIMESTAMP, timestamp_col_20 TIMESTAMP, tinyint_col_21 TINYINT, float_col_22 FLOAT, timestamp_col_23 TIMESTAMP)
STORED AS orc;

explain
SELECT
    COALESCE(498,
      LEAD(COALESCE(-973, -684, 515)) OVER (
        PARTITION BY (t2.tinyint_col_21 + t1.smallint_col_24)
        ORDER BY (t2.tinyint_col_21 + t1.smallint_col_24),
        FLOOR(t1.double_col_60) DESC),
      524) AS int_col
FROM table_1 t1 INNER JOIN table_18 t2
ON (((t2.tinyint_col_18) = (t1.bigint_col_13))
    AND ((t2.decimal2709_col_9) = (t1.decimal1309_col_65)))
    AND ((t2.tinyint_col_21) = (t1.tinyint_col_46))
WHERE (t2.tinyint_col_21) IN (
        SELECT COALESCE(-92, -994) AS int_col_3
        FROM table_1 tt1 INNER JOIN table_18 tt2
        ON (tt2.decimal1911_col_16) = (tt1.decimal1309_col_65)
        WHERE (tt1.timestamp_col_66) = (tt2.timestamp_col_19));
-- rhs table is a view and reference the view in where clause
select a.value from src a left semi join (select key , value from src where key > 100) b on a.key = b.key where b.value = 'val_108' ;

EXPLAIN
CREATE TABLE serde_opencsv(
                          words STRING,
                          int1 INT,
                          tinyint1 TINYINT,
                          smallint1 SMALLINT,
                          bigint1 BIGINT,
                          boolean1 BOOLEAN,
                          float1 FLOAT,
                          double1 DOUBLE)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES(
  "separatorChar" = ",",
  "quoteChar"     = "'",
  "escapeChar"    = "\\"
) stored as textfile;

CREATE TABLE serde_opencsv(
                          words STRING,
                          int1 INT,
                          tinyint1 TINYINT,
                          smallint1 SMALLINT,
                          bigint1 BIGINT,
                          boolean1 BOOLEAN,
                          float1 FLOAT,
                          double1 DOUBLE)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES(
  "separatorChar" = ",",
  "quoteChar"     = "'",
  "escapeChar"    = "\\"
) stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/opencsv-data.txt" INTO TABLE serde_opencsv;

SELECT count(*) FROM serde_opencsv;set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

EXPLAIN
CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?",
  "output.format.string" = "%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s"
)
STORED AS TEXTFILE;

CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?",
  "output.format.string" = "%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s"
)
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH "../../data/files/apache.access.log" INTO TABLE serde_regex;
LOAD DATA LOCAL INPATH "../../data/files/apache.access.2.log" INTO TABLE serde_regex;

SELECT * FROM serde_regex ORDER BY time;USE default;
--  This should fail because Regex SerDe doesn't support STRUCT
CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time TIMESTAMP,
  request STRING,
  status INT,
  size INT,
  referer STRING,
  agent STRING,
  strct STRUCT<a:INT, b:STRING>)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?")
STORED AS TEXTFILE;
set hive.mapred.mode=nonstrict;
EXPLAIN
CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time STRING,
  request STRING,
  status STRING,
  size INT,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?"
)
STORED AS TEXTFILE;

CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time STRING,
  request STRING,
  status STRING,
  size INT,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?"
)
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH "../../data/files/apache.access.log" INTO TABLE serde_regex;
LOAD DATA LOCAL INPATH "../../data/files/apache.access.2.log" INTO TABLE serde_regex;

SELECT * FROM serde_regex ORDER BY time;

SELECT host, size, status, time from serde_regex ORDER BY time;

DROP TABLE serde_regex;

EXPLAIN
CREATE TABLE serde_regex1(
  key decimal(38,18),
  value int)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*)"
)
STORED AS TEXTFILE;

CREATE TABLE serde_regex1(
  key decimal(38,18),
  value int)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*)"
)
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH "../../data/files/kv7.txt" INTO TABLE serde_regex1;

SELECT key, value FROM serde_regex1 ORDER BY key, value;

DROP TABLE serde_regex1;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

USE default;

--  This should fail because Regex SerDe supports only columns of type string
EXPLAIN
CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time STRING,
  request STRING,
  status INT,
  size INT,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?",
  "output.format.string" = "%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s"
)
STORED AS TEXTFILE;

CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time STRING,
  request STRING,
  status INT,
  size INT,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?",
  "output.format.string" = "%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s"
)
STORED AS TEXTFILE;set hive.mapred.mode=nonstrict;
USE default;
-- Mismatch between the number of matching groups and columns, throw run time exception. Ideally this should throw a compile time exception. See JIRA-3023 for more details.
 CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)"
)
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH "../../data/files/apache.access.log" INTO TABLE serde_regex;
LOAD DATA LOCAL INPATH "../../data/files/apache.access.2.log" INTO TABLE serde_regex;

-- raise an exception
SELECT * FROM serde_regex ORDER BY time;USE default;
-- null input.regex, raise an exception
 CREATE TABLE serde_regex(
  host STRING,
  identity STRING,
  `user` STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
STORED AS TEXTFILE;
create table int_string
  partitioned by (b string)
  row format serde "org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer"
    with serdeproperties (
      "serialization.class"="org.apache.hadoop.hive.serde2.thrift.test.IntString",
      "serialization.format"="org.apache.thrift.protocol.TBinaryProtocol");
describe extended int_string;
alter table int_string add partition (b='part1');
describe extended int_string partition (b='part1');
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

DROP TABLE s3log;
CREATE TABLE s3log
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer'
STORED AS TEXTFILE;

DESCRIBE s3log;

LOAD DATA LOCAL INPATH '../../contrib/data/files/s3.log' INTO TABLE s3log;

SELECT a.* FROM s3log a;

DROP TABLE s3log;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

drop table dest1;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

SELECT dest1.* FROM dest1;

drop table dest1;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

drop table dest1;
CREATE TABLE dest1(key SMALLINT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(cast(src.key as smallint), src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'cat'
  AS (tkey smallint, tvalue string) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

FROM (
  FROM src
  SELECT TRANSFORM(cast(src.key as smallint), src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'cat'
  AS (tkey smallint, tvalue string) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

SELECT dest1.* FROM dest1;

drop table dest1;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

drop table dest1;
CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(cast(src.key as smallint), src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

FROM (
  FROM src
  SELECT TRANSFORM(cast(src.key as smallint), src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

SELECT dest1.* FROM dest1;

drop table dest1;
set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

drop table dest1;
CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(cast(src.key as tinyint), src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
  WHERE key < 100
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue ORDER by tkey, tvalue;

FROM (
  FROM src
  SELECT TRANSFORM(cast(src.key as tinyint), src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'cat'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
  WHERE key < 100
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue ORDER by tkey, tvalue;

SELECT dest1.* FROM dest1;

drop table dest1;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

drop table dest1;
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

EXPLAIN
FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'python ../../data/scripts/cat.py'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

FROM (
  FROM src
  SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDWRITER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter'
  USING 'python ../../data/scripts/cat.py'
  AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe'
  RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader'
) tmap
INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue;

SELECT dest1.* FROM dest1;

drop table dest1;
DROP TABLE table1;
DROP TABLE table2;
DROP TABLE table3;

CREATE TABLE table1 (a STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe';
CREATE TABLE table2 (a STRING, b STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe';
CREATE TABLE table3 (a STRING, b STRING, c STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe';

DROP TABLE table1;
DROP TABLE table2;
DROP TABLE table3;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

DROP TABLE table1;

CREATE TABLE table1 (a STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe' STORED AS SEQUENCEFILE;

INSERT OVERWRITE TABLE table1 SELECT NULL FROM SRC;

SELECT * FROM table1;

SELECT a FROM table1 WHERE a IS NULL;

SELECT a FROM table1 WHERE a IS NOT NULL;

DROP TABLE table1;

-- HIVE-2906 Table properties in SQL

explain extended select key from src;
explain extended select a.key from src a;
explain extended select a.key from src tablesample(1 percent) a;
explain extended select key from src ('user.defined.key'='some.value');
explain extended select key from src ('user.defined.key'='some.value') tablesample(1 percent);
explain extended select a.key from src ('user.defined.key'='some.value') a;
explain extended select a.key from src ('user.defined.key'='some.value') tablesample(1 percent) a;
-- should fail: for some internal variables which should not be settable via set command
desc src;

set hive.added.jars.path=file://rootdir/test/added/a.jar;
-- should fail: hive.conf.internal.variable.list is in restricted list
desc src;

set hive.conf.internal.variable.list=;
-- should fail: hive.join.cache.size accepts int type
desc src;

set hive.conf.validation=true;
set hive.join.cache.size=test;
-- should fail: hive.map.aggr.hash.min.reduction accepts float type
desc src;

set hive.conf.validation=true;
set hive.map.aggr.hash.min.reduction=false;
-- should fail: hive.fetch.task.conversion accepts none, minimal or more
desc src;

set hive.conf.validation=true;
set hive.fetch.task.conversion=true;
set metaconf:hive.metastore.try.direct.sql;

set metaconf:hive.metastore.try.direct.sql=false;
set metaconf:hive.metastore.try.direct.sql;
set hive.metastore.try.direct.sql;
set hive.mapred.mode=nonstrict;
set zzz=5;
set zzz;

set system:xxx=5;
set system:xxx;

set system:yyy=${system:xxx};
set system:yyy;

set go=${hiveconf:zzz};
set go;

set hive.variable.substitute=false;
set raw=${hiveconf:zzz};
set raw;

set hive.variable.substitute=true;

EXPLAIN SELECT * FROM src where key=${hiveconf:zzz};
SELECT * FROM src where key=${hiveconf:zzz};

set a=1;
set b=a;
set c=${hiveconf:${hiveconf:b}};
set c;

set jar=${system:maven.local.repository}/org/apache/derby/derby/${system:derby.version}/derby-${system:derby.version}.jar;

add file ${hiveconf:jar};
delete file ${hiveconf:jar};
list file;
create table testTable(col1 int, col2 int);

-- set a table property = null, it should be caught by the grammar
alter table testTable set tblproperties ('a'=);
set hivevar:key1=value1;

EXPLAIN SELECT * FROM src where key="${key1}";
EXPLAIN SELECT * FROM src where key="${hivevar:key1}";

set hivevar:a=1;
set hivevar:b=a;
set hivevar:c=${hivevar:${hivevar:b}};
EXPLAIN SELECT * FROM src where key="${hivevar:c}";

set hivevar:a;
set hivevar:b;
set hivevar:c;
set hivevar:key1;

EXPLAIN
SHOW PARTITIONS srcpart;

SHOW PARTITIONS srcpart;
CREATE TABLE shcol_test(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;

EXPLAIN
SHOW COLUMNS from shcol_test;

SHOW COLUMNS from shcol_test;

-- SHOW COLUMNS
CREATE DATABASE test_db;
USE test_db;
CREATE TABLE foo(a INT);

-- SHOW COLUMNS basic syntax tests
USE test_db;
SHOW COLUMNS from foo;
SHOW COLUMNS in foo;

-- SHOW COLUMNS from a database with a name that requires escaping
CREATE DATABASE `database`;
USE `database`;
CREATE TABLE foo(a INT);
SHOW COLUMNS from foo;

use default;
SHOW COLUMNS from test_db.foo;
SHOW COLUMNS from foo from test_db;
SHOW COLUMNS from shcol_test;

SHOW COLUMNS from shcol_test foo;

CREATE DATABASE test_db;
USE test_db;
CREATE TABLE foo(a INT);

use default;
SHOW COLUMNS from test_db.foo from test_db;

show conf "hive.auto.convert.sortmerge.join.to.mapjoin";

show conf "hive.zookeeper.session.timeout";
CREATE DATABASE some_database comment 'for show create db test' WITH DBPROPERTIES ('somekey'='somevalue');
SHOW CREATE DATABASE some_database;

-- Test SHOW CREATE TABLE on an external, clustered and sorted table. Then test the query again after ALTERs.

CREATE EXTERNAL TABLE tmp_showcrt1 (key smallint, value float)
CLUSTERED BY (key) SORTED BY (value DESC) INTO 5 BUCKETS;
SHOW CREATE TABLE tmp_showcrt1;

-- Add a comment to the table, change the EXTERNAL property, and test SHOW CREATE TABLE on the change.
ALTER TABLE tmp_showcrt1 SET TBLPROPERTIES ('comment'='temporary table', 'EXTERNAL'='FALSE');
SHOW CREATE TABLE tmp_showcrt1;

-- Alter the table comment, change the EXTERNAL property back and test SHOW CREATE TABLE on the change.
ALTER TABLE tmp_showcrt1 SET TBLPROPERTIES ('comment'='changed comment', 'EXTERNAL'='TRUE');
SHOW CREATE TABLE tmp_showcrt1;

-- Change the 'SORTBUCKETCOLSPREFIX' property and test SHOW CREATE TABLE. The output should not change.
ALTER TABLE tmp_showcrt1 SET TBLPROPERTIES ('SORTBUCKETCOLSPREFIX'='FALSE');
SHOW CREATE TABLE tmp_showcrt1;

-- Alter the storage handler of the table, and test SHOW CREATE TABLE.
ALTER TABLE tmp_showcrt1 SET TBLPROPERTIES ('storage_handler'='org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler');
SHOW CREATE TABLE tmp_showcrt1;
DROP TABLE tmp_showcrt1;

-- Test SHOW CREATE TABLE on a table name of format "db.table".

CREATE DATABASE tmp_feng comment 'for show create table test';
SHOW DATABASES;
CREATE TABLE tmp_feng.tmp_showcrt (key string, value int);
USE default;
SHOW CREATE TABLE tmp_feng.tmp_showcrt;
DROP TABLE tmp_feng.tmp_showcrt;
DROP DATABASE tmp_feng;

-- Test SHOW CREATE TABLE on a table with delimiters, stored format, and location.

CREATE TABLE tmp_showcrt1 (key int, value string, newvalue bigint)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY '|' MAP KEYS TERMINATED BY '\045' LINES TERMINATED BY '\n'
STORED AS textfile
LOCATION 'file:${system:test.tmp.dir}/tmp_showcrt1';
SHOW CREATE TABLE tmp_showcrt1;
DROP TABLE tmp_showcrt1;

SHOW CREATE TABLE tmp_nonexist;

CREATE TABLE tmp_showcrt (key int, value string);
CREATE INDEX tmp_index on table tmp_showcrt(key) as 'compact' WITH DEFERRED REBUILD;
SHOW CREATE TABLE default__tmp_showcrt_tmp_index__;
DROP INDEX tmp_index on tmp_showcrt;
DROP TABLE tmp_showcrt;

-- Test SHOW CREATE TABLE on a table with partitions and column comments.

CREATE EXTERNAL TABLE tmp_showcrt1 (key string, newvalue boolean COMMENT 'a new value')
COMMENT 'temporary table'
PARTITIONED BY (value bigint COMMENT 'some value');
SHOW CREATE TABLE tmp_showcrt1;
DROP TABLE tmp_showcrt1;

-- Test SHOW CREATE TABLE on a table with serde.

CREATE TABLE tmp_showcrt1 (key int, value string, newvalue bigint);
ALTER TABLE tmp_showcrt1 SET SERDEPROPERTIES ('custom.property.key1'='custom.property.value1', 'custom.property.key2'='custom.property.value2');
SHOW CREATE TABLE tmp_showcrt1;
DROP TABLE tmp_showcrt1;

-- without a storage handler
CREATE TABLE tmp_showcrt1 (key int, value string, newvalue bigint)
COMMENT 'temporary table'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileOutputFormat';
SHOW CREATE TABLE tmp_showcrt1;
DROP TABLE tmp_showcrt1;

-- without a storage handler / with custom serde params
CREATE TABLE tmp_showcrt1 (key int, value string, newvalue bigint)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
WITH SERDEPROPERTIES ('custom.property.key1'='custom.property.value1', 'custom.property.key2'='custom.property.value2')
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileOutputFormat';
SHOW CREATE TABLE tmp_showcrt1;
DROP TABLE tmp_showcrt1;

-- with a storage handler and serde properties
CREATE EXTERNAL TABLE tmp_showcrt1 (key string, value boolean)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler'
WITH SERDEPROPERTIES ('field.delim'=',', 'serialization.format'='$');
SHOW CREATE TABLE tmp_showcrt1;
DROP TABLE tmp_showcrt1;


create database tmpdb;
create temporary table tmpdb.tmp1 (c1 string, c2 string);
show create table tmpdb.tmp1;
drop table tmp1;
drop database tmpdb;
-- Test SHOW CREATE TABLE on a view name.

CREATE VIEW tmp_copy_src AS SELECT * FROM src;
SHOW CREATE TABLE tmp_copy_src;
DROP VIEW tmp_copy_src;

SHOW FUNCTIONS 'concat';

SHOW FUNCTIONS concat;

DESCRIBE FUNCTION 'concat';

DESCRIBE FUNCTION concat;
SHOW FUNCTIONS;

SHOW FUNCTIONS '^c.*';

SHOW FUNCTIONS '.*e$';

SHOW FUNCTIONS 'log.*';

SHOW FUNCTIONS '.*date.*';

SHOW FUNCTIONS '***';

SHOW FUNCTIONS LIKE 'When';

SHOW FUNCTIONS LIKE 'max|min';

SHOW FUNCTIONS LIKE 'xpath*|m*';

SHOW FUNCTIONS LIKE 'nomatch';

SHOW FUNCTIONS LIKE "log";

SHOW FUNCTIONS LIKE 'log';

SHOW FUNCTIONS LIKE `log`;

SHOW FUNCTIONS LIKE 'log*';

SHOW FUNCTIONS LIKE "log*";

SHOW FUNCTIONS LIKE `log*`;
set hive.stats.dbclass=fs;
DROP TABLE show_idx_empty;
DROP TABLE show_idx_full;

CREATE TABLE show_idx_empty(KEY STRING, VALUE STRING);
CREATE TABLE show_idx_full(KEY STRING, VALUE1 STRING, VALUE2 STRING);

CREATE INDEX idx_1 ON TABLE show_idx_full(KEY) AS "COMPACT" WITH DEFERRED REBUILD;
CREATE INDEX idx_2 ON TABLE show_idx_full(VALUE1) AS "COMPACT" WITH DEFERRED REBUILD;

CREATE INDEX idx_comment ON TABLE show_idx_full(VALUE2) AS "COMPACT" WITH DEFERRED REBUILD COMMENT "index comment";
CREATE INDEX idx_compound ON TABLE show_idx_full(KEY, VALUE1) AS "COMPACT" WITH DEFERRED REBUILD;

ALTER INDEX idx_1 ON show_idx_full REBUILD;
ALTER INDEX idx_2 ON show_idx_full REBUILD;
ALTER INDEX idx_comment ON show_idx_full REBUILD;
ALTER INDEX idx_compound ON show_idx_full REBUILD;

EXPLAIN SHOW INDEXES ON show_idx_full;
SHOW INDEXES ON show_idx_full;

EXPLAIN SHOW INDEXES ON show_idx_empty;
SHOW INDEXES ON show_idx_empty;

DROP INDEX idx_1 on show_idx_full;
DROP INDEX idx_2 on show_idx_full;
DROP TABLE show_idx_empty;
DROP TABLE show_idx_full;
set hive.stats.dbclass=fs;
DROP TABLE show_idx_t1;

CREATE TABLE show_idx_t1(KEY STRING, VALUE STRING);

CREATE INDEX idx_t1 ON TABLE show_idx_t1(KEY) AS "COMPACT" WITH DEFERRED REBUILD;
ALTER INDEX idx_t1 ON show_idx_t1 REBUILD;

EXPLAIN
SHOW INDEX ON show_idx_t1;

SHOW INDEX ON show_idx_t1;

EXPLAIN
SHOW INDEXES ON show_idx_t1;

SHOW INDEXES ON show_idx_t1;

EXPLAIN
SHOW FORMATTED INDEXES ON show_idx_t1;

SHOW FORMATTED INDEXES ON show_idx_t1;

DROP TABLE show_idx_t1;
SHOW PARTITIONS srcpart;
SHOW PARTITIONS default.srcpart;
SHOW PARTITIONS srcpart PARTITION(hr='11');
SHOW PARTITIONS srcpart PARTITION(ds='2008-04-08');
SHOW PARTITIONS srcpart PARTITION(ds='2008-04-08', hr='12');


SHOW PARTITIONS default.srcpart;
SHOW PARTITIONS default.srcpart PARTITION(hr='11');
SHOW PARTITIONS default.srcpart PARTITION(ds='2008-04-08');
SHOW PARTITIONS default.srcpart PARTITION(ds='2008-04-08', hr='12');

CREATE DATABASE db1;
USE db1;

CREATE TABLE srcpart (key1 INT, value1 STRING) PARTITIONED BY (ds STRING, hr STRING);
ALTER TABLE srcpart ADD PARTITION (ds='3', hr='3');
ALTER TABLE srcpart ADD PARTITION (ds='4', hr='4');
ALTER TABLE srcpart ADD PARTITION (ds='4', hr='5');

-- from db1 to default db
SHOW PARTITIONS default.srcpart PARTITION(hr='11');
SHOW PARTITIONS default.srcpart PARTITION(ds='2008-04-08', hr='12');

-- from db1 to db1
SHOW PARTITIONS srcpart PARTITION(ds='4');
SHOW PARTITIONS srcpart PARTITION(ds='3', hr='3');

use default;
-- from default to db1
SHOW PARTITIONS db1.srcpart PARTITION(ds='4');
SHOW PARTITIONS db1.srcpart PARTITION(ds='3', hr='3');SHOW PARTITIONS NonExistentTable;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;

create role role1;
create role role2;

show roles;
SHOW TBLPROPERTIES NonExistentTable;
CREATE TABLE shtb_test1(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;
CREATE TABLE shtb_test2(KEY STRING, VALUE STRING) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;

EXPLAIN
SHOW TABLES 'shtb_*';

SHOW TABLES 'shtb_*';

EXPLAIN
SHOW TABLES LIKE 'shtb_test1|shtb_test2';

SHOW TABLES LIKE 'shtb_test1|shtb_test2';

-- SHOW TABLES FROM/IN database
CREATE DATABASE test_db;
USE test_db;
CREATE TABLE foo(a INT);
CREATE TABLE bar(a INT);
CREATE TABLE baz(a INT);

-- SHOW TABLES basic syntax tests
USE default;
SHOW TABLES FROM test_db;
SHOW TABLES FROM default;
SHOW TABLES IN test_db;
SHOW TABLES IN default;
SHOW TABLES IN test_db "test*";
SHOW TABLES IN test_db LIKE "nomatch";

-- SHOW TABLE EXTENDED basic syntax tests and wildcard
SHOW TABLE EXTENDED IN test_db LIKE foo;
SHOW TABLE EXTENDED IN test_db LIKE "foo";
SHOW TABLE EXTENDED IN test_db LIKE 'foo';
SHOW TABLE EXTENDED IN test_db LIKE `foo`;
SHOW TABLE EXTENDED IN test_db LIKE 'ba*';
SHOW TABLE EXTENDED IN test_db LIKE "ba*";
SHOW TABLE EXTENDED IN test_db LIKE `ba*`;

-- SHOW TABLES from a database with a name that requires escaping
CREATE DATABASE `database`;
USE `database`;
CREATE TABLE foo(a INT);
USE default;
SHOW TABLES FROM `database` LIKE "foo";
set hive.support.quoted.identifiers=none;
EXPLAIN
SHOW TABLE EXTENDED IN default LIKE `src`;

SHOW TABLE EXTENDED IN default LIKE `src`;

SHOW TABLE EXTENDED from default LIKE `src`;

SHOW TABLE EXTENDED LIKE `src`;

SHOW TABLE EXTENDED LIKE `src.?`;

SHOW TABLE EXTENDED from default LIKE `src.?`;

SHOW TABLE EXTENDED LIKE `^s.*`;

SHOW TABLE EXTENDED from default LIKE `^s.*`;

SHOW TABLE EXTENDED LIKE `srcpart` PARTITION(ds='2008-04-08', hr=11);

SHOW TABLE EXTENDED from default LIKE src;SHOW TABLE EXTENDED LIKE `srcpar*` PARTITION(ds='2008-04-08', hr=11);SHOW TABLE EXTENDED LIKE `srcpart` PARTITION(ds='2008-14-08', hr=11);SHOW TABLES JOIN;
SHOW TABLES FROM default LIKE a b;
SHOW TABLES FROM nonexistent;SHOW TABLES FROM nonexistent LIKE 'test';
create table tmpfoo (a String);
show tblproperties tmpfoo("bar");
show tblproperties default.tmpfoo("bar");

alter table tmpfoo set tblproperties ("bar" = "bar value");
alter table tmpfoo set tblproperties ("tmp" = "true");

show tblproperties tmpfoo;
show tblproperties tmpfoo("bar");

show tblproperties default.tmpfoo;
show tblproperties default.tmpfoo("bar");

CREATE DATABASE db1;
USE db1;

CREATE TABLE tmpfoo (b STRING);
alter table tmpfoo set tblproperties ("bar" = "bar value1");
alter table tmpfoo set tblproperties ("tmp" = "true1");

-- from db1 to default db
show tblproperties default.tmpfoo;
show tblproperties default.tmpfoo("bar");

-- from db1 to db1
show tblproperties tmpfoo;
show tblproperties tmpfoo("bar");

use default;
-- from default to db1
show tblproperties db1.tmpfoo;
show tblproperties db1.tmpfoo("bar");

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.optimize.skewjoin = true;
set hive.skewjoin.key = 2;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T4(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE dest_j1(key INT, value STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;
LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T4;

EXPLAIN
FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

FROM src src1 JOIN src src2 ON (src1.key = src2.key)
INSERT OVERWRITE TABLE dest_j1 SELECT src1.key, src2.value;

SELECT sum(hash(key)), sum(hash(value)) FROM dest_j1;

EXPLAIN
SELECT /*+ STREAMTABLE(a) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;

SELECT /*+ STREAMTABLE(a) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;

EXPLAIN
SELECT /*+ STREAMTABLE(a,c) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;

SELECT /*+ STREAMTABLE(a,c) */ *
FROM T1 a JOIN T2 b ON a.key = b.key
          JOIN T3 c ON b.key = c.key
          JOIN T4 d ON c.key = d.key;

EXPLAIN FROM T1 a JOIN src c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));
FROM T1 a JOIN src c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));

EXPLAIN FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
SELECT sum(hash(Y.key)), sum(hash(Y.value));

FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key)
SELECT sum(hash(Y.key)), sum(hash(Y.value));

EXPLAIN FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key and substring(x.value, 5)=substring(y.value, 5)+1)
SELECT sum(hash(Y.key)), sum(hash(Y.value));

FROM
(SELECT src.* FROM src) x
JOIN
(SELECT src.* FROM src) Y
ON (x.key = Y.key and substring(x.value, 5)=substring(y.value, 5)+1)
SELECT sum(hash(Y.key)), sum(hash(Y.value));

EXPLAIN
SELECT sum(hash(src1.c1)), sum(hash(src2.c4))
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 = src3.c5 AND src3.c5 < 80;

SELECT sum(hash(src1.c1)), sum(hash(src2.c4))
FROM
(SELECT src.key as c1, src.value as c2 from src) src1
JOIN
(SELECT src.key as c3, src.value as c4 from src) src2
ON src1.c1 = src2.c3 AND src1.c1 < 100
JOIN
(SELECT src.key as c5, src.value as c6 from src) src3
ON src1.c1 = src3.c5 AND src3.c5 < 80;

EXPLAIN
SELECT /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) FROM T1 k LEFT OUTER JOIN T1 v ON k.key+1=v.key;
SELECT /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) FROM T1 k LEFT OUTER JOIN T1 v ON k.key+1=v.key;

select /*+ mapjoin(k)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.val;

select /*+ mapjoin(k)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.key;

select sum(hash(k.key)), sum(hash(v.val)) from T1 k join T1 v on k.key=v.key;

select count(1) from  T1 a join T1 b on a.key = b.key;

FROM T1 a LEFT OUTER JOIN T2 c ON c.key+1=a.key SELECT sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));

FROM T1 a RIGHT OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));

FROM T1 a FULL OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key));

SELECT sum(hash(src1.key)), sum(hash(src1.val)), sum(hash(src2.key)) FROM T1 src1 LEFT OUTER JOIN T2 src2 ON src1.key+1 = src2.key RIGHT OUTER JOIN T2 src3 ON src2.key = src3.key;

SELECT sum(hash(src1.key)), sum(hash(src1.val)), sum(hash(src2.key)) FROM T1 src1 JOIN T2 src2 ON src1.key+1 = src2.key JOIN T2 src3 ON src2.key = src3.key;

select /*+ mapjoin(v)*/ sum(hash(k.key)), sum(hash(v.val)) from T1 k left outer join T1 v on k.key+1=v.key;






set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- a simple join query with skew on both the tables on the join key
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- test outer joins also

EXPLAIN
SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- an aggregation at the end should not change anything

EXPLAIN
SELECT count(1) FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT count(1) FROM T1 a JOIN T2 b ON a.key = b.key;

EXPLAIN
SELECT count(1) FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT count(1) FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, value STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

drop table array_valued_T1;
create table array_valued_T1 (key string, value array<string>) SKEWED BY (key) ON ((8));
insert overwrite table array_valued_T1 select key, array(value) from T1;

-- This test is to verify the skew join compile optimization when the join is followed by a lateral view
-- adding a order by at the end to make the results deterministic

explain
select * from (select a.key as key, b.value as array_val from T1 a join array_valued_T1 b on a.key=b.key) i lateral view explode (array_val) c as val;

select * from (select a.key as key, b.value as array_val from T1 a join array_valued_T1 b on a.key=b.key) i lateral view explode (array_val) c as val
ORDER BY key, val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- This test is to verify the skew join compile optimization when the join is followed
-- by a union. Both sides of a union consist of a join, which should have used
-- skew join compile time optimization.
-- adding an order by at the end to make the results deterministic

EXPLAIN
select * from
(
  select a.key, a.val as val1, b.val as val2 from T1 a join T2 b on a.key = b.key
    union all
  select a.key, a.val as val1, b.val as val2 from T1 a join T2 b on a.key = b.key
) subq1;

select * from
(
  select a.key, a.val as val1, b.val as val2 from T1 a join T2 b on a.key = b.key
    union all
  select a.key, a.val as val1, b.val as val2 from T1 a join T2 b on a.key = b.key
) subq1
ORDER BY key, val1, val2;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key, val) ON ((2, 12), (8, 18)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key, val) ON ((3, 13), (8, 18)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- Both the join tables are skewed by 2 keys, and one of the skewed values
-- is common to both the tables. The join key matches the skewed key set.
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

CREATE TABLE T3(key STRING, val STRING)
SKEWED BY (val) ON ((12)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- This test is for skewed join compile time optimization for more than 2 tables.
-- The join key for table 3 is different from the join key used for joining
-- tables 1 and 2. Table 3 is skewed, but since one of the join sources for table
-- 3 consist of a sub-query which contains a join, the compile time skew join
-- optimization is not performed
-- adding a order by at the end to make the results deterministic

EXPLAIN
select *
from
T1 a join T2 b on a.key = b.key
join T3 c on a.val = c.val;

select *
from
T1 a join T2 b on a.key = b.key
join T3 c on a.val = c.val
order by a.key, b.key, c.key, a.val, b.val, c.val;

set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

CREATE TABLE T3(key STRING, val STRING)
SKEWED BY (val) ON ((12)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- This test is for skewed join compile time optimization for more than 2 tables.
-- The join key for table 3 is different from the join key used for joining
-- tables 1 and 2. Tables 1 and 3 are skewed. Since one of the join sources for table
-- 3 consist of a sub-query which contains a join, the compile time skew join
-- optimization is not enabled for table 3, but it is used for the first join between
-- tables 1 and 2
-- adding a order by at the end to make the results deterministic

EXPLAIN
select *
from
T1 a join T2 b on a.key = b.key
join T3 c on a.val = c.val;

select *
from
T1 a join T2 b on a.key = b.key
join T3 c on a.val = c.val
order by a.key, b.key, a.val, b.val;

set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE tmpT1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE tmpT1;

-- testing skew on other data types - int
CREATE TABLE T1(key INT, val STRING) SKEWED BY (key) ON ((2));
INSERT OVERWRITE TABLE T1 SELECT key, val FROM tmpT1;

CREATE TABLE tmpT2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE tmpT2;

CREATE TABLE T2(key INT, val STRING) SKEWED BY (key) ON ((3));

INSERT OVERWRITE TABLE T2 SELECT key, val FROM tmpT2;

-- The skewed key is a integer column.
-- Otherwise this test is similar to skewjoinopt1.q
-- Both the joined tables are skewed, and the joined column
-- is an integer
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- test outer joins also

EXPLAIN
SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- an aggregation at the end should not change anything

EXPLAIN
SELECT count(1) FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT count(1) FROM T1 a JOIN T2 b ON a.key = b.key;

EXPLAIN
SELECT count(1) FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT count(1) FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key, val) ON ((2, 12)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- One of the tables is skewed by 2 columns, and the other table is
-- skewed by one column. Ths join is performed on the both the columns
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key, val) ON ((2, 12)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- One of the tables is skewed by 2 columns, and the other table is
-- skewed by one column. Ths join is performed on the first skewed column
-- The skewed value for the jon key is common to both the tables.
-- In this case, the skewed join value is not repeated in the filter.
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

DROP TABLE T1;
DROP TABLE T2;


CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key, val) ON ((2, 12)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- One of the tables is skewed by 2 columns, and the other table is
-- skewed by one column. Ths join is performed on the both the columns
-- In this case, the skewed join value is repeated in the filter.

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE tmpT1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE tmpT1;

-- testing skew on other data types - int
CREATE TABLE T1(key INT, val STRING) SKEWED BY (key) ON ((2));
INSERT OVERWRITE TABLE T1 SELECT key, val FROM tmpT1;

-- Tke skewed column is same in both the tables, however it is
-- INT in one of the tables, and STRING in the other table

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- Once HIVE-3445 is fixed, the compile time skew join optimization would be
-- applicable here. Till the above jira is fixed, it would be performed as a
-- regular join
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) INTO 4 BUCKETS
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- add a test where the skewed key is also the bucketized key
-- it should not matter, and the compile time skewed join
-- optimization is performed
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

-- SORT_QUERY_RESULTS

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2), (7)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- a simple query with skew on both the tables on the join key
-- multiple skew values are present for the skewed keys
-- but the skewed values do not overlap.
-- The join values are a superset of the skewed keys.
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val
ORDER BY a.key, b.key, a.val, b.val;

-- test outer joins also

EXPLAIN
SELECT a.*, b.* FROM T1 a LEFT OUTER JOIN T2 b ON a.key = b.key and a.val = b.val;

SELECT a.*, b.* FROM T1 a LEFT OUTER JOIN T2 b ON a.key = b.key and a.val = b.val
ORDER BY a.key, b.key, a.val, b.val;

-- a group by at the end should not change anything

EXPLAIN
SELECT a.key, count(1) FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val group by a.key;

SELECT a.key, count(1) FROM T1 a JOIN T2 b ON a.key = b.key and a.val = b.val group by a.key;

EXPLAIN
SELECT a.key, count(1) FROM T1 a LEFT OUTER JOIN T2 b ON a.key = b.key and a.val = b.val group by a.key;

SELECT a.key, count(1) FROM T1 a LEFT OUTER JOIN T2 b ON a.key = b.key and a.val = b.val group by a.key;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- add a test where the skewed key is also the bucketized/sorted key
-- it should not matter, and the compile time skewed join
-- optimization is performed
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- a simple join query with skew on both the tables on the join key
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.*
FROM
  (SELECT key as k, val as v FROM T1) a
  JOIN
  (SELECT key as k, val as v FROM T2) b
ON a.k = b.k;

SELECT a.*, b.*
FROM
  (SELECT key as k, val as v FROM T1) a
  JOIN
  (SELECT key as k, val as v FROM T2) b
ON a.k = b.k
ORDER BY a.k, b.k, a.v, b.v;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- a simple query with skew on both the tables. One of the skewed
-- value is common to both the tables. The skewed value should not be
-- repeated in the filter.
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- test outer joins also

EXPLAIN
SELECT a.*, b.* FROM T1 a FULL OUTER JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a FULL OUTER JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- only of the tables of the join (the left table of the join) is skewed
-- the skewed filter would still be applied to both the tables
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- the order of the join should not matter, just confirming
EXPLAIN
SELECT a.*, b.* FROM T2 a JOIN T1 b ON a.key = b.key;

SELECT a.*, b.* FROM T2 a JOIN T1 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key, val) ON ((2, 12)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- One of the tables is skewed by 2 columns, and the other table is
-- skewed by one column. Ths join is performed on the first skewed column
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key, val) ON ((2, 12), (8, 18)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key, val) ON ((3, 13), (8, 18)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- Both the join tables are skewed by 2 keys, and one of the skewed values
-- is common to both the tables. The join key is a subset of the skewed key set:
-- it only contains the first skewed key for both the tables
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- This test is for validating skewed join compile time optimization for more than
-- 2 tables. The join key is the same, and so a 3-way join would be performed.
-- 2 of the 3 tables are skewed on the join key
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.*, c.* FROM T1 a JOIN T2 b ON a.key = b.key JOIN T3 c on a.key = c.key;

SELECT a.*, b.*, c.* FROM T1 a JOIN T2 b ON a.key = b.key JOIN T3 c on a.key = c.key
ORDER BY a.key, b.key, c.key, a.val, b.val, c.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- This test is for validating skewed join compile time optimization for more than
-- 2 tables. The join key is the same, and so a 3-way join would be performed.
-- 1 of the 3 tables are skewed on the join key
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.*, c.* FROM T1 a JOIN T2 b ON a.key = b.key JOIN T3 c on a.key = c.key;

SELECT a.*, b.*, c.* FROM T1 a JOIN T2 b ON a.key = b.key JOIN T3 c on a.key = c.key
ORDER BY a.key, b.key, c.key, a.val, b.val, c.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- no skew join compile time optimization would be performed if one of the
-- join sources is a sub-query consisting of a union all
-- adding a order by at the end to make the results deterministic
EXPLAIN
select * from
(
select key, val from T1
  union all
select key, val from T1
) subq1
join T2 b on subq1.key = b.key;

select * from
(
select key, val from T1
  union all
select key, val from T1
) subq1
join T2 b on subq1.key = b.key
ORDER BY subq1.key, b.key, subq1.val, b.val;

-- no skew join compile time optimization would be performed if one of the
-- join sources is a sub-query consisting of a group by
EXPLAIN
select * from
(
select key, count(1) as cnt from T1 group by key
) subq1
join T2 b on subq1.key = b.key;

select * from
(
select key, count(1) as cnt from T1 group by key
) subq1
join T2 b on subq1.key = b.key
ORDER BY subq1.key, b.key, subq1.cnt, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- copy from skewjoinopt1
-- test compile time skew join and auto map join
-- a simple join query with skew on both the tables on the join key
-- adding an order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- test outer joins also

EXPLAIN
SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- an aggregation at the end should not change anything

EXPLAIN
SELECT count(1) FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT count(1) FROM T1 a JOIN T2 b ON a.key = b.key;

EXPLAIN
SELECT count(1) FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT count(1) FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE tmpT1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE tmpT1;

-- testing skew on other data types - int
CREATE TABLE T1(key INT, val STRING) SKEWED BY (key) ON ((2));
INSERT OVERWRITE TABLE T1 SELECT key, val FROM tmpT1;

CREATE TABLE tmpT2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE tmpT2;

CREATE TABLE T2(key INT, val STRING) SKEWED BY (key) ON ((3));

INSERT OVERWRITE TABLE T2 SELECT key, val FROM tmpT2;

-- copy from skewjoinopt15
-- test compile time skew join and auto map join
-- The skewed key is a integer column.
-- Otherwise this test is similar to skewjoinopt1.q
-- Both the joined tables are skewed, and the joined column
-- is an integer
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- test outer joins also

EXPLAIN
SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- an aggregation at the end should not change anything

EXPLAIN
SELECT count(1) FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT count(1) FROM T1 a JOIN T2 b ON a.key = b.key;

EXPLAIN
SELECT count(1) FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT count(1) FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING)
CLUSTERED BY (key) INTO 4 BUCKETS
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- copy from skewjoinopt19
-- test compile time skew join and auto map join
-- add a test where the skewed key is also the bucketized key
-- it should not matter, and the compile time skewed join
-- optimization is performed
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- copy from skewjoinopt3
-- test compile time skew join and auto map join
-- a simple query with skew on both the tables. One of the skewed
-- value is common to both the tables. The skewed value should not be
-- repeated in the filter.
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- test outer joins also

EXPLAIN
SELECT a.*, b.* FROM T1 a FULL OUTER JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a FULL OUTER JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key, val) ON ((2, 12), (8, 18)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key, val) ON ((3, 13), (8, 18)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- copy from skewjoinopt6
-- test compile time skew join and auto map join
-- Both the join tables are skewed by 2 keys, and one of the skewed values
-- is common to both the tables. The join key is a subset of the skewed key set:
-- it only contains the first skewed key for both the tables
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- copy from skewjoinopt7
-- test compile time skew join and auto map join
-- This test is for validating skewed join compile time optimization for more than
-- 2 tables. The join key is the same, and so a 3-way join would be performed.
-- 2 of the 3 tables are skewed on the join key
-- adding a order by at the end to make the results deterministic

EXPLAIN
SELECT a.*, b.*, c.* FROM T1 a JOIN T2 b ON a.key = b.key JOIN T3 c on a.key = c.key;

SELECT a.*, b.*, c.* FROM T1 a JOIN T2 b ON a.key = b.key JOIN T3 c on a.key = c.key
ORDER BY a.key, b.key, c.key, a.val, b.val, c.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- copy from skewjoinopt9
-- test compile time skew join and auto map join
-- no skew join compile time optimization would be performed if one of the
-- join sources is a sub-query consisting of a union all
-- adding a order by at the end to make the results deterministic
EXPLAIN
select * from
(
select key, val from T1
  union all
select key, val from T1
) subq1
join T2 b on subq1.key = b.key;

select * from
(
select key, val from T1
  union all
select key, val from T1
) subq1
join T2 b on subq1.key = b.key
ORDER BY subq1.key, b.key, subq1.val, b.val;

-- no skew join compile time optimization would be performed if one of the
-- join sources is a sub-query consisting of a group by
EXPLAIN
select * from
(
select key, count(1) as cnt from T1 group by key
) subq1
join T2 b on subq1.key = b.key;

select * from
(
select key, count(1) as cnt from T1 group by key
) subq1
join T2 b on subq1.key = b.key
ORDER BY subq1.key, b.key, subq1.cnt, b.val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, value STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

drop table array_valued_T1;
create table array_valued_T1 (key string, value array<string>) SKEWED BY (key) ON ((8));
insert overwrite table array_valued_T1 select key, array(value) from T1;

-- copy from skewjoinopt10
-- test compile time skew join and auto map join
-- This test is to verify the skew join compile optimization when the join is followed by a lateral view
-- adding a order by at the end to make the results deterministic

explain
select * from (select a.key as key, b.value as array_val from T1 a join array_valued_T1 b on a.key=b.key) i lateral view explode (array_val) c as val;

select * from (select a.key as key, b.value as array_val from T1 a join array_valued_T1 b on a.key=b.key) i lateral view explode (array_val) c as val
ORDER BY key, val;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- copy from skewjoinopt11
-- test compile time skew join and auto map join
-- This test is to verify the skew join compile optimization when the join is followed
-- by a union. Both sides of a union consist of a join, which should have used
-- skew join compile time optimization.
-- adding an order by at the end to make the results deterministic

EXPLAIN
select * from
(
  select a.key, a.val as val1, b.val as val2 from T1 a join T2 b on a.key = b.key
    union all
  select a.key, a.val as val1, b.val as val2 from T1 a join T2 b on a.key = b.key
) subq1;

select * from
(
  select a.key, a.val as val1, b.val as val2 from T1 a join T2 b on a.key = b.key
    union all
  select a.key, a.val as val1, b.val as val2 from T1 a join T2 b on a.key = b.key
) subq1
ORDER BY key, val1, val2;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

CREATE TABLE T3(key STRING, val STRING)
SKEWED BY (val) ON ((12)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- copy from skewjoinopt13
-- test compile time skew join and auto map join
-- This test is for skewed join compile time optimization for more than 2 tables.
-- The join key for table 3 is different from the join key used for joining
-- tables 1 and 2. Table 3 is skewed, but since one of the join sources for table
-- 3 consist of a sub-query which contains a join, the compile time skew join
-- optimization is not performed
-- adding a order by at the end to make the results deterministic

EXPLAIN
select *
from
T1 a join T2 b on a.key = b.key
join T3 c on a.val = c.val;

select *
from
T1 a join T2 b on a.key = b.key
join T3 c on a.val = c.val
order by a.key, b.key, c.key, a.val, b.val, c.val;

set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;
set hive.auto.convert.join=true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

CREATE TABLE T3(key STRING, val STRING)
SKEWED BY (val) ON ((12)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- copy from skewjoinopt14
-- test compile time skew join and auto map join
-- This test is for skewed join compile time optimization for more than 2 tables.
-- The join key for table 3 is different from the join key used for joining
-- tables 1 and 2. Tables 1 and 3 are skewed. Since one of the join sources for table
-- 3 consist of a sub-query which contains a join, the compile time skew join
-- optimization is not enabled for table 3, but it is used for the first join between
-- tables 1 and 2
-- adding a order by at the end to make the results deterministic

EXPLAIN
select *
from
T1 a join T2 b on a.key = b.key
join T3 c on a.val = c.val;

select *
from
T1 a join T2 b on a.key = b.key
join T3 c on a.val = c.val
order by a.key, b.key, a.val, b.val;

set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=false;
set hive.optimize.skewjoin=true;

explain
create table noskew as select a.* from src a join src b on a.key=b.key order by a.key limit 30;

create table noskew as select a.* from src a join src b on a.key=b.key order by a.key limit 30;

select * from noskew;
set hive.auto.convert.join=false;
set hive.optimize.skewjoin=true;
set hive.skewjoin.key=2;
set hive.mapred.mode=nonstrict;

DROP TABLE IF EXISTS skewtable;
CREATE TABLE skewtable (key STRING, value STRING) STORED AS TEXTFILE;
INSERT INTO TABLE skewtable VALUES ("0", "val_0");
INSERT INTO TABLE skewtable VALUES ("0", "val_0");
INSERT INTO TABLE skewtable VALUES ("0", "val_0");

DROP TABLE IF EXISTS nonskewtable;
CREATE TABLE nonskewtable (key STRING, value STRING) STORED AS TEXTFILE;
INSERT INTO TABLE nonskewtable VALUES ("1", "val_1");
INSERT INTO TABLE nonskewtable VALUES ("2", "val_2");

EXPLAIN
CREATE TABLE result AS SELECT a.* FROM skewtable a JOIN nonskewtable b ON a.key=b.key;
CREATE TABLE result AS SELECT a.* FROM skewtable a JOIN nonskewtable b ON a.key=b.key;

SELECT * FROM result;

set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink and skewjoin optimization
-- Union of 2 map-reduce subqueries is performed for the skew join
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output, it might be easier to run the test
-- only on hadoop 23

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

-- a simple join query with skew on both the tables on the join key

EXPLAIN
SELECT * FROM T1 a JOIN T2 b ON a.key = b.key;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

SELECT * FROM T1 a JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

-- test outer joins also

EXPLAIN
SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT a.*, b.* FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key
ORDER BY a.key, b.key, a.val, b.val;

create table DEST1(key1 STRING, val1 STRING, key2 STRING, val2 STRING);

EXPLAIN
INSERT OVERWRITE TABLE DEST1
SELECT * FROM T1 a JOIN T2 b ON a.key = b.key;

INSERT OVERWRITE TABLE DEST1
SELECT * FROM T1 a JOIN T2 b ON a.key = b.key;

SELECT * FROM DEST1
ORDER BY key1, key2, val1, val2;

EXPLAIN
INSERT OVERWRITE TABLE DEST1
SELECT * FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

INSERT OVERWRITE TABLE DEST1
SELECT * FROM T1 a RIGHT OUTER JOIN T2 b ON a.key = b.key;

SELECT * FROM DEST1
ORDER BY key1, key2, val1, val2;
set hive.mapred.mode=nonstrict;
set hive.optimize.skewjoin.compiletime = true;

set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.sparkfiles=false;
set mapred.input.dir.recursive=true;

CREATE TABLE T1(key STRING, val STRING)
SKEWED BY (key) ON ((2), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING)
SKEWED BY (key) ON ((3), (8)) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;

CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- This is to test the union->selectstar->filesink and skewjoin optimization
-- Union of 3 map-reduce subqueries is performed for the skew join
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table, it might be easier
-- to run the test only on hadoop 23

EXPLAIN
SELECT a.*, b.*, c.* FROM T1 a JOIN T2 b ON a.key = b.key JOIN T3 c on a.key = c.key;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

SELECT a.*, b.*, c.* FROM T1 a JOIN T2 b ON a.key = b.key JOIN T3 c on a.key = c.key
ORDER BY a.key, b.key, c.key, a.val, b.val, c.val;
DROP TABLE IF EXISTS skipHTbl;

CREATE TABLE skipHTbl (a int)
PARTITIONED BY (b int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
TBLPROPERTIES('skip.header.line.count'='1');

INSERT OVERWRITE TABLE skipHTbl PARTITION (b = 1) VALUES (1), (2), (3), (4);
INSERT OVERWRITE TABLE skipHTbl PARTITION (b = 2) VALUES (1), (2), (3), (4);

SELECT * FROM skipHTbl;

SELECT DISTINCT b FROM skipHTbl;
SELECT MAX(b) FROM skipHTbl;
SELECT DISTINCT a FROM skipHTbl;

INSERT OVERWRITE TABLE skipHTbl PARTITION (b = 1) VALUES (1);
INSERT OVERWRITE TABLE skipHTbl PARTITION (b = 2) VALUES (1), (2), (3), (4);

SELECT DISTINCT b FROM skipHTbl;
SELECT MIN(b) FROM skipHTbl;
SELECT DISTINCT a FROM skipHTbl;

DROP TABLE IF EXISTS skipFTbl;

CREATE TABLE skipFTbl (a int)
PARTITIONED BY (b int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
TBLPROPERTIES('skip.footer.line.count'='1');

INSERT OVERWRITE TABLE skipFTbl PARTITION (b = 1) VALUES (1), (2), (3), (4);
INSERT OVERWRITE TABLE skipFTbl PARTITION (b = 2) VALUES (1), (2), (3), (4);

SELECT * FROM skipFTbl;

SELECT DISTINCT b FROM skipFTbl;
SELECT MAX(b) FROM skipFTbl;
SELECT DISTINCT a FROM skipFTbl;

DROP TABLE skipHTbl;
DROP TABLE skipFTbl;



set hive.exec.reducers.max = 1;


CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS;


CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS;

insert overwrite table smb_bucket4_1
select * from src;

insert overwrite table smb_bucket4_2
select * from src;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

select /*+mapjoin(a)*/ * from smb_bucket4_1 a left outer join smb_bucket4_2 b on a.key = b.key;



create table bug_201_input_a (
       userid                                       int
) clustered by (userid) sorted by (userid) into 64 BUCKETS ;

create table bug_201_input_b (
       userid                  int
) clustered by (userid) sorted by (userid) into 64 BUCKETS ;

insert overwrite table bug_201_input_a
select distinct(userid) as userid from (
  select arr as userid from (
     select explode(array (
            12804352        ,251326720,50029057        ,251155969,60217858        ,251995906,78744835        ,250561795,13637380        ,207184132,58189573        ,251988997,
            62314246        ,251565574,63912199        ,250889479,57424648        ,208269832,39819529        ,251811337,39396106        ,250124554,25833739        ,251722507,
            48908812        ,252057100,39894541        ,251633677,20268046        ,251462926,46375183        ,251292175,64902160        ,251879440,40980241        ,204206353,
            50411026        ,251790610,54030355        ,251367187,29097748        ,205968148,12755989        ,250773013,45685270        ,208406038,47788567        ,208235287,
            57724696        ,207559192,13083673        ,206630425,41717530        ,250929946,50642971        ,250759195,43144732        ,250841116,51059485        ,250670365,
            39771166        ,250752286,53244703        ,250834207,43725088        ,207961888,46586401        ,207285793,77241634        ,251837986,19967011        ,251919907,
            37230628        ,251496484,47419429        ,251578405,50786086        ,250144294,48593959        ,251742247,44885800        ,251318824,52295209        ,251400745,
            66274090        ,251988010,28960555        ,208357675,51024940        ,206923564,32156461        ,251981101,47398702        ,252063022,48238639        ,251386927,
            77377840        ,251468848,64068145        ,207333169,16142386        ,251380018,15971635        ,251461939,60018484        ,250027828,43171381        ,250867765,
            47548726        ,250191670,37776439        ,208330039,59588152        ,251871544,75335737        ,251953465,57477946        ,251782714,63623995        ,250348603,
            57641788        ,250935868,42058045        ,250007101,59574334        ,251352382,42474559        ,250928959,52663360        ,252021568,60578113        ,251598145,
            60407362        ,251174722,55941187        ,208302403,65119300        ,251843908,61916485        ,251673157,65535814        ,207790150,62838343        ,208124743,
            26030152        ,251666248,45315145        ,204498505,59799370        ,251577418,72514891        ,250648651,64258636        ,208281676,53475661        ,251823181,
            30564430        ,207940174,32162383        ,250976335,62059600        ,251563600,64920913        ,251392849,50347858        ,250716754,40828243        ,251304019,
            44447572        ,204388948,14966869        ,251973205,59013718        ,250791766,12856663        ,251631703,40227160        ,250450264,20600665        ,252048217,
            48223834        ,251119450,76099675        ,207741787,49145692        ,250019932,50490973        ,207905629,58405726        ,250689118,47370079        ,250013023,
            44419936        ,250347616,36416353        ,250934881,62776162        ,251269474,48455779        ,251098723,43736932        ,251938660,12740197        ,251767909,
            54007654        ,252102502,49794151        ,251931751,63267688        ,252013672,28480873        ,252095593,63684202        ,251419498,76652395        ,252006763,
            64606060        ,251077996,53317741        ,251665261,21310318        ,250989166,41353327        ,208116847,52805488        ,208451440,43033201        ,251992945,
            48673906        ,251064178,15655795        ,207686515,44794996        ,251228020,14303605        ,251815285,48243574        ,251897206,65507191        ,207508855,
            63820408        ,250797688,45457273        ,251637625,62973562        ,251466874,42083707        ,252054139,63642748        ,251630716,40731517        ,251712637,
            29948542        ,252047230,40137343        ,250360447,43756672        ,251200384,13517953        ,251534977,77273218        ,251616898,54867331        ,250435459,
            46105732        ,250517380,48714373        ,208150405,79369606        ,250933894,57216391        ,250257799,20408200        ,250845064,61675657        ,206962057,
            15013258        ,250756234,54512011        ,250585483,70512268        ,251172748,78932365        ,251254669,12814222       , 251841934,49533583       , 251923855,
            44309392       , 208040848,73953937       , 250824337,56601490       , 251664274,35458963       , 208033939,50953876       , 251828116,74281621       , 204155029,
            64509334       , 250223254,44630167       , 252073879,52292248       , 207685528,63997081       , 251732377,46897306       , 250803610,60370843       , 251138203,
            65506204       , 251725468,48406429       , 205821085,15893662       , 250878622,38716063       , 251213215,35007904       , 251295136,66926497       , 251629729,
            56143522       , 251964322,73407139       , 250277539,71720356       , 251117476,80645797       , 205465765,43079590       , 250270630,75756199       , 208156327,
            58656424       , 251445160,57474985       , 251779753,45428650       , 251356330,62186923       , 250680235,54183340       , 250004140,55781293       , 207637165,
            15941038       , 251684014,14254255       , 251513263,42130096       , 251342512,34884529       , 208217521,70087858       , 251253682,50208691       , 250830259,
            54333364       , 250406836,17019829       , 251752117,75468982       , 251834038,17689015       , 208203703,22571704       , 250481848,52974265       , 250816441,
            52803514       , 251656378,61981627       , 251485627,13045180       , 250556860,71494333       , 250133437,81935806       , 251478718,50433727       , 252065983,
            26764480       , 250631872,64747201       , 251977153,64829122       , 206830786,47982019       , 251888323,64992964       , 207499972,52188613       , 251799493,
            39384262       , 251376070,41992903       , 250699975,41822152       , 250781896,39882697       , 250358473,56135626       , 251198410,35751115       , 251785675,
            75249868       , 251867596,55118029       , 207479245,70107598       , 251526094,50481103       , 207895759,55869136       , 251689936,45086161       , 251519185,
            46431442       , 251348434,53335507       , 251683027,39520468       , 251512276,53246677       , 250836181,42211030       , 251928790,56442583       , 251252695,
            75727576       , 251334616,15673561       , 250911193,42286042       , 250993114,63845083       , 251833051,37396444       , 251156956,77653213       , 251744221,
            16335838       , 251068126,61393375       , 250897375,53642464       , 250221280,58525153       , 206843617,56585698       , 207683554,30642403       , 250972387,
            63319012       , 250801636,76287205       , 207929317,44785126       , 251723494,50425831       , 251300071,45706984       , 251634664,13952233       , 251463913,
            77960170       , 251293162,27760363       , 251627755,69280492       , 206734060,13521901       , 208332013,35333614       , 250862830,56892655       , 251702767,
            49394416       , 251532016,79291633       , 250097905,50316274       , 206467570,45344755       , 251525107,17127412       , 250596340,53846773       , 206966005,
            53170678       , 251012854,46430455       , 252105463,59651320       , 251682040,54427129       , 251763961,54509050       , 252098554,65203195       , 251422459,
            40270588       , 250241020,13316605       , 208379389,45235198       , 251668222,60477439       , 251497471,
            101510977       ,114200836       ,156174985       ,60512971        ,181554703       ,
            148365841       ,52703827        ,182530846       ,149341984       ,116153122       ,
            162031843       ,141532840       ,154222699       ,109320121       ,155198842
                       )) as arr )a )b;



insert overwrite table bug_201_input_b
select distinct(userid) as userid from (
  select arr as userid from (
     select explode(array (
            55632256        ,243051712    ,39037825        ,163984129    ,22443394        ,147389698    ,68322115        ,193268419,
            51727684        ,176673988    ,35133253        ,160079557    ,81011974        ,205958278    ,64417543        ,251836999,
            47823112        ,172769416    ,31228681        ,156174985    ,14634250        ,202053706    ,60512971        ,247932427,
            43918540        ,168864844    ,89797261        ,152270413    ,73202830        ,73202830    ,56608399        ,244027855,
            40013968        ,164960272    ,85892689        ,148365841    ,69298258        ,69298258    ,52703827        ,177650131,
            36109396        ,161055700    ,19514965        ,206934421    ,65393686        ,190339990    ,48799255        ,111272407,
            94677976        ,157151128    ,15610393        ,203029849    ,61489114        ,248908570    ,44894683        ,169840987,
            90773404        ,153246556    ,11705821        ,74178973    ,57584542        ,245003998    ,40990111        ,165936415,
            24395680        ,149341984    ,70274401        ,70274401    ,53679970        ,178626274    ,37085539        ,162031843,
            145437412       ,207910564    ,66369829        ,191316133    ,49775398        ,112248550    ,33180967        ,158127271,
            79059688        ,204005992    ,62465257        ,187411561    ,45870826        ,170817130    ,29276395        ,154222699,
            12681964        ,75155116     ,58560685        ,245980141    ,41966254        ,166912558    ,150318127       ,150318127,
            8777392         ,71250544     ,54656113        ,179602417    ,38061682        ,163007986    ,146413555       ,146413555,
            67345972        ,192292276    ,50751541        ,175697845    ,34157110        ,159103414    ,17562679        ,204982135,
            63441400        ,250860856    ,46846969        ,171793273    ,30252538        ,155198842    ,76131259        ,201077563,
            59536828        ,246956284    ,42942397        ,167888701    ,26347966        ,151294270    ,9753535         ,72226687,
            101510977       ,114200836       ,156174985       ,60512971        ,181554703       ,
            148365841       ,52703827        ,182530846       ,149341984       ,116153122       ,
            162031843       ,141532840       ,154222699       ,109320121       ,155198842
        )) as arr )a ) b;

explain
select
t1.userid,
  fa.userid   as  fa_userid
  from bug_201_input_b as t1
  join bug_201_input_a as fa on (t1.userid = fa.userid) ;

select
t1.userid,
  fa.userid   as  fa_userid
  from bug_201_input_b as t1
  join bug_201_input_a as fa on (t1.userid = fa.userid) ;


set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask.size=100 ;
set hive.auto.convert.sortmerge.join=true
set hive.convert.join.bucket.mapjoin.tez = true;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

explain
select
t1.userid,
  fa.userid   as  fa_userid
  from bug_201_input_b as t1
  join bug_201_input_a as fa on (t1.userid = fa.userid) ;

select
t1.userid,
  fa.userid   as  fa_userid
  from bug_201_input_b as t1
  join bug_201_input_a as fa on (t1.userid = fa.userid) ;

set hive.mapred.mode=nonstrict;
SET hive.execution.engine=mr;
SET hive.enforce.sortmergebucketmapjoin=false;
SET hive.auto.convert.sortmerge.join=true;
SET hive.optimize.bucketmapjoin = true;
SET hive.optimize.bucketmapjoin.sortedmerge = true;
SET hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
SET hive.exec.dynamic.partition.mode=nonstrict;

CREATE TABLE data_table (key INT, value STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';

insert into table data_table values(1, 'one');
insert into table data_table values(2, 'two');

CREATE TABLE smb_table (key INT, value STRING) CLUSTERED BY (key)
SORTED BY (key) INTO 1 BUCKETS STORED AS ORC;

CREATE TABLE smb_table_part (key INT, value STRING) PARTITIONED BY (p1 DECIMAL)
CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS
STORED AS ORC;

INSERT OVERWRITE TABLE smb_table SELECT * FROM data_table;

INSERT OVERWRITE TABLE smb_table_part PARTITION (p1) SELECT key, value, 100 as p1 FROM data_table;

SELECT s1.key, s2.p1 FROM smb_table s1 INNER JOIN smb_table_part s2 ON s1.key = s2.key ORDER BY s1.key;

drop table smb_table_part;

CREATE TABLE smb_table_part (key INT, value STRING) PARTITIONED BY (p1 double)
CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS
STORED AS ORC;

INSERT OVERWRITE TABLE smb_table_part PARTITION (p1) SELECT key, value, 100 as p1 FROM data_table;

SELECT s1.key, s2.p1 FROM smb_table s1 INNER JOIN smb_table_part s2 ON s1.key = s2.key ORDER BY s1.key;create table hive_test_smb_bucket1 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;
create table hive_test_smb_bucket2 (key int, value string) partitioned by (ds string) clustered by (key) sorted by (key) into 2 buckets;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- empty partitions (HIVE-3205)
explain extended
SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;

SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;

explain extended
SELECT /* + MAPJOIN(a) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;

SELECT /* + MAPJOIN(a) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;

;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.exec.reducers.max = 1;

insert overwrite table hive_test_smb_bucket1 partition (ds='2010-10-15') select key, value from src;
insert overwrite table hive_test_smb_bucket2 partition (ds='2010-10-15') select key, value from src;

set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
create table smb_mapjoin9_results as
SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;


create table smb_mapjoin9_results as
SELECT /* + MAPJOIN(b) */ b.key as k1, b.value, b.ds, a.key as k2
FROM hive_test_smb_bucket1 a JOIN
hive_test_smb_bucket2 b
ON a.key = b.key WHERE a.ds = '2010-10-15' and b.ds='2010-10-15' and  b.key IS NOT NULL;

drop table smb_mapjoin9_results;
drop table hive_test_smb_bucket1;
drop table hive_test_smb_bucket2;



create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;

load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- SORT_QUERY_RESULTS

explain
select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;


explain
select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;






create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE;

alter table tmp_smb_bucket_10 add partition (ds = '1');
alter table tmp_smb_bucket_10 add partition (ds = '2');

-- add dummy files to make sure that the number of files in each partition is same as number of buckets

load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1');
load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1');

load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2');
load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2');

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
select /*+mapjoin(a)*/ * from tmp_smb_bucket_10 a join tmp_smb_bucket_10 b
on (a.ds = '1' and b.ds = '2' and
    a.userid = b.userid and
    a.pageid = b.pageid and
    a.postid = b.postid and
    a.type = b.type);

set hive.mapred.mode=nonstrict;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- This test verifies that the output of a sort merge join on 2 partitions (one on each side of the join) is bucketed

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *;




-- Create a bucketed table
CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) INTO 16 BUCKETS;

-- Insert data into the bucketed table by joining the two bucketed and sorted tables, bucketing is not enforced
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1';

SELECT * FROM test_table1 ORDER BY key;
SELECT * FROM test_table3 ORDER BY key;
EXPLAIN EXTENDED SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16);
EXPLAIN EXTENDED SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16);
SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16);
SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16);

-- Join data from a sampled bucket to verify the data is bucketed
SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1';

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- This test verifies that the output of a sort merge join on 1 big partition with multiple small partitions is bucketed and sorted

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2') SELECT *
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '3') SELECT *;




-- Create a bucketed table
CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;

-- Insert data into the bucketed table by joining the two bucketed and sorted tables, bucketing is not enforced
EXPLAIN EXTENDED
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1';

-- Join data from a sampled bucket to verify the data is bucketed
SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1';

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- Join data from the sampled buckets of 2 tables to verify the data is bucketed and sorted
explain extended
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2')
SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2')
SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1';

SELECT count(*) from test_table3 tablesample (bucket 2 out of 16) a where ds = '2';set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- This test verifies that the sort merge join optimizer works when the tables are joined on columns with different names

-- Create bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC) INTO 16 BUCKETS;
CREATE TABLE test_table2 (value INT, key STRING) CLUSTERED BY (value) SORTED BY (value ASC) INTO 16 BUCKETS;
CREATE TABLE test_table3 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS;
CREATE TABLE test_table4 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (value ASC, key ASC) INTO 16 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 SELECT *
INSERT OVERWRITE TABLE test_table2 SELECT *
INSERT OVERWRITE TABLE test_table3 SELECT *
INSERT OVERWRITE TABLE test_table4 SELECT *;

-- Join data from 2 tables on their respective sorted columns (one each, with different names) and
-- verify sort merge join is used
EXPLAIN EXTENDED
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10;

SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10;

-- Join data from 2 tables on their respective columns (two each, with the same names but sorted
-- with different priorities) and verify sort merge join is not used
EXPLAIN EXTENDED
SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10;

SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 1;

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1
select * from src where key < 10;

insert overwrite table tbl2
select * from src where key < 10;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- The mapjoin is being performed as part of sub-query. It should be converted to a sort-merge join
explain
select count(*) from (
  select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

select count(*) from (
  select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

-- The mapjoin is being performed as part of sub-query. It should be converted to a sort-merge join
-- Add a order by at the end to make the results deterministic.
explain
select key, count(*) from
(
  select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
group by key
order by key;

select key, count(*) from
(
  select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
group by key
order by key;

-- The mapjoin is being performed as part of more than one sub-query. It should be converted to a sort-merge join
explain
select count(*) from
(
  select key, count(*) from
  (
    select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

select count(*) from
(
  select key, count(*) from
  (
    select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

-- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join.
explain
select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

-- The subquery itself is being map-joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join, although there is more than one level of sub-query
explain
select /*+mapjoin(subq2)*/ count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

select /*+mapjoin(subq2)*/ count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

-- Both the big table and the small table are nested sub-queries i.e more then 1 level of sub-query.
-- The join should be converted to a sort-merge join
explain
select /*+mapjoin(subq2)*/ count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

select /*+mapjoin(subq2)*/ count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

-- The subquery itself is being map-joined. Since the sub-query only contains selects and filters and the join key
-- is not getting modified, it should be converted to a sort-merge join. Note that the sub-query modifies one
-- item, but that is not part of the join key.
explain
select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

-- Since the join key is modified by the sub-query, neither sort-merge join not bucketized map-side
-- join should be performed
explain
select /*+mapjoin(subq1)*/ count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

select /*+mapjoin(subq1)*/ count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

-- The small table is a sub-query and the big table is not.
-- It should be converted to a sort-merge join.
explain
select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

select /*+mapjoin(subq1)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

-- The big table is a sub-query and the small table is not.
-- It should be converted to a sort-merge join.
explain
select /*+mapjoin(a)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

select /*+mapjoin(a)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

-- There are more than 2 inputs to the join, all of them being sub-queries.
-- It should be converted to to a sort-merge join
explain
select /*+mapjoin(subq1, subq2)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on (subq1.key = subq2.key)
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

select /*+mapjoin(subq1, subq2)*/ count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

-- The mapjoin is being performed on a nested sub-query, and an aggregation is performed after that.
-- The join should be converted to a sort-merge join
explain
select count(*) from (
  select /*+mapjoin(subq2)*/ subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

select count(*) from (
  select /*+mapjoin(subq2)*/ subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;
set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 1;

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

insert overwrite table tbl1
select * from src where key < 10;

insert overwrite table tbl2
select * from src where key < 10;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- A join is being performed across different sub-queries, where a mapjoin is being performed in each of them.
-- Each sub-query should be converted to a sort-merge join.
-- A join followed by mapjoin is not allowed, so this query should fail.
-- Once HIVE-3403 is in, this should be automatically converted to a sort-merge join without the hint
explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select /*+mapjoin(a)*/ a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key
order by src1.key, src1.cnt1, src2.cnt1;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- This test verifies that the sort merge join optimizer works when the tables are sorted on columns which is a superset
-- of join columns

-- Create bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 SELECT *
INSERT OVERWRITE TABLE test_table2 SELECT *;

-- it should be converted to a sort-merge join, since the first sort column (#join columns = 1) contains the join columns
EXPLAIN EXTENDED
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.key ORDER BY a.key LIMIT 10;
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.key ORDER BY a.key LIMIT 10;

DROP TABLE test_table1;
DROP TABLE test_table2;

-- Create bucketed and sorted tables
CREATE TABLE test_table1 (key INT, key2 INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC, key2 ASC, value ASC) INTO 16 BUCKETS;
CREATE TABLE test_table2 (key INT, key2 INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC, key2 ASC, value ASC) INTO 16 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 SELECT key, key, value
INSERT OVERWRITE TABLE test_table2 SELECT key, key, value;

-- it should be converted to a sort-merge join, since the first 2 sort columns (#join columns = 2) contain the join columns
EXPLAIN EXTENDED
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.key and a.key2 = b.key2 ORDER BY a.key LIMIT 10;
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.key and a.key2 = b.key2 ORDER BY a.key LIMIT 10;

-- it should be converted to a sort-merge join, since the first 2 sort columns (#join columns = 2) contain the join columns
-- even if the order is not the same
EXPLAIN EXTENDED
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key2 = b.key2 and a.key = b.key ORDER BY a.key LIMIT 10;
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key2 = b.key2 and a.key = b.key ORDER BY a.key LIMIT 10;

-- it should not be converted to a sort-merge join, since the first 2 sort columns (#join columns = 2) do not contain all
-- the join columns
EXPLAIN EXTENDED
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.key and a.value = b.value ORDER BY a.key LIMIT 10;
SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.key and a.value = b.value ORDER BY a.key LIMIT 10;

DROP TABLE test_table1;
DROP TABLE test_table2;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Create bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 SELECT *
INSERT OVERWRITE TABLE test_table2 SELECT *;

-- Mapjoin followed by a aggregation should be performed in a single MR job
EXPLAIN
SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key;
SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Create bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table3 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table4 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table5 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table6 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table7 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table8 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

INSERT OVERWRITE TABLE test_table1
SELECT * FROM src WHERE key < 10;

INSERT OVERWRITE TABLE test_table2
SELECT * FROM src  WHERE key < 10;

INSERT OVERWRITE TABLE test_table3
SELECT * FROM src WHERE key < 10;

INSERT OVERWRITE TABLE test_table4
SELECT * FROM src  WHERE key < 10;

INSERT OVERWRITE TABLE test_table5
SELECT * FROM src WHERE key < 10;

INSERT OVERWRITE TABLE test_table6
SELECT * FROM src  WHERE key < 10;

INSERT OVERWRITE TABLE test_table7
SELECT * FROM src WHERE key < 10;

INSERT OVERWRITE TABLE test_table8
SELECT * FROM src  WHERE key < 10;

-- Mapjoin followed by a aggregation should be performed in a single MR job upto 7 tables
EXPLAIN
SELECT /*+ mapjoin(b, c, d, e, f, g) */ count(*)
FROM test_table1 a JOIN test_table2 b ON a.key = b.key
JOIN test_table3 c ON a.key = c.key
JOIN test_table4 d ON a.key = d.key
JOIN test_table5 e ON a.key = e.key
JOIN test_table6 f ON a.key = f.key
JOIN test_table7 g ON a.key = g.key;

SELECT /*+ mapjoin(b, c, d, e, f, g) */ count(*)
FROM test_table1 a JOIN test_table2 b ON a.key = b.key
JOIN test_table3 c ON a.key = c.key
JOIN test_table4 d ON a.key = d.key
JOIN test_table5 e ON a.key = e.key
JOIN test_table6 f ON a.key = f.key
JOIN test_table7 g ON a.key = g.key;

set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;

-- It should be automatically converted to a sort-merge join followed by a groupby in
-- a single MR job
EXPLAIN
SELECT count(*)
FROM test_table1 a LEFT OUTER JOIN test_table2 b ON a.key = b.key
LEFT OUTER JOIN test_table3 c ON a.key = c.key
LEFT OUTER JOIN test_table4 d ON a.key = d.key
LEFT OUTER JOIN test_table5 e ON a.key = e.key
LEFT OUTER JOIN test_table6 f ON a.key = f.key
LEFT OUTER JOIN test_table7 g ON a.key = g.key;

SELECT count(*)
FROM test_table1 a LEFT OUTER JOIN test_table2 b ON a.key = b.key
LEFT OUTER JOIN test_table3 c ON a.key = c.key
LEFT OUTER JOIN test_table4 d ON a.key = d.key
LEFT OUTER JOIN test_table5 e ON a.key = e.key
LEFT OUTER JOIN test_table6 f ON a.key = f.key
LEFT OUTER JOIN test_table7 g ON a.key = g.key;

EXPLAIN
SELECT count(*)
FROM test_table1 a LEFT OUTER JOIN test_table2 b ON a.key = b.key
LEFT OUTER JOIN test_table3 c ON a.key = c.key
LEFT OUTER JOIN test_table4 d ON a.key = d.key
LEFT OUTER JOIN test_table5 e ON a.key = e.key
LEFT OUTER JOIN test_table6 f ON a.key = f.key
LEFT OUTER JOIN test_table7 g ON a.key = g.key
LEFT OUTER JOIN test_table8 h ON a.key = h.key;

SELECT count(*)
FROM test_table1 a LEFT OUTER JOIN test_table2 b ON a.key = b.key
LEFT OUTER JOIN test_table3 c ON a.key = c.key
LEFT OUTER JOIN test_table4 d ON a.key = d.key
LEFT OUTER JOIN test_table5 e ON a.key = e.key
LEFT OUTER JOIN test_table6 f ON a.key = f.key
LEFT OUTER JOIN test_table7 g ON a.key = g.key
LEFT OUTER JOIN test_table8 h ON a.key = h.key;

-- outer join with max 16 aliases
EXPLAIN
SELECT a.*
FROM test_table1 a
LEFT OUTER JOIN test_table2 b ON a.key = b.key
LEFT OUTER JOIN test_table3 c ON a.key = c.key
LEFT OUTER JOIN test_table4 d ON a.key = d.key
LEFT OUTER JOIN test_table5 e ON a.key = e.key
LEFT OUTER JOIN test_table6 f ON a.key = f.key
LEFT OUTER JOIN test_table7 g ON a.key = g.key
LEFT OUTER JOIN test_table8 h ON a.key = h.key
LEFT OUTER JOIN test_table4 i ON a.key = i.key
LEFT OUTER JOIN test_table5 j ON a.key = j.key
LEFT OUTER JOIN test_table6 k ON a.key = k.key
LEFT OUTER JOIN test_table7 l ON a.key = l.key
LEFT OUTER JOIN test_table8 m ON a.key = m.key
LEFT OUTER JOIN test_table7 n ON a.key = n.key
LEFT OUTER JOIN test_table8 o ON a.key = o.key
LEFT OUTER JOIN test_table4 p ON a.key = p.key
LEFT OUTER JOIN test_table5 q ON a.key = q.key
LEFT OUTER JOIN test_table6 r ON a.key = r.key
LEFT OUTER JOIN test_table7 s ON a.key = s.key
LEFT OUTER JOIN test_table8 t ON a.key = t.key;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

select count(*) from test_table1 where ds = '1';
select count(*) from test_table1 where ds = '1' and hash(key) % 2 = 0;
select count(*) from test_table1 where ds = '1' and hash(key) % 2 = 1;
select count(*) from test_table1 tablesample (bucket 1 out of 2) s where ds = '1';
select count(*) from test_table1 tablesample (bucket 2 out of 2) s where ds = '1';

select count(*) from test_table2 where ds = '1';
select count(*) from test_table2 where ds = '1' and hash(key) % 2 = 0;
select count(*) from test_table2 where ds = '1' and hash(key) % 2 = 1;
select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '1';
select count(*) from test_table2 tablesample (bucket 2 out of 2) s where ds = '1';

set hive.optimize.constant.propagation=false;
-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation, one of the buckets should be empty

EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1' and a.key = 238;

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1' and a.key = 238;

set hive.optimize.constant.propagation=true;
select count(*) from test_table2 where ds = '2';
select count(*) from test_table2 where ds = '2' and hash(key) % 2 = 0;
select count(*) from test_table2 where ds = '2' and hash(key) % 2 = 1;
select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '2';
select count(*) from test_table2 tablesample (bucket 2 out of 2) s where ds = '2';

EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '3')
SELECT a.key, a.value FROM test_table2 a WHERE a.ds = '2';

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2')
SELECT a.key, a.value FROM test_table2 a WHERE a.ds = '2';

select count(*) from test_table2 where ds = '3';
select count(*) from test_table2 where ds = '3' and hash(key) % 2 = 0;
select count(*) from test_table2 where ds = '3' and hash(key) % 2 = 1;
select count(*) from test_table2 tablesample (bucket 1 out of 2) s where ds = '3';
select count(*) from test_table2 tablesample (bucket 2 out of 2) s where ds = '3';
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

select count(*) from test_table1 where ds = '1';
select count(*) from test_table1 where ds = '1' and hash(key) % 16 = 0;
select count(*) from test_table1 where ds = '1' and hash(key) % 16 = 5;
select count(*) from test_table1 where ds = '1' and hash(key) % 16 = 12;
select count(*) from test_table1 tablesample (bucket 1 out of 16) s where ds = '1';
select count(*) from test_table1 tablesample (bucket 6 out of 16) s where ds = '1';
select count(*) from test_table1 tablesample (bucket 13 out of 16) s where ds = '1';

select count(*) from test_table2 where ds = '1';
select count(*) from test_table2 where ds = '1' and hash(key) % 16 = 0;
select count(*) from test_table2 where ds = '1' and hash(key) % 16 = 5;
select count(*) from test_table2 where ds = '1' and hash(key) % 16 = 12;
select count(*) from test_table2 tablesample (bucket 1 out of 16) s where ds = '1';
select count(*) from test_table2 tablesample (bucket 6 out of 16) s where ds = '1';
select count(*) from test_table2 tablesample (bucket 13 out of 16) s where ds = '1';




create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;

load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- SORT_QUERY_RESULTS

explain
select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;


explain
select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;





set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key int, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key STRING, value1 STRING, value2 string) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *;

-- Insert data into the bucketed table by selecting from another bucketed table
-- with different datatypes. This should be a map-reduce operation
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value, a.value FROM test_table1 a WHERE a.ds = '1';

INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value, a.value FROM test_table1 a WHERE a.ds = '1';

select count(*) from test_table2 where ds = '1';
select count(*) from test_table2 where ds = '1' and hash(key) % 2 = 0;
select count(*) from test_table2 where ds = '1' and hash(key) % 2 = 1;

CREATE TABLE test_table3 (key STRING, value1 int, value2 string) PARTITIONED BY (ds STRING)
CLUSTERED BY (value1) SORTED BY (value1) INTO 2 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation, although the bucketing positions dont match
EXPLAIN
INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.value, a.key, a.value FROM test_table1 a WHERE a.ds = '1';

INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1')
SELECT a.value, a.key, a.value FROM test_table1 a WHERE a.ds = '1';

select count(*) from test_table3 where ds = '1';
select count(*) from test_table3 where ds = '1' and hash(value1) % 2 = 0;
select count(*) from test_table3 where ds = '1' and hash(value1) % 2 = 1;
select count(*) from test_table3 tablesample (bucket 1 out of 2) s where ds = '1';
select count(*) from test_table3 tablesample (bucket 2 out of 2) s where ds = '1';

-- Insert data into the bucketed table by selecting from another bucketed table
-- However, since an expression is being selected, it should involve a reducer
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2')
SELECT a.key+a.key, a.value, a.value FROM test_table1 a WHERE a.ds = '1';
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

drop table test_table2;

CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key desc) INTO 2 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation since the sort orders does not match
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

drop table test_table2;

CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key, value) INTO 2 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation since the sort columns do not match
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

drop table test_table2;

CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (value) INTO 2 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation since the sort columns do not match
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

drop table test_table2;

CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation since the number of buckets do not match
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';

drop table test_table2;

CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING)
CLUSTERED BY (key) INTO 2 BUCKETS;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-reduce operation since sort columns do not match
EXPLAIN
INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1')
SELECT a.key, a.value FROM test_table1 a WHERE a.ds = '1';
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;


set hive.exec.reducers.max = 1;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

-- Create two bucketed and sorted tables
CREATE TABLE test_table1 (key INT, value STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING)
CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 SELECT *;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN INSERT OVERWRITE TABLE test_table2
SELECT * FROM test_table1;

INSERT OVERWRITE TABLE test_table2
SELECT * FROM test_table1;

select count(*) from test_table1;
select count(*) from test_table1 tablesample (bucket 2 out of 2) s;

select count(*) from test_table2;
select count(*) from test_table2 tablesample (bucket 2 out of 2) s;

drop table test_table1;
drop table test_table2;

CREATE TABLE test_table1 (key INT, value STRING)
CLUSTERED BY (key) INTO 2 BUCKETS;
CREATE TABLE test_table2 (key INT, value STRING)
CLUSTERED BY (key) INTO 2 BUCKETS;

FROM src
INSERT OVERWRITE TABLE test_table1 SELECT *;

-- Insert data into the bucketed table by selecting from another bucketed table
-- This should be a map-only operation
EXPLAIN INSERT OVERWRITE TABLE test_table2
SELECT * FROM test_table1;

INSERT OVERWRITE TABLE test_table2
SELECT * FROM test_table1;

select count(*) from test_table1;
select count(*) from test_table1 tablesample (bucket 2 out of 2) s;

select count(*) from test_table2;
select count(*) from test_table2 tablesample (bucket 2 out of 2) s;
set hive.mapred.mode=nonstrict;


set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.max.dynamic.partitions.pernode=1000000;
set hive.exec.max.dynamic.partitions=1000000;
set hive.exec.max.created.files=1000000;
set hive.map.aggr=true;

create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;

load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;

explain
select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5;

set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge=true;
set hive.mapred.reduce.tasks.speculative.execution=false;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000000000;
set hive.optimize.reducededuplication.min.reducer=1;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy=org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

-- explain
-- select * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join src c on a.key=c.value

-- select a.key from smb_bucket_1 a

explain
select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5;

select * from (select a.key from smb_bucket_1 a join smb_bucket_2 b on (a.key = b.key) where a.key = 5) t1 left outer join (select c.key from smb_bucket_2 c join smb_bucket_3 d on (c.key = d.key) where c.key=5) t2 on (t1.key=t2.key) where t2.key=5;

-- SORT_QUERY_RESULTS



create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;

load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;


explain
select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key;

explain
select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;









create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;

load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- SORT_QUERY_RESULTS

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;









create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;

load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- SORT_QUERY_RESULTS

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key left outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key right outer join smb_bucket_3 c on b.key=c.key;

explain
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;
select /*+mapjoin(a,c)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key full outer join smb_bucket_3 c on b.key=c.key;





set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 1;


CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS RCFILE;


CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS RCFILE;

create table smb_join_results(k1 int, v1 string, k2 int, v2 string);
create table normal_join_results(k1 int, v1 string, k2 int, v2 string);

insert overwrite table smb_bucket4_1
select * from src;

insert overwrite table smb_bucket4_2
select * from src;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
insert overwrite table smb_join_results
select /*+mapjoin(a)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key;

insert overwrite table smb_join_results
select /*+mapjoin(a)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key;

select * from smb_join_results order by k1;

insert overwrite table normal_join_results select * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key;

select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results;
select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results;


explain
insert overwrite table smb_join_results
select /*+mapjoin(b)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key;
insert overwrite table smb_join_results
select /*+mapjoin(b)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key;


insert overwrite table smb_join_results
select /*+mapjoin(a)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key;

select * from smb_join_results order by k1;

insert overwrite table normal_join_results select * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key;

select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results;
select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results;


explain
insert overwrite table smb_join_results
select /*+mapjoin(a)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key where a.key>1000;
insert overwrite table smb_join_results
select /*+mapjoin(a)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key where a.key>1000;


explain
insert overwrite table smb_join_results
select /*+mapjoin(b)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key where a.key>1000;
insert overwrite table smb_join_results
select /*+mapjoin(b)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key where a.key>1000;


explain
select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key join smb_bucket4_2 c on b.key = c.key where a.key>1000;
select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a join smb_bucket4_2 b on a.key = b.key join smb_bucket4_2 c on b.key = c.key where a.key>1000;





set hive.mapred.mode=nonstrict;
set hive.exec.reducers.max = 1;


CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;


CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;




create table smb_join_results(k1 int, v1 string, k2 int, v2 string);
create table smb_join_results_empty_bigtable(k1 int, v1 string, k2 int, v2 string);
create table normal_join_results(k1 int, v1 string, k2 int, v2 string);

load data local inpath '../../data/files/empty1.txt' into table smb_bucket4_1;
load data local inpath '../../data/files/empty2.txt' into table smb_bucket4_1;

insert overwrite table smb_bucket4_2
select * from src;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

insert overwrite table smb_join_results_empty_bigtable
select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;

insert overwrite table smb_join_results_empty_bigtable
select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;

select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2;

explain
insert overwrite table smb_join_results
select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;

insert overwrite table smb_join_results
select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;

select * from smb_join_results order by k1, v1, k2, v2;

insert overwrite table normal_join_results select * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;

select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results;
select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results;
select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable;






set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
;

set hive.exec.reducers.max = 1;

-- SORT_QUERY_RESULTS

create table smb_bucket_input (key int, value string) stored as rcfile;
load data local inpath '../../data/files/smb_bucket_input.rc' into table smb_bucket_input;


CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS;

CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS;

CREATE TABLE smb_bucket4_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS;

insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=4 or key=2000 or key=4000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=484 or key=3000 or key=5000;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;


insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=2000 or key=4000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=3000 or key=5000;

select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;


insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=4000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=5000;

select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;


insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=1000 or key=4000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=1000 or key=5000;

select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;


insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=1000 or key=4000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=1000 or key=5000;
insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=1000 or key=5000;

select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
full outer join smb_bucket4_3 c on a.key=c.key;


insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=1000 or key=4000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=1000 or key=5000;
insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=1000 or key=4000;

select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
full outer join smb_bucket4_3 c on a.key=c.key;


insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=4000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=5000;
insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=4000;

select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
full outer join smb_bucket4_3 c on a.key=c.key;


insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=00000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=4000;
insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=5000;

select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
full outer join smb_bucket4_3 c on a.key=c.key;


insert overwrite table smb_bucket4_1 select * from smb_bucket_input where key=1000;
insert overwrite table smb_bucket4_2 select * from smb_bucket_input where key=4000;
insert overwrite table smb_bucket4_3 select * from smb_bucket_input where key=5000;

select /*+mapjoin(b,c)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
full outer join smb_bucket4_3 c on a.key=c.key;




-- SORT_QUERY_RESULTS

EXPLAIN
SELECT x.* FROM SRC x SORT BY key;

SELECT x.* FROM SRC x SORT BY key;
create table table_asc(key int, value string) CLUSTERED BY (key) SORTED BY (key asc)
INTO 1 BUCKETS STORED AS RCFILE;
create table table_desc(key int, value string) CLUSTERED BY (key) SORTED BY (key desc)
INTO 1 BUCKETS STORED AS RCFILE;




insert overwrite table table_asc select key, value from src;
insert overwrite table table_desc select key, value from src;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- If the user asked for sort merge join to be enforced (by setting
-- hive.enforce.sortmergebucketmapjoin to true), an error should be thrown, since
-- one of the tables is in ascending order and the other is in descending order,
-- and sort merge bucket mapjoin cannot be performed. In the default mode, the
-- query would succeed, although a regular map-join would be performed instead of
-- what the user asked.

explain
select /*+mapjoin(a)*/ * from table_asc a join table_desc b on a.key = b.key;

set hive.enforce.sortmergebucketmapjoin=true;

explain
select /*+mapjoin(a)*/ * from table_asc a join table_desc b on a.key = b.key;
drop table table_desc1;
drop table table_desc2;



create table table_desc1(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS;
create table table_desc2(key string, value string) clustered by (key) sorted by (key DESC) into 1 BUCKETS;

insert overwrite table table_desc1 select key, value from src;
insert overwrite table table_desc2 select key, value from src;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- The columns of the tables above are sorted in same descending order.
-- So, sort merge join should be performed

explain
select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b on a.key=b.key where a.key < 10;

select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b on a.key=b.key where a.key < 10;

drop table table_desc1;
drop table table_desc2;



create table table_desc1(key string, value string) clustered by (key, value)
sorted by (key DESC, value DESC) into 1 BUCKETS;
create table table_desc2(key string, value string) clustered by (key, value)
sorted by (key DESC, value DESC) into 1 BUCKETS;

insert overwrite table table_desc1 select key, value from src;
insert overwrite table table_desc2 select key, value from src;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- The columns of the tables above are sorted in same order.
-- descending followed by descending
-- So, sort merge join should be performed

explain
select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b
on a.key=b.key and a.value=b.value where a.key < 10;

select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b
on a.key=b.key and a.value=b.value where a.key < 10;

drop table table_desc1;
drop table table_desc2;



create table table_desc1(key string, value string) clustered by (key, value)
sorted by (key DESC, value ASC) into 1 BUCKETS;
create table table_desc2(key string, value string) clustered by (key, value)
sorted by (key DESC, value ASC) into 1 BUCKETS;

insert overwrite table table_desc1 select key, value from src;
insert overwrite table table_desc2 select key, value from src;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- The columns of the tables above are sorted in same orders.
-- descending followed by ascending
-- So, sort merge join should be performed

explain
select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b
on a.key=b.key and a.value=b.value where a.key < 10;

select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b
on a.key=b.key and a.value=b.value where a.key < 10;

drop table table_desc1;
drop table table_desc2;



create table table_desc1(key string, value string) clustered by (key, value)
sorted by (key DESC, value ASC) into 1 BUCKETS;
create table table_desc2(key string, value string) clustered by (key, value)
sorted by (key DESC, value DESC) into 1 BUCKETS;

insert overwrite table table_desc1 select key, value from src;
insert overwrite table table_desc2 select key, value from src;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- The columns of the tables above are sorted in different orders.
-- So, sort merge join should not be performed

explain
select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b
on a.key=b.key and a.value=b.value where a.key < 10;

select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b
on a.key=b.key and a.value=b.value where a.key < 10;




CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) SORTED BY (key DESC) INTO 1 BUCKETS;
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_1 PARTITION (part='1') SELECT * FROM src;

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) SORTED BY (key DESC) INTO 1 BUCKETS;
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_2 PARTITION (part='1') SELECT * FROM src;

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key) SORTED BY (value DESC) INTO 1 BUCKETS;

set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

-- The partition sorting metadata matches but the table metadata does not, sorted merge join should still be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' AND b.part = '1';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' AND b.part = '1';


set hive.exec.reducers.max = 1;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) SORTED BY (key DESC) INTO 2 BUCKETS;
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_1 PARTITION (part='1') SELECT * FROM src;

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key) SORTED BY (value DESC) INTO 2 BUCKETS;
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_2 PARTITION (part='1') SELECT * FROM src;

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key) SORTED BY (key DESC) INTO 2 BUCKETS;

set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

-- The table sorting metadata matches but the partition metadata does not, sorted merge join should not be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' AND b.part = '1';

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part = '1' AND b.part = '1';


set hive.exec.reducers.max = 1;

CREATE TABLE srcbucket_mapjoin_part_1 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key, value) SORTED BY (key DESC) INTO 2 BUCKETS;
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_1 PARTITION (part='1') SELECT * FROM src;

ALTER TABLE srcbucket_mapjoin_part_1 CLUSTERED BY (key, value) SORTED BY (value DESC) INTO 2 BUCKETS;
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_1 PARTITION (part='2') SELECT * FROM src;

CREATE TABLE srcbucket_mapjoin_part_2 (key INT, value STRING) PARTITIONED BY (part STRING)
CLUSTERED BY (key, value) SORTED BY (value DESC) INTO 2 BUCKETS;
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_2 PARTITION (part='1') SELECT * FROM src;

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key, value) SORTED BY (key DESC) INTO 2 BUCKETS;
INSERT OVERWRITE TABLE srcbucket_mapjoin_part_2 PARTITION (part='2') SELECT * FROM src;

ALTER TABLE srcbucket_mapjoin_part_2 CLUSTERED BY (key, value) SORTED BY (value DESC) INTO 2 BUCKETS;

set hive.optimize.bucketmapjoin=true;
set hive.optimize.bucketmapjoin.sortedmerge = true;

-- The table sorting metadata matches but the partition metadata does not, sorted merge join should not be used

EXPLAIN EXTENDED
SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part IS NOT NULL AND b.part IS NOT NULL;

SELECT /*+ MAPJOIN(b) */ count(*)
FROM srcbucket_mapjoin_part_1 a JOIN srcbucket_mapjoin_part_2 b
ON a.key = b.key AND a.part IS NOT NULL AND b.part IS NOT NULL;
drop table table_desc1;
drop table table_desc2;
drop table table_desc3;
drop table table_desc4;



create table table_desc1(key string, value string) clustered by (key)
sorted by (key DESC) into 1 BUCKETS;
create table table_desc2(key string, value string) clustered by (key)
sorted by (key DESC, value DESC) into 1 BUCKETS;
create table table_desc3(key string, value1 string, value2 string) clustered by (key)
sorted by (key DESC, value1 DESC,value2 DESC) into 1 BUCKETS;
create table table_desc4(key string, value2 string) clustered by (key)
sorted by (key DESC, value2 DESC) into 1 BUCKETS;

insert overwrite table table_desc1 select key, value from src sort by key DESC;
insert overwrite table table_desc2 select key, value from src sort by key DESC;
insert overwrite table table_desc3 select key, value, concat(value,"_2") as value2 from src sort by key, value, value2 DESC;
insert overwrite table table_desc4 select key, concat(value,"_2") as value2 from src sort by key, value2 DESC;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

-- columns are sorted by one key in first table, two keys in second table but in same sort order for key. Hence SMB join should pass

explain
select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b
on a.key=b.key where a.key < 10;

select /*+ mapjoin(b) */ count(*) from table_desc1 a join table_desc2 b
on a.key=b.key where a.key < 10;

-- columns are sorted by 3 keys(a, b, c) in first table, two keys(a, c) in second table with same sort order. Hence SMB join should not pass

explain
select /*+ mapjoin(b) */ count(*) from table_desc3 a join table_desc4 b
on a.key=b.key and a.value2=b.value2 where a.key < 10;

select /*+ mapjoin(b) */ count(*) from table_desc3 a join table_desc4 b
on a.key=b.key and a.value2=b.value2 where a.key < 10;
source ../../data/files/source.txt;

source ${system:test.data.dir}/source.txt;
set hive.support.sql11.reserved.keywords=false;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.spark.dynamic.partition.pruning=true;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;

-- SORT_QUERY_RESULTS

select distinct ds from srcpart;
select distinct hr from srcpart;

EXPLAIN create table srcpart_date as select ds as ds, ds as date from srcpart group by ds;
create table srcpart_date as select ds as ds, ds as date from srcpart group by ds;
create table srcpart_hour as select hr as hr, hr as hour from srcpart group by hr;
create table srcpart_date_hour as select ds as ds, ds as date, hr as hr, hr as hour from srcpart group by ds, hr;
create table srcpart_double_hour as select (hr*2) as hr, hr as hour from srcpart group by hr;

-- single column, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = '2008-04-08';

-- multiple sources, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where hr = 11 and ds = '2008-04-08';

-- multiple columns single source
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = '2008-04-08' and hr = 11;

-- empty set
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = 'I DONT EXIST';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = 'I DONT EXIST';
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = 'I DONT EXIST';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = 'I DONT EXIST';
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = 'I DONT EXIST';

-- expressions
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (cast(srcpart.hr*2 as string) = cast(srcpart_double_hour.hr as string)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (cast(srcpart.hr*2 as string) = cast(srcpart_double_hour.hr as string)) where srcpart_double_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where cast(hr as string) = 11;


-- parent is reduce tasks
EXPLAIN select count(*) from srcpart join (select ds as ds, ds as date from srcpart group by ds) s on (srcpart.ds = s.ds) where s.date = '2008-04-08';
select count(*) from srcpart join (select ds as ds, ds as date from srcpart group by ds) s on (srcpart.ds = s.ds) where s.date = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- non-equi join
EXPLAIN select count(*) from srcpart, srcpart_date_hour where (srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11) and (srcpart.ds = srcpart_date_hour.ds or srcpart.hr = srcpart_date_hour.hr);
select count(*) from srcpart, srcpart_date_hour where (srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11) and (srcpart.ds = srcpart_date_hour.ds or srcpart.hr = srcpart_date_hour.hr);

-- old style join syntax
EXPLAIN select count(*) from srcpart, srcpart_date_hour where srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11 and srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr;
select count(*) from srcpart, srcpart_date_hour where srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11 and srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr;

-- left join
EXPLAIN select count(*) from srcpart left join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';
EXPLAIN select count(*) from srcpart_date left join srcpart on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';

-- full outer
EXPLAIN select count(*) from srcpart full outer join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';

-- with static pruning
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart.hr = 13;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart.hr = 13;

-- union + subquery
EXPLAIN select count(*) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select count(*) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
EXPLAIN select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
EXPLAIN select ds from (select distinct(ds) as ds from srcpart union all select distinct(ds) as ds from srcpart) s where s.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select ds from (select distinct(ds) as ds from srcpart union all select distinct(ds) as ds from srcpart) s where s.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask = true;
set hive.auto.convert.join.noconditionaltask.size = 10000000;

-- single column, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- multiple sources, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart where hr = 11 and ds = '2008-04-08';

-- multiple columns single source
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.date = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart where ds = '2008-04-08' and hr = 11;

-- empty set
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = 'I DONT EXIST';
-- Disabled until TEZ-1486 is fixed
-- select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = 'I DONT EXIST';

-- expressions
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart where hr = 11;

-- parent is reduce tasks
EXPLAIN select count(*) from srcpart join (select ds as ds, ds as date from srcpart group by ds) s on (srcpart.ds = s.ds) where s.date = '2008-04-08';
select count(*) from srcpart join (select ds as ds, ds as date from srcpart group by ds) s on (srcpart.ds = s.ds) where s.date = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- left join
EXPLAIN select count(*) from srcpart left join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';
EXPLAIN select count(*) from srcpart_date left join srcpart on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';

-- full outer
EXPLAIN select count(*) from srcpart full outer join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.date = '2008-04-08';

-- with static pruning
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.date = '2008-04-08' and srcpart.hr = 13;
-- Disabled until TEZ-1486 is fixed
-- select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
-- where srcpart_date.date = '2008-04-08' and srcpart.hr = 13;

-- union + subquery
EXPLAIN select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);


drop table srcpart_orc;
drop table srcpart_date;
drop table srcpart_hour;
drop table srcpart_date_hour;
drop table srcpart_double_hour;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.spark.dynamic.partition.pruning=true;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask = true;
set hive.auto.convert.join.noconditionaltask.size = 10000000;

-- SORT_QUERY_RESULTS

create table dim_shops (id int, label string) row format delimited fields terminated by ',' stored as textfile;
load data local inpath '../../data/files/dim_shops.txt' into table dim_shops;

create table agg_01 (amount decimal) partitioned by (dim_shops_id int) row format delimited fields terminated by ',' stored as textfile;
alter table agg_01 add partition (dim_shops_id = 1);
alter table agg_01 add partition (dim_shops_id = 2);
alter table agg_01 add partition (dim_shops_id = 3);

load data local inpath '../../data/files/agg_01-p1.txt' into table agg_01 partition (dim_shops_id=1);
load data local inpath '../../data/files/agg_01-p2.txt' into table agg_01 partition (dim_shops_id=2);
load data local inpath '../../data/files/agg_01-p3.txt' into table agg_01 partition (dim_shops_id=3);

analyze table dim_shops compute statistics;
analyze table agg_01 partition (dim_shops_id) compute statistics;

select * from dim_shops;
select * from agg_01;

EXPLAIN SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

set hive.spark.dynamic.partition.pruning.max.data.size=1;

EXPLAIN SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

EXPLAIN SELECT d1.label
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id;

SELECT d1.label
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id;

EXPLAIN SELECT agg.amount
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and agg.dim_shops_id = 1;

SELECT agg.amount
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and agg.dim_shops_id = 1;

set hive.spark.dynamic.partition.pruning.max.data.size=1000000;

EXPLAIN SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;

SELECT d1.label, count(*), sum(agg.amount)
FROM agg_01 agg,
dim_shops d1
WHERE agg.dim_shops_id = d1.id
and
d1.label in ('foo', 'bar')
GROUP BY d1.label
ORDER BY d1.label;


EXPLAIN
SELECT amount FROM agg_01, dim_shops WHERE dim_shops_id = id AND label = 'foo'
UNION ALL
SELECT amount FROM agg_01, dim_shops WHERE dim_shops_id = id AND label = 'bar';

SELECT amount FROM agg_01, dim_shops WHERE dim_shops_id = id AND label = 'foo'
UNION ALL
SELECT amount FROM agg_01, dim_shops WHERE dim_shops_id = id AND label = 'bar';
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.spark.dynamic.partition.pruning=true;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.vectorized.execution.enabled=true;


select distinct ds from srcpart;
select distinct hr from srcpart;

EXPLAIN create table srcpart_date as select ds as ds, ds as `date` from srcpart group by ds;
create table srcpart_date stored as orc as select ds as ds, ds as `date` from srcpart group by ds;
create table srcpart_hour stored as orc as select hr as hr, hr as hour from srcpart group by hr;
create table srcpart_date_hour stored as orc as select ds as ds, ds as `date`, hr as hr, hr as hour from srcpart group by ds, hr;
create table srcpart_double_hour stored as orc as select (hr*2) as hr, hr as hour from srcpart group by hr;

-- single column, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = '2008-04-08';

-- multiple sources, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where hr = 11 and ds = '2008-04-08';

-- multiple columns single source
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = '2008-04-08' and hr = 11;

-- empty set
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = 'I DONT EXIST';

-- expressions
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (cast(srcpart.hr*2 as string) = cast(srcpart_double_hour.hr as string)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (cast(srcpart.hr*2 as string) = cast(srcpart_double_hour.hr as string)) where srcpart_double_hour.hour = 11;
set hive.spark.dynamic.partition.pruning=true;
select count(*) from srcpart where cast(hr as string) = 11;


-- parent is reduce tasks
EXPLAIN select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- non-equi join
EXPLAIN select count(*) from srcpart, srcpart_date_hour where (srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11) and (srcpart.ds = srcpart_date_hour.ds or srcpart.hr = srcpart_date_hour.hr);
select count(*) from srcpart, srcpart_date_hour where (srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11) and (srcpart.ds = srcpart_date_hour.ds or srcpart.hr = srcpart_date_hour.hr);

-- old style join syntax
EXPLAIN select count(*) from srcpart, srcpart_date_hour where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11 and srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr;
select count(*) from srcpart, srcpart_date_hour where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11 and srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr;

-- left join
EXPLAIN select count(*) from srcpart left join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
EXPLAIN select count(*) from srcpart_date left join srcpart on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- full outer
EXPLAIN select count(*) from srcpart full outer join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- with static pruning
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;

-- union + subquery
EXPLAIN select count(*) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select count(*) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
EXPLAIN select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
EXPLAIN select ds from (select distinct(ds) as ds from srcpart union all select distinct(ds) as ds from srcpart) s where s.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select ds from (select distinct(ds) as ds from srcpart union all select distinct(ds) as ds from srcpart) s where s.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask = true;
set hive.auto.convert.join.noconditionaltask.size = 10000000;

-- single column, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- multiple sources, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart where hr = 11 and ds = '2008-04-08';

-- multiple columns single source
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart where ds = '2008-04-08' and hr = 11;

-- empty set
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
-- Disabled until TEZ-1486 is fixed
-- select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';

-- expressions
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart where hr = 11;

-- parent is reduce tasks
EXPLAIN select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- left join
EXPLAIN select count(*) from srcpart left join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
EXPLAIN select count(*) from srcpart_date left join srcpart on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- full outer
EXPLAIN select count(*) from srcpart full outer join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- with static pruning
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;
-- Disabled until TEZ-1486 is fixed
-- select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
-- where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;

-- union + subquery
EXPLAIN select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);


-- different file format
create table srcpart_orc (key int, value string) partitioned by (ds string, hr int) stored as orc;


set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false;
set hive.exec.max.dynamic.partitions=1000;

insert into table srcpart_orc partition (ds, hr) select key, value, ds, hr from srcpart;
EXPLAIN select count(*) from srcpart_orc join srcpart_date_hour on (srcpart_orc.ds = srcpart_date_hour.ds and srcpart_orc.hr = srcpart_date_hour.hr) where srcpart_date_hour.hour = 11 and (srcpart_date_hour.`date` = '2008-04-08' or srcpart_date_hour.`date` = '2008-04-09');
select count(*) from srcpart_orc join srcpart_date_hour on (srcpart_orc.ds = srcpart_date_hour.ds and srcpart_orc.hr = srcpart_date_hour.hr) where srcpart_date_hour.hour = 11 and (srcpart_date_hour.`date` = '2008-04-08' or srcpart_date_hour.`date` = '2008-04-09');
select count(*) from srcpart where (ds = '2008-04-08' or ds = '2008-04-09') and hr = 11;

drop table srcpart_orc;
drop table srcpart_date;
drop table srcpart_hour;
drop table srcpart_date_hour;
drop table srcpart_double_hour;
set hive.cbo.enable=true;
set hive.exec.check.crossproducts=false;
set hive.stats.fetch.column.stats=true;
set hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS

create table `c/b/o_t1`(key string, value string, c_int int, c_float float, c_boolean boolean)  partitioned by (dt string) row format delimited fields terminated by ',' STORED AS TEXTFILE;
create table `//cbo_t2`(key string, value string, c_int int, c_float float, c_boolean boolean)  partitioned by (dt string) row format delimited fields terminated by ',' STORED AS TEXTFILE;
create table `cbo_/t3////`(key string, value string, c_int int, c_float float, c_boolean boolean)  row format delimited fields terminated by ',' STORED AS TEXTFILE;

load data local inpath '../../data/files/cbo_t1.txt' into table `c/b/o_t1` partition (dt='2014');
load data local inpath '../../data/files/cbo_t2.txt' into table `//cbo_t2` partition (dt='2014');
load data local inpath '../../data/files/cbo_t3.txt' into table `cbo_/t3////`;

CREATE TABLE `p/a/r/t`(
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
);

LOAD DATA LOCAL INPATH '../../data/files/part_tiny.txt' overwrite into table `p/a/r/t`;

CREATE TABLE `line/item` (L_ORDERKEY      INT,
                                L_PARTKEY       INT,
                                L_SUPPKEY       INT,
                                L_LINENUMBER    INT,
                                L_QUANTITY      DOUBLE,
                                L_EXTENDEDPRICE DOUBLE,
                                L_DISCOUNT      DOUBLE,
                                L_TAX           DOUBLE,
                                L_RETURNFLAG    STRING,
                                L_LINESTATUS    STRING,
                                l_shipdate      STRING,
                                L_COMMITDATE    STRING,
                                L_RECEIPTDATE   STRING,
                                L_SHIPINSTRUCT  STRING,
                                L_SHIPMODE      STRING,
                                L_COMMENT       STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|';

LOAD DATA LOCAL INPATH '../../data/files/lineitem.txt' OVERWRITE INTO TABLE `line/item`;

create table `src/_/cbo` as select * from src;

analyze table `c/b/o_t1` partition (dt) compute statistics;

analyze table `c/b/o_t1` compute statistics for columns key, value, c_int, c_float, c_boolean;

analyze table `//cbo_t2` partition (dt) compute statistics;

analyze table `//cbo_t2` compute statistics for columns key, value, c_int, c_float, c_boolean;

analyze table `cbo_/t3////` compute statistics;

analyze table `cbo_/t3////` compute statistics for columns key, value, c_int, c_float, c_boolean;

analyze table `src/_/cbo` compute statistics;

analyze table `src/_/cbo` compute statistics for columns;

analyze table `p/a/r/t` compute statistics;

analyze table `p/a/r/t` compute statistics for columns;

analyze table `line/item` compute statistics;

analyze table `line/item` compute statistics for columns;

select key, (c_int+1)+2 as x, sum(c_int) from `c/b/o_t1` group by c_float, `c/b/o_t1`.c_int, key;

select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from `c/b/o_t1` group by c_float, `c/b/o_t1`.c_int, key) R group by y, x;

select `cbo_/t3////`.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0) group by c_float, `c/b/o_t1`.c_int, key order by a) `c/b/o_t1` join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key order by q/10 desc, r asc) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q >= 0) and (b > 0 or c_int >= 0) group by `cbo_/t3////`.c_int, c order by `cbo_/t3////`.c_int+c desc, c;

select `cbo_/t3////`.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)  group by c_float, `c/b/o_t1`.c_int, key having `c/b/o_t1`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b % c asc, b desc) `c/b/o_t1` left outer join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key  having `//cbo_t2`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) `//cbo_t2` on `c/b/o_t1`.a=p left outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q >= 0) and (b > 0 or c_int >= 0) group by `cbo_/t3////`.c_int, c  having `cbo_/t3////`.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0  order by `cbo_/t3////`.c_int % c asc, `cbo_/t3////`.c_int desc;

select `cbo_/t3////`.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)  group by c_float, `c/b/o_t1`.c_int, key having `c/b/o_t1`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b+c, a desc) `c/b/o_t1` right outer join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key having `//cbo_t2`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) `//cbo_t2` on `c/b/o_t1`.a=p right outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q >= 2) and (b > 0 or c_int >= 0) group by `cbo_/t3////`.c_int, c;



select `cbo_/t3////`.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)  group by c_float, `c/b/o_t1`.c_int, key having `c/b/o_t1`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by c+a desc) `c/b/o_t1` full outer join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key having `//cbo_t2`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by p+q desc, r asc) `//cbo_t2` on `c/b/o_t1`.a=p full outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q >= 0) and (b > 0 or c_int >= 0) group by `cbo_/t3////`.c_int, c having `cbo_/t3////`.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0 order by `cbo_/t3////`.c_int;



select `cbo_/t3////`.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)  group by c_float, `c/b/o_t1`.c_int, key having `c/b/o_t1`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) `c/b/o_t1` join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key having `//cbo_t2`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q >= 0) and (b > 0 or c_int >= 0) group by `cbo_/t3////`.c_int, c;



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 21. Test groupby is empty and there is no other cols in aggr

select unionsrc.key FROM (select 'tst1' as key, count(1) as value from src) unionsrc;



select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src) unionsrc;



select unionsrc.key FROM (select 'max' as key, max(c_int) as value from `cbo_/t3////` s1

UNION  ALL

    select 'min' as key,  min(c_int) as value from `cbo_/t3////` s2

    UNION ALL

        select 'avg' as key,  avg(c_int) as value from `cbo_/t3////` s3) unionsrc order by unionsrc.key;



select unionsrc.key, unionsrc.value FROM (select 'max' as key, max(c_int) as value from `cbo_/t3////` s1

UNION  ALL

    select 'min' as key,  min(c_int) as value from `cbo_/t3////` s2

    UNION ALL

        select 'avg' as key,  avg(c_int) as value from `cbo_/t3////` s3) unionsrc order by unionsrc.key;



select unionsrc.key, count(1) FROM (select 'max' as key, max(c_int) as value from `cbo_/t3////` s1

    UNION  ALL

        select 'min' as key,  min(c_int) as value from `cbo_/t3////` s2

    UNION ALL

        select 'avg' as key,  avg(c_int) as value from `cbo_/t3////` s3) unionsrc group by unionsrc.key order by unionsrc.key;



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- SORT_QUERY_RESULTS

-- 4. Test Select + Join + TS

select `c/b/o_t1`.c_int, `//cbo_t2`.c_int from `c/b/o_t1` join             `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key;

select `c/b/o_t1`.key from `c/b/o_t1` join `cbo_/t3////`;

select `c/b/o_t1`.key from `c/b/o_t1` join `cbo_/t3////` where `c/b/o_t1`.key=`cbo_/t3////`.key and `c/b/o_t1`.key >= 1;

select `c/b/o_t1`.c_int, `//cbo_t2`.c_int from `c/b/o_t1` left outer join  `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key;

select `c/b/o_t1`.c_int, `//cbo_t2`.c_int from `c/b/o_t1` right outer join `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key;

select `c/b/o_t1`.c_int, `//cbo_t2`.c_int from `c/b/o_t1` full outer join  `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key;



select b, `c/b/o_t1`.c, `//cbo_t2`.p, q, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`) `c/b/o_t1` join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key;

select key, `c/b/o_t1`.c_int, `//cbo_t2`.p, q from `c/b/o_t1` join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`) `//cbo_t2` on `c/b/o_t1`.key=p join (select key as a, c_int as b, `cbo_/t3////`.c_float as c from `cbo_/t3////`)`cbo_/t3////` on `c/b/o_t1`.key=a;

select a, `c/b/o_t1`.b, key, `//cbo_t2`.c_int, `cbo_/t3////`.p from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`) `c/b/o_t1` join `//cbo_t2`  on `c/b/o_t1`.a=key join (select key as p, c_int as q, `cbo_/t3////`.c_float as r from `cbo_/t3////`)`cbo_/t3////` on `c/b/o_t1`.a=`cbo_/t3////`.p;

select b, `c/b/o_t1`.c, `//cbo_t2`.c_int, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`) `c/b/o_t1` join `//cbo_t2` on `c/b/o_t1`.a=`//cbo_t2`.key join `cbo_/t3////` on `c/b/o_t1`.a=`cbo_/t3////`.key;

select `cbo_/t3////`.c_int, b, `//cbo_t2`.c_int, `c/b/o_t1`.c from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`) `c/b/o_t1` join `//cbo_t2` on `c/b/o_t1`.a=`//cbo_t2`.key join `cbo_/t3////` on `c/b/o_t1`.a=`cbo_/t3////`.key;



select b, `c/b/o_t1`.c, `//cbo_t2`.p, q, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`) `c/b/o_t1` left outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key;

select key, `c/b/o_t1`.c_int, `//cbo_t2`.p, q from `c/b/o_t1` join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`) `//cbo_t2` on `c/b/o_t1`.key=p left outer join (select key as a, c_int as b, `cbo_/t3////`.c_float as c from `cbo_/t3////`)`cbo_/t3////` on `c/b/o_t1`.key=a;



select b, `c/b/o_t1`.c, `//cbo_t2`.p, q, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`) `c/b/o_t1` right outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key;

select key, `c/b/o_t1`.c_int, `//cbo_t2`.p, q from `c/b/o_t1` join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`) `//cbo_t2` on `c/b/o_t1`.key=p right outer join (select key as a, c_int as b, `cbo_/t3////`.c_float as c from `cbo_/t3////`)`cbo_/t3////` on `c/b/o_t1`.key=a;



select b, `c/b/o_t1`.c, `//cbo_t2`.p, q, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`) `c/b/o_t1` full outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key;

select key, `c/b/o_t1`.c_int, `//cbo_t2`.p, q from `c/b/o_t1` join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`) `//cbo_t2` on `c/b/o_t1`.key=p full outer join (select key as a, c_int as b, `cbo_/t3////`.c_float as c from `cbo_/t3////`)`cbo_/t3////` on `c/b/o_t1`.key=a;



-- 5. Test Select + Join + FIL + TS

select `c/b/o_t1`.c_int, `//cbo_t2`.c_int from `c/b/o_t1` join `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key where (`c/b/o_t1`.c_int + `//cbo_t2`.c_int == 2) and (`c/b/o_t1`.c_int > 0 or `//cbo_t2`.c_float >= 0);

select `c/b/o_t1`.c_int, `//cbo_t2`.c_int from `c/b/o_t1` left outer join  `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key where (`c/b/o_t1`.c_int + `//cbo_t2`.c_int == 2) and (`c/b/o_t1`.c_int > 0 or `//cbo_t2`.c_float >= 0);

select `c/b/o_t1`.c_int, `//cbo_t2`.c_int from `c/b/o_t1` right outer join `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key where (`c/b/o_t1`.c_int + `//cbo_t2`.c_int == 2) and (`c/b/o_t1`.c_int > 0 or `//cbo_t2`.c_float >= 0);

select `c/b/o_t1`.c_int, `//cbo_t2`.c_int from `c/b/o_t1` full outer join  `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key where (`c/b/o_t1`.c_int + `//cbo_t2`.c_int == 2) and (`c/b/o_t1`.c_int > 0 or `//cbo_t2`.c_float >= 0);



select b, `c/b/o_t1`.c, `//cbo_t2`.p, q, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or `//cbo_t2`.q >= 0);



select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` left outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0);



select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` right outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0);



select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` full outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` full outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` left outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p left outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` left outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p right outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` left outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p full outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` right outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p right outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` right outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p left outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` right outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p full outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` full outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p full outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` full outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p left outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



select * from (select q, b, `//cbo_t2`.p, `c/b/o_t1`.c, `cbo_/t3////`.c_int from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` full outer join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p right outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q == 2) and (b > 0 or c_int >= 0)) R where  (q + 1 = 2) and (R.b > 0 or c_int >= 0);



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 7. Test Select + TS + Join + Fil + GB + GB Having + Limit

select key, (c_int+1)+2 as x, sum(c_int) from `c/b/o_t1` group by c_float, `c/b/o_t1`.c_int, key order by x limit 1;

select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from `c/b/o_t1` group by c_float, `c/b/o_t1`.c_int, key) R group by y, x order by x,y limit 1;

select key from(select key from (select key from `c/b/o_t1` limit 5)`//cbo_t2`  limit 5)`cbo_/t3////`  limit 5;

select key, c_int from(select key, c_int from (select key, c_int from `c/b/o_t1` order by c_int limit 5)`c/b/o_t1`  order by c_int limit 5)`//cbo_t2`  order by c_int limit 5;



select `cbo_/t3////`.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0) group by c_float, `c/b/o_t1`.c_int, key order by a limit 5) `c/b/o_t1` join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key order by q/10 desc, r asc limit 5) `//cbo_t2` on `c/b/o_t1`.a=p join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q >= 0) and (b > 0 or c_int >= 0) group by `cbo_/t3////`.c_int, c order by `cbo_/t3////`.c_int+c desc, c limit 5;



select `cbo_/t3////`.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)  group by c_float, `c/b/o_t1`.c_int, key having `c/b/o_t1`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by b % c asc, b desc limit 5) `c/b/o_t1` left outer join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key  having `//cbo_t2`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 limit 5) `//cbo_t2` on `c/b/o_t1`.a=p left outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `//cbo_t2`.q >= 0) and (b > 0 or c_int >= 0) group by `cbo_/t3////`.c_int, c  having `cbo_/t3////`.c_int > 0 and (c_int >=1 or c >= 1) and (c_int + c) >= 0  order by `cbo_/t3////`.c_int % c asc, `cbo_/t3////`.c_int, c desc limit 5;

set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 12. SemiJoin

select `c/b/o_t1`.c_int           from `c/b/o_t1` left semi join   `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key;

select `c/b/o_t1`.c_int           from `c/b/o_t1` left semi join   `//cbo_t2` on `c/b/o_t1`.key=`//cbo_t2`.key where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0);

select * from (select c, b, a from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` left semi join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p left semi join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + 1 == 2) and (b > 0 or c >= 0)) R where  (b + 1 = 2) and (R.b > 0 or c >= 0);

select * from (select `cbo_/t3////`.c_int, `c/b/o_t1`.c, b from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 = 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` left semi join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p left outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + `cbo_/t3////`.c_int  == 2) and (b > 0 or c_int >= 0)) R where  (R.c_int + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select c_int, b, `c/b/o_t1`.c from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` left semi join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p right outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + 1 == 2) and (b > 0 or c_int >= 0)) R where  (c + 1 = 2) and (R.b > 0 or c_int >= 0);

select * from (select c_int, b, `c/b/o_t1`.c from (select key as a, c_int as b, `c/b/o_t1`.c_float as c from `c/b/o_t1`  where (`c/b/o_t1`.c_int + 1 == 2) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)) `c/b/o_t1` left semi join (select `//cbo_t2`.key as p, `//cbo_t2`.c_int as q, c_float as r from `//cbo_t2`  where (`//cbo_t2`.c_int + 1 == 2) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)) `//cbo_t2` on `c/b/o_t1`.a=p full outer join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + 1 == 2) and (b > 0 or c_int >= 0)) R where  (c + 1 = 2) and (R.b > 0 or c_int >= 0);

select a, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)  group by c_float, `c/b/o_t1`.c_int, key having `c/b/o_t1`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by a+b desc, c asc) `c/b/o_t1` left semi join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key having `//cbo_t2`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by q+r/10 desc, p) `//cbo_t2` on `c/b/o_t1`.a=p left semi join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + 1  >= 0) and (b > 0 or a >= 0) group by a, c  having a > 0 and (a >=1 or c >= 1) and (a + c) >= 0 order by c, a;

select a, c, count(*)  from (select key as a, c_int+1 as b, sum(c_int) as c from `c/b/o_t1` where (`c/b/o_t1`.c_int + 1 >= 0) and (`c/b/o_t1`.c_int > 0 or `c/b/o_t1`.c_float >= 0)  group by c_float, `c/b/o_t1`.c_int, key having `c/b/o_t1`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by a+b desc, c asc limit 5) `c/b/o_t1` left semi join (select key as p, c_int+1 as q, sum(c_int) as r from `//cbo_t2` where (`//cbo_t2`.c_int + 1 >= 0) and (`//cbo_t2`.c_int > 0 or `//cbo_t2`.c_float >= 0)  group by c_float, `//cbo_t2`.c_int, key having `//cbo_t2`.c_float > 0 and (c_int >=1 or c_float >= 1) and (c_int + c_float) >= 0 order by q+r/10 desc, p limit 5) `//cbo_t2` on `c/b/o_t1`.a=p left semi join `cbo_/t3////` on `c/b/o_t1`.a=key where (b + 1  >= 0) and (b > 0 or a >= 0) group by a, c  having a > 0 and (a >=1 or c >= 1) and (a + c) >= 0 order by c, a;



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 1. Test Select + TS

select * from `c/b/o_t1`;

select * from `c/b/o_t1` as `c/b/o_t1`;

select * from `c/b/o_t1` as `//cbo_t2`;



select `c/b/o_t1`.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from `c/b/o_t1`;

select * from `c/b/o_t1` where (((key=1) and (c_float=10)) and (c_int=20));



-- 2. Test Select + TS + FIL

select * from `c/b/o_t1` where `c/b/o_t1`.c_int >= 0;

select * from `c/b/o_t1` as `c/b/o_t1`  where `c/b/o_t1`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

select * from `c/b/o_t1` as `//cbo_t2` where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;



select `//cbo_t2`.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from `c/b/o_t1` as `//cbo_t2`  where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;



-- 3 Test Select + Select + TS + FIL

select * from (select * from `c/b/o_t1` where `c/b/o_t1`.c_int >= 0) as `c/b/o_t1`;

select * from (select * from `c/b/o_t1` as `c/b/o_t1`  where `c/b/o_t1`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `c/b/o_t1`;

select * from (select * from `c/b/o_t1` as `//cbo_t2` where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `c/b/o_t1`;

select * from (select `//cbo_t2`.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from `c/b/o_t1` as `//cbo_t2`  where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `c/b/o_t1`;



select * from (select * from `c/b/o_t1` where `c/b/o_t1`.c_int >= 0) as `c/b/o_t1` where `c/b/o_t1`.c_int >= 0;

select * from (select * from `c/b/o_t1` as `c/b/o_t1`  where `c/b/o_t1`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `c/b/o_t1`  where `c/b/o_t1`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

select * from (select * from `c/b/o_t1` as `//cbo_t2` where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `//cbo_t2` where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

select * from (select `//cbo_t2`.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from `c/b/o_t1` as `//cbo_t2`  where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `c/b/o_t1` where `c/b/o_t1`.c_int >= 0 and y+c_int >= 0 or x <= 100;



select `c/b/o_t1`.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from `c/b/o_t1` where `c/b/o_t1`.c_int >= 0) as `c/b/o_t1` where `c/b/o_t1`.c_int >= 0;

select `//cbo_t2`.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from `c/b/o_t1` where `c/b/o_t1`.c_int >= 0) as `//cbo_t2` where `//cbo_t2`.c_int >= 0;







select * from (select * from `c/b/o_t1` where `c/b/o_t1`.c_int >= 0) as `c/b/o_t1` where `c/b/o_t1`.c_int >= 0;

select * from (select * from `c/b/o_t1` as `c/b/o_t1`  where `c/b/o_t1`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `c/b/o_t1`  where `c/b/o_t1`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

select * from (select * from `c/b/o_t1` as `//cbo_t2` where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `//cbo_t2` where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100;

select * from (select `//cbo_t2`.key as x, c_int as c_int, (((c_int+c_float)*10)+5) as y from `c/b/o_t1` as `//cbo_t2`  where `//cbo_t2`.c_int >= 0 and c_float+c_int >= 0 or c_float <= 100) as `c/b/o_t1` where `c/b/o_t1`.c_int >= 0 and y+c_int >= 0 or x <= 100;



select `c/b/o_t1`.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from `c/b/o_t1` where `c/b/o_t1`.c_int >= 0) as `c/b/o_t1` where `c/b/o_t1`.c_int >= 0;

select `//cbo_t2`.c_int+c_float as x , c_int as c_int, (((c_int+c_float)*10)+5) as y from (select * from `c/b/o_t1` where `c/b/o_t1`.c_int >= 0) as `//cbo_t2` where `//cbo_t2`.c_int >= 0;







-- 13. null expr in select list

select null from `cbo_/t3////`;



-- 14. unary operator

select key from `c/b/o_t1` where c_int = -6  or c_int = +6;



-- 15. query referencing only partition columns

select count(`c/b/o_t1`.dt) from `c/b/o_t1` join `//cbo_t2` on `c/b/o_t1`.dt  = `//cbo_t2`.dt  where `c/b/o_t1`.dt = '2014' ;

set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 20. Test get stats with empty partition list

select `c/b/o_t1`.value from `c/b/o_t1` join `//cbo_t2` on `c/b/o_t1`.key = `//cbo_t2`.key where `c/b/o_t1`.dt = '10' and `c/b/o_t1`.c_boolean = true;



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 18. SubQueries Not Exists

-- distinct, corr

select *

from `src/_/cbo` b

where not exists

  (select distinct a.key

  from `src/_/cbo` a

  where b.value = a.value and a.value > 'val_2'

  )

;



-- no agg, corr, having

select *

from `src/_/cbo` b

group by key, value

having not exists

  (select a.key

  from `src/_/cbo` a

  where b.value = a.value  and a.key = b.key and a.value > 'val_12'

  )

;



-- 19. SubQueries Exists

-- view test

create view cv1 as

select *

from `src/_/cbo` b

where exists

  (select a.key

  from `src/_/cbo` a

  where b.value = a.value  and a.key = b.key and a.value > 'val_9')

;



select * from cv1

;



-- sq in from

select *

from (select *

      from `src/_/cbo` b

      where exists

          (select a.key

          from `src/_/cbo` a

          where b.value = a.value  and a.key = b.key and a.value > 'val_9')

     ) a

;



-- sq in from, having

select *

from (select b.key, count(*)

  from `src/_/cbo` b

  group by b.key

  having exists

    (select a.key

    from `src/_/cbo` a

    where a.key = b.key and a.value > 'val_9'

    )

) a

;



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 17. SubQueries In

-- non agg, non corr

select *

from `src/_/cbo`

where `src/_/cbo`.key in (select key from `src/_/cbo` s1 where s1.key > '9') order by key

;



-- agg, corr

-- add back once rank issue fixed for cbo



-- distinct, corr

select *

from `src/_/cbo` b

where b.key in

        (select distinct a.key

         from `src/_/cbo` a

         where b.value = a.value and a.key > '9'

        ) order by b.key

;



-- non agg, corr, with join in Parent Query

select p.p_partkey, li.l_suppkey

from (select distinct l_partkey as p_partkey from `line/item`) p join `line/item` li on p.p_partkey = li.l_partkey

where li.l_linenumber = 1 and

 li.l_orderkey in (select l_orderkey from `line/item` where l_shipmode = 'AIR' and l_linenumber = li.l_linenumber)

 order by p.p_partkey

;



-- where and having

-- Plan is:

-- Stage 1: b semijoin sq1:`src/_/cbo` (subquery in where)

-- Stage 2: group by Stage 1 o/p

-- Stage 5: group by on sq2:`src/_/cbo` (subquery in having)

-- Stage 6: Stage 2 o/p semijoin Stage 5

select key, value, count(*)

from `src/_/cbo` b

where b.key in (select key from `src/_/cbo` where `src/_/cbo`.key > '8')

group by key, value

having count(*) in (select count(*) from `src/_/cbo` s1 where s1.key > '9' group by s1.key ) order by key

;



-- non agg, non corr, windowing

select p_mfgr, p_name, avg(p_size)

from `p/a/r/t`

group by p_mfgr, p_name

having p_name in

  (select first_value(p_name) over(partition by p_mfgr order by p_size) from `p/a/r/t`) order by p_mfgr

;



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 16. SubQueries Not In

-- non agg, non corr

select *

from `src/_/cbo`

where `src/_/cbo`.key not in

  ( select key  from `src/_/cbo` s1

    where s1.key > '2'

  ) order by key

;



-- non agg, corr

select p_mfgr, b.p_name, p_size

from `p/a/r/t` b

where b.p_name not in

  (select p_name

  from (select p_mfgr, p_name, p_size as r from `p/a/r/t`) a

  where r < 10 and b.p_mfgr = a.p_mfgr

  ) order by p_mfgr,p_size

;



-- agg, non corr

select p_name, p_size

from

`p/a/r/t` where `p/a/r/t`.p_size not in

  (select avg(p_size)

  from (select p_size from `p/a/r/t`) a

  where p_size < 10

  ) order by p_name

;



-- agg, corr

select p_mfgr, p_name, p_size

from `p/a/r/t` b where b.p_size not in

  (select min(p_size)

  from (select p_mfgr, p_size from `p/a/r/t`) a

  where p_size < 10 and b.p_mfgr = a.p_mfgr

  ) order by  p_name

;



-- non agg, non corr, Group By in Parent Query

select li.l_partkey, count(*)

from `line/item` li

where li.l_linenumber = 1 and

  li.l_orderkey not in (select l_orderkey from `line/item` where l_shipmode = 'AIR')

group by li.l_partkey order by li.l_partkey

;



-- add null check test from sq_notin.q once HIVE-7721 resolved.



-- non agg, corr, having

select b.p_mfgr, min(p_retailprice)

from `p/a/r/t` b

group by b.p_mfgr

having b.p_mfgr not in

  (select p_mfgr

  from (select p_mfgr, min(p_retailprice) l, max(p_retailprice) r, avg(p_retailprice) a from `p/a/r/t` group by p_mfgr) a

  where min(p_retailprice) = l and r - l > 600

  )

  order by b.p_mfgr

;



-- agg, non corr, having

select b.p_mfgr, min(p_retailprice)

from `p/a/r/t` b

group by b.p_mfgr

having b.p_mfgr not in

  (select p_mfgr

  from `p/a/r/t` a

  group by p_mfgr

  having max(p_retailprice) - min(p_retailprice) > 600

  )

  order by b.p_mfgr

;



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- SORT_QUERY_RESULTS



-- 8. Test UDF/UDAF

select count(*), count(c_int), sum(c_int), avg(c_int), max(c_int), min(c_int) from `c/b/o_t1`;

select count(*), count(c_int) as a, sum(c_int), avg(c_int), max(c_int), min(c_int), case c_int when 0  then 1 when 1 then 2 else 3 end, sum(case c_int when 0  then 1 when 1 then 2 else 3 end) from `c/b/o_t1` group by c_int order by a;

select * from (select count(*) as a, count(distinct c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from `c/b/o_t1`) `c/b/o_t1`;

select * from (select count(*) as a, count(distinct c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f, case c_int when 0  then 1 when 1 then 2 else 3 end as g, sum(case c_int when 0  then 1 when 1 then 2 else 3 end) as h from `c/b/o_t1` group by c_int) `c/b/o_t1` order by a;

select f,a,e,b from (select count(*) as a, count(c_int) as b, sum(c_int) as c, avg(c_int) as d, max(c_int) as e, min(c_int) as f from `c/b/o_t1`) `c/b/o_t1`;

select f,a,e,b from (select count(*) as a, count(distinct c_int) as b, sum(distinct c_int) as c, avg(distinct c_int) as d, max(distinct c_int) as e, min(distinct c_int) as f from `c/b/o_t1`) `c/b/o_t1`;

select key,count(c_int) as a, avg(c_float) from `c/b/o_t1` group by key order by a;

select count(distinct c_int) as a, avg(c_float) from `c/b/o_t1` group by c_float order by a;

select count(distinct c_int) as a, avg(c_float) from `c/b/o_t1` group by c_int order by a;

select count(distinct c_int) as a, avg(c_float) from `c/b/o_t1` group by c_float, c_int order by a;

set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- SORT_QUERY_RESULTS



-- 11. Union All

select * from (select * from `c/b/o_t1` order by key, c_boolean, value, dt)a union all select * from (select * from `//cbo_t2` order by key, c_boolean, value, dt)b;

select key from (select key, c_int from (select * from `c/b/o_t1` union all select * from `//cbo_t2` where `//cbo_t2`.key >=0)r1 union all select key, c_int from `cbo_/t3////`)r2 where key >=0 order by key;

select r2.key from (select key, c_int from (select key, c_int from `c/b/o_t1` union all select key, c_int from `cbo_/t3////` )r1 union all select key, c_int from `cbo_/t3////`)r2 join   (select key, c_int from (select * from `c/b/o_t1` union all select * from `//cbo_t2` where `//cbo_t2`.key >=0)r1 union all select key, c_int from `cbo_/t3////`)r3 on r2.key=r3.key where r3.key >=0 order by r2.key;



set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 10. Test views

create view v1 as select c_int, value, c_boolean, dt from `c/b/o_t1`;

create view v2 as select c_int, value from `//cbo_t2`;



select value from v1 where c_boolean=false;

select max(c_int) from v1 group by (c_boolean);



select count(v1.c_int)  from v1 join `//cbo_t2` on v1.c_int = `//cbo_t2`.c_int;

select count(v1.c_int)  from v1 join v2 on v1.c_int = v2.c_int;



select count(*) from v1 a join v1 b on a.value = b.value;



create view v3 as select v1.value val from v1 join `c/b/o_t1` on v1.c_boolean = `c/b/o_t1`.c_boolean;



select count(val) from v3 where val != '1';

with q1 as ( select key from `c/b/o_t1` where key = '1')

select count(*) from q1;



with q1 as ( select value from v1 where c_boolean = false)

select count(value) from q1 ;



create view v4 as

with q1 as ( select key,c_int from `c/b/o_t1`  where key = '1')

select * from q1

;



with q1 as ( select c_int from q2 where c_boolean = false),

q2 as ( select c_int,c_boolean from v1  where value = '1')

select sum(c_int) from (select c_int from q1) a;



with q1 as ( select `c/b/o_t1`.c_int c_int from q2 join `c/b/o_t1` where q2.c_int = `c/b/o_t1`.c_int  and `c/b/o_t1`.dt='2014'),

q2 as ( select c_int,c_boolean from v1  where value = '1' or dt = '14')

select count(*) from q1 join q2 join v4 on q1.c_int = q2.c_int and v4.c_int = q2.c_int;





drop view v1;

drop view v2;

drop view v3;

drop view v4;

set hive.cbo.enable=false;

set hive.exec.check.crossproducts=false;



set hive.stats.fetch.column.stats=true;

set hive.auto.convert.join=false;



-- 9. Test Windowing Functions

-- SORT_QUERY_RESULTS



select count(c_int) over() from `c/b/o_t1`;

select count(c_int) over(partition by c_float order by key), sum(c_float) over(partition by c_float order by key), max(c_int) over(partition by c_float order by key), min(c_int) over(partition by c_float order by key), row_number() over(partition by c_float order by key) as rn, rank() over(partition by c_float order by key), dense_rank() over(partition by c_float order by key), round(percent_rank() over(partition by c_float order by key), 2), lead(c_int, 2, c_int) over(partition by c_float order by key), lag(c_float, 2, c_float) over(partition by c_float order by key) from `c/b/o_t1` order by rn;

select * from (select count(c_int) over(partition by c_float order by key), sum(c_float) over(partition by c_float order by key), max(c_int) over(partition by c_float order by key), min(c_int) over(partition by c_float order by key), row_number() over(partition by c_float order by key) as rn, rank() over(partition by c_float order by key), dense_rank() over(partition by c_float order by key), round(percent_rank() over(partition by c_float order by key),2), lead(c_int, 2, c_int) over(partition by c_float   order by key  ), lag(c_float, 2, c_float) over(partition by c_float   order by key) from `c/b/o_t1` order by rn) `c/b/o_t1`;

select x from (select count(c_int) over() as x, sum(c_float) over() from `c/b/o_t1`) `c/b/o_t1`;

select 1+sum(c_int) over() from `c/b/o_t1`;

select sum(c_int)+sum(sum(c_int)) over() from `c/b/o_t1`;

select * from (select max(c_int) over (partition by key order by value Rows UNBOUNDED PRECEDING), min(c_int) over (partition by key order by value rows current row), count(c_int) over(partition by key order by value ROWS 1 PRECEDING), avg(value) over (partition by key order by value Rows between unbounded preceding and unbounded following), sum(value) over (partition by key order by value rows between unbounded preceding and current row), avg(c_float) over (partition by key order by value Rows between 1 preceding and unbounded following), sum(c_float) over (partition by key order by value rows between 1 preceding and current row), max(c_float) over (partition by key order by value rows between 1 preceding and unbounded following), min(c_float) over (partition by key order by value rows between 1 preceding and 1 following) from `c/b/o_t1`) `c/b/o_t1`;

select i, a, h, b, c, d, e, f, g, a as x, a +1 as y from (select max(c_int) over (partition by key order by value range UNBOUNDED PRECEDING) a, min(c_int) over (partition by key order by value range current row) b, count(c_int) over(partition by key order by value range 1 PRECEDING) c, avg(value) over (partition by key order by value range between unbounded preceding and unbounded following) d, sum(value) over (partition by key order by value range between unbounded preceding and current row) e, avg(c_float) over (partition by key order by value range between 1 preceding and unbounded following) f, sum(c_float) over (partition by key order by value range between 1 preceding and current row) g, max(c_float) over (partition by key order by value range between 1 preceding and unbounded following) h, min(c_float) over (partition by key order by value range between 1 preceding and 1 following) i from `c/b/o_t1`) `c/b/o_t1`;

select *, rank() over(partition by key order by value) as rr from src1;

select *, rank() over(partition by key order by value) from src1;

insert into table `src/_/cbo` select * from src;

select * from `src/_/cbo` limit 1;

insert overwrite table `src/_/cbo` select * from src;

select * from `src/_/cbo` limit 1;

drop table `t//`;
create table `t//` (col string);
insert into `t//` values(1);
insert into `t//` values(null);
analyze table `t//` compute statistics;
explain select * from `t//`;
set hive.support.special.characters.tablename=false;

-- If hive.support.special.characters.tablename=false, we can not use special characters in table names.
-- The same query would work when it is set to true(default value).
-- Note that there is a positive test with the same name in clientpositive


create table `c/b/o_t1`(key string, value string, c_int int, c_float float, c_boolean boolean)  partitioned by (dt string) row format delimited fields terminated by ',' STORED AS TEXTFILE;





set hive.cbo.enable=true;

-- try the query without indexing, with manual indexing, and with automatic indexing
-- SORT_QUERY_RESULTS

DROP TABLE IF EXISTS `s/c`;

CREATE TABLE `s/c` (key STRING COMMENT 'default', value STRING COMMENT 'default') STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH "../../data/files/kv1.txt" INTO TABLE `s/c`;

ANALYZE TABLE `s/c` COMPUTE STATISTICS;

ANALYZE TABLE `s/c` COMPUTE STATISTICS FOR COLUMNS key,value;

-- without indexing
SELECT key, value FROM `s/c` WHERE key > 80 AND key < 100;

set hive.stats.dbclass=fs;
CREATE INDEX src_index ON TABLE `s/c`(key) as 'COMPACT' WITH DEFERRED REBUILD;
ALTER INDEX src_index ON `s/c` REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- manual indexing
INSERT OVERWRITE DIRECTORY "${system:test.tmp.dir}/index_where" SELECT `_bucketname` ,  `_offsets` FROM `default__s/c_src_index__` WHERE key > 80 AND key < 100;
SET hive.index.compact.file=${system:test.tmp.dir}/index_where;
SET hive.optimize.index.filter=false;
SET hive.input.format=org.apache.hadoop.hive.ql.index.compact.HiveCompactIndexInputFormat;

EXPLAIN SELECT key, value FROM `s/c` WHERE key > 80 AND key < 100;
SELECT key, value FROM `s/c` WHERE key > 80 AND key < 100;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

-- automatic indexing
EXPLAIN SELECT key, value FROM `s/c` WHERE key > 80 AND key < 100;
SELECT key, value FROM `s/c` WHERE key > 80 AND key < 100;

DROP INDEX src_index on `s/c`;
DROP TABLE tmp_jo_tab_test;
CREATE table tmp_jo_tab_test (message_line STRING)
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/input.txt'
OVERWRITE INTO TABLE tmp_jo_tab_test;

select size(split(message_line, '\t')) from tmp_jo_tab_test;
set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

select key from src tablesample(105 percent);
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

select key from src tablesample(1 percent);
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

select key from src tablesample(1K);
set hive.stats.autogather=true;
set datanucleus.cache.collections=false;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

CREATE TABLE stats_non_partitioned (key string, value string);

explain extended
insert overwrite table stats_non_partitioned
select * from src;

insert overwrite table stats_non_partitioned
select * from src;

desc extended stats_non_partitioned;

select * from stats_non_partitioned;


CREATE TABLE stats_partitioned(key string, value string) partitioned by (ds string);

explain
insert overwrite table stats_partitioned partition (ds='1')
select * from src;

insert overwrite table stats_partitioned partition (ds='1')
select * from src;

show partitions stats_partitioned;
select * from stats_partitioned where ds is not null;

describe extended stats_partitioned partition (ds='1');
describe extended stats_partitioned;


set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;

drop table stats_non_partitioned;
drop table stats_partitioned;

CREATE TABLE stats_non_partitioned (key string, value string);

explain extended
insert overwrite table stats_non_partitioned
select * from src;

insert overwrite table stats_non_partitioned
select * from src;

desc extended stats_non_partitioned;

select * from stats_non_partitioned;


CREATE TABLE stats_partitioned(key string, value string) partitioned by (ds string);

explain
insert overwrite table stats_partitioned partition (ds='1')
select * from src;

insert overwrite table stats_partitioned partition (ds='1')
select * from src;

show partitions stats_partitioned;
select * from stats_partitioned where ds is not null;

describe extended stats_partitioned partition (ds='1');
describe extended stats_partitioned;
set hive.mapred.mode=nonstrict;
set datanucleus.cache.collections=false;
set hive.stats.autogather=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.map.aggr=true;

-- SORT_QUERY_RESULTS

create table tmptable(key string, value string);

EXPLAIN
INSERT OVERWRITE TABLE tmptable
SELECT unionsrc.key, unionsrc.value
FROM (SELECT 'tst1' AS key, cast(count(1) AS string) AS value FROM src s1
      UNION  ALL
      SELECT s2.key AS key, s2.value AS value FROM src1 s2) unionsrc;

INSERT OVERWRITE TABLE tmptable
SELECT unionsrc.key, unionsrc.value
FROM (SELECT 'tst1' AS key, cast(count(1) AS string) AS value FROM src s1
      UNION  ALL
      SELECT s2.key AS key, s2.value AS value FROM src1 s2) unionsrc;

SELECT * FROM tmptable x SORT BY x.key, x.value;

DESCRIBE FORMATTED tmptable;

-- Load a file into a existing table
-- Some stats (numFiles, totalSize) should be updated correctly
-- Some other stats (numRows, rawDataSize) should be cleared
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE tmptable;
DESCRIBE FORMATTED tmptable;
set hive.mapred.mode=nonstrict;
set datanucleus.cache.collections=false;
set hive.stats.autogather=true;
;
set hive.exec.reducers.max = 1;

CREATE TABLE bucket3_1(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS;

explain
insert overwrite table bucket3_1 partition (ds='1')
select * from src;

insert overwrite table bucket3_1 partition (ds='1')
select * from src;

insert overwrite table bucket3_1 partition (ds='1')
select * from src;

insert overwrite table bucket3_1 partition (ds='2')
select * from src;

select * from bucket3_1 tablesample (bucket 1 out of 2) s where ds = '1' order by key;

explain analyze table bucket3_1 partition (ds) compute statistics;
analyze table bucket3_1 partition (ds) compute statistics;

describe formatted bucket3_1 partition (ds='1');
describe formatted bucket3_1 partition (ds='2');
describe formatted bucket3_1;
set datanucleus.cache.collections=false;
set hive.stats.autogather=true;

CREATE TABLE srcbucket_mapjoin(key int, value string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin;
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin;

CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
explain
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');

desc formatted srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
desc formatted srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
desc formatted srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
desc formatted srcbucket_mapjoin_part partition(ds='2008-04-08');

CREATE TABLE srcbucket_mapjoin_part_2 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part_2 partition(ds='2008-04-08');

create table bucketmapjoin_hash_result_1 (key bigint , value1 bigint, value2 bigint);
create table bucketmapjoin_hash_result_2 (key bigint , value1 bigint, value2 bigint);

set hive.optimize.bucketmapjoin = true;
create table bucketmapjoin_tmp_result (key string , value1 string, value2 string);

explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;

insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(b)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;


select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;


set hive.optimize.bucketmapjoin = true;
explain extended
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;


insert overwrite table bucketmapjoin_hash_result_1
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

set hive.optimize.bucketmapjoin = false;
insert overwrite table bucketmapjoin_tmp_result
select /*+mapjoin(a)*/ a.key, a.value, b.value
from srcbucket_mapjoin a join srcbucket_mapjoin_part b
on a.key=b.key where b.ds="2008-04-08";

select count(1) from bucketmapjoin_tmp_result;
insert overwrite table bucketmapjoin_hash_result_2
select sum(hash(key)), sum(hash(value1)), sum(hash(value2)) from bucketmapjoin_tmp_result;

select a.key-b.key, a.value1-b.value1, a.value2-b.value2
from bucketmapjoin_hash_result_1 a left outer join bucketmapjoin_hash_result_2 b
on a.key = b.key;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table analyze_srcpart like srcpart;
insert overwrite table analyze_srcpart partition (ds, hr) select * from srcpart where ds is not null;

explain extended
analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr) compute statistics;

analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr) compute statistics;

desc formatted analyze_srcpart;
desc formatted analyze_srcpart partition (ds='2008-04-08', hr=11);
desc formatted analyze_srcpart partition (ds='2008-04-08', hr=12);
desc formatted analyze_srcpart partition (ds='2008-04-09', hr=11);
desc formatted analyze_srcpart partition (ds='2008-04-09', hr=12);

set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table analyze_srcpart like srcpart;
insert overwrite table analyze_srcpart partition (ds, hr) select * from srcpart where ds is not null;

explain extended
analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics;

analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics;

desc formatted analyze_srcpart;
desc formatted analyze_srcpart partition (ds='2008-04-08', hr=11);
desc formatted analyze_srcpart partition (ds='2008-04-08', hr=12);
desc formatted analyze_srcpart partition (ds='2008-04-09', hr=11);
desc formatted analyze_srcpart partition (ds='2008-04-09', hr=12);

create table analyze_srcpart2 like analyze_srcpart;

desc formatted analyze_srcpart2;
set datanucleus.cache.collections=false;

create table stats_src like src;
insert overwrite table stats_src select * from src;
analyze table stats_src compute statistics;
desc formatted stats_src;

create table stats_part like srcpart;

insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src;
insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src;

analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics;
analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics;

insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src;

desc formatted stats_part;
desc formatted stats_part partition (ds='2010-04-08', hr = '11');
desc formatted stats_part partition (ds='2010-04-08', hr = '12');

analyze table stats_part partition(ds, hr) compute statistics;
desc formatted stats_part;

drop table stats_src;
drop table stats_part;set datanucleus.cache.collections=false;
set hive.stats.collect.rawdatasize=false;

create table stats_src like src;
insert overwrite table stats_src select * from src;
analyze table stats_src compute statistics;
desc formatted stats_src;

create table stats_part like srcpart;

insert overwrite table stats_part partition (ds='2010-04-08', hr = '11') select key, value from src;
insert overwrite table stats_part partition (ds='2010-04-08', hr = '12') select key, value from src;

analyze table stats_part partition(ds='2010-04-08', hr='11') compute statistics;
analyze table stats_part partition(ds='2010-04-08', hr='12') compute statistics;

insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src;

desc formatted stats_part;
desc formatted stats_part partition (ds='2010-04-08', hr = '11');
desc formatted stats_part partition (ds='2010-04-08', hr = '12');

analyze table stats_part partition(ds, hr) compute statistics;
desc formatted stats_part;

drop table stats_src;
drop table stats_part;
set hive.stats.autogather=false;

drop table stats16;

create table stats16 (key int, value string);
desc formatted stats16;

insert into table stats16 select * from src;
analyze table stats16 compute statistics;
desc formatted stats16;

drop table stats16;
set datanucleus.cache.collections=false;
set hive.stats.autogather=true;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.map.aggr=true;

create table stats_part like srcpart;

insert overwrite table stats_part partition (ds='2010-04-08', hr = '13') select key, value from src;

-- Load a file into a existing partition
-- Some stats (numFiles, totalSize) should be updated correctly
-- Some other stats (numRows, rawDataSize) should be cleared
desc formatted stats_part partition (ds='2010-04-08', hr='13');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE stats_part partition (ds='2010-04-08', hr='13');

desc formatted stats_part partition (ds='2010-04-08', hr='13');

drop table stats_src;
drop table stats_part;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.merge.mapfiles=false;

create table analyze_t1 like srcpart;


explain
insert overwrite table analyze_t1 partition (ds, hr) select * from srcpart where ds is not null;

insert overwrite table analyze_t1 partition (ds, hr) select * from srcpart where ds is not null;

desc formatted analyze_t1;

explain analyze table analyze_t1 partition (ds, hr) compute statistics;

analyze table analyze_t1 partition (ds, hr) compute statistics;

describe formatted analyze_t1;
set hive.stats.autogather=true;
set datanucleus.cache.collections=false;

set hive.stats.collect.rawdatasize=true;
CREATE TABLE stats_partitioned(key string, value string) partitioned by (ds string);
insert overwrite table stats_partitioned partition (ds='1')
select * from src;
-- rawDataSize is 5312 after config is turned on
describe formatted stats_partitioned;

set hive.stats.collect.rawdatasize=false;
insert overwrite table stats_partitioned partition (ds='1')
select * from src;
-- rawDataSize is 0 after config is turned off
describe formatted stats_partitioned;

set hive.mapred.mode=nonstrict;
set datanucleus.cache.collections=false;
set hive.stats.autogather=true;
drop table hive_test_src;
drop table hive_test_dst;

create table hive_test_src ( col1 string ) stored as textfile ;
explain extended
load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src ;

load data local inpath '../../data/files/test.dat' overwrite into table hive_test_src ;

desc formatted hive_test_src;

create table hive_test_dst ( col1 string ) partitioned by ( pcol1 string , pcol2 string) stored as sequencefile;
insert overwrite table hive_test_dst partition ( pcol1='test_part', pCol2='test_Part') select col1 from hive_test_src ;
select * from hive_test_dst where pcol1='test_part' and pcol2='test_Part';

select count(1) from hive_test_dst;

insert overwrite table hive_test_dst partition ( pCol1='test_part', pcol2='test_Part') select col1 from hive_test_src ;
select * from hive_test_dst where pcol1='test_part' and pcol2='test_part';

select count(1) from hive_test_dst;

select * from hive_test_dst where pcol1='test_part';
select * from hive_test_dst where pcol1='test_part' and pcol2='test_part';
select * from hive_test_dst where pcol1='test_Part';

describe formatted hive_test_dst;

drop table hive_test_src;
drop table hive_test_dst;
set datanucleus.cache.collections=false;
set hive.stats.autogather=true;

-- SORT_AND_HASH_QUERY_RESULTS

show partitions srcpart;

drop table nzhang_part1;
drop table nzhang_part2;

create table if not exists nzhang_part1 like srcpart;
create table if not exists nzhang_part2 like srcpart;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

explain
from srcpart
insert overwrite table nzhang_part1 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
insert overwrite table nzhang_part2 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08';

from srcpart
insert overwrite table nzhang_part1 partition (ds, hr) select key, value, ds, hr where ds <= '2008-04-08'
insert overwrite table nzhang_part2 partition(ds='2008-12-31', hr) select key, value, hr where ds > '2008-04-08';


show partitions nzhang_part1;
show partitions nzhang_part2;

select * from nzhang_part1 where ds is not null and hr is not null;
select * from nzhang_part2 where ds is not null and hr is not null;

describe formatted nzhang_part1 partition(ds='2008-04-08',hr=11);
describe formatted nzhang_part1 partition(ds='2008-04-08',hr=12);
describe formatted nzhang_part2 partition(ds='2008-12-31',hr=11);
describe formatted nzhang_part2 partition(ds='2008-12-31',hr=12);

describe formatted nzhang_part1;
describe formatted nzhang_part2;

drop table nzhang_part1;
drop table nzhang_part2;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider;
set datanucleus.cache.collections=false;

create table analyze_src as select * from src;

explain analyze table analyze_src compute statistics;

analyze table analyze_src compute statistics;

describe formatted analyze_src;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table analyze_srcpart like srcpart;
insert overwrite table analyze_srcpart partition (ds, hr) select * from srcpart where ds is not null;

analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics;
analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=12) compute statistics;

describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=12);
describe formatted analyze_srcpart PARTITION(ds='2008-04-09',hr=11);
describe formatted analyze_srcpart PARTITION(ds='2008-04-09',hr=12);

describe formatted analyze_srcpart;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table analyze_srcpart like srcpart;
insert overwrite table analyze_srcpart partition (ds, hr) select * from srcpart where ds is not null;

explain analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr) compute statistics;

analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr) compute statistics;

describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=12);

describe formatted analyze_srcpart;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table analyze_srcpart like srcpart;
insert overwrite table analyze_srcpart partition (ds, hr) select * from srcpart where ds is not null;

explain analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics;
analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics;
describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart;

explain analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=12) compute statistics;
analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=12) compute statistics;
describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=12);

explain analyze table analyze_srcpart PARTITION(ds='2008-04-09',hr=11) compute statistics;
analyze table analyze_srcpart PARTITION(ds='2008-04-09',hr=11) compute statistics;
describe formatted analyze_srcpart PARTITION(ds='2008-04-09',hr=11);

explain analyze table analyze_srcpart PARTITION(ds='2008-04-09',hr=12) compute statistics;
analyze table analyze_srcpart PARTITION(ds='2008-04-09',hr=12) compute statistics;
describe formatted analyze_srcpart PARTITION(ds='2008-04-09',hr=12);

explain analyze table analyze_srcpart PARTITION(ds, hr) compute statistics;
analyze table analyze_srcpart PARTITION(ds, hr) compute statistics;

describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=12);
describe formatted analyze_srcpart PARTITION(ds='2008-04-09',hr=11);
describe formatted analyze_srcpart PARTITION(ds='2008-04-09',hr=12);
describe formatted analyze_srcpart;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;

create table analyze_srcbucket like srcbucket;
insert overwrite table analyze_srcbucket select * from srcbucket;

explain analyze table analyze_srcbucket compute statistics;
analyze table analyze_srcbucket compute statistics;
describe formatted analyze_srcbucket;
set hive.stats.dbclass=fs;

-- stats computation on partitioned table with analyze command

create table t1 (key string, value string) partitioned by (ds string);
load data local inpath '../../data/files/kv1.txt' into table t1 partition (ds = '2010');
load data local inpath '../../data/files/kv1.txt' into table t1 partition (ds = '2011');

analyze table t1 partition (ds) compute statistics;

describe formatted t1 partition (ds='2010');
describe formatted t1 partition (ds='2011');

drop table t1;

-- stats computation on partitioned table with autogather on insert query

create table t1 (key string, value string) partitioned by (ds string);

insert into table t1 partition (ds='2010') select * from src;
insert into table t1 partition (ds='2011') select * from src;

describe formatted t1 partition (ds='2010');
describe formatted t1 partition (ds='2011');

drop table t1;

-- analyze stmt on unpartitioned table

create table t1 (key string, value string);
load data local inpath '../../data/files/kv1.txt' into table t1;

analyze table t1 compute statistics;

describe formatted t1 ;

drop table t1;

-- stats computation on unpartitioned table with autogather on insert query

create table t1 (key string, value string);

insert into table t1  select * from src;

describe formatted t1 ;

drop table t1;

-- stats computation on partitioned table with autogather on insert query with dynamic partitioning


create table t1 (key string, value string) partitioned by (ds string, hr string);

set hive.exec.dynamic.partition.mode=nonstrict;
insert into table t1 partition (ds,hr) select * from srcpart;

describe formatted t1 partition (ds='2008-04-08',hr='11');
describe formatted t1 partition (ds='2008-04-09',hr='12');

drop table t1;
set hive.exec.dynamic.partition.mode=strict;
-- In this test, there is a dummy stats aggregator which throws an error when various
-- methods are called (as indicated by the parameter hive.test.dummystats.aggregator)
-- Since stats need not be reliable (by setting hive.stats.reliable to false), the
-- insert statements succeed. The insert statement succeeds even if the stats aggregator
-- is set to null, since stats need not be reliable.

create table tmptable(key string, value string);

set hive.stats.dbclass=custom;
set hive.stats.default.publisher=org.apache.hadoop.hive.ql.stats.DummyStatsPublisher;
set hive.stats.default.aggregator=org.apache.hadoop.hive.ql.stats.DummyStatsAggregator;
set hive.stats.reliable=false;

set hive.test.dummystats.aggregator=connect;

INSERT OVERWRITE TABLE tmptable select * from src;
select count(1) from tmptable;

set hive.test.dummystats.aggregator=closeConnection;
INSERT OVERWRITE TABLE tmptable select * from src;
select count(1) from tmptable;

set hive.test.dummystats.aggregator=cleanUp;
INSERT OVERWRITE TABLE tmptable select * from src;
select count(1) from tmptable;

set hive.stats.default.aggregator="";
INSERT OVERWRITE TABLE tmptable select * from src;
select count(1) from tmptable;
-- In this test, there is a dummy stats aggregator which throws an error when the
-- method connect is called (as indicated by the parameter hive.test.dummystats.aggregator)
-- If stats need not be reliable, the statement succeeds. However, if stats are supposed
-- to be reliable (by setting hive.stats.reliable to true), the insert statement fails
-- because stats cannot be collected for this statement

create table tmptable(key string, value string);

set hive.stats.dbclass=custom;
set hive.stats.default.publisher=org.apache.hadoop.hive.ql.stats.DummyStatsPublisher;
set hive.stats.default.aggregator=org.apache.hadoop.hive.ql.stats.DummyStatsAggregator;
set hive.test.dummystats.aggregator=connect;

set hive.stats.reliable=false;
INSERT OVERWRITE TABLE tmptable select * from src;

set hive.stats.reliable=true;
INSERT OVERWRITE TABLE tmptable select * from src;
-- In this test, the stats aggregator does not exists.
-- If stats need not be reliable, the statement succeeds. However, if stats are supposed
-- to be reliable (by setting hive.stats.reliable to true), the insert statement fails
-- because stats cannot be collected for this statement

create table tmptable(key string, value string);

set hive.stats.dbclass=custom;
set hive.stats.default.publisher=org.apache.hadoop.hive.ql.stats.DummyStatsPublisher;
set hive.stats.default.aggregator="";

set hive.stats.reliable=false;
INSERT OVERWRITE TABLE tmptable select * from src;

set hive.stats.reliable=true;
INSERT OVERWRITE TABLE tmptable select * from src;
-- This test verifies writing a query using dynamic partitions
-- which results in no partitions actually being created with
-- hive.stats.reliable set to true

create table tmptable(key string) partitioned by (part string);

set hive.stats.autogather=true;
set hive.stats.reliable=true;
set hive.exec.dynamic.partition.mode=nonstrict;

explain insert overwrite table tmptable partition (part) select key, value from src where key = 'no_such_value';

insert overwrite table tmptable partition (part) select key, value from src where key = 'no_such_value';
-- This test verifies that writing an empty partition succeeds when
-- hive.stats.reliable is set to true.

create table tmptable(key string, value string) partitioned by (part string);

set hive.stats.autogather=true;
set hive.stats.reliable=true;

insert overwrite table tmptable partition (part = '1') select * from src where key = 'no_such_value';

describe formatted tmptable partition (part = '1');
set hive.stats.autogather=true;

CREATE TABLE stats_invalid (key string, value string);

insert overwrite table stats_invalid
select * from src;

analyze table stats_invalid compute statistics for columns key,value;

desc formatted stats_invalid;
alter table stats_invalid add  columns (new_col string);

desc formatted stats_invalid;
drop table stats_invalid;


-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- JAVA_VERSION_SPECIFIC_OUTPUT

drop table stats_list_bucket;
drop table stats_list_bucket_1;

create table stats_list_bucket (
  c1 string,
  c2 string
) partitioned by (ds string, hr string)
skewed by (c1, c2) on  (('466','val_466'),('287','val_287'),('82','val_82'))
stored as directories
stored as rcfile;


-- Try partitioned table with list bucketing.
-- The stats should show 500 rows loaded, as many rows as the src table has.

insert overwrite table stats_list_bucket partition (ds = '2008-04-08',  hr = '11')
  select key, value from src;

desc formatted stats_list_bucket partition (ds = '2008-04-08',  hr = '11');

-- Also try non-partitioned table with list bucketing.
-- Stats should show the same number of rows.

create table stats_list_bucket_1 (
  c1 string,
  c2 string
)
skewed by (c1, c2) on  (('466','val_466'),('287','val_287'),('82','val_82'))
stored as directories
stored as rcfile;

insert overwrite table stats_list_bucket_1
  select key, value from src;

desc formatted stats_list_bucket_1;

drop table stats_list_bucket;
drop table stats_list_bucket_1;
set hive.explain.user=false;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

-- test analyze table ... compute statistics noscan

-- 1. test full spec
create table analyze_srcpart like srcpart;
insert overwrite table analyze_srcpart partition (ds, hr) select * from srcpart where ds is not null;
explain
analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics noscan;
analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=11) compute statistics noscan;
analyze table analyze_srcpart PARTITION(ds='2008-04-08',hr=12) compute statistics noscan;
-- confirm result
describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart PARTITION(ds='2008-04-08',hr=12);
describe formatted analyze_srcpart PARTITION(ds='2008-04-09',hr=11);
describe formatted analyze_srcpart PARTITION(ds='2008-04-09',hr=12);
describe formatted analyze_srcpart;
drop table analyze_srcpart;

-- 2. test partial spec
create table analyze_srcpart_partial like srcpart;
insert overwrite table analyze_srcpart_partial partition (ds, hr) select * from srcpart where ds is not null;
explain
analyze table analyze_srcpart_partial PARTITION(ds='2008-04-08') compute statistics noscan;
analyze table analyze_srcpart_partial PARTITION(ds='2008-04-08') compute statistics noscan;
-- confirm result
describe formatted analyze_srcpart_partial PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart_partial PARTITION(ds='2008-04-08',hr=12);
describe formatted analyze_srcpart_partial PARTITION(ds='2008-04-09',hr=11);
describe formatted analyze_srcpart_partial PARTITION(ds='2008-04-09',hr=12);
drop table analyze_srcpart_partial;



dfs -cp ${system:hive.root}/data/files/ext_test ${system:test.tmp.dir}/analyze_external;

-- test analyze table compute statistiscs [noscan] on external table
-- 1 test table
CREATE EXTERNAL TABLE anaylyze_external (a INT) LOCATION '${system:test.tmp.dir}/analyze_external';
SELECT * FROM anaylyze_external;
analyze table anaylyze_external compute statistics noscan;
describe formatted anaylyze_external;
analyze table anaylyze_external compute statistics;
describe formatted anaylyze_external;
drop table anaylyze_external;

-- 2 test partition
-- prepare data
create table texternal(key string, val string) partitioned by (insertdate string);
dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/texternal/2008-01-01;
alter table texternal add partition (insertdate='2008-01-01') location 'pfile://${system:test.tmp.dir}/texternal/2008-01-01';
from src insert overwrite table texternal partition (insertdate='2008-01-01') select *;
select count(*) from texternal where insertdate='2008-01-01';
-- create external table
CREATE EXTERNAL TABLE anaylyze_external (key string, val string) partitioned by (insertdate string) LOCATION "pfile://${system:test.tmp.dir}/texternal";
ALTER TABLE anaylyze_external ADD PARTITION (insertdate='2008-01-01') location 'pfile://${system:test.tmp.dir}/texternal/2008-01-01';
select count(*) from anaylyze_external where insertdate='2008-01-01';
-- analyze
analyze table anaylyze_external PARTITION (insertdate='2008-01-01') compute statistics noscan;
describe formatted anaylyze_external PARTITION (insertdate='2008-01-01');
analyze table anaylyze_external PARTITION (insertdate='2008-01-01') compute statistics;
describe formatted anaylyze_external PARTITION (insertdate='2008-01-01');
dfs -rmr ${system:test.tmp.dir}/texternal;
drop table anaylyze_external;




CREATE TABLE non_native1(key int, value string)
STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler';

-- we do not support analyze table ... noscan on non-native tables yet
analyze table non_native1 compute statistics noscan;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.stats.dbclass=fs;
set hive.compute.query.using.stats=true;
set hive.stats.autogather=true;
set hive.stats.fetch.column.stats=true;
CREATE TABLE temps_null(a double, b int, c STRING, d smallint) STORED AS TEXTFILE;

CREATE TABLE stats_null(a double, b int, c STRING, d smallint) STORED AS TEXTFILE;

CREATE TABLE stats_null_part(a double, b int, c STRING, d smallint) partitioned by (dt string) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/null.txt' INTO TABLE temps_null;

insert overwrite table stats_null select * from temps_null;
insert into table stats_null_part partition(dt='2010') select * from temps_null where d <=5;

insert into table stats_null_part partition(dt='2011') select * from temps_null where d > 5;
explain
select count(*), count(a), count(b), count(c), count(d) from stats_null;
explain
select count(*), count(a), count(b), count(c), count(d) from stats_null_part;


analyze table stats_null compute statistics for columns a,b,c,d;
analyze table stats_null_part partition(dt='2010') compute statistics for columns a,b,c,d;
analyze table stats_null_part partition(dt='2011') compute statistics for columns a,b,c,d;

describe formatted stats_null_part partition (dt='2010');
describe formatted stats_null_part partition (dt='2011');

explain
select count(*), count(a), count(b), count(c), count(d) from stats_null;
explain
select count(*), count(a), count(b), count(c), count(d) from stats_null_part;


select count(*), count(a), count(b), count(c), count(d) from stats_null;
select count(*), count(a), count(b), count(c), count(d) from stats_null_part;

drop table stats_null_part;
set hive.exec.dynamic.partition.mode=nonstrict;
CREATE TABLE stats_null_part(a double, b int, c STRING, d smallint) partitioned by (dt int) STORED AS TEXTFILE;

insert into table stats_null_part partition(dt) select a,b,c,d,b from temps_null ;
analyze table stats_null_part compute statistics for columns;

describe formatted stats_null_part partition(dt = 1) a;

reset hive.exec.dynamic.partition.mode;
drop table stats_null;
drop table stats_null_part;
drop table temps_null;
set hive.compute.query.using.stats=false;

reset hive.stats.fetch.column.stats;
set hive.mapred.mode=nonstrict;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;

-- test analyze table ... compute statistics partialscan

-- 1. prepare data
CREATE table analyze_srcpart_partial_scan (key STRING, value STRING)
partitioned by (ds string, hr string)
stored as rcfile;
insert overwrite table analyze_srcpart_partial_scan partition (ds, hr) select * from srcpart where ds is not null order by key;
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11);


-- 2. partialscan
explain
analyze table analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11) compute statistics partialscan;
analyze table analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11) compute statistics partialscan;

-- 3. confirm result
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-09',hr=11);
drop table analyze_srcpart_partial_scan;




CREATE EXTERNAL TABLE external_table (key int, value string);

-- we do not support analyze table ... partialscan on EXTERNAL tables yet
analyze table external_table compute statistics partialscan;

CREATE TABLE non_native1(key int, value string)
STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler';

-- we do not support analyze table ... partialscan on non-native tables yet
analyze table non_native1 compute statistics partialscan;
set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)
-- This test uses mapred.max.split.size/mapred.max.split.size for controlling
-- number of input splits, which is not effective in hive 0.20.
-- stats_partscan_1_23.q is the same test with this but has different result.

-- test analyze table ... compute statistics partialscan

-- 1. prepare data
CREATE table analyze_srcpart_partial_scan (key STRING, value STRING)
partitioned by (ds string, hr string)
stored as rcfile;
insert overwrite table analyze_srcpart_partial_scan partition (ds, hr) select * from srcpart where ds is not null;
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11);

set hive.stats.autogather=true;

-- 2. partialscan
explain
analyze table analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11) compute statistics partialscan;
analyze table analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11) compute statistics partialscan;

-- 3. confirm result
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-09',hr=11);
drop table analyze_srcpart_partial_scan;



set datanucleus.cache.collections=false;
set hive.stats.autogather=false;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set mapred.min.split.size=256;
set mapred.min.split.size.per.node=256;
set mapred.min.split.size.per.rack=256;
set mapred.max.split.size=256;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- This test uses mapred.max.split.size/mapred.max.split.size for controlling
-- number of input splits.
-- stats_partscan_1.q is the same test with this but has different result.

-- test analyze table ... compute statistics partialscan

-- 1. prepare data
CREATE table analyze_srcpart_partial_scan (key STRING, value STRING)
partitioned by (ds string, hr string)
stored as rcfile;
insert overwrite table analyze_srcpart_partial_scan partition (ds, hr) select * from srcpart where ds is not null;
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11);

set hive.stats.autogather=true;

-- 2. partialscan
explain
analyze table analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11) compute statistics partialscan;
analyze table analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11) compute statistics partialscan;

-- 3. confirm result
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11);
describe formatted analyze_srcpart_partial_scan PARTITION(ds='2008-04-09',hr=11);
drop table analyze_srcpart_partial_scan;



set datanucleus.cache.collections=false;
set hive.stats.autogather=true;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

-- test analyze table ... compute statistics partialscan

create table analyze_srcpart_partial_scan like srcpart;
insert overwrite table analyze_srcpart_partial_scan partition (ds, hr) select * from srcpart where ds is not null;
analyze table analyze_srcpart_partial_scan PARTITION(ds='2008-04-08',hr=11) compute statistics partialscan;


set hive.mapred.mode=nonstrict;
set hive.stats.fetch.column.stats=true;

drop table ss;

CREATE TABLE ss (
    sales_order_id  BIGINT,
    order_amount    FLOAT)
PARTITIONED BY (country STRING, year INT, month INT, day INT) stored as orc;

insert into ss partition(country="US", year=2015, month=1, day=1) values(1,22.0);
insert into ss partition(country="US", year=2015, month=2, day=1) values(2,2.0);
insert into ss partition(country="US", year=2015, month=1, day=2) values(1,2.0);

ANALYZE TABLE ss PARTITION(country,year,month,day) compute statistics for columns;

explain select sum(order_amount) from ss where (country="US" and year=2015 and month=2 and day=1);

explain select sum(order_amount) from ss where (year*10000+month*100+day) = "2015010" and 1>0;

explain select sum(order_amount) from ss where (year*100+month*10+day) = "201511" and 1>0;

explain select sum(order_amount) from ss where (year*100+month*10+day) > "201511" and 1>0;

explain select '1' from ss where (year*100+month*10+day) > "201511";-- In this test, there is a dummy stats publisher which throws an error when various
-- methods are called (as indicated by the parameter hive.test.dummystats.publisher)
-- Since stats need not be reliable (by setting hive.stats.reliable to false), the
-- insert statements succeed. The insert statement succeeds even if the stats publisher
-- is set to null, since stats need not be reliable.

create table tmptable(key string, value string);

set hive.stats.dbclass=custom;
set hive.stats.default.publisher=org.apache.hadoop.hive.ql.stats.DummyStatsPublisher;
set hive.stats.default.aggregator=org.apache.hadoop.hive.ql.stats.DummyStatsAggregator;
set hive.stats.reliable=false;

set hive.test.dummystats.publisher=connect;

INSERT OVERWRITE TABLE tmptable select * from src;
select count(1) from tmptable;

set hive.test.dummystats.publisher=publishStat;
INSERT OVERWRITE TABLE tmptable select * from src;
select count(1) from tmptable;

set hive.test.dummystats.publisher=closeConnection;
INSERT OVERWRITE TABLE tmptable select * from src;
select count(1) from tmptable;

set hive.stats.default.publisher="";
INSERT OVERWRITE TABLE tmptable select * from src;
select count(1) from tmptable;
-- In this test, there is a dummy stats publisher which throws an error when the
-- method connect is called (as indicated by the parameter hive.test.dummystats.publisher)
-- If stats need not be reliable, the statement succeeds. However, if stats are supposed
-- to be reliable (by setting hive.stats.reliable to true), the insert statement fails
-- because stats cannot be collected for this statement

create table tmptable(key string, value string);

set hive.stats.dbclass=custom;
set hive.stats.default.publisher=org.apache.hadoop.hive.ql.stats.DummyStatsPublisher;
set hive.stats.default.aggregator=org.apache.hadoop.hive.ql.stats.DummyStatsAggregator;
set hive.test.dummystats.publisher=connect;

set hive.stats.reliable=false;
INSERT OVERWRITE TABLE tmptable select * from src;

set hive.stats.reliable=true;
INSERT OVERWRITE TABLE tmptable select * from src;
-- In this test, the stats publisher does not exists.
-- If stats need not be reliable, the statement succeeds. However, if stats are supposed
-- to be reliable (by setting hive.stats.reliable to true), the insert statement fails
-- because stats cannot be collected for this statement

create table tmptable(key string, value string);

set hive.stats.dbclass=custom;
set hive.stats.default.publisher="";
set hive.stats.default.aggregator=org.apache.hadoop.hive.ql.stats.DummyStatsAggregator;

set hive.stats.reliable=false;
INSERT OVERWRITE TABLE tmptable select * from src;

set hive.stats.reliable=true;
INSERT OVERWRITE TABLE tmptable select * from src;
set hive.mapred.mode=nonstrict;
DROP TABLE stored_as_custom_text_serde;
CREATE TABLE stored_as_custom_text_serde(key string, value string) STORED AS customtextserde;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE stored_as_custom_text_serde;
SELECT * FROM stored_as_custom_text_serde ORDER BY key, value;
DROP TABLE stored_as_custom_text_serde;
set hive.mapred.mode=strict;

SELECT *  FROM src src1 JOIN src src2;
set hive.mapred.mode=strict;

EXPLAIN
SELECT src.key, src.value from src order by src.key;

SELECT src.key, src.value from src order by src.key;

set hive.mapred.mode=strict;

EXPLAIN
SELECT count(1) FROM srcPART;

SELECT count(1) FROM srcPART;
create table t11 (`id` string, `lineid` string);
set hive.cbo.enable=false;
set hive.tez.dynamic.partition.pruning=false;
set hive.vectorized.execution.enabled=true;

explain select * from t11 where struct(`id`, `lineid`)
IN (
struct('1234-1111-0074578664','3'),
struct('1234-1111-0074578695','1'),
struct('1234-1111-0074580704','1'),
struct('1234-1111-0074581619','2'),
struct('1234-1111-0074582745','1'),
struct('1234-1111-0074586625','1'),
struct('1234-1111-0074019112','1'),
struct('1234-1111-0074019610','1'),
struct('1234-1111-0074022106','1')
);

explain select * from t11 where struct(`id`, `lineid`)
IN (
struct('1234-1111-0074578664','3'),
struct('1234-1111-0074578695',1)
);
drop table testreserved;

create table testreserved (data struct<`end`:string, id: string>);

create view testreservedview as select data.`end` as data_end, data.id as data_id from testreserved;

describe extended testreservedview;

select data.`end` from testreserved;

drop view testreservedview;

drop table testreserved;

create table s (default struct<src:struct<`end`:struct<key:string>, id: string>, id: string>);

create view vs1 as select default.src.`end`.key from s;

describe extended vs1;

create view vs2 as select default.src.`end` from s;

describe extended vs2;

drop view vs1;

drop view vs2;

create view v as select named_struct('key', 1).key from src limit 1;

desc extended v;

select * from v;

drop view v;

create view v as select named_struct('end', 1).`end` from src limit 1;

desc extended v;

select * from v;

drop view v;

set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

-- JAVA_VERSION_SPECIFIC_OUTPUT

desc function str_to_map;
desc function extended str_to_map;

explain select str_to_map('a=1,b=2,c=3',',','=')['a'] from src limit 3;
select str_to_map('a=1,b=2,c=3',',','=')['a'] from src limit 3;

explain select str_to_map('a:1,b:2,c:3') from src limit 3;
select str_to_map('a:1,b:2,c:3') from src limit 3;

explain select str_to_map('a:1,b:2,c:3',',',':') from src limit 3;
select str_to_map('a:1,b:2,c:3',',',':') from src limit 3;

explain select str_to_map(t.ss,',',':')['a']
from (select transform('a:1,b:2,c:3') using 'cat' as (ss) from src) t
limit 3;
select str_to_map(t.ss,',',':')['a']
from (select transform('a:1,b:2,c:3') using 'cat' as (ss) from src) t
limit 3;


drop table tbl_s2m;
create table tbl_s2m as select 'ABC=CC_333=444' as t from src tablesample (3 rows);

select str_to_map(t,'_','=')['333'] from tbl_s2m;

drop table tbl_s2m;
FROM (
  FROM src select src.* WHERE src.key < 100
) unioninput
INSERT OVERWRITE DIRECTORY '../build/ql/test/data/warehouse/union.out' SELECT unioninput.*
set hive.mapred.mode=nonstrict;
EXPLAIN
FROM (
  FROM src select src.* WHERE src.key < 100
) unioninput
INSERT OVERWRITE DIRECTORY 'target/warehouse/union.out' SELECT unioninput.*;

FROM (
  FROM src select src.* WHERE src.key < 100
) unioninput
INSERT OVERWRITE DIRECTORY 'target/warehouse/union.out' SELECT unioninput.*;

dfs -cat ${system:test.warehouse.dir}/union.out/*;

set hive.mapred.mode=nonstrict;
EXPLAIN
SELECT a.k, a.c
FROM (SELECT b.key as k, count(1) as c FROM src b GROUP BY b.key) a
WHERE a.k >= 90;

SELECT a.k, a.c
FROM (SELECT b.key as k, count(1) as c FROM src b GROUP BY b.key) a
WHERE a.k >= 90;
set hive.mapred.mode=nonstrict;
EXPLAIN
FROM (
  FROM src select src.* WHERE src.key < 100
) as unioninput
INSERT OVERWRITE DIRECTORY 'target/warehouse/union.out' SELECT unioninput.*;

EXPLAIN
SELECT * FROM
( SELECT * FROM
   ( SELECT * FROM src as s ) as src1
) as src2;

SELECT * FROM
( SELECT * FROM
   ( SELECT * FROM src as s ) as src1
) as src2;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

-- no agg, corr
-- SORT_QUERY_RESULTS
explain
select *
from src b
where exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9'
  )
;

select *
from src b
where exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9'
  )
;

-- view test
create view cv1 as
select *
from src b
where exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9')
;

select * from cv1
;

-- sq in from
select *
from (select *
      from src b
      where exists
          (select a.key
          from src a
          where b.value = a.value  and a.key = b.key and a.value > 'val_9')
     ) a
;
set hive.cbo.enable=false;

-- no agg, corr
explain rewrite
select *
from src b
where exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9'
  )
;

-- sq in from
explain rewrite
select *
from (select *
      from src b
      where exists
          (select a.key
          from src a
          where b.value = a.value  and a.key = b.key and a.value > 'val_9')
     ) a
;set hive.mapred.mode=nonstrict;
set hive.optimize.correlation=false;

-- no agg, corr
explain
select b.key, count(*)
from src b
group by b.key
having exists
  (select a.key
  from src a
  where a.key = b.key and a.value > 'val_9'
  )
;

select b.key, count(*)
from src b
group by b.key
having exists
  (select a.key
  from src a
  where a.key = b.key and a.value > 'val_9'
  )
;

set hive.optimize.correlation=true;

-- no agg, corr
explain
select b.key, count(*)
from src b
group by b.key
having exists
  (select a.key
  from src a
  where a.key = b.key and a.value > 'val_9'
  )
;

select b.key, count(*)
from src b
group by b.key
having exists
  (select a.key
  from src a
  where a.key = b.key and a.value > 'val_9'
  )
;

-- view test
create view cv1 as
select b.key, count(*) as c
from src b
group by b.key
having exists
  (select a.key
  from src a
  where a.key = b.key and a.value > 'val_9'
  )
;

select * from cv1;

-- sq in from
select *
from (select b.key, count(*)
  from src b
  group by b.key
  having exists
    (select a.key
    from src a
    where a.key = b.key and a.value > 'val_9'
    )
) a
;

-- join on agg
select b.key, min(b.value)
from src b
group by b.key
having exists ( select a.key
                from src a
                where a.value > 'val_9' and a.value = min(b.value)
                )
;

select *
from src b
where exists
  (select count(*)
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9'
  )
;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

-- non agg, non corr
explain
 select *
from src
where src.key in (select key from src s1 where s1.key > '9')
;

select *
from src
where src.key in (select key from src s1 where s1.key > '9')
;

-- non agg, corr
explain
select *
from src b
where b.key in
        (select a.key
         from src a
         where b.value = a.value and a.key > '9'
        )
;

select *
from src b
where b.key in
        (select a.key
         from src a
         where b.value = a.value and a.key > '9'
        )
;

-- agg, non corr
explain
select p_name, p_size
from
part where part.p_size in
	(select avg(p_size)
	 from (select p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
	 where r <= 2
	)
;
select p_name, p_size
from
part where part.p_size in
	(select avg(p_size)
	 from (select p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
	 where r <= 2
	)
;

-- agg, corr
explain
select p_mfgr, p_name, p_size
from part b where b.p_size in
	(select min(p_size)
	 from (select p_mfgr, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
	 where r <= 2 and b.p_mfgr = a.p_mfgr
	)
;

select p_mfgr, p_name, p_size
from part b where b.p_size in
	(select min(p_size)
	 from (select p_mfgr, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
	 where r <= 2 and b.p_mfgr = a.p_mfgr
	)
;

-- distinct, corr
explain
select *
from src b
where b.key in
        (select distinct a.key
         from src a
         where b.value = a.value and a.key > '9'
        )
;

select *
from src b
where b.key in
        (select distinct a.key
         from src a
         where b.value = a.value and a.key > '9'
        )
;

-- non agg, non corr, windowing
select p_mfgr, p_name, p_size
from part
where part.p_size in
  (select first_value(p_size) over(partition by p_mfgr order by p_size) from part)
;

-- non agg, non corr, with join in Parent Query
explain
select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR')
;

select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR')
;

-- non agg, corr, with join in Parent Query
select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR' and l_linenumber = li.l_linenumber)
;
set hive.cbo.enable=false;

-- non agg, non corr
explain rewrite
 select *
from src
where src.key in (select key from src s1 where s1.key > '9')
;

-- non agg, corr
explain rewrite
select *
from src b
where b.key in
        (select a.key
         from src a
         where b.value = a.value and a.key > '9'
        )
;

-- agg, non corr
explain rewrite
select p_name, p_size
from
part where part.p_size in
	(select avg(p_size)
	 from (select p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
	 where r <= 2
	)
;

-- agg, corr
explain rewrite
select p_mfgr, p_name, p_size
from part b where b.p_size in
	(select min(p_size)
	 from (select p_mfgr, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
	 where r <= 2 and b.p_mfgr = a.p_mfgr
	)
;

-- distinct, corr
explain rewrite
select *
from src b
where b.key in
        (select distinct a.key
         from src a
         where b.value = a.value and a.key > '9'
        )
;

-- non agg, non corr, windowing
explain rewrite
select p_mfgr, p_name, p_size
from part
where part.p_size in
  (select first_value(p_size) over(partition by p_mfgr order by p_size) from part)
;

-- non agg, non corr, with join in Parent Query
explain rewrite
select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR')
;

-- non agg, corr, with join in Parent Query
explain rewrite
select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR' and l_linenumber = li.l_linenumber)
;


select count(*)
from src
group by src.key in (select key from src s1 where s1.key > '9')set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

-- data setup
DROP TABLE IF EXISTS part_subq;

CREATE TABLE part_subq(
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
);

LOAD DATA LOCAL INPATH '../../data/files/part_tiny.txt' overwrite into table part_subq;

-- non agg, non corr
explain
 select key, count(*)
from src
group by key
having count(*) in (select count(*) from src s1 where s1.key > '9' group by s1.key )
;


select s1.key, count(*) from src s1 where s1.key > '9' group by s1.key;

select key, count(*)
from src
group by key
having count(*) in (select count(*) from src s1 where s1.key = '90' group by s1.key )
;

-- non agg, corr
explain
 select key, value, count(*)
from src b
group by key, value
having count(*) in (select count(*) from src s1 where s1.key > '9'  and s1.value = b.value group by s1.key )
;

set hive.optimize.correlation=false;

-- agg, non corr
explain
select p_mfgr, avg(p_size)
from part_subq b
group by b.p_mfgr
having b.p_mfgr in
   (select p_mfgr
    from part_subq
    group by p_mfgr
    having max(p_size) - min(p_size) < 20
   )
;

set hive.optimize.correlation=true;

-- agg, non corr
explain
select p_mfgr, avg(p_size)
from part_subq b
group by b.p_mfgr
having b.p_mfgr in
   (select p_mfgr
    from part_subq
    group by p_mfgr
    having max(p_size) - min(p_size) < 20
   )
;

-- join on agg
select b.key, min(b.value)
from src b
group by b.key
having b.key in ( select a.key
                from src a
                where a.value > 'val_9' and a.value = min(b.value)
                )
;

-- where and having
-- Plan is:
-- Stage 1: b semijoin sq1:src (subquery in where)
-- Stage 2: group by Stage 1 o/p
-- Stage 5: group by on sq2:src (subquery in having)
-- Stage 6: Stage 2 o/p semijoin Stage 5
explain
select key, value, count(*)
from src b
where b.key in (select key from src where src.key > '8')
group by key, value
having count(*) in (select count(*) from src s1 where s1.key > '9' group by s1.key )
;

set hive.auto.convert.join=true;
-- Plan is:
-- Stage  5: group by on sq2:src (subquery in having)
-- Stage 10: hashtable for sq1:src (subquery in where)
-- Stage  2: b map-side semijoin Stage 10 o/p
-- Stage  3: Stage 2 semijoin Stage 5
-- Stage  9: construct hastable for Stage 5 o/p
-- Stage  6: Stage 2 map-side semijoin Stage 9
explain
select key, value, count(*)
from src b
where b.key in (select key from src where src.key > '8')
group by key, value
having count(*) in (select count(*) from src s1 where s1.key > '9' group by s1.key )
;

-- non agg, non corr, windowing
explain
select p_mfgr, p_name, avg(p_size)
from part_subq
group by p_mfgr, p_name
having p_name in
  (select first_value(p_name) over(partition by p_mfgr order by p_size) from part_subq)
;

DROP TABLE part_subq;


select src.key in (select key from src s1 where s1.key > '9')
from src
;select * from src where src.key in (select key);set hive.mapred.mode=nonstrict;
set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecutePrinter,org.apache.hadoop.hive.ql.hooks.PrintCompletedTasksHook;

-- SORT_QUERY_RESULTS
-- JAVA_VERSION_SPECIFIC_OUTPUT

CREATE TABLE src_4(
  key STRING,
  value STRING
)
;

CREATE TABLE src_5(
  key STRING,
  value STRING
)
;

explain
from src b
INSERT OVERWRITE TABLE src_4
  select *
  where b.key in
   (select a.key
    from src a
    where b.value = a.value and a.key > '9'
   )
INSERT OVERWRITE TABLE src_5
  select *
  where b.key not in  ( select key from src s1 where s1.key > '2')
  order by key
;

from src b
INSERT OVERWRITE TABLE src_4
  select *
  where b.key in
   (select a.key
    from src a
    where b.value = a.value and a.key > '9'
   )
INSERT OVERWRITE TABLE src_5
  select *
  where b.key not in  ( select key from src s1 where s1.key > '2')
  order by key
;

select * from src_4
;
select * from src_5
;
set hive.auto.convert.join=true;

explain
from src b
INSERT OVERWRITE TABLE src_4
  select *
  where b.key in
   (select a.key
    from src a
    where b.value = a.value and a.key > '9'
   )
INSERT OVERWRITE TABLE src_5
  select *
  where b.key not in  ( select key from src s1 where s1.key > '2')
  order by key
;

from src b
INSERT OVERWRITE TABLE src_4
  select *
  where b.key in
   (select a.key
    from src a
    where b.value = a.value and a.key > '9'
   )
INSERT OVERWRITE TABLE src_5
  select *
  where b.key not in  ( select key from src s1 where s1.key > '2')
  order by key
;

select * from src_4
;
select * from src_5
;


explain
 select *
from src
where src.key in (select * from src s1 where s1.key > '9')
;select *
from part x
where x.p_name in (select y.p_name from part y where exists (select z.p_name from part z where y.p_name = z.p_name))
;set hive.mapred.mode=nonstrict;


-- no agg, corr
explain
select *
from src b
where not exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_2'
  )
;

select *
from src b
where not exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_2'
  )
;

-- distinct, corr
explain
select *
from src b
where not exists
  (select distinct a.key
  from src a
  where b.value = a.value and a.value > 'val_2'
  )
;

select *
from src b
where not exists
  (select a.key
  from src a
  where b.value = a.value and a.value > 'val_2'
  )
;set hive.mapred.mode=nonstrict;


-- no agg, corr
explain
select *
from src b
group by key, value
having not exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_12'
  )
;

select *
from src b
group by key, value
having not exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_12'
  )
;


-- distinct, corr
explain
select *
from src b
group by key, value
having not exists
  (select distinct a.key
  from src a
  where b.value = a.value and a.value > 'val_12'
  )
;

select *
from src b
group by key, value
having not exists
  (select distinct a.key
  from src a
  where b.value = a.value and a.value > 'val_12'
  )
;

select *
from src b
where not exists
  (select sum(1)
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9'
  )
;set hive.mapred.mode=nonstrict;
-- non agg, non corr
explain
select *
from src
where src.key not in
  ( select key  from src s1
    where s1.key > '2'
  )
;

select *
from src
where src.key not in  ( select key from src s1 where s1.key > '2')
order by key
;

-- non agg, corr
explain
select p_mfgr, b.p_name, p_size
from part b
where b.p_name not in
  (select p_name
  from (select p_mfgr, p_name, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
  where r <= 2 and b.p_mfgr = a.p_mfgr
  )
;

select p_mfgr, b.p_name, p_size
from part b
where b.p_name not in
  (select p_name
  from (select p_mfgr, p_name, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
  where r <= 2 and b.p_mfgr = a.p_mfgr
  )
order by p_mfgr, b.p_name
;

-- agg, non corr
explain
select p_name, p_size
from
part where part.p_size not in
  (select avg(p_size)
  from (select p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
  where r <= 2
  )
;
select p_name, p_size
from
part where part.p_size not in
  (select avg(p_size)
  from (select p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
  where r <= 2
  )
order by p_name, p_size
;

-- agg, corr
explain
select p_mfgr, p_name, p_size
from part b where b.p_size not in
  (select min(p_size)
  from (select p_mfgr, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
  where r <= 2 and b.p_mfgr = a.p_mfgr
  )
;

select p_mfgr, p_name, p_size
from part b where b.p_size not in
  (select min(p_size)
  from (select p_mfgr, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
  where r <= 2 and b.p_mfgr = a.p_mfgr
  )
order by p_mfgr, p_size
;

-- non agg, non corr, Group By in Parent Query
select li.l_partkey, count(*)
from lineitem li
where li.l_linenumber = 1 and
  li.l_orderkey not in (select l_orderkey from lineitem where l_shipmode = 'AIR')
group by li.l_partkey
;

-- alternate not in syntax
select *
from src
where not src.key in  ( select key from src s1 where s1.key > '2')
order by key
;

-- null check
create view T1_v as
select key from src where key <'11';

create view T2_v as
select case when key > '104' then null else key end as key from T1_v;

explain
select *
from T1_v where T1_v.key not in (select T2_v.key from T2_v);

select *
from T1_v where T1_v.key not in (select T2_v.key from T2_v);
set hive.mapred.mode=nonstrict;
-- non agg, non corr
-- JAVA_VERSION_SPECIFIC_OUTPUT

explain
select key, count(*)
from src
group by key
having key not in
  ( select key  from src s1
    where s1.key > '12'
  )
;

-- non agg, corr
explain
select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from (select p_mfgr, min(p_retailprice) l, max(p_retailprice) r, avg(p_retailprice) a from part group by p_mfgr) a
  where min(p_retailprice) = l and r - l > 600
  )
;

select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from (select p_mfgr, min(p_retailprice) l, max(p_retailprice) r, avg(p_retailprice) a from part group by p_mfgr) a
  where min(p_retailprice) = l and r - l > 600
  )
;

-- agg, non corr
explain
select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from part a
  group by p_mfgr
  having max(p_retailprice) - min(p_retailprice) > 600
  )
;

select b.p_mfgr, min(p_retailprice)
from part b
group by b.p_mfgr
having b.p_mfgr not in
  (select p_mfgr
  from part a
  group by p_mfgr
  having max(p_retailprice) - min(p_retailprice) > 600
  )
;


select *
from src
where src.key in (select key from src where key > '9')
;
explain
select *
from src
where src.key in (select key from src) in (select key from src)
;set hive.mapred.mode=nonstrict;
create table src11 (key1 string, value1 string);

create table part2(
    p2_partkey INT,
    p2_name STRING,
    p2_mfgr STRING,
    p2_brand STRING,
    p2_type STRING,
    p2_size INT,
    p2_container STRING,
    p2_retailprice DOUBLE,
    p2_comment STRING
);

-- non agg, corr
explain select * from src11 where src11.key1 in (select key from src where src11.value1 = value and key > '9');

explain select * from src a where a.key in (select key from src where a.value = value and key > '9');

-- agg, corr
explain
select p_mfgr, p_name, p_size
from part b where b.p_size in
  (select min(p2_size)
    from (select p2_mfgr, p2_size, rank() over(partition by p2_mfgr order by p2_size) as r from part2) a
    where r <= 2 and b.p_mfgr = p2_mfgr
  )
;


explain
select p_mfgr, p_name, p_size
from part b where b.p_size in
  (select min(p_size)
   from (select p_mfgr, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
   where r <= 2 and b.p_mfgr = p_mfgr
  )
;

-- distinct, corr
explain
select *
from src b
where b.key in
        (select distinct key
         from src
         where b.value = value and key > '9'
        )
;

-- non agg, corr, having
explain
 select key, value, count(*)
from src b
group by key, value
having count(*) in (select count(*) from src where src.key > '9'  and src.value = b.value group by key )
;

-- non agg, corr
explain
select p_mfgr, b.p_name, p_size
from part b
where b.p_name not in
  (select p_name
  from (select p_mfgr, p_name, p_size, rank() over(partition by p_mfgr order by p_size) as r from part) a
  where r <= 2 and b.p_mfgr = p_mfgr
  )
;set hive.mapred.mode=nonstrict;

explain
select * from src tablesample (10 rows) where lower(key) in (select key from src);
select * from src tablesample (10 rows) where lower(key) in (select key from src);

explain
select * from src tablesample (10 rows) where concat(key,value) not in (select key from src);
select * from src tablesample (10 rows) where concat(key,value) not in (select key from src);
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

-- exists test
create view cv1 as
select *
from src b
where exists
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9')
;

describe extended cv1;

select *
from cv1 where cv1.key in (select key from cv1 c where c.key > '95');
;


-- not in test
create view cv2 as
select *
from src b
where b.key not in
  (select a.key
  from src a
  where b.value = a.value  and a.key = b.key and a.value > 'val_11'
  )
;

describe extended cv2;

explain
select *
from cv2 where cv2.key in (select key from cv2 c where c.key < '11');
;

select *
from cv2 where cv2.key in (select key from cv2 c where c.key < '11');
;

-- in where + having
create view cv3 as
select key, value, count(*)
from src b
where b.key in (select key from src where src.key > '8')
group by key, value
having count(*) in (select count(*) from src s1 where s1.key > '9' group by s1.key )
;

describe extended cv3;

select * from cv3;


-- join of subquery views
select *
from cv3
where cv3.key in (select key from cv1);

drop table tc;

create table tc (`@d` int);

insert overwrite table tc select 1 from src limit 1;

drop view tcv;

create view tcv as select * from tc b where exists (select a.`@d` from tc a where b.`@d`=a.`@d`);

describe extended tcv;

select * from tcv;-- corr and windowing
select p_mfgr, p_name, p_size
from part a
where a.p_size in
  (select first_value(p_size) over(partition by p_mfgr order by p_size)
   from part b
   where a.p_brand = b.p_brand)
;
select count(*)
from src
where src.key in (select key from src s1 where s1.key > '9') or src.value is not null
;EXPLAIN
SELECT * FROM (INSERT OVERWRITE TABLE src1 SELECT * FROM src ) y;
set hive.mapred.mode=nonstrict;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask.size=10000000;
explain select src.key from src where src.key in ( select distinct key from src);

set hive.auto.convert.join=false;
set hive.mapred.mode=nonstrict;

select
cast(sum(key)*100 as decimal(15,3)) as c1
from src
order by c1;
set hive.mapred.mode=nonstrict;
DROP TABLE IF EXISTS symlink_text_input_format;

EXPLAIN
CREATE TABLE symlink_text_input_format (key STRING, value STRING) STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';

CREATE TABLE symlink_text_input_format (key STRING, value STRING) STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat';

dfs -cp ../../data/files/symlink1.txt ${system:test.warehouse.dir}/symlink_text_input_format/symlink1.txt;
dfs -cp ../../data/files/symlink2.txt ${system:test.warehouse.dir}/symlink_text_input_format/symlink2.txt;

EXPLAIN SELECT * FROM symlink_text_input_format order by key, value;

SELECT * FROM symlink_text_input_format order by key, value;

EXPLAIN SELECT value FROM symlink_text_input_format order by value;

SELECT value FROM symlink_text_input_format order by value;

EXPLAIN SELECT count(1) FROM symlink_text_input_format;

SELECT count(1) FROM symlink_text_input_format;

DROP TABLE symlink_text_input_format;
set hive.mapred.mode=nonstrict;

CREATE TABLE tmp_select(a INT, b STRING);
DESCRIBE tmp_select;

INSERT OVERWRITE TABLE tmp_select SELECT key, value FROM src;

SELECT a, b FROM tmp_select ORDER BY a;


set hive.mapred.mode=nonstrict;
SET hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.CheckTableAccessHook;
SET hive.stats.collect.tablekeys=true;

-- SORT_QUERY_RESULTS
-- This test is used for testing the TableAccessAnalyzer

CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;

-- Simple group-by queries
SELECT key, count(1) FROM T1 GROUP BY key;
SELECT key, val, count(1) FROM T1 GROUP BY key, val;

-- With subqueries and column aliases
SELECT key, count(1) FROM (SELECT key, val FROM T1) subq1 GROUP BY key;
SELECT k, count(1) FROM (SELECT key as k, val as v FROM T1) subq1 GROUP BY k;

-- With constants
SELECT 1, key, count(1) FROM T1 GROUP BY 1, key;
SELECT key, 1, val, count(1) FROM T1 GROUP BY key, 1, val;
SELECT key, 1, val, 2, count(1) FROM T1 GROUP BY key, 1, val, 2;

-- no mapping with functions
SELECT key, key + 1, count(1) FROM T1 GROUP BY key, key + 1;

SELECT key + key, sum(cnt) from
(SELECT key, count(1) as cnt FROM T1 GROUP BY key) subq1
group by key + key;

-- group by followed by union
SELECT * FROM (
SELECT key, count(1) as c FROM T1 GROUP BY key
 UNION ALL
SELECT key, count(1) as c FROM T1 GROUP BY key
) subq1;

-- group by followed by a join
SELECT * FROM
(SELECT key, count(1) as c FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, count(1) as c FROM T1 GROUP BY key) subq2
ON subq1.key = subq2.key;

SELECT * FROM
(SELECT key, count(1) as c FROM T1 GROUP BY key) subq1
JOIN
(SELECT key, val, count(1) as c FROM T1 GROUP BY key, val) subq2
ON subq1.key = subq2.key
ORDER BY subq1.key ASC, subq1.c ASC, subq2.key ASC, subq2.val ASC, subq2.c ASC;

-- constants from sub-queries should work fine
SELECT key, constant, val, count(1) from
(SELECT key, 1 as constant, val from T1) subq1
group by key, constant, val;

-- multiple levels of constants from sub-queries should work fine
SELECT key, constant3, val, count(1) FROM
(
  SELECT key, constant AS constant2, val, 2 AS constant3
  FROM
  (
    SELECT key, 1 AS constant, val
    FROM T1
  ) subq
) subq2
GROUP BY key, constant3, val;

-- work with insert overwrite
FROM T1
INSERT OVERWRITE TABLE T2 SELECT key, count(1) GROUP BY key, 1
INSERT OVERWRITE TABLE T3 SELECT key, sum(val) GROUP BY key;

-- simple joins
SELECT *
FROM T1 JOIN T2
ON T1.key = t2.key
ORDER BY T1.key ASC, T1.val ASC;

SELECT *
FROM T1 JOIN T2
ON T1.key = T2.key AND T1.val = T2.val;

-- map join
SELECT /*+ MAPJOIN(a) */ *
FROM T1 a JOIN T2 b
ON a.key = b.key;

-- with constant in join condition
SELECT *
FROM T1 JOIN T2
ON T1.key = T2.key AND T1.val = 3 and T2.val = 3;

-- subqueries
SELECT *
FROM
(
  SELECT val FROM T1 WHERE key = 5
) subq1
JOIN
(
  SELECT val FROM T2 WHERE key = 6
) subq2
ON subq1.val = subq2.val;

SELECT *
FROM
(
  SELECT val FROM T1 WHERE key = 5
) subq1
JOIN
T2
ON subq1.val = T2.val;

-- with column aliases in subqueries
SELECT *
FROM
(
  SELECT val as v FROM T1 WHERE key = 5
) subq1
JOIN
(
  SELECT val FROM T2 WHERE key = 6
) subq2
ON subq1.v = subq2.val;

-- with constants in subqueries
SELECT *
FROM
(
  SELECT key, val FROM T1
) subq1
JOIN
(
  SELECT key, 'teststring' as val FROM T2
) subq2
ON subq1.val = subq2.val AND subq1.key = subq2.key;

-- multiple levels of constants in subqueries
SELECT *
FROM
(
  SELECT key, val from
  (
    SELECT key, 'teststring' as val from T1
  ) subq1
) subq2
JOIN
(
  SELECT key, val FROM T2
) subq3
ON subq3.val = subq2.val AND subq3.key = subq2.key;

-- no mapping on functions
SELECT *
FROM
(
  SELECT key, val from T1
) subq1
JOIN
(
  SELECT key, val FROM T2
) subq2
ON subq1.val = subq2.val AND subq1.key + 1 = subq2.key;

-- join followed by group by
SELECT subq1.val, COUNT(*)
FROM
(
  SELECT key, val FROM T1
) subq1
JOIN
(
  SELECT key, 'teststring' as val FROM T2
) subq2
ON subq1.val = subq2.val AND subq1.key = subq2.key
GROUP BY subq1.val;

-- join followed by union
SELECT *
FROM
(
  SELECT subq1.val, COUNT(*)
  FROM
  (
    SELECT key, val FROM T1
  ) subq1
  JOIN
  (
    SELECT key, 'teststring' as val FROM T2
  ) subq2
  ON subq1.val = subq2.val AND subq1.key = subq2.key
  GROUP BY subq1.val
 UNION ALL
  SELECT val, COUNT(*)
  FROM T3
  GROUP BY val
) subq4;

-- join followed by join
SELECT *
FROM
(
  SELECT subq1.val as val, COUNT(*)
  FROM
  (
    SELECT key, val FROM T1
  ) subq1
  JOIN
  (
    SELECT key, 'teststring' as val FROM T2
  ) subq2
  ON subq1.val = subq2.val AND subq1.key = subq2.key
  GROUP by subq1.val
) T4
JOIN T3
ON T3.val = T4.val;
set hive.msck.path.validation=skip;
set hive.mapred.mode=nonstrict;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/temp_table_external/day=¢Bar;
dfs -copyFromLocal ../../data/files/in1.txt hdfs:///tmp/temp_table_external/day=¢Bar;
dfs -ls hdfs:///tmp/temp_table_external/day=¢Bar;

dfs ${system:test.dfs.mkdir} hdfs:///tmp/temp_table_external/day=Foo;
dfs -copyFromLocal ../../data/files/in1.txt hdfs:///tmp/temp_table_external/day=Foo;
dfs -ls hdfs:///tmp/temp_table_external/day=Foo;

dfs -ls hdfs:///tmp/temp_table_external;

create external table table_external (c1 int, c2 int)
partitioned by (day string)
location 'hdfs:///tmp/temp_table_external';

msck repair table table_external;

dfs -ls hdfs:///tmp/temp_table_external;

show partitions table_external;
select * from table_external;

alter table table_external drop partition (day='¢Bar');

show partitions table_external;

drop table table_external;

dfs -rmr hdfs:///tmp/temp_table_external;
set hive.msck.path.validation=throw;

dfs ${system:test.dfs.mkdir} hdfs:///tmp/temp_table_external/day=Foo;
dfs -copyFromLocal ../../data/files/in1.txt hdfs:///tmp/temp_table_external/day=Foo;
dfs -ls hdfs:///tmp/temp_table_external/day=Foo;

create external table table_external (c1 int, c2 int)
partitioned by (day string)
location 'hdfs:///tmp/temp_table_external';

msck repair table table_external;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
EXPLAIN CREATE TEMPORARY TABLE foo AS SELECT * FROM src WHERE key % 2 = 0;
CREATE TEMPORARY TABLE foo AS SELECT * FROM src WHERE key % 2 = 0;

EXPLAIN CREATE TEMPORARY TABLE bar AS SELECT * FROM src WHERE key % 2 = 1;
CREATE TEMPORARY TABLE bar AS SELECT * FROM src WHERE key % 2 = 1;

DESCRIBE foo;
DESCRIBE bar;

explain select * from foo order by key limit 10;
select * from foo order by key limit 10;

explain select * from (select * from foo union all select * from bar) u order by key limit 10;
select * from (select * from foo union all select * from bar) u order by key limit 10;

CREATE TEMPORARY TABLE baz LIKE foo;

INSERT OVERWRITE TABLE baz SELECT * from foo;

CREATE TEMPORARY TABLE bay (key string, value string) STORED AS orc;
select * from bay;

INSERT OVERWRITE TABLE bay SELECT * FROM src ORDER BY key;

select * from bay order by key limit 10;

SHOW TABLES;

CREATE DATABASE two;

USE two;

SHOW TABLES;

CREATE TEMPORARY TABLE foo AS SELECT * FROM default.foo;

SHOW TABLES;

use default;

DROP DATABASE two CASCADE;

DROP TABLE bay;

create table s as select * from src limit 10;

select count(*) from s;

create temporary table s as select * from s limit 2;

select count(*) from s;

with s as ( select * from src limit 1)
select count(*) from s;

with src as ( select * from s)
select count(*) from src;

drop table s;

select count(*) from s;

with s as ( select * from src limit 1)
select count(*) from s;

with src as ( select * from s)
select count(*) from src;

drop table s;
set hive.test.authz.sstd.hs2.mode=true;
set hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;
set hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;

set hive.security.authorization.enabled=true;
set user.name=user33;
create database db23221;
use db23221;

set user.name=user44;
create temporary table twew221(a string);
create table tab1 (c1 string) partitioned by (p1 string);
create table tmp1 like tab1;
show table create tmp1;
-- Based on display_colstats_tbllvl.q, output should be almost exactly the same.
DROP TABLE IF EXISTS UserVisits_web_text_none;

-- Hack, set external location because generated filename changes during test runs
CREATE TEMPORARY EXTERNAL TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile
location 'pfile://${system:test.tmp.dir}/uservisits_web_text_none';

LOAD DATA LOCAL INPATH "../../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none;

desc extended UserVisits_web_text_none sourceIP;
desc formatted UserVisits_web_text_none sourceIP;

explain
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

explain extended
analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;

analyze table UserVisits_web_text_none compute statistics for columns sourceIP, avgTimeOnSite, adRevenue;
desc formatted UserVisits_web_text_none sourceIP;
desc formatted UserVisits_web_text_none avgTimeOnSite;
desc formatted UserVisits_web_text_none adRevenue;

CREATE TEMPORARY TABLE empty_tab(
   a int,
   b double,
   c string,
   d boolean,
   e binary)
row format delimited fields terminated by '|'  stored as textfile;

desc formatted empty_tab a;
explain
analyze table empty_tab compute statistics for columns a,b,c,d,e;

analyze table empty_tab compute statistics for columns a,b,c,d,e;
desc formatted empty_tab a;
desc formatted empty_tab b;

CREATE DATABASE test;
USE test;

CREATE TEMPORARY TABLE UserVisits_web_text_none (
  sourceIP string,
  destURL string,
  visitDate string,
  adRevenue float,
  userAgent string,
  cCode string,
  lCode string,
  sKeyword string,
  avgTimeOnSite int)
row format delimited fields terminated by '|'  stored as textfile;

LOAD DATA LOCAL INPATH "../../data/files/UserVisits.dat" INTO TABLE UserVisits_web_text_none;

desc extended UserVisits_web_text_none sourceIP;
desc extended test.UserVisits_web_text_none sourceIP;
desc extended default.UserVisits_web_text_none sourceIP;
desc formatted UserVisits_web_text_none sourceIP;
desc formatted test.UserVisits_web_text_none sourceIP;
desc formatted default.UserVisits_web_text_none sourceIP;

analyze table UserVisits_web_text_none compute statistics for columns sKeyword;
desc extended UserVisits_web_text_none sKeyword;
desc formatted UserVisits_web_text_none sKeyword;
desc formatted test.UserVisits_web_text_none sKeyword;


dfs ${system:test.dfs.mkdir} hdfs:///tmp/temp_table_external;
dfs -copyFromLocal ../../data/files/in1.txt hdfs:///tmp/temp_table_external/;
dfs -ls hdfs:///tmp/temp_table_external/;

create temporary external table temp_table_external (c1 int, c2 int) location 'hdfs:///tmp/temp_table_external';
select * from temp_table_external;

-- Even after we drop the table, the data directory should still be there
drop table temp_table_external;
dfs -ls hdfs:///tmp/temp_table_external/;

dfs -rmr hdfs:///tmp/temp_table_external;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.map.aggr=false;
set hive.groupby.skewindata=true;

-- Taken from groupby2.q
CREATE TABLE dest_g2(key STRING, c1 INT, c2 STRING) STORED AS TEXTFILE;
CREATE TEMPORARY TABLE src_temp AS SELECT * FROM src;

FROM src_temp
INSERT OVERWRITE TABLE dest_g2 SELECT substr(src_temp.key,1,1), count(DISTINCT substr(src_temp.value,5)), concat(substr(src_temp.key,1,1),sum(substr(src_temp.value,5))) GROUP BY substr(src_temp.key,1,1);

SELECT dest_g2.* FROM dest_g2;

DROP TABLE dest_g2;
DROP TABLE src_temp;
create temporary table tmp1 (c1 string);
create index tmp1_idx on table tmp1 (c1) as 'COMPACT' with deferred rebuild;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

CREATE TABLE src_nontemp AS SELECT * FROM src limit 10;
CREATE TEMPORARY TABLE src_temp AS SELECT * FROM src limit 10;

-- Non temp table join
EXPLAIN
FROM src_nontemp src1 JOIN src_nontemp src2 ON (src1.key = src2.key)
SELECT src1.key, src2.value;

FROM src_nontemp src1 JOIN src_nontemp src2 ON (src1.key = src2.key)
SELECT src1.key, src2.value;

-- Non temp table join with temp table
EXPLAIN
FROM src_nontemp src1 JOIN src_temp src2 ON (src1.key = src2.key)
SELECT src1.key, src2.value;

FROM src_nontemp src1 JOIN src_temp src2 ON (src1.key = src2.key)
SELECT src1.key, src2.value;

-- temp table join with temp table
EXPLAIN
FROM src_temp src1 JOIN src_temp src2 ON (src1.key = src2.key)
SELECT src1.key, src2.value;

FROM src_temp src1 JOIN src_temp src2 ON (src1.key = src2.key)
SELECT src1.key, src2.value;

DROP TABLE src_nontemp;
DROP TABLE src_temp;

-- Test temp tables with upper/lower case names
create temporary table Default.Temp_Table_Names (C1 string, c2 string);

show tables 'Temp_Table*';
show tables in default 'temp_table_names';
show tables in DEFAULT 'TEMP_TABLE_NAMES';

select c1 from default.temp_table_names;
select C1 from DEFAULT.TEMP_TABLE_NAMES;

drop table Default.TEMP_TABLE_names;
show tables 'temp_table_names';
-- Delimiter test, taken from delimiter.q
create temporary table impressions (imp string, msg string)
row format delimited
fields terminated by '\t'
lines terminated by '\n'
stored as textfile;
LOAD DATA LOCAL INPATH '../../data/files/in7.txt' INTO TABLE impressions;

select * from impressions;

select imp,msg from impressions;

drop table impressions;


-- Try different SerDe formats, taken from date_serde.q

--
-- RegexSerDe
--
create temporary table date_serde_regex (
  ORIGIN_CITY_NAME string,
  DEST_CITY_NAME string,
  FL_DATE date,
  ARR_DELAY float,
  FL_NUM int
)
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe'
with serdeproperties (
  "input.regex" = "([^]*)([^]*)([^]*)([^]*)([0-9]*)"
)
stored as textfile;

load data local inpath '../../data/files/flights_tiny.txt.1' overwrite into table date_serde_regex;

select * from date_serde_regex;
select fl_date, count(*) from date_serde_regex group by fl_date;

--
-- LazyBinary
--
create temporary table date_serde_lb (
  c1 date,
  c2 int
);
alter table date_serde_lb set serde 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe';

insert overwrite table date_serde_lb
  select fl_date, fl_num from date_serde_regex limit 1;

select * from date_serde_lb;
select c1, sum(c2) from date_serde_lb group by c1;

--
-- LazySimple
--
create temporary table date_serde_ls (
  c1 date,
  c2 int
);
alter table date_serde_ls set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';

insert overwrite table date_serde_ls
  select c1, c2 from date_serde_lb limit 1;

select * from date_serde_ls;
select c1, sum(c2) from date_serde_ls group by c1;

--
-- Columnar
--
create temporary table date_serde_c (
  c1 date,
  c2 int
) stored as rcfile;
alter table date_serde_c set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';

insert overwrite table date_serde_c
  select c1, c2 from date_serde_ls limit 1;

select * from date_serde_c;
select c1, sum(c2) from date_serde_c group by c1;

--
-- LazyBinaryColumnar
--
create temporary table date_serde_lbc (
  c1 date,
  c2 int
) stored as rcfile;
alter table date_serde_lbc set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';

insert overwrite table date_serde_lbc
  select c1, c2 from date_serde_c limit 1;

select * from date_serde_lbc;
select c1, sum(c2) from date_serde_lbc group by c1;

--
-- ORC
--
create temporary table date_serde_orc (
  c1 date,
  c2 int
) stored as orc;
alter table date_serde_orc set serde 'org.apache.hadoop.hive.ql.io.orc.OrcSerde';

insert overwrite table date_serde_orc
  select c1, c2 from date_serde_lbc limit 1;

select * from date_serde_orc;
select c1, sum(c2) from date_serde_orc group by c1;
-- temp tables with partition columns not currently supported
create temporary table tmp1 (c1 string) partitioned by (p1 string);

create database ttp;

-- Create non-temp tables
create table ttp.tab1 (a1 string, a2 string);
insert overwrite table ttp.tab1 select * from src where key = 5 limit 5;
describe ttp.tab1;
select * from ttp.tab1;
create table ttp.tab2 (b1 string, b2 string);
insert overwrite table ttp.tab2 select * from src where key = 2 limit 5;
describe ttp.tab2;
select * from ttp.tab2;

-- Now create temp table with same name
create temporary table ttp.tab1 (c1 int, c2 string);
insert overwrite table ttp.tab1 select * from src where key = 0 limit 5;

-- describe/select should now use temp table
describe ttp.tab1;
select * from ttp.tab1;

-- rename the temp table, and now we can see our non-temp table again
use ttp;
alter table tab1 rename to tab2;
use default;
describe ttp.tab1;
select * from ttp.tab1;

-- now the non-temp tab2 should be hidden
describe ttp.tab2;
select * from ttp.tab2;

-- drop the temp table, and now we should be able to see the non-temp tab2 again
drop table ttp.tab2;
describe ttp.tab2;
select * from ttp.tab2;

drop database ttp cascade;
create temporary table tmp1 (c1 string);
create temporary table tmp2 (d1 string);
alter table tmp2 rename to tmp1;

create temporary table src_temp as select * from src;

-- subquery exists
select *
from src_temp b
where exists
  (select a.key
  from src_temp a
  where b.value = a.value  and a.key = b.key and a.value > 'val_9'
  )
;

-- subquery in
select *
from src_temp
where src_temp.key in (select key from src_temp s1 where s1.key > '9')
;

select b.key, min(b.value)
from src_temp b
group by b.key
having b.key in ( select a.key
                from src_temp a
                where a.value > 'val_9' and a.value = min(b.value)
                )
;

drop table src_temp;
drop table over10k;

create temporary table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select p_mfgr, p_retailprice, p_size,
round(sum(p_retailprice) over w1 , 2) = round(sum(lag(p_retailprice,1,0.0)) over w1 + last_value(p_retailprice) over w1 , 2),
max(p_retailprice) over w1 - min(p_retailprice) over w1 = last_value(p_retailprice) over w1 - first_value(p_retailprice) over w1
from part
window w1 as (distribute by p_mfgr sort by p_retailprice)
;
select p_mfgr, p_retailprice, p_size,
rank() over (distribute by p_mfgr sort by p_retailprice) as r,
sum(p_retailprice) over (distribute by p_mfgr sort by p_retailprice rows between unbounded preceding and current row) as s2,
sum(p_retailprice) over (distribute by p_mfgr sort by p_retailprice rows between unbounded preceding and current row) -5 as s1
from part
;

select s, si, f, si - lead(f, 3) over (partition by t order by bo,s,si,f desc) from over10k limit 100;
select s, i, i - lead(i, 3, 0) over (partition by si order by i,s) from over10k limit 100;
select s, si, d, si - lag(d, 3) over (partition by b order by si,s,d) from over10k limit 100;
select s, lag(s, 3, 'fred') over (partition by f order by b) from over10k limit 100;

select p_mfgr, avg(p_retailprice) over(partition by p_mfgr, p_type order by p_mfgr) from part;

select p_mfgr, avg(p_retailprice) over(partition by p_mfgr order by p_type,p_mfgr rows between unbounded preceding and current row) from part;

-- multi table insert test
create table t1 (a1 int, b1 string);
create table t2 (a1 int, b1 string);
from (select sum(i) over (partition by ts order by i), s from over10k) tt insert overwrite table t1 select * insert overwrite table t2 select * ;
select * from t1 limit 3;
select * from t2 limit 3;

select p_mfgr, p_retailprice, p_size,
round(sum(p_retailprice) over w1 , 2) + 50.0 = round(sum(lag(p_retailprice,1,50.0)) over w1 + (last_value(p_retailprice) over w1),2)
from part
window w1 as (distribute by p_mfgr sort by p_retailprice)
limit 11;
create table if not exists test_boolean(dummy tinyint);
insert overwrite table test_boolean  select 1 from src tablesample (1 rows);

SELECT 1
FROM (
SELECT TRUE AS flag
FROM test_boolean
) a
WHERE flag;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;


set hive.optimize.bucketingsorting=false;
set hive.auto.convert.join.noconditionaltask.size=10000;

create table test (key int, value string) partitioned by (p int) clustered by (key) into 2 buckets stored as textfile;
create table test1 (key int, value string) stored as textfile;

insert into table test partition (p=1) select * from src;

alter table test set fileformat orc;

insert into table test partition (p=2) select * from src;
insert into table test1 select * from src;

describe test;
set hive.auto.convert.join = true;
set hive.convert.join.bucket.mapjoin.tez = true;

explain select test.key, test.value from test join test1 on (test.key = test1.key) order by test.key;

select test.key, test.value from test join test1 on (test.key = test1.key) order by test.key;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.exec.dynamic.partition.mode=nonstrict;

-- CTAS
-- SORT_QUERY_RESULTS

EXPLAIN CREATE TABLE tmp_src AS SELECT * FROM (SELECT value, count(value) AS cnt FROM src GROUP BY value) f1 ORDER BY cnt;
CREATE TABLE tmp_src AS SELECT * FROM (SELECT value, count(value) AS cnt FROM src GROUP BY value) f1 ORDER BY cnt;

SELECT * FROM tmp_src;

-- dyn partitions
CREATE TABLE tmp_src_part (c string) PARTITIONED BY (d int);
EXPLAIN INSERT INTO TABLE tmp_src_part PARTITION (d) SELECT * FROM tmp_src;
INSERT INTO TABLE tmp_src_part PARTITION (d) SELECT * FROM tmp_src;

SELECT * FROM tmp_src_part;

-- multi insert
CREATE TABLE even (c int, d string);
CREATE TABLE odd (c int, d string);

EXPLAIN
FROM src
INSERT INTO TABLE even SELECT key, value WHERE key % 2 = 0
INSERT INTO TABLE odd SELECT key, value WHERE key % 2 = 1;

FROM src
INSERT INTO TABLE even SELECT key, value WHERE key % 2 = 0
INSERT INTO TABLE odd SELECT key, value WHERE key % 2 = 1;

SELECT * FROM even;
SELECT * FROM odd;

-- create empty table
CREATE TABLE empty STORED AS orc AS SELECT * FROM tmp_src_part WHERE d = -1000;
SELECT * FROM empty;

-- drop the tables
DROP TABLE even;
DROP TABLE odd;
DROP TABLE tmp_src;
DROP TABLE tmp_src_part;
set hive.mapred.mode=nonstrict;

set hive.explain.user=false;
set hive.auto.convert.join=false;
set hive.optimize.dynamic.partition.hashjoin=false;

-- First try with regular mergejoin
explain
select
  *
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
order by a.cint;

select
  *
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
order by a.cint;

explain
select
  count(*)
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null;

select
  count(*)
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null;

explain
select
  a.csmallint, count(*) c1
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
group by a.csmallint
order by c1;

select
  a.csmallint, count(*) c1
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
group by a.csmallint
order by c1;

set hive.auto.convert.join=true;
set hive.optimize.dynamic.partition.hashjoin=true;
set hive.auto.convert.join.noconditionaltask.size=200000;
set hive.exec.reducers.bytes.per.reducer=200000;

-- Try with dynamically partitioned hashjoin
explain
select
  *
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
order by a.cint;

select
  *
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
order by a.cint;

explain
select
  count(*)
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null;

select
  count(*)
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null;

explain
select
  a.csmallint, count(*) c1
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
group by a.csmallint
order by c1;

select
  a.csmallint, count(*) c1
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
group by a.csmallint
order by c1;
set hive.mapred.mode=nonstrict;

set hive.explain.user=false;
set hive.auto.convert.join=false;
set hive.optimize.dynamic.partition.hashjoin=false;

-- Multiple tables, and change the order of the big table (alltypesorc)
-- First try with regular mergejoin
explain
select
  a.*
from
  alltypesorc a,
  src b,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

select
  a.*
from
  alltypesorc a,
  src b,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

set hive.auto.convert.join=true;
set hive.optimize.dynamic.partition.hashjoin=true;
set hive.auto.convert.join.noconditionaltask.size=2000;
set hive.exec.reducers.bytes.per.reducer=200000;

-- noconditionaltask.size needs to be low enough that entire filtered table results do not fit in one task's hash table
-- Try with dynamically partitioned hash join
explain
select
  a.*
from
  alltypesorc a,
  src b,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

select
  a.*
from
  alltypesorc a,
  src b,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

-- Try different order of tables
explain
select
  a.*
from
  src b,
  alltypesorc a,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

select
  a.*
from
  src b,
  alltypesorc a,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;
set hive.optimize.limittranspose=true;
set hive.optimize.limittranspose.reductionpercentage=0.1f;
set hive.optimize.limittranspose.reductiontuples=100;
set hive.explain.user=false;
set hive.auto.convert.join=false;
set hive.optimize.dynamic.partition.hashjoin=false;
set hive.mapred.mode=nonstrict;
explain
select a.*
from alltypesorc a left outer join src b
on a.cint = cast(b.key as int) and (a.cint < 100)
limit 1;


set hive.auto.convert.join=true;
set hive.optimize.dynamic.partition.hashjoin=true;
set hive.auto.convert.join.noconditionaltask.size=200000;
set hive.exec.reducers.bytes.per.reducer=200000;

explain
select a.*
from alltypesorc a left outer join src b
on a.cint = cast(b.key as int) and (a.cint < 100)
limit 1;
set hive.mapred.mode=nonstrict;
set hive.execution.engine=tez;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
CREATE TABLE t1 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE t1 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE t1 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE t1 partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE t1 partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
set hive.stats.dbclass=fs;

insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from t1;
describe formatted tab_part partition(ds='2008-04-08');
insert overwrite local directory '${system:test.tmp.dir}/tez_local_src_table_1'
select * from src order by key limit 10 ;
dfs -cat ${system:test.tmp.dir.uri}/tez_local_src_table_1/* ;

dfs -rmr ${system:test.tmp.dir.uri}/tez_local_src_table_1/ ;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.sortmerge.join = true;

create table t1(
id string,
od string);

create table t2(
id string,
od string);

explain
select vt1.id from
(select rt1.id from
(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1
join
(select rt2.id from
(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2
where vt1.id=vt2.id;

select vt1.id from
(select rt1.id from
(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1
join
(select rt2.id from
(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2
where vt1.id=vt2.id;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS
explain
select * from (select b.key, b.value from src1 a left outer join src b on (a.key = b.key) order by b.key) x right outer join src c on (x.value = c.value) order by x.key;

select * from (select b.key, b.value from src1 a left outer join src b on (a.key = b.key) order by b.key) x right outer join src c on (x.value = c.value) order by x.key;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

create table orc_src (key string, value string) STORED AS ORC;
insert into table orc_src select * from src;

set hive.execution.engine=tez;
set hive.vectorized.execution.enabled=true;
set hive.auto.convert.join.noconditionaltask.size=1;
set hive.exec.reducers.bytes.per.reducer=20000;

explain
SELECT count(*) FROM src, orc_src where src.key=orc_src.key;

SELECT count(*) FROM src, orc_src where src.key=orc_src.key;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=3000;
set hive.mapjoin.hybridgrace.minwbsize=350;
set hive.mapjoin.hybridgrace.minnumpartitions=8;

explain
select count(*) from (select x.key as key, y.value as value from
srcpart x join srcpart y on (x.key = y.key)
union all
select key, value from srcpart z) a join src b on (a.value = b.value) group by a.key, a.value;

select key, count(*) from (select x.key as key, y.value as value from
srcpart x join srcpart y on (x.key = y.key)
union all
select key, value from srcpart z) a join src b on (a.value = b.value) group by a.key, a.value;

set hive.execution.engine=mr;
select key, count(*) from (select x.key as key, y.value as value from
srcpart x join srcpart y on (x.key = y.key)
union all
select key, value from srcpart z) a join src b on (a.value = b.value) group by a.key, a.value;


SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;
set hive.mapjoin.optimized.hashtable=true;
set hive.mapred.mode=nonstrict;
create table service_request_clean(
cnctevn_id          	string              ,
svcrqst_id          	string              ,
svcrqst_crt_dts     	string              ,
subject_seq_no      	int                 ,
plan_component      	string              ,
cust_segment        	string              ,
cnctyp_cd           	string              ,
cnctmd_cd           	string              ,
cnctevs_cd          	string              ,
svcrtyp_cd          	string              ,
svrstyp_cd          	string              ,
cmpltyp_cd          	string              ,
catsrsn_cd          	string              ,
apealvl_cd          	string              ,
cnstnty_cd          	string              ,
svcrqst_asrqst_ind  	string              ,
svcrqst_rtnorig_in  	string              ,
svcrqst_vwasof_dt   	string              ,
sum_reason_cd       	string              ,
sum_reason          	string              ,
crsr_master_claim_index	string              ,
svcrqct_cds         	array<string>       ,
svcrqst_lupdt       	string              ,
crsr_lupdt          	timestamp           ,
cntevsds_lupdt      	string              ,
ignore_me           	int                 ,
notes               	array<string>       )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY  '\t' LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '../../data/files/service_request_clean.txt' into table service_request_clean;

create table ct_events_clean(
contact_event_id    	string              ,
ce_create_dt        	string              ,
ce_end_dt           	string              ,
contact_type        	string              ,
cnctevs_cd          	string              ,
contact_mode        	string              ,
cntvnst_stts_cd     	string              ,
total_transfers     	int                 ,
ce_notes            	array<string>       )
ROW FORMAT DELIMITED
FIELDS TERMINATED BY  '\t' LINES TERMINATED BY '\n'
STORED AS TEXTFILE;

load data local inpath '../../data/files/ct_events_clean.txt' into table ct_events_clean;

set hive.mapjoin.hybridgrace.hashtable=false;
drop table if exists ct_events1_test;

explain extended
create table ct_events1_test
as select  a.*,
b.svcrqst_id,
b.svcrqct_cds,
b.svcrtyp_cd,
b.cmpltyp_cd,
b.sum_reason_cd as src,
b.cnctmd_cd,
b.notes
from ct_events_clean a
inner join
service_request_clean b
on a.contact_event_id = b.cnctevn_id;

-- SORT_QUERY_RESULTS

create table ct_events1_test
as select  a.*,
b.svcrqst_id,
b.svcrqct_cds,
b.svcrtyp_cd,
b.cmpltyp_cd,
b.sum_reason_cd as src,
b.cnctmd_cd,
b.notes
from ct_events_clean a
inner join
service_request_clean b
on a.contact_event_id = b.cnctevn_id;

select * from ct_events1_test;

set hive.mapjoin.hybridgrace.hashtable=true;
drop table if exists ct_events1_test;

explain extended
create table ct_events1_test
as select  a.*,
b.svcrqst_id,
b.svcrqct_cds,
b.svcrtyp_cd,
b.cmpltyp_cd,
b.sum_reason_cd as src,
b.cnctmd_cd,
b.notes
from ct_events_clean a
inner join
service_request_clean b
on a.contact_event_id = b.cnctevn_id;

-- SORT_QUERY_RESULTS

create table ct_events1_test
as select  a.*,
b.svcrqst_id,
b.svcrqct_cds,
b.svcrtyp_cd,
b.cmpltyp_cd,
b.sum_reason_cd as src,
b.cnctmd_cd,
b.notes
from ct_events_clean a
inner join
service_request_clean b
on a.contact_event_id = b.cnctevn_id;

select * from ct_events1_test;













set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

explain
select * from (select b.key, b.value from src1 a left outer join src b on (a.key = b.key) order by b.key) x right outer join src c on (x.value = c.value) order by x.key;

select * from (select b.key, b.value from src1 a left outer join src b on (a.key = b.key) order by b.key) x right outer join src c on (x.value = c.value) order by x.key;
select * from (select b.key, b.value from src1 a left outer join src b on (a.key = b.key)) x right outer join src c on (x.value = c.value) order by x.key;
select * from src1 a left outer join src b on (a.key = b.key) right outer join src c on (a.value = c.value) order by a.key;
select * from src1 a left outer join src b on (a.key = b.key) left outer join src c on (a.value = c.value) order by a.key;
select * from src1 a left outer join src b on (a.key = b.key) join src c on (a.key = c.key);
select * from src1 a join src b on (a.key = b.key) join src c on (a.key = c.key);

select count(*) from src1 a join src b on (a.key = b.key) join src c on (a.key = c.key);

-- SORT_QUERY_RESULTS
select key from
(
select key from src
union all
select key from src
) tab group by key
union all
select key from src;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

create table test (key int, value string) partitioned by (p int) stored as textfile;

insert into table test partition (p=1) select * from src order by key limit 10;

alter table test set fileformat orc;

insert into table test partition (p=2) select * from src order by key limit 10;

describe test;

select * from test where p=1 and key > 0 order by key;
select * from test where p=2 and key > 0 order by key;
select * from test where key > 0 order by key;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
drop table if exists tez_self_join1;
drop table if exists tez_self_join2;

create table tez_self_join1(id1 int, id2 string, id3 string);
insert into table tez_self_join1 values(1, 'aa','bb'), (2, 'ab','ab'), (3,'ba','ba');

create table tez_self_join2(id1 int);
insert into table tez_self_join2 values(1),(2),(3);

explain
select s.id2, s.id3
from
(
 select self1.id1, self1.id2, self1.id3
 from tez_self_join1 self1 join tez_self_join1 self2
 on self1.id2=self2.id3 ) s
join tez_self_join2
on s.id1=tez_self_join2.id1
where s.id2='ab';

select s.id2, s.id3
from
(
 select self1.id1, self1.id2, self1.id3
 from tez_self_join1 self1 join tez_self_join1 self2
 on self1.id2=self2.id3 ) s
join tez_self_join2
on s.id1=tez_self_join2.id1
where s.id2='ab';

drop table tez_self_join1;
drop table tez_self_join2;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join=true;
set hive.join.emit.interval=2;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin_part;

CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

set hive.convert.join.bucket.mapjoin.tez = true;
set hive.auto.convert.sortmerge.join = true;

set hive.auto.convert.join.noconditionaltask.size=500;

explain
select count(*) from tab s1 join tab s3 on s1.key=s3.key;

set hive.convert.join.bucket.mapjoin.tez = false;
explain
select count(*) from
tab vt1
join
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
where vt1.key=vt2.id;

select count(*) from
tab vt1
join
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
where vt1.key=vt2.id;

explain
select count(*) from
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
join
tab vt1
where vt1.key=vt2.id;

select count(*) from
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
join
tab vt1
where vt1.key=vt2.id;

set hive.auto.convert.join=false;

explain
select count(*) from
(select rt1.id from
(select t1.key as id, t1.value as od from tab t1 order by id, od) rt1) vt1
join
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
where vt1.id=vt2.id;

select count(*) from
(select rt1.id from
(select t1.key as id, t1.value as od from tab t1 order by id, od) rt1) vt1
join
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
where vt1.id=vt2.id;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.mapjoin.hybridgrace.hashtable=false;
set hive.join.emit.interval=2;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

-- SORT_QUERY_RESULTS

CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin_part;

CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

set hive.auto.convert.sortmerge.join = true;

set hive.auto.convert.join.noconditionaltask.size=500;
CREATE TABLE empty(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;

explain
select count(*) from tab s1 join empty s3 on s1.key=s3.key;

select count(*) from tab s1 join empty s3 on s1.key=s3.key;

explain
select * from tab s1 left outer join empty s3 on s1.key=s3.key;

select * from tab s1 left outer join empty s3 on s1.key=s3.key;

explain
select count(*) from tab s1 left outer join tab s2 on s1.key=s2.key join empty s3 on s1.key = s3.key;

select count(*) from tab s1 left outer join tab s2 on s1.key=s2.key join empty s3 on s1.key = s3.key;

explain
select count(*) from tab s1 left outer join empty s2 on s1.key=s2.key join tab s3 on s1.key = s3.key;

select count(*) from tab s1 left outer join empty s2 on s1.key=s2.key join tab s3 on s1.key = s3.key;

explain
select count(*) from empty s1 join empty s3 on s1.key=s3.key;

select count(*) from empty s1 join empty s3 on s1.key=s3.key;

set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ;

explain
select count(*) from empty s1 join tab s3 on s1.key=s3.key;

select count(*) from empty s1 join tab s3 on s1.key=s3.key;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.join.emit.interval=2;
explain
select * from src a join src1 b on a.key = b.key;

select * from src a join src1 b on a.key = b.key;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;
set hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ;

CREATE TABLE srcbucket_mapjoin(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;
CREATE TABLE srcbucket_mapjoin_part (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin partition(ds='2008-04-08');

load data local inpath '../../data/files/srcbucket20.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket21.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket22.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');
load data local inpath '../../data/files/srcbucket23.txt' INTO TABLE srcbucket_mapjoin_part partition(ds='2008-04-08');



set hive.optimize.bucketingsorting=false;
insert overwrite table tab_part partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin_part;

CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;
insert overwrite table tab partition (ds='2008-04-08')
select key,value from srcbucket_mapjoin;

set hive.convert.join.bucket.mapjoin.tez = true;
set hive.auto.convert.sortmerge.join = true;

explain
select count(*)
from tab a join tab_part b on a.key = b.key;

select count(*)
from tab a join tab_part b on a.key = b.key;

set hive.auto.convert.join.noconditionaltask.size=2000;
set hive.mapjoin.hybridgrace.minwbsize=500;
set hive.mapjoin.hybridgrace.minnumpartitions=4;
explain
select count (*)
from tab a join tab_part b on a.key = b.key;

select count(*)
from tab a join tab_part b on a.key = b.key;

set hive.auto.convert.join.noconditionaltask.size=1000;
set hive.mapjoin.hybridgrace.minwbsize=250;
set hive.mapjoin.hybridgrace.minnumpartitions=4;
explain
select count (*)
from tab a join tab_part b on a.key = b.key;

select count(*)
from tab a join tab_part b on a.key = b.key;

set hive.auto.convert.join.noconditionaltask.size=500;
set hive.mapjoin.hybridgrace.minwbsize=125;
set hive.mapjoin.hybridgrace.minnumpartitions=4;
explain select count(*) from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value;
select count(*) from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value;

explain select count(*) from tab a join tab_part b on a.value = b.value;
select count(*) from tab a join tab_part b on a.value = b.value;

explain
select count(*) from (select s1.key as key, s1.value as value from tab s1 join tab s3 on s1.key=s3.key
UNION  ALL
select s2.key as key, s2.value as value from tab s2
) a join tab_part b on (a.key = b.key);

set hive.auto.convert.join.noconditionaltask.size=10000;
explain select count(*) from tab a join tab_part b on a.value = b.value;
select count(*) from tab a join tab_part b on a.value = b.value;

explain select count(*) from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value;
select count(*) from tab a join tab_part b on a.key = b.key join src1 c on a.value = c.value;

explain
select count(*) from (select s1.key as key, s1.value as value from tab s1 join tab s3 on s1.key=s3.key
UNION  ALL
select s2.key as key, s2.value as value from tab s2
) a join tab_part b on (a.key = b.key);

explain
select count(*) from
(select rt1.id from
(select t1.key as id, t1.value as od from tab t1 order by id, od) rt1) vt1
join
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
where vt1.id=vt2.id;

select count(*) from
(select rt1.id from
(select t1.key as id, t1.value as od from tab t1 order by id, od) rt1) vt1
join
(select rt2.id from
(select t2.key as id, t2.value as od from tab_part t2 order by id, od) rt2) vt2
where vt1.id=vt2.id;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.auto.convert.join=true;

explain
select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key
UNION  ALL
select s2.key as key, s2.value as value from src s2;

create table ut as
select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key
UNION  ALL
select s2.key as key, s2.value as value from src s2;

select * from ut order by key, value limit 20;
drop table ut;

set hive.auto.convert.join=false;

explain
with u as (select * from src union all select * from src)
select count(*) from (select u1.key as k1, u2.key as k2 from
u as u1 join u as u2 on (u1.key = u2.key)) a;

create table ut as
with u as (select * from src union all select * from src)
select count(*) as cnt from (select u1.key as k1, u2.key as k2 from
u as u1 join u as u2 on (u1.key = u2.key)) a;

select * from ut order by cnt limit 20;
drop table ut;

set hive.auto.convert.join=true;

explain select s1.key as skey, u1.key as ukey from
src s1
join (select * from src union all select * from src) u1 on s1.key = u1.key;

create table ut as
select s1.key as skey, u1.key as ukey from
src s1
join (select * from src union all select * from src) u1 on s1.key = u1.key;

select * from ut order by skey, ukey limit 20;
drop table ut;

explain select s1.key as skey, u1.key as ukey, s8.key as lkey from
src s1
join (select s2.key as key from src s2 join src s3 on s2.key = s3.key
      union all select s4.key from src s4 join src s5 on s4.key = s5.key
      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)
join src s8 on (u1.key = s8.key)
order by lkey;

create table ut as
select s1.key as skey, u1.key as ukey, s8.key as lkey from
src s1
join (select s2.key as key from src s2 join src s3 on s2.key = s3.key
      union all select s4.key from src s4 join src s5 on s4.key = s5.key
      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)
join src s8 on (u1.key = s8.key)
order by lkey;

select * from ut order by skey, ukey, lkey limit 100;

drop table ut;

explain
select s2.key as key from src s2 join src s3 on s2.key = s3.key
union all select s4.key from src s4 join src s5 on s4.key = s5.key;

create table ut as
select s2.key as key from src s2 join src s3 on s2.key = s3.key
union all select s4.key from src s4 join src s5 on s4.key = s5.key;

select * from ut order by key limit 30;

drop table ut;

explain
select * from
(select * from src union all select * from src) u
left outer join src s on u.key = s.key;

explain
select u.key as ukey, s.key as skey from
(select * from src union all select * from src) u
right outer join src s on u.key = s.key;

create table ut as
select u.key as ukey, s.key as skey from
(select * from src union all select * from src) u
right outer join src s on u.key = s.key;

select * from ut order by ukey, skey limit 20;
drop table ut;

set hive.vectorized.execution.enabled=true;

create table TABLE1(EMP_NAME STRING, EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1;

explain formatted select count(*) from TABLE3;

drop table table2;

create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1 UNION ALL select EMP_NAME,EMP_ID from TABLE2;

explain formatted select count(*) from TABLE3;
set hive.explain.user=false;
explain
SELECT key, value FROM
(
  SELECT key, value FROM src
  UNION ALL
  SELECT key, key as value FROM (
    SELECT distinct key FROM (
      SELECT key, value FROM (
        SELECT key, value FROM src
        UNION ALL
        SELECT key, value FROM src
      )t1
    group by key, value)t2
  )t3
)t4
group by key, value;

SELECT key, value FROM
(
  SELECT key, value FROM src
  UNION ALL
  SELECT key, key as value FROM (
    SELECT distinct key FROM (
      SELECT key, value FROM (
        SELECT key, value FROM src
        UNION ALL
        SELECT key, value FROM src
      )t1
    group by key, value)t2
  )t3
)t4
group by key, value;
select sum(a) from (
  select cast(1.1 as decimal) a from src tablesample (1 rows)
  union all
  select cast(null as decimal) a from src tablesample (1 rows)
) t;

select sum(a) from (
  select cast(1 as tinyint) a from src tablesample (1 rows)
  union all
  select cast(null as tinyint) a from src tablesample (1 rows)
  union all
  select cast(1.1 as decimal) a from src tablesample (1 rows)
) t;

select sum(a) from (
  select cast(1 as smallint) a from src tablesample (1 rows)
  union all
  select cast(null as smallint) a from src tablesample (1 rows)
  union all
  select cast(1.1 as decimal) a from src tablesample (1 rows)
) t;

select sum(a) from (
  select cast(1 as int) a from src tablesample (1 rows)
  union all
  select cast(null as int) a from src tablesample (1 rows)
  union all
  select cast(1.1 as decimal) a from src tablesample (1 rows)
) t;

select sum(a) from (
  select cast(1 as bigint) a from src tablesample (1 rows)
  union all
  select cast(null as bigint) a from src tablesample (1 rows)
  union all
  select cast(1.1 as decimal) a from src tablesample (1 rows)
) t;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
create table dummy(i int);
insert into table dummy values (1);
select * from dummy;

create table partunion1(id1 int) partitioned by (part1 string);

set hive.exec.dynamic.partition.mode=nonstrict;

explain insert into table partunion1 partition(part1)
select temps.* from (
select 1 as id1, '2014' as part1 from dummy
union all
select 2 as id1, '2014' as part1 from dummy ) temps;

insert into table partunion1 partition(part1)
select temps.* from (
select 1 as id1, '2014' as part1 from dummy
union all
select 2 as id1, '2014' as part1 from dummy ) temps;

select * from partunion1;
set hive.explain.user=false;
CREATE TABLE x
(
u bigint,
t string,
st string
)
PARTITIONED BY (`date` string)
STORED AS ORC
TBLPROPERTIES ("orc.compress"="ZLIB");

CREATE TABLE y
(
u bigint
)
PARTITIONED BY (`date` string)
STORED AS ORC
TBLPROPERTIES ("orc.compress"="ZLIB");

CREATE TABLE z
(
u bigint
)
PARTITIONED BY (`date` string)
STORED AS ORC
TBLPROPERTIES ("orc.compress"="ZLIB");

CREATE TABLE v
(
t string,
st string,
id int
)
STORED AS ORC
TBLPROPERTIES ("orc.compress"="ZLIB");

EXPLAIN
SELECT o.u, n.u
FROM
(
SELECT m.u, Min(`date`) as ft
FROM
(
SELECT u, `date` FROM x WHERE `date` < '2014-09-02'
UNION ALL
SELECT u, `date` FROM y WHERE `date` < '2014-09-02'
UNION ALL
SELECT u, `date` FROM z WHERE `date` < '2014-09-02'
) m
GROUP BY m.u
) n
LEFT OUTER JOIN
(
SELECT x.u
FROM x
JOIN v
ON (x.t = v.t AND x.st <=> v.st)
WHERE x.`date` >= '2014-03-04' AND x.`date` < '2014-09-03'
GROUP BY x.u
) o
ON n.u = o.u
WHERE n.u <> 0 AND n.ft <= '2014-09-02';

SELECT o.u, n.u
FROM
(
SELECT m.u, Min(`date`) as ft
FROM
(
SELECT u, `date` FROM x WHERE `date` < '2014-09-02'
UNION ALL
SELECT u, `date` FROM y WHERE `date` < '2014-09-02'
UNION ALL
SELECT u, `date` FROM z WHERE `date` < '2014-09-02'
) m
GROUP BY m.u
) n
LEFT OUTER JOIN
(
SELECT x.u
FROM x
JOIN v
ON (x.t = v.t AND x.st <=> v.st)
WHERE x.`date` >= '2014-03-04' AND x.`date` < '2014-09-03'
GROUP BY x.u
) o
ON n.u = o.u
WHERE n.u <> 0 AND n.ft <= '2014-09-02';
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;

CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;

explain
FROM (
      select key, value from (
      select 'tst1' as key, cast(count(1) as string) as value, 'tst1' as value2 from src s1
                         UNION all
      select s2.key as key, s2.value as value, 'tst1' as value2 from src s2) unionsub
                         UNION all
      select key, value from src s0
                             ) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

FROM (
      select key, value from (
      select 'tst1' as key, cast(count(1) as string) as value, 'tst1' as value2 from src s1
                         UNION all
      select s2.key as key, s2.value as value, 'tst1' as value2 from src s2) unionsub
                         UNION all
      select key, value from src s0
                             ) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

select * from DEST1;
select * from DEST2;

explain
FROM (
      select key, value from src s0
                         UNION all
      select key, value from (
      select 'tst1' as key, cast(count(1) as string) as value, 'tst1' as value2 from src s1
                         UNION all
      select s2.key as key, s2.value as value, 'tst1' as value2 from src s2) unionsub) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

FROM (
      select key, value from src s0
                         UNION all
      select key, value from (
      select 'tst1' as key, cast(count(1) as string) as value, 'tst1' as value2 from src s1
                         UNION all
      select s2.key as key, s2.value as value, 'tst1' as value2 from src s2) unionsub) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

select * from DEST1;
select * from DEST2;


explain
FROM (
      select key, value from src s0
                         UNION all
      select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION all
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

FROM (
      select key, value from src s0
                         UNION all
      select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION all
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

select * from DEST1;
select * from DEST2;

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION all
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION all
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

select * from DEST1;
select * from DEST2;

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION distinct
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION distinct
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5))
GROUP BY unionsrc.key, unionsrc.value;

select * from DEST1;
select * from DEST2;select * from (select key + key from src limit 1) a
union all
select * from (select key + key from src limit 1) b;


add jar ${system:maven.local.repository}/org/apache/hive/hive-it-test-serde/${system:hive.version}/hive-it-test-serde-${system:hive.version}.jar;

create temporary function example_add as 'org.apache.hadoop.hive.udf.example.GenericUDFExampleAdd';

-- Now try the query with the UDF
select example_add(key, key)from (select key from src limit 1) a
union all
select example_add(key, key)from (select key from src limit 1) b;
set hive.mapred.mode=nonstrict;

set hive.explain.user=false;
set hive.auto.convert.join=false;
set hive.optimize.dynamic.partition.hashjoin=false;

-- First try with regular mergejoin
explain
select
  *
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
order by a.cint;

select
  *
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
order by a.cint;

explain
select
  count(*)
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null;

select
  count(*)
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null;

explain
select
  a.csmallint, count(*) c1
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
group by a.csmallint
order by c1;

select
  a.csmallint, count(*) c1
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
group by a.csmallint
order by c1;

set hive.auto.convert.join=true;
set hive.optimize.dynamic.partition.hashjoin=true;
set hive.auto.convert.join.noconditionaltask.size=200000;
set hive.exec.reducers.bytes.per.reducer=200000;
set hive.vectorized.execution.enabled=true;

-- Try with dynamically partitioned hashjoin
explain
select
  *
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
order by a.cint;

select
  *
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
order by a.cint;

explain
select
  count(*)
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null;

select
  count(*)
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null;

explain
select
  a.csmallint, count(*) c1
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
group by a.csmallint
order by c1;

select
  a.csmallint, count(*) c1
from alltypesorc a join alltypesorc b on a.cint = b.cint
where
  a.cint between 1000000 and 3000000 and b.cbigint is not null
group by a.csmallint
order by c1;
set hive.mapred.mode=nonstrict;

set hive.explain.user=false;
set hive.auto.convert.join=false;
set hive.optimize.dynamic.partition.hashjoin=false;

-- Multiple tables, and change the order of the big table (alltypesorc)
-- First try with regular mergejoin
explain
select
  a.*
from
  alltypesorc a,
  src b,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

select
  a.*
from
  alltypesorc a,
  src b,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

set hive.auto.convert.join=true;
set hive.optimize.dynamic.partition.hashjoin=true;
set hive.auto.convert.join.noconditionaltask.size=2000;
set hive.exec.reducers.bytes.per.reducer=200000;
set hive.vectorized.execution.enabled=true;

-- noconditionaltask.size needs to be low enough that entire filtered table results do not fit in one task's hash table
-- Try with dynamically partitioned hash join
explain
select
  a.*
from
  alltypesorc a,
  src b,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

select
  a.*
from
  alltypesorc a,
  src b,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

-- Try different order of tables
explain
select
  a.*
from
  src b,
  alltypesorc a,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;

select
  a.*
from
  src b,
  alltypesorc a,
  src c
where
  a.csmallint = cast(b.key as int) and a.csmallint = (cast(c.key as int) + 0)
  and (a.csmallint < 100)
order by a.csmallint, a.ctinyint, a.cint;
set hive.fetch.task.conversion=more;

drop table timestamp_1;

create table timestamp_1 (t timestamp);
alter table timestamp_1 set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';

insert overwrite table timestamp_1
  select cast('2011-01-01 01:01:01' as timestamp) from src tablesample (1 rows);
select cast(t as boolean) from timestamp_1 limit 1;
select cast(t as tinyint) from timestamp_1 limit 1;
select cast(t as smallint) from timestamp_1 limit 1;
select cast(t as int) from timestamp_1 limit 1;
select cast(t as bigint) from timestamp_1 limit 1;
select cast(t as float) from timestamp_1 limit 1;
select cast(t as double) from timestamp_1 limit 1;
select cast(t as string) from timestamp_1 limit 1;

insert overwrite table timestamp_1
  select '2011-01-01 01:01:01' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_1 limit 1;
select cast(t as tinyint) from timestamp_1 limit 1;
select cast(t as smallint) from timestamp_1 limit 1;
select cast(t as int) from timestamp_1 limit 1;
select cast(t as bigint) from timestamp_1 limit 1;
select cast(t as float) from timestamp_1 limit 1;
select cast(t as double) from timestamp_1 limit 1;
select cast(t as string) from timestamp_1 limit 1;

insert overwrite table timestamp_1
  select '2011-01-01 01:01:01.1' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_1 limit 1;
select cast(t as tinyint) from timestamp_1 limit 1;
select cast(t as smallint) from timestamp_1 limit 1;
select cast(t as int) from timestamp_1 limit 1;
select cast(t as bigint) from timestamp_1 limit 1;
select cast(t as float) from timestamp_1 limit 1;
select cast(t as double) from timestamp_1 limit 1;
select cast(t as string) from timestamp_1 limit 1;

insert overwrite table timestamp_1
  select '2011-01-01 01:01:01.0001' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_1 limit 1;
select cast(t as tinyint) from timestamp_1 limit 1;
select cast(t as smallint) from timestamp_1 limit 1;
select cast(t as int) from timestamp_1 limit 1;
select cast(t as bigint) from timestamp_1 limit 1;
select cast(t as float) from timestamp_1 limit 1;
select cast(t as double) from timestamp_1 limit 1;
select cast(t as string) from timestamp_1 limit 1;

insert overwrite table timestamp_1
  select '2011-01-01 01:01:01.000100000' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_1 limit 1;
select cast(t as tinyint) from timestamp_1 limit 1;
select cast(t as smallint) from timestamp_1 limit 1;
select cast(t as int) from timestamp_1 limit 1;
select cast(t as bigint) from timestamp_1 limit 1;
select cast(t as float) from timestamp_1 limit 1;
select cast(t as double) from timestamp_1 limit 1;
select cast(t as string) from timestamp_1 limit 1;

insert overwrite table timestamp_1
  select '2011-01-01 01:01:01.001000011' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_1 limit 1;
select cast(t as tinyint) from timestamp_1 limit 1;
select cast(t as smallint) from timestamp_1 limit 1;
select cast(t as int) from timestamp_1 limit 1;
select cast(t as bigint) from timestamp_1 limit 1;
select cast(t as float) from timestamp_1 limit 1;
select cast(t as double) from timestamp_1 limit 1;
select cast(t as string) from timestamp_1 limit 1;

drop table timestamp_1;
set hive.fetch.task.conversion=more;

drop table timestamp_2;

create table timestamp_2 (t timestamp);
alter table timestamp_2 set serde 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe';

insert overwrite table timestamp_2
  select cast('2011-01-01 01:01:01' as timestamp) from src tablesample (1 rows);
select cast(t as boolean) from timestamp_2 limit 1;
select cast(t as tinyint) from timestamp_2 limit 1;
select cast(t as smallint) from timestamp_2 limit 1;
select cast(t as int) from timestamp_2 limit 1;
select cast(t as bigint) from timestamp_2 limit 1;
select cast(t as float) from timestamp_2 limit 1;
select cast(t as double) from timestamp_2 limit 1;
select cast(t as string) from timestamp_2 limit 1;

insert overwrite table timestamp_2
  select '2011-01-01 01:01:01' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_2 limit 1;
select cast(t as tinyint) from timestamp_2 limit 1;
select cast(t as smallint) from timestamp_2 limit 1;
select cast(t as int) from timestamp_2 limit 1;
select cast(t as bigint) from timestamp_2 limit 1;
select cast(t as float) from timestamp_2 limit 1;
select cast(t as double) from timestamp_2 limit 1;
select cast(t as string) from timestamp_2 limit 1;

insert overwrite table timestamp_2
  select '2011-01-01 01:01:01.1' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_2 limit 1;
select cast(t as tinyint) from timestamp_2 limit 1;
select cast(t as smallint) from timestamp_2 limit 1;
select cast(t as int) from timestamp_2 limit 1;
select cast(t as bigint) from timestamp_2 limit 1;
select cast(t as float) from timestamp_2 limit 1;
select cast(t as double) from timestamp_2 limit 1;
select cast(t as string) from timestamp_2 limit 1;

insert overwrite table timestamp_2
  select '2011-01-01 01:01:01.0001' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_2 limit 1;
select cast(t as tinyint) from timestamp_2 limit 1;
select cast(t as smallint) from timestamp_2 limit 1;
select cast(t as int) from timestamp_2 limit 1;
select cast(t as bigint) from timestamp_2 limit 1;
select cast(t as float) from timestamp_2 limit 1;
select cast(t as double) from timestamp_2 limit 1;
select cast(t as string) from timestamp_2 limit 1;

insert overwrite table timestamp_2
  select '2011-01-01 01:01:01.000100000' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_2 limit 1;
select cast(t as tinyint) from timestamp_2 limit 1;
select cast(t as smallint) from timestamp_2 limit 1;
select cast(t as int) from timestamp_2 limit 1;
select cast(t as bigint) from timestamp_2 limit 1;
select cast(t as float) from timestamp_2 limit 1;
select cast(t as double) from timestamp_2 limit 1;
select cast(t as string) from timestamp_2 limit 1;

insert overwrite table timestamp_2
  select '2011-01-01 01:01:01.001000011' from src tablesample (1 rows);
select cast(t as boolean) from timestamp_2 limit 1;
select cast(t as tinyint) from timestamp_2 limit 1;
select cast(t as smallint) from timestamp_2 limit 1;
select cast(t as int) from timestamp_2 limit 1;
select cast(t as bigint) from timestamp_2 limit 1;
select cast(t as float) from timestamp_2 limit 1;
select cast(t as double) from timestamp_2 limit 1;
select cast(t as string) from timestamp_2 limit 1;

drop table timestamp_2;
set hive.fetch.task.conversion=more;

drop table timestamp_3;

create table timestamp_3 (t timestamp);
alter table timestamp_3 set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';

insert overwrite table timestamp_3
  select cast(cast('1.3041352164485E9' as double) as timestamp) from src tablesample (1 rows);
select cast(t as boolean) from timestamp_3 limit 1;
select cast(t as tinyint) from timestamp_3 limit 1;
select cast(t as smallint) from timestamp_3 limit 1;
select cast(t as int) from timestamp_3 limit 1;
select cast(t as bigint) from timestamp_3 limit 1;
select cast(t as float) from timestamp_3 limit 1;
select cast(t as double) from timestamp_3 limit 1;
select cast(t as string) from timestamp_3 limit 1;

select t, sum(t), count(*), sum(t)/count(*), avg(t) from timestamp_3 group by t;

drop table timestamp_3;
set hive.fetch.task.conversion=more;

select cast('2011-05-06 07:08:09' as timestamp) >
  cast('2011-05-06 07:08:09' as timestamp) from src limit 1;

select cast('2011-05-06 07:08:09' as timestamp) <
  cast('2011-05-06 07:08:09' as timestamp) from src limit 1;

select cast('2011-05-06 07:08:09' as timestamp) =
  cast('2011-05-06 07:08:09' as timestamp) from src limit 1;

select cast('2011-05-06 07:08:09' as timestamp) <>
  cast('2011-05-06 07:08:09' as timestamp) from src limit 1;

select cast('2011-05-06 07:08:09' as timestamp) >=
  cast('2011-05-06 07:08:09' as timestamp) from src limit 1;

select cast('2011-05-06 07:08:09' as timestamp) <=
  cast('2011-05-06 07:08:09' as timestamp) from src limit 1;

select cast('2011-05-06 07:08:09' as timestamp) >=
  cast('2011-05-06 07:08:09.1' as timestamp) from src limit 1;

select cast('2011-05-06 07:08:09' as timestamp) <
  cast('2011-05-06 07:08:09.1' as timestamp) from src limit 1;

select cast('2011-05-06 07:08:09.1000' as timestamp) =
  cast('2011-05-06 07:08:09.1' as timestamp) from src limit 1;

-- Test timestamp-to-numeric comparison
select count(*)
FROM   alltypesorc
WHERE
((ctinyint != 0)
    AND
        (((ctimestamp1 <= 0)
            OR ((ctinyint = cint) OR (cstring2 LIKE 'ss')))
         AND ((988888 < cdouble)
             OR ((ctimestamp2 > -29071) AND (3569 >= cdouble)))))
;

-- Should have same result as previous query
select count(*)
FROM   alltypesorc
WHERE
((ctinyint != 0)
    AND
        (((ctimestamp1 <= timestamp('1969-12-31 16:00:00'))
            OR ((ctinyint = cint) OR (cstring2 LIKE 'ss')))
         AND ((988888 < cdouble)
             OR ((ctimestamp2 > timestamp('1969-12-31 07:55:29')) AND (3569 >= cdouble)))))
;

CREATE TABLE timestamp_formats (
  c1 string,
  c1_ts timestamp,
  c2 string,
  c2_ts timestamp,
  c3 string,
  c3_ts timestamp
);

LOAD DATA LOCAL INPATH '../../data/files/ts_formats.txt' overwrite into table timestamp_formats;

SELECT * FROM timestamp_formats;

-- Add single timestamp format. This should allow c3_ts to parse
ALTER TABLE timestamp_formats SET SERDEPROPERTIES ("timestamp.formats"="yyyy-MM-dd'T'HH:mm:ss");
SELECT * FROM timestamp_formats;

-- Add another format, to allow c2_ts to parse
ALTER TABLE timestamp_formats SET SERDEPROPERTIES ("timestamp.formats"="yyyy-MM-dd'T'HH:mm:ss,yyyy-MM-dd'T'HH:mm:ss.SSSSSSSSS");
SELECT * FROM timestamp_formats;

DROP TABLE timestamp_formats;
set hive.int.timestamp.conversion.in.seconds=false;

explain
select
-- to timestamp
  cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;

select
-- to timestamp
  cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;

set hive.int.timestamp.conversion.in.seconds=true;

explain
select
-- to timestamp
  cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;

select
-- to timestamp
  cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;

set hive.mapred.mode=nonstrict;
drop table timestamp_lazy;
create table timestamp_lazy (t timestamp, key string, value string);
insert overwrite table timestamp_lazy select cast('2011-01-01 01:01:01' as timestamp), key, value from src tablesample (5 rows);

select t,key,value from timestamp_lazy ORDER BY key ASC, value ASC;
select t,key,value from timestamp_lazy distribute by t sort by key ASC, value ASC;
explain
select timestamp '2011-01-01 01:01:01';
select timestamp '2011-01-01 01:01:01';

explain
select '2011-01-01 01:01:01.101' <> timestamp '2011-01-01 01:01:01.100';
select '2011-01-01 01:01:01.101' <> timestamp '2011-01-01 01:01:01.100';

explain
select 1 where timestamp '2011-01-01 01:01:01.101' <> timestamp '2011-01-01 01:01:01.100';
select 1 where timestamp '2011-01-01 01:01:01.101' <> timestamp '2011-01-01 01:01:01.100';

-- TimeZone is not yet supported
SELECT TIMESTAMP '2012-12-29 20:01:00 +03:00';
DROP TABLE IF EXISTS timestamp_null;
CREATE TABLE timestamp_null (t1 TIMESTAMP);
LOAD DATA LOCAL INPATH '../../data/files/test.dat' OVERWRITE INTO TABLE timestamp_null;

SELECT * FROM timestamp_null LIMIT 1;

SELECT t1 FROM timestamp_null LIMIT 1;
set hive.fetch.task.conversion=more;

drop table timestamp_udf;
drop table timestamp_udf_string;

create table timestamp_udf (t timestamp);
create table timestamp_udf_string (t string);
from (select * from src tablesample (1 rows)) s
  insert overwrite table timestamp_udf
    select '2011-05-06 07:08:09.1234567'
  insert overwrite table timestamp_udf_string
    select '2011-05-06 07:08:09.1234567';

-- Test UDFs with Timestamp input
select unix_timestamp(t), year(t), month(t), day(t), dayofmonth(t),
    weekofyear(t), hour(t), minute(t), second(t), to_date(t)
  from timestamp_udf;

select date_add(t, 5), date_sub(t, 10)
  from timestamp_udf;

select datediff(t, t), datediff(t, '2002-03-21'), datediff('2002-03-21', t)
  from timestamp_udf;

select from_utc_timestamp(t, 'America/Chicago')
  from timestamp_udf;

select to_utc_timestamp(t, 'America/Chicago')
  from timestamp_udf;

select t, from_utc_timestamp(t, 'America/Chicago')
  from timestamp_udf;

select t, from_utc_timestamp(t, 'America/Chicago'), t, from_utc_timestamp(t, 'America/Chicago')
  from timestamp_udf;

select t, to_utc_timestamp(t, 'America/Chicago')
  from timestamp_udf;

select t, to_utc_timestamp(t, 'America/Chicago'), t, to_utc_timestamp(t, 'America/Chicago')
  from timestamp_udf;

-- Test UDFs with string input
select unix_timestamp(t), year(t), month(t), day(t), dayofmonth(t),
    weekofyear(t), hour(t), minute(t), second(t), to_date(t)
  from timestamp_udf_string;

select date_add(t, 5), date_sub(t, 10)  from timestamp_udf_string;

select datediff(t, t), datediff(t, '2002-03-21'), datediff('2002-03-21', t)
  from timestamp_udf_string;

select from_utc_timestamp(t, 'America/Chicago')
  from timestamp_udf_string;

select to_utc_timestamp(t, 'America/Chicago')
  from timestamp_udf_string;

drop table timestamp_udf;
drop table timestamp_udf_string;
set hive.mapred.mode=nonstrict;
CREATE TABLE `sample_07` ( `code` string , `description` string , `total_emp` int , `salary` int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' STORED AS TextFile;
set hive.limit.pushdown.memory.usage=0.9999999;

select * from sample_07 order by salary LIMIT 999999999;

SELECT * FROM (
SELECT *, rank() over(PARTITION BY code ORDER BY salary DESC) as rank
FROM sample_07
) ranked_claim
WHERE ranked_claim.rank < 2
ORDER BY code;

select sum(total_emp) over(partition by salary+salary order by code) from sample_07 limit 9999999;create database tc;

create table tc.tstsrc like default.src;
insert overwrite table tc.tstsrc select key, value from default.src;

create table tc.tstsrcpart like default.srcpart;
insert overwrite table tc.tstsrcpart partition (ds='2008-04-08', hr='12')
select key, value from default.srcpart where ds='2008-04-08' and hr='12';

ALTER TABLE tc.tstsrc TOUCH;
ALTER TABLE tc.tstsrcpart TOUCH;
ALTER TABLE tc.tstsrcpart TOUCH PARTITION (ds='2008-04-08', hr='12');

drop table tc.tstsrc;
drop table tc.tstsrcpart;

drop database tc;
ALTER TABLE srcpart TOUCH PARTITION (ds='2008-04-08', hr='13');
ALTER TABLE src TOUCH PARTITION (ds='2008-04-08', hr='12');
set hive.explain.user=false;
set hive.entity.capture.transform=true;

create table transform1_t1(a string, b string);

EXPLAIN
SELECT transform(*) USING 'cat' AS (col array<bigint>) FROM transform1_t1;

SELECT transform(*) USING 'cat' AS (col array<bigint>) FROM transform1_t1;




create table transform1_t2(col array<int>);

insert overwrite table transform1_t2
select array(1,2,3) from src tablesample (1 rows);

EXPLAIN
SELECT transform('0\0021\0022') USING 'cat' AS (col array<int>) FROM transform1_t2;

SELECT transform('0\0021\0022') USING 'cat' AS (col array<int>) FROM transform1_t2;



-- Transform with a function that has many parameters
SELECT TRANSFORM(substr(key, 1, 2)) USING 'cat' FROM src LIMIT 1;
set hive.entity.capture.transform=true;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


-- EXCLUDE_OS_WINDOWS

create table transform_acid(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');
insert into table transform_acid select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint < 0 order by cint limit 1;


ADD FILE ../../ql/src/test/scripts/transform_acid_grep.sh;

SELECT transform(*) USING 'transform_acid_grep.sh' AS (col string) FROM transform_acid;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;
set hive.entity.capture.transform=true;

-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
FROM (
  FROM srcpart src
  SELECT TRANSFORM(src.ds, src.key, src.value)
         USING 'cat' AS (ds, tkey, tvalue)
  CLUSTER BY tkey
) tmap
SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100 AND tmap.ds = '2008-04-08';

FROM (
  FROM srcpart src
  SELECT TRANSFORM(src.ds, src.key, src.value)
         USING 'cat' AS (ds, tkey, tvalue)
  CLUSTER BY tkey
) tmap
SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100 AND tmap.ds = '2008-04-08';

set hive.optimize.ppd=true;
set hive.entity.capture.transform=true;

-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
FROM (
  FROM srcpart src
  SELECT TRANSFORM(src.ds, src.key, src.value)
         USING 'cat' AS (ds, tkey, tvalue)
  WHERE src.ds = '2008-04-08'
  CLUSTER BY tkey
) tmap
SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

FROM (
  FROM srcpart src
  SELECT TRANSFORM(src.ds, src.key, src.value)
         USING 'cat' AS (ds, tkey, tvalue)
  WHERE src.ds = '2008-04-08'
  CLUSTER BY tkey
) tmap
SELECT tmap.tkey, tmap.tvalue WHERE tmap.tkey < 100;

-- Tests truncating a bucketed column

CREATE TABLE test_tab (key STRING, value STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS RCFILE;

INSERT OVERWRITE TABLE test_tab SELECT * FROM src;

TRUNCATE TABLE test_tab COLUMNS (key);
set hive.mapred.mode=nonstrict;
-- Tests truncating column(s) from a table, also tests that stats are updated

CREATE TABLE test_tab (key STRING, value STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe' STORED AS RCFILE;

set hive.stats.autogather=true;

INSERT OVERWRITE TABLE test_tab SELECT * FROM src tablesample (10 rows);

DESC FORMATTED test_tab;

SELECT * FROM test_tab ORDER BY value;

-- Truncate 1 column
TRUNCATE TABLE test_tab COLUMNS (key);

DESC FORMATTED test_tab;

-- First column should be null
SELECT * FROM test_tab ORDER BY value;

-- Truncate multiple columns
INSERT OVERWRITE TABLE test_tab SELECT * FROM src tablesample (10 rows);

TRUNCATE TABLE test_tab COLUMNS (key, value);

DESC FORMATTED test_tab;

-- Both columns should be null
SELECT * FROM test_tab ORDER BY value;

-- Truncate columns again
TRUNCATE TABLE test_tab COLUMNS (key, value);

DESC FORMATTED test_tab;

-- Both columns should be null
SELECT * FROM test_tab ORDER BY value;

-- Test truncating with a binary serde
ALTER TABLE test_tab SET SERDE 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';

INSERT OVERWRITE TABLE test_tab SELECT * FROM src tablesample (10 rows);

DESC FORMATTED test_tab;

SELECT * FROM test_tab ORDER BY value;

-- Truncate 1 column
TRUNCATE TABLE test_tab COLUMNS (key);

DESC FORMATTED test_tab;

-- First column should be null
SELECT * FROM test_tab ORDER BY value;

-- Truncate 2 columns
TRUNCATE TABLE test_tab COLUMNS (key, value);

DESC FORMATTED test_tab;

-- Both columns should be null
SELECT * FROM test_tab ORDER BY value;

-- Test truncating a partition
CREATE TABLE test_tab_part (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE;

INSERT OVERWRITE TABLE test_tab_part PARTITION (part = '1') SELECT * FROM src tablesample (10 rows);

DESC FORMATTED test_tab_part PARTITION (part = '1');

SELECT * FROM test_tab_part WHERE part = '1' ORDER BY value;

TRUNCATE TABLE test_tab_part PARTITION (part = '1') COLUMNS (key);

DESC FORMATTED test_tab_part PARTITION (part = '1');

-- First column should be null
SELECT * FROM test_tab_part WHERE part = '1' ORDER BY value;
set hive.mapred.mode=nonstrict;
-- Tests truncating columns from a bucketed table, table should remain bucketed

CREATE TABLE test_tab (key STRING, value STRING) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS RCFILE;



INSERT OVERWRITE TABLE test_tab SELECT * FROM src;

-- Check how many rows there are in each bucket, there should be two rows
SELECT cnt FROM (
SELECT INPUT__FILE__NAME file_name, count(*) cnt FROM
test_tab GROUP BY INPUT__FILE__NAME
ORDER BY file_name DESC)a;

-- Truncate a column on which the table is not bucketed
TRUNCATE TABLE test_tab COLUMNS (value);

-- Check how many rows there are in each bucket, this should produce the same rows as before
-- because truncate should not break bucketing
SELECT cnt FROM (
SELECT INPUT__FILE__NAME file_name, count(*) cnt FROM
test_tab GROUP BY INPUT__FILE__NAME
ORDER BY file_name DESC)a;
-- Tests truncating a column from an indexed table

CREATE TABLE test_tab (key STRING, value STRING) STORED AS RCFILE;

INSERT OVERWRITE TABLE test_tab SELECT * FROM src;

CREATE INDEX test_tab_index ON TABLE test_tab (key) as 'COMPACT' WITH DEFERRED REBUILD;

TRUNCATE TABLE test_tab COLUMNS (value);
set hive.mapred.mode=nonstrict;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- Tests truncating a column from a list bucketing table

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)

CREATE TABLE test_tab (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE;

ALTER TABLE test_tab
SKEWED BY (key) ON ("484")
STORED AS DIRECTORIES;

INSERT OVERWRITE TABLE test_tab PARTITION (part = '1') SELECT * FROM src;

set hive.optimize.listbucketing=true;
SELECT * FROM test_tab WHERE part = '1' AND key = '0';

TRUNCATE TABLE test_tab PARTITION (part ='1') COLUMNS (value);

-- In the following select statements the list bucketing optimization should still be used
-- In both cases value should be null

EXPLAIN EXTENDED SELECT * FROM test_tab WHERE part = '1' AND key = '484';

SELECT * FROM test_tab WHERE part = '1' AND key = '484';

EXPLAIN EXTENDED SELECT * FROM test_tab WHERE part = '1' AND key = '0';

SELECT * FROM test_tab WHERE part = '1' AND key = '0';
set mapred.input.dir.recursive=true;

-- Tests truncating a column on which a table is list bucketed

CREATE TABLE test_tab (key STRING, value STRING) STORED AS RCFILE;

ALTER TABLE test_tab
SKEWED BY (key) ON ("484")
STORED AS DIRECTORIES;

INSERT OVERWRITE TABLE test_tab SELECT * FROM src;

TRUNCATE TABLE test_tab COLUMNS (key);
set hive.mapred.mode=nonstrict;
-- Tests truncating a column from a table with multiple files, then merging those files

CREATE TABLE test_tab (key STRING, value STRING) STORED AS RCFILE;

INSERT OVERWRITE TABLE test_tab SELECT * FROM src tablesample (5 rows);

INSERT INTO TABLE test_tab SELECT * FROM src tablesample (5 rows);

-- The value should be 2 indicating the table has 2 files
SELECT COUNT(DISTINCT INPUT__FILE__NAME) FROM test_tab;

TRUNCATE TABLE test_tab COLUMNS (key);

ALTER TABLE test_tab CONCATENATE;

-- The first column (key) should be null for all 10 rows
SELECT * FROM test_tab ORDER BY value;

-- The value should be 1 indicating the table has 1 file
SELECT COUNT(DISTINCT INPUT__FILE__NAME) FROM test_tab;
-- Tests truncating a column from a table stored as a sequence file

CREATE TABLE test_tab (key STRING, value STRING) STORED AS SEQUENCEFILE;

INSERT OVERWRITE TABLE test_tab SELECT * FROM src;

TRUNCATE TABLE test_tab COLUMNS (key);
-- Tests attempting to truncate a column in a table that doesn't exist

CREATE TABLE test_tab (key STRING, value STRING) STORED AS RCFILE;

INSERT OVERWRITE TABLE test_tab SELECT * FROM src;

TRUNCATE TABLE test_tab COLUMNS (doesnt_exist);
-- Tests truncating a partition column

CREATE TABLE test_tab (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE;

INSERT OVERWRITE TABLE test_tab PARTITION (part = '1') SELECT * FROM src;

TRUNCATE TABLE test_tab COLUMNS (part);
-- Tests truncating a partition column

CREATE TABLE test_tab (key STRING, value STRING) PARTITIONED BY (part STRING) STORED AS RCFILE;

INSERT OVERWRITE TABLE test_tab PARTITION (part = '1') SELECT * FROM src;

TRUNCATE TABLE test_tab PARTITION (part = '1') COLUMNS (part);
create table src_truncate (key string, value string);
load data local inpath '../../data/files/kv1.txt' into table src_truncate;;

create table srcpart_truncate (key string, value string) partitioned by (ds string, hr string);
alter table srcpart_truncate add partition (ds='2008-04-08', hr='11');
alter table srcpart_truncate add partition (ds='2008-04-08', hr='12');
alter table srcpart_truncate add partition (ds='2008-04-09', hr='11');
alter table srcpart_truncate add partition (ds='2008-04-09', hr='12');

load data local inpath '../../data/files/kv1.txt' into table srcpart_truncate partition (ds='2008-04-08', hr='11');
load data local inpath '../../data/files/kv1.txt' into table srcpart_truncate partition (ds='2008-04-08', hr='12');
load data local inpath '../../data/files/kv1.txt' into table srcpart_truncate partition (ds='2008-04-09', hr='11');
load data local inpath '../../data/files/kv1.txt' into table srcpart_truncate partition (ds='2008-04-09', hr='12');

analyze table src_truncate     compute statistics;
analyze table srcpart_truncate partition(ds,hr) compute statistics;
set hive.fetch.task.conversion=more;
set hive.compute.query.using.stats=true;

-- truncate non-partitioned table
explain TRUNCATE TABLE src_truncate;
TRUNCATE TABLE src_truncate;
select * from src_truncate;
select count (*) from src_truncate;

-- truncate a partition
explain TRUNCATE TABLE srcpart_truncate partition (ds='2008-04-08', hr='11');
TRUNCATE TABLE srcpart_truncate partition (ds='2008-04-08', hr='11');
select * from srcpart_truncate where ds='2008-04-08' and hr='11';
select count(*) from srcpart_truncate where ds='2008-04-08' and hr='11';

-- truncate partitions with partial spec
explain TRUNCATE TABLE srcpart_truncate partition (ds, hr='12');
TRUNCATE TABLE srcpart_truncate partition (ds, hr='12');
select * from srcpart_truncate where hr='12';
select count(*) from srcpart_truncate where hr='12';

-- truncate partitioned table
explain TRUNCATE TABLE srcpart_truncate;
TRUNCATE TABLE srcpart_truncate;
select * from srcpart_truncate;
select count(*) from srcpart_truncate;
-- partition spec for non-partitioned table
TRUNCATE TABLE src partition (ds='2008-04-08', hr='11');
-- full partition spec for not existing partition
TRUNCATE TABLE srcpart partition (ds='2012-12-17', hr='15');
create external table external1 (a int, b int) partitioned by (ds string);

-- trucate for non-managed table
TRUNCATE TABLE external1;
CREATE TABLE non_native(key int, value string)
STORED BY 'org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler';

-- trucate for non-native table
TRUNCATE TABLE non_native;
set hive.fetch.task.conversion=more;

EXPLAIN
SELECT IF(false, 1, cast(2 as smallint)) + 3 FROM src LIMIT 1;

SELECT IF(false, 1, cast(2 as smallint)) + 3 FROM src LIMIT 1;
set hive.fetch.task.conversion=more;

-- casting from null should yield null
select
  cast(null as tinyint),
  cast(null as smallint),
  cast(null as int),
  cast(null as bigint),
  cast(null as float),
  cast(null as double),
  cast(null as decimal),
  cast(null as date),
  cast(null as timestamp),
  cast(null as string),
  cast(null as varchar(10)),
  cast(null as boolean),
  cast(null as binary)
from src limit 1;

-- Invalid conversions, should all be null
select
  cast('abcd' as date),
  cast('abcd' as timestamp)
from src limit 1;

set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;
-- Check for int, bigint automatic type widening conversions in UDFs, UNIONS
EXPLAIN SELECT COALESCE(0, 9223372036854775807) FROM src LIMIT 1;
SELECT COALESCE(0, 9223372036854775807) FROM src LIMIT 1;

EXPLAIN SELECT * FROM (SELECT 0 AS numcol FROM src UNION ALL SELECT 9223372036854775807 AS numcol FROM src) a ORDER BY numcol;
SELECT * FROM (SELECT 0 AS numcol FROM src UNION ALL SELECT 9223372036854775807 AS numcol FROM src) a ORDER BY numcol;
SET mapreduce.job.ubertask.enable=true;
SET mapreduce.job.ubertask.maxreduces=1;
SET mapred.reduce.tasks=1;

-- Uberized mode is a YARN option, ignore this test for non-YARN Hadoop versions
-- EXCLUDE_HADOOP_MAJOR_VERSIONS(0.20S)

CREATE TABLE T1(key STRING, val STRING);
LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;

SELECT count(*) FROM T1;
DESCRIBE FUNCTION collect_set;
DESCRIBE FUNCTION EXTENDED collect_set;

DESCRIBE FUNCTION collect_list;
DESCRIBE FUNCTION EXTENDED collect_list;

set hive.map.aggr = false;
set hive.groupby.skewindata = false;

SELECT key, collect_set(value)
FROM src
GROUP BY key ORDER BY key limit 20;

SELECT key, collect_list(value)
FROM src
GROUP BY key ORDER by key limit 20;

set hive.map.aggr = true;
set hive.groupby.skewindata = false;

SELECT key, collect_set(value)
FROM src
GROUP BY key ORDER BY key limit 20;

SELECT key, collect_list(value)
FROM src
GROUP BY key ORDER BY key limit 20;

set hive.map.aggr = false;
set hive.groupby.skewindata = true;

SELECT key, collect_set(value)
FROM src
GROUP BY key ORDER BY key limit 20;

set hive.map.aggr = true;
set hive.groupby.skewindata = true;

SELECT key, collect_set(value)
FROM src
GROUP BY key ORDER BY key limit 20;
set hive.support.sql11.reserved.keywords=false;

DESCRIBE FUNCTION collect_set;
DESCRIBE FUNCTION EXTENDED collect_set;

DESCRIBE FUNCTION collect_list;
DESCRIBE FUNCTION EXTENDED collect_list;


-- initialize tables

CREATE TABLE customers (id int, name varchar(10), age int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';

LOAD DATA LOCAL INPATH "../../data/files/customers.txt" INTO TABLE customers;

CREATE TABLE orders (id int, cid int, date date, amount double)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';

LOAD DATA LOCAL INPATH "../../data/files/orders.txt" INTO TABLE orders;

CREATE TABLE nested_orders (id int, cid int, date date, sub map<string,double>)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY '$'
MAP KEYS TERMINATED BY ':';

LOAD DATA LOCAL INPATH "../../data/files/nested_orders.txt" INTO TABLE nested_orders;

-- 1. test struct

-- 1.1 when field is primitive

SELECT c.id, sort_array(collect_set(named_struct("name", c.name, "date", o.date, "amount", o.amount)))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(named_struct("name", c.name, "date", o.date, "amount", o.amount)))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

-- cast decimal

SELECT c.id, sort_array(collect_set(named_struct("name", c.name, "date", o.date, "amount", cast(o.amount as decimal(10,1)))))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(named_struct("name", c.name, "date", o.date, "amount", cast(o.amount as decimal(10,1)))))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;


SELECT c.id, sort_array(collect_set(struct(c.name, o.date, o.amount)))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(struct(c.name, o.date, o.amount)))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;


-- 1.2 when field is map

SELECT c.id, sort_array(collect_set(named_struct("name", c.name, "date", o.date, "sub", o.sub)))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(named_struct("name", c.name, "date", o.date, "sub", o.sub)))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_set(struct(c.name, o.date, o.sub)))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(struct(c.name, o.date, o.sub)))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;


-- 1.3 when field is list

SELECT c.id, sort_array(collect_set(named_struct("name", c.name, "date", o.date, "sub", map_values(o.sub))))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(named_struct("name", c.name, "date", o.date, "sub", map_values(o.sub))))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_set(struct(c.name, o.date, map_values(o.sub))))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(struct(c.name, o.date, map_values(o.sub))))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;


-- 2. test array

-- 2.1 when field is primitive

SELECT c.id, sort_array(collect_set(array(o.amount)))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(array(o.amount)))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

-- cast decimal

SELECT c.id, sort_array(collect_set(array(cast(o.amount as decimal(10,1)))))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(array(cast(o.amount as decimal(10,1)))))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

-- 2.2 when field is struct

SELECT c.id, sort_array(collect_set(array(o.sub)))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(array(o.sub)))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

-- 2.3 when field is list

SELECT c.id, sort_array(collect_set(array(map_values(o.sub))))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(array(map_values(o.sub))))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;


-- 3. test map

-- 3.1 when field is primitive

SELECT c.id, sort_array(collect_set(map("amount", o.amount)))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(map("amount", o.amount)))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

-- cast decimal

SELECT c.id, sort_array(collect_set(map("amount", cast(o.amount as decimal(10,1)))))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(map("amount", cast(o.amount as decimal(10,1)))))
FROM customers c
INNER JOIN orders o
ON (c.id = o.cid) GROUP BY c.id;

-- 3.2 when field is struct

SELECT c.id, sort_array(collect_set(map("sub", o.sub)))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(map("sub", o.sub)))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

-- 3.3 when field is list

SELECT c.id, sort_array(collect_set(map("sub", map_values(o.sub))))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;

SELECT c.id, sort_array(collect_list(map("sub", map_values(o.sub))))
FROM customers c
INNER JOIN nested_orders o
ON (c.id = o.cid) GROUP BY c.id;


-- clean up

DROP TABLE customer;
DROP TABLE orders;
DROP TABLE nested_orders
SELECT key, collect_set(create_union(value))
FROM src
GROUP BY key ORDER BY key limit 20;
CREATE TABLE kafka (contents STRING);
LOAD DATA LOCAL INPATH '../../data/files/text-en.txt' INTO TABLE kafka;
set mapred.reduce.tasks=1;
set hive.exec.reducers.max=1;

SELECT context_ngrams(sentences(lower(contents)), array(null), 100, 1000).estfrequency FROM kafka;
SELECT context_ngrams(sentences(lower(contents)), array("he",null), 100, 1000) FROM kafka;
SELECT context_ngrams(sentences(lower(contents)), array(null,"salesmen"), 100, 1000) FROM kafka;
SELECT context_ngrams(sentences(lower(contents)), array("what","i",null), 100, 1000) FROM kafka;
SELECT context_ngrams(sentences(lower(contents)), array(null,null), 100, 1000).estfrequency FROM kafka;

DROP TABLE kafka;
set hive.mapred.mode=nonstrict;
DROP TABLE covar_tab;
CREATE TABLE covar_tab (a INT, b INT, c INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/covar_tab.txt' OVERWRITE
INTO TABLE covar_tab;

DESCRIBE FUNCTION corr;
DESCRIBE FUNCTION EXTENDED corr;
SELECT corr(b, c) FROM covar_tab WHERE a < 1;
SELECT corr(b, c) FROM covar_tab WHERE a < 3;
SELECT corr(b, c) FROM covar_tab WHERE a = 3;
SELECT a, corr(b, c) FROM covar_tab GROUP BY a ORDER BY a;
SELECT corr(b, c) FROM covar_tab;

DROP TABLE covar_tab;
set hive.mapred.mode=nonstrict;
DROP TABLE covar_tab;
CREATE TABLE covar_tab (a INT, b INT, c INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/covar_tab.txt' OVERWRITE
INTO TABLE covar_tab;

DESCRIBE FUNCTION covar_pop;
DESCRIBE FUNCTION EXTENDED covar_pop;
SELECT covar_pop(b, c) FROM covar_tab WHERE a < 1;
SELECT covar_pop(b, c) FROM covar_tab WHERE a < 3;
SELECT covar_pop(b, c) FROM covar_tab WHERE a = 3;
SELECT a, covar_pop(b, c) FROM covar_tab GROUP BY a ORDER BY a;
SELECT ROUND(covar_pop(b, c), 5) FROM covar_tab;

DROP TABLE covar_tab;
set hive.mapred.mode=nonstrict;
DROP TABLE covar_tab;
CREATE TABLE covar_tab (a INT, b INT, c INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/covar_tab.txt' OVERWRITE
INTO TABLE covar_tab;

DESCRIBE FUNCTION covar_samp;
DESCRIBE FUNCTION EXTENDED covar_samp;
SELECT covar_samp(b, c) FROM covar_tab WHERE a < 1;
SELECT covar_samp(b, c) FROM covar_tab WHERE a < 3;
SELECT covar_samp(b, c) FROM covar_tab WHERE a = 3;
SELECT a, covar_samp(b, c) FROM covar_tab GROUP BY a ORDER BY a;
SELECT ROUND(covar_samp(b, c), 5) FROM covar_tab;

DROP TABLE covar_tab;
set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_avg AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg';

EXPLAIN
SELECT example_avg(substr(value,5)),
       example_avg(IF(substr(value,5) > 250, NULL, substr(value,5)))
FROM src;

SELECT example_avg(substr(value,5)),
       example_avg(IF(substr(value,5) > 250, NULL, substr(value,5)))
FROM src;

DROP TEMPORARY FUNCTION example_avg;
set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_group_concat AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleGroupConcat';

EXPLAIN
SELECT substr(value,5,1), example_group_concat("(", key, ":", value, ")")
FROM src
GROUP BY substr(value,5,1);

SELECT substr(value,5,1), example_group_concat("(", key, ":", value, ")")
FROM src
GROUP BY substr(value,5,1);


DROP TEMPORARY FUNCTION example_group_concat;
set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_max AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMax';

DESCRIBE FUNCTION EXTENDED example_max;

EXPLAIN
SELECT example_max(substr(value,5)),
       example_max(IF(substr(value,5) > 250, NULL, substr(value,5)))
FROM src;

SELECT example_max(substr(value,5)),
       example_max(IF(substr(value,5) > 250, NULL, substr(value,5)))
FROM src;

DROP TEMPORARY FUNCTION example_max;
set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_max_n AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMaxN';

EXPLAIN
SELECT example_max_n(substr(value,5),10),
       example_max_n(IF(substr(value,5) > 250, NULL, substr(value,5)),10)
FROM src;

SELECT example_max_n(substr(value,5),10),
       example_max_n(IF(substr(value,5) > 250, NULL, substr(value,5)),10)
FROM src;

DROP TEMPORARY FUNCTION example_max_n;
set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_min AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMin';

DESCRIBE FUNCTION EXTENDED example_min;

EXPLAIN
SELECT example_min(substr(value,5)),
       example_min(IF(substr(value,5) > 250, NULL, substr(value,5)))
FROM src;

SELECT example_min(substr(value,5)),
       example_min(IF(substr(value,5) > 250, NULL, substr(value,5)))
FROM src;

DROP TEMPORARY FUNCTION example_min;
set hive.mapred.mode=nonstrict;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;
CREATE TEMPORARY FUNCTION example_min_n AS 'org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleMinN';

EXPLAIN
SELECT example_min_n(substr(value,5),10),
       example_min_n(IF(substr(value,5) < 250, NULL, substr(value,5)),10)
FROM src;

SELECT example_min_n(substr(value,5),10),
       example_min_n(IF(substr(value,5) < 250, NULL, substr(value,5)),10)
FROM src;

DROP TEMPORARY FUNCTION example_min_n;

SELECT histogram_numeric(cast(substr(src.value,5) AS double), 2) FROM src;
SELECT histogram_numeric(cast(substr(src.value,5) AS double), 3) FROM src;
SELECT histogram_numeric(cast(substr(src.value,5) AS double), 20) FROM src;
SELECT histogram_numeric(cast(substr(src.value,5) AS double), 200) FROM src;
select distinct key, sum(key) from src;
CREATE TABLE kafka (contents STRING);
LOAD DATA LOCAL INPATH '../../data/files/text-en.txt' INTO TABLE kafka;
set mapred.reduce.tasks=1;
set hive.exec.reducers.max=1;

SELECT ngrams(sentences(lower(contents)), 1, 100, 1000).estfrequency FROM kafka;
SELECT ngrams(sentences(lower(contents)), 2, 100, 1000).estfrequency FROM kafka;
SELECT ngrams(sentences(lower(contents)), 3, 100, 1000).estfrequency FROM kafka;
SELECT ngrams(sentences(lower(contents)), 4, 100, 1000).estfrequency FROM kafka;
SELECT ngrams(sentences(lower(contents)), 5, 100, 1000).estfrequency FROM kafka;

DROP TABLE kafka;

EXPLAIN SELECT
  sum('a'),
  avg('a'),
  variance('a'),
  std('a')
FROM src;

SELECT
  sum('a'),
  avg('a'),
  variance('a'),
  std('a')
FROM src;
select percentile(cast(key as bigint), 0.3) from src;
-- INCLUDE_HADOOP_MAJOR_VERSIONS( 0.20S)

CREATE TABLE bucket (key double, value string) CLUSTERED BY (key) SORTED BY (key DESC)  INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket;

create table t1 (result double);
create table t2 (result double);
create table t3 (result double);
create table t4 (result double);
create table t5 (result double);
create table t6 (result double);
create table t7 (result array<double>);
create table t8 (result array<double>);
create table t9 (result array<double>);
create table t10 (result array<double>);
create table t11 (result array<double>);
create table t12 (result array<double>);

set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.map.aggr=false;
-- disable map-side aggregation
FROM bucket
insert overwrite table t1 SELECT percentile_approx(cast(key AS double), 0.5)
insert overwrite table t2 SELECT percentile_approx(cast(key AS double), 0.5, 100)
insert overwrite table t3 SELECT percentile_approx(cast(key AS double), 0.5, 1000)

insert overwrite table t4 SELECT percentile_approx(cast(key AS int), 0.5)
insert overwrite table t5 SELECT percentile_approx(cast(key AS int), 0.5, 100)
insert overwrite table t6 SELECT percentile_approx(cast(key AS int), 0.5, 1000)

insert overwrite table t7 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98))
insert overwrite table t8 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t9 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 1000)

insert overwrite table t10 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98))
insert overwrite table t11 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t12 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 1000);

select * from t1;
select * from t2;
select * from t3;
select * from t4;
select * from t5;
select * from t6;
select * from t7;
select * from t8;
select * from t9;
select * from t10;
select * from t11;
select * from t12;

set hive.map.aggr=true;
-- enable map-side aggregation
FROM bucket
insert overwrite table t1 SELECT percentile_approx(cast(key AS double), 0.5)
insert overwrite table t2 SELECT percentile_approx(cast(key AS double), 0.5, 100)
insert overwrite table t3 SELECT percentile_approx(cast(key AS double), 0.5, 1000)

insert overwrite table t4 SELECT percentile_approx(cast(key AS int), 0.5)
insert overwrite table t5 SELECT percentile_approx(cast(key AS int), 0.5, 100)
insert overwrite table t6 SELECT percentile_approx(cast(key AS int), 0.5, 1000)

insert overwrite table t7 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98))
insert overwrite table t8 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t9 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 1000)

insert overwrite table t10 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98))
insert overwrite table t11 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t12 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 1000);

select * from t1;
select * from t2;
select * from t3;
select * from t4;
select * from t5;
select * from t6;
select * from t7;
select * from t8;
select * from t9;
select * from t10;
select * from t11;
select * from t12;
set hive.mapred.mode=nonstrict;
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- 0.23 changed input order of data in reducer task, which affects result of percentile_approx

CREATE TABLE bucket (key double, value string) CLUSTERED BY (key) SORTED BY (key DESC)  INTO 4 BUCKETS STORED AS TEXTFILE;
load data local inpath '../../data/files/srcsortbucket1outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket2outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket3outof4.txt' INTO TABLE bucket;
load data local inpath '../../data/files/srcsortbucket4outof4.txt' INTO TABLE bucket;

create table t1 (result double);
create table t2 (result double);
create table t3 (result double);
create table t4 (result double);
create table t5 (result double);
create table t6 (result double);
create table t7 (result array<double>);
create table t8 (result array<double>);
create table t9 (result array<double>);
create table t10 (result array<double>);
create table t11 (result array<double>);
create table t12 (result array<double>);

set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
set hive.map.aggr=false;
-- disable map-side aggregation
FROM bucket
insert overwrite table t1 SELECT percentile_approx(cast(key AS double), 0.5)
insert overwrite table t2 SELECT percentile_approx(cast(key AS double), 0.5, 100)
insert overwrite table t3 SELECT percentile_approx(cast(key AS double), 0.5, 1000)

insert overwrite table t4 SELECT percentile_approx(cast(key AS int), 0.5)
insert overwrite table t5 SELECT percentile_approx(cast(key AS int), 0.5, 100)
insert overwrite table t6 SELECT percentile_approx(cast(key AS int), 0.5, 1000)

insert overwrite table t7 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98))
insert overwrite table t8 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t9 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 1000)

insert overwrite table t10 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98))
insert overwrite table t11 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t12 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 1000);

select * from t1;
select * from t2;
select * from t3;
select * from t4;
select * from t5;
select * from t6;
select * from t7;
select * from t8;
select * from t9;
select * from t10;
select * from t11;
select * from t12;

set hive.map.aggr=true;
-- enable map-side aggregation
FROM bucket
insert overwrite table t1 SELECT percentile_approx(cast(key AS double), 0.5)
insert overwrite table t2 SELECT percentile_approx(cast(key AS double), 0.5, 100)
insert overwrite table t3 SELECT percentile_approx(cast(key AS double), 0.5, 1000)

insert overwrite table t4 SELECT percentile_approx(cast(key AS int), 0.5)
insert overwrite table t5 SELECT percentile_approx(cast(key AS int), 0.5, 100)
insert overwrite table t6 SELECT percentile_approx(cast(key AS int), 0.5, 1000)

insert overwrite table t7 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98))
insert overwrite table t8 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t9 SELECT percentile_approx(cast(key AS double), array(0.05,0.5,0.95,0.98), 1000)

insert overwrite table t10 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98))
insert overwrite table t11 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 100)
insert overwrite table t12 SELECT percentile_approx(cast(key AS int), array(0.05,0.5,0.95,0.98), 1000);

select * from t1;
select * from t2;
select * from t3;
select * from t4;
select * from t5;
select * from t6;
select * from t7;
select * from t8;
select * from t9;
select * from t10;
select * from t11;
select * from t12;

-- NaN
explain
select percentile_approx(case when key < 100 then cast('NaN' as double) else key end, 0.5) from bucket;
select percentile_approx(case when key < 100 then cast('NaN' as double) else key end, 0.5) between 340.5 and 343.0 from bucket;

-- with CBO
explain
select percentile_approx(key, 0.5) from bucket;
select percentile_approx(key, 0.5) between 255.0 and 257.0 from bucket;
-- HIVE-5279
-- GenericUDAFSumList has Converter which does not have default constructor
-- After
create temporary function sum_list as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSumList';

select sum_list(array(key, key)) from src;
FROM src SELECT 'a' LIKE '%a%', 'b' LIKE '%a%', 'ab' LIKE '%a%', 'ab' LIKE '%a_',
  '%_' LIKE '\%\_', 'ab' LIKE '\%\_', 'ab' LIKE '_a%', 'ab' LIKE 'a',
  '' RLIKE '.*', 'a' RLIKE '[ab]', '' RLIKE '[ab]', 'hadoop' RLIKE '[a-z]*', 'hadoop' RLIKE 'o*',
  REGEXP_REPLACE('abc', 'b', 'c'), REGEXP_REPLACE('abc', 'z', 'a'), REGEXP_REPLACE('abbbb', 'bb', 'b'), REGEXP_REPLACE('hadoop', '(.)[a-z]*', '$1ive')
  WHERE src.key = 86
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(c1 STRING, c2 STRING, c3 STRING, c4 STRING,
  c5 STRING, c6 STRING, c7 STRING, c8 STRING,
  c9 STRING, c10 STRING, c11 STRING, c12 STRING, c13 STRING,
  c14 STRING, c15 STRING, c16 STRING, c17 STRING,
  c18 STRING, c19 STRING, c20 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT 'a' LIKE '%a%', 'b' LIKE '%a%', 'ab' LIKE '%a%', 'ab' LIKE '%a_',
  '%_' LIKE '\%\_', 'ab' LIKE '\%\_', 'ab' LIKE '_a%', 'ab' LIKE 'a',
  '' RLIKE '.*', 'a' RLIKE '[ab]', '' RLIKE '[ab]', 'hadoop' RLIKE '[a-z]*', 'hadoop' RLIKE 'o*',
  REGEXP_REPLACE('abc', 'b', 'c'), REGEXP_REPLACE('abc', 'z', 'a'), REGEXP_REPLACE('abbbb', 'bb', 'b'),
  REGEXP_REPLACE('hadoop', '(.)[a-z]*', '$1ive'), REGEXP_REPLACE('hadoopAAA','A.*',''),
  REGEXP_REPLACE('abc', '', 'A'), 'abc' RLIKE ''
  WHERE src.key = 86;

FROM src INSERT OVERWRITE TABLE dest1 SELECT 'a' LIKE '%a%', 'b' LIKE '%a%', 'ab' LIKE '%a%', 'ab' LIKE '%a_',
  '%_' LIKE '\%\_', 'ab' LIKE '\%\_', 'ab' LIKE '_a%', 'ab' LIKE 'a',
  '' RLIKE '.*', 'a' RLIKE '[ab]', '' RLIKE '[ab]', 'hadoop' RLIKE '[a-z]*', 'hadoop' RLIKE 'o*',
  REGEXP_REPLACE('abc', 'b', 'c'), REGEXP_REPLACE('abc', 'z', 'a'), REGEXP_REPLACE('abbbb', 'bb', 'b'),
  REGEXP_REPLACE('hadoop', '(.)[a-z]*', '$1ive'), REGEXP_REPLACE('hadoopAAA','A.*',''),
  REGEXP_REPLACE('abc', '', 'A'), 'abc' RLIKE ''
  WHERE src.key = 86;

SELECT dest1.* FROM dest1;
CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

FROM src INSERT OVERWRITE TABLE dest1 SELECT '  abc  ' WHERE src.key = 86;

EXPLAIN
SELECT '|', trim(dest1.c1), '|', rtrim(dest1.c1), '|', ltrim(dest1.c1), '|' FROM dest1;

SELECT '|', trim(dest1.c1), '|', rtrim(dest1.c1), '|', ltrim(dest1.c1), '|' FROM dest1;
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(c1 STRING, c2 STRING, c3 STRING, c4 STRING, c5 STRING) STORED AS TEXTFILE;

EXPLAIN
FROM src INSERT OVERWRITE TABLE dest1 SELECT count(CAST('' AS INT)), sum(CAST('' AS INT)), avg(CAST('' AS INT)),
min(CAST('' AS INT)), max(CAST('' AS INT));

FROM src INSERT OVERWRITE TABLE dest1 SELECT count(CAST('' AS INT)), sum(CAST('' AS INT)), avg(CAST('' AS INT)),
min(CAST('' AS INT)), max(CAST('' AS INT));

SELECT dest1.* FROM dest1;
SELECT round(1.0), round(1.5), round(-1.5), floor(1.0), floor(1.5), floor(-1.5), sqrt(1.0), sqrt(-1.0), sqrt(0.0), ceil(1.0), ceil(1.5), ceil(-1.5), ceiling(1.0), rand(3), +3, -3, 1++2, 1+-2, ~1 FROM dest1
CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

FROM src INSERT OVERWRITE TABLE dest1 SELECT '  abc  ' WHERE src.key = 86;

EXPLAIN
SELECT round(1.0), round(1.5), round(-1.5), floor(1.0), floor(1.5), floor(-1.5), sqrt(1.0), sqrt(-1.0), sqrt(0.0), ceil(1.0), ceil(1.5), ceil(-1.5), ceiling(1.0), rand(3), +3, -3, 1++2, 1+-2,

~1,
~CAST(1 AS TINYINT),
~CAST(1 AS SMALLINT),
~CAST(1 AS BIGINT),

CAST(1 AS TINYINT) & CAST(2 AS TINYINT),
CAST(1 AS SMALLINT) & CAST(2 AS SMALLINT),
1 & 2,
CAST(1 AS BIGINT) & CAST(2 AS BIGINT),

CAST(1 AS TINYINT) | CAST(2 AS TINYINT),
CAST(1 AS SMALLINT) | CAST(2 AS SMALLINT),
1 | 2,
CAST(1 AS BIGINT) | CAST(2 AS BIGINT),

CAST(1 AS TINYINT) ^ CAST(3 AS TINYINT),
CAST(1 AS SMALLINT) ^ CAST(3 AS SMALLINT),
1 ^ 3,
CAST(1 AS BIGINT) ^ CAST(3 AS BIGINT)

FROM dest1;

SELECT round(1.0), round(1.5), round(-1.5), floor(1.0), floor(1.5), floor(-1.5), sqrt(1.0), sqrt(-1.0), sqrt(0.0), ceil(1.0), ceil(1.5), ceil(-1.5), ceiling(1.0), rand(3), +3, -3, 1++2, 1+-2,
~1,
~CAST(1 AS TINYINT),
~CAST(1 AS SMALLINT),
~CAST(1 AS BIGINT),

CAST(1 AS TINYINT) & CAST(2 AS TINYINT),
CAST(1 AS SMALLINT) & CAST(2 AS SMALLINT),
1 & 2,
CAST(1 AS BIGINT) & CAST(2 AS BIGINT),

CAST(1 AS TINYINT) | CAST(2 AS TINYINT),
CAST(1 AS SMALLINT) | CAST(2 AS SMALLINT),
1 | 2,
CAST(1 AS BIGINT) | CAST(2 AS BIGINT),

CAST(1 AS TINYINT) ^ CAST(3 AS TINYINT),
CAST(1 AS SMALLINT) ^ CAST(3 AS SMALLINT),
1 ^ 3,
CAST(1 AS BIGINT) ^ CAST(3 AS BIGINT)

FROM dest1;
CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

FROM src INSERT OVERWRITE TABLE dest1 SELECT '  abc  ' WHERE src.key = 86;

EXPLAIN
SELECT from_unixtime(1226446340), to_date(from_unixtime(1226446340)), day('2008-11-01'), month('2008-11-01'), year('2008-11-01'), day('2008-11-01 15:32:20'), month('2008-11-01 15:32:20'), year('2008-11-01 15:32:20') FROM dest1;

SELECT from_unixtime(1226446340), to_date(from_unixtime(1226446340)), day('2008-11-01'), month('2008-11-01'), year('2008-11-01'), day('2008-11-01 15:32:20'), month('2008-11-01 15:32:20'), year('2008-11-01 15:32:20') FROM dest1;

EXPLAIN
SELECT from_unixtime(unix_timestamp('2010-01-13 11:57:40', 'yyyy-MM-dd HH:mm:ss'), 'MM/dd/yy HH:mm:ss'), from_unixtime(unix_timestamp('2010-01-13 11:57:40')) from dest1;

SELECT from_unixtime(unix_timestamp('2010-01-13 11:57:40', 'yyyy-MM-dd HH:mm:ss'), 'MM/dd/yy HH:mm:ss'), from_unixtime(unix_timestamp('2010-01-13 11:57:40')) from dest1;
FROM src SELECT CONCAT('a', 'b'), IF(TRUE, 1 ,2) + key
CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

FROM src INSERT OVERWRITE TABLE dest1 SELECT '  abc  ' WHERE src.key = 86;

EXPLAIN
SELECT IF(TRUE, 1, 2) FROM dest1;

SELECT IF(TRUE, 1, 2) FROM dest1;

EXPLAIN
SELECT IF(TRUE, 1, 2), IF(FALSE, 1, 2), IF(NULL, 1, 2), IF(TRUE, "a", "b"),
       IF(TRUE, 0.1, 0.2), IF(FALSE, CAST(1 AS BIGINT), CAST(2 AS BIGINT)),
       IF(FALSE, CAST(127 AS TINYINT), CAST(126 AS TINYINT)),
       IF(FALSE, CAST(127 AS SMALLINT), CAST(128 AS SMALLINT)),
       CAST(128 AS INT), CAST(1.0 AS DOUBLE),
       CAST('128' AS STRING) FROM dest1;

SELECT IF(TRUE, 1, 2), IF(FALSE, 1, 2), IF(NULL, 1, 2), IF(TRUE, "a", "b"),
       IF(TRUE, 0.1, 0.2), IF(FALSE, CAST(1 AS BIGINT), CAST(2 AS BIGINT)),
       IF(FALSE, CAST(127 AS TINYINT), CAST(126 AS TINYINT)),
       IF(FALSE, CAST(127 AS SMALLINT), CAST(128 AS SMALLINT)),
       CAST(128 AS INT), CAST(1.0 AS DOUBLE),
       CAST('128' AS STRING) FROM dest1;
CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

FROM src INSERT OVERWRITE TABLE dest1 SELECT '  abc  ' WHERE src.key = 86;

EXPLAIN
SELECT ROUND(LN(3.0),12), LN(0.0), LN(-1), ROUND(LOG(3.0),12), LOG(0.0),
       LOG(-1), ROUND(LOG2(3.0),12), LOG2(0.0), LOG2(-1),
       ROUND(LOG10(3.0),12), LOG10(0.0), LOG10(-1), ROUND(LOG(2, 3.0),12),
       LOG(2, 0.0), LOG(2, -1), LOG(0.5, 2), LOG(2, 0.5), ROUND(EXP(2.0),12),
       POW(2,3), POWER(2,3), POWER(2,-3), POWER(0.5, -3), POWER(4, 0.5),
       POWER(-1, 0.5), POWER(-1, 2), POWER(CAST (1 AS DECIMAL), CAST (0 AS INT)),
       POWER(CAST (2 AS DECIMAL), CAST (3 AS INT)),
       POW(CAST (2 AS DECIMAL), CAST(3 AS INT)) FROM dest1;

SELECT ROUND(LN(3.0),12), LN(0.0), LN(-1), ROUND(LOG(3.0),12), LOG(0.0),
       LOG(-1), ROUND(LOG2(3.0),12), LOG2(0.0), LOG2(-1),
       ROUND(LOG10(3.0),12), LOG10(0.0), LOG10(-1), ROUND(LOG(2, 3.0),12),
       LOG(2, 0.0), LOG(2, -1), LOG(0.5, 2), LOG(2, 0.5), ROUND(EXP(2.0),12),
       POW(2,3), POWER(2,3), POWER(2,-3), POWER(0.5, -3), POWER(4, 0.5),
       POWER(-1, 0.5), POWER(-1, 2), POWER(CAST (1 AS DECIMAL), CAST (0 AS INT)),
       POWER(CAST (2 AS DECIMAL), CAST (3 AS INT)),
       POW(CAST (2 AS DECIMAL), CAST(3 AS INT)) FROM dest1;
CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

FROM src INSERT OVERWRITE TABLE dest1 SELECT '' WHERE src.key = 86;

FROM src INSERT OVERWRITE TABLE dest1 SELECT '1' WHERE src.key = 86;

EXPLAIN
SELECT avg(c1), sum(c1), count(c1) FROM dest1;

SELECT avg(c1), sum(c1), count(c1) FROM dest1;
set hive.mapred.mode=nonstrict;
EXPLAIN
SELECT DATEDIFF('2008-12-31', '2009-01-01'), DATEDIFF('2008-03-01', '2008-02-28'),
       DATEDIFF('2007-03-01', '2007-01-28'), DATEDIFF('2008-03-01 23:59:59', '2008-03-02 00:00:00'),
       DATE_ADD('2008-12-31', 1), DATE_ADD('2008-12-31', 365),
       DATE_ADD('2008-02-28', 2), DATE_ADD('2009-02-28', 2),
       DATE_ADD('2007-02-28', 365), DATE_ADD('2007-02-28 23:59:59', 730),
       DATE_SUB('2009-01-01', 1), DATE_SUB('2009-01-01', 365),
       DATE_SUB('2008-02-28', 2), DATE_SUB('2009-02-28', 2),
       DATE_SUB('2007-02-28', 365), DATE_SUB('2007-02-28 01:12:34', 730)
       FROM src WHERE src.key = 86;

SELECT DATEDIFF('2008-12-31', '2009-01-01'), DATEDIFF('2008-03-01', '2008-02-28'),
       DATEDIFF('2007-03-01', '2007-01-28'), DATEDIFF('2008-03-01 23:59:59', '2008-03-02 00:00:00'),
       DATE_ADD('2008-12-31', 1), DATE_ADD('2008-12-31', 365),
       DATE_ADD('2008-02-28', 2), DATE_ADD('2009-02-28', 2),
       DATE_ADD('2007-02-28', 365), DATE_ADD('2007-02-28 23:59:59', 730),
       DATE_SUB('2009-01-01', 1), DATE_SUB('2009-01-01', 365),
       DATE_SUB('2008-03-01', 2), DATE_SUB('2009-03-01', 2),
       DATE_SUB('2007-02-28', 365), DATE_SUB('2007-02-28 01:12:34', 730)
       FROM src WHERE src.key = 86;
set hive.mapred.mode=nonstrict;
CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

EXPLAIN
INSERT OVERWRITE TABLE dest1
SELECT trim(trim(trim(trim(trim(trim(trim(trim(trim(trim( '  abc  '))))))))))
FROM src
WHERE src.key = 86;

INSERT OVERWRITE TABLE dest1
SELECT trim(trim(trim(trim(trim(trim(trim(trim(trim(trim( '  abc  '))))))))))
FROM src
WHERE src.key = 86;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION abs;
DESCRIBE FUNCTION EXTENDED abs;

EXPLAIN SELECT
  abs(0),
  abs(-1),
  abs(123),
  abs(-9223372036854775807),
  abs(9223372036854775807)
FROM src tablesample (1 rows);

SELECT
  abs(0),
  abs(-1),
  abs(123),
  abs(-9223372036854775807),
  abs(9223372036854775807)
FROM src tablesample (1 rows);

EXPLAIN SELECT
  abs(0.0),
  abs(-3.14159265),
  abs(3.14159265)
FROM src tablesample (1 rows);

SELECT
  abs(0.0),
  abs(-3.14159265),
  abs(3.14159265)
FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION acos;
DESCRIBE FUNCTION EXTENDED acos;

SELECT acos(null)
FROM src tablesample (1 rows);

SELECT acos(0)
FROM src tablesample (1 rows);

SELECT acos(-0.5), asin(0.66)
FROM src tablesample (1 rows);

SELECT acos(2)
FROM src tablesample (1 rows);
DESCRIBE FUNCTION +;
DESCRIBE FUNCTION EXTENDED +;
DESCRIBE FUNCTION add_months;
DESCRIBE FUNCTION EXTENDED add_months;

explain select add_months('2014-01-14', 1);

select
add_months('2014-01-14', 1),
add_months('2014-01-31', 1),
add_months('2014-02-28', -1),
add_months('2014-02-28', 2),
add_months('2014-04-30', -2),
add_months('2015-02-28', 12),
add_months('2016-02-29', -12),
add_months('2016-01-29', 1),
add_months('2016-02-29', -1),
add_months('2014-01-32', 1),
add_months('01/14/2014', 1),
add_months(cast(null as string), 1),
add_months('2014-01-14', cast(null as int));

select
add_months('2014-01-14 10:30:00', 1),
add_months('2014-01-31 10:30:00', 1),
add_months('2014-02-28 10:30:00', -1),
add_months('2014-02-28 16:30:00', 2),
add_months('2014-04-30 10:30:00', -2),
add_months('2015-02-28 10:30:00', 12),
add_months('2016-02-29 10:30:00', -12),
add_months('2016-01-29 10:30:00', 1),
add_months('2016-02-29 10:30:00', -1),
add_months('2014-01-32 10:30:00', 1);

select
add_months(cast('2014-01-14 10:30:00' as timestamp), 1),
add_months(cast('2014-01-31 10:30:00' as timestamp), 1),
add_months(cast('2014-02-28 10:30:00' as timestamp), -1),
add_months(cast('2014-02-28 16:30:00' as timestamp), 2),
add_months(cast('2014-04-30 10:30:00' as timestamp), -2),
add_months(cast('2015-02-28 10:30:00' as timestamp), 12),
add_months(cast('2016-02-29 10:30:00' as timestamp), -12),
add_months(cast('2016-01-29 10:30:00' as timestamp), 1),
add_months(cast('2016-02-29 10:30:00' as timestamp), -1),
add_months(cast(null as timestamp), 1);select add_months(14567893456, 3);select add_months('2015-02-03', 2.4);DESCRIBE FUNCTION aes_decrypt;
DESC FUNCTION EXTENDED aes_decrypt;

explain select aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), '1234567890123456');

select
aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), '1234567890123456'),
aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), binary('1234567890123456')),
aes_decrypt(unbase64("BQGHoM3lqYcsurCRq3PlUw=="), '1234567890123456') = binary(''),
aes_decrypt(unbase64("BQGHoM3lqYcsurCRq3PlUw=="), binary('1234567890123456')) = binary(''),
aes_decrypt(cast(null as binary), '1234567890123456'),
aes_decrypt(cast(null as binary), binary('1234567890123456'));

--bad key
select
aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), '12345678901234567'),
aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), binary('123456789012345')),
aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), ''),
aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), binary('')),
aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), cast(null as string)),
aes_decrypt(unbase64("y6Ss+zCYObpCbgfWfyNWTw=="), cast(null as binary));DESCRIBE FUNCTION aes_encrypt;
DESC FUNCTION EXTENDED aes_encrypt;

explain select aes_encrypt('ABC', '1234567890123456');

select
base64(aes_encrypt('ABC', '1234567890123456')),
base64(aes_encrypt('', '1234567890123456')),
base64(aes_encrypt(binary('ABC'), binary('1234567890123456'))),
base64(aes_encrypt(binary(''), binary('1234567890123456'))),
aes_encrypt(cast(null as string), '1234567890123456'),
aes_encrypt(cast(null as binary), binary('1234567890123456'));

--bad key
select
aes_encrypt('ABC', '12345678901234567'),
aes_encrypt(binary('ABC'), binary('123456789012345')),
aes_encrypt('ABC', ''),
aes_encrypt(binary('ABC'), binary('')),
aes_encrypt('ABC', cast(null as string)),
aes_encrypt(binary('ABC'), cast(null as binary));set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION array;
DESCRIBE FUNCTION EXTENDED array;

EXPLAIN SELECT array(), array()[1], array(1, 2, 3), array(1, 2, 3)[2], array(1,"a", 2, 3), array(1,"a", 2, 3)[2],
array(array(1), array(2), array(3), array(4))[1][0] FROM src tablesample (1 rows);

SELECT array(), array()[1], array(1, 2, 3), array(1, 2, 3)[2], array(1,"a", 2, 3), array(1,"a", 2, 3)[2],
array(array(1), array(2), array(3), array(4))[1][0] FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION array_contains;
DESCRIBE FUNCTION EXTENDED array_contains;

-- evalutes function for array of primitives
SELECT array_contains(array(1, 2, 3), 1) FROM src tablesample (1 rows);

-- evaluates function for nested arrays
SELECT array_contains(array(array(1,2), array(2,3), array(3,4)), array(1,2))
FROM src tablesample (1 rows);
-- invalid first argument
SELECT array_contains(1, 2) FROM src LIMIT 1;
-- invalid second argument
SELECT array_contains(array(1, 2, 3), '2') FROM src LIMIT 1;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION ascii;
DESCRIBE FUNCTION EXTENDED ascii;

EXPLAIN SELECT
  ascii('Facebook'),
  ascii(''),
  ascii('!')
FROM src tablesample (1 rows);

SELECT
  ascii('Facebook'),
  ascii(''),
  ascii('!')
FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION asin;
DESCRIBE FUNCTION EXTENDED asin;

SELECT asin(null)
FROM src tablesample (1 rows);

SELECT asin(0)
FROM src tablesample (1 rows);

SELECT asin(-0.5), asin(0.66)
FROM src tablesample (1 rows);

SELECT asin(2)
FROM src tablesample (1 rows);
DESCRIBE FUNCTION ASSERT_TRUE;

EXPLAIN SELECT ASSERT_TRUE(x > 0) FROM src LATERAL VIEW EXPLODE(ARRAY(1, 2)) a AS x LIMIT 2;
SELECT ASSERT_TRUE(x > 0) FROM src LATERAL VIEW EXPLODE(ARRAY(1, 2)) a AS x LIMIT 2;

EXPLAIN SELECT ASSERT_TRUE(x < 2) FROM src LATERAL VIEW EXPLODE(ARRAY(1, 2)) a AS x LIMIT 2;
SELECT ASSERT_TRUE(x < 2) FROM src LATERAL VIEW EXPLODE(ARRAY(1, 2)) a AS x LIMIT 2;
EXPLAIN SELECT 1 + ASSERT_TRUE(x < 2) FROM src LATERAL VIEW EXPLODE(ARRAY(1, 2)) a AS x LIMIT 2;
SELECT 1 + ASSERT_TRUE(x < 2) FROM src LATERAL VIEW EXPLODE(ARRAY(1, 2)) a AS x LIMIT 2;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION atan;
DESCRIBE FUNCTION EXTENDED atan;

SELECT atan(null)
FROM src tablesample (1 rows);

SELECT atan(1), atan(6), atan(-1.0)
FROM src tablesample (1 rows);
DESCRIBE FUNCTION atan;
DESCRIBE FUNCTION EXTENDED atan;

SELECT atan(null)
FROM src tablesample (1 rows);

SELECT atan(1), atan(6), atan(-1.0)
FROM src tablesample (1 rows);
DESCRIBE FUNCTION avg;
DESCRIBE FUNCTION EXTENDED avg;
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

describe function between;
describe function extended between;

explain SELECT * FROM src where key + 100 between (150 + -50) AND (150 + 50) LIMIT 20;
SELECT * FROM src where key + 100 between (150 + -50) AND (150 + 50) LIMIT 20;

explain SELECT * FROM src where key + 100 not between (150 + -50) AND (150 + 50) LIMIT 20;
SELECT * FROM src where key + 100 not between (150 + -50) AND (150 + 50) LIMIT 20;

explain SELECT * FROM src where 'b' between 'a' AND 'c' LIMIT 1;
SELECT * FROM src where 'b' between 'a' AND 'c' LIMIT 1;

explain SELECT * FROM src where 2 between 2 AND '3' LIMIT 1;
SELECT * FROM src where 2 between 2 AND '3' LIMIT 1;
DESCRIBE FUNCTION bigint;
DESCRIBE FUNCTION EXTENDED bigint;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION bin;
DESCRIBE FUNCTION EXTENDED bin;

SELECT
  bin(1),
  bin(0),
  bin(99992421)
FROM src tablesample (1 rows);

-- Negative numbers should be treated as two's complement (64 bit).
SELECT bin(-5) FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

select ewah_bitmap_and(array(13,2,4,8589934592,4096,0), array(13,2,4,8589934592,4096,0)) from src tablesample (1 rows);
select ewah_bitmap_and(array(13,2,4,8589934592,4096,0), array(8,2,4,8589934592,128,0)) from src tablesample (1 rows);

drop table bitmap_test;
create table bitmap_test (a array<bigint>, b array<bigint>);

insert overwrite table bitmap_test
select array(13,2,4,8589934592,4096,0), array(8,2,4,8589934592,128,0) from src tablesample (10 rows);

select ewah_bitmap_and(a,b) from bitmap_test;

drop table bitmap_test;
set hive.fetch.task.conversion=more;

select ewah_bitmap_empty(array(13,2,4,8589934592,0,0)) from src tablesample (1 rows);

select ewah_bitmap_empty(array(13,2,4,8589934592,4096,0)) from src tablesample (1 rows);
set hive.fetch.task.conversion=more;

select ewah_bitmap_or(array(13,2,4,8589934592,4096,0), array(13,2,4,8589934592,4096,0)) from src tablesample (1 rows);
select ewah_bitmap_or(array(13,2,4,8589934592,4096,0), array(8,2,4,8589934592,128,0)) from src tablesample (1 rows);

drop table bitmap_test;
create table bitmap_test (a array<bigint>, b array<bigint>);

insert overwrite table bitmap_test
select array(13,2,4,8589934592,4096,0), array(8,2,4,8589934592,128,0) from src tablesample (10 rows);

select ewah_bitmap_or(a,b) from bitmap_test;

drop table bitmap_test;
DESCRIBE FUNCTION &;
DESCRIBE FUNCTION EXTENDED &;
DESCRIBE FUNCTION ~;
DESCRIBE FUNCTION EXTENDED ~;
DESCRIBE FUNCTION |;
DESCRIBE FUNCTION EXTENDED |;
DESCRIBE FUNCTION shiftleft;
DESC FUNCTION EXTENDED shiftleft;

explain select shiftleft(4, 1);

select
shiftleft(a, 0),
shiftleft(a, 1),
shiftleft(a, 2),
shiftleft(a, 3),
shiftleft(a, 4),
shiftleft(a, 5),
shiftleft(a, 6),
shiftleft(a, 7),
shiftleft(a, 8),
shiftleft(a, 13),
shiftleft(a, 14),
shiftleft(a, 29),
shiftleft(a, 30),
shiftleft(a, 61),
shiftleft(a, 62)
from (
  select cast(4 as tinyint) a
) t;

select
shiftleft(a, 0),
shiftleft(a, 1),
shiftleft(a, 2),
shiftleft(a, 3),
shiftleft(a, 4),
shiftleft(a, 5),
shiftleft(a, 6),
shiftleft(a, 7),
shiftleft(a, 8),
shiftleft(a, 13),
shiftleft(a, 14),
shiftleft(a, 29),
shiftleft(a, 30),
shiftleft(a, 61),
shiftleft(a, 62)
from (
  select cast(4 as smallint) a
) t;

select
shiftleft(a, 0),
shiftleft(a, 1),
shiftleft(a, 2),
shiftleft(a, 3),
shiftleft(a, 4),
shiftleft(a, 5),
shiftleft(a, 6),
shiftleft(a, 7),
shiftleft(a, 8),
shiftleft(a, 13),
shiftleft(a, 14),
shiftleft(a, 29),
shiftleft(a, 30),
shiftleft(a, 61),
shiftleft(a, 62)
from (
  select cast(4 as int) a
) t;

select
shiftleft(a, 0),
shiftleft(a, 1),
shiftleft(a, 2),
shiftleft(a, 3),
shiftleft(a, 4),
shiftleft(a, 5),
shiftleft(a, 6),
shiftleft(a, 7),
shiftleft(a, 8),
shiftleft(a, 13),
shiftleft(a, 14),
shiftleft(a, 29),
shiftleft(a, 30),
shiftleft(a, 61),
shiftleft(a, 62)
from (
  select cast(4 as bigint) a
) t;

select
shiftleft(4, 33),
shiftleft(4, 65),
shiftleft(4, 4001),
shiftleft(16, -2),
shiftleft(4, cast(null as int)),
shiftleft(cast(null as int), 4),
shiftleft(cast(null as int), cast(null as int));DESCRIBE FUNCTION shiftright;
DESC FUNCTION EXTENDED shiftright;

explain select shiftright(4, 1);

select
shiftright(a, 0),
shiftright(a, 1),
shiftright(a, 2),
shiftright(a, 3),
shiftright(a, 4),
shiftright(a, 5),
shiftright(a, 6),
shiftright(a, 31),
shiftright(a, 32)
from (
  select cast(-128 as tinyint) a
) t;

select
shiftright(a, 0),
shiftright(a, 1),
shiftright(a, 2),
shiftright(a, 8),
shiftright(a, 9),
shiftright(a, 10),
shiftright(a, 11),
shiftright(a, 12),
shiftright(a, 13),
shiftright(a, 14),
shiftright(a, 31),
shiftright(a, 32)
from (
  select cast(-32768 as smallint) a
) t;

select
shiftright(a, 0),
shiftright(a, 1),
shiftright(a, 2),
shiftright(a, 24),
shiftright(a, 25),
shiftright(a, 26),
shiftright(a, 27),
shiftright(a, 28),
shiftright(a, 29),
shiftright(a, 30),
shiftright(a, 31),
shiftright(a, 32)
from (
  select cast(-2147483648 as int) a
) t;

select
shiftright(a, 0),
shiftright(a, 1),
shiftright(a, 2),
shiftright(a, 56),
shiftright(a, 57),
shiftright(a, 58),
shiftright(a, 59),
shiftright(a, 60),
shiftright(a, 61),
shiftright(a, 62),
shiftright(a, 63),
shiftright(a, 64)
from (
  select cast(-9223372036854775808 as bigint) a
) t;

select
shiftright(1024, 33),
shiftright(1024, 65),
shiftright(1024, 4001),
shiftright(1024, -2),
shiftright(1024, cast(null as int)),
shiftright(cast(null as int), 4),
shiftright(cast(null as int), cast(null as int));DESCRIBE FUNCTION shiftrightunsigned;
DESC FUNCTION EXTENDED shiftrightunsigned;

explain select shiftrightunsigned(4, 1);

select
shiftrightunsigned(a, 0),
shiftrightunsigned(a, 1),
shiftrightunsigned(a, 2),
shiftrightunsigned(a, 31),
shiftrightunsigned(a, 32)
from (
  select cast(-128 as tinyint) a
) t;

select
shiftrightunsigned(a, 0),
shiftrightunsigned(a, 1),
shiftrightunsigned(a, 2),
shiftrightunsigned(a, 31),
shiftrightunsigned(a, 32)
from (
  select cast(-32768 as smallint) a
) t;

select
shiftrightunsigned(a, 0),
shiftrightunsigned(a, 1),
shiftrightunsigned(a, 2),
shiftrightunsigned(a, 31),
shiftrightunsigned(a, 32)
from (
  select cast(-2147483648 as int) a
) t;

select
shiftrightunsigned(a, 0),
shiftrightunsigned(a, 1),
shiftrightunsigned(a, 2),
shiftrightunsigned(a, 63),
shiftrightunsigned(a, 64)
from (
  select cast(-9223372036854775808 as bigint) a
) t;

select
shiftrightunsigned(1024, 33),
shiftrightunsigned(1024, 65),
shiftrightunsigned(1024, 4001),
shiftrightunsigned(1024, -2),
shiftrightunsigned(1024, cast(null as int)),
shiftrightunsigned(cast(null as int), 4),
shiftrightunsigned(cast(null as int), cast(null as int));DESCRIBE FUNCTION ^;
DESCRIBE FUNCTION EXTENDED ^;
DESCRIBE FUNCTION boolean;
DESCRIBE FUNCTION EXTENDED boolean;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION bround;
DESC FUNCTION EXTENDED bround;

select
bround(2.5),
bround(3.5),
bround(2.49),
bround(3.49),
bround(2.51),
bround(3.51);

select
bround(1.25, 1),
bround(1.35, 1),
bround(1.249, 1),
bround(1.349, 1),
bround(1.251, 1),
bround(1.351, 1);

select
bround(-1.25, 1),
bround(-1.35, 1),
bround(-1.249, 1),
bround(-1.349, 1),
bround(-1.251, 1),
bround(-1.351, 1);

select
bround(55.0, -1),
bround(45.0, -1),
bround(54.9, -1),
bround(44.9, -1),
bround(55.1, -1),
bround(45.1, -1);

select
bround(-55.0, -1),
bround(-45.0, -1),
bround(-54.9, -1),
bround(-44.9, -1),
bround(-55.1, -1),
bround(-45.1, -1);
SELECT CASE 1
        WHEN 1 THEN 2
        WHEN 3 THEN 4
        ELSE 5
       END,
       CASE 11
        WHEN 12 THEN 13
        WHEN 14 THEN 15
       END
FROM src LIMIT 1
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION case;
DESCRIBE FUNCTION EXTENDED case;

EXPLAIN
SELECT CASE 1
        WHEN 1 THEN 2
        WHEN 3 THEN 4
        ELSE 5
       END,
       CASE 2
        WHEN 1 THEN 2
        ELSE 5
       END,
       CASE 14
        WHEN 12 THEN 13
        WHEN 14 THEN 15
       END,
       CASE 16
        WHEN 12 THEN 13
        WHEN 14 THEN 15
       END,
       CASE 17
        WHEN 18 THEN NULL
        WHEN 17 THEN 20
       END,
       CASE 21
        WHEN 22 THEN 23
        WHEN 21 THEN 24
       END
FROM src tablesample (1 rows);

SELECT CASE 1
        WHEN 1 THEN 2
        WHEN 3 THEN 4
        ELSE 5
       END,
       CASE 2
        WHEN 1 THEN 2
        ELSE 5
       END,
       CASE 14
        WHEN 12 THEN 13
        WHEN 14 THEN 15
       END,
       CASE 16
        WHEN 12 THEN 13
        WHEN 14 THEN 15
       END,
       CASE 17
        WHEN 18 THEN NULL
        WHEN 17 THEN 20
       END,
       CASE 21
        WHEN 22 THEN 23
        WHEN 21 THEN 24
       END
FROM src tablesample (1 rows);

-- verify that short-circuiting is working correctly for CASE
-- we should never get to the ELSE branch, which would raise an exception
SELECT CASE 1 WHEN 1 THEN 'yo'
ELSE reflect('java.lang.String', 'bogus', 1) END
FROM src tablesample (1 rows);

-- Allow compatible types in when/return type
SELECT CASE 1
        WHEN 1 THEN 123.0BD
        ELSE 0.0BD
       END,
       CASE 1
        WHEN 1.0 THEN 123
        WHEN 2 THEN 1.0
        ELSE 222.02BD
       END,
       CASE 'abc'
        WHEN cast('abc' as varchar(3)) THEN 'abcd'
        WHEN 'efg' THEN cast('efgh' as varchar(10))
        ELSE cast('ijkl' as char(4))
       END
FROM src tablesample (1 rows);
set hive.mapred.mode=nonstrict;
EXPLAIN
SELECT CASE a.key
        WHEN '1' THEN 2
        WHEN '3' THEN 4
        ELSE 5
       END as key
FROM src a JOIN src b
ON a.key = b.key
ORDER BY key LIMIT 10;

SELECT CASE a.key
        WHEN '1' THEN 2
        WHEN '3' THEN 4
        ELSE 5
       END as key
FROM src a JOIN src b
ON a.key = b.key
ORDER BY key LIMIT 10;
set hive.fetch.task.conversion=more;

EXPLAIN
SELECT CASE src_thrift.lint[0]
        WHEN 0 THEN src_thrift.lint[0] + 1
        WHEN 1 THEN src_thrift.lint[0] + 2
        WHEN 2 THEN 100
        ELSE 5
       END,
       CASE src_thrift.lstring[0]
        WHEN '0' THEN 'zero'
        WHEN '10' THEN CONCAT(src_thrift.lstring[0], " is ten")
        ELSE 'default'
       END,
       (CASE src_thrift.lstring[0]
        WHEN '0' THEN src_thrift.lstring
        ELSE NULL
       END)[0]
FROM src_thrift tablesample (3 rows);

SELECT CASE src_thrift.lint[0]
        WHEN 0 THEN src_thrift.lint[0] + 1
        WHEN 1 THEN src_thrift.lint[0] + 2
        WHEN 2 THEN 100
        ELSE 5
       END,
       CASE src_thrift.lstring[0]
        WHEN '0' THEN 'zero'
        WHEN '10' THEN CONCAT(src_thrift.lstring[0], " is ten")
        ELSE 'default'
       END,
       (CASE src_thrift.lstring[0]
        WHEN '0' THEN src_thrift.lstring
        ELSE NULL
       END)[0]
FROM src_thrift tablesample (3 rows);
DESCRIBE FUNCTION cbrt;
DESC FUNCTION EXTENDED cbrt;

explain select cbrt(27.0);

select
cbrt(0.0),
cbrt(1.0),
cbrt(-1),
cbrt(27),
cbrt(-27.0),
cbrt(87860583272930481),
cbrt(cast(null as double));DESCRIBE FUNCTION ceil;
DESCRIBE FUNCTION EXTENDED ceil;
DESCRIBE FUNCTION ceiling;
DESCRIBE FUNCTION EXTENDED ceiling;
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-custom-udfs/udf-classloader-udf1/${system:hive.version}/udf-classloader-udf1-${system:hive.version}.jar;
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-custom-udfs/udf-classloader-util/${system:hive.version}/udf-classloader-util-${system:hive.version}.jar;
ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-custom-udfs/udf-classloader-udf2/${system:hive.version}/udf-classloader-udf2-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION f1 AS 'hive.it.custom.udfs.UDF1';
CREATE TEMPORARY FUNCTION f2 AS 'hive.it.custom.udfs.UDF2';

-- udf-classloader-udf1.jar contains f1 which relies on udf-classloader-util.jar,
-- similiary udf-classloader-udf2.jar contains f2 which also relies on udf-classloader-util.jar.
SELECT f1(*), f2(*) FROM SRC limit 1;

DELETE JAR ${system:maven.local.repository}/org/apache/hive/hive-it-custom-udfs/udf-classloader-udf2/${system:hive.version}/udf-classloader-udf2-${system:hive.version}.jar;
SELECT f1(*) FROM SRC limit 1;

ADD JAR ${system:maven.local.repository}/org/apache/hive/hive-it-custom-udfs/udf-classloader-udf2/${system:hive.version}/udf-classloader-udf2-${system:hive.version}.jar;
SELECT f2(*) FROM SRC limit 1;
ADD JAR ivy://org.apache.hive.hive-it-custom-udfs:udf-classloader-udf1:+;
ADD JAR ivy://org.apache.hive.hive-it-custom-udfs:udf-classloader-util:+;
ADD JAR ivy://org.apache.hive.hive-it-custom-udfs:udf-classloader-udf2:+;

CREATE TEMPORARY FUNCTION f1 AS 'hive.it.custom.udfs.UDF1';
CREATE TEMPORARY FUNCTION f2 AS 'hive.it.custom.udfs.UDF2';

-- udf-classloader-udf1.jar contains f1 which relies on udf-classloader-util.jar,
-- similiary udf-classloader-udf2.jar contains f2 which also relies on udf-classloader-util.jar.
SELECT f1(*), f2(*) FROM SRC limit 1;

DELETE JAR ivy://org.apache.hive.hive-it-custom-udfs:udf-classloader-udf2:+;
SELECT f1(*) FROM SRC limit 1;

ADD JAR ivy://org.apache.hive.hive-it-custom-udfs:udf-classloader-udf2:+;
SELECT f2(*) FROM SRC limit 1;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION coalesce;
DESCRIBE FUNCTION EXTENDED coalesce;

EXPLAIN
SELECT COALESCE(1),
       COALESCE(1, 2),
       COALESCE(NULL, 2),
       COALESCE(1, NULL),
       COALESCE(NULL, NULL, 3),
       COALESCE(4, NULL, NULL, NULL),
       COALESCE('1'),
       COALESCE('1', '2'),
       COALESCE(NULL, '2'),
       COALESCE('1', NULL),
       COALESCE(NULL, NULL, '3'),
       COALESCE('4', NULL, NULL, NULL),
       COALESCE(1.0),
       COALESCE(1.0, 2.0),
       COALESCE(NULL, 2.0),
       COALESCE(NULL, 2.0, 3.0),
       COALESCE(2.0, NULL, 3.0),
       COALESCE(IF(TRUE, NULL, 0), NULL)
FROM src tablesample (1 rows);

SELECT COALESCE(1),
       COALESCE(1, 2),
       COALESCE(NULL, 2),
       COALESCE(1, NULL),
       COALESCE(NULL, NULL, 3),
       COALESCE(4, NULL, NULL, NULL),
       COALESCE('1'),
       COALESCE('1', '2'),
       COALESCE(NULL, '2'),
       COALESCE('1', NULL),
       COALESCE(NULL, NULL, '3'),
       COALESCE('4', NULL, NULL, NULL),
       COALESCE(1.0),
       COALESCE(1.0, 2.0),
       COALESCE(NULL, 2.0),
       COALESCE(NULL, 2.0, 3.0),
       COALESCE(2.0, NULL, 3.0),
       COALESCE(IF(TRUE, NULL, 0), NULL)
FROM src tablesample (1 rows);

EXPLAIN
SELECT COALESCE(src_thrift.lint[1], 999),
       COALESCE(src_thrift.lintstring[0].mystring, '999'),
       COALESCE(src_thrift.mstringstring['key_2'], '999')
FROM src_thrift;

SELECT COALESCE(src_thrift.lint[1], 999),
       COALESCE(src_thrift.lintstring[0].mystring, '999'),
       COALESCE(src_thrift.mstringstring['key_2'], '999')
FROM src_thrift;
SELECT COALESCE(array('a', 'b'), '2.0') FROM src LIMIT 1;
set hive.fetch.task.conversion=more;

EXPLAIN
CREATE TEMPORARY FUNCTION test_udf_get_java_string AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString';

CREATE TEMPORARY FUNCTION test_udf_get_java_string AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaString';

select * from src where value = test_udf_get_java_string("val_66");
select * from (select * from src where value = 'val_66' or value = 'val_8') t where value <> test_udf_get_java_string("val_8");


DROP TEMPORARY FUNCTION test_udf_get_java_boolean;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION concat;
DESCRIBE FUNCTION EXTENDED concat;

SELECT
  concat('a', 'b'),
  concat('a', 'b', 'c'),
  concat('a', null, 'c'),
  concat(null),
  concat('a'),
  concat(null, 1, 2),
  concat(1, 2, 3, 'a'),
  concat(1, 2),
  concat(1),
  concat('1234', 'abc', 'extra argument')
FROM src tablesample (1 rows);

-- binary/mixed
SELECT
  concat(cast('ab' as binary), cast('cd' as binary)),
  concat('ab', cast('cd' as binary))
FROM src tablesample (1 rows);
CREATE TABLE dest1(key INT, value STRING) STORED AS TEXTFILE;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', concat(src.key) WHERE src.key < 100 group by src.key;

SELECT dest1.* FROM dest1;

CREATE TABLE dest1(key STRING, value STRING) STORED AS TEXTFILE;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT concat('1234', 'abc', 'extra argument'), src.value WHERE src.key < 100;

SELECT dest1.* FROM dest1;


set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION concat_ws;
DESCRIBE FUNCTION EXTENDED concat_ws;

CREATE TABLE dest1(c1 STRING, c2 STRING, c3 STRING);

FROM src INSERT OVERWRITE TABLE dest1 SELECT 'abc', 'xyz', '8675309'  WHERE src.key = 86;

EXPLAIN
SELECT concat_ws(dest1.c1, dest1.c2, dest1.c3),
       concat_ws(',', dest1.c1, dest1.c2, dest1.c3),
       concat_ws(NULL, dest1.c1, dest1.c2, dest1.c3),
       concat_ws('**', dest1.c1, NULL, dest1.c3) FROM dest1;

SELECT concat_ws(dest1.c1, dest1.c2, dest1.c3),
       concat_ws(',', dest1.c1, dest1.c2, dest1.c3),
       concat_ws(NULL, dest1.c1, dest1.c2, dest1.c3),
       concat_ws('**', dest1.c1, NULL, dest1.c3) FROM dest1;

-- evalutes function for array of strings
EXPLAIN
SELECT concat_ws('.', array('www', 'face', 'book', 'com'), '1234'),
       concat_ws('-', 'www', array('face', 'book', 'com'), '1234'),
       concat_ws('F', 'www', array('face', 'book', 'com', '1234')),
       concat_ws('_', array('www', 'face'), array('book', 'com', '1234')),
       concat_ws('**', 'www', array('face'), array('book', 'com', '1234')),
       concat_ws('[]', array('www'), 'face', array('book', 'com', '1234')),
       concat_ws('AAA', array('www'), array('face', 'book', 'com'), '1234') FROM dest1 tablesample (1 rows);

SELECT concat_ws('.', array('www', 'face', 'book', 'com'), '1234'),
       concat_ws('-', 'www', array('face', 'book', 'com'), '1234'),
       concat_ws('F', 'www', array('face', 'book', 'com', '1234')),
       concat_ws('_', array('www', 'face'), array('book', 'com', '1234')),
       concat_ws('**', 'www', array('face'), array('book', 'com', '1234')),
       concat_ws('[]', array('www'), 'face', array('book', 'com', '1234')),
       concat_ws('AAA', array('www'), array('face', 'book', 'com'), '1234') FROM dest1 tablesample (1 rows);

SELECT concat_ws(NULL, array('www', 'face', 'book', 'com'), '1234'),
       concat_ws(NULL, 'www', array('face', 'book', 'com'), '1234'),
       concat_ws(NULL, 'www', array('face', 'book', 'com', '1234')),
       concat_ws(NULL, array('www', 'face'), array('book', 'com', '1234')),
       concat_ws(NULL, 'www', array('face'), array('book', 'com', '1234')),
       concat_ws(NULL, array('www'), 'face', array('book', 'com', '1234')),
       concat_ws(NULL, array('www'), array('face', 'book', 'com'), '1234') FROM dest1 tablesample (1 rows);
-- invalid argument number
SELECT concat_ws('-') FROM src LIMIT 1;
-- invalid argument type
SELECT concat_ws('[]', array(100, 200, 50)) FROM src LIMIT 1;
-- invalid argument type
SELECT concat_ws(1234, array('www', 'facebook', 'com')) FROM src LIMIT 1;
create temporary function counter as 'org.apache.hadoop.hive.ql.udf.generic.DummyContextUDF';

set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

select *, counter(key) from src limit 20;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION conv;
DESCRIBE FUNCTION EXTENDED conv;

-- conv must work on both strings and integers up to 64-bit precision

-- Some simple conversions to test different bases
SELECT
  conv('4521', 10, 36),
  conv('22', 10, 10),
  conv('110011', 2, 16),
  conv('facebook', 36, 16)
FROM src tablesample (1 rows);

-- Test negative numbers. If to_base is positive, the number should be handled
-- as a two's complement (64-bit)
SELECT
  conv('-641', 10, -10),
  conv('1011', 2, -16),
  conv('-1', 10, 16),
  conv('-15', 10, 16)
FROM src tablesample (1 rows);

-- Test overflow. If a number is two large, the result should be -1 (if signed)
-- or MAX_LONG (if unsigned)
SELECT
  conv('9223372036854775807', 36, 16),
  conv('9223372036854775807', 36, -16),
  conv('-9223372036854775807', 36, 16),
  conv('-9223372036854775807', 36, -16)
FROM src tablesample (1 rows);

-- Test with invalid input. If one of the bases is invalid, the result should
-- be NULL. If there is an invalid digit in the number, the longest valid
-- prefix should be converted.
SELECT
  conv('123455', 3, 10),
  conv('131', 1, 5),
  conv('515', 5, 100),
  conv('10', -2, 2)
FROM src tablesample (1 rows);

-- Perform the same tests with number arguments.

SELECT
  conv(4521, 10, 36),
  conv(22, 10, 10),
  conv(110011, 2, 16)
FROM src tablesample (1 rows);

SELECT
  conv(-641, 10, -10),
  conv(1011, 2, -16),
  conv(-1, 10, 16),
  conv(-15, 10, 16)
FROM src tablesample (1 rows);

SELECT
  conv(9223372036854775807, 36, 16),
  conv(9223372036854775807, 36, -16),
  conv(-9223372036854775807, 36, 16),
  conv(-9223372036854775807, 36, -16)
FROM src tablesample (1 rows);

SELECT
  conv(123455, 3, 10),
  conv(131, 1, 5),
  conv(515, 5, 100),
  conv('10', -2, 2)
FROM src tablesample (1 rows);

-- Make sure that state is properly reset.

SELECT conv(key, 10, 16),
       conv(key, 16, 10)
FROM src tablesample (3 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION cos;
DESCRIBE FUNCTION EXTENDED cos;

SELECT cos(null)
FROM src tablesample (1 rows);

SELECT cos(0.98), cos(1.57), cos(-0.5)
FROM src tablesample (1 rows);
DESCRIBE FUNCTION count;
DESCRIBE FUNCTION EXTENDED count;

EXPLAIN SELECT count(key) FROM src;
SELECT count(key) FROM src;

EXPLAIN SELECT count(DISTINCT key) FROM src;
SELECT count(DISTINCT key) FROM src;

EXPLAIN SELECT count(DISTINCT key, value) FROM src;
SELECT count(DISTINCT key, value) FROM src;

EXPLAIN SELECT count(*) FROM src;
SELECT count(*) FROM src;

EXPLAIN SELECT count(1) FROM src;
SELECT count(1) FROM src;

select count(1) from src where false;
select count(*) from src where false;
DESCRIBE FUNCTION crc32;
DESC FUNCTION EXTENDED crc32;

explain select crc32('ABC');

select
crc32('ABC'),
crc32(''),
crc32(binary('ABC')),
crc32(binary('')),
crc32(cast(null as string)),
crc32(cast(null as binary)),
crc32(null);
DESCRIBE FUNCTION current_database;

explain
select current_database();
select current_database();

create database xxx;
use xxx;

explain
select current_database();
select current_database();

set hive.fetch.task.conversion=more;

use default;

explain
select current_database();
select current_database();

use xxx;

explain
select current_database();
select current_database();
DESCRIBE FUNCTION current_user;
DESCRIBE FUNCTION EXTENDED current_user;

select current_user()
FROM src tablesample (1 rows);
DESCRIBE FUNCTION datediff;
DESCRIBE FUNCTION EXTENDED datediff;
DESCRIBE FUNCTION date_add;
DESCRIBE FUNCTION EXTENDED date_add;

-- Test different numeric data types for date_add
SELECT date_add('1900-01-01', cast(10 as tinyint)),
       date_add('1900-01-01', cast(10 as smallint)),
       date_add('1900-01-01', cast(10 as int));DESCRIBE FUNCTION date_format;
DESC FUNCTION EXTENDED date_format;

explain select date_format('2015-04-08', 'EEEE');

--string date
select
date_format('2015-04-08', 'E'),
date_format('2015-04-08', 'G'),
date_format('2015-04-08', 'y'),
date_format('2015-04-08', 'Y'),
date_format('2015-04-08', 'MMM'),
date_format('2015-04-08', 'w'),
date_format('2015-04-08', 'W'),
date_format('2015-04-08', 'D'),
date_format('2015-04-08', 'd'),
date_format(cast(null as string), 'dd'),
date_format('01/29/2014', 'dd');

--string timestamp
select
date_format('2015-04-08 10:30:45', 'HH'),
date_format('2015-04-08 10:30:45', 'mm'),
date_format('2015-04-08 10:30:45', 'ss'),
date_format('2015-04-08 21:30:45', 'hh a'),
date_format('2015-04-08 10:30', 'dd'),
date_format('2015-04-08 10:30:45.123', 'S'),
date_format('2015-04-08T10:30:45', 'dd'),
date_format('2015-04-08 10', 'dd'),
date_format(cast(null as string), 'dd'),
date_format('04/08/2015 10:30:45', 'dd');

--date
select
date_format(cast('2015-04-08' as date), 'EEEE'),
date_format(cast('2015-04-08' as date), 'G'),
date_format(cast('2015-04-08' as date), 'yyyy'),
date_format(cast('2015-04-08' as date), 'YY'),
date_format(cast('2015-04-08' as date), 'MMM'),
date_format(cast('2015-04-08' as date), 'w'),
date_format(cast('2015-04-08' as date), 'W'),
date_format(cast('2015-04-08' as date), 'D'),
date_format(cast('2015-04-08' as date), 'd'),
date_format(cast(null as date), 'dd');

--timestamp
select
date_format(cast('2015-04-08 10:30:45' as timestamp), 'HH'),
date_format(cast('2015-04-08 10:30:45' as timestamp), 'mm'),
date_format(cast('2015-04-08 10:30:45' as timestamp), 'ss'),
date_format(cast('2015-04-08 10:30:45' as timestamp), 'hh a'),
date_format(cast('2015-04-08 10:30:45' as timestamp), 'dd'),
date_format(cast('2015-04-08 10:30:45.123' as timestamp), 'SSS'),
date_format(cast('2015-04-08 10:30:45.123456789' as timestamp), 'SSS'),
date_format(cast(null as timestamp), 'HH');

-- wrong fmt
select
date_format('2015-04-08', ''),
date_format('2015-04-08', 'Q');
DESCRIBE FUNCTION date_sub;
DESCRIBE FUNCTION EXTENDED date_sub;

-- Test different numeric data types for date_add
SELECT date_sub('1900-01-01', cast(10 as tinyint)),
       date_sub('1900-01-01', cast(10 as smallint)),
       date_sub('1900-01-01', cast(10 as int));DESCRIBE FUNCTION day;
DESCRIBE FUNCTION EXTENDED day;
DESCRIBE FUNCTION dayofmonth;
DESCRIBE FUNCTION EXTENDED dayofmonth;
DESCRIBE FUNCTION decode;
DESC FUNCTION EXTENDED decode;

explain select decode(binary('TestDecode1'), 'UTF-8');

select
decode(binary('TestDecode1'), 'UTF-8'),
decode(binary('TestDecode2'), cast('UTF-8' as varchar(10))),
decode(binary('TestDecode3'), cast('UTF-8' as char(5))),
decode(cast(null as binary), 'UTF-8');set hive.fetch.task.conversion=more;

explain
select degrees(PI()) FROM src tablesample (1 rows);

select degrees(PI()) FROM src tablesample (1 rows);

DESCRIBE FUNCTION degrees;
DESCRIBE FUNCTION EXTENDED degrees;
explain
select degrees(PI()) FROM src tablesample (1 rows);

select degrees(PI()) FROM src tablesample (1 rows);

DESCRIBE FUNCTION degrees;
DESCRIBE FUNCTION EXTENDED degrees;set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION div;
DESCRIBE FUNCTION EXTENDED div;

SELECT 3 DIV 2 FROM SRC tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION /;
DESCRIBE FUNCTION EXTENDED /;

SELECT 3 / 2 FROM SRC tablesample (1 rows);
DESCRIBE FUNCTION double;
DESCRIBE FUNCTION EXTENDED double;
set hive.fetch.task.conversion=more;

explain
select E() FROM src tablesample (1 rows);

select E() FROM src tablesample (1 rows);

DESCRIBE FUNCTION E;
DESCRIBE FUNCTION EXTENDED E;
explain
select E() FROM src tablesample (1 rows);

select E() FROM src tablesample (1 rows);

DESCRIBE FUNCTION E;
DESCRIBE FUNCTION EXTENDED E;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION elt;
DESCRIBE FUNCTION EXTENDED elt;

EXPLAIN
SELECT elt(2, 'abc', 'defg'),
       elt(3, 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg'),
       elt('1', 'abc', 'defg'),
       elt(2, 'aa', CAST('2' AS TINYINT)),
       elt(2, 'aa', CAST('12345' AS SMALLINT)),
       elt(2, 'aa', CAST('123456789012' AS BIGINT)),
       elt(2, 'aa', CAST(1.25 AS FLOAT)),
       elt(2, 'aa', CAST(16.0 AS DOUBLE)),
       elt(null, 'abc', 'defg'),
       elt(0, 'abc', 'defg'),
       elt(3, 'abc', 'defg')
FROM src tablesample (1 rows);

SELECT elt(2, 'abc', 'defg'),
       elt(3, 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg'),
       elt('1', 'abc', 'defg'),
       elt(2, 'aa', CAST('2' AS TINYINT)),
       elt(2, 'aa', CAST('12345' AS SMALLINT)),
       elt(2, 'aa', CAST('123456789012' AS BIGINT)),
       elt(2, 'aa', CAST(1.25 AS FLOAT)),
       elt(2, 'aa', CAST(16.0 AS DOUBLE)),
       elt(null, 'abc', 'defg'),
       elt(0, 'abc', 'defg'),
       elt(3, 'abc', 'defg')
FROM src tablesample (1 rows);
SELECT elt(3) FROM src;
FROM src_thrift
SELECT elt(1, src_thrift.lintstring)
WHERE src_thrift.lintstring IS NOT NULL;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION =;
DESCRIBE FUNCTION EXTENDED =;

DESCRIBE FUNCTION ==;
DESCRIBE FUNCTION EXTENDED ==;

SELECT true=false, false=true, false=false, true=true, NULL=NULL, true=NULL, NULL=true, false=NULL, NULL=false FROM src tablesample (1 rows);

DESCRIBE FUNCTION <=>;
DESCRIBE FUNCTION EXTENDED <=>;

SELECT true<=>false, false<=>true, false<=>false, true<=>true, NULL<=>NULL, true<=>NULL, NULL<=>true, false<=>NULL, NULL<=>false FROM src tablesample (1 rows);
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_add AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleAdd';

EXPLAIN
SELECT example_add(1, 2),
       example_add(1, 2, 3),
       example_add(1, 2, 3, 4),
       example_add(1.1, 2.2),
       example_add(1.1, 2.2, 3.3),
       example_add(1.1, 2.2, 3.3, 4.4),
       example_add(1, 2, 3, 4.4)
FROM src LIMIT 1;

SELECT example_add(1, 2),
       example_add(1, 2, 3),
       example_add(1, 2, 3, 4),
       example_add(1.1, 2.2),
       example_add(1.1, 2.2, 3.3),
       example_add(1.1, 2.2, 3.3, 4.4),
       example_add(1, 2, 3, 4.4)
FROM src LIMIT 1;

DROP TEMPORARY FUNCTION example_add;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_add AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleAdd';

EXPLAIN
SELECT example_add(1, 2),
       example_add(1, 2, 3),
       example_add(1, 2, 3, 4),
       example_add(1.1, 2.2),
       example_add(1.1, 2.2, 3.3),
       example_add(1.1, 2.2, 3.3, 4.4),
       example_add(1, 2, 3, 4.4)
FROM src LIMIT 1;

SELECT example_add(1, 2),
       example_add(1, 2, 3),
       example_add(1, 2, 3, 4),
       example_add(1.1, 2.2),
       example_add(1.1, 2.2, 3.3),
       example_add(1.1, 2.2, 3.3, 4.4),
       example_add(1, 2, 3, 4.4)
FROM src LIMIT 1;

DROP TEMPORARY FUNCTION example_add;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_arraysum    AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleArraySum';
CREATE TEMPORARY FUNCTION example_mapconcat   AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleMapConcat';
CREATE TEMPORARY FUNCTION example_structprint AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleStructPrint';

EXPLAIN
SELECT example_arraysum(lint), example_mapconcat(mstringstring), example_structprint(lintstring[0])
FROM src_thrift;

SELECT example_arraysum(lint), example_mapconcat(mstringstring), example_structprint(lintstring[0])
FROM src_thrift;

DROP TEMPORARY FUNCTION example_arraysum;
DROP TEMPORARY FUNCTION example_mapconcat;
DROP TEMPORARY FUNCTION example_structprint;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION example_format AS 'org.apache.hadoop.hive.contrib.udf.example.UDFExampleFormat';

EXPLAIN
SELECT example_format("abc"),
       example_format("%1$s", 1.1),
       example_format("%1$s %2$e", 1.1, 1.2),
       example_format("%1$x %2$o %3$d", 10, 10, 10)
FROM src LIMIT 1;

SELECT example_format("abc"),
       example_format("%1$s", 1.1),
       example_format("%1$s %2$e", 1.1, 1.2),
       example_format("%1$x %2$o %3$d", 10, 10, 10)
FROM src LIMIT 1;

DROP TEMPORARY FUNCTION example_format;
DESCRIBE FUNCTION exp;
DESCRIBE FUNCTION EXTENDED exp;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION explode;
DESCRIBE FUNCTION EXTENDED explode;

EXPLAIN EXTENDED SELECT explode(array(1,2,3)) AS myCol FROM src tablesample (1 rows);
EXPLAIN EXTENDED SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src tablesample (1 rows)) a GROUP BY a.myCol;

SELECT explode(array(1,2,3)) AS myCol FROM src tablesample (1 rows);
SELECT explode(array(1,2,3)) AS (myCol) FROM src tablesample (1 rows);
SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src tablesample (1 rows)) a GROUP BY a.myCol;

EXPLAIN EXTENDED SELECT explode(map(1,'one',2,'two',3,'three')) AS (key,val) FROM src tablesample (1 rows);
EXPLAIN EXTENDED SELECT a.key, a.val, count(1) FROM (SELECT explode(map(1,'one',2,'two',3,'three')) AS (key,val) FROM src tablesample (1 rows)) a GROUP BY a.key, a.val;

SELECT explode(map(1,'one',2,'two',3,'three')) AS (key,val) FROM src tablesample (1 rows);
SELECT a.key, a.val, count(1) FROM (SELECT explode(map(1,'one',2,'two',3,'three')) AS (key,val) FROM src tablesample (1 rows)) a GROUP BY a.key, a.val;

drop table lazy_array_map;
create table lazy_array_map (map_col map<int,string>, array_col array<string>);
INSERT OVERWRITE TABLE lazy_array_map select map(1,'one',2,'two',3,'three'), array('100','200','300') FROM src tablesample (1 rows);

SELECT array_col, myCol from lazy_array_map lateral view explode(array_col) X AS myCol;
SELECT map_col, myKey, myValue from lazy_array_map lateral view explode(map_col) X AS myKey, myValue;DESCRIBE FUNCTION factorial;
DESC FUNCTION EXTENDED factorial;

explain select factorial(5);

select
factorial(5),
factorial(0),
factorial(20),
factorial(-1),
factorial(21),
factorial(cast(null as int));set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION field;
DESCRIBE FUNCTION EXTENDED field;

SELECT
  field("x", "a", "b", "c", "d"),
  field(NULL, "a", "b", "c", "d"),
  field(0, 1, 2, 3, 4)
FROM src tablesample (1 rows);

SELECT
  field("a", "a", "b", "c", "d"),
  field("b", "a", "b", "c", "d"),
  field("c", "a", "b", "c", "d"),
  field("d", "a", "b", "c", "d"),
  field("d", "a", "b", NULL, "d")
FROM src tablesample (1 rows);

SELECT
  field(1, 1, 2, 3, 4),
  field(2, 1, 2, 3, 4),
  field(3, 1, 2, 3, 4),
  field(4, 1, 2, 3, 4),
  field(4, 1, 2, NULL, 4)
FROM src tablesample (1 rows);


CREATE TABLE test_table(col1 STRING, col2 STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE test_table;

select col1,col2,
  field("66",col1),
  field("66",col1, col2),
  field("val_86",col1, col2),
  field(NULL, col1, col2),
  field(col1, 66, 88),
  field(col1, "66", "88"),
  field(col1, "666", "888"),
  field(col2, "66", "88"),
  field(col1, col2, col1),
  field(col1, col2, "66")
from test_table where col1="86" or col1="66";


CREATE TABLE test_table1(col1 int, col2 string) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv1.txt' INTO TABLE test_table1;

select col1,col2,
  field(66,col1),
  field(66,col1, col2),
  field(86, col2, col1),
  field(86, col1, col1),
  field(86,col1,n,col2),
  field(NULL,col1,n,col2),
  field(col1, col2)
from (select col1, col2, NULL as n from test_table1 where col1=86 or col1=66) t;
SELECT field(3) FROM src;
FROM src_thrift
SELECT field(1, src_thrift.lintstring)
WHERE src_thrift.lintstring IS NOT NULL;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION find_in_set;
DESCRIBE FUNCTION EXTENDED find_in_set;

EXPLAIN
FROM src1 SELECT find_in_set(src1.key,concat(src1.key,',',src1.value));

FROM src1 SELECT find_in_set(src1.key,concat(src1.key,',',src1.value));

SELECT find_in_set('ab','ab,abc,abcde') FROM src1 tablesample (1 rows);
SELECT find_in_set('ab','abc,ab,bbb') FROM src1 tablesample (1 rows);
SELECT find_in_set('ab','def,abc,ab') FROM src1 tablesample (1 rows);
SELECT find_in_set('ab','abc,abd,abf') FROM src1 tablesample (1 rows);
SELECT find_in_set(null,'a,b,c') FROM src1 tablesample (1 rows);
SELECT find_in_set('a',null) FROM src1 tablesample (1 rows);
SELECT find_in_set('', '') FROM src1 tablesample (1 rows);
SELECT find_in_set('',',') FROM src1 tablesample (1 rows);
SELECT find_in_set('','a,,b') FROM src1 tablesample (1 rows);
SELECT find_in_set('','a,b,') FROM src1 tablesample (1 rows);
SELECT find_in_set(',','a,b,d,') FROM src1 tablesample (1 rows);
SELECT find_in_set('a','') FROM src1 tablesample (1 rows);
SELECT find_in_set('a,','a,b,c,d') FROM src1 tablesample (1 rows);

SELECT * FROM src1 WHERE NOT find_in_set(key,'311,128,345,2,956')=0;
DESCRIBE FUNCTION float;
DESCRIBE FUNCTION EXTENDED float;
DESCRIBE FUNCTION floor;
DESCRIBE FUNCTION EXTENDED floor;
set hive.mapred.mode=nonstrict;
drop table if exists udf_tb1;
drop table if exists udf_tb2;

create table udf_tb1 (year int, month int);
create table udf_tb2(month int);
insert into udf_tb1 values(2001, 11);
insert into udf_tb2 values(11);

explain
select unix_timestamp(concat(a.year, '-01-01 00:00:00')) from (select * from udf_tb1 where year=2001) a join udf_tb2 b on (a.month=b.month);
select unix_timestamp(concat(a.year, '-01-01 00:00:00')) from (select * from udf_tb1 where year=2001) a join udf_tb2 b on (a.month=b.month);
set hive.fetch.task.conversion=more;

use default;
-- Test format_number() UDF

DESCRIBE FUNCTION format_number;
DESCRIBE FUNCTION EXTENDED format_number;

EXPLAIN
SELECT format_number(12332.123456, 4),
    format_number(12332.1,4),
    format_number(12332.2,0) FROM src tablesample (1 rows);

SELECT format_number(12332.123456, 4),
    format_number(12332.1,4),
    format_number(12332.2,0)
FROM src tablesample (1 rows);

-- positive numbers
SELECT format_number(0.123456789, 12),
    format_number(12345678.123456789, 5),
    format_number(1234567.123456789, 7),
    format_number(123456.123456789, 0)
FROM src tablesample (1 rows);

-- negative numbers
SELECT format_number(-123456.123456789, 0),
    format_number(-1234567.123456789, 2),
    format_number(-0.123456789, 15),
    format_number(-12345.123456789, 4)
FROM src tablesample (1 rows);

-- zeros
SELECT format_number(0.0, 4),
    format_number(0.000000, 1),
    format_number(000.0000, 1),
    format_number(00000.0000, 1),
    format_number(-00.0, 4)
FROM src tablesample (1 rows);

-- integers
SELECT format_number(0, 0),
    format_number(1, 4),
    format_number(12, 2),
    format_number(123, 5),
    format_number(1234, 7)
FROM src tablesample (1 rows);

-- long and double boundary
-- 9223372036854775807 is LONG_MAX
-- -9223372036854775807 is one more than LONG_MIN,
-- due to HIVE-2733, put it here to check LONG_MIN boundary
-- 4.9E-324 and 1.7976931348623157E308 are Double.MIN_VALUE and Double.MAX_VALUE
-- check them for Double boundary
SELECT format_number(-9223372036854775807, 10),
    format_number(9223372036854775807, 20),
    format_number(4.9E-324, 324),
    format_number(1.7976931348623157E308, 308)
FROM src tablesample (1 rows);

-- floats
SELECT format_number(CAST(12332.123456 AS FLOAT), 4),
    format_number(CAST(12332.1 AS FLOAT), 4),
    format_number(CAST(-12332.2 AS FLOAT), 0)
FROM src tablesample (1 rows);

-- decimals
SELECT format_number(12332.123456BD, 4),
    format_number(12332.123456BD, 2),
    format_number(12332.1BD, 4),
    format_number(-12332.2BD, 0),
    format_number(CAST(12332.567 AS DECIMAL(8, 1)), 4)
FROM src tablesample (1 rows);

-- nulls
SELECT
  format_number(cast(null as int), 0),
  format_number(12332.123456BD, cast(null as int)),
  format_number(cast(null as int), cast(null as int));
-- invalid argument length
SELECT format_number(12332.123456) FROM src LIMIT 1;
-- invalid argument length
SELECT format_number(12332.123456, 2, 3) FROM src LIMIT 1;
-- invalid argument(second argument should be >= 0)
SELECT format_number(12332.123456, -4) FROM src LIMIT 1;
-- invalid argument type
SELECT format_number(12332.123456, 4.01) FROM src LIMIT 1;
-- invalid argument type
SELECT format_number(array(12332.123456, 321.23), 5) FROM src LIMIT 1;
-- invalid argument type
SELECT format_number(12332.123456, "4") FROM src LIMIT 1;
-- invalid argument type(format_number returns the result as a string)
SELECT format_number(format_number(12332.123456, 4), 2) FROM src LIMIT 1;
DESCRIBE FUNCTION from_unixtime;
DESCRIBE FUNCTION EXTENDED from_unixtime;
DESCRIBE FUNCTION from_utc_timestamp;
DESC FUNCTION EXTENDED from_utc_timestamp;

explain select from_utc_timestamp('2012-02-11 10:30:00', 'PST');

select
from_utc_timestamp('2012-02-11 04:30:00', 'PST'),
from_utc_timestamp('2012-02-11 04:30:00', 'Europe/Moscow'),
from_utc_timestamp('2012-02-11 04:30:00', 'GMT+8'),
from_utc_timestamp('2012-02-11 04:30:00', 'GMT'),
from_utc_timestamp('2012-02-11 04:30:00', ''),
from_utc_timestamp('2012-02-11 04:30:00', '---'),
from_utc_timestamp(cast(null as string), 'PST'),
from_utc_timestamp('2012-02-11 04:30:00', cast(null as string));

select
from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), 'PST'),
from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), 'Europe/Moscow'),
from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), 'GMT+8'),
from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), 'GMT'),
from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), ''),
from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), '---'),
from_utc_timestamp(cast(null as timestamp), 'PST'),
from_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), cast(null as string));

select
from_utc_timestamp('2012-02-11-04:30:00', 'UTC'),
from_utc_timestamp('2012-02-11-04:30:00', 'PST');

CREATE TEMPORARY FUNCTION moo AS 'org.apache.hadoop.hive.ql.Driver';
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION get_json_object;
DESCRIBE FUNCTION EXTENDED get_json_object;

CREATE TABLE dest1(c1 STRING) STORED AS TEXTFILE;

FROM src INSERT OVERWRITE TABLE dest1 SELECT '  abc  ' WHERE src.key = 86;

set hive.fetch.task.conversion=more;

EXPLAIN
SELECT get_json_object(src_json.json, '$.owner') FROM src_json;

SELECT get_json_object(src_json.json, '$') FROM src_json;

SELECT get_json_object(src_json.json, '$.owner'), get_json_object(src_json.json, '$.store') FROM src_json;

SELECT get_json_object(src_json.json, '$.store.bicycle'), get_json_object(src_json.json, '$.store.book') FROM src_json;

SELECT get_json_object(src_json.json, '$.store.book[0]'), get_json_object(src_json.json, '$.store.book[*]') FROM src_json;

SELECT get_json_object(src_json.json, '$.store.book[0].category'), get_json_object(src_json.json, '$.store.book[*].category'), get_json_object(src_json.json, '$.store.book[*].isbn'), get_json_object(src_json.json, '$.store.book[*].reader') FROM src_json;

SELECT get_json_object(src_json.json, '$.store.book[*].reader[0].age'), get_json_object(src_json.json, '$.store.book[*].reader[*].age')  FROM src_json;

SELECT get_json_object(src_json.json, '$.store.basket[0][1]'), get_json_object(src_json.json, '$.store.basket[*]'), get_json_object(src_json.json, '$.store.basket[*][0]'), get_json_object(src_json.json, '$.store.basket[0][*]'), get_json_object(src_json.json, '$.store.basket[*][*]'), get_json_object(src_json.json, '$.store.basket[0][2].b'), get_json_object(src_json.json, '$.store.basket[0][*].b') FROM src_json;

SELECT get_json_object(src_json.json, '$.non_exist_key'),  get_json_object(src_json.json, '$..no_recursive'), get_json_object(src_json.json, '$.store.book[10]'), get_json_object(src_json.json, '$.store.book[0].non_exist_key'), get_json_object(src_json.json, '$.store.basket[*].non_exist_key'), get_json_object(src_json.json, '$.store.basket[0][*].non_exist_key') FROM src_json;

SELECT get_json_object(src_json.json, '$.zip code') FROM src_json;

SELECT get_json_object(src_json.json, '$.fb:testid') FROM src_json;


-- Verify that get_json_object can handle new lines in JSON values

CREATE TABLE dest2(c1 STRING) STORED AS RCFILE;

INSERT OVERWRITE TABLE dest2 SELECT '{"a":"b\nc"}' FROM src tablesample (1 rows);

SELECT * FROM dest2;

SELECT get_json_object(c1, '$.a') FROM dest2;

--root is array
SELECT
get_json_object('[1,2,3]', '$[0]'),
get_json_object('[1,2,3]', '$.[0]'),
get_json_object('[1,2,3]', '$.[1]'),
get_json_object('[1,2,3]', '$[1]'),
get_json_object('[1,2,3]', '$[2]'),
get_json_object('[1,2,3]', '$[*]'),
get_json_object('[1,2,3]', '$'),
get_json_object('[{"k1":"v1"},{"k2":"v2"},{"k3":"v3"}]', '$[2]'),
get_json_object('[{"k1":"v1"},{"k2":"v2"},{"k3":"v3"}]', '$[2].k3'),
get_json_object('[[1,2,3],[4,5,6],[7,8,9]]', '$[1]'),
get_json_object('[[1,2,3],[4,5,6],[7,8,9]]', '$[1][0]'),
get_json_object('[{"k1":[{"k11":[1,2,3]}]}]', '$[0].k1[0].k11[1]');

--null
SELECT
get_json_object('[1,2,3]', '[2]'),
get_json_object('[1,2,3]', '$0'),
get_json_object('[1,2,3]', '$[3]');set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION >;
DESCRIBE FUNCTION EXTENDED >;

SELECT true>false, false>true, false>false, true>true FROM src tablesample (1 rows);set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION >=;
DESCRIBE FUNCTION EXTENDED >=;

SELECT true>=false, false>=true, false>=false, true>=true FROM src tablesample (1 rows);set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION greatest;
DESCRIBE FUNCTION EXTENDED greatest;

EXPLAIN
SELECT GREATEST('a', 'b', 'c'),
       GREATEST('C', 'a', 'B'),
       GREATEST('AAA', 'AaA', 'AAa'),
       GREATEST('A', 'AA', 'AAA'),
       GREATEST('11', '13', '12'),
       GREATEST('11', '2', '12'),
       GREATEST('01', '03', '02'),
       GREATEST('01', '1', '02'),
       GREATEST(null, 'b', 'c' ),
       GREATEST('a', null, 'c'),
       GREATEST('a', 'b', null ),
       GREATEST('a', null, null),
       GREATEST(null, 'b', null),
       GREATEST(cast(null as string), null, null)
FROM src tablesample (1 rows);

SELECT GREATEST('a', 'b', 'c'),
       GREATEST('C', 'a', 'B'),
       GREATEST('AAA', 'AaA', 'AAa'),
       GREATEST('A', 'AA', 'AAA'),
       GREATEST('11', '13', '12'),
       GREATEST('11', '2', '12'),
       GREATEST('01', '03', '02'),
       GREATEST('01', '1', '02'),
       GREATEST(null, 'b', 'c' ),
       GREATEST('a', null, 'c'),
       GREATEST('a', 'b', null ),
       GREATEST('a', null, null),
       GREATEST(null, 'b', null),
       GREATEST(cast(null as string), null, null)
FROM src tablesample (1 rows);

SELECT GREATEST(11, 13, 12),
       GREATEST(1, 13, 2),
       GREATEST(-11, -13, -12),
       GREATEST(1, -13, 2),
       GREATEST(null, 1, 2),
       GREATEST(1, null, 2),
       GREATEST(1, 2, null),
       GREATEST(cast(null as int), null, null)
FROM src tablesample (1 rows);

SELECT GREATEST(11.4, 13.5, 12.2),
       GREATEST(1.0, 13.2, 2.0),
       GREATEST(-11.4, -13.1, -12.2),
       GREATEST(1.0, -13.3, 2.2),
       GREATEST(null, 1.1, 2.2),
       GREATEST(1.1, null, 2.2),
       GREATEST(1.1, 2.2, null),
       GREATEST(cast(null as double), null, null)
FROM src tablesample (1 rows);

SELECT GREATEST(101Y, -101S, 100, -100L, null),
       GREATEST(-101Y, 101S, 100, -100L, 0),
       GREATEST(100Y, -100S, 101, -101L, null),
       GREATEST(100Y, -100S, -101, 101L, 0)
FROM src tablesample (1 rows);

SELECT GREATEST(cast(1.1 as float), cast(-1.1 as double), cast(0.5 as decimal)),
       GREATEST(cast(-1.1 as float), cast(1.1 as double), cast(0.5 as decimal)),
       GREATEST(cast(0.1 as float), cast(-0.1 as double), cast(0.5 as decimal)),
       GREATEST(null, cast(-0.1 as double), cast(0.5 as decimal))
FROM src tablesample (1 rows);

SELECT GREATEST(-100Y, -80S, -60, -40L, cast(-20 as float), cast(0 as double), cast(0.5 as decimal)),
       GREATEST(100Y, 80S, 60, 40L, cast(20 as float), cast(0 as double), cast(-0.5 as decimal)),
       GREATEST(100Y, 80S, 60, 40L, null, cast(0 as double), cast(-0.5 as decimal))
FROM src tablesample (1 rows);

SELECT GREATEST(10L, 'a', date('2001-01-28'))
FROM src tablesample (1 rows);SELECT GREATEST(array('a', 'b'), '2.0') FROM src LIMIT 1;
SELECT GREATEST(1) FROM src LIMIT 1;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION hash;
DESCRIBE FUNCTION EXTENDED hash;

EXPLAIN
SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),
       hash(3), hash(CAST('123456789012' AS BIGINT)),
       hash(CAST(1.25 AS FLOAT)), hash(CAST(16.0 AS DOUBLE)),
       hash('400'), hash('abc'), hash(TRUE), hash(FALSE),
       hash(1, 2, 3)
FROM src tablesample (1 rows);

SELECT hash(CAST(1 AS TINYINT)), hash(CAST(2 AS SMALLINT)),
       hash(3), hash(CAST('123456789012' AS BIGINT)),
       hash(CAST(1.25 AS FLOAT)), hash(CAST(16.0 AS DOUBLE)),
       hash('400'), hash('abc'), hash(TRUE), hash(FALSE),
       hash(1, 2, 3)
FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION hex;
DESCRIBE FUNCTION EXTENDED hex;

-- If the argument is a string, hex should return a string containing two hex
-- digits for every character in the input.
SELECT
  hex('Facebook'),
  hex('\0'),
  hex('qwertyuiopasdfghjkl')
FROM src tablesample (1 rows);

-- If the argument is a number, hex should convert it to hexadecimal.
SELECT
  hex(1),
  hex(0),
  hex(4207849477)
FROM src tablesample (1 rows);

-- Negative numbers should be treated as two's complement (64 bit).
SELECT hex(-5) FROM src tablesample (1 rows);
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION hour;
DESCRIBE FUNCTION EXTENDED hour;

EXPLAIN
SELECT hour('2009-08-07 13:14:15'), hour('13:14:15'), hour('2009-08-07')
FROM src WHERE key = 86;

SELECT hour('2009-08-07 13:14:15'), hour('13:14:15'), hour('2009-08-07')
FROM src WHERE key = 86;


SELECT hour(cast('2009-08-07 13:14:15'  as timestamp))
FROM src WHERE key=86;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION if;
DESCRIBE FUNCTION EXTENDED if;

EXPLAIN
SELECT IF(TRUE, 1, 2) AS COL1,
       IF(FALSE, CAST(NULL AS STRING), CAST(1 AS STRING)) AS COL2,
       IF(1=1, IF(2=2, 1, 2), IF(3=3, 3, 4)) AS COL3,
       IF(2=2, 1, NULL) AS COL4,
       IF(2=2, NULL, 1) AS COL5,
       IF(IF(TRUE, NULL, FALSE), 1, 2) AS COL6
FROM src tablesample (1 rows);


SELECT IF(TRUE, 1, 2) AS COL1,
       IF(FALSE, CAST(NULL AS STRING), CAST(1 AS STRING)) AS COL2,
       IF(1=1, IF(2=2, 1, 2), IF(3=3, 3, 4)) AS COL3,
       IF(2=2, 1, NULL) AS COL4,
       IF(2=2, NULL, 1) AS COL5,
       IF(IF(TRUE, NULL, FALSE), 1, 2) AS COL6
FROM src tablesample (1 rows);

-- Type conversions
EXPLAIN
SELECT IF(TRUE, CAST(128 AS SMALLINT), CAST(1 AS TINYINT)) AS COL1,
       IF(FALSE, 1, 1.1) AS COL2,
       IF(FALSE, 1, 'ABC') AS COL3,
       IF(FALSE, 'ABC', 12.3) AS COL4
FROM src tablesample (1 rows);

SELECT IF(TRUE, CAST(128 AS SMALLINT), CAST(1 AS TINYINT)) AS COL1,
       IF(FALSE, 1, 1.1) AS COL2,
       IF(FALSE, 1, 'ABC') AS COL3,
       IF(FALSE, 'ABC', 12.3) AS COL4
FROM src tablesample (1 rows);
SELECT IF('STRING', 1, 1) FROM src;
SELECT IF(TRUE, 1) FROM src;
set hive.fetch.task.conversion=more;

SELECT 1 IN (1, 2, 3),
       4 IN (1, 2, 3),
       array(1,2,3) IN (array(1,2,3)),
       "bee" IN("aee", "bee", "cee", 1),
       "dee" IN("aee", "bee", "cee"),
       1 = 1 IN(true, false),
       true IN (true, false) = true,
       1 IN (1, 2, 3) OR false IN(false),
       NULL IN (1, 2, 3),
       4 IN (1, 2, 3, NULL),
       (1+3) IN (5, 6, (1+2) + 1) FROM src tablesample (1 rows);

SELECT key FROM src WHERE key IN ("238", 86);SELECT 3 IN (array(1,2,3)) FROM src;set hive.support.quoted.identifiers=none;
DESCRIBE FUNCTION `index`;
DESCRIBE FUNCTION EXTENDED `index`;
DESCRIBE FUNCTION initcap;
DESCRIBE FUNCTION EXTENDED initcap;
set hive.fetch.task.conversion=more;

describe function inline;

explain SELECT inline(
  ARRAY(
    STRUCT (1,'dude!'),
    STRUCT (2,'Wheres'),
    STRUCT (3,'my car?')
  )
)  as (id, text) FROM SRC limit 2;

SELECT inline(
  ARRAY(
    STRUCT (1,'dude!'),
    STRUCT (2,'Wheres'),
    STRUCT (3,'my car?')
  )
)  as (id, text) FROM SRC limit 2;

-- HIVE-3475 INLINE UDTF doesn't convert types properly
select * from (SELECT
  ARRAY(
    STRUCT (1,'dude!'),
    STRUCT (2,'Wheres'),
    STRUCT (3,'my car?')
  ) as value FROM SRC limit 1) input
 LATERAL VIEW inline(value) myTable AS id, text;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION instr;
DESCRIBE FUNCTION EXTENDED instr;

EXPLAIN
SELECT instr('abcd', 'abc'),
       instr('abcabc', 'ccc'),
       instr(123, '23'),
       instr(123, 23),
       instr(TRUE, 1),
       instr(FALSE, 1),
       instr('12345', CAST('2' AS TINYINT)),
       instr(CAST('12345' AS SMALLINT), '34'),
       instr(CAST('123456789012' AS BIGINT), '456'),
       instr(CAST(1.25 AS FLOAT), '.25'),
       instr(CAST(16.0 AS DOUBLE), '.0'),
       instr(null, 'abc'),
       instr('abcd', null)
FROM src tablesample (1 rows);

SELECT instr('abcd', 'abc'),
       instr('abcabc', 'ccc'),
       instr(123, '23'),
       instr(123, 23),
       instr(TRUE, 1),
       instr(FALSE, 1),
       instr('12345', CAST('2' AS TINYINT)),
       instr(CAST('12345' AS SMALLINT), '34'),
       instr(CAST('123456789012' AS BIGINT), '456'),
       instr(CAST(1.25 AS FLOAT), '.25'),
       instr(CAST(16.0 AS DOUBLE), '.0'),
       instr(null, 'abc'),
       instr('abcd', null)
FROM src tablesample (1 rows);
SELECT instr('abcd') FROM src;
FROM src_thrift
SELECT instr('abcd', src_thrift.lintstring)
WHERE src_thrift.lintstring IS NOT NULL;
DESCRIBE FUNCTION int;
DESCRIBE FUNCTION EXTENDED int;
select default.nonexistfunc() from src;
DESCRIBE FUNCTION in_file;

CREATE TABLE value_src (str_val char(3), ch_val STRING, vch_val varchar(10),
                        str_val_neg char(3), ch_val_neg STRING, vch_val_neg varchar(10))
       ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

LOAD DATA LOCAL INPATH '../../data/files/in_file.dat' INTO TABLE value_src;

EXPLAIN
SELECT in_file(str_val, "../../data/files/test2.dat"),
       in_file(ch_val, "../../data/files/test2.dat"),
       in_file(vch_val, "../../data/files/test2.dat"),
       in_file(str_val_neg, "../../data/files/test2.dat"),
       in_file(ch_val_neg, "../../data/files/test2.dat"),
       in_file(vch_val_neg, "../../data/files/test2.dat"),
       in_file("303", "../../data/files/test2.dat"),
       in_file("304", "../../data/files/test2.dat"),
       in_file(CAST(NULL AS STRING), "../../data/files/test2.dat")
FROM value_src LIMIT 1;

SELECT in_file(str_val, "../../data/files/test2.dat"),
       in_file(ch_val, "../../data/files/test2.dat"),
       in_file(vch_val, "../../data/files/test2.dat"),
       in_file(str_val_neg, "../../data/files/test2.dat"),
       in_file(ch_val_neg, "../../data/files/test2.dat"),
       in_file(vch_val_neg, "../../data/files/test2.dat"),
       in_file("303", "../../data/files/test2.dat"),
       in_file("304", "../../data/files/test2.dat"),
       in_file(CAST(NULL AS STRING), "../../data/files/test2.dat")
FROM value_src LIMIT 1;DESCRIBE FUNCTION isnotnull;
DESCRIBE FUNCTION EXTENDED isnotnull;
DESCRIBE FUNCTION isnull;
DESCRIBE FUNCTION EXTENDED isnull;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION isnull;
DESCRIBE FUNCTION EXTENDED isnull;

DESCRIBE FUNCTION isnotnull;
DESCRIBE FUNCTION EXTENDED isnotnull;


EXPLAIN
SELECT NULL IS NULL,
       1 IS NOT NULL,
       'my string' IS NOT NULL
FROM src
WHERE true IS NOT NULL LIMIT 1;


SELECT NULL IS NULL,
       1 IS NOT NULL,
       'my string' IS NOT NULL
FROM src
WHERE true IS NOT NULL LIMIT 1;


EXPLAIN
FROM src_thrift
SELECT src_thrift.lint IS NOT NULL,
       src_thrift.lintstring IS NOT NULL,
       src_thrift.mstringstring IS NOT NULL
WHERE  src_thrift.lint IS NOT NULL
       AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1;


FROM src_thrift
SELECT src_thrift.lint IS NOT NULL,
       src_thrift.lintstring IS NOT NULL,
       src_thrift.mstringstring IS NOT NULL
WHERE  src_thrift.lint IS NOT NULL
       AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION java_method;
DESCRIBE FUNCTION EXTENDED java_method;

-- java_method() is a synonym for reflect()

EXPLAIN EXTENDED
SELECT java_method("java.lang.String", "valueOf", 1),
       java_method("java.lang.String", "isEmpty"),
       java_method("java.lang.Math", "max", 2, 3),
       java_method("java.lang.Math", "min", 2, 3),
       java_method("java.lang.Math", "round", 2.5),
       round(java_method("java.lang.Math", "exp", 1.0), 6),
       java_method("java.lang.Math", "floor", 1.9)
FROM src tablesample (1 rows);


SELECT java_method("java.lang.String", "valueOf", 1),
       java_method("java.lang.String", "isEmpty"),
       java_method("java.lang.Math", "max", 2, 3),
       java_method("java.lang.Math", "min", 2, 3),
       java_method("java.lang.Math", "round", 2.5),
       round(java_method("java.lang.Math", "exp", 1.0), 6),
       java_method("java.lang.Math", "floor", 1.9)
FROM src tablesample (1 rows);

DESCRIBE FUNCTION last_day;
DESCRIBE FUNCTION EXTENDED last_day;

explain select last_day('2015-02-05');

select
last_day('2014-01-01'),
last_day('2014-01-14'),
last_day('2014-01-31'),
last_day('2014-02-02'),
last_day('2014-02-28'),
last_day('2016-02-03'),
last_day('2016-02-28'),
last_day('2016-02-29'),
last_day('2014-01-34'),
last_day(cast(null as string)),
last_day('01/29/2014');

select
last_day('2014-01-01 10:30:45'),
last_day('2014-01-14 10:30:45'),
last_day('2014-01-31 10:30:45'),
last_day('2014-02-02 10:30:45'),
last_day('2014-02-28 10:30:45'),
last_day('2016-02-03 10:30:45'),
last_day('2016-02-28 10:30:45'),
last_day('2016-02-29 10:30:45'),
last_day('2014-01-34 10:30:45'),
last_day(cast(null as string)),
last_day('01/29/2014 10:30:45');

select
last_day(cast('2014-01-01' as date)),
last_day(cast('2014-01-14' as date)),
last_day(cast('2014-01-31' as date)),
last_day(cast('2014-02-02' as date)),
last_day(cast('2014-02-28' as date)),
last_day(cast('2016-02-03' as date)),
last_day(cast('2016-02-28' as date)),
last_day(cast('2016-02-29' as date)),
last_day(cast(null as date));

select
last_day(cast('2014-01-01 10:30:45' as timestamp)),
last_day(cast('2014-01-14 10:30:45' as timestamp)),
last_day(cast('2014-01-31 10:30:45' as timestamp)),
last_day(cast('2014-02-02 10:30:45' as timestamp)),
last_day(cast('2014-02-28 10:30:45' as timestamp)),
last_day(cast('2016-02-03 10:30:45' as timestamp)),
last_day(cast('2016-02-28 10:30:45' as timestamp)),
last_day(cast('2016-02-29 10:30:45' as timestamp)),
last_day(cast(null as timestamp));select last_day(1423199465);select last_day(map('2014-01-14','test'));DESCRIBE FUNCTION lcase;
DESCRIBE FUNCTION EXTENDED lcase;

set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION least;
DESCRIBE FUNCTION EXTENDED least;

EXPLAIN
SELECT LEAST('a', 'b', 'c'),
       LEAST('C', 'a', 'B'),
       LEAST('AAA', 'AaA', 'AAa'),
       LEAST('A', 'AA', 'AAA'),
       LEAST('11', '13', '12'),
       LEAST('11', '2', '12'),
       LEAST('01', '03', '02'),
       LEAST('01', '1', '02'),
       LEAST(null, 'b', 'c' ),
       LEAST('a', null, 'c'),
       LEAST('a', 'b', null ),
       LEAST('a', null, null),
       LEAST(null, 'b', null),
       LEAST(cast(null as string), null, null)
FROM src tablesample (1 rows);

SELECT LEAST('a', 'b', 'c'),
       LEAST('C', 'a', 'B'),
       LEAST('AAA', 'AaA', 'AAa'),
       LEAST('A', 'AA', 'AAA'),
       LEAST('11', '13', '12'),
       LEAST('11', '2', '12'),
       LEAST('01', '03', '02'),
       LEAST('01', '1', '02'),
       LEAST(null, 'b', 'c' ),
       LEAST('a', null, 'c'),
       LEAST('a', 'b', null ),
       LEAST('a', null, null),
       LEAST(null, 'b', null),
       LEAST(cast(null as string), null, null)
FROM src tablesample (1 rows);

SELECT LEAST(11, 13, 12),
       LEAST(1, 13, 2),
       LEAST(-11, -13, -12),
       LEAST(1, -13, 2),
       LEAST(null, 1, 2),
       LEAST(1, null, 2),
       LEAST(1, 2, null),
       LEAST(cast(null as int), null, null)
FROM src tablesample (1 rows);

SELECT LEAST(11.4, 13.5, 12.2),
       LEAST(1.0, 13.2, 2.0),
       LEAST(-11.4, -13.1, -12.2),
       LEAST(1.0, -13.3, 2.2),
       LEAST(null, 1.1, 2.2),
       LEAST(1.1, null, 2.2),
       LEAST(1.1, 2.2, null),
       LEAST(cast(null as double), null, null)
FROM src tablesample (1 rows);

SELECT LEAST(101Y, -101S, 100, -100L, null),
       LEAST(-101Y, 101S, 100, -100L, 0),
       LEAST(100Y, -100S, 101, -101L, null),
       LEAST(100Y, -100S, -101, 101L, 0)
FROM src tablesample (1 rows);

SELECT LEAST(cast(1.1 as float), cast(-1.1 as double), cast(0.5 as decimal)),
       LEAST(cast(-1.1 as float), cast(1.1 as double), cast(0.5 as decimal)),
       LEAST(cast(0.1 as float), cast(-0.1 as double), cast(0.5 as decimal)),
       LEAST(null, cast(-0.1 as double), cast(0.5 as decimal))
FROM src tablesample (1 rows);

SELECT LEAST(-100Y, -80S, -60, -40L, cast(-20 as float), cast(0 as double), cast(0.5 as decimal)),
       LEAST(100Y, 80S, 60, 40L, cast(20 as float), cast(0 as double), cast(-0.5 as decimal)),
       LEAST(100Y, 80S, 60, 40L, null, cast(0 as double), cast(-0.5 as decimal))
FROM src tablesample (1 rows);

SELECT LEAST(10L, 'a', date('2001-01-28'))
FROM src tablesample (1 rows);set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION length;
DESCRIBE FUNCTION EXTENDED length;

CREATE TABLE dest1(len INT);
EXPLAIN FROM src1 INSERT OVERWRITE TABLE dest1 SELECT length(src1.value);
FROM src1 INSERT OVERWRITE TABLE dest1 SELECT length(src1.value);
SELECT dest1.* FROM dest1;
DROP TABLE dest1;

-- Test with non-ascii characters.
CREATE TABLE dest1(name STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv4.txt' INTO TABLE dest1;
EXPLAIN SELECT length(dest1.name) FROM dest1;
SELECT length(dest1.name) FROM dest1;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION <;
DESCRIBE FUNCTION EXTENDED <;

SELECT true<false, false<true, false<false, true<true FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION <=;
DESCRIBE FUNCTION EXTENDED <=;

SELECT true<=false, false<=true, false<=false, true<=true FROM src tablesample (1 rows);
DESCRIBE FUNCTION levenshtein;
DESC FUNCTION EXTENDED levenshtein;

explain select levenshtein('Test String1', 'Test String2');

select
levenshtein('kitten', 'sitting'),
levenshtein('Test String1', 'Test String2'),
levenshtein('Test String1', 'test String2'),
levenshtein('', 'Test String2'),
levenshtein('Test String1', ''),
levenshtein(cast(null as string), 'Test String2'),
levenshtein('Test String1', cast(null as string)),
levenshtein(cast(null as string), cast(null as string));set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION like;
DESCRIBE FUNCTION EXTENDED like;

EXPLAIN
SELECT '_%_' LIKE '%\_\%\_%', '__' LIKE '%\_\%\_%', '%%_%_' LIKE '%\_\%\_%', '%_%_%' LIKE '%\%\_\%',
  '_%_' LIKE '\%\_%', '%__' LIKE '__\%%', '_%' LIKE '\_\%\_\%%', '_%' LIKE '\_\%_%',
  '%_' LIKE '\%\_', 'ab' LIKE '\%\_', 'ab' LIKE '_a%', 'ab' LIKE 'a','ab' LIKE '','' LIKE ''
FROM src WHERE src.key = 86;

SELECT '_%_' LIKE '%\_\%\_%', '__' LIKE '%\_\%\_%', '%%_%_' LIKE '%\_\%\_%', '%_%_%' LIKE '%\%\_\%',
  '_%_' LIKE '\%\_%', '%__' LIKE '__\%%', '_%' LIKE '\_\%\_\%%', '_%' LIKE '\_\%_%',
  '%_' LIKE '\%\_', 'ab' LIKE '\%\_', 'ab' LIKE '_a%', 'ab' LIKE 'a','ab' LIKE '','' LIKE ''
FROM src WHERE src.key = 86;


SELECT '1+2' LIKE '_+_',
       '1+2' LIKE '1+_',
       '112' LIKE '1+_',
       '|||' LIKE '|_|',
       '+++' LIKE '1+_'
FROM src tablesample (1 rows);
DESCRIBE FUNCTION ln;
DESCRIBE FUNCTION EXTENDED ln;
create function lookup as 'org.apache.hadoop.hive.ql.udf.UDFFileLookup' using file '../../data/files/sales.txt';
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION locate;
DESCRIBE FUNCTION EXTENDED locate;

EXPLAIN
SELECT locate('abc', 'abcd'),
       locate('ccc', 'abcabc'),
       locate('23', 123),
       locate(23, 123),
       locate('abc', 'abcabc', 2),
       locate('abc', 'abcabc', '2'),
       locate(1, TRUE),
       locate(1, FALSE),
       locate(CAST('2' AS TINYINT), '12345'),
       locate('34', CAST('12345' AS SMALLINT)),
       locate('456', CAST('123456789012' AS BIGINT)),
       locate('.25', CAST(1.25 AS FLOAT)),
       locate('.0', CAST(16.0 AS DOUBLE)),
       locate(null, 'abc'),
       locate('abc', null),
       locate('abc', 'abcd', null),
       locate('abc', 'abcd', 'invalid number')
FROM src tablesample (1 rows);

SELECT locate('abc', 'abcd'),
       locate('ccc', 'abcabc'),
       locate('23', 123),
       locate(23, 123),
       locate('abc', 'abcabc', 2),
       locate('abc', 'abcabc', '2'),
       locate(1, TRUE),
       locate(1, FALSE),
       locate(CAST('2' AS TINYINT), '12345'),
       locate('34', CAST('12345' AS SMALLINT)),
       locate('456', CAST('123456789012' AS BIGINT)),
       locate('.25', CAST(1.25 AS FLOAT)),
       locate('.0', CAST(16.0 AS DOUBLE)),
       locate(null, 'abc'),
       locate('abc', null),
       locate('abc', 'abcd', null),
       locate('abc', 'abcd', 'invalid number')
FROM src tablesample (1 rows);
SELECT locate('a', 'b', 1, 2) FROM src;
FROM src_thrift
SELECT locate('abcd', src_thrift.lintstring)
WHERE src_thrift.lintstring IS NOT NULL;
DESCRIBE FUNCTION log;
DESCRIBE FUNCTION EXTENDED log;
DESCRIBE FUNCTION log10;
DESCRIBE FUNCTION EXTENDED log10;
DESCRIBE FUNCTION log2;
DESCRIBE FUNCTION EXTENDED log2;
set hive.fetch.task.conversion=more;

EXPLAIN
CREATE TEMPORARY FUNCTION test_udf_get_java_boolean AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean';

CREATE TEMPORARY FUNCTION test_udf_get_java_boolean AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean';

select 1 from src where test_udf_get_java_boolean("false") and True limit 1;
select 1 from src where test_udf_get_java_boolean("true") and True limit 1;
select 1 from src where True and test_udf_get_java_boolean("false") limit 1;
select 1 from src where False and test_udf_get_java_boolean("false") limit 1;
select 1 from src where test_udf_get_java_boolean("true") and test_udf_get_java_boolean("true") limit 1;
select 1 from src where test_udf_get_java_boolean("true") and test_udf_get_java_boolean("false") limit 1;
select 1 from src where test_udf_get_java_boolean("false") and test_udf_get_java_boolean("true") limit 1;
select 1 from src where test_udf_get_java_boolean("false") and test_udf_get_java_boolean("false") limit 1;

select 1 from src where test_udf_get_java_boolean("false") or True limit 1;
select 1 from src where test_udf_get_java_boolean("true") or True limit 1;
select 1 from src where True or test_udf_get_java_boolean("false") limit 1;
select 1 from src where False or test_udf_get_java_boolean("false") limit 1;
select 1 from src where test_udf_get_java_boolean("true") or test_udf_get_java_boolean("true") limit 1;
select 1 from src where test_udf_get_java_boolean("true") or test_udf_get_java_boolean("false") limit 1;
select 1 from src where test_udf_get_java_boolean("false") or test_udf_get_java_boolean("true") limit 1;
select 1 from src where test_udf_get_java_boolean("false") or test_udf_get_java_boolean("false") limit 1;

select 1 from src where not(test_udf_get_java_boolean("false")) limit 1;
select 1 from src where not(test_udf_get_java_boolean("true")) limit 1;


DROP TEMPORARY FUNCTION test_udf_get_java_boolean;
set hive.mapred.mode=nonstrict;
DESCRIBE FUNCTION lower;
DESCRIBE FUNCTION EXTENDED lower;

EXPLAIN
SELECT lower('AbC 123'), upper('AbC 123') FROM src WHERE key = 86;

SELECT lower('AbC 123'), upper('AbC 123') FROM src WHERE key = 86;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION lpad;
DESCRIBE FUNCTION EXTENDED lpad;

EXPLAIN SELECT
  lpad('hi', 1, '?'),
  lpad('hi', 5, '.'),
  lpad('hi', 6, '123')
FROM src tablesample (1 rows);

SELECT
  lpad('hi', 1, '?'),
  lpad('hi', 5, '.'),
  lpad('hi', 6, '123')
FROM src tablesample (1 rows);
DESCRIBE FUNCTION ltrim;
DESCRIBE FUNCTION EXTENDED ltrim;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION map;
DESCRIBE FUNCTION EXTENDED map;

EXPLAIN SELECT map(), map(1, "a", 2, "b", 3, "c"), map(1, 2, "a", "b"),
map(1, "a", 2, "b", 3, "c")[2],  map(1, 2, "a", "b")["a"], map(1, array("a"))[1][0] FROM src tablesample (1 rows);

SELECT map(), map(1, "a", 2, "b", 3, "c"), map(1, 2, "a", "b"),
map(1, "a", 2, "b", 3, "c")[2],  map(1, 2, "a", "b")["a"], map(1, array("a"))[1][0] FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

use default;
-- Test map_keys() UDF

DESCRIBE FUNCTION map_keys;
DESCRIBE FUNCTION EXTENDED map_keys;

-- Evaluate function against INT valued keys
SELECT map_keys(map(1, "a", 2, "b", 3, "c")) FROM src tablesample (1 rows);

-- Evaluate function against STRING valued keys
SELECT map_keys(map("a", 1, "b", 2, "c", 3)) FROM src tablesample (1 rows);
SELECT map_keys(map("a", "1"), map("b", "2")) FROM src LIMIT 1;
SELECT map_keys(array(1, 2, 3)) FROM src LIMIT 1;
set hive.fetch.task.conversion=more;

use default;
-- Test map_values() UDF

DESCRIBE FUNCTION map_values;
DESCRIBE FUNCTION EXTENDED map_values;

-- Evaluate function against STRING valued values
SELECT map_values(map(1, "a", 2, "b", 3, "c")) FROM src tablesample (1 rows);

-- Evaluate function against INT valued keys
SELECT map_values(map("a", 1, "b", 2, "c", 3)) FROM src tablesample (1 rows);
SELECT map_values(map("a", "1"), map("b", "2")) FROM src LIMIT 1;
SELECT map_values(array(1, 2, 3, 4)) FROM src LIMIT 1;
DESCRIBE FUNCTION max;
DESCRIBE FUNCTION EXTENDED max;


set hive.map.aggr = false;
set hive.groupby.skewindata = false;

SELECT max(struct(CAST(key as INT), value)),
       max(struct(key, value))
FROM src;


set hive.map.aggr = true;
set hive.groupby.skewindata = false;

SELECT max(struct(CAST(key as INT), value)),
       max(struct(key, value))
FROM src;


set hive.map.aggr = false;
set hive.groupby.skewindata = true;

SELECT max(struct(CAST(key as INT), value)),
       max(struct(key, value))
FROM src;


set hive.map.aggr = true;
set hive.groupby.skewindata = true;

SELECT max(struct(CAST(key as INT), value)),
       max(struct(key, value))
FROM src;
SELECT max(map("key", key, "value", value))
FROM src;
DESCRIBE FUNCTION md5;
DESC FUNCTION EXTENDED md5;

explain select md5('ABC');

select
md5('ABC'),
md5(''),
md5(binary('ABC')),
md5(binary('')),
md5(cast(null as string)),
md5(cast(null as binary)),
md5(null);
DESCRIBE FUNCTION min;
DESCRIBE FUNCTION EXTENDED min;


set hive.map.aggr = false;
set hive.groupby.skewindata = false;

SELECT min(struct(CAST(key as INT), value)),
       min(struct(key, value))
FROM src;


set hive.map.aggr = true;
set hive.groupby.skewindata = false;

SELECT min(struct(CAST(key as INT), value)),
       min(struct(key, value))
FROM src;


set hive.map.aggr = false;
set hive.groupby.skewindata = true;

SELECT min(struct(CAST(key as INT), value)),
       min(struct(key, value))
FROM src;


set hive.map.aggr = true;
set hive.groupby.skewindata = true;

SELECT min(struct(CAST(key as INT), value)),
       min(struct(key, value))
FROM src;
SELECT min(map("key", key, "value", value))
FROM src;
set hive.mapred.mode=nonstrict;
DESCRIBE FUNCTION minute;
DESCRIBE FUNCTION EXTENDED minute;

EXPLAIN
SELECT minute('2009-08-07 13:14:15'), minute('13:14:15'), minute('2009-08-07')
FROM src WHERE key = 86;

SELECT minute('2009-08-07 13:14:15'), minute('13:14:15'), minute('2009-08-07')
FROM src WHERE key = 86;
DESCRIBE FUNCTION %;
DESCRIBE FUNCTION EXTENDED %;
DESCRIBE FUNCTION minute;
DESCRIBE FUNCTION EXTENDED minute;
describe function months_between;
desc function extended months_between;

--test string format
explain select months_between('1995-02-02', '1995-01-01');

select
  months_between('1995-02-02', '1995-01-01'),
  months_between('2003-07-17', '2005-07-06'),
  months_between('2001-06-30', '2000-05-31'),
  months_between('2000-06-01', '2004-07-01'),
  months_between('2002-02-28', '2002-03-01'),
  months_between('2002-02-31', '2002-03-01'),
  months_between('2012-02-29', '2012-03-01'),
  months_between('2012-02-31', '2012-03-01'),
  months_between('1976-01-01 00:00:00', '1975-12-31 23:59:59'),
  months_between('1976-01-01', '1975-12-31 23:59:59'),
  months_between('1997-02-28 10:30:00', '1996-10-30'),
  -- if both are last day of the month then time part should be ignored
  months_between('2002-03-31', '2002-02-28'),
  months_between('2002-03-31', '2002-02-28 10:30:00'),
  months_between('2002-03-31 10:30:00', '2002-02-28'),
  -- if the same day of the month then time part should be ignored
  months_between('2002-03-24', '2002-02-24'),
  months_between('2002-03-24', '2002-02-24 10:30:00'),
  months_between('2002-03-24 10:30:00', '2002-02-24'),
  -- partial time. time part will be skipped
  months_between('1995-02-02 10:39', '1995-01-01'),
  months_between('1995-02-02', '1995-01-01 10:39'),
  -- no leading 0 for month and day should work
  months_between('1995-02-2', '1995-1-01'),
  months_between('1995-2-02', '1995-01-1'),
  -- short year should work
  months_between('495-2-02', '495-01-1'),
  months_between('95-2-02', '95-01-1'),
  months_between('5-2-02', '5-01-1');

--test timestamp format
select
  months_between(cast('1995-02-02 00:00:00' as timestamp), cast('1995-01-01 00:00:00' as timestamp)),
  months_between(cast('2003-07-17 00:00:00' as timestamp), cast('2005-07-06 00:00:00' as timestamp)),
  months_between(cast('2001-06-30 00:00:00' as timestamp), cast('2000-05-31 00:00:00' as timestamp)),
  months_between(cast('2000-06-01 00:00:00' as timestamp), cast('2004-07-01 00:00:00' as timestamp)),
  months_between(cast('2002-02-28 00:00:00' as timestamp), cast('2002-03-01 00:00:00' as timestamp)),
  months_between(cast('2002-02-31 00:00:00' as timestamp), cast('2002-03-01 00:00:00' as timestamp)),
  months_between(cast('2012-02-29 00:00:00' as timestamp), cast('2012-03-01 00:00:00' as timestamp)),
  months_between(cast('2012-02-31 00:00:00' as timestamp), cast('2012-03-01 00:00:00' as timestamp)),
  months_between(cast('1976-01-01 00:00:00' as timestamp), cast('1975-12-31 23:59:59' as timestamp)),
  months_between(cast('1976-01-01' as date), cast('1975-12-31 23:59:59' as timestamp)),
  months_between(cast('1997-02-28 10:30:00' as timestamp), cast('1996-10-30' as date)),
  -- if both are last day of the month then time part should be ignored
  months_between(cast('2002-03-31 00:00:00' as timestamp), cast('2002-02-28 00:00:00' as timestamp)),
  months_between(cast('2002-03-31 00:00:00' as timestamp), cast('2002-02-28 10:30:00' as timestamp)),
  months_between(cast('2002-03-31 10:30:00' as timestamp), cast('2002-02-28 00:00:00' as timestamp)),
  -- if the same day of the month then time part should be ignored
  months_between(cast('2002-03-24 00:00:00' as timestamp), cast('2002-02-24 00:00:00' as timestamp)),
  months_between(cast('2002-03-24 00:00:00' as timestamp), cast('2002-02-24 10:30:00' as timestamp)),
  months_between(cast('2002-03-24 10:30:00' as timestamp), cast('2002-02-24 00:00:00' as timestamp));

--test date format
select
  months_between(cast('1995-02-02' as date), cast('1995-01-01' as date)),
  months_between(cast('2003-07-17' as date), cast('2005-07-06' as date)),
  months_between(cast('2001-06-30' as date), cast('2000-05-31' as date)),
  months_between(cast('2000-06-01' as date), cast('2004-07-01' as date)),
  months_between(cast('2002-02-28' as date), cast('2002-03-01' as date)),
  months_between(cast('2002-02-31' as date), cast('2002-03-01' as date)),
  months_between(cast('2012-02-29' as date), cast('2012-03-01' as date)),
  months_between(cast('2012-02-31' as date), cast('2012-03-01' as date));

--test misc with null
select
  months_between(cast(null as string), '2012-03-01'),
  months_between('2012-02-31', cast(null as timestamp)),
  months_between(cast(null as timestamp), cast(null as date)),
  months_between(cast(null as string), cast('2012-03-01 00:00:00' as timestamp)),
  months_between(cast('2012-02-31 00:00:00' as timestamp), cast(null as string)),
  months_between(cast(null as timestamp), cast('2012-03-01' as string)),
  months_between(cast('2012-02-31' as date), cast(null as string)),
  months_between('2012-02-10', cast(null as string)),
  months_between(cast(null as string), '2012-02-10'),
  months_between(cast(null as string), cast(null as string)),
  months_between('2012-02-10', cast(null as timestamp)),
  months_between(cast(null as timestamp), '2012-02-10'),
  months_between(cast(null as timestamp), cast(null as timestamp)),
  -- string dates without day should be parsed to null
  months_between('2012-03', '2012-02-24'),
  months_between('2012-03-24', '2012-02');
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION named_struct;
DESCRIBE FUNCTION EXTENDED named_struct;

EXPLAIN
SELECT named_struct("foo", 1, "bar", 2),
       named_struct("foo", 1, "bar", 2).foo FROM src tablesample (1 rows);

SELECT named_struct("foo", 1, "bar", 2),
       named_struct("foo", 1, "bar", 2).foo FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION negative;
DESCRIBE FUNCTION EXTENDED negative;

-- synonym
DESCRIBE FUNCTION -;
DESCRIBE FUNCTION EXTENDED -;

select - null from src tablesample (1 rows);
select - cast(null as int) from src tablesample (1 rows);
select - cast(null as smallint) from src tablesample (1 rows);
select - cast(null as bigint) from src tablesample (1 rows);
select - cast(null as double) from src tablesample (1 rows);
select - cast(null as float) from src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION next_day;
DESCRIBE FUNCTION EXTENDED next_day;

EXPLAIN SELECT next_day('2014-01-14', 'MO')
FROM src tablesample (1 rows);

SELECT next_day('2015-01-11', 'su'),
       next_day('2015-01-11', 'MO'),
       next_day('2015-01-11', 'Tu'),
       next_day('2015-01-11', 'wE'),
       next_day('2015-01-11', 'th'),
       next_day('2015-01-11', 'FR'),
       next_day('2015-01-11', 'Sa')
FROM src tablesample (1 rows);

SELECT next_day('2015-01-17 00:02:30', 'sun'),
       next_day('2015-01-17 00:02:30', 'MON'),
       next_day('2015-01-17 00:02:30', 'Tue'),
       next_day('2015-01-17 00:02:30', 'weD'),
       next_day('2015-01-17 00:02:30', 'tHu'),
       next_day('2015-01-17 00:02:30', 'FrI'),
       next_day('2015-01-17 00:02:30', 'SAt')
FROM src tablesample (1 rows);

SELECT next_day(cast('2015-01-14 14:04:34' as timestamp), 'sunday'),
       next_day(cast('2015-01-14 14:04:34' as timestamp), 'Monday'),
       next_day(cast('2015-01-14 14:04:34' as timestamp), 'Tuesday'),
       next_day(cast('2015-01-14 14:04:34' as timestamp), 'wednesday'),
       next_day(cast('2015-01-14 14:04:34' as timestamp), 'thursDAY'),
       next_day(cast('2015-01-14 14:04:34' as timestamp), 'FRIDAY'),
       next_day(cast('2015-01-14 14:04:34' as timestamp), 'SATurday')
FROM src tablesample (1 rows);

SELECT next_day(cast(null as string), 'MO'),
       next_day(cast(null as timestamp), 'MO'),
       next_day('2015-01-11', cast(null as string)),
       next_day(cast(null as string), cast(null as string)),
       next_day(cast(null as timestamp), cast(null as string))
FROM src tablesample (1 rows);

SELECT next_day('2015-02-02', 'VT'),
       next_day('2015-02-30', 'WE'),
       next_day('02/15/2015', 'WE')
FROM src tablesample (1 rows);
SELECT NEXT_DAY(145622345, 'TU');SELECT NEXT_DAY('2015-01-14', 4);create function lookup as 'org.apache.hadoop.hive.ql.udf.UDFFileLookup' using file 'nonexistent_file.txt';
DESCRIBE FUNCTION not;
DESCRIBE FUNCTION EXTENDED not;

-- synonym
DESCRIBE FUNCTION !;
DESCRIBE FUNCTION EXTENDED !;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION <>;
DESCRIBE FUNCTION EXTENDED <>;

DESCRIBE FUNCTION !=;
DESCRIBE FUNCTION EXTENDED !=;

EXPLAIN
SELECT key, value
FROM src
WHERE key <> '302';

SELECT key, value
FROM src
WHERE key <> '302';

EXPLAIN
SELECT key, value
FROM src
WHERE key != '302';

SELECT key, value
FROM src
WHERE key != '302';
set hive.fetch.task.conversion=more;

SELECT 1 NOT IN (1, 2, 3),
       4 NOT IN (1, 2, 3),
       1 = 2 NOT IN (true, false),
       "abc" NOT LIKE "a%",
       "abc" NOT LIKE "b%",
       "abc" NOT RLIKE "^ab",
       "abc" NOT RLIKE "^bc",
       "abc" NOT REGEXP "^ab",
       "abc" NOT REGEXP "^bc",
       1 IN (1, 2) AND "abc" NOT LIKE "bc%" FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION nvl;
DESCRIBE FUNCTION EXTENDED nvl;

EXPLAIN
SELECT NVL( 1 , 2 ) AS COL1,
       NVL( NULL, 5 ) AS COL2
FROM src tablesample (1 rows);

SELECT NVL( 1 , 2 ) AS COL1,
       NVL( NULL, 5 ) AS COL2
FROM src tablesample (1 rows);

DESCRIBE FUNCTION or;
DESCRIBE FUNCTION EXTENDED or;
set hive.mapred.mode=nonstrict;
DESCRIBE FUNCTION parse_url;
DESCRIBE FUNCTION EXTENDED parse_url;

EXPLAIN
SELECT parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST'),
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'PATH'),
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'REF') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k2') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k3') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'FILE') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'PROTOCOL') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'USERINFO') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'AUTHORITY')
  FROM src WHERE key = 86;

SELECT parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'HOST'),
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'PATH'),
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'REF') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k2') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k1') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'QUERY', 'k3') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'FILE') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'PROTOCOL') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'USERINFO') ,
parse_url('http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1', 'AUTHORITY')
  FROM src WHERE key = 86;DESCRIBE FUNCTION percentile;
DESCRIBE FUNCTION EXTENDED percentile;


set hive.map.aggr = false;
set hive.groupby.skewindata = false;

-- SORT_QUERY_RESULTS

SELECT CAST(key AS INT) DIV 10,
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


set hive.map.aggr = true;
set hive.groupby.skewindata = false;

SELECT CAST(key AS INT) DIV 10,
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;



set hive.map.aggr = false;
set hive.groupby.skewindata = true;

SELECT CAST(key AS INT) DIV 10,
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


set hive.map.aggr = true;
set hive.groupby.skewindata = true;

SELECT CAST(key AS INT) DIV 10,
       percentile(CAST(substr(value, 5) AS INT), 0.0),
       percentile(CAST(substr(value, 5) AS INT), 0.5),
       percentile(CAST(substr(value, 5) AS INT), 1.0),
       percentile(CAST(substr(value, 5) AS INT), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


set hive.map.aggr = true;
set hive.groupby.skewindata = false;

-- test null handling
SELECT CAST(key AS INT) DIV 10,
       percentile(NULL, 0.0),
       percentile(NULL, array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;


-- test empty array handling
SELECT CAST(key AS INT) DIV 10,
       percentile(IF(CAST(key AS INT) DIV 10 < 5, 1, NULL), 0.5),
       percentile(IF(CAST(key AS INT) DIV 10 < 5, 1, NULL), array(0.0, 0.5, 0.99, 1.0))
FROM src
GROUP BY CAST(key AS INT) DIV 10;

select percentile(cast(key as bigint), 0.5) from src where false;

-- test where percentile list is empty
select percentile(cast(key as bigint), array()) from src where false;
set hive.fetch.task.conversion=more;

explain
select PI() FROM src tablesample (1 rows);

select PI() FROM src tablesample (1 rows);

DESCRIBE FUNCTION PI;
DESCRIBE FUNCTION EXTENDED PI;
explain
select PI() FROM src tablesample (1 rows);

select PI() FROM src tablesample (1 rows);

DESCRIBE FUNCTION PI;
DESCRIBE FUNCTION EXTENDED PI;set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION pmod;
DESCRIBE FUNCTION EXTENDED pmod;

SELECT pmod(null, null)
FROM src tablesample (1 rows);

SELECT pmod(-100,9), pmod(-50,101), pmod(-1000,29)
FROM src tablesample (1 rows);

SELECT pmod(100,19), pmod(50,125), pmod(300,15)
FROM src tablesample (1 rows);

SELECT pmod(CAST(-100 AS TINYINT),CAST(9 AS TINYINT)), pmod(CAST(-50 AS TINYINT),CAST(101 AS TINYINT)), pmod(CAST(-100 AS TINYINT),CAST(29 AS TINYINT)) FROM src tablesample (1 rows);
SELECT pmod(CAST(-100 AS SMALLINT),CAST(9 AS SMALLINT)), pmod(CAST(-50 AS SMALLINT),CAST(101 AS SMALLINT)), pmod(CAST(-100 AS SMALLINT),CAST(29 AS SMALLINT)) FROM src tablesample (1 rows);
SELECT pmod(CAST(-100 AS BIGINT),CAST(9 AS BIGINT)), pmod(CAST(-50 AS BIGINT),CAST(101 AS BIGINT)), pmod(CAST(-100 AS BIGINT),CAST(29 AS BIGINT)) FROM src tablesample (1 rows);

SELECT pmod(CAST(-100.91 AS FLOAT),CAST(9.8 AS FLOAT)), pmod(CAST(-50.1 AS FLOAT),CAST(101.8 AS FLOAT)), pmod(CAST(-100.91 AS FLOAT),CAST(29.75 AS FLOAT)) FROM src tablesample (1 rows);
SELECT pmod(CAST(-100.91 AS DOUBLE),CAST(9.8 AS DOUBLE)), pmod(CAST(-50.1 AS DOUBLE),CAST(101.8 AS DOUBLE)), pmod(CAST(-100.91 AS DOUBLE),CAST(29.75 AS DOUBLE)) FROM src tablesample (1 rows);
SELECT pmod(CAST(-100.91 AS DECIMAL(5,2)),CAST(9.8 AS DECIMAL(2,1))), pmod(CAST(-50.1 AS DECIMAL(3,1)),CAST(101.8 AS DECIMAL(4,1))), pmod(CAST(-100.91 AS DECIMAL(5,2)),CAST(29.75 AS DECIMAL(4,2))) FROM src tablesample (1 rows);

DESCRIBE FUNCTION positive;
DESCRIBE FUNCTION EXTENDED positive;

-- synonym
DESCRIBE FUNCTION +;
DESCRIBE FUNCTION EXTENDED +;
DESCRIBE FUNCTION pow;
DESCRIBE FUNCTION EXTENDED pow;
DESCRIBE FUNCTION power;
DESCRIBE FUNCTION EXTENDED power;
use default;
-- Test printf() UDF

DESCRIBE FUNCTION printf;
DESCRIBE FUNCTION EXTENDED printf;

set hive.fetch.task.conversion=more;

EXPLAIN
SELECT printf("Hello World %d %s", 100, "days") FROM src tablesample (1 rows);

-- Test Primitive Types
SELECT printf("Hello World %d %s", 100, "days") FROM src tablesample (1 rows);
SELECT printf("All Type Test: %b, %c, %d, %e, %+10.4f, %g, %h, %s, %a", false, 65, 15000, 12.3400, 27183.240051, 2300.41, 50, "corret", 256.125) FROM src tablesample (1 rows);

-- Test NULL Values
SELECT printf("Color %s, String Null: %s, number1 %d, number2 %05d, Integer Null: %d, hex %#x, float %5.2f Double Null: %f\n", "red", NULL, 123456, 89, NULL, 255, 3.14159, NULL) FROM src tablesample (1 rows);

-- Test Timestamp
create table timestamp_udf (t timestamp);
from (select * from src tablesample (1 rows)) s
  insert overwrite table timestamp_udf
    select '2011-05-06 07:08:09.1234567';
select printf("timestamp: %s", t) from timestamp_udf;
drop table timestamp_udf;

-- Test Binary
CREATE TABLE binay_udf(key binary, value int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '9'
STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/string.txt' INTO TABLE binay_udf;
create table dest1 (key binary, value int);
insert overwrite table dest1 select transform(*) using 'cat' as key binary, value int from binay_udf;
select value, printf("format key: %s", key) from dest1;
drop table dest1;
drop table binary_udf;
-- invalid argument length
SELECT printf() FROM src LIMIT 1;
-- invalid argument type
SELECT printf(100) FROM src LIMIT 1;
-- invalid argument type
SELECT printf("Hello World %s", array("invalid", "argument")) FROM src LIMIT 1;
-- invalid argument type
SELECT printf("Hello World %s", array("invalid", "argument")) FROM src LIMIT 1;
create temporary function default.myfunc as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum';
DESCRIBE FUNCTION quarter;
DESC FUNCTION EXTENDED quarter;

explain select quarter('2015-04-24');

-- string date
select
quarter('2014-01-10'),
quarter('2014-02-10'),
quarter('2014-03-31'),
quarter('2014-04-02'),
quarter('2014-05-28'),
quarter('2016-06-03'),
quarter('2016-07-28'),
quarter('2016-08-29'),
quarter('2016-09-29'),
quarter('2016-10-29'),
quarter('2016-11-29'),
quarter('2016-12-29'),
-- wrong date str
quarter('2016-03-35'),
quarter('2014-01-32'),
quarter('01/14/2014'),
-- null string
quarter(cast(null as string)),
-- negative Unix time
quarter('1966-01-01'),
quarter('1966-03-31'),
quarter('1966-04-01'),
quarter('1966-12-31');

-- string timestamp
select
quarter('2014-01-10 00:00:00'),
quarter('2014-02-10 15:23:00'),
quarter('2014-03-31 15:23:00'),
quarter('2014-04-02 15:23:00'),
quarter('2014-05-28 15:23:00'),
quarter('2016-06-03 15:23:00'),
quarter('2016-07-28 15:23:00'),
quarter('2016-08-29 15:23:00'),
quarter('2016-09-29 15:23:00'),
quarter('2016-10-29 15:23:00'),
quarter('2016-11-29 15:23:00'),
quarter('2016-12-29 15:23:00'),
-- wrong date str
quarter('2016-03-35 15:23:00'),
quarter('2014-01-32 15:23:00'),
quarter('01/14/2014 15:23:00'),
-- null VOID type
quarter(null),
-- negative Unix time
quarter('1966-01-01 00:00:00'),
quarter('1966-03-31 23:59:59.999'),
quarter('1966-04-01 00:00:00'),
quarter('1966-12-31 23:59:59.999');

-- date
select
quarter(cast('2014-01-10' as date)),
quarter(cast('2014-02-10' as date)),
quarter(cast('2014-03-31' as date)),
quarter(cast('2014-04-02' as date)),
quarter(cast('2014-05-28' as date)),
quarter(cast('2016-06-03' as date)),
quarter(cast('2016-07-28' as date)),
quarter(cast('2016-08-29' as date)),
quarter(cast('2016-09-29' as date)),
quarter(cast('2016-10-29' as date)),
quarter(cast('2016-11-29' as date)),
quarter(cast('2016-12-29' as date)),
-- null date
quarter(cast(null as date)),
-- negative Unix time
quarter(cast('1966-01-01' as date)),
quarter(cast('1966-03-31' as date)),
quarter(cast('1966-04-01' as date)),
quarter(cast('1966-12-31' as date));

-- timestamp
select
quarter(cast('2014-01-10 00:00:00' as timestamp)),
quarter(cast('2014-02-10 15:23:00' as timestamp)),
quarter(cast('2014-03-31 15:23:00' as timestamp)),
quarter(cast('2014-04-02 15:23:00' as timestamp)),
quarter(cast('2014-05-28 15:23:00' as timestamp)),
quarter(cast('2016-06-03 15:23:00' as timestamp)),
quarter(cast('2016-07-28 15:23:00' as timestamp)),
quarter(cast('2016-08-29 15:23:00' as timestamp)),
quarter(cast('2016-09-29 15:23:00' as timestamp)),
quarter(cast('2016-10-29 15:23:00' as timestamp)),
quarter(cast('2016-11-29 15:23:00' as timestamp)),
quarter(cast('2016-12-29 15:23:00' as timestamp)),
-- null timestamp
quarter(cast(null as timestamp)),
-- negative Unix time
quarter(cast('1966-01-01 00:00:00' as timestamp)),
quarter(cast('1966-03-31 23:59:59.999' as timestamp)),
quarter(cast('1966-04-01 00:00:00' as timestamp)),
quarter(cast('1966-12-31 23:59:59.999' as timestamp));
set hive.fetch.task.conversion=more;

explain
select radians(57.2958) FROM src tablesample (1 rows);

select radians(57.2958) FROM src tablesample (1 rows);
select radians(143.2394) FROM src tablesample (1 rows);

DESCRIBE FUNCTION radians;
DESCRIBE FUNCTION EXTENDED radians;
explain
select radians(57.2958) FROM src tablesample (1 rows);

select radians(57.2958) FROM src tablesample (1 rows);
select radians(143.2394) FROM src tablesample (1 rows);

DESCRIBE FUNCTION radians;
DESCRIBE FUNCTION EXTENDED radians;DESCRIBE FUNCTION rand;
DESCRIBE FUNCTION EXTENDED rand;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION reflect;
DESCRIBE FUNCTION EXTENDED reflect;

EXPLAIN EXTENDED
SELECT reflect("java.lang.String", "valueOf", 1),
       reflect("java.lang.String", "isEmpty"),
       reflect("java.lang.Math", "max", 2, 3),
       reflect("java.lang.Math", "min", 2, 3),
       reflect("java.lang.Math", "round", 2.5),
       round(reflect("java.lang.Math", "exp", 1.0), 6),
       reflect("java.lang.Math", "floor", 1.9),
       reflect("java.lang.Integer", "valueOf", key, 16)
FROM src tablesample (1 rows);


SELECT reflect("java.lang.String", "valueOf", 1),
       reflect("java.lang.String", "isEmpty"),
       reflect("java.lang.Math", "max", 2, 3),
       reflect("java.lang.Math", "min", 2, 3),
       reflect("java.lang.Math", "round", 2.5),
       round(reflect("java.lang.Math", "exp", 1.0), 6),
       reflect("java.lang.Math", "floor", 1.9),
       reflect("java.lang.Integer", "valueOf", key, 16)
FROM src tablesample (1 rows);
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION reflect2;
DESCRIBE FUNCTION EXTENDED reflect2;

EXPLAIN EXTENDED
SELECT key,
       reflect2(key,   "byteValue"),
       reflect2(key,   "shortValue"),
       reflect2(key,   "intValue"),
       reflect2(key,   "longValue"),
       reflect2(key,   "floatValue"),
       reflect2(key,   "doubleValue"),
       reflect2(key,   "toString"),
       value,
       reflect2(value, "concat", "_concat"),
       reflect2(value, "contains", "86"),
       reflect2(value, "startsWith", "v"),
       reflect2(value, "endsWith", "6"),
       reflect2(value, "equals", "val_86"),
       reflect2(value, "equalsIgnoreCase", "VAL_86"),
       reflect2(value, "getBytes"),
       reflect2(value, "indexOf", "1"),
       reflect2(value, "lastIndexOf", "1"),
       reflect2(value, "replace", "val", "VALUE"),
       reflect2(value, "substring", 1),
       reflect2(value, "substring", 1, 5),
       reflect2(value, "toUpperCase"),
       reflect2(value, "trim"),
       ts,
       reflect2(ts, "getYear"),
       reflect2(ts, "getMonth"),
       reflect2(ts, "getDay"),
       reflect2(ts, "getHours"),
       reflect2(ts, "getMinutes"),
       reflect2(ts, "getSeconds"),
       reflect2(ts, "getTime")
FROM (select cast(key as int) key, value, cast('2013-02-15 19:41:20' as timestamp) ts from src) a LIMIT 5;


SELECT key,
       reflect2(key,   "byteValue"),
       reflect2(key,   "shortValue"),
       reflect2(key,   "intValue"),
       reflect2(key,   "longValue"),
       reflect2(key,   "floatValue"),
       reflect2(key,   "doubleValue"),
       reflect2(key,   "toString"),
       value,
       reflect2(value, "concat", "_concat"),
       reflect2(value, "contains", "86"),
       reflect2(value, "startsWith", "v"),
       reflect2(value, "endsWith", "6"),
       reflect2(value, "equals", "val_86"),
       reflect2(value, "equalsIgnoreCase", "VAL_86"),
       reflect2(value, "getBytes"),
       reflect2(value, "indexOf", "1"),
       reflect2(value, "lastIndexOf", "1"),
       reflect2(value, "replace", "val", "VALUE"),
       reflect2(value, "substring", 1),
       reflect2(value, "substring", 1, 5),
       reflect2(value, "toUpperCase"),
       reflect2(value, "trim"),
       ts,
       reflect2(ts, "getYear"),
       reflect2(ts, "getMonth"),
       reflect2(ts, "getDay"),
       reflect2(ts, "getHours"),
       reflect2(ts, "getMinutes"),
       reflect2(ts, "getSeconds"),
       reflect2(ts, "getTime")
FROM (select cast(key as int) key, value, cast('2013-02-15 19:41:20' as timestamp) ts from src) a LIMIT 5;
SELECT reflect("java.lang.StringClassThatDoesNotExist", "valueOf", 1),
       reflect("java.lang.String", "methodThatDoesNotExist"),
       reflect("java.lang.Math", "max", "overloadthatdoesnotexist", 3),
       reflect("java.lang.Math", "min", 2, 3),
       reflect("java.lang.Math", "round", 2.5),
       reflect("java.lang.Math", "exp", 1.0),
       reflect("java.lang.Math", "floor", 1.9)
FROM src LIMIT 1;

set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION regexp;
DESCRIBE FUNCTION EXTENDED regexp;

SELECT 'fofo' REGEXP '^fo', 'fo\no' REGEXP '^fo\no$', 'Bn' REGEXP '^Ba*n', 'afofo' REGEXP 'fo',
'afofo' REGEXP '^fo', 'Baan' REGEXP '^Ba?n', 'axe' REGEXP 'pi|apa', 'pip' REGEXP '^(pi)*$'
FROM src tablesample (1 rows);
DESCRIBE FUNCTION regexp_extract;
DESCRIBE FUNCTION EXTENDED regexp_extract;
DESCRIBE FUNCTION regexp_replace;
DESCRIBE FUNCTION EXTENDED regexp_replace;set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION repeat;
DESCRIBE FUNCTION EXTENDED repeat;

EXPLAIN SELECT
  repeat("Facebook", 3),
  repeat("", 4),
  repeat("asd", 0),
  repeat("asdf", -1)
FROM src tablesample (1 rows);

SELECT
  repeat("Facebook", 3),
  repeat("", 4),
  repeat("asd", 0),
  repeat("asdf", -1)
FROM src tablesample (1 rows);
DESCRIBE FUNCTION reverse;
DESCRIBE FUNCTION EXTENDED reverse;

CREATE TABLE dest1(len STRING);
EXPLAIN FROM src1 INSERT OVERWRITE TABLE dest1 SELECT reverse(src1.value);
FROM src1 INSERT OVERWRITE TABLE dest1 SELECT reverse(src1.value);
SELECT dest1.* FROM dest1;
DROP TABLE dest1;

-- Test with non-ascii characters
-- kv4.txt contains the text 0xE982B5E993AE, which should be reversed to
-- 0xE993AEE982B5
CREATE TABLE dest1(name STRING) STORED AS TEXTFILE;
LOAD DATA LOCAL INPATH '../../data/files/kv4.txt' INTO TABLE dest1;
SELECT count(1) FROM dest1 WHERE reverse(dest1.name) = _UTF-8 0xE993AEE982B5;
DESCRIBE FUNCTION rlike;
DESCRIBE FUNCTION EXTENDED rlike;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION round;
DESCRIBE FUNCTION EXTENDED round;

SELECT round(null), round(null, 0), round(125, null),
round(1.0/0.0, 0), round(power(-1.0,0.5), 0)
FROM src tablesample (1 rows);

SELECT
  round(55555), round(55555, 0), round(55555, 1), round(55555, 2), round(55555, 3),
  round(55555, -1), round(55555, -2), round(55555, -3), round(55555, -4),
  round(55555, -5), round(55555, -6), round(55555, -7), round(55555, -8)
FROM src tablesample (1 rows);

SELECT
  round(125.315), round(125.315, 0),
  round(125.315, 1), round(125.315, 2), round(125.315, 3), round(125.315, 4),
  round(125.315, -1), round(125.315, -2), round(125.315, -3), round(125.315, -4),
  round(-125.315), round(-125.315, 0),
  round(-125.315, 1), round(-125.315, 2), round(-125.315, 3), round(-125.315, 4),
  round(-125.315, -1), round(-125.315, -2), round(-125.315, -3), round(-125.315, -4)
FROM src tablesample (1 rows);

SELECT
  round(3.141592653589793, -15), round(3.141592653589793, -16),
  round(3.141592653589793, -13), round(3.141592653589793, -14),
  round(3.141592653589793, -11), round(3.141592653589793, -12),
  round(3.141592653589793, -9), round(3.141592653589793, -10),
  round(3.141592653589793, -7), round(3.141592653589793, -8),
  round(3.141592653589793, -5), round(3.141592653589793, -6),
  round(3.141592653589793, -3), round(3.141592653589793, -4),
  round(3.141592653589793, -1), round(3.141592653589793, -2),
  round(3.141592653589793, 0), round(3.141592653589793, 1),
  round(3.141592653589793, 2), round(3.141592653589793, 3),
  round(3.141592653589793, 4), round(3.141592653589793, 5),
  round(3.141592653589793, 6), round(3.141592653589793, 7),
  round(3.141592653589793, 8), round(3.141592653589793, 9),
  round(3.141592653589793, 10), round(3.141592653589793, 11),
  round(3.141592653589793, 12), round(3.141592653589793, 13),
  round(3.141592653589793, 13), round(3.141592653589793, 14),
  round(3.141592653589793, 15), round(3.141592653589793, 16)
FROM src tablesample (1 rows);

SELECT round(1809242.3151111344, 9), round(-1809242.3151111344, 9), round(1809242.3151111344BD, 9), round(-1809242.3151111344BD, 9)
FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

-- test for NaN (not-a-number)
create table tstTbl1(n double);

insert overwrite table tstTbl1
select 'NaN' from src tablesample (1 rows);

select * from tstTbl1;

select round(n, 1) from tstTbl1;
select round(n) from tstTbl1;

-- test for Infinity
select round(1/0), round(1/0, 2), round(1.0/0.0), round(1.0/0.0, 2) from src tablesample (1 rows);
set hive.fetch.task.conversion=more;

-- test for TINYINT
select round(-128), round(127), round(0) from src tablesample (1 rows);

-- test for SMALLINT
select round(-32768), round(32767), round(-129), round(128) from src tablesample (1 rows);

-- test for INT
select round(cast(negative(pow(2, 31)) as INT)), round(cast((pow(2, 31) - 1) as INT)), round(-32769), round(32768) from src tablesample (1 rows);

-- test for BIGINT
select round(cast(negative(pow(2, 63)) as BIGINT)), round(cast((pow(2, 63) - 1) as BIGINT)), round(cast(negative(pow(2, 31) + 1) as BIGINT)), round(cast(pow(2, 31) as BIGINT)) from src tablesample (1 rows);

-- test for DOUBLE
select round(126.1), round(126.7), round(32766.1), round(32766.7) from src tablesample (1 rows);
-- The ORDER BY on the outer query will typically have no effect,
-- but there is really no guarantee that the ordering is preserved
-- across various SQL operators.

drop temporary function row_sequence;

add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

create temporary function row_sequence as
'org.apache.hadoop.hive.contrib.udf.UDFRowSequence';

DESCRIBE FUNCTION EXTENDED row_sequence;

set mapred.reduce.tasks=1;
set hive.mapred.mode=nonstrict;
explain
select key, row_sequence() as r
from (select key from src order by key) x
order by r;

select key, row_sequence() as r
from (select key from src order by key) x
order by r;

-- make sure stateful functions do not get short-circuited away
-- a true result for key=105 would indicate undesired short-circuiting
select key, (key = 105) and (row_sequence() = 1)
from (select key from src order by key) x
order by key limit 20;

drop temporary function row_sequence;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION rpad;
DESCRIBE FUNCTION EXTENDED rpad;

EXPLAIN SELECT
  rpad('hi', 1, '?'),
  rpad('hi', 5, '.'),
  rpad('hi', 6, '123')
FROM src tablesample (1 rows);

SELECT
  rpad('hi', 1, '?'),
  rpad('hi', 5, '.'),
  rpad('hi', 6, '123')
FROM src tablesample (1 rows);
DESCRIBE FUNCTION rtrim;
DESCRIBE FUNCTION EXTENDED rtrim;
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION second;
DESCRIBE FUNCTION EXTENDED second;

EXPLAIN
SELECT second('2009-08-07 13:14:15'), second('13:14:15'), second('2009-08-07')
FROM src WHERE key = 86;

SELECT second('2009-08-07 13:14:15'), second('13:14:15'), second('2009-08-07')
FROM src WHERE key = 86;
set hive.mapred.mode=nonstrict;
CREATE TABLE sent_tmp (val array<string>);
CREATE TABLE sent_tmp2 (val string);
INSERT OVERWRITE TABLE sent_tmp
SELECT explode(sentences(decode(unhex("486976652065737420756E20657863656C6C656E74206F7574696C20706F7572206C65732072657175C3AA74657320646520646F6E6EC3A965732C20657420706575742DC3AA74726520706C757320706F6C7976616C656E7420717565206C612074726164756374696F6E206175746F6D61746971756521206C6120706F6E6374756174696F6E206D756C7469706C65732C206465732070687261736573206D616C20666F726DC3A96573202E2E2E20636F6E667573696F6E202D20657420706F757274616E742063652055444620666F6E6374696F6E6E6520656E636F72652121"), "UTF-8"), "fr")) AS val FROM src LIMIT 3;
INSERT OVERWRITE TABLE sent_tmp2
SELECT explode(val) AS val FROM sent_tmp;
SELECT hex(val) AS value FROM sent_tmp2 ORDER BY value ASC;

DROP TABLE sent_tmp;
DROP TABLE sent_tmp2;

CREATE TABLE sent_tmp (val array<string>);
CREATE TABLE sent_tmp2 (val string);
INSERT OVERWRITE TABLE sent_tmp
SELECT explode(sentences(decode(unhex("48697665206973742065696E2061757367657A656963686E65746573205765726B7A6575672066C3BC7220646965204162667261676520766F6E20446174656E2C20756E64207669656C6C6569636874207669656C736569746967657220616C7320646965206D61736368696E656C6C6520C39C6265727365747A756E6721204D756C7469706C652C207363686C6563687420676562696C646574656E2053C3A4747A65202E2E2E205665727765636873656C756E6720496E74657270756E6B74696F6E202D20756E6420646F636820697374206469657365205544462066756E6B74696F6E6965727420696D6D6572206E6F63682121"), "UTF-8"), "de")) AS val FROM src LIMIT 3;
INSERT OVERWRITE TABLE sent_tmp2
SELECT explode(val) AS val FROM sent_tmp;
SELECT hex(val) AS value FROM sent_tmp2 ORDER BY value ASC;



SELECT sentences("Hive is an excellent tool for data querying\; and perhaps more versatile than machine translation!! Multiple, ill-formed sentences...confounding punctuation--and yet this UDF still works!!!!") AS value FROM src ORDER BY value ASC LIMIT 1;
DESCRIBE FUNCTION sha1;
DESC FUNCTION EXTENDED sha;

explain select sha1('ABC');

select
sha1('ABC'),
sha(''),
sha(binary('ABC')),
sha1(binary('')),
sha1(cast(null as string)),
sha(cast(null as binary)),
sha1(null);
DESCRIBE FUNCTION sha2;
DESC FUNCTION EXTENDED sha2;

explain select sha2('ABC', 256);

select
sha2('ABC', 0),
sha2('', 0),
sha2(binary('ABC'), 0),
sha2(binary(''), 0),
sha2(cast(null as string), 0),
sha2(cast(null as binary), 0);

select
sha2('ABC', 256),
sha2('', 256),
sha2(binary('ABC'), 256),
sha2(binary(''), 256),
sha2(cast(null as string), 256),
sha2(cast(null as binary), 256);

select
sha2('ABC', 384),
sha2('', 384),
sha2(binary('ABC'), 384),
sha2(binary(''), 384),
sha2(cast(null as string), 384),
sha2(cast(null as binary), 384);

select
sha2('ABC', 512),
sha2('', 512),
sha2(binary('ABC'), 512),
sha2(binary(''), 512),
sha2(cast(null as string), 512),
sha2(cast(null as binary), 512);

--null
select
sha2('ABC', 200),
sha2('ABC', cast(null as int));set hive.fetch.task.conversion=more;

explain
select sign(0) FROM src tablesample (1 rows);
select sign(0) FROM src tablesample (1 rows);

select sign(-45) FROM src tablesample (1 rows);

select sign(46)  FROM src tablesample (1 rows);

DESCRIBE FUNCTION sign;
DESCRIBE FUNCTION EXTENDED sign;
explain
select sign(0) FROM src tablesample (1 rows);
select sign(0) FROM src tablesample (1 rows);

select sign(-45) FROM src tablesample (1 rows);

select sign(46)  FROM src tablesample (1 rows);

DESCRIBE FUNCTION sign;
DESCRIBE FUNCTION EXTENDED sign;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION sin;
DESCRIBE FUNCTION EXTENDED sin;

SELECT sin(null)
FROM src tablesample (1 rows);

SELECT sin(0.98), sin(1.57), sin(-0.5)
FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION size;
DESCRIBE FUNCTION EXTENDED size;

EXPLAIN
FROM src_thrift
SELECT size(src_thrift.lint),
       size(src_thrift.lintstring),
       size(src_thrift.mstringstring),
       size(null)
WHERE  src_thrift.lint IS NOT NULL
       AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1;


FROM src_thrift
SELECT size(src_thrift.lint),
       size(src_thrift.lintstring),
       size(src_thrift.mstringstring),
       size(null)
WHERE  src_thrift.lint IS NOT NULL
       AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1;
FROM src_thrift
SELECT size(src_thrift.lint, src_thrift.lintstring),
       size()
WHERE  src_thrift.lint IS NOT NULL
       AND NOT (src_thrift.mstringstring IS NULL) LIMIT 1;
SELECT SIZE('wrong type: string') FROM src;
DESCRIBE FUNCTION smallint;
DESCRIBE FUNCTION EXTENDED smallint;
set hive.fetch.task.conversion=more;

use default;
-- Test sort_array() UDF

DESCRIBE FUNCTION sort_array;
DESCRIBE FUNCTION EXTENDED sort_array;

-- Evaluate function against STRING valued keys
EXPLAIN
SELECT sort_array(array("b", "d", "c", "a")) FROM src tablesample (1 rows);

SELECT sort_array(array("f", "a", "g", "c", "b", "d", "e")) FROM src tablesample (1 rows);
SELECT sort_array(sort_array(array("hadoop distributed file system", "enterprise databases", "hadoop map-reduce"))) FROM src tablesample (1 rows);

-- Evaluate function against INT valued keys
SELECT sort_array(array(2, 9, 7, 3, 5, 4, 1, 6, 8)) FROM src tablesample (1 rows);

-- Evaluate function against FLOAT valued keys
SELECT sort_array(sort_array(array(2.333, 9, 1.325, 2.003, 0.777, -3.445, 1))) FROM src tablesample (1 rows);

-- Evaluate function against LIST valued keys
SELECT sort_array(array(array(2, 9, 7), array(3, 5, 4), array(1, 6, 8))) FROM src tablesample (1 rows);

-- Evaluate function against STRUCT valued keys
SELECT sort_array(array(struct(2, 9, 7), struct(3, 5, 4), struct(1, 6, 8))) FROM src tablesample (1 rows);

-- Evaluate function against MAP valued keys
SELECT sort_array(array(map("b", 2, "a", 9, "c", 7), map("c", 3, "b", 5, "a", 1), map("a", 1, "c", 6, "b", 8))) FROM src tablesample (1 rows);


-- Test it against data in a table.
CREATE TABLE dest1 (
	tinyints ARRAY<TINYINT>,
	smallints ARRAY<SMALLINT>,
	ints ARRAY<INT>,
	bigints ARRAY<BIGINT>,
	booleans ARRAY<BOOLEAN>,
	floats ARRAY<FLOAT>,
	doubles ARRAY<DOUBLE>,
	strings ARRAY<STRING>,
	timestamps ARRAY<TIMESTAMP>
) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/primitive_type_arrays.txt' OVERWRITE INTO TABLE dest1;

SELECT	sort_array(tinyints), sort_array(smallints), sort_array(ints),
	sort_array(bigints), sort_array(booleans), sort_array(floats),
	sort_array(doubles), sort_array(strings), sort_array(timestamps)
	FROM dest1;
-- invalid argument number
SELECT sort_array(array(2, 5, 4), 3) FROM src LIMIT 1;
-- invalid argument type
SELECT sort_array("Invalid") FROM src LIMIT 1;
-- invalid argument type
SELECT sort_array(array(create_union(0,"a"))) FROM src LIMIT 1;
DESCRIBE FUNCTION soundex;
DESC FUNCTION EXTENDED soundex;

explain select soundex('Miller');

select
soundex('Miller'),
soundex('miler'),
soundex('myller'),
soundex('muller'),
soundex('m'),
soundex('mu'),
soundex('mul'),
soundex('Peterson'),
soundex('Pittersen'),
soundex(''),
soundex(cast(null as string));set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION space;
DESCRIBE FUNCTION EXTENDED space;

EXPLAIN SELECT
  space(10),
  space(0),
  space(1),
  space(-1),
  space(-100)
FROM src tablesample (1 rows);

SELECT
  length(space(10)),
  length(space(0)),
  length(space(1)),
  length(space(-1)),
  length(space(-100))
FROM src tablesample (1 rows);

SELECT
  space(10),
  space(0),
  space(1),
  space(-1),
  space(-100)
FROM src tablesample (1 rows);

set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION split;
DESCRIBE FUNCTION EXTENDED split;

EXPLAIN SELECT
  split('a b c', ' '),
  split('oneAtwoBthreeC', '[ABC]'),
  split('', '.'),
  split(50401020, 0)
FROM src tablesample (1 rows);

SELECT
  split('a b c', ' '),
  split('oneAtwoBthreeC', '[ABC]'),
  split('', '.'),
  split(50401020, 0)
FROM src tablesample (1 rows);
DESCRIBE FUNCTION sqrt;
DESCRIBE FUNCTION EXTENDED sqrt;
DESCRIBE FUNCTION std;
DESCRIBE FUNCTION EXTENDED std;
DESCRIBE FUNCTION stddev;
DESCRIBE FUNCTION EXTENDED stddev;
DESCRIBE FUNCTION udf_stddev_pop;
DESCRIBE FUNCTION EXTENDED udf_stddev_pop;
DESCRIBE FUNCTION stddev_samp;
DESCRIBE FUNCTION EXTENDED stddev_samp;
DESCRIBE FUNCTION stddev_samp;
DESCRIBE FUNCTION EXTENDED stddev_samp;
DESCRIBE FUNCTION string;
DESCRIBE FUNCTION EXTENDED string;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION struct;
DESCRIBE FUNCTION EXTENDED struct;

EXPLAIN
SELECT struct(1), struct(1, "a"), struct(1, "b", 1.5).col1, struct(1, struct("a", 1.5)).col2.col1
FROM src tablesample (1 rows);

SELECT struct(1), struct(1, "a"), struct(1, "b", 1.5).col1, struct(1, struct("a", 1.5)).col2.col1
FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION substr;
DESCRIBE FUNCTION EXTENDED substr;

SELECT
  substr(null, 1), substr(null, 1, 1),
  substr('ABC', null), substr('ABC', null, 1),
  substr('ABC', 1, null)
FROM src tablesample (1 rows);

SELECT
  substr('ABC', 1, 0), substr('ABC', 1, -1), substr('ABC', 2, -100),
  substr('ABC', 4), substr('ABC', 4, 100),
  substr('ABC', -4), substr('ABC', -4, 100),
  substr('ABC', 100), substr('ABC', 100, 100),
  substr('ABC', -100), substr('ABC', -100, 100),
  substr('ABC', 2147483647), substr('ABC', 2147483647, 2147483647)
FROM src tablesample (1 rows);

SELECT
  substr('ABCDEFG', 3, 4), substr('ABCDEFG', -5, 4),
  substr('ABCDEFG', 3), substr('ABCDEFG', -5),
  substr('ABC', 0), substr('ABC', 1), substr('ABC', 2), substr('ABC', 3),
  substr('ABC', 1, 2147483647), substr('ABC', 2, 2147483647),
  substr('A', 0), substr('A', 1), substr('A', -1)
FROM src tablesample (1 rows);

SELECT
  substr('ABC', 0, 1), substr('ABC', 0, 2), substr('ABC', 0, 3), substr('ABC', 0, 4),
  substr('ABC', 1, 1), substr('ABC', 1, 2), substr('ABC', 1, 3), substr('ABC', 1, 4),
  substr('ABC', 2, 1), substr('ABC', 2, 2), substr('ABC', 2, 3), substr('ABC', 2, 4),
  substr('ABC', 3, 1), substr('ABC', 3, 2), substr('ABC', 3, 3), substr('ABC', 3, 4),
  substr('ABC', 4, 1)
FROM src tablesample (1 rows);

SELECT
  substr('ABC', -1, 1), substr('ABC', -1, 2), substr('ABC', -1, 3), substr('ABC', -1, 4),
  substr('ABC', -2, 1), substr('ABC', -2, 2), substr('ABC', -2, 3), substr('ABC', -2, 4),
  substr('ABC', -3, 1), substr('ABC', -3, 2), substr('ABC', -3, 3), substr('ABC', -3, 4),
  substr('ABC', -4, 1)
FROM src tablesample (1 rows);

-- substring() is a synonim of substr(), so just perform some basic tests
SELECT
  substring('ABCDEFG', 3, 4), substring('ABCDEFG', -5, 4),
  substring('ABCDEFG', 3), substring('ABCDEFG', -5),
  substring('ABC', 0), substring('ABC', 1), substring('ABC', 2), substring('ABC', 3),
  substring('ABC', 1, 2147483647), substring('ABC', 2, 2147483647),
  substring('A', 0), substring('A', 1), substring('A', -1)
FROM src tablesample (1 rows);

-- test for binary substr
SELECT
  substr(null, 1), substr(null, 1, 1),
  substr(ABC, null), substr(ABC, null, 1),
  substr(ABC, 1, null),
  substr(ABC, 0, 1), substr(ABC, 0, 2), substr(ABC, 0, 3), substr(ABC, 0, 4),
  substr(ABC, 1, 1), substr(ABC, 1, 2), substr(ABC, 1, 3), substr(ABC, 1, 4),
  substr(ABC, 2, 1), substr(ABC, 2, 2), substr(ABC, 2, 3), substr(ABC, 2, 4),
  substr(ABC, 3, 1), substr(ABC, 3, 2), substr(ABC, 3, 3), substr(ABC, 3, 4),
  substr(ABC, 4, 1),
  substr(ABC, -1, 1), substr(ABC, -1, 2), substr(ABC, -1, 3), substr(ABC, -1, 4),
  substr(ABC, -2, 1), substr(ABC, -2, 2), substr(ABC, -2, 3), substr(ABC, -2, 4),
  substr(ABC, -3, 1), substr(ABC, -3, 2), substr(ABC, -3, 3), substr(ABC, -3, 4),
  substr(ABC, -4, 1)
FROM (
   select CAST(concat(substr(value, 1, 0), 'ABC') as BINARY) as ABC from src tablesample (1 rows)
) X;

-- test UTF-8 substr
SELECT
  substr("玩", 1),
  substr("abc 玩", 5),
  substr("abc 玩玩玩 abc", 5),
  substr("abc 玩玩玩 abc", 5, 3)
FROM src tablesample (1 rows);
-- Synonym. See udf_substr.q
DESCRIBE FUNCTION substring;
DESCRIBE FUNCTION EXTENDED substring;
DESCRIBE FUNCTION substring_index;
DESCRIBE FUNCTION EXTENDED substring_index;

explain select substring_index('www.apache.org', '.', 2);

select
substring_index('www.apache.org', '.', 3),
substring_index('www.apache.org', '.', 2),
substring_index('www.apache.org', '.', 1),
substring_index('www.apache.org', '.', 0),
substring_index('www.apache.org', '.', -1),
substring_index('www.apache.org', '.', -2),
substring_index('www.apache.org', '.', -3);

select
--str is empty string
substring_index('', '.', 2),
--delim is empty string
substring_index('www.apache.org', '', 1),
--delim does not exist in str
substring_index('www.apache.org', '-', 2),
--delim is two chars
substring_index('www||apache||org', '||', 2),
--null
substring_index(cast(null as string), '.', 2),
substring_index('www.apache.org', cast(null as string), 2),
substring_index('www.apache.org', '.', cast(null as int));

--varchar and char
select
substring_index(cast('www.apache.org' as varchar(20)), '.', 2),
substring_index(cast('www.apache.org' as char(20)), '.', 2);
DESCRIBE FUNCTION -;
DESCRIBE FUNCTION EXTENDED -;
DESCRIBE FUNCTION sum;
DESCRIBE FUNCTION EXTENDED sum;

DESCRIBE FUNCTION sum;
DESCRIBE FUNCTION EXTENDED sum;

set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION tan;
DESCRIBE FUNCTION EXTENDED tan;

SELECT tan(null)
FROM src tablesample (1 rows);

SELECT tan(1), tan(6), tan(-1.0)
FROM src tablesample (1 rows);
DESCRIBE FUNCTION tan;
DESCRIBE FUNCTION EXTENDED tan;

SELECT tan(null)
FROM src tablesample (1 rows);

SELECT tan(1), tan(6), tan(-1.0)
FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

EXPLAIN
CREATE TEMPORARY FUNCTION testlength AS 'org.apache.hadoop.hive.ql.udf.UDFTestLength';

CREATE TEMPORARY FUNCTION testlength AS 'org.apache.hadoop.hive.ql.udf.UDFTestLength';

SELECT testlength(src.value) FROM src;

DROP TEMPORARY FUNCTION testlength;
set hive.fetch.task.conversion=more;

EXPLAIN
CREATE TEMPORARY FUNCTION testlength2 AS 'org.apache.hadoop.hive.ql.udf.UDFTestLength2';

CREATE TEMPORARY FUNCTION testlength2 AS 'org.apache.hadoop.hive.ql.udf.UDFTestLength2';

SELECT testlength2(src.value) FROM src;

DROP TEMPORARY FUNCTION testlength2;
CREATE TEMPORARY FUNCTION test_error AS 'org.apache.hadoop.hive.ql.udf.UDFTestErrorOnFalse';

SELECT test_error(key < 125 OR key > 130) FROM src;
CREATE TEMPORARY FUNCTION test_error AS 'org.apache.hadoop.hive.ql.udf.UDFTestErrorOnFalse';


SELECT test_error(key < 125 OR key > 130)
FROM (
  SELECT *
  FROM src
  DISTRIBUTE BY rand()
) map_output;


DESCRIBE FUNCTION tinyint;
DESCRIBE FUNCTION EXTENDED tinyint;
set hive.fetch.task.conversion=more;

-- 'true' cases:

SELECT CAST(CAST(1 AS TINYINT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(2 AS SMALLINT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(-4 AS INT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(-444 AS BIGINT) AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST(7.0 AS FLOAT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(-8.0 AS DOUBLE) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(-99.0 AS DECIMAL) AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST('Foo' AS STRING) AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST('2011-05-06 07:08:09' as timestamp) AS BOOLEAN) FROM src tablesample (1 rows);

-- 'false' cases:

SELECT CAST(CAST(0 AS TINYINT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(0 AS SMALLINT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(0 AS INT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(0 AS BIGINT) AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST(0.0 AS FLOAT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(0.0 AS DOUBLE) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(0.0 AS DECIMAL) AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST('' AS STRING) AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST(0 as timestamp) AS BOOLEAN) FROM src tablesample (1 rows);

-- 'NULL' cases:
SELECT CAST(NULL AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST(NULL AS TINYINT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(NULL AS SMALLINT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(NULL AS INT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(NULL AS BIGINT) AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST(NULL AS FLOAT) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(NULL AS DOUBLE) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(NULL AS DECIMAL) AS BOOLEAN) FROM src tablesample (1 rows);

SELECT CAST(CAST(NULL AS STRING) AS BOOLEAN) FROM src tablesample (1 rows);
SELECT CAST(CAST(NULL as timestamp) AS BOOLEAN) FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

-- Conversion of main primitive types to Byte type:
SELECT CAST(NULL AS TINYINT) FROM src tablesample (1 rows);

SELECT CAST(TRUE AS TINYINT) FROM src tablesample (1 rows);

SELECT CAST(CAST(-18 AS SMALLINT) AS TINYINT) FROM src tablesample (1 rows);
SELECT CAST(-129 AS TINYINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-1025 AS BIGINT) AS TINYINT) FROM src tablesample (1 rows);

SELECT CAST(CAST(-3.14 AS DOUBLE) AS TINYINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS FLOAT) AS TINYINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS DECIMAL) AS TINYINT) FROM src tablesample (1 rows);

SELECT CAST('-38' AS TINYINT) FROM src tablesample (1 rows);

DESCRIBE FUNCTION to_date;
DESCRIBE FUNCTION EXTENDED to_date;
set hive.fetch.task.conversion=more;

-- Conversion of main primitive types to Double type:
SELECT CAST(NULL AS DOUBLE) FROM src tablesample (1 rows);

SELECT CAST(TRUE AS DOUBLE) FROM src tablesample (1 rows);

SELECT CAST(CAST(-7 AS TINYINT) AS DOUBLE) FROM src tablesample (1 rows);
SELECT CAST(CAST(-18 AS SMALLINT) AS DOUBLE) FROM src tablesample (1 rows);
SELECT CAST(-129 AS DOUBLE) FROM src tablesample (1 rows);
SELECT CAST(CAST(-1025 AS BIGINT) AS DOUBLE) FROM src tablesample (1 rows);

SELECT CAST(CAST(-3.14 AS FLOAT) AS DOUBLE) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS DECIMAL(3,2)) AS DOUBLE) FROM src tablesample (1 rows);

SELECT CAST('-38.14' AS DOUBLE) FROM src tablesample (1 rows);

set hive.fetch.task.conversion=more;

-- Conversion of main primitive types to Float type:
SELECT CAST(NULL AS FLOAT) FROM src tablesample (1 rows);

SELECT CAST(TRUE AS FLOAT) FROM src tablesample (1 rows);

SELECT CAST(CAST(-7 AS TINYINT) AS FLOAT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-18 AS SMALLINT) AS FLOAT) FROM src tablesample (1 rows);
SELECT CAST(-129 AS FLOAT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-1025 AS BIGINT) AS FLOAT) FROM src tablesample (1 rows);

SELECT CAST(CAST(-3.14 AS DOUBLE) AS FLOAT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS DECIMAL(3,2)) AS FLOAT) FROM src tablesample (1 rows);

SELECT CAST('-38.14' AS FLOAT) FROM src tablesample (1 rows);

set hive.fetch.task.conversion=more;

-- Conversion of main primitive types to Long type:
SELECT CAST(NULL AS BIGINT) FROM src tablesample (1 rows);

SELECT CAST(TRUE AS BIGINT) FROM src tablesample (1 rows);

SELECT CAST(CAST(-7 AS TINYINT) AS BIGINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-18 AS SMALLINT) AS BIGINT) FROM src tablesample (1 rows);
SELECT CAST(-129 AS BIGINT) FROM src tablesample (1 rows);

SELECT CAST(CAST(-3.14 AS DOUBLE) AS BIGINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS FLOAT) AS BIGINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS DECIMAL) AS BIGINT) FROM src tablesample (1 rows);

SELECT CAST('-38' AS BIGINT) FROM src tablesample (1 rows);

set hive.fetch.task.conversion=more;

-- Conversion of main primitive types to Short type:
SELECT CAST(NULL AS SMALLINT) FROM src tablesample (1 rows);

SELECT CAST(TRUE AS SMALLINT) FROM src tablesample (1 rows);

SELECT CAST(CAST(-18 AS TINYINT) AS SMALLINT) FROM src tablesample (1 rows);
SELECT CAST(-129 AS SMALLINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-1025 AS BIGINT) AS SMALLINT) FROM src tablesample (1 rows);

SELECT CAST(CAST(-3.14 AS DOUBLE) AS SMALLINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS FLOAT) AS SMALLINT) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS DECIMAL) AS SMALLINT) FROM src tablesample (1 rows);

SELECT CAST('-38' AS SMALLINT) FROM src tablesample (1 rows);

set hive.fetch.task.conversion=more;

-- Conversion of main primitive types to String type:
SELECT CAST(NULL AS STRING) FROM src tablesample (1 rows);

SELECT CAST(TRUE AS STRING) FROM src tablesample (1 rows);

SELECT CAST(CAST(1 AS TINYINT) AS STRING) FROM src tablesample (1 rows);
SELECT CAST(CAST(-18 AS SMALLINT) AS STRING) FROM src tablesample (1 rows);
SELECT CAST(-129 AS STRING) FROM src tablesample (1 rows);
SELECT CAST(CAST(-1025 AS BIGINT) AS STRING) FROM src tablesample (1 rows);

SELECT CAST(CAST(-3.14 AS DOUBLE) AS STRING) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS FLOAT) AS STRING) FROM src tablesample (1 rows);
SELECT CAST(CAST(-3.14 AS DECIMAL(3,2)) AS STRING) FROM src tablesample (1 rows);

SELECT CAST('Foo' AS STRING) FROM src tablesample (1 rows);

set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION to_unix_timestamp;
DESCRIBE FUNCTION EXTENDED to_unix_timestamp;

create table oneline(key int, value string);
load data local inpath '../../data/files/things.txt' into table oneline;

SELECT
  '2009-03-20 11:30:01',
  to_unix_timestamp('2009-03-20 11:30:01')
FROM oneline;

SELECT
  '2009-03-20',
  to_unix_timestamp('2009-03-20', 'yyyy-MM-dd')
FROM oneline;

SELECT
  '2009 Mar 20 11:30:01 am',
  to_unix_timestamp('2009 Mar 20 11:30:01 am', 'yyyy MMM dd h:mm:ss a')
FROM oneline;

SELECT
  'random_string',
  to_unix_timestamp('random_string')
FROM oneline;

-- PPD
explain select * from (select * from src) a where unix_timestamp(a.key) > 10;
explain select * from (select * from src) a where to_unix_timestamp(a.key) > 10;
DESCRIBE FUNCTION to_utc_timestamp;
DESC FUNCTION EXTENDED to_utc_timestamp;

explain select to_utc_timestamp('2012-02-11 10:30:00', 'PST');

select
to_utc_timestamp('2012-02-10 20:30:00', 'PST'),
to_utc_timestamp('2012-02-11 08:30:00', 'Europe/Moscow'),
to_utc_timestamp('2012-02-11 12:30:00', 'GMT+8'),
to_utc_timestamp('2012-02-11 04:30:00', 'GMT'),
to_utc_timestamp('2012-02-11 04:30:00', ''),
to_utc_timestamp('2012-02-11 04:30:00', '---'),
to_utc_timestamp(cast(null as string), 'PST'),
to_utc_timestamp('2012-02-11 04:30:00', cast(null as string));

select
to_utc_timestamp(cast('2012-02-10 20:30:00' as timestamp), 'PST'),
to_utc_timestamp(cast('2012-02-11 08:30:00' as timestamp), 'Europe/Moscow'),
to_utc_timestamp(cast('2012-02-11 12:30:00' as timestamp), 'GMT+8'),
to_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), 'GMT'),
to_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), ''),
to_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), '---'),
to_utc_timestamp(cast(null as timestamp), 'PST'),
to_utc_timestamp(cast('2012-02-11 04:30:00' as timestamp), cast(null as string));
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION translate;
DESCRIBE FUNCTION EXTENDED translate;

-- Create some tables to serve some input data
CREATE TABLE table_input(input STRING);
CREATE TABLE table_translate(input_string STRING, from_string STRING, to_string STRING);

FROM src INSERT OVERWRITE TABLE table_input SELECT 'abcd' WHERE src.key = 86;
FROM src INSERT OVERWRITE TABLE table_translate SELECT 'abcd', 'ahd', '12' WHERE src.key = 86;

-- Run some queries on constant input parameters
SELECT  translate('abcd', 'ab', '12'),
        translate('abcd', 'abc', '12') FROM src tablesample (1 rows);

-- Run some queries where first parameter being a table column while the other two being constants
SELECT translate(table_input.input, 'ab', '12'),
       translate(table_input.input, 'abc', '12') FROM table_input tablesample (1 rows);

-- Run some queries where all parameters are coming from table columns
SELECT translate(input_string, from_string, to_string) FROM table_translate tablesample (1 rows);

-- Run some queries where some parameters are NULL
SELECT translate(NULL, 'ab', '12'),
       translate('abcd', NULL, '12'),
       translate('abcd', 'ab', NULL),
       translate(NULL, NULL, NULL) FROM src tablesample (1 rows);

-- Run some queries where the same character appears several times in the from string (2nd argument) of the UDF
SELECT translate('abcd', 'aba', '123'),
       translate('abcd', 'aba', '12') FROM src tablesample (1 rows);

-- Run some queries for the ignorant case when the 3rd parameter has more characters than the second one
SELECT translate('abcd', 'abc', '1234') FROM src tablesample (1 rows);

-- Test proper function over UTF-8 characters
SELECT translate('Àbcd', 'À', 'Ã') FROM src tablesample (1 rows);

-- Run some queries where the arguments are not strings but chars and varchars
SELECT translate(CAST('abcd' AS CHAR(5)), CAST('aba' AS VARCHAR(5)), CAST('123' AS CHAR(5))),
       translate(CAST('abcd' AS VARCHAR(9)), CAST('aba' AS CHAR(9)), CAST('12' AS VARCHAR(9)))
       FROM src tablesample (1 rows);
DESCRIBE FUNCTION trim;
DESCRIBE FUNCTION EXTENDED trim;
DESCRIBE FUNCTION trunc;
DESCRIBE FUNCTION EXTENDED trunc;

--test string with 'MM' as format
EXPLAIN
SELECT
      TRUNC('2014-01-01', 'MM'),
      TRUNC('2014-01-14', 'MM'),
      TRUNC('2014-01-31', 'MM'),
      TRUNC('2014-02-02', 'MM'),
      TRUNC('2014-02-28', 'MM'),
      TRUNC('2016-02-03', 'MM'),
      TRUNC('2016-02-28', 'MM'),
      TRUNC('2016-02-29', 'MM'),
      TRUNC('2014-01-01 10:30:45', 'MM'),
      TRUNC('2014-01-14 10:30:45', 'MM'),
      TRUNC('2014-01-31 10:30:45', 'MM'),
      TRUNC('2014-02-02 10:30:45', 'MM'),
      TRUNC('2014-02-28 10:30:45', 'MM'),
      TRUNC('2016-02-03 10:30:45', 'MM'),
      TRUNC('2016-02-28 10:30:45', 'MM'),
      TRUNC('2016-02-29 10:30:45', 'MM');


SELECT
      TRUNC('2014-01-01', 'MM'),
      TRUNC('2014-01-14', 'MM'),
      TRUNC('2014-01-31', 'MM'),
      TRUNC('2014-02-02', 'MM'),
      TRUNC('2014-02-28', 'MM'),
      TRUNC('2016-02-03', 'MM'),
      TRUNC('2016-02-28', 'MM'),
      TRUNC('2016-02-29', 'MM'),
      TRUNC('2014-01-01 10:30:45', 'MM'),
      TRUNC('2014-01-14 10:30:45', 'MM'),
      TRUNC('2014-01-31 10:30:45', 'MM'),
      TRUNC('2014-02-02 10:30:45', 'MM'),
      TRUNC('2014-02-28 10:30:45', 'MM'),
      TRUNC('2016-02-03 10:30:45', 'MM'),
      TRUNC('2016-02-28 10:30:45', 'MM'),
      TRUNC('2016-02-29 10:30:45', 'MM');

--test string with 'YEAR' as format
EXPLAIN
SELECT
      TRUNC('2014-01-01', 'YEAR'),
      TRUNC('2014-01-14', 'YEAR'),
      TRUNC('2014-01-31', 'YEAR'),
      TRUNC('2014-02-02', 'YEAR'),
      TRUNC('2014-02-28', 'YEAR'),
      TRUNC('2016-02-03', 'YEAR'),
      TRUNC('2016-02-28', 'YEAR'),
      TRUNC('2016-02-29', 'YEAR'),
      TRUNC('2014-01-01 10:30:45', 'YEAR'),
      TRUNC('2014-01-14 10:30:45', 'YEAR'),
      TRUNC('2014-01-31 10:30:45', 'YEAR'),
      TRUNC('2014-02-02 10:30:45', 'YEAR'),
      TRUNC('2014-02-28 10:30:45', 'YEAR'),
      TRUNC('2016-02-03 10:30:45', 'YEAR'),
      TRUNC('2016-02-28 10:30:45', 'YEAR'),
      TRUNC('2016-02-29 10:30:45', 'YEAR');


SELECT
      TRUNC('2014-01-01', 'YEAR'),
      TRUNC('2014-01-14', 'YEAR'),
      TRUNC('2014-01-31', 'YEAR'),
      TRUNC('2014-02-02', 'YEAR'),
      TRUNC('2014-02-28', 'YEAR'),
      TRUNC('2016-02-03', 'YEAR'),
      TRUNC('2016-02-28', 'YEAR'),
      TRUNC('2016-02-29', 'YEAR'),
      TRUNC('2014-01-01 10:30:45', 'YEAR'),
      TRUNC('2014-01-14 10:30:45', 'YEAR'),
      TRUNC('2014-01-31 10:30:45', 'YEAR'),
      TRUNC('2014-02-02 10:30:45', 'YEAR'),
      TRUNC('2014-02-28 10:30:45', 'YEAR'),
      TRUNC('2016-02-03 10:30:45', 'YEAR'),
      TRUNC('2016-02-28 10:30:45', 'YEAR'),
      TRUNC('2016-02-29 10:30:45', 'YEAR');


--test timestamp with 'MM' as format
EXPLAIN
SELECT
      TRUNC(CAST('2014-01-01 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-14 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-31 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-02-02 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-02-28 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-03 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-28 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-29 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-01 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-14 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-31 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-02-02 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-02-28 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-03 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-28 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-29 10:30:45' AS TIMESTAMP), 'MM');


SELECT
      TRUNC(CAST('2014-01-01 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-14 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-31 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-02-02 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-02-28 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-03 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-28 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-29 00:00:00' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-01 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-14 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-01-31 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-02-02 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2014-02-28 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-03 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-28 10:30:45' AS TIMESTAMP), 'MM'),
      TRUNC(CAST('2016-02-29 10:30:45' AS TIMESTAMP), 'MM');

--test timestamp with 'YEAR' as format
EXPLAIN
SELECT
      TRUNC(CAST('2014-01-01 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-14 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-31 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-02-02 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-02-28 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-03 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-28 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-29 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-01 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-14 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-31 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-02-02 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-02-28 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-03 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-28 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-29 10:30:45' AS TIMESTAMP), 'YEAR');


SELECT
      TRUNC(CAST('2014-01-01 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-14 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-31 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-02-02 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-02-28 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-03 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-28 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-29 00:00:00' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-01 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-14 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-01-31 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-02-02 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2014-02-28 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-03 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-28 10:30:45' AS TIMESTAMP), 'YEAR'),
      TRUNC(CAST('2016-02-29 10:30:45' AS TIMESTAMP), 'YEAR');

--test date with 'MM' as format
EXPLAIN
SELECT
      TRUNC(CAST('2014-01-01' AS DATE), 'MM'),
      TRUNC(CAST('2014-01-14' AS DATE), 'MM'),
      TRUNC(CAST('2014-01-31' AS DATE), 'MM'),
      TRUNC(CAST('2014-02-02' AS DATE), 'MM'),
      TRUNC(CAST('2014-02-28' AS DATE), 'MM'),
      TRUNC(CAST('2016-02-03' AS DATE), 'MM'),
      TRUNC(CAST('2016-02-28' AS DATE), 'MM'),
      TRUNC(CAST('2016-02-29' AS DATE), 'MM');


SELECT
      TRUNC(CAST('2014-01-01' AS DATE), 'MM'),
      TRUNC(CAST('2014-01-14' AS DATE), 'MM'),
      TRUNC(CAST('2014-01-31' AS DATE), 'MM'),
      TRUNC(CAST('2014-02-02' AS DATE), 'MM'),
      TRUNC(CAST('2014-02-28' AS DATE), 'MM'),
      TRUNC(CAST('2016-02-03' AS DATE), 'MM'),
      TRUNC(CAST('2016-02-28' AS DATE), 'MM'),
      TRUNC(CAST('2016-02-29' AS DATE), 'MM');

--test date with 'YEAR' as format
EXPLAIN
SELECT
      TRUNC(CAST('2014-01-01' AS DATE), 'YEAR'),
      TRUNC(CAST('2014-01-14' AS DATE), 'YEAR'),
      TRUNC(CAST('2014-01-31' AS DATE), 'YEAR'),
      TRUNC(CAST('2014-02-02' AS DATE), 'YEAR'),
      TRUNC(CAST('2014-02-28' AS DATE), 'YEAR'),
      TRUNC(CAST('2016-02-03' AS DATE), 'YEAR'),
      TRUNC(CAST('2016-02-28' AS DATE), 'YEAR'),
      TRUNC(CAST('2016-02-29' AS DATE), 'YEAR');


SELECT
      TRUNC(CAST('2014-01-01' AS DATE), 'YEAR'),
      TRUNC(CAST('2014-01-14' AS DATE), 'YEAR'),
      TRUNC(CAST('2014-01-31' AS DATE), 'YEAR'),
      TRUNC(CAST('2014-02-02' AS DATE), 'YEAR'),
      TRUNC(CAST('2014-02-28' AS DATE), 'YEAR'),
      TRUNC(CAST('2016-02-03' AS DATE), 'YEAR'),
      TRUNC(CAST('2016-02-28' AS DATE), 'YEAR'),
      TRUNC(CAST('2016-02-29' AS DATE), 'YEAR');


--test misc with 'MM' as format
EXPLAIN
SELECT
      TRUNC('2014-01-34', 'MM'),
      TRUNC(CAST(null AS STRING), 'MM'),
      TRUNC(CAST(null AS DATE), 'MM'),
      TRUNC(CAST(null AS TIMESTAMP), 'MM'),
      TRUNC('2014-01-01', 'M'),
      TRUNC('2014-01-01', CAST(null AS STRING));

SELECT
      TRUNC('2014-01-34', 'MM'),
      TRUNC(CAST(null AS STRING), 'MM'),
      TRUNC(CAST(null AS DATE), 'MM'),
      TRUNC(CAST(null AS TIMESTAMP), 'MM'),
      TRUNC('2014-01-01', 'M'),
      TRUNC('2014-01-01', CAST(null AS STRING));


--test misc with 'YEAR' as format
EXPLAIN
SELECT
      TRUNC('2014-01-34', 'YEAR'),
      TRUNC(CAST(null AS STRING), 'YEAR'),
      TRUNC(CAST(null AS DATE), 'YEAR'),
      TRUNC(CAST(null AS TIMESTAMP), 'YEAR'),
      TRUNC('2014-01-01', 'M'),
      TRUNC('2014-01-01', CAST(null AS STRING));

SELECT
      TRUNC('2014-01-34', 'YEAR'),
      TRUNC(CAST(null AS STRING), 'YEAR'),
      TRUNC(CAST(null AS DATE), 'YEAR'),
      TRUNC(CAST(null AS TIMESTAMP), 'YEAR'),
      TRUNC('2014-01-01', 'M'),
      TRUNC('2014-01-01', CAST(null AS STRING));SELECT TRUNC('2014-01-01', 1);SELECT TRUNC(1.0, 'MM');DESCRIBE FUNCTION ucase;
DESCRIBE FUNCTION EXTENDED ucase;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION unhex;
DESCRIBE FUNCTION EXTENDED unhex;

-- Good inputs

SELECT
  unhex('4D7953514C'),
  unhex('31323637'),
  unhex('61'),
  unhex('2D34'),
  unhex('')
FROM src tablesample (1 rows);

-- Bad inputs
SELECT
  unhex('MySQL'),
  unhex('G123'),
  unhex('\0')
FROM src tablesample (1 rows);
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION create_union;
DESCRIBE FUNCTION EXTENDED create_union;

EXPLAIN
SELECT create_union(0, key), create_union(if(key<100, 0, 1), 2.0, value),
create_union(1, "a", struct(2, "b"))
FROM src tablesample (2 rows);

SELECT create_union(0, key), create_union(if(key<100, 0, 1), 2.0, value),
create_union(1, "a", struct(2, "b"))
FROM src tablesample (2 rows);
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION unix_timestamp;
DESCRIBE FUNCTION EXTENDED unix_timestamp;

create table oneline(key int, value string);
load data local inpath '../../data/files/things.txt' into table oneline;

SELECT
  '2009-03-20 11:30:01',
  unix_timestamp('2009-03-20 11:30:01')
FROM oneline;

SELECT
  '2009-03-20',
  unix_timestamp('2009-03-20', 'yyyy-MM-dd')
FROM oneline;

SELECT
  '2009 Mar 20 11:30:01 am',
  unix_timestamp('2009 Mar 20 11:30:01 am', 'yyyy MMM dd h:mm:ss a')
FROM oneline;

create table foo as SELECT
  'deprecated' as a,
  unix_timestamp() as b
FROM oneline;
drop table foo;

SELECT
  'random_string',
  unix_timestamp('random_string')
FROM oneline;
DESCRIBE FUNCTION upper;
DESCRIBE FUNCTION EXTENDED upper;
dfs ${system:test.dfs.mkdir} hdfs:///tmp/udf_using;

dfs -copyFromLocal ../../data/files/sales.txt hdfs:///tmp/udf_using/sales.txt;

create function lookup as 'org.apache.hadoop.hive.ql.udf.UDFFileLookup' using file 'hdfs:///tmp/udf_using/sales.txt';

create table udf_using (c1 string);
insert overwrite table udf_using select 'Joe' from src limit 2;

select c1, lookup(c1) from udf_using;

drop table udf_using;
drop function lookup;

dfs -rmr hdfs:///tmp/udf_using;
DESCRIBE FUNCTION variance;
DESCRIBE FUNCTION EXTENDED variance;

DESCRIBE FUNCTION var_pop;
DESCRIBE FUNCTION EXTENDED var_pop;
DESCRIBE FUNCTION variance;
DESCRIBE FUNCTION EXTENDED variance;

DESCRIBE FUNCTION var_pop;
DESCRIBE FUNCTION EXTENDED var_pop;
DESCRIBE FUNCTION var_pop;
DESCRIBE FUNCTION EXTENDED var_pop;
DESCRIBE FUNCTION var_samp;
DESCRIBE FUNCTION EXTENDED var_samp;
DESCRIBE FUNCTION var_samp;
DESCRIBE FUNCTION EXTENDED var_samp;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION weekofyear;
DESCRIBE FUNCTION EXTENDED weekofyear;

SELECT weekofyear('1980-01-01'), weekofyear('1980-01-06'), weekofyear('1980-01-07'), weekofyear('1980-12-31'),
weekofyear('1984-1-1'), weekofyear('2008-02-20 00:00:00'), weekofyear('1980-12-28 23:59:59'), weekofyear('1980-12-29 23:59:59')
FROM src tablesample (1 rows);
SELECT CASE
        WHEN 1=1 THEN 2
        WHEN 3=5 THEN 4
        ELSE 5
       END,
       CASE
        WHEN 12=11 THEN 13
        WHEN 14=10 THEN 15
       END
FROM src LIMIT 1
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION when;
DESCRIBE FUNCTION EXTENDED when;

EXPLAIN
SELECT CASE
        WHEN 1=1 THEN 2
        WHEN 1=3 THEN 4
        ELSE 5
       END,
       CASE
        WHEN 6=7 THEN 8
        ELSE 9
       END,
       CASE
        WHEN 10=11 THEN 12
        WHEN 13=13 THEN 14
       END,
       CASE
        WHEN 15=16 THEN 17
        WHEN 18=19 THEN 20
       END,
       CASE
        WHEN 21=22 THEN NULL
        WHEN 23=23 THEN 24
       END,
       CASE
        WHEN 25=26 THEN 27
        WHEN 28=28 THEN NULL
       END
FROM src tablesample (1 rows);

SELECT CASE
        WHEN 1=1 THEN 2
        WHEN 1=3 THEN 4
        ELSE 5
       END,
       CASE
        WHEN 6=7 THEN 8
        ELSE 9
       END,
       CASE
        WHEN 10=11 THEN 12
        WHEN 13=13 THEN 14
       END,
       CASE
        WHEN 15=16 THEN 17
        WHEN 18=19 THEN 20
       END,
       CASE
        WHEN 21=22 THEN NULL
        WHEN 23=23 THEN 24
       END,
       CASE
        WHEN 25=26 THEN 27
        WHEN 28=28 THEN NULL
       END
FROM src tablesample (1 rows);

-- Allow compatible types to be used in return value
SELECT CASE
        WHEN 1=1 THEN 123.0BD
        ELSE 0.0BD
       END,
       CASE
        WHEN 1=1 THEN 123
        WHEN 1=2 THEN 1.0
        ELSE 222.02BD
       END,
       CASE
        WHEN 1=1 THEN 'abcd'
        WHEN 1=2 THEN cast('efgh' as varchar(10))
        ELSE cast('ijkl' as char(4))
       END
FROM src tablesample (1 rows);
SELECT CASE
        WHEN TRUE THEN 2
        WHEN '1' THEN 4
        ELSE 5
       END
FROM src LIMIT 1;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION xpath ;
DESCRIBE FUNCTION EXTENDED xpath ;

SELECT xpath ('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>', 'a/text()') FROM src tablesample (1 rows) ;
SELECT xpath ('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>', 'a/*/text()') FROM src tablesample (1 rows) ;
SELECT xpath ('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>', 'a/b/text()') FROM src tablesample (1 rows) ;
SELECT xpath ('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>', 'a/c/text()') FROM src tablesample (1 rows) ;
SELECT xpath ('<a><b class="bb">b1</b><b>b2</b><b>b3</b><c class="bb">c1</c><c>c2</c></a>', 'a/*[@class="bb"]/text()') FROM src tablesample (1 rows) ;set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION xpath_boolean ;
DESCRIBE FUNCTION EXTENDED xpath_boolean ;

SELECT xpath_boolean ('<a><b>b</b></a>', 'a/b') FROM src tablesample (1 rows) ;
SELECT xpath_boolean ('<a><b>b</b></a>', 'a/c') FROM src tablesample (1 rows) ;
SELECT xpath_boolean ('<a><b>b</b></a>', 'a/b = "b"') FROM src tablesample (1 rows) ;
SELECT xpath_boolean ('<a><b>b</b></a>', 'a/b = "c"') FROM src tablesample (1 rows) ;
SELECT xpath_boolean ('<a><b>10</b></a>', 'a/b < 10') FROM src tablesample (1 rows) ;
SELECT xpath_boolean ('<a><b>10</b></a>', 'a/b = 10') FROM src tablesample (1 rows) ;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION xpath_number ;
DESCRIBE FUNCTION EXTENDED xpath_number ;

DESCRIBE FUNCTION xpath_double ;
DESCRIBE FUNCTION EXTENDED xpath_double ;

SELECT xpath_double ('<a>this is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_double ('<a>this 2 is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_double ('<a><b>2000000000</b><c>40000000000</c></a>', 'a/b * a/c') FROM src tablesample (1 rows) ;
SELECT xpath_double ('<a>try a boolean</a>', 'a = 10') FROM src tablesample (1 rows) ;
SELECT xpath_double ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'a/b') FROM src tablesample (1 rows) ;
SELECT xpath_double ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/*)') FROM src tablesample (1 rows) ;
SELECT xpath_double ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b)') FROM src tablesample (1 rows) ;
SELECT xpath_double ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b[@class="odd"])') FROM src tablesample (1 rows) ;set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION xpath_float ;
DESCRIBE FUNCTION EXTENDED xpath_float ;

SELECT xpath_float ('<a>this is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_float ('<a>this 2 is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_float ('<a><b>2000000000</b><c>40000000000</c></a>', 'a/b * a/c') FROM src tablesample (1 rows) ;
SELECT xpath_float ('<a>try a boolean</a>', 'a = 10') FROM src tablesample (1 rows) ;
SELECT xpath_float ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'a/b') FROM src tablesample (1 rows) ;
SELECT xpath_float ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/*)') FROM src tablesample (1 rows) ;
SELECT xpath_float ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b)') FROM src tablesample (1 rows) ;
SELECT xpath_float ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b[@class="odd"])') FROM src tablesample (1 rows) ;set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION xpath_int ;
DESCRIBE FUNCTION EXTENDED xpath_int ;

SELECT xpath_int ('<a>this is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_int ('<a>this 2 is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_int ('<a><b>2000000000</b><c>40000000000</c></a>', 'a/b * a/c') FROM src tablesample (1 rows) ;
SELECT xpath_int ('<a>try a boolean</a>', 'a = 10') FROM src tablesample (1 rows) ;
SELECT xpath_int ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'a/b') FROM src tablesample (1 rows) ;
SELECT xpath_int ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/*)') FROM src tablesample (1 rows) ;
SELECT xpath_int ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b)') FROM src tablesample (1 rows) ;
SELECT xpath_int ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b[@class="odd"])') FROM src tablesample (1 rows) ;set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION xpath_long ;
DESCRIBE FUNCTION EXTENDED xpath_long ;

SELECT xpath_long ('<a>this is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_long ('<a>this 2 is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_long ('<a><b>2000000000</b><c>40000000000</c></a>', 'a/b * a/c') FROM src tablesample (1 rows) ;
SELECT xpath_long ('<a>try a boolean</a>', 'a = 10') FROM src tablesample (1 rows) ;
SELECT xpath_long ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'a/b') FROM src tablesample (1 rows) ;
SELECT xpath_long ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/*)') FROM src tablesample (1 rows) ;
SELECT xpath_long ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b)') FROM src tablesample (1 rows) ;
SELECT xpath_long ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b[@class="odd"])') FROM src tablesample (1 rows) ;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION xpath_short ;
DESCRIBE FUNCTION EXTENDED xpath_short ;

SELECT xpath_short ('<a>this is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_short ('<a>this 2 is not a number</a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_short ('<a><b>2000000000</b><c>40000000000</c></a>', 'a/b * a/c') FROM src tablesample (1 rows) ;
SELECT xpath_short ('<a>try a boolean</a>', 'a = 10') FROM src tablesample (1 rows) ;
SELECT xpath_short ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'a/b') FROM src tablesample (1 rows) ;
SELECT xpath_short ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/*)') FROM src tablesample (1 rows) ;
SELECT xpath_short ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b)') FROM src tablesample (1 rows) ;
SELECT xpath_short ('<a><b class="odd">1</b><b class="even">2</b><b class="odd">4</b><c>8</c></a>', 'sum(a/b[@class="odd"])') FROM src tablesample (1 rows) ;set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION xpath_string ;
DESCRIBE FUNCTION EXTENDED xpath_string ;

SELECT xpath_string ('<a><b>bb</b><c>cc</c></a>', 'a') FROM src tablesample (1 rows) ;
SELECT xpath_string ('<a><b>bb</b><c>cc</c></a>', 'a/b') FROM src tablesample (1 rows) ;
SELECT xpath_string ('<a><b>bb</b><c>cc</c></a>', 'a/c') FROM src tablesample (1 rows) ;
SELECT xpath_string ('<a><b>bb</b><c>cc</c></a>', 'a/d') FROM src tablesample (1 rows) ;
SELECT xpath_string ('<a><b>b1</b><b>b2</b></a>', '//b') FROM src tablesample (1 rows) ;
SELECT xpath_string ('<a><b>b1</b><b>b2</b></a>', 'a/b[1]') FROM src tablesample (1 rows) ;
SELECT xpath_string ('<a><b>b1</b><b>b2</b></a>', 'a/b[2]') FROM src tablesample (1 rows) ;
SELECT xpath_string ('<a><b>b1</b><b id="b_2">b2</b></a>', 'a/b[@id="b_2"]') FROM src tablesample (1 rows) ;
set hive.mapred.mode=nonstrict;
set hive.fetch.task.conversion=more;

DESCRIBE FUNCTION explode;
DESCRIBE FUNCTION EXTENDED explode;

EXPLAIN EXTENDED SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3;
EXPLAIN EXTENDED SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol;

SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3;
SELECT explode(array(1,2,3)) AS (myCol) FROM src LIMIT 3;
SELECT a.myCol, count(1) FROM (SELECT explode(array(1,2,3)) AS myCol FROM src LIMIT 3) a GROUP BY a.myCol;

EXPLAIN SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3;
EXPLAIN EXTENDED SELECT a.myKey, a.myVal, count(1) FROM (SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3) a GROUP BY a.myKey, a.myVal;

SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3;
SELECT a.myKey, a.myVal, count(1) FROM (SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src LIMIT 3) a GROUP BY a.myKey, a.myVal;

SELECT src.key, myCol FROM src lateral view explode(array(1,2,3)) x AS myCol LIMIT 3;
SELECT src.key, myKey, myVal FROM src lateral view explode(map(1,'one',2,'two',3,'three')) x AS myKey,myVal LIMIT 3;

-- HIVE-4295
SELECT BLOCK__OFFSET__INSIDE__FILE, src.key, myKey, myVal FROM src lateral view explode(map(1,'one',2,'two',3,'three')) x AS myKey,myVal LIMIT 3;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION explode2 AS 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2';

SELECT explode2(array(1,2,3)) AS (col1, col2) FROM src LIMIT 3;

DROP TEMPORARY FUNCTION explode2;add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION explode2 AS 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFExplode2';

SELECT explode2(array(1,2,3)) AS col1 FROM src LIMIT 3;

DROP TEMPORARY FUNCTION explode2;
SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal) FROM src GROUP BY key;
SELECT explode(map(1,'one',2,'two',3,'three')) as (myKey,myVal,myVal2) FROM src;select explode(array(1),array(2)) as myCol from src;SELECT explode(null) as myNull FROM src GROUP BY key;select distinct key, explode(key) from src;
set hive.mapred.mode=nonstrict;
create table json_t (key string, jstring string);

insert overwrite table json_t
select * from (
  select '1', '{"f1": "value1", "f2": "value2", "f3": 3, "f5": 5.23}' from src tablesample (1 rows)
  union all
  select '2', '{"f1": "value12", "f3": "value3", "f2": 2, "f4": 4.01}' from src tablesample (1 rows)
  union all
  select '3', '{"f1": "value13", "f4": "value44", "f3": "value33", "f2": 2, "f5": 5.01}' from src tablesample (1 rows)
  union all
  select '4', cast(null as string) from src tablesample (1 rows)
  union all
  select '5', '{"f1": "", "f5": null}' from src tablesample (1 rows)
  union all
  select '6', '[invalid JSON string]' from src tablesample (1 rows)
) s;

explain
select a.key, b.* from json_t a lateral view json_tuple(a.jstring, 'f1', 'f2', 'f3', 'f4', 'f5') b as f1, f2, f3, f4, f5 order by a.key;

select a.key, b.* from json_t a lateral view json_tuple(a.jstring, 'f1', 'f2', 'f3', 'f4', 'f5') b as f1, f2, f3, f4, f5 order by a.key;

explain
select json_tuple(a.jstring, 'f1', 'f2', 'f3', 'f4', 'f5') as (f1, f2, f3, f4, f5) from json_t a order by f1, f2, f3;

select json_tuple(a.jstring, 'f1', 'f2', 'f3', 'f4', 'f5') as (f1, f2, f3, f4, f5) from json_t a order by f1, f2, f3;

explain
select a.key, b.f2, b.f5 from json_t a lateral view json_tuple(a.jstring, 'f1', 'f2', 'f3', 'f4', 'f5') b as f1, f2, f3, f4, f5 order by a.key;

select a.key, b.f2, b.f5 from json_t a lateral view json_tuple(a.jstring, 'f1', 'f2', 'f3', 'f4', 'f5') b as f1, f2, f3, f4, f5 order by a.key;

explain
select f2, count(*) from json_t a lateral view json_tuple(a.jstring, 'f1', 'f2', 'f3', 'f4', 'f5') b as f1, f2, f3, f4, f5 where f1 is not null group by f2 order by f2;

select f2, count(*) from json_t a lateral view json_tuple(a.jstring, 'f1', 'f2', 'f3', 'f4', 'f5') b as f1, f2, f3, f4, f5 where f1 is not null group by f2 order by f2;


-- Verify that json_tuple can handle new lines in JSON values

CREATE TABLE dest1(c1 STRING) STORED AS RCFILE;

INSERT OVERWRITE TABLE dest1 SELECT '{"a":"b\nc"}' FROM src tablesample (1 rows);

SELECT * FROM dest1;

SELECT json FROM dest1 a LATERAL VIEW json_tuple(c1, 'a') b AS json;create temporary function udtfCount2 as 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2';

set hive.fetch.task.conversion=minimal;
-- Correct output should be 2 rows
select udtfCount2() from src;

set hive.fetch.task.conversion=more;
-- Should still have the same output with fetch task conversion enabled
select udtfCount2() from src;

SELECT explode(array(1,2,3)) as myCol, key FROM src;
SELECT explode(array(1,2,3)) as myCol FROM src GROUP BY key;
add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;

CREATE TEMPORARY FUNCTION udtfCount2 AS 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2';

SELECT udtfCount2(key) AS count FROM src;

SELECT * FROM src LATERAL VIEW udtfCount2(key) myTable AS myCol;

DROP TEMPORARY FUNCTION udtfCount;set hive.mapred.mode=nonstrict;
create table url_t (key string, fullurl string);

insert overwrite table url_t
select * from (
  select '1', 'http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1' from src tablesample (1 rows)
  union all
  select '2', 'https://www.socs.uts.edu.au:80/MosaicDocs-old/url-primer.html?k1=tps#chapter1' from src tablesample (1 rows)
  union all
  select '3', 'ftp://sites.google.com/a/example.com/site/page' from src tablesample (1 rows)
  union all
  select '4', cast(null as string) from src tablesample (1 rows)
  union all
  select '5', 'htttp://' from src tablesample (1 rows)
  union all
  select '6', '[invalid url string]' from src tablesample (1 rows)
) s;

describe function parse_url_tuple;
describe function extended parse_url_tuple;

explain
select a.key, b.* from url_t a lateral view parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE', 'AUTHORITY', 'USERINFO', 'QUERY:k1') b as ho, pa, qu, re, pr, fi, au, us, qk1 order by a.key;

select a.key, b.* from url_t a lateral view parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE', 'AUTHORITY', 'USERINFO', 'QUERY:k1') b as ho, pa, qu, re, pr, fi, au, us, qk1 order by a.key;

explain
select parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE', 'AUTHORITY', 'USERINFO', 'QUERY:k1') as (ho, pa, qu, re, pr, fi, au, us, qk1) from url_t a order by ho, pa, qu;

select parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE', 'AUTHORITY', 'USERINFO', 'QUERY:k1') as (ho, pa, qu, re, pr, fi, au, us, qk1) from url_t a order by ho, pa, qu;

-- should return null for 'host', 'query', 'QUERY:nonExistCol'
explain
select a.key, b.ho, b.qu, b.qk1, b.err1, b.err2, b.err3 from url_t a lateral view parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE', 'AUTHORITY', 'USERINFO', 'QUERY:k1', 'host', 'query', 'QUERY:nonExistCol') b as ho, pa, qu, re, pr, fi, au, us, qk1, err1, err2, err3 order by a.key;

select a.key, b.ho, b.qu, b.qk1, b.err1, b.err2, b.err3 from url_t a lateral view parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE', 'AUTHORITY', 'USERINFO', 'QUERY:k1', 'host', 'query', 'QUERY:nonExistCol') b as ho, pa, qu, re, pr, fi, au, us, qk1, err1, err2, err3 order by a.key;


explain
select ho, count(*) from url_t a lateral view parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE', 'AUTHORITY', 'USERINFO', 'QUERY:k1') b as ho, pa, qu, re, pr, fi, au, us, qk1 where qk1 is not null group by ho;

select ho, count(*) from url_t a lateral view parse_url_tuple(a.fullurl, 'HOST', 'PATH', 'QUERY', 'REF', 'PROTOCOL', 'FILE', 'AUTHORITY', 'USERINFO', 'QUERY:k1') b as ho, pa, qu, re, pr, fi, au, us, qk1 where qk1 is not null group by ho;

CREATE TABLE employees (
name STRING,
salary FLOAT,
subordinates ARRAY<STRING>,
deductions MAP<STRING, FLOAT>,
address STRUCT<street:STRING, city:STRING, state:STRING, zip:INT>);

LOAD DATA LOCAL INPATH '../../data/files/posexplode_data.txt' INTO TABLE employees;

SELECT
  name, pos, sub
FROM
  employees
LATERAL VIEW
  posexplode(subordinates) subView AS pos, sub;
DESCRIBE FUNCTION stack;

EXPLAIN SELECT x, y FROM src LATERAL VIEW STACK(2, 'x', array(1), 'z') a AS x, y LIMIT 2;
EXPLAIN SELECT x, y FROM src LATERAL VIEW STACK(2, 'x', array(1), 'z', array(4)) a AS x, y LIMIT 2;

SELECT x, y FROM src LATERAL VIEW STACK(2, 'x', array(1), 'z') a AS x, y LIMIT 2;
SELECT x, y FROM src LATERAL VIEW STACK(2, 'x', array(1), 'z', array(4)) a AS x, y LIMIT 2;

EXPLAIN
SELECT stack(1, "en", "dbpedia", NULL );
SELECT stack(1, "en", "dbpedia", NULL );-- HIVE-4618 hive should accept unicode notation like \uxxxx

CREATE  TABLE k1( a string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\u0001';
desc formatted k1;
drop table k1;

CREATE  TABLE k1( a string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001';
desc formatted k1;
drop table k1;

CREATE  TABLE k1( a string)ROW FORMAT DELIMITED FIELDS TERMINATED BY '|';
desc formatted k1;
drop table k1;
FROM (
  FROM src select src.key, src.value WHERE src.key < 100
  UNION ALL
  FROM src SELECT src.* WHERE src.key > 100
) unioninput
INSERT OVERWRITE DIRECTORY '../build/ql/test/data/warehouse/union.out' SELECT unioninput.*
set hive.mapred.mode=nonstrict;
-- SORT_BEFORE_DIFF
-- union case: both subqueries are map jobs on same input, followed by filesink

EXPLAIN
FROM (
  FROM src select src.key, src.value WHERE src.key < 100
  UNION ALL
  FROM src SELECT src.* WHERE src.key > 100
) unioninput
INSERT OVERWRITE DIRECTORY 'target/warehouse/union.out' SELECT unioninput.*;

FROM (
  FROM src select src.key, src.value WHERE src.key < 100
  UNION ALL
  FROM src SELECT src.* WHERE src.key > 100
) unioninput
INSERT OVERWRITE DIRECTORY 'target/warehouse/union.out' SELECT unioninput.*;

dfs -cat ${system:test.warehouse.dir}/union.out/*;
set hive.mapred.mode=nonstrict;
set hive.map.aggr = true;

-- SORT_QUERY_RESULTS

-- union case: all subqueries are a map-reduce jobs, 3 way union, same input for all sub-queries, followed by filesink

create table tmptable(key string, value int);

explain
insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION  ALL
                                            select 'tst2' as key, count(1) as value from src s2
                                        UNION ALL
                                            select 'tst3' as key, count(1) as value from src s3) unionsrc;


insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION  ALL
                                            select 'tst2' as key, count(1) as value from src s2
                                        UNION ALL
                                            select 'tst3' as key, count(1) as value from src s3) unionsrc;


select * from tmptable x sort by x.key;

set hive.mapred.mode=nonstrict;
set hive.map.aggr = true;
-- SORT_BEFORE_DIFF
-- union case: all subqueries are a map-reduce jobs, 3 way union, same input for all sub-queries, followed by reducesink

explain
  select unionsrc.key, count(1) FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION  ALL
                                            select 'tst2' as key, count(1) as value from src s2
                                        UNION ALL
                                            select 'tst3' as key, count(1) as value from src s3) unionsrc group by unionsrc.key;


  select unionsrc.key, count(1) FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION  ALL
                                            select 'tst2' as key, count(1) as value from src s2
                                        UNION ALL
                                            select 'tst3' as key, count(1) as value from src s3) unionsrc group by unionsrc.key;



set hive.mapred.mode=nonstrict;
set hive.map.aggr = true;

-- SORT_QUERY_RESULTS

-- union case: all subqueries are a map-reduce jobs, 3 way union, different inputs for all sub-queries, followed by filesink

create table tmptable(key string, value int);

explain
insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION  ALL
                                            select 'tst2' as key, count(1) as value from src1 s2
                                        UNION ALL
                                            select 'tst3' as key, count(1) as value from srcbucket s3) unionsrc;


insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION  ALL
                                            select 'tst2' as key, count(1) as value from src1 s2
                                        UNION ALL
                                            select 'tst3' as key, count(1) as value from srcbucket s3) unionsrc;

select * from tmptable x sort by x.key;
set hive.mapred.mode=nonstrict;
-- SORT_BEFORE_DIFF
-- union case: both subqueries are a map-only jobs, same input, followed by filesink

explain
  select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL
                                            select s2.key as key, s2.value as value from src s2) unionsrc;

select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL
                                          select s2.key as key, s2.value as value from src s2) unionsrc;
set hive.mapred.mode=nonstrict;
set hive.map.aggr = true;
-- SORT_BEFORE_DIFF
-- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by reducesink

explain
  select unionsrc.key, count(1) FROM (select s2.key as key, s2.value as value from src1 s2
                                        UNION  ALL
                                      select 'tst1' as key, cast(count(1) as string) as value from src s1)
  unionsrc group by unionsrc.key;



  select unionsrc.key, count(1) FROM (select s2.key as key, s2.value as value from src1 s2
                                        UNION  ALL
                                      select 'tst1' as key, cast(count(1) as string) as value from src s1)
  unionsrc group by unionsrc.key;
set hive.mapred.mode=nonstrict;
set hive.map.aggr = true;
-- SORT_BEFORE_DIFF
-- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by reducesink

explain
  select unionsrc.key, count(1) FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                        UNION  ALL
                                            select s2.key as key, s2.value as value from src1 s2
                                        UNION  ALL
                                            select s3.key as key, s3.value as value from src1 s3) unionsrc group by unionsrc.key;

  select unionsrc.key, count(1) FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                        UNION  ALL
                                            select s2.key as key, s2.value as value from src1 s2
                                        UNION  ALL
                                            select s3.key as key, s3.value as value from src1 s3) unionsrc group by unionsrc.key;


set hive.mapred.mode=nonstrict;
-- SORT_BEFORE_DIFF
EXPLAIN
SELECT count(1) FROM (
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL

  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL

  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL

  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL

  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src) src;


SELECT count(1) FROM (
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL

  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL

  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL

  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL

  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src UNION ALL
  SELECT key, value FROM src) src;
CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;

-- SORT_QUERY_RESULTS
-- union case:map-reduce sub-queries followed by multi-table insert

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value;

FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;

-- SORT_QUERY_RESULTS

-- union case:map-reduce sub-queries followed by multi-table insert

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, unionsrc.value
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, unionsrc.value;

FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, unionsrc.value
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, unionsrc.value;

SELECT DEST1.* FROM DEST1 SORT BY DEST1.key, DEST1.value;
SELECT DEST2.* FROM DEST2 SORT BY DEST2.key, DEST2.val1, DEST2.val2;
-- SORT_QUERY_RESULTS

CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;

-- union case:map-reduce sub-queries followed by multi-table insert

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, count(unionsrc.value) group by unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, unionsrc.value;

FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, count(unionsrc.value) group by unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, unionsrc.value;

SELECT DEST1.* FROM DEST1 SORT BY DEST1.key, DEST1.value;
SELECT DEST2.* FROM DEST2 SORT BY DEST2.key, DEST2.val1, DEST2.val2;



set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_BEFORE_DIFF
-- union case: both subqueries are map-reduce jobs on same input, followed by reduce sink

explain
  select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL
                        select s2.key as key, s2.value as value from src s2) unionsrc;

select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL
                      select s2.key as key, s2.value as value from src s2) unionsrc;


create table if not exists union2_t1(r string, c string, v array<string>);
create table if not exists union2_t2(s string, c string, v string);

explain
SELECT s.r, s.c, sum(s.v)
FROM (
  SELECT a.r AS r, a.c AS c, a.v AS v FROM union2_t1 a
  UNION ALL
  SELECT b.s AS r, b.c AS c, 0 + b.v AS v FROM union2_t2 b
) s
GROUP BY s.r, s.c;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
-- union :map-reduce sub-queries followed by join

explain
SELECT unionsrc1.key, unionsrc1.value, unionsrc2.key, unionsrc2.value
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2 where s2.key < 10) unionsrc1
JOIN
     (select 'tst1' as key, cast(count(1) as string) as value from src s3
                         UNION  ALL
      select s4.key as key, s4.value as value from src s4 where s4.key < 10) unionsrc2
ON (unionsrc1.key = unionsrc2.key);

SELECT unionsrc1.key, unionsrc1.value, unionsrc2.key, unionsrc2.value
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION  ALL
      select s2.key as key, s2.value as value from src s2 where s2.key < 10) unionsrc1
JOIN
     (select 'tst1' as key, cast(count(1) as string) as value from src s3
                         UNION  ALL
      select s4.key as key, s4.value as value from src s4 where s4.key < 10) unionsrc2
ON (unionsrc1.key = unionsrc2.key);
-- SORT_QUERY_RESULTS
-- union of constants, udf outputs, and columns from text table and thrift table

explain
SELECT key, count(1)
FROM (
  SELECT '1' as key from src
  UNION ALL
  SELECT reverse(key) as key from src
  UNION ALL
  SELECT key as key from src
  UNION ALL
  SELECT astring as key from src_thrift
  UNION ALL
  SELECT lstring[0] as key from src_thrift
) union_output
GROUP BY key;

SELECT key, count(1)
FROM (
  SELECT '1' as key from src
  UNION ALL
  SELECT reverse(key) as key from src
  UNION ALL
  SELECT key as key from src
  UNION ALL
  SELECT astring as key from src_thrift
  UNION ALL
  SELECT lstring[0] as key from src_thrift
) union_output
GROUP BY key;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

create table dst_union22(k1 string, k2 string, k3 string, k4 string) partitioned by (ds string);
create table dst_union22_delta(k0 string, k1 string, k2 string, k3 string, k4 string, k5 string) partitioned by (ds string);

insert overwrite table dst_union22 partition (ds='1')
select key, value, key , value from src;

insert overwrite table dst_union22_delta partition (ds='1')
select key, key, value, key, value, value from src;

set hive.merge.mapfiles=false;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

explain extended
insert overwrite table dst_union22 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta where ds = '1' and k0 <= 50
union all
select a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22 a left outer join (select * from dst_union22_delta where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq;

insert overwrite table dst_union22 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta where ds = '1' and k0 <= 50
union all
select a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22 a left outer join (select * from dst_union22_delta where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq;

select * from dst_union22 where ds = '2';
set hive.mapred.mode=nonstrict;
create table dst_union22(k1 string, k2 string, k3 string, k4 string) partitioned by (ds string);
create table dst_union22_delta(k0 string, k1 string, k2 string, k3 string, k4 string, k5 string) partitioned by (ds string);

insert overwrite table dst_union22 partition (ds='1')
select key, value, key , value from src;

insert overwrite table dst_union22_delta partition (ds='1')
select key, key, value, key, value, value from src;

set hive.merge.mapfiles=false;

-- Union followed by Mapjoin is not supported.
-- The same query would work without the hint
-- Note that there is a positive test with the same name in clientpositive
explain extended
insert overwrite table dst_union22 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta where ds = '1' and k0 <= 50
union all
select /*+ MAPJOIN(b) */ a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22 a left outer join (select * from dst_union22_delta where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq;
set hive.mapred.mode=nonstrict;
explain
select s.key2, s.value2
from (
  select transform(key, value) using 'cat' as (key2, value2)
  from src
  union all
  select key as key2, value as value2 from src) s
order by s.key2, s.value2;

select s.key2, s.value2
from (
  select transform(key, value) using 'cat' as (key2, value2)
  from src
  union all
  select key as key2, value as value2 from src) s
order by s.key2, s.value2;

set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

create table src2 as select key, count(1) as count from src group by key;
create table src3 as select * from src2;
create table src4 as select * from src2;
create table src5 as select * from src2;


set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;


explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select key, count from src4  where key < 10
  union all
  select key, count(1) as count from src5 where key < 10 group by key
)s
;

select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select key, count from src4  where key < 10
  union all
  select key, count(1) as count from src5 where key < 10 group by key
)s
;

explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, b.count as count from src4 a join src5 b on a.key=b.key where a.key < 10
)s
;

select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, b.count as count from src4 a join src5 b on a.key=b.key where a.key < 10
)s
;

explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, count(1) as count from src4 a join src5 b on a.key=b.key where a.key < 10 group by a.key
)s
;

select s.key, s.count from (
  select key, count from src2  where key < 10
  union all
  select key, count from src3  where key < 10
  union all
  select a.key as key, count(1) as count from src4 a join src5 b on a.key=b.key where a.key < 10 group by a.key
)s
;
set hive.mapred.mode=nonstrict;
create table tmp_srcpart like srcpart;

insert overwrite table tmp_srcpart partition (ds='2008-04-08', hr='11')
select key, value from srcpart where ds='2008-04-08' and hr='11';

explain
create table tmp_unionall as
SELECT count(1) as counts, key, value
FROM
(
  SELECT key, value FROM srcpart a WHERE a.ds='2008-04-08' and a.hr='11'

    UNION ALL

  SELECT key, key as value FROM (
    SELECT distinct key FROM (
      SELECT key, value FROM tmp_srcpart a WHERE a.ds='2008-04-08' and a.hr='11'
        UNION ALL
      SELECT key, value FROM tmp_srcpart b WHERE b.ds='2008-04-08' and b.hr='11'
    )t
  ) master_table
) a GROUP BY key, value
;
-- SORT_QUERY_RESULTS

EXPLAIN
SELECT
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key

UNION ALL

select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
;

SELECT
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key

UNION ALL

select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
;


SELECT
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key

UNION ALL

select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
;

SELECT
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key

UNION ALL

select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS
create table jackson_sev_same as select * from src;
create table dim_pho as select * from src;
create table jackson_sev_add as select * from src;
explain select b.* from jackson_sev_same a join (select * from dim_pho union all select * from jackson_sev_add)b on a.key=b.key and b.key=97;
select b.* from jackson_sev_same a join (select * from dim_pho union all select * from jackson_sev_add)b on a.key=b.key and b.key=97;
set hive.mapred.mode=nonstrict;
create table union_subq_union(key int, value string);

explain
insert overwrite table union_subq_union
select * from (
  select key, value from src
  union all
  select key, value from
  (
    select key, value, count(1) from src group by key, value
    union all
    select key, value, count(1) from src group by key, value
  ) subq
) a
;

insert overwrite table union_subq_union
select * from (
  select key, value from src
  union all
  select key, value from
  (
    select key, value, count(1) from src group by key, value
    union all
    select key, value, count(1) from src group by key, value
  ) subq
) a
;

select * from union_subq_union order by key, value limit 20;
create table union_subq_union(key int, value string);

explain
insert overwrite table union_subq_union
select * from (
  select key, value from src
  union all
  select key, value from
  (
    select key, value from src
    union all
    select key, value from src
  ) subq
) a
;

insert overwrite table union_subq_union
select * from (
  select key, value from src
  union all
  select key, value from
  (
    select key, value from src
    union all
    select key, value from src
  ) subq
) a
;

select * from union_subq_union order by key, value limit 20;
set hive.explain.user=false;
-- SORT_QUERY_RESULTS

explain
SELECT *
FROM (
  SELECT 1 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION ALL
  SELECT 2 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION ALL
  SELECT 3 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  UNION ALL
  SELECT 4 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  CLUSTER BY id
) a;



CREATE TABLE union_out (id int);

insert overwrite table union_out
SELECT *
FROM (
  SELECT 1 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION ALL
  SELECT 2 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION ALL
  SELECT 3 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  UNION ALL
  SELECT 4 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  CLUSTER BY id
) a;

select * from union_out;
set hive.mapred.mode=nonstrict;
create table union_subq_union(key int, value string);

explain
insert overwrite table union_subq_union
select * from (

select * from (
  select key, value from src
  union all
  select key, value from
  (
    select key, value, count(1) from src group by key, value
    union all
    select key, value, count(1) from src group by key, value
  ) subq
) a

union all

select key, value from src
) aa
;

insert overwrite table union_subq_union
select * from (

select * from (
  select key, value from src
  union all
  select key, value from
  (
    select key, value, count(1) from src group by key, value
    union all
    select key, value, count(1) from src group by key, value
  ) subq
) a

union all

select key, value from src
) aa
;

select * from union_subq_union order by key, value limit 20;
-- SORT_QUERY_RESULTS

drop table t1;
drop table t2;


create table t1 as select * from src where key < 10;
create table t2 as select * from src where key < 10;

create table t3(key string, cnt int);
create table t4(value string, cnt int);

explain
from
(select * from t1
 union all
 select * from t2
) x
insert overwrite table t3
  select key, count(1) group by key
insert overwrite table t4
  select value, count(1) group by value;

from
(select * from t1
 union all
 select * from t2
) x
insert overwrite table t3
  select key, count(1) group by key
insert overwrite table t4
  select value, count(1) group by value;

select * from t3;
select * from t4;

create table t5(c1 string, cnt int);
create table t6(c1 string, cnt int);

explain
from
(
 select key as c1, count(1) as cnt from t1 group by key
   union all
 select key as c1, count(1) as cnt from t2 group by key
) x
insert overwrite table t5
  select c1, sum(cnt) group by c1
insert overwrite table t6
  select c1, sum(cnt) group by c1;

from
(
 select key as c1, count(1) as cnt from t1 group by key
   union all
 select key as c1, count(1) as cnt from t2 group by key
) x
insert overwrite table t5
  select c1, sum(cnt) group by c1
insert overwrite table t6
  select c1, sum(cnt) group by c1;

select * from t5;
select * from t6;

drop table t1;
drop table t2;

create table t1 as select * from src where key < 10;
create table t2 as select key, count(1) as cnt from src where key < 10 group by key;

create table t7(c1 string, cnt int);
create table t8(c1 string, cnt int);

explain
from
(
 select key as c1, count(1) as cnt from t1 group by key
   union all
 select key as c1, cnt from t2
) x
insert overwrite table t7
  select c1, count(1) group by c1
insert overwrite table t8
  select c1, count(1) group by c1;

from
(
 select key as c1, count(1) as cnt from t1 group by key
   union all
 select key as c1, cnt from t2
) x
insert overwrite table t7
  select c1, count(1) group by c1
insert overwrite table t8
  select c1, count(1) group by c1;

select * from t7;
select * from t8;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

-- This tests various union queries which have columns on one side of the query
-- being of double type and those on the other side another

CREATE TABLE t1 AS SELECT * FROM src WHERE key < 10;
CREATE TABLE t2 AS SELECT * FROM src WHERE key < 10;

-- Test simple union with double
EXPLAIN
SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key FROM t1
UNION ALL
SELECT CAST(key AS BIGINT) AS key FROM t2) a
ORDER BY key;

SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key FROM t1
UNION ALL
SELECT CAST(key AS BIGINT) AS key FROM t2) a
;

-- Test union with join on the left
EXPLAIN
SELECT * FROM
(SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key
UNION ALL
SELECT CAST(key AS DOUBLE) AS key FROM t2) a
;

SELECT * FROM
(SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key
UNION ALL
SELECT CAST(key AS DOUBLE) AS key FROM t2) a
;

-- Test union with join on the right
EXPLAIN
SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key FROM t2
UNION ALL
SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key) a
;

SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key FROM t2
UNION ALL
SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key) a
;

-- Test union with join on the left selecting multiple columns
EXPLAIN
SELECT * FROM
(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
UNION ALL
SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
;

SELECT * FROM
(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
UNION ALL
SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
;

-- Test union with join on the right selecting multiple columns
EXPLAIN
SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
UNION ALL
SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
;

SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
UNION ALL
SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
;
set hive.mapred.mode=nonstrict;
set hive.groupby.skewindata=true;
-- SORT_BEFORE_DIFF
-- This tests that a union all with a map only subquery on one side and a
-- subquery involving two map reduce jobs on the other runs correctly.

CREATE TABLE test_src (key STRING, value STRING);

EXPLAIN INSERT OVERWRITE TABLE test_src
SELECT key, value FROM (
	SELECT key, value FROM src
	WHERE key = 0
UNION ALL
 	SELECT key, COUNT(*) AS value FROM src
 	GROUP BY key
)a;

INSERT OVERWRITE TABLE test_src
SELECT key, value FROM (
	SELECT key, value FROM src
	WHERE key = 0
UNION ALL
 	SELECT key, COUNT(*) AS value FROM src
 	GROUP BY key
)a;

SELECT COUNT(*) FROM test_src;

EXPLAIN INSERT OVERWRITE TABLE test_src
SELECT key, value FROM (
	SELECT key, COUNT(*) AS value FROM src
 	GROUP BY key
UNION ALL
 	SELECT key, value FROM src
	WHERE key = 0
)a;

INSERT OVERWRITE TABLE test_src
SELECT key, value FROM (
	SELECT key, COUNT(*) AS value FROM src
 	GROUP BY key
UNION ALL
 	SELECT key, value FROM src
	WHERE key = 0
)a;

SELECT COUNT(*) FROM test_src;
 set hive.mapred.mode=nonstrict;
create table src10_1 (key string, value string);
create table src10_2 (key string, value string);
create table src10_3 (key string, value string);
create table src10_4 (key string, value string);

from (select * from src tablesample (10 rows)) a
insert overwrite table src10_1 select *
insert overwrite table src10_2 select *
insert overwrite table src10_3 select *
insert overwrite table src10_4 select *;

set hive.auto.convert.join=true;
-- When we convert the Join of sub1 and sub0 into a MapJoin,
-- we can use a single MR job to evaluate this entire query.
explain
SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION ALL
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0
) alias1 order by key;

SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION ALL
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0
) alias1 order by key;

set hive.auto.convert.join=false;
-- When we do not convert the Join of sub1 and sub0 into a MapJoin,
-- we need to use two MR jobs to evaluate this query.
-- The first job is for the Join of sub1 and sub2. The second job
-- is for the UNION ALL and ORDER BY.
explain
SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION ALL
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0
) alias1 order by key;

SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION ALL
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION ALL SELECT * FROM src10_4 ) alias0
) alias1 order by key;
select * from (
     select * from ( select 1 as id , 'foo' as str_1 from src tablesample(5 rows)) f
 union all
     select * from ( select 2 as id , 'bar' as str_2 from src tablesample(5 rows)) g
) e ;

set hive.cbo.enable=false;

select * from (
     select * from ( select 1 as id , 'foo' as str_1 from src tablesample(5 rows)) f
 union all
     select * from ( select 2 as id , 'bar' as str_2 from src tablesample(5 rows)) g
) e ;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=false;

select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u order by y;

select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u order by y;





set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.map.aggr = true;

-- SORT_QUERY_RESULTS

-- union case: both subqueries are map-reduce jobs on same input, followed by filesink


create table tmptable(key string, value int);

explain
insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION  ALL
                                            select 'tst2' as key, count(1) as value from src s2) unionsrc;

insert overwrite table tmptable
select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION  ALL
                                          select 'tst2' as key, count(1) as value from src s2) unionsrc;

select * from tmptable x sort by x.key;


set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.map.aggr = true;
-- SORT_BEFORE_DIFF
-- union case: both subqueries are map-reduce jobs on same input, followed by reduce sink

explain
  select unionsrc.key, count(1) FROM (select 'tst1' as key, count(1) as value from src s1
                                    UNION  ALL
                                      select 'tst2' as key, count(1) as value from src s2) unionsrc group by unionsrc.key;

select unionsrc.key, count(1) FROM (select 'tst1' as key, count(1) as value from src s1
                                  UNION  ALL
                                    select 'tst2' as key, count(1) as value from src s2) unionsrc group by unionsrc.key;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.map.aggr = true;

-- SORT_QUERY_RESULTS

-- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by filesink

create table tmptable(key string, value string);

explain
insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                        UNION  ALL
                                            select s2.key as key, s2.value as value from src1 s2) unionsrc;

insert overwrite table tmptable
select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                      UNION  ALL
                                          select s2.key as key, s2.value as value from src1 s2) unionsrc;

select * from tmptable x sort by x.key, x.value;


set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.map.aggr = true;

-- SORT_BEFORE_DIFF
-- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by reducesink

explain
  select unionsrc.key, count(1) FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                        UNION  ALL
                                            select s2.key as key, s2.value as value from src1 s2) unionsrc group by unionsrc.key;

select unionsrc.key, count(1) FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                      UNION  ALL
                                    select s2.key as key, s2.value as value from src1 s2) unionsrc group by unionsrc.key;


set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_BEFORE_DIFF
-- union case: all subqueries are a map-only jobs, 3 way union, same input for all sub-queries, followed by filesink

explain
  select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL
                                            select s2.key as key, s2.value as value from src s2 UNION  ALL
                                            select s3.key as key, s3.value as value from src s3) unionsrc;

select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL
                                          select s2.key as key, s2.value as value from src s2 UNION  ALL
                                          select s3.key as key, s3.value as value from src s3) unionsrc;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- SORT_BEFORE_DIFF
-- union case: all subqueries are a map-only jobs, 3 way union, same input for all sub-queries, followed by reducesink

explain
  select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL
                        select s2.key as key, s2.value as value from src s2 UNION ALL
                        select s3.key as key, s3.value as value from src s3) unionsrc;

  select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION  ALL
                        select s2.key as key, s2.value as value from src s2 UNION ALL
                        select s3.key as key, s3.value as value from src s3) unionsrc;
set hive.mapred.mode=nonstrict;
set hive.optimize.ppd=true;

-- SORT_QUERY_RESULTS

drop table if exists union_all_bug_test_1;
drop table if exists union_all_bug_test_2;

create table if not exists union_all_bug_test_1
(
f1 int,
f2 int
);

create table if not exists union_all_bug_test_2
(
f1 int
);

explain SELECT f1
FROM (

SELECT
f1
, if('helloworld' like '%hello%' ,f1,f2) as filter
FROM union_all_bug_test_1

union all

select
f1
, 0 as filter
from union_all_bug_test_2
) A
WHERE (filter = 1);

SELECT f1
FROM (

SELECT
f1
, if('helloworld' like '%hello%' ,f1,f2) as filter
FROM union_all_bug_test_1

union all

select
f1
, 0 as filter
from union_all_bug_test_2
) A
WHERE (filter = 1);

insert into table union_all_bug_test_1 values (1,1);
insert into table union_all_bug_test_2 values (1);
insert into table union_all_bug_test_1 values (0,0);
insert into table union_all_bug_test_2 values (0);

SELECT f1
FROM (

SELECT
f1
, if('helloworld' like '%hello%' ,f1,f2) as filter
FROM union_all_bug_test_1

union all

select
f1
, 0 as filter
from union_all_bug_test_2
) A
WHERE (filter = 1);

SELECT f1
FROM (

SELECT
f1
, if('helloworld' like '%hello%' ,f1,f2) as filter
FROM union_all_bug_test_1

union all

select
f1
, 0 as filter
from union_all_bug_test_2
) A
WHERE (filter = 0);

SELECT f1
FROM (

SELECT
f1
, if('helloworld' like '%hello%' ,f1,f2) as filter
FROM union_all_bug_test_1

union all

select
f1
, 0 as filter
from union_all_bug_test_2
) A
WHERE (filter = 1 or filter = 0);

explain

SELECT f1
FROM (

SELECT
f1
, if('helloworld' like '%hello%' ,f1,f2) as filter
FROM union_all_bug_test_1

union all

select
f1
, 0 as filter
from union_all_bug_test_2
) A
WHERE (f1 = 1);

SELECT f1
FROM (

SELECT
f1
, if('helloworld' like '%hello%' ,f1,f2) as filter
FROM union_all_bug_test_1

union all

select
f1
, 0 as filter
from union_all_bug_test_2
) A
WHERE (f1 = 1);

drop table if exists map_json;
drop table if exists map_json1;
drop table if exists map_json2;

create table map_json1(
  id int,
  val array<string>);

create table map_json2(
  id int,
  val array<string>);

create table map_json(
  id int,
  val array<string>);

create view explode as
select id, l from map_json1 LATERAL VIEW explode(val) tup as l
UNION ALL
select id, get_json_object(l, '$.daysLeft') as l
from map_json2 LATERAL VIEW explode(val) tup as l
UNION ALL
select id, l from map_json LATERAL VIEW explode(val) elems as l;

select count(*) from explode where get_json_object(l, '$') is NOT NULL;

drop view explode;
drop table map_json;
drop table map_json1;
drop table map_json2;
SELECT f1
FROM (

SELECT
f1
, if('helloworld' like '%hello%' ,f1,f2) as filter
FROM union_all_bug_test_1

union all

select
f1
, 0 as filter
from union_all_bug_test_2
) A
WHERE (filter = 1 and f1 = 1);
-- orderByClause clusterByClause distributeByClause sortByClause limitClause
-- can only be applied to the whole union.

select key from src cluster by key
union all
select key from src;



set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- union10.q



-- union case: all subqueries are a map-reduce jobs, 3 way union, same input for all sub-queries, followed by filesink

create table tmptable(key string, value int);

explain
insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION DISTINCT
                                            select 'tst2' as key, count(1) as value from src s2
                                        UNION DISTINCT
                                            select 'tst3' as key, count(1) as value from src s3) unionsrc;


insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION DISTINCT
                                            select 'tst2' as key, count(1) as value from src s2
                                        UNION DISTINCT
                                            select 'tst3' as key, count(1) as value from src s3) unionsrc;


select * from tmptable x sort by x.key;

-- union11.q


-- SORT_BEFORE_DIFF
-- union case: all subqueries are a map-reduce jobs, 3 way union, same input for all sub-queries, followed by reducesink

explain
  select unionsrc.key, count(1) FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION DISTINCT
                                            select 'tst2' as key, count(1) as value from src s2
                                        UNION DISTINCT
                                            select 'tst3' as key, count(1) as value from src s3) unionsrc group by unionsrc.key;


  select unionsrc.key, count(1) FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION DISTINCT
                                            select 'tst2' as key, count(1) as value from src s2
                                        UNION DISTINCT
                                            select 'tst3' as key, count(1) as value from src s3) unionsrc group by unionsrc.key;



-- union12.q



-- union case: all subqueries are a map-reduce jobs, 3 way union, different inputs for all sub-queries, followed by filesink

create table tmptable12(key string, value int);

explain
insert overwrite table tmptable12
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION DISTINCT
                                            select 'tst2' as key, count(1) as value from src1 s2
                                        UNION DISTINCT
                                            select 'tst3' as key, count(1) as value from srcbucket s3) unionsrc;


insert overwrite table tmptable12
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION DISTINCT
                                            select 'tst2' as key, count(1) as value from src1 s2
                                        UNION DISTINCT
                                            select 'tst3' as key, count(1) as value from srcbucket s3) unionsrc;

select * from tmptable12 x sort by x.key;
-- union13.q

-- SORT_BEFORE_DIFF
-- union case: both subqueries are a map-only jobs, same input, followed by filesink

explain
  select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT
                                            select s2.key as key, s2.value as value from src s2) unionsrc;

select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT
                                          select s2.key as key, s2.value as value from src s2) unionsrc;
-- union14.q


-- SORT_BEFORE_DIFF
-- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by reducesink

explain
  select unionsrc.key, count(1) FROM (select s2.key as key, s2.value as value from src1 s2
                                        UNION DISTINCT
                                      select 'tst1' as key, cast(count(1) as string) as value from src s1)
  unionsrc group by unionsrc.key;



  select unionsrc.key, count(1) FROM (select s2.key as key, s2.value as value from src1 s2
                                        UNION DISTINCT
                                      select 'tst1' as key, cast(count(1) as string) as value from src s1)
  unionsrc group by unionsrc.key;
-- union15.q


-- SORT_BEFORE_DIFF
-- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by reducesink

explain
  select unionsrc.key, count(1) FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                        UNION DISTINCT
                                            select s2.key as key, s2.value as value from src1 s2
                                        UNION DISTINCT
                                            select s3.key as key, s3.value as value from src1 s3) unionsrc group by unionsrc.key;

  select unionsrc.key, count(1) FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                        UNION DISTINCT
                                            select s2.key as key, s2.value as value from src1 s2
                                        UNION DISTINCT
                                            select s3.key as key, s3.value as value from src1 s3) unionsrc group by unionsrc.key;


-- union16.q

-- SORT_BEFORE_DIFF
EXPLAIN
SELECT count(1) FROM (
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT

  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT

  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT

  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT

  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src) src;


SELECT count(1) FROM (
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT

  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT

  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT

  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT

  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src UNION DISTINCT
  SELECT key, value FROM src) src;
-- union17.q

CREATE TABLE DEST1(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST2(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;

-- SORT_BEFORE_DIFF
-- union case:map-reduce sub-queries followed by multi-table insert

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value;

FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST1 SELECT unionsrc.key, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key
INSERT OVERWRITE TABLE DEST2 SELECT unionsrc.key, unionsrc.value, COUNT(DISTINCT SUBSTR(unionsrc.value,5)) GROUP BY unionsrc.key, unionsrc.value;

SELECT DEST1.* FROM DEST1;
SELECT DEST2.* FROM DEST2;
-- union18.q

CREATE TABLE DEST118(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST218(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;

-- union case:map-reduce sub-queries followed by multi-table insert

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST118 SELECT unionsrc.key, unionsrc.value
INSERT OVERWRITE TABLE DEST218 SELECT unionsrc.key, unionsrc.value, unionsrc.value;

FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST118 SELECT unionsrc.key, unionsrc.value
INSERT OVERWRITE TABLE DEST218 SELECT unionsrc.key, unionsrc.value, unionsrc.value;

SELECT DEST118.* FROM DEST118 SORT BY DEST118.key, DEST118.value;
SELECT DEST218.* FROM DEST218 SORT BY DEST218.key, DEST218.val1, DEST218.val2;
-- union19.q




CREATE TABLE DEST119(key STRING, value STRING) STORED AS TEXTFILE;
CREATE TABLE DEST219(key STRING, val1 STRING, val2 STRING) STORED AS TEXTFILE;

-- union case:map-reduce sub-queries followed by multi-table insert

explain
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST119 SELECT unionsrc.key, count(unionsrc.value) group by unionsrc.key
INSERT OVERWRITE TABLE DEST219 SELECT unionsrc.key, unionsrc.value, unionsrc.value;

FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2) unionsrc
INSERT OVERWRITE TABLE DEST119 SELECT unionsrc.key, count(unionsrc.value) group by unionsrc.key
INSERT OVERWRITE TABLE DEST219 SELECT unionsrc.key, unionsrc.value, unionsrc.value;

SELECT DEST119.* FROM DEST119 SORT BY DEST119.key, DEST119.value;
SELECT DEST219.* FROM DEST219 SORT BY DEST219.key, DEST219.val1, DEST219.val2;



-- union2.q

-- SORT_BEFORE_DIFF
-- union case: both subqueries are map-reduce jobs on same input, followed by reduce sink

explain
  select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT
                        select s2.key as key, s2.value as value from src s2) unionsrc;

select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT
                      select s2.key as key, s2.value as value from src s2) unionsrc;
-- union20.q

-- SORT_BEFORE_DIFF
-- union :map-reduce sub-queries followed by join

explain
SELECT unionsrc1.key, unionsrc1.value, unionsrc2.key, unionsrc2.value
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2 where s2.key < 10) unionsrc1
JOIN
     (select 'tst1' as key, cast(count(1) as string) as value from src s3
                         UNION DISTINCT
      select s4.key as key, s4.value as value from src s4 where s4.key < 10) unionsrc2
ON (unionsrc1.key = unionsrc2.key);

SELECT unionsrc1.key, unionsrc1.value, unionsrc2.key, unionsrc2.value
FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                         UNION DISTINCT
      select s2.key as key, s2.value as value from src s2 where s2.key < 10) unionsrc1
JOIN
     (select 'tst1' as key, cast(count(1) as string) as value from src s3
                         UNION DISTINCT
      select s4.key as key, s4.value as value from src s4 where s4.key < 10) unionsrc2
ON (unionsrc1.key = unionsrc2.key);
-- union21.q

-- SORT_BEFORE_DIFF
-- union of constants, udf outputs, and columns from text table and thrift table

explain
SELECT key, count(1)
FROM (
  SELECT '1' as key from src
  UNION DISTINCT
  SELECT reverse(key) as key from src
  UNION DISTINCT
  SELECT key as key from src
  UNION DISTINCT
  SELECT astring as key from src_thrift
  UNION DISTINCT
  SELECT lstring[0] as key from src_thrift
) union_output
GROUP BY key;

SELECT key, count(1)
FROM (
  SELECT '1' as key from src
  UNION DISTINCT
  SELECT reverse(key) as key from src
  UNION DISTINCT
  SELECT key as key from src
  UNION DISTINCT
  SELECT astring as key from src_thrift
  UNION DISTINCT
  SELECT lstring[0] as key from src_thrift
) union_output
GROUP BY key;
-- union22.q

-- SORT_QUERY_RESULTS

create table dst_union22(k1 string, k2 string, k3 string, k4 string) partitioned by (ds string);
create table dst_union22_delta(k0 string, k1 string, k2 string, k3 string, k4 string, k5 string) partitioned by (ds string);

insert overwrite table dst_union22 partition (ds='1')
select key, value, key , value from src;

insert overwrite table dst_union22_delta partition (ds='1')
select key, key, value, key, value, value from src;

set hive.merge.mapfiles=false;

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- Since the inputs are small, it should be automatically converted to mapjoin

explain extended
insert overwrite table dst_union22 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta where ds = '1' and k0 <= 50
UNION DISTINCT
select a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22 a left outer join (select * from dst_union22_delta where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq;

insert overwrite table dst_union22 partition (ds='2')
select * from
(
select k1 as k1, k2 as k2, k3 as k3, k4 as k4 from dst_union22_delta where ds = '1' and k0 <= 50
UNION DISTINCT
select a.k1 as k1, a.k2 as k2, b.k3 as k3, b.k4 as k4
from dst_union22 a left outer join (select * from dst_union22_delta where ds = '1' and k0 > 50) b on
a.k1 = b.k1 and a.ds='1'
where a.k1 > 20
)
subq;

select * from dst_union22 where ds = '2';
-- union23.q

explain
select s.key2, s.value2
from (
  select transform(key, value) using 'cat' as (key2, value2)
  from src
  UNION DISTINCT
  select key as key2, value as value2 from src) s
order by s.key2, s.value2;

select s.key2, s.value2
from (
  select transform(key, value) using 'cat' as (key2, value2)
  from src
  UNION DISTINCT
  select key as key2, value as value2 from src) s
order by s.key2, s.value2;

-- union24.q

-- SORT_QUERY_RESULTS

create table src2 as select key, count(1) as count from src group by key;
create table src3 as select * from src2;
create table src4 as select * from src2;
create table src5 as select * from src2;


set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;


explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select key, count from src4  where key < 10
  UNION DISTINCT
  select key, count(1) as count from src5 where key < 10 group by key
)s
;

select s.key, s.count from (
  select key, count from src2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select key, count from src4  where key < 10
  UNION DISTINCT
  select key, count(1) as count from src5 where key < 10 group by key
)s
;

explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, b.count as count from src4 a join src5 b on a.key=b.key where a.key < 10
)s
;

select s.key, s.count from (
  select key, count from src2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, b.count as count from src4 a join src5 b on a.key=b.key where a.key < 10
)s
;

explain extended
select s.key, s.count from (
  select key, count from src2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, count(1) as count from src4 a join src5 b on a.key=b.key where a.key < 10 group by a.key
)s
;

select s.key, s.count from (
  select key, count from src2  where key < 10
  UNION DISTINCT
  select key, count from src3  where key < 10
  UNION DISTINCT
  select a.key as key, count(1) as count from src4 a join src5 b on a.key=b.key where a.key < 10 group by a.key
)s
;
-- union25.q

create table tmp_srcpart like srcpart;

insert overwrite table tmp_srcpart partition (ds='2008-04-08', hr='11')
select key, value from srcpart where ds='2008-04-08' and hr='11';

explain
create table tmp_unionall as
SELECT count(1) as counts, key, value
FROM
(
  SELECT key, value FROM srcpart a WHERE a.ds='2008-04-08' and a.hr='11'

    UNION DISTINCT

  SELECT key, key as value FROM (
    SELECT distinct key FROM (
      SELECT key, value FROM tmp_srcpart a WHERE a.ds='2008-04-08' and a.hr='11'
        UNION DISTINCT
      SELECT key, value FROM tmp_srcpart b WHERE b.ds='2008-04-08' and b.hr='11'
    )t
  ) master_table
) a GROUP BY key, value
;
-- union26.q

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key

UNION DISTINCT

select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
;

SELECT
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key

UNION DISTINCT

select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
;


SELECT
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key

UNION DISTINCT

select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
;

SELECT
count(1) as counts,
key,
value
FROM
(

SELECT
a.key, a.value
FROM srcpart a JOIN srcpart b
ON a.ds='2008-04-08' and a.hr='11' and b.ds='2008-04-08' and b.hr='12'
AND a.key = b.key

UNION DISTINCT

select key, value
FROM srcpart LATERAL VIEW explode(array(1,2,3)) myTable AS myCol
WHERE ds='2008-04-08' and hr='11'
) a
group by key, value
;
-- union27.q

-- SORT_BEFORE_DIFF
create table jackson_sev_same as select * from src;
create table dim_pho as select * from src;
create table jackson_sev_add as select * from src;
explain select b.* from jackson_sev_same a join (select * from dim_pho UNION DISTINCT select * from jackson_sev_add)b on a.key=b.key and b.key=97;
select b.* from jackson_sev_same a join (select * from dim_pho UNION DISTINCT select * from jackson_sev_add)b on a.key=b.key and b.key=97;
-- union28.q

create table union_subq_union(key int, value string);

explain
insert overwrite table union_subq_union
select * from (
  select key, value from src
  UNION DISTINCT
  select key, value from
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a
;

insert overwrite table union_subq_union
select * from (
  select key, value from src
  UNION DISTINCT
  select key, value from
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a
;

select * from union_subq_union order by key, value limit 20;
-- union29.q

create table union_subq_union29(key int, value string);

explain
insert overwrite table union_subq_union29
select * from (
  select key, value from src
  UNION DISTINCT
  select key, value from
  (
    select key, value from src
    UNION DISTINCT
    select key, value from src
  ) subq
) a
;

insert overwrite table union_subq_union29
select * from (
  select key, value from src
  UNION DISTINCT
  select key, value from
  (
    select key, value from src
    UNION DISTINCT
    select key, value from src
  ) subq
) a
;

select * from union_subq_union29 order by key, value limit 20;
-- union3.q

-- SORT_BEFORE_DIFF

explain
SELECT *
FROM (
  SELECT 1 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 2 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 3 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  UNION DISTINCT
  SELECT 4 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  CLUSTER BY id
) a;



CREATE TABLE union_out (id int);

insert overwrite table union_out
SELECT *
FROM (
  SELECT 1 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 2 AS id
  FROM (SELECT * FROM src LIMIT 1) s1
  UNION DISTINCT
  SELECT 3 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  UNION DISTINCT
  SELECT 4 AS id
  FROM (SELECT * FROM src LIMIT 1) s2
  CLUSTER BY id
) a;

select * from union_out;
-- union30.q

create table union_subq_union30(key int, value string);

explain
insert overwrite table union_subq_union30
select * from (

select * from (
  select key, value from src
  UNION DISTINCT
  select key, value from
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a

UNION DISTINCT

select key, value from src
) aa
;

insert overwrite table union_subq_union30
select * from (

select * from (
  select key, value from src
  UNION DISTINCT
  select key, value from
  (
    select key, value, count(1) from src group by key, value
    UNION DISTINCT
    select key, value, count(1) from src group by key, value
  ) subq
) a

UNION DISTINCT

select key, value from src
) aa
;

select * from union_subq_union30 order by key, value limit 20;
-- union31.q

-- SORT_QUERY_RESULTS

drop table t1;
drop table t2;


create table t1 as select * from src where key < 10;
create table t2 as select * from src where key < 10;

create table t3(key string, cnt int);
create table t4(value string, cnt int);

explain
from
(select * from t1
 UNION DISTINCT
 select * from t2
) x
insert overwrite table t3
  select key, count(1) group by key
insert overwrite table t4
  select value, count(1) group by value;

from
(select * from t1
 UNION DISTINCT
 select * from t2
) x
insert overwrite table t3
  select key, count(1) group by key
insert overwrite table t4
  select value, count(1) group by value;

select * from t3;
select * from t4;

create table t5(c1 string, cnt int);
create table t6(c1 string, cnt int);

explain
from
(
 select key as c1, count(1) as cnt from t1 group by key
   UNION DISTINCT
 select key as c1, count(1) as cnt from t2 group by key
) x
insert overwrite table t5
  select c1, sum(cnt) group by c1
insert overwrite table t6
  select c1, sum(cnt) group by c1;

from
(
 select key as c1, count(1) as cnt from t1 group by key
   UNION DISTINCT
 select key as c1, count(1) as cnt from t2 group by key
) x
insert overwrite table t5
  select c1, sum(cnt) group by c1
insert overwrite table t6
  select c1, sum(cnt) group by c1;

select * from t5;
select * from t6;

drop table t1;
drop table t2;

create table t1 as select * from src where key < 10;
create table t2 as select key, count(1) as cnt from src where key < 10 group by key;

create table t7(c1 string, cnt int);
create table t8(c1 string, cnt int);

explain
from
(
 select key as c1, count(1) as cnt from t1 group by key
   UNION DISTINCT
 select key as c1, cnt from t2
) x
insert overwrite table t7
  select c1, count(1) group by c1
insert overwrite table t8
  select c1, count(1) group by c1;

from
(
 select key as c1, count(1) as cnt from t1 group by key
   UNION DISTINCT
 select key as c1, cnt from t2
) x
insert overwrite table t7
  select c1, count(1) group by c1
insert overwrite table t8
  select c1, count(1) group by c1;

select * from t7;
select * from t8;
-- union32.q

-- SORT_QUERY_RESULTS

-- This tests various union queries which have columns on one side of the query
-- being of double type and those on the other side another

drop table if exists t1;

drop table if exists t2;

CREATE TABLE t1 AS SELECT * FROM src WHERE key < 10;
CREATE TABLE t2 AS SELECT * FROM src WHERE key < 10;

-- Test simple union with double
EXPLAIN
SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key FROM t1
UNION DISTINCT
SELECT CAST(key AS BIGINT) AS key FROM t2) a
ORDER BY key;

SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key FROM t1
UNION DISTINCT
SELECT CAST(key AS BIGINT) AS key FROM t2) a
;

-- Test union with join on the left
EXPLAIN
SELECT * FROM
(SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key FROM t2) a
;

SELECT * FROM
(SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key FROM t2) a
;

-- Test union with join on the right
EXPLAIN
SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key FROM t2
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key) a
;

SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key FROM t2
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key FROM t1 a JOIN t2 b ON a.key = b.key) a
;

-- Test union with join on the left selecting multiple columns
EXPLAIN
SELECT * FROM
(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
;

SELECT * FROM
(SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key
UNION DISTINCT
SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2) a
;

-- Test union with join on the right selecting multiple columns
EXPLAIN
SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
;

SELECT * FROM
(SELECT CAST(key AS DOUBLE) AS key, CAST(key AS STRING) AS value FROM t2
UNION DISTINCT
SELECT CAST(a.key AS BIGINT) AS key, CAST(b.key AS DOUBLE) AS value FROM t1 a JOIN t2 b ON a.key = b.key) a
;
-- union33.q

-- SORT_BEFORE_DIFF
-- This tests that a UNION DISTINCT with a map only subquery on one side and a
-- subquery involving two map reduce jobs on the other runs correctly.

drop table if exists test_src;

CREATE TABLE test_src (key STRING, value STRING);

EXPLAIN INSERT OVERWRITE TABLE test_src
SELECT key, value FROM (
	SELECT key, value FROM src
	WHERE key = 0
UNION DISTINCT
 	SELECT key, COUNT(*) AS value FROM src
 	GROUP BY key
)a;

INSERT OVERWRITE TABLE test_src
SELECT key, value FROM (
	SELECT key, value FROM src
	WHERE key = 0
UNION DISTINCT
 	SELECT key, COUNT(*) AS value FROM src
 	GROUP BY key
)a;

SELECT COUNT(*) FROM test_src;

EXPLAIN INSERT OVERWRITE TABLE test_src
SELECT key, value FROM (
	SELECT key, COUNT(*) AS value FROM src
 	GROUP BY key
UNION DISTINCT
 	SELECT key, value FROM src
	WHERE key = 0
)a;

INSERT OVERWRITE TABLE test_src
SELECT key, value FROM (
	SELECT key, COUNT(*) AS value FROM src
 	GROUP BY key
UNION DISTINCT
 	SELECT key, value FROM src
	WHERE key = 0
)a;

SELECT COUNT(*) FROM test_src;
 -- union34.q

create table src10_1 (key string, value string);
create table src10_2 (key string, value string);
create table src10_3 (key string, value string);
create table src10_4 (key string, value string);

from (select * from src tablesample (10 rows)) a
insert overwrite table src10_1 select *
insert overwrite table src10_2 select *
insert overwrite table src10_3 select *
insert overwrite table src10_4 select *;

analyze table src10_1 compute statistics;
analyze table src10_2 compute statistics;
analyze table src10_3 compute statistics;
analyze table src10_4 compute statistics;

set hive.auto.convert.join=true;
-- When we convert the Join of sub1 and sub0 into a MapJoin,
-- we can use a single MR job to evaluate this entire query.
explain
SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1 order by key;

SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1 order by key;

set hive.auto.convert.join=false;
-- When we do not convert the Join of sub1 and sub0 into a MapJoin,
-- we need to use two MR jobs to evaluate this query.
-- The first job is for the Join of sub1 and sub2. The second job
-- is for the UNION DISTINCT and ORDER BY.
explain
SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1 order by key;

SELECT * FROM (
  SELECT sub1.key,sub1.value FROM (SELECT * FROM src10_1) sub1 JOIN (SELECT * FROM src10_2) sub0 ON (sub0.key = sub1.key)
  UNION DISTINCT
  SELECT key,value FROM (SELECT * FROM (SELECT * FROM src10_3) sub2 UNION DISTINCT SELECT * FROM src10_4 ) alias0
) alias1 order by key;
-- union4.q



-- union case: both subqueries are map-reduce jobs on same input, followed by filesink

drop table if exists tmptable;

create table tmptable(key string, value int);

explain
insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION DISTINCT
                                            select 'tst2' as key, count(1) as value from src s2) unionsrc;

insert overwrite table tmptable
select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, count(1) as value from src s1
                                        UNION DISTINCT
                                          select 'tst2' as key, count(1) as value from src s2) unionsrc;

select * from tmptable x sort by x.key;


-- union5.q


-- SORT_BEFORE_DIFF
-- union case: both subqueries are map-reduce jobs on same input, followed by reduce sink

explain
  select unionsrc.key, count(1) FROM (select 'tst1' as key, count(1) as value from src s1
                                    UNION DISTINCT
                                      select 'tst2' as key, count(1) as value from src s2) unionsrc group by unionsrc.key;

select unionsrc.key, count(1) FROM (select 'tst1' as key, count(1) as value from src s1
                                  UNION DISTINCT
                                    select 'tst2' as key, count(1) as value from src s2) unionsrc group by unionsrc.key;
-- union6.q



-- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by filesink

drop table if exists tmptable;

create table tmptable(key string, value string);

explain
insert overwrite table tmptable
  select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                        UNION DISTINCT
                                            select s2.key as key, s2.value as value from src1 s2) unionsrc;

insert overwrite table tmptable
select unionsrc.key, unionsrc.value FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                      UNION DISTINCT
                                          select s2.key as key, s2.value as value from src1 s2) unionsrc;

select * from tmptable x sort by x.key, x.value;


-- union7.q



-- SORT_BEFORE_DIFF
-- union case: 1 subquery is a map-reduce job, different inputs for sub-queries, followed by reducesink

explain
  select unionsrc.key, count(1) FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                        UNION DISTINCT
                                            select s2.key as key, s2.value as value from src1 s2) unionsrc group by unionsrc.key;

select unionsrc.key, count(1) FROM (select 'tst1' as key, cast(count(1) as string) as value from src s1
                                      UNION DISTINCT
                                    select s2.key as key, s2.value as value from src1 s2) unionsrc group by unionsrc.key;


-- union8.q

-- SORT_BEFORE_DIFF
-- union case: all subqueries are a map-only jobs, 3 way union, same input for all sub-queries, followed by filesink

explain
  select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT
                                            select s2.key as key, s2.value as value from src s2 UNION DISTINCT
                                            select s3.key as key, s3.value as value from src s3) unionsrc;

select unionsrc.key, unionsrc.value FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT
                                          select s2.key as key, s2.value as value from src s2 UNION DISTINCT
                                          select s3.key as key, s3.value as value from src s3) unionsrc;
-- union9.q

-- SORT_BEFORE_DIFF
-- union case: all subqueries are a map-only jobs, 3 way union, same input for all sub-queries, followed by reducesink

explain
  select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT
                        select s2.key as key, s2.value as value from src s2 UNION DISTINCT
                        select s3.key as key, s3.value as value from src s3) unionsrc;

  select count(1) FROM (select s1.key as key, s1.value as value from src s1 UNION DISTINCT
                        select s2.key as key, s2.value as value from src s2 UNION DISTINCT
                        select s3.key as key, s3.value as value from src s3) unionsrc;
-- SORT_QUERY_RESULTS

CREATE TABLE u1 as select key, value from src order by key limit 5;

CREATE TABLE u2 as select key, value from src order by key limit 3;

CREATE TABLE u3 as select key, value from src order by key desc limit 5;

select * from u1;

select * from u2;

select * from u3;

select key, value from
(
select key, value from u1
union all
select key, value from u2
union all
select key as key, value from u3
) tab;

select key, value from
(
select key, value from u1
union
select key, value from u2
union all
select key, value from u3
) tab;

select key, value from
(
select key, value from u1
union distinct
select key, value from u2
union all
select key as key, value from u3
) tab;

select key, value from
(
select key, value from u1
union all
select key, value from u2
union
select key, value from u3
) tab;

select key, value from
(
select key, value from u1
union
select key, value from u2
union
select key as key, value from u3
) tab;

select distinct * from
(
select key, value from u1
union all
select key, value from u2
union all
select key as key, value from u3
) tab;

select distinct * from
(
select distinct * from u1
union
select key, value from u2
union all
select key as key, value from u3
) tab;

drop view if exists v;

create view v as select distinct * from
(
select distinct * from u1
union
select key, value from u2
union all
select key as key, value from u3
) tab;

describe extended v;

select * from v;

drop view if exists v;

create view v as select tab.* from
(
select distinct * from u1
union
select distinct * from u2
) tab;

describe extended v;

select * from v;

drop view if exists v;

create view v as select * from
(
select distinct u1.* from u1
union all
select distinct * from u2
) tab;

describe extended v;

select * from v;

select distinct * from
(
select key, value from u1
union all
select key, value from u2
union
select key as key, value from u3
) tab;

-- orderByClause clusterByClause distributeByClause sortByClause limitClause
-- can only be applied to the whole union.

select key from src distribute by key
union all
select key from src;




-- orderByClause clusterByClause distributeByClause sortByClause limitClause
-- can only be applied to the whole union.

select key from src limit 1
union all
select key from src;




-- orderByClause clusterByClause distributeByClause sortByClause limitClause
-- can only be applied to the whole union.

select key from src order by key
union all
select key from src;



-- orderByClause clusterByClause distributeByClause sortByClause limitClause
-- can only be applied to the whole union.

select key from src sort by key
union all
select key from src;


-- SORT_QUERY_RESULTS

drop table union_date_1;
drop table union_date_2;

create table union_date_1 (
  ORIGIN_CITY_NAME string,
  DEST_CITY_NAME string,
  FL_DATE date,
  ARR_DELAY float,
  FL_NUM int
);

create table union_date_2 (
  ORIGIN_CITY_NAME string,
  DEST_CITY_NAME string,
  FL_DATE date,
  ARR_DELAY float,
  FL_NUM int
);

LOAD DATA LOCAL INPATH '../../data/files/flights_join.txt' OVERWRITE INTO TABLE union_date_1;
LOAD DATA LOCAL INPATH '../../data/files/flights_join.txt' OVERWRITE INTO TABLE union_date_2;

select * from (
  select fl_num, fl_date from union_date_1
  union all
  select fl_num, fl_date from union_date_2
) union_result;

drop table union_date_1;
drop table union_date_2;


drop table if exists testDate;
create table testDate(id int, dt date);
insert into table testDate select 1, '2014-04-07' from src where key=100 limit 1;
insert into table testDate select 2, '2014-04-08' from src where key=100 limit 1;
insert into table testDate select 3, '2014-04-09' from src where key=100 limit 1;
--- without the fix following query will throw HiveException: Incompatible types for union operator
insert into table testDate select id, tm from (select id, dt as tm from testDate where id = 1 union all select id, dt as tm from testDate where id = 2 union all select id, trim(Cast (dt as string)) as tm from testDate where id = 3 ) a;
set hive.stats.dbclass=fs;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set hive.merge.tezfiles=false;

drop table small_alltypesorc1a;
drop table small_alltypesorc2a;
drop table small_alltypesorc3a;
drop table small_alltypesorc4a;
drop table small_alltypesorc_a;

create table small_alltypesorc1a as select * from alltypesorc where cint is not null and ctinyint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc2a as select * from alltypesorc where cint is null and ctinyint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc3a as select * from alltypesorc where cint is not null and ctinyint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc4a as select * from alltypesorc where cint is null and ctinyint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;

create table small_alltypesorc_a stored as orc as select * from
(select * from (select * from small_alltypesorc1a) sq1
 union all
 select * from (select * from small_alltypesorc2a) sq2
 union all
 select * from (select * from small_alltypesorc3a) sq3
 union all
 select * from (select * from small_alltypesorc4a) sq4) q;

desc formatted small_alltypesorc_a;

ANALYZE TABLE small_alltypesorc_a COMPUTE STATISTICS;

desc formatted small_alltypesorc_a;

insert into table small_alltypesorc_a select * from small_alltypesorc1a;

desc formatted small_alltypesorc_a;

set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.tezfiles=true;

drop table small_alltypesorc1a;
drop table small_alltypesorc2a;
drop table small_alltypesorc3a;
drop table small_alltypesorc4a;
drop table small_alltypesorc_a;

create table small_alltypesorc1a as select * from alltypesorc where cint is not null and ctinyint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc2a as select * from alltypesorc where cint is null and ctinyint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc3a as select * from alltypesorc where cint is not null and ctinyint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc4a as select * from alltypesorc where cint is null and ctinyint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;

create table small_alltypesorc_a stored as orc as select * from
(select * from (select * from small_alltypesorc1a) sq1
 union all
 select * from (select * from small_alltypesorc2a) sq2
 union all
 select * from (select * from small_alltypesorc3a) sq3
 union all
 select * from (select * from small_alltypesorc4a) sq4) q;

desc formatted small_alltypesorc_a;

ANALYZE TABLE small_alltypesorc_a COMPUTE STATISTICS;

desc formatted small_alltypesorc_a;

insert into table small_alltypesorc_a select * from small_alltypesorc1a;

desc formatted small_alltypesorc_a;

create table test_union_lateral_view(key int, arr_ele int, value string);

EXPLAIN
INSERT OVERWRITE TABLE test_union_lateral_view
SELECT b.key, d.arr_ele, d.value
FROM (
 SELECT c.arr_ele as arr_ele, a.key as key, a.value as value
 FROM (
   SELECT key, value, array(1,2,3) as arr
   FROM src

   UNION ALL

   SELECT key, value, array(1,2,3) as arr
   FROM srcpart
   WHERE ds = '2008-04-08' and hr='12'
 ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele
) d
LEFT OUTER JOIN src b
ON d.key = b.key
;

INSERT OVERWRITE TABLE test_union_lateral_view
SELECT b.key, d.arr_ele, d.value
FROM (
 SELECT c.arr_ele as arr_ele, a.key as key, a.value as value
 FROM (
   SELECT key, value, array(1,2,3) as arr
   FROM src

   UNION ALL

   SELECT key, value, array(1,2,3) as arr
   FROM srcpart
   WHERE ds = '2008-04-08' and hr='12'
 ) a LATERAL VIEW EXPLODE(arr) c AS arr_ele
) d
LEFT OUTER JOIN src b
ON d.key = b.key
;

select key, arr_ele, value from test_union_lateral_view order by key, arr_ele limit 20;
-- SORT_BEFORE_DIFF

-- HIVE-2901
select x from (select * from (select value as x from src order by x limit 5)a union all select * from (select NULL as x from src limit 5)b )a;

-- HIVE-4837
select * from (select * from (select null as N from src1 group by key)a UNION ALL select * from (select null as N from src1 group by key)b ) a;
-- SORT_QUERY_RESULTS

EXPLAIN EXTENDED
SELECT * FROM (
  SELECT X.* FROM SRCPART X WHERE X.key < 100
  UNION ALL
  SELECT Y.* FROM SRCPART Y WHERE Y.key < 100
) A
WHERE A.ds = '2008-04-08'
SORT BY A.key, A.value, A.ds, A.hr;

SELECT * FROM (
  SELECT X.* FROM SRCPART X WHERE X.key < 100
  UNION ALL
  SELECT Y.* FROM SRCPART Y WHERE Y.key < 100
) A
WHERE A.ds = '2008-04-08'
SORT BY A.key, A.value, A.ds, A.hr;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 subqueries is performed (one of which is a map-only query, and the
-- other one contains a nested union where one of the sub-queries requires a map-reduce
-- job), followed by select star and a file sink.
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The outer union can be removed completely.
-- The final file format is different from the input and intermediate file format.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT * FROM
(
select key, 1 as `values` from inputTbl1
union all
select * FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a
)b;

insert overwrite table outputTbl1
SELECT * FROM
(
select key, 1 as `values` from inputTbl1
union all
select * FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a
)b;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 subqueries is performed (one of which is a map-only query, and the
-- other one contains a nested union where also contains map only sub-queries),
-- followed by select star and a file sink.
-- There is no need for the union optimization, since the whole query can be performed
-- in a single map-only job
-- The final file format is different from the input and intermediate file format.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT * FROM
(
select key, 1 as `values` from inputTbl1
union all
select * FROM (
  SELECT key, 2 `values` from inputTbl1
  UNION ALL
  SELECT key, 3 as `values` from inputTbl1
) a
)b;

insert overwrite table outputTbl1
SELECT * FROM
(
select key, 1 as `values` from inputTbl1
union all
select * FROM (
  SELECT key, 2 as `values` from inputTbl1
  UNION ALL
  SELECT key, 3 as `values` from inputTbl1
) a
)b;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;
set hive.auto.convert.join=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 subqueries is performed (one of which is a map-only query, and the
-- other one is a map-join query), followed by select star and a file sink.
-- The union optimization is applied, and the union is removed.

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

-- The final file format is different from the input and intermediate file format.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT * FROM
(
select key, 1 as `values` from inputTbl1
union all
select a.key as key, b.val as `values`
FROM inputTbl1 a join inputTbl1 b on a.key=b.key
)c;

insert overwrite table outputTbl1
SELECT * FROM
(
select key, 1 as `values` from inputTbl1
union all
select a.key as key, b.val as `values`
FROM inputTbl1 a join inputTbl1 b on a.key=b.key
)c;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;
set hive.auto.convert.join=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 subqueries is performed (one of which is a mapred query, and the
-- other one is a map-join query), followed by select star and a file sink.
-- The union selectstar optimization should be performed, and the union should be removed.

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

-- The final file format is different from the input and intermediate file format.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT * FROM
(
select key, count(1) as `values` from inputTbl1 group by key
union all
select a.key as key, b.val as `values`
FROM inputTbl1 a join inputTbl1 b on a.key=b.key
)c;

insert overwrite table outputTbl1
SELECT * FROM
(
select key, count(1) as `values` from inputTbl1 group by key
union all
select a.key as key, b.val as `values`
FROM inputTbl1 a join inputTbl1 b on a.key=b.key
)c;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.auto.convert.join=true;
set hive.merge.smallfiles.avgsize=1;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 subqueries is performed (one of which is a map-only query, and the
-- other one contains a join, which should be performed as a map-join query at runtime),
-- followed by select star and a file sink.
-- The union selectstar optimization should be performed, and the union should be removed.

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

-- The final file format is different from the input and intermediate file format.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT * FROM
(
select key, 1 as `values` from inputTbl1
union all
select a.key as key, b.val as `values`
FROM inputTbl1 a join inputTbl1 b on a.key=b.key
)c;

insert overwrite table outputTbl1
SELECT * FROM
(
select key, 1 as `values` from inputTbl1
union all
select a.key as key, b.val as `values`
FROM inputTbl1 a join inputTbl1 b on a.key=b.key
)c;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- and the results are written to a table using dynamic partitions.
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- This tests demonstrates that this optimization works in the presence of dynamic partitions.

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) partitioned by (ds string) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1 partition (ds)
SELECT *
FROM (
  SELECT key, count(1) as `values`, '1' as ds from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values`, '2' as ds from inputTbl1 group by key
) a;

insert overwrite table outputTbl1 partition (ds)
SELECT *
FROM (
  SELECT key, count(1) as `values`, '1' as ds from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values`, '2' as ds from inputTbl1 group by key
) a;

desc formatted outputTbl1;

show partitions outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 where ds = '1' order by key, `values`;
select * from outputTbl1 where ds = '2' order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1;
set mapred.input.dir.recursive=true;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- and the results are written to a table using dynamic partitions.
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on
-- This test demonstrates that this optimization works in the presence of dynamic partitions.
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) partitioned by (ds string) stored as rcfile ;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1 partition (ds)
SELECT *
FROM (
  SELECT key, count(1) as `values`, '1' as ds from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values`, '2' as ds from inputTbl1 group by key
) a;

insert overwrite table outputTbl1 partition (ds)
SELECT *
FROM (
  SELECT key, count(1) as `values`, '1' as ds from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values`, '2' as ds from inputTbl1 group by key
) a;

desc formatted outputTbl1;
show partitions outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 where ds = '1' order by key, `values`;
select * from outputTbl1 where ds = '2' order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- and the results are written to a table using dynamic partitions.
-- There is no need for this optimization, since the query is a map-only query.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) partitioned by (ds string) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1 partition (ds)
SELECT *
FROM (
  SELECT key, 1 as `values`, '1' as ds from inputTbl1
  UNION ALL
  SELECT key, 2 as `values`, '2' as ds from inputTbl1
) a;

insert overwrite table outputTbl1 partition (ds)
SELECT *
FROM (
  SELECT key, 1 as `values`, '1' as ds from inputTbl1
  UNION ALL
  SELECT key, 2 as `values`, '2' as ds from inputTbl1
) a;

desc formatted outputTbl1;
show partitions outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 where ds = '1' order by key, `values`;
select * from outputTbl1 where ds = '2' order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- This test demonstrates that the optimization works with dynamic partitions irrespective of the
-- file format of the output file
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, ds string) stored as textfile;
create table outputTbl1(key string, `values` bigint) partitioned by (ds string) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1 partition (ds)
SELECT *
FROM (
  SELECT key, count(1) as `values`, ds from inputTbl1 group by key, ds
  UNION ALL
  SELECT key, count(1) as `values`, ds from inputTbl1 group by key, ds
) a;

insert overwrite table outputTbl1 partition (ds)
SELECT *
FROM (
  SELECT key, count(1) as `values`, ds from inputTbl1 group by key, ds
  UNION ALL
  SELECT key, count(1) as `values`, ds from inputTbl1 group by key, ds
) a;

desc formatted outputTbl1;

show partitions outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 where ds = '11' order by key, `values`;
select * from outputTbl1 where ds = '18' order by key, `values`;
select * from outputTbl1 where ds is not null order by key, `values`, ds;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

-- SORT_QUERY_RESULTS

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT a.key, a.`values`
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1
SELECT a.key, a.`values`
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

desc formatted outputTbl1;

select * from outputTbl1;

-- filter should be fine
explain
insert overwrite table outputTbl1
SELECT a.key, a.`values`
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a where a.key = 7;

insert overwrite table outputTbl1
SELECT a.key, a.`values`
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a where a.key = 7;

select * from outputTbl1;

-- filters and sub-queries should be fine
explain
insert overwrite table outputTbl1
select key, `values` from
(
SELECT a.key + a.key as key, a.`values`
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a
) b where b.key >= 7;

insert overwrite table outputTbl1
select key, `values` from
(
SELECT a.key + a.key as key, a.`values`
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a
) b where b.key >= 7;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 3 subqueries is performed (exactly one of which requires a map-reduce job)
-- followed by select star and a file sink.
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a;

insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;

set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select and a file sink
-- However, the order of the columns in the select list is different. So, union cannot
-- be removed.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23. The union is removed, the select (which changes the order of
-- columns being selected) is pushed above the union.

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(`values` bigint, key string) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT a.`values`, a.key
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1
SELECT a.`values`, a.key
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select and a file sink
-- However, all the columns are not selected. So, union cannot
-- be removed.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23. The union is removed, the select (which changes the order of
-- columns being selected) is pushed above the union.

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT a.key
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1
SELECT a.key
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- SORT_QUERY_RESULTS

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select and a file sink
-- However, some columns are repeated. So, union cannot be removed.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23. The union is removed, the select (which selects columns from
-- both the sub-qeuries of the union) is pushed above the union.

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint, values2 bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT a.key, a.`values`, a.`values`
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1
SELECT a.key, a.`values`, a.`values`
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

desc formatted outputTbl1;

select * from outputTbl1;

explain
insert overwrite table outputTbl1
SELECT a.key, concat(a.`values`, a.`values`), concat(a.`values`, a.`values`)
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1
SELECT a.key, concat(a.`values`, a.`values`), concat(a.`values`, a.`values`)
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely. One of the sub-queries
-- would have multiple map-reduce jobs.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from
  (SELECT a.key, b.val from inputTbl1 a join inputTbl1 b on a.key=b.key) subq group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) subq2;

insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from
  (SELECT a.key, b.val from inputTbl1 a join inputTbl1 b on a.key=b.key) subq group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) subq2;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- One sub-query has a double and the other sub-query has a bigint.
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key double, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

EXPLAIN
INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM
(
  SELECT CAST(key AS DOUBLE) AS key, count(1) as `values` FROM inputTbl1 group by key
  UNION ALL
  SELECT CAST(key AS BIGINT) AS key, count(1) as `values` FROM inputTbl1 group by key
) a;

INSERT OVERWRITE TABLE outputTbl1
SELECT * FROM
(
  SELECT CAST(key AS DOUBLE) AS key, count(1) as `values` FROM inputTbl1 group by key
  UNION ALL
  SELECT CAST(key AS BIGINT) AS key, count(1) as `values` FROM inputTbl1 group by key
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) partitioned by (ds string) stored as textfile;
create table outputTbl2(key string, `values` bigint) partitioned by (ds string) stored as textfile;
create table outputTbl3(key string, `values` bigint) partitioned by (ds string,hr string) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1 partition(ds='2004')
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1 partition(ds='2004')
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

desc formatted outputTbl1 partition(ds='2004');

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;

explain
insert overwrite table outputTbl2 partition(ds)
SELECT *
FROM (
  select * from (SELECT key, value, ds from srcpart where ds='2008-04-08' limit 500)a
  UNION ALL
  select * from (SELECT key, value, ds from srcpart where ds='2008-04-08' limit 500)b
) a;

insert overwrite table outputTbl2 partition(ds)
SELECT *
FROM (
  select * from (SELECT key, value, ds from srcpart where ds='2008-04-08' limit 500)a
  UNION ALL
  select * from (SELECT key, value, ds from srcpart where ds='2008-04-08' limit 500)b
) a;

show partitions outputTbl2;
desc formatted outputTbl2 partition(ds='2008-04-08');

explain insert overwrite table outputTbl3 partition(ds, hr)
SELECT *
FROM (
  select * from (SELECT key, value, ds, hr from srcpart where ds='2008-04-08' limit 1000)a
  UNION ALL
  select * from (SELECT key, value, ds, hr from srcpart where ds='2008-04-08' limit 1000)b
) a;

insert overwrite table outputTbl3 partition(ds, hr)
SELECT *
FROM (
  select * from (SELECT key, value, ds, hr from srcpart where ds='2008-04-08' limit 1000)a
  UNION ALL
  select * from (SELECT key, value, ds, hr from srcpart where ds='2008-04-08' limit 1000)b
) a;

show partitions outputTbl3;
desc formatted outputTbl3 partition(ds='2008-04-08', hr='11');
set hive.stats.autogather=true;

-- This is to test the union remove optimization with stats optimization

create table inputSrcTbl1(key string, val int) stored as textfile;
create table inputSrcTbl2(key string, val int) stored as textfile;
create table inputSrcTbl3(key string, val int) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputSrcTbl1;
load data local inpath '../../data/files/T2.txt' into table inputSrcTbl2;
load data local inpath '../../data/files/T3.txt' into table inputSrcTbl3;

create table inputTbl1(key string, val int) stored as textfile;
create table inputTbl2(key string, val int) stored as textfile;
create table inputTbl3(key string, val int) stored as textfile;

insert into inputTbl1 select * from inputSrcTbl1;
insert into inputTbl2 select * from inputSrcTbl2;
insert into inputTbl3 select * from inputSrcTbl3;

set hive.compute.query.using.stats=true;
set hive.optimize.union.remove=true;
set mapred.input.dir.recursive=true;

--- union remove optimization effects, stats optimization does not though it is on since inputTbl2 column stats is not available
analyze table inputTbl1 compute statistics for columns;
analyze table inputTbl3 compute statistics for columns;
explain
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3;


select count(*) from (
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3) t;

--- union remove optimization and stats optimization are effective after inputTbl2 column stats is calculated
analyze table inputTbl2 compute statistics for columns;
explain
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3;


select count(*) from (
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3) t;

--- union remove optimization effects but stats optimization does not (with group by) though it is on
explain
  SELECT key, count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2 group by key
  UNION ALL
  SELECT key, count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3 group by key;

select count(*) from (
  SELECT key, count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2 group by key
  UNION ALL
  SELECT key, count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3 group by key) t;


set hive.compute.query.using.stats=false;
set hive.optimize.union.remove=true;
set mapred.input.dir.recursive=true;

explain
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3;

select count(*) from (
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3) t;


set hive.compute.query.using.stats=false;
set hive.optimize.union.remove=false;

explain
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3;


select count(*) from (
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl1
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl2
  UNION ALL
  SELECT count(1) as rowcnt, min(val) as ms, max(val) as mx from inputTbl3) t;set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->remove->filesink optimization
-- Union of 3 subqueries is performed (all of which are map-only queries)
-- followed by select star and a file sink.
-- There is no need for any optimization, since the whole query can be processed in
-- a single map-only job
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
  UNION ALL
  SELECT key, 3 as `values` from inputTbl1
) a;

insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
  UNION ALL
  SELECT key, 3 as `values` from inputTbl1
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;

set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set mapred.input.dir.recursive=true;
set hive.merge.smallfiles.avgsize=1;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on
-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 3 subqueries is performed (exactly one of which requires a map-reduce job)
-- followed by select star and a file sink.
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a;

insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 subqueries is performed (all of which are mapred queries)
-- followed by select star and a file sink in 2 output tables.
-- The optimiaztion does not take affect since it is a multi-table insert.
-- It does not matter, whether the output is merged or not. In this case,
-- merging is turned off

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;
create table outputTbl2(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a
insert overwrite table outputTbl1 select *
insert overwrite table outputTbl2 select *;

FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a
insert overwrite table outputTbl1 select *
insert overwrite table outputTbl2 select *;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
select * from outputTbl2 order by key, `values`;;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- SORT_QUERY_RESULTS

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 subqueries is performed (all of which are mapred queries)
-- followed by select star and a file sink in 2 output tables.
-- The optimiaztion does not take affect since it is a multi-table insert.
-- It does not matter, whether the output is merged or not. In this case,
-- merging is turned off

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as textfile;
create table outputTbl2(key string, `values` bigint) stored as textfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
FROM (
  select * from(
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
  )subq
) a
insert overwrite table outputTbl1 select *
insert overwrite table outputTbl2 select *;

FROM (
  select * from(
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
  )subq
) a
insert overwrite table outputTbl1 select *
insert overwrite table outputTbl2 select *;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
select * from outputTbl2 order by key, `values`;

-- The following queries guarantee the correctness.
explain
select avg(c) from(
  SELECT count(1)-200 as c from src
  UNION ALL
  SELECT count(1) as c from src
)subq;

select avg(c) from(
  SELECT count(1)-200 as c from src
  UNION ALL
  SELECT count(1) as c from src
)subq;

explain
select key, avg(c) over w from(
  SELECT key, count(1)*2 as c from src group by key
  UNION ALL
  SELECT key, count(1) as c from src group by key
)subq group by key, c
WINDOW w AS (PARTITION BY key ORDER BY c ROWS UNBOUNDED PRECEDING);

select key, avg(c) over w from(
  SELECT key, count(1)*2 as c from src group by key
  UNION ALL
  SELECT key, count(1) as c from src group by key
)subq group by key, c
WINDOW w AS (PARTITION BY key ORDER BY c ROWS UNBOUNDED PRECEDING);


set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 map-reduce subqueries is performed followed by select star and a file sink
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- The final file format is different from the input and intermediate file format.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, count(1) as `values` from inputTbl1 group by key
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=false;
set hive.merge.mapfiles=false;
set hive.merge.mapredfiles=false;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 3 subqueries is performed (exactly one of which requires a map-reduce job)
-- followed by select star and a file sink.
-- There is no need to write the temporary results of the sub-queries, and then read them
-- again to process the union. The union can be removed completely.
-- The final file format is different from the input and intermediate file format.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- off

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a;

insert overwrite table outputTbl1
SELECT *
FROM (
  SELECT key, count(1) as `values` from inputTbl1 group by key
  UNION ALL
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
set hive.stats.autogather=false;
set hive.optimize.union.remove=true;

set hive.merge.sparkfiles=true;
set hive.merge.mapfiles=true;
set hive.merge.mapredfiles=true;
set hive.merge.smallfiles.avgsize=1;
set mapred.input.dir.recursive=true;

-- This is to test the union->selectstar->filesink optimization
-- Union of 2 subqueries is performed (one of which contains a union and is map-only),
-- and the other one is a map-reduce query followed by select star and a file sink.
-- There is no need for the outer union.
-- The final file format is different from the input and intermediate file format.
-- It does not matter, whether the output is merged or not. In this case, merging is turned
-- on

-- INCLUDE_HADOOP_MAJOR_VERSIONS(0.23)
-- Since this test creates sub-directories for the output table outputTbl1, it might be easier
-- to run the test only on hadoop 23

create table inputTbl1(key string, val string) stored as textfile;
create table outputTbl1(key string, `values` bigint) stored as rcfile;

load data local inpath '../../data/files/T1.txt' into table inputTbl1;

explain
insert overwrite table outputTbl1
SELECT * FROM
(
select key, count(1) as `values` from inputTbl1 group by key
union all
select * FROM (
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a
)b;

insert overwrite table outputTbl1
SELECT * FROM
(
select key, count(1) as `values` from inputTbl1 group by key
union all
select * FROM (
  SELECT key, 1 as `values` from inputTbl1
  UNION ALL
  SELECT key, 2 as `values` from inputTbl1
) a
)b;

desc formatted outputTbl1;

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
select * from outputTbl1 order by key, `values`;
set hive.mapred.mode=nonstrict;
select * from (
  select transform(key) using 'cat' as cola from src)s order by cola;

select * from (
  select transform(key) using 'cat' as cola from src
  union all
  select transform(key) using 'cat' as cola from src) s order by cola;
set hive.mapred.mode=nonstrict;
-- SORT_QUERY_RESULTS

-- top level
explain
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

explain
select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)a
union all
select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)b;

select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)a
union all
select * from (select s1.key as k, s2.value as v from src s1 join src s2 on (s1.key = s2.key) limit 10)b;

-- ctas
explain
create table union_top as
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

create table union_top as
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

select * from union_top;

truncate table union_top;

-- insert into
explain
insert into table union_top
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

insert into table union_top
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

select * from union_top;

explain
insert overwrite table union_top
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

insert overwrite table union_top
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

select * from union_top;

-- create view
explain
create view union_top_view as
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

create view union_top_view as
select * from (select key, 0 as value from src where key % 3 == 0 limit 3)a
union all
select * from (select key, 1 as value from src where key % 3 == 1 limit 3)b
union all
select * from (select key, 2 as value from src where key % 3 == 2 limit 3)c;

select * from union_top_view;

drop table union_top;
drop view union_top_view;
set hive.mapred.mode=nonstrict;
set hive.cbo.enable=false;
set hive.execution.engine=mr;
set hive.mapred.mode=nonstrict;

select (x/sum(x) over())  as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select '100000000' x from (select * from src limit 2) s3)u order by y;

select (x/sum(x) over()) as y from(select cast(1 as decimal(10,0))  as x from (select * from src limit 2)s1 union all select cast(1 as decimal(10,0)) x from (select * from src limit 2) s2 union all select cast (null as string) x from (select * from src limit 2) s3)u order by y;
set hive.mapred.mode=nonstrict;
set hive.stats.dbclass=fs;
set hive.explain.user=false;

CREATE TABLE src_union_1 (key int, value string) PARTITIONED BY (ds string);
CREATE INDEX src_union_1_key_idx ON TABLE src_union_1(key) AS 'COMPACT' WITH DEFERRED REBUILD;

CREATE TABLE src_union_2 (key int, value string) PARTITIONED BY (ds string, part_1 string);
CREATE INDEX src_union_2_key_idx ON TABLE src_union_2(key) AS 'COMPACT' WITH DEFERRED REBUILD;

CREATE TABLE src_union_3(key int, value string) PARTITIONED BY (ds string, part_1 string, part_2 string);
CREATE INDEX src_union_3_key_idx ON TABLE src_union_3(key) AS 'COMPACT' WITH DEFERRED REBUILD;

SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;

SET hive.optimize.index.filter=true;
SET hive.optimize.index.filter.compact.minsize=0;

SET hive.exec.pre.hooks=;
SET hive.exec.post.hooks=;
SET hive.semantic.analyzer.hook=;
SET hive.merge.mapfiles=false;
SET hive.merge.mapredfiles=false;

INSERT OVERWRITE TABLE src_union_1 PARTITION (ds='1') SELECT * FROM src;
ALTER INDEX src_union_1_key_idx ON src_union_1 PARTITION (ds='1') REBUILD;

INSERT OVERWRITE TABLE src_union_2 PARTITION (ds='2', part_1='1') SELECT * FROM src;
INSERT OVERWRITE TABLE src_union_2 PARTITION (ds='2', part_1='2') SELECT * FROM src;
ALTER INDEX src_union_2_key_idx ON src_union_2 PARTITION (ds='2', part_1='1') REBUILD;
ALTER INDEX src_union_2_key_idx ON src_union_2 PARTITION (ds='2', part_1='2') REBUILD;

INSERT OVERWRITE TABLE src_union_3 PARTITION (ds='3', part_1='1', part_2='2:3+4') SELECT * FROM src;
INSERT OVERWRITE TABLE src_union_3 PARTITION (ds='3', part_1='2', part_2='2:3+4') SELECT * FROM src;
ALTER INDEX src_union_3_key_idx ON src_union_3 PARTITION (ds='3', part_1='1', part_2='2:3+4') REBUILD;
ALTER INDEX src_union_3_key_idx ON src_union_3 PARTITION (ds='3', part_1='2', part_2='2:3+4') REBUILD;

EXPLAIN SELECT key, value, ds FROM src_union_1 WHERE key=86 and ds='1';
EXPLAIN SELECT key, value, ds FROM src_union_2 WHERE key=86 and ds='2';
EXPLAIN SELECT key, value, ds FROM src_union_3 WHERE key=86 and ds='3';

SELECT key, value, ds FROM src_union_1 WHERE key=86 AND ds ='1';
SELECT key, value, ds FROM src_union_2 WHERE key=86 AND ds ='2';
SELECT key, value, ds FROM src_union_3 WHERE key=86 AND ds ='3';

EXPLAIN SELECT count(1) from src_union_1 WHERE ds ='1';
EXPLAIN SELECT count(1) from src_union_2 WHERE ds ='2';
EXPLAIN SELECT count(1) from src_union_3 WHERE ds ='3';

SELECT count(1) from src_union_1 WHERE ds ='1';
SELECT count(1) from src_union_2 WHERE ds ='2';
SELECT count(1) from src_union_3 WHERE ds ='3';

CREATE VIEW src_union_view PARTITIONED ON (ds) as
SELECT key, value, ds FROM (
SELECT key, value, ds FROM src_union_1
UNION ALL
SELECT key, value, ds FROM src_union_2
UNION ALL
SELECT key, value, ds FROM src_union_3
) subq;

EXPLAIN SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds ='1';
EXPLAIN SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds ='2';
EXPLAIN SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds ='3';
EXPLAIN SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds IS NOT NULL order by ds;

SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds ='1';
SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds ='2';
SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds ='3';
SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds IS NOT NULL order by ds;

EXPLAIN SELECT count(1) from src_union_view WHERE ds ='1';
EXPLAIN SELECT count(1) from src_union_view WHERE ds ='2';
EXPLAIN SELECT count(1) from src_union_view WHERE ds ='3';

SELECT count(1) from src_union_view WHERE ds ='1';
SELECT count(1) from src_union_view WHERE ds ='2';
SELECT count(1) from src_union_view WHERE ds ='3';

INSERT OVERWRITE TABLE src_union_3 PARTITION (ds='4', part_1='1', part_2='2:3+4') SELECT * FROM src;
ALTER INDEX src_union_3_key_idx ON src_union_3 PARTITION (ds='4', part_1='1', part_2='2:3+4') REBUILD;

EXPLAIN SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds ='4';
SELECT key, value, ds FROM src_union_view WHERE key=86 AND ds ='4';

EXPLAIN SELECT count(1) from src_union_view WHERE ds ='4';
SELECT count(1) from src_union_view WHERE ds ='4';
CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T2(key STRING, val STRING) STORED AS TEXTFILE;
CREATE TABLE T3(key STRING, val STRING) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/T1.txt' INTO TABLE T1;
LOAD DATA LOCAL INPATH '../../data/files/T2.txt' INTO TABLE T2;
LOAD DATA LOCAL INPATH '../../data/files/T3.txt' INTO TABLE T3;

-- SORT_QUERY_RESULTS

FROM UNIQUEJOIN PRESERVE T1 a (a.key), PRESERVE T2 b (b.key), PRESERVE T3 c (c.key)
SELECT a.key, b.key, c.key;

FROM UNIQUEJOIN T1 a (a.key), T2 b (b.key), T3 c (c.key)
SELECT a.key, b.key, c.key;

FROM UNIQUEJOIN T1 a (a.key), T2 b (b.key-1), T3 c (c.key)
SELECT a.key, b.key, c.key;

FROM UNIQUEJOIN PRESERVE T1 a (a.key, a.val), PRESERVE T2 b (b.key, b.val), PRESERVE T3 c (c.key, c.val)
SELECT a.key, a.val, b.key, b.val, c.key, c.val;

FROM UNIQUEJOIN PRESERVE T1 a (a.key), T2 b (b.key), PRESERVE T3 c (c.key)
SELECT a.key, b.key, c.key;

FROM UNIQUEJOIN PRESERVE T1 a (a.key), T2 b(b.key)
SELECT a.key, b.key;
FROM UNIQUEJOIN (SELECT src.key from src WHERE src.key<4) a (a.key), PRESERVE  src b(b.key)
SELECT a.key, b.key;

FROM UNIQUEJOIN src a (a.key), PRESERVE src b (b.key, b.val)
SELECT a.key, b.key;

FROM UNIQUEJOIN src a (a.key), PRESERVE src b (b.key) JOIN src c ON c.key
SELECT a.key;

FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.dummycol WHERE src.key < 100
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.value WHERE src.dummykey < 100
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.value WHERE src.key < 100 group by src.dummycol
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.value WHERE src.key < 100 group by dummysrc.key
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.value WHERE dummysrc.key < 100 group by src.key
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', dummysrc.value WHERE src.key < 100 group by src.key
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', dummyfn(src.value, 10) WHERE src.key < 100
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.value WHERE anotherdummyfn('abc', src.key) + 10 < 100
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.value WHERE anotherdummyfn('abc', src.key) + 10 < 100 group by src.key
FROM src
INSERT OVERWRITE TABLE dest1 SELECT '1234', dummyfn(src.key) WHERE src.key < 100 group by src.key
FROM dummySrc
INSERT OVERWRITE TABLE dest1 SELECT '1234', src.value WHERE src.key < 100
FROM src
INSERT OVERWRITE TABLE dummyDest SELECT '1234', src.value WHERE src.key < 100
CREATE TABLE testTable(col1 INT, col2 INT);
ALTER TABLE testTable SET TBLPROPERTIES ('a'='1', 'c'='3');
SHOW TBLPROPERTIES testTable;

-- unset a subset of the properties and some non-existed properties without if exists
ALTER TABLE testTable UNSET TBLPROPERTIES ('c', 'x', 'y', 'z');
CREATE DATABASE vt;

CREATE TABLE vt.testTable(col1 INT, col2 INT);
SHOW TBLPROPERTIES vt.testTable;

-- UNSET TABLE PROPERTIES
ALTER TABLE vt.testTable SET TBLPROPERTIES ('a'='1', 'c'='3');
SHOW TBLPROPERTIES vt.testTable;

-- UNSET all the properties
ALTER TABLE vt.testTable UNSET TBLPROPERTIES ('a', 'c');
SHOW TBLPROPERTIES vt.testTable;

ALTER TABLE vt.testTable SET TBLPROPERTIES ('a'='1', 'c'='3', 'd'='4');
SHOW TBLPROPERTIES vt.testTable;

-- UNSET a subset of the properties
ALTER TABLE vt.testTable UNSET TBLPROPERTIES ('a', 'd');
SHOW TBLPROPERTIES vt.testTable;

-- the same property being UNSET multiple times
ALTER TABLE vt.testTable UNSET TBLPROPERTIES ('c', 'c', 'c');
SHOW TBLPROPERTIES vt.testTable;

ALTER TABLE vt.testTable SET TBLPROPERTIES ('a'='1', 'b' = '2', 'c'='3', 'd'='4');
SHOW TBLPROPERTIES vt.testTable;

-- UNSET a subset of the properties and some non-existed properties using IF EXISTS
ALTER TABLE vt.testTable UNSET TBLPROPERTIES IF EXISTS ('b', 'd', 'b', 'f');
SHOW TBLPROPERTIES vt.testTable;

-- UNSET a subset of the properties and some non-existed properties using IF EXISTS
ALTER TABLE vt.testTable UNSET TBLPROPERTIES IF EXISTS ('b', 'd', 'c', 'f', 'x', 'y', 'z');
SHOW TBLPROPERTIES vt.testTable;

DROP TABLE vt.testTable;

-- UNSET VIEW PROPERTIES
CREATE VIEW vt.testView AS SELECT value FROM src WHERE key=86;
ALTER VIEW vt.testView SET TBLPROPERTIES ('propA'='100', 'propB'='200');
SHOW TBLPROPERTIES vt.testView;

-- UNSET all the properties
ALTER VIEW vt.testView UNSET TBLPROPERTIES ('propA', 'propB');
SHOW TBLPROPERTIES vt.testView;

ALTER VIEW vt.testView SET TBLPROPERTIES ('propA'='100', 'propC'='300', 'propD'='400');
SHOW TBLPROPERTIES vt.testView;

-- UNSET a subset of the properties
ALTER VIEW vt.testView UNSET TBLPROPERTIES ('propA', 'propC');
SHOW TBLPROPERTIES vt.testView;

-- the same property being UNSET multiple times
ALTER VIEW vt.testView UNSET TBLPROPERTIES ('propD', 'propD', 'propD');
SHOW TBLPROPERTIES vt.testView;

ALTER VIEW vt.testView SET TBLPROPERTIES ('propA'='100', 'propB' = '200', 'propC'='300', 'propD'='400');
SHOW TBLPROPERTIES vt.testView;

-- UNSET a subset of the properties and some non-existed properties using IF EXISTS
ALTER VIEW vt.testView UNSET TBLPROPERTIES IF EXISTS ('propC', 'propD', 'propD', 'propC', 'propZ');
SHOW TBLPROPERTIES vt.testView;

-- UNSET a subset of the properties and some non-existed properties using IF EXISTS
ALTER VIEW vt.testView UNSET TBLPROPERTIES IF EXISTS ('propB', 'propC', 'propD', 'propF');
SHOW TBLPROPERTIES vt.testView;

DROP VIEW vt.testView;

DROP DATABASE vt;CREATE VIEW testView AS SELECT value FROM src WHERE key=86;
ALTER VIEW testView SET TBLPROPERTIES ('propA'='100', 'propB'='200');
SHOW TBLPROPERTIES testView;

-- unset a subset of the properties and some non-existed properties without if exists
ALTER VIEW testView UNSET TBLPROPERTIES ('propB', 'propX', 'propY', 'propZ');
set hive.mapred.mode=nonstrict;
drop table tstsrc;

set hive.exec.pre.hooks = org.apache.hadoop.hive.ql.hooks.PreExecutePrinter,org.apache.hadoop.hive.ql.hooks.EnforceReadOnlyTables,org.apache.hadoop.hive.ql.hooks.UpdateInputAccessTimeHook$PreExec;

create table tstsrc as select * from src;
desc extended tstsrc;
select count(1) from tstsrc;
desc extended tstsrc;
drop table tstsrc;

drop table tstsrcpart;
create table tstsrcpart like srcpart;

set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;


insert overwrite table tstsrcpart partition (ds, hr) select key, value, ds, hr from srcpart;

desc extended tstsrcpart;
desc extended tstsrcpart partition (ds='2008-04-08', hr='11');
desc extended tstsrcpart partition (ds='2008-04-08', hr='12');

select count(1) from tstsrcpart where ds = '2008-04-08' and hr = '11';

desc extended tstsrcpart;
desc extended tstsrcpart partition (ds='2008-04-08', hr='11');
desc extended tstsrcpart partition (ds='2008-04-08', hr='12');

drop table tstsrcpart;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


-- SORT_QUERY_RESULTS

create table acid_uami(i int,
                 de decimal(5,2),
                 vc varchar(128)) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_uami values
    (1, 109.23, 'mary had a little lamb'),
    (6553, 923.19, 'its fleece was white as snow');

insert into table acid_uami values
    (10, 119.23, 'and everywhere that mary went'),
    (65530, 823.19, 'the lamb was sure to go');

select * from acid_uami order by de;

update acid_uami set de = 3.14 where de = 109.23 or de = 119.23;

select * from acid_uami order by de;
set hive.mapred.mode=nonstrict;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


-- SORT_QUERY_RESULTS

create table `aci/d_u/ami`(i int,
                 `d?*de e` decimal(5,2),
                 vc varchar(128)) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table `aci/d_u/ami` values
    (1, 109.23, 'mary had a little lamb'),
    (6553, 923.19, 'its fleece was white as snow');

insert into table `aci/d_u/ami` values
    (10, 119.23, 'and everywhere that mary went'),
    (65530, 823.19, 'the lamb was sure to go');

select * from `aci/d_u/ami` order by `d?*de e`;

update `aci/d_u/ami` set `d?*de e` = 3.14 where `d?*de e` = 109.23 or `d?*de e` = 119.23;

select * from `aci/d_u/ami` order by `d?*de e`;
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_uanp(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_uanp select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint < 0 order by cint limit 10;

select a,b from acid_uanp order by a;

update acid_uanp set b = 'fred';

select a,b from acid_uanp order by a;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_uap(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_uap partition (ds='today') select cint, cast(cstring1 as varchar(128)) as cs from alltypesorc where cint is not null and cint < 0 order by cint, cs limit 10;
insert into table acid_uap partition (ds='tomorrow') select cint, cast(cstring1 as varchar(128)) as cs from alltypesorc where cint is not null and cint > 10 order by cint, cs limit 10;

select a,b,ds from acid_uap order by a,b;

update acid_uap set b = 'fred';

select a,b,ds from acid_uap order by a,b;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


-- SORT_QUERY_RESULTS

create table acid_uat(ti tinyint,
                 si smallint,
                 i int,
                 j int,
                 bi bigint,
                 f float,
                 d double,
                 de decimal(5,2),
                 t timestamp,
                 dt date,
                 s string,
                 vc varchar(128),
                 ch char(36),
                 b boolean) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('TRANSACTIONAL'='TRUE');

insert into table acid_uat
    select ctinyint,
           csmallint,
           cint,
           cint j,
           cbigint,
           cfloat,
           cdouble,
           cast(cfloat as decimal(5,2)),
           ctimestamp1,
           cast(ctimestamp2 as date),
           cstring1,
           cast(cstring1 as varchar(128)),
           cast(cstring2 as char(36)),
           cboolean1
        from alltypesorc where cint < 0 order by cint limit 10;

select * from acid_uat order by i;

update acid_uat set
    ti = 1,
    si = 2,
    j = 3,
    bi = 4,
    f = 3.14,
    d = 6.28,
    de = 5.99,
    t = '2014-09-01 09:44.23.23',
    dt = '2014-09-01',
    s = 'its a beautiful day in the neighbhorhood',
    vc = 'a beautiful day for a neighbor',
    ch = 'wont you be mine',
    b = true
  where s = '0ruyd6Y50JpdGRf6HqD';

select * from acid_uat order by i;

update acid_uat set
  ti = ti * 2,
  si = cast(f as int),
  d = floor(de)
  where s = 'aw724t8c5558x2xneC624';


select * from acid_uat order by i;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table foo(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

update foo set a = 5;set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;


create table not_an_acid_table(a int, b varchar(128));

insert into table not_an_acid_table select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select a,b from not_an_acid_table order by a;

update not_an_acid_table set b = 'fred' where b = '0ruyd6Y50JpdGRf6HqD';
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;

create table foo(a int, b varchar(128)) clustered by (a) into 1 buckets stored as orc;

update foo set b = 'fred';
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;



update no_such_table set b = 'fred';
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/update_orig_table;
dfs -copyFromLocal ../../data/files/alltypesorc ${system:test.tmp.dir}/update_orig_table/00000_0;

-- SORT_QUERY_RESULTS

create table acid_uot(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cstring2 STRING,
    ctimestamp1 TIMESTAMP,
    ctimestamp2 TIMESTAMP,
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN) clustered by (cint) into 1 buckets stored as orc location '${system:test.tmp.dir}/update_orig_table' TBLPROPERTIES ('transactional'='true');

update acid_uot set cstring1 = 'fred' where cint < -1070551679;

select * from acid_uot where cstring1 = 'fred';

dfs -rmr ${system:test.tmp.dir}/update_orig_table;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table foo(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

update foo set ds = 'fred';
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_insertsort(a int, b varchar(128)) clustered by (a) sorted by (b) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

update acid_insertsort set b = 'fred' where b = 'bob';
set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_utt(a int, b varchar(128)) clustered by (b) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_utt select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select a,b from acid_utt order by a;

update acid_utt set a = 'fred' where b = '0ruyd6Y50JpdGRf6HqD';

select * from acid_utt order by a;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_utc(a int, b varchar(128), c float) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_utc select cint, cast(cstring1 as varchar(128)), cfloat from alltypesorc where cint < 0 order by cint limit 10;

select * from acid_utc order by a;

update acid_utc set b = 'fred',c = 3.14;

select * from acid_utc order by a;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_uwnp(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_uwnp select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select a,b from acid_uwnp order by a;

update acid_uwnp set b = 'fred' where b = '0ruyd6Y50JpdGRf6HqD';

select * from acid_uwnp order by a;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_wnm(a int, b varchar(128)) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_wnm select cint, cast(cstring1 as varchar(128)) from alltypesorc where cint is not null order by cint limit 10;

select a,b from acid_wnm order by a;

update acid_wnm set b = 'fred' where b = 'nosuchvalue';

select * from acid_wnm order by a;


set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;


create table acid_uwp(a int, b varchar(128)) partitioned by (ds string) clustered by (a) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');

insert into table acid_uwp partition (ds='today') select cint, cast(cstring1 as varchar(128)) as cs from alltypesorc where cint is not null and cint < 0 order by cint, cs limit 10;
insert into table acid_uwp partition (ds='tomorrow') select cint, cast(cstring1 as varchar(128)) as cs from alltypesorc where cint is not null and cint > 100 order by cint, cs limit 10;

select a,b,ds from acid_uwp order by a, ds, b;

update acid_uwp set b = 'fred' where b = 'k17Am8uPHWk02cEf1jet';

select * from acid_uwp order by a, ds, b;


add jar ${system:maven.local.repository}/org/apache/hive/hive-contrib/${system:hive.version}/hive-contrib-${system:hive.version}.jar;
SHOW TABLES 'src';

set hive.metastore.ds.connection.url.hook=org.apache.hadoop.hive.contrib.metastore.hooks.SampleURLHook;
-- changes to dummy derby store.. should return empty result
SHOW TABLES 'src';
set hive.mapred.mode=nonstrict;
drop table varchar1;
drop table varchar1_1;

create table varchar1 (key varchar(10), value varchar(20));
create table varchar1_1 (key string, value string);

-- load from file
load data local inpath '../../data/files/srcbucket0.txt' overwrite into table varchar1;
select * from varchar1 order by key, value limit 2;

-- insert overwrite, from same/different length varchar
insert overwrite table varchar1
  select cast(key as varchar(10)), cast(value as varchar(15)) from src order by key, value limit 2;
select key, value from varchar1 order by key, value;

-- insert overwrite, from string
insert overwrite table varchar1
  select key, value from src order by key, value limit 2;
select key, value from varchar1 order by key, value;

-- insert string from varchar
insert overwrite table varchar1_1
  select key, value from varchar1 order by key, value limit 2;
select key, value from varchar1_1 order by key, value;

-- respect string length
insert overwrite table varchar1
  select key, cast(value as varchar(3)) from src order by key, value limit 2;
select key, value from varchar1 order by key, value;

drop table varchar1;
drop table varchar1_1;
drop table varchar_2;

create table varchar_2 (
  key varchar(10),
  value varchar(20)
);

insert overwrite table varchar_2 select * from src;

select value, sum(cast(key as int)), count(*) numrows
from src
group by value
order by value asc
limit 5;

-- should match the query from src
select value, sum(cast(key as int)), count(*) numrows
from varchar_2
group by value
order by value asc
limit 5;

select value, sum(cast(key as int)), count(*) numrows
from src
group by value
order by value desc
limit 5;

-- should match the query from src
select value, sum(cast(key as int)), count(*) numrows
from varchar_2
group by value
order by value desc
limit 5;

drop table varchar_2;
set hive.fetch.task.conversion=more;

-- Cast from varchar to other data types
select
  cast(cast('11' as string) as tinyint),
  cast(cast('11' as string) as smallint),
  cast(cast('11' as string) as int),
  cast(cast('11' as string) as bigint),
  cast(cast('11.00' as string) as float),
  cast(cast('11.00' as string) as double),
  cast(cast('11.00' as string) as decimal)
from src limit 1;

select
  cast(cast('11' as varchar(10)) as tinyint),
  cast(cast('11' as varchar(10)) as smallint),
  cast(cast('11' as varchar(10)) as int),
  cast(cast('11' as varchar(10)) as bigint),
  cast(cast('11.00' as varchar(10)) as float),
  cast(cast('11.00' as varchar(10)) as double),
  cast(cast('11.00' as varchar(10)) as decimal)
from src limit 1;

select
  cast(cast('2011-01-01' as string) as date),
  cast(cast('2011-01-01 01:02:03' as string) as timestamp)
from src limit 1;

select
  cast(cast('2011-01-01' as varchar(10)) as date),
  cast(cast('2011-01-01 01:02:03' as varchar(30)) as timestamp)
from src limit 1;

-- no tests from string/varchar to boolean, that conversion doesn't look useful
select
  cast(cast('abc123' as string) as string),
  cast(cast('abc123' as string) as varchar(10))
from src limit 1;

select
  cast(cast('abc123' as varchar(10)) as string),
  cast(cast('abc123' as varchar(10)) as varchar(10))
from src limit 1;

-- cast from other types to varchar
select
  cast(cast(11 as tinyint) as string),
  cast(cast(11 as smallint) as string),
  cast(cast(11 as int) as string),
  cast(cast(11 as bigint) as string),
  cast(cast(11.00 as float) as string),
  cast(cast(11.00 as double) as string),
  cast(cast(11.00 as decimal) as string)
from src limit 1;

select
  cast(cast(11 as tinyint) as varchar(10)),
  cast(cast(11 as smallint) as varchar(10)),
  cast(cast(11 as int) as varchar(10)),
  cast(cast(11 as bigint) as varchar(10)),
  cast(cast(11.00 as float) as varchar(10)),
  cast(cast(11.00 as double) as varchar(10)),
  cast(cast(11.00 as decimal) as varchar(10))
from src limit 1;

select
  cast(date '2011-01-01' as string),
  cast(timestamp('2011-01-01 01:02:03') as string)
from src limit 1;

select
  cast(date '2011-01-01' as varchar(10)),
  cast(timestamp('2011-01-01 01:02:03') as varchar(30))
from src limit 1;

select
  cast(true as string),
  cast(false as string)
from src limit 1;

select
  cast(true as varchar(10)),
  cast(false as varchar(10))
from src limit 1;

set hive.fetch.task.conversion=more;

-- Should all be true
select
  cast('abc' as varchar(10)) =  cast('abc' as varchar(10)),
  cast('abc' as varchar(10)) <= cast('abc' as varchar(10)),
  cast('abc' as varchar(10)) >= cast('abc' as varchar(10)),
  cast('abc' as varchar(10)) <  cast('abd' as varchar(10)),
  cast('abc' as varchar(10)) >  cast('abb' as varchar(10)),
  cast('abc' as varchar(10)) <> cast('abb' as varchar(10))
from src limit 1;

-- Different varchar lengths should still compare the same
select
  cast('abc' as varchar(10)) =  cast('abc' as varchar(3)),
  cast('abc' as varchar(10)) <= cast('abc' as varchar(3)),
  cast('abc' as varchar(10)) >= cast('abc' as varchar(3)),
  cast('abc' as varchar(10)) <  cast('abd' as varchar(3)),
  cast('abc' as varchar(10)) >  cast('abb' as varchar(3)),
  cast('abc' as varchar(10)) <> cast('abb' as varchar(3))
from src limit 1;

-- Should work with string types as well
select
  cast('abc' as varchar(10)) =  'abc',
  cast('abc' as varchar(10)) <= 'abc',
  cast('abc' as varchar(10)) >= 'abc',
  cast('abc' as varchar(10)) <  'abd',
  cast('abc' as varchar(10)) >  'abb',
  cast('abc' as varchar(10)) <> 'abb'
from src limit 1;

-- leading space is significant for varchar
select
  cast(' abc' as varchar(10)) <> cast('abc' as varchar(10))
from src limit 1;

-- trailing space is significant for varchar
select
  cast('abc ' as varchar(10)) <> cast('abc' as varchar(10))
from src limit 1;
set hive.mapred.mode=nonstrict;
drop table varchar_join1_vc1;
drop table varchar_join1_vc2;
drop table varchar_join1_str;

create table  varchar_join1_vc1 (
  c1 int,
  c2 varchar(10)
);

create table  varchar_join1_vc2 (
  c1 int,
  c2 varchar(20)
);

create table  varchar_join1_str (
  c1 int,
  c2 string
);

load data local inpath '../../data/files/vc1.txt' into table varchar_join1_vc1;
load data local inpath '../../data/files/vc1.txt' into table varchar_join1_vc2;
load data local inpath '../../data/files/vc1.txt' into table varchar_join1_str;

-- Join varchar with same length varchar
select * from varchar_join1_vc1 a join varchar_join1_vc1 b on (a.c2 = b.c2) order by a.c1;

-- Join varchar with different length varchar
select * from varchar_join1_vc1 a join varchar_join1_vc2 b on (a.c2 = b.c2) order by a.c1;

-- Join varchar with string
select * from varchar_join1_vc1 a join varchar_join1_str b on (a.c2 = b.c2) order by a.c1;

drop table varchar_join1_vc1;
drop table varchar_join1_vc2;
drop table varchar_join1_str;
drop table varchar_nested_1;
drop table varchar_nested_array;
drop table varchar_nested_map;
drop table varchar_nested_struct;
drop table varchar_nested_cta;
drop table varchar_nested_view;

create table varchar_nested_1 (key int, value varchar(20));
insert overwrite table varchar_nested_1
  select key, value from src order by key limit 1;

-- arrays
create table varchar_nested_array (c1 array<varchar(20)>);
insert overwrite table varchar_nested_array
  select array(value, value) from varchar_nested_1;
describe varchar_nested_array;
select * from varchar_nested_array;

-- maps
create table varchar_nested_map (c1 map<int, varchar(20)>);
insert overwrite table varchar_nested_map
  select map(key, value) from varchar_nested_1;
describe varchar_nested_map;
select * from varchar_nested_map;

-- structs
create table varchar_nested_struct (c1 struct<a:int, b:varchar(20), c:string>);
insert overwrite table varchar_nested_struct
  select named_struct('a', key,
                      'b', value,
                      'c', cast(value as string))
  from varchar_nested_1;
describe varchar_nested_struct;
select * from varchar_nested_struct;

-- nested type with create table as
create table varchar_nested_cta as
  select * from varchar_nested_struct;
describe varchar_nested_cta;
select * from varchar_nested_cta;

-- nested type with view
create table varchar_nested_view as
  select * from varchar_nested_struct;
describe varchar_nested_view;
select * from varchar_nested_view;

drop table varchar_nested_1;
drop table varchar_nested_array;
drop table varchar_nested_map;
drop table varchar_nested_struct;
drop table varchar_nested_cta;
drop table varchar_nested_view;
drop table if exists varchar_serde_regex;
drop table if exists varchar_serde_lb;
drop table if exists varchar_serde_ls;
drop table if exists varchar_serde_c;
drop table if exists varchar_serde_lbc;
drop table if exists varchar_serde_orc;

--
-- RegexSerDe
--
create table  varchar_serde_regex (
  key varchar(10),
  value varchar(20)
)
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe'
with serdeproperties (
  "input.regex" = "([^]*)([^]*)"
)
stored as textfile;

load data local inpath '../../data/files/srcbucket0.txt' overwrite into table varchar_serde_regex;

select * from varchar_serde_regex limit 5;
select value, count(*) from varchar_serde_regex group by value limit 5;

--
-- LazyBinary
--
create table  varchar_serde_lb (
  key varchar(10),
  value varchar(20)
);
alter table varchar_serde_lb set serde 'org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe';

insert overwrite table varchar_serde_lb
  select key, value from varchar_serde_regex;
select * from varchar_serde_lb limit 5;
select value, count(*) from varchar_serde_lb group by value limit 5;

--
-- LazySimple
--
create table  varchar_serde_ls (
  key varchar(10),
  value varchar(20)
);
alter table varchar_serde_ls set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe';

insert overwrite table varchar_serde_ls
  select key, value from varchar_serde_lb;
select * from varchar_serde_ls limit 5;
select value, count(*) from varchar_serde_ls group by value limit 5;

--
-- Columnar
--
create table  varchar_serde_c (
  key varchar(10),
  value varchar(20)
) stored as rcfile;
alter table varchar_serde_c set serde 'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe';

insert overwrite table varchar_serde_c
  select key, value from varchar_serde_ls;
select * from varchar_serde_c limit 5;
select value, count(*) from varchar_serde_c group by value limit 5;

--
-- LazyBinaryColumnar
--
create table varchar_serde_lbc (
  key varchar(10),
  value varchar(20)
) stored as rcfile;
alter table varchar_serde_lbc set serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe';

insert overwrite table varchar_serde_lbc
  select key, value from varchar_serde_c;
select * from varchar_serde_lbc limit 5;
select value, count(*) from varchar_serde_lbc group by value limit 5;

--
-- ORC
--
create table varchar_serde_orc (
  key varchar(10),
  value varchar(20)
) stored as orc;
alter table varchar_serde_orc set serde 'org.apache.hadoop.hive.ql.io.orc.OrcSerde';


insert overwrite table varchar_serde_orc
  select key, value from varchar_serde_lbc;
select * from varchar_serde_orc limit 5;
select value, count(*) from varchar_serde_orc group by value limit 5;

drop table if exists varchar_serde_regex;
drop table if exists varchar_serde_lb;
drop table if exists varchar_serde_ls;
drop table if exists varchar_serde_c;
drop table if exists varchar_serde_lbc;
drop table if exists varchar_serde_orc;
drop table varchar_udf_1;

create table varchar_udf_1 (c1 string, c2 string, c3 varchar(10), c4 varchar(20));
insert overwrite table varchar_udf_1
  select key, value, key, value from src where key = '238' limit 1;

-- JAVA_VERSION_SPECIFIC_OUTPUT

-- UDFs with varchar support
select
  concat(c1, c2),
  concat(c3, c4),
  concat(c1, c2) = concat(c3, c4)
from varchar_udf_1 limit 1;

select
  upper(c2),
  upper(c4),
  upper(c2) = upper(c4)
from varchar_udf_1 limit 1;

select
  lower(c2),
  lower(c4),
  lower(c2) = lower(c4)
from varchar_udf_1 limit 1;

-- Scalar UDFs
select
  ascii(c2),
  ascii(c4),
  ascii(c2) = ascii(c4)
from varchar_udf_1 limit 1;

select
  concat_ws('|', c1, c2),
  concat_ws('|', c3, c4),
  concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
from varchar_udf_1 limit 1;

select
  decode(encode(c2, 'US-ASCII'), 'US-ASCII'),
  decode(encode(c4, 'US-ASCII'), 'US-ASCII'),
  decode(encode(c2, 'US-ASCII'), 'US-ASCII') = decode(encode(c4, 'US-ASCII'), 'US-ASCII')
from varchar_udf_1 limit 1;

select
  instr(c2, '_'),
  instr(c4, '_'),
  instr(c2, '_') = instr(c4, '_')
from varchar_udf_1 limit 1;

select
  length(c2),
  length(c4),
  length(c2) = length(c4)
from varchar_udf_1 limit 1;

select
  locate('a', 'abcdabcd', 3),
  locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3),
  locate('a', 'abcdabcd', 3) = locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3)
from varchar_udf_1 limit 1;

select
  lpad(c2, 15, ' '),
  lpad(c4, 15, ' '),
  lpad(c2, 15, ' ') = lpad(c4, 15, ' ')
from varchar_udf_1 limit 1;

select
  ltrim(c2),
  ltrim(c4),
  ltrim(c2) = ltrim(c4)
from varchar_udf_1 limit 1;

select
  c2 regexp 'val',
  c4 regexp 'val',
  (c2 regexp 'val') = (c4 regexp 'val')
from varchar_udf_1 limit 1;

select
  regexp_extract(c2, 'val_([0-9]+)', 1),
  regexp_extract(c4, 'val_([0-9]+)', 1),
  regexp_extract(c2, 'val_([0-9]+)', 1) = regexp_extract(c4, 'val_([0-9]+)', 1)
from varchar_udf_1 limit 1;

select
  regexp_replace(c2, 'val', 'replaced'),
  regexp_replace(c4, 'val', 'replaced'),
  regexp_replace(c2, 'val', 'replaced') = regexp_replace(c4, 'val', 'replaced')
from varchar_udf_1 limit 1;

select
  reverse(c2),
  reverse(c4),
  reverse(c2) = reverse(c4)
from varchar_udf_1 limit 1;

select
  rpad(c2, 15, ' '),
  rpad(c4, 15, ' '),
  rpad(c2, 15, ' ') = rpad(c4, 15, ' ')
from varchar_udf_1 limit 1;

select
  rtrim(c2),
  rtrim(c4),
  rtrim(c2) = rtrim(c4)
from varchar_udf_1 limit 1;

select
  sentences('See spot run.  See jane run.'),
  sentences(cast('See spot run.  See jane run.' as varchar(50)))
from varchar_udf_1 limit 1;

select
  split(c2, '_'),
  split(c4, '_')
from varchar_udf_1 limit 1;

select
  str_to_map('a:1,b:2,c:3',',',':'),
  str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
from varchar_udf_1 limit 1;

select
  substr(c2, 1, 3),
  substr(c4, 1, 3),
  substr(c2, 1, 3) = substr(c4, 1, 3)
from varchar_udf_1 limit 1;

select
  trim(c2),
  trim(c4),
  trim(c2) = trim(c4)
from varchar_udf_1 limit 1;


-- Aggregate Functions
select
  compute_stats(c2, 16),
  compute_stats(c4, 16)
from varchar_udf_1;

select
  min(c2),
  min(c4)
from varchar_udf_1;

select
  max(c2),
  max(c4)
from varchar_udf_1;


drop table varchar_udf_1;
drop table varchar_union1_vc1;
drop table varchar_union1_vc2;
drop table varchar_union1_str;

create table  varchar_union1_vc1 (
  c1 int,
  c2 varchar(10)
);

create table  varchar_union1_vc2 (
  c1 int,
  c2 varchar(20)
);

create table  varchar_union1_str (
  c1 int,
  c2 string
);

load data local inpath '../../data/files/vc1.txt' into table varchar_union1_vc1;
load data local inpath '../../data/files/vc1.txt' into table varchar_union1_vc2;
load data local inpath '../../data/files/vc1.txt' into table varchar_union1_str;

-- union varchar with same length varchar
select * from (
  select * from varchar_union1_vc1
  union all
  select * from varchar_union1_vc1 limit 1
) q1 sort by c1;

-- union varchar with different length varchar
select * from (
  select * from varchar_union1_vc1
  union all
  select * from varchar_union1_vc2 limit 1
) q1 sort by c1;

-- union varchar with string
select * from (
  select * from varchar_union1_vc1
  union all
  select * from varchar_union1_str limit 1
) q1 sort by c1;

drop table varchar_union1_vc1;
drop table varchar_union1_vc2;
drop table varchar_union1_str;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

-- Use ORDER BY clauses to generate 2 stages.
EXPLAIN
SELECT MIN(ctinyint) as c1,
       MAX(ctinyint),
       COUNT(ctinyint),
       COUNT(*)
FROM   alltypesorc
ORDER BY c1;

SELECT MIN(ctinyint) as c1,
       MAX(ctinyint),
       COUNT(ctinyint),
       COUNT(*)
FROM   alltypesorc
ORDER BY c1;

EXPLAIN
SELECT SUM(ctinyint) as c1
FROM   alltypesorc
ORDER BY c1;

SELECT SUM(ctinyint) as c1
FROM   alltypesorc
ORDER BY c1;

EXPLAIN
SELECT
  avg(ctinyint) as c1,
  variance(ctinyint),
  var_pop(ctinyint),
  var_samp(ctinyint),
  std(ctinyint),
  stddev(ctinyint),
  stddev_pop(ctinyint),
  stddev_samp(ctinyint)
FROM alltypesorc
ORDER BY c1;

SELECT
  avg(ctinyint) as c1,
  variance(ctinyint),
  var_pop(ctinyint),
  var_samp(ctinyint),
  std(ctinyint),
  stddev(ctinyint),
  stddev_pop(ctinyint),
  stddev_samp(ctinyint)
FROM alltypesorc
ORDER BY c1;

EXPLAIN
SELECT MIN(cbigint) as c1,
       MAX(cbigint),
       COUNT(cbigint),
       COUNT(*)
FROM   alltypesorc
ORDER BY c1;

SELECT MIN(cbigint) as c1,
       MAX(cbigint),
       COUNT(cbigint),
       COUNT(*)
FROM   alltypesorc
ORDER BY c1;

EXPLAIN
SELECT SUM(cbigint) as c1
FROM   alltypesorc
ORDER BY c1;

SELECT SUM(cbigint) as c1
FROM   alltypesorc
ORDER BY c1;

EXPLAIN
SELECT
  avg(cbigint) as c1,
  variance(cbigint),
  var_pop(cbigint),
  var_samp(cbigint),
  std(cbigint),
  stddev(cbigint),
  stddev_pop(cbigint),
  stddev_samp(cbigint)
FROM alltypesorc
ORDER BY c1;

SELECT
  avg(cbigint) as c1,
  variance(cbigint),
  var_pop(cbigint),
  var_samp(cbigint),
  std(cbigint),
  stddev(cbigint),
  stddev_pop(cbigint),
  stddev_samp(cbigint)
FROM alltypesorc
ORDER BY c1;

EXPLAIN
SELECT MIN(cfloat) as c1,
       MAX(cfloat),
       COUNT(cfloat),
       COUNT(*)
FROM   alltypesorc
ORDER BY c1;

SELECT MIN(cfloat) as c1,
       MAX(cfloat),
       COUNT(cfloat),
       COUNT(*)
FROM   alltypesorc
ORDER BY c1;

EXPLAIN
SELECT SUM(cfloat) as c1
FROM   alltypesorc
ORDER BY c1;

SELECT SUM(cfloat) as c1
FROM   alltypesorc
ORDER BY c1;

EXPLAIN
SELECT
  avg(cfloat) as c1,
  variance(cfloat),
  var_pop(cfloat),
  var_samp(cfloat),
  std(cfloat),
  stddev(cfloat),
  stddev_pop(cfloat),
  stddev_samp(cfloat)
FROM alltypesorc
ORDER BY c1;

SELECT
  avg(cfloat) as c1,
  variance(cfloat),
  var_pop(cfloat),
  var_samp(cfloat),
  std(cfloat),
  stddev(cfloat),
  stddev_pop(cfloat),
  stddev_samp(cfloat)
FROM alltypesorc
ORDER BY c1;

EXPLAIN
SELECT AVG(cbigint),
       (-(AVG(cbigint))),
       (-6432 + AVG(cbigint)),
       STDDEV_POP(cbigint),
       (-((-6432 + AVG(cbigint)))),
       ((-((-6432 + AVG(cbigint)))) + (-6432 + AVG(cbigint))),
       VAR_SAMP(cbigint),
       (-((-6432 + AVG(cbigint)))),
       (-6432 + (-((-6432 + AVG(cbigint))))),
       (-((-6432 + AVG(cbigint)))),
       ((-((-6432 + AVG(cbigint)))) / (-((-6432 + AVG(cbigint))))),
       COUNT(*),
       SUM(cfloat),
       (VAR_SAMP(cbigint) % STDDEV_POP(cbigint)),
       (-(VAR_SAMP(cbigint))),
       ((-((-6432 + AVG(cbigint)))) * (-(AVG(cbigint)))),
       MIN(ctinyint),
       (-(MIN(ctinyint)))
FROM   alltypesorc
WHERE  (((cstring2 LIKE '%b%')
         OR ((79.553 != cint)
             OR (cbigint < cdouble)))
        OR ((ctinyint >= csmallint)
            AND ((cboolean2 = 1)
                 AND (3569 = ctinyint))));

SELECT AVG(cbigint),
       (-(AVG(cbigint))),
       (-6432 + AVG(cbigint)),
       STDDEV_POP(cbigint),
       (-((-6432 + AVG(cbigint)))),
       ((-((-6432 + AVG(cbigint)))) + (-6432 + AVG(cbigint))),
       VAR_SAMP(cbigint),
       (-((-6432 + AVG(cbigint)))),
       (-6432 + (-((-6432 + AVG(cbigint))))),
       (-((-6432 + AVG(cbigint)))),
       ((-((-6432 + AVG(cbigint)))) / (-((-6432 + AVG(cbigint))))),
       COUNT(*),
       SUM(cfloat),
       (VAR_SAMP(cbigint) % STDDEV_POP(cbigint)),
       (-(VAR_SAMP(cbigint))),
       ((-((-6432 + AVG(cbigint)))) * (-(AVG(cbigint)))),
       MIN(ctinyint),
       (-(MIN(ctinyint)))
FROM   alltypesorc
WHERE  (((cstring2 LIKE '%b%')
         OR ((79.553 != cint)
             OR (cbigint < cdouble)))
        OR ((ctinyint >= csmallint)
            AND ((cboolean2 = 1)
                 AND (3569 = ctinyint))));

SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT VAR_POP(ctinyint),
       (VAR_POP(ctinyint) / -26.28),
       SUM(cfloat),
       (-1.389 + SUM(cfloat)),
       (SUM(cfloat) * (-1.389 + SUM(cfloat))),
       MAX(ctinyint),
       (-((SUM(cfloat) * (-1.389 + SUM(cfloat))))),
       MAX(cint),
       (MAX(cint) * 79.553),
       VAR_SAMP(cdouble),
       (10.175 % (-((SUM(cfloat) * (-1.389 + SUM(cfloat)))))),
       COUNT(cint),
       (-563 % MAX(cint))
FROM   alltypesorc
WHERE  (((cdouble > ctinyint)
         AND (cboolean2 > 0))
        OR ((cbigint < ctinyint)
            OR ((cint > cbigint)
                OR (cboolean1 < 0))));

SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT cdouble,
       ctimestamp1,
       ctinyint,
       cboolean1,
       cstring1,
       (-(cdouble)),
       (cdouble + csmallint),
       ((cdouble + csmallint) % 33),
       (-(cdouble)),
       (ctinyint % cdouble),
       (ctinyint % csmallint),
       (-(cdouble)),
       (cbigint * (ctinyint % csmallint)),
       (9763215.5639 - (cdouble + csmallint)),
       (-((-(cdouble))))
FROM   alltypesorc
WHERE  (((cstring2 <= '10')
         OR ((ctinyint > cdouble)
             AND (-5638.15 >= ctinyint)))
        OR ((cdouble > 6981)
            AND ((csmallint = 9763215.5639)
                 OR (cstring1 LIKE '%a'))));

SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT cstring1,
       cboolean1,
       cdouble,
       ctimestamp1,
       (-3728 * csmallint),
       (cdouble - 9763215.5639),
       (-(cdouble)),
       ((-(cdouble)) + 6981),
       (cdouble * -5638.15)
FROM   alltypesorc
WHERE  ((cstring2 = cstring1)
        OR ((ctimestamp1 IS NULL)
            AND (cstring1 LIKE '%a')));

set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT   cbigint,
         cboolean1,
         cstring1,
         ctimestamp1,
         cdouble,
         (-6432 * cdouble),
         (-(cbigint)),
         COUNT(cbigint),
         (cbigint * COUNT(cbigint)),
         STDDEV_SAMP(cbigint),
         ((-6432 * cdouble) / -6432),
         (-(((-6432 * cdouble) / -6432))),
         AVG(cdouble),
         (-((-6432 * cdouble))),
         (-5638.15 + cbigint),
         SUM(cbigint),
         (AVG(cdouble) / (-6432 * cdouble)),
         AVG(cdouble),
         (-((-(((-6432 * cdouble) / -6432))))),
         (((-6432 * cdouble) / -6432) + (-((-6432 * cdouble)))),
         STDDEV_POP(cdouble)
FROM     alltypesorc
WHERE    (((ctimestamp1 IS NULL)
           AND ((cboolean1 >= cboolean2)
                OR (ctinyint != csmallint)))
          AND ((cstring1 LIKE '%a')
              OR ((cboolean2 <= 1)
                  AND (cbigint >= csmallint))))
GROUP BY cbigint, cboolean1, cstring1, ctimestamp1, cdouble
ORDER BY ctimestamp1, cdouble, cbigint, cstring1;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT   cboolean1,
         ctinyint,
         ctimestamp1,
         cfloat,
         cstring1,
         (-(ctinyint)) as c1,
         MAX(ctinyint) as c2,
         ((-(ctinyint)) + MAX(ctinyint)) as c3,
         SUM(cfloat) as c4,
         (SUM(cfloat) * ((-(ctinyint)) + MAX(ctinyint))) as c5,
         (-(SUM(cfloat))) as c6,
         (79.553 * cfloat) as c7,
         STDDEV_POP(cfloat) as c8,
         (-(SUM(cfloat))) as c9,
         STDDEV_POP(ctinyint) as c10,
         (((-(ctinyint)) + MAX(ctinyint)) - 10.175) as c11,
         (-((-(SUM(cfloat))))) as c12,
         (-26.28 / (-((-(SUM(cfloat)))))) as c13,
         MAX(cfloat) as c14,
         ((SUM(cfloat) * ((-(ctinyint)) + MAX(ctinyint))) / ctinyint) as c15,
         MIN(ctinyint) as c16
FROM     alltypesorc
WHERE    (((cfloat < 3569)
           AND ((10.175 >= cdouble)
                AND (cboolean1 != 1)))
          OR ((ctimestamp1 > 11)
              AND ((ctimestamp2 != 12)
                   AND (ctinyint < 9763215.5639))))
GROUP BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1
ORDER BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16
LIMIT 40;

SELECT   cboolean1,
         ctinyint,
         ctimestamp1,
         cfloat,
         cstring1,
         (-(ctinyint)) as c1,
         MAX(ctinyint) as c2,
         ((-(ctinyint)) + MAX(ctinyint)) as c3,
         SUM(cfloat) as c4,
         (SUM(cfloat) * ((-(ctinyint)) + MAX(ctinyint))) as c5,
         (-(SUM(cfloat))) as c6,
         (79.553 * cfloat) as c7,
         STDDEV_POP(cfloat) as c8,
         (-(SUM(cfloat))) as c9,
         STDDEV_POP(ctinyint) as c10,
         (((-(ctinyint)) + MAX(ctinyint)) - 10.175) as c11,
         (-((-(SUM(cfloat))))) as c12,
         (-26.28 / (-((-(SUM(cfloat)))))) as c13,
         MAX(cfloat) as c14,
         ((SUM(cfloat) * ((-(ctinyint)) + MAX(ctinyint))) / ctinyint) as c15,
         MIN(ctinyint) as c16
FROM     alltypesorc
WHERE    (((cfloat < 3569)
           AND ((10.175 >= cdouble)
                AND (cboolean1 != 1)))
          OR ((ctimestamp1 > 11)
              AND ((ctimestamp2 != 12)
                   AND (ctinyint < 9763215.5639))))
GROUP BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1
ORDER BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16
LIMIT 40;

-- double compare timestamp
EXPLAIN
SELECT   cboolean1,
         ctinyint,
         ctimestamp1,
         cfloat,
         cstring1,
         (-(ctinyint)) as c1,
         MAX(ctinyint) as c2,
         ((-(ctinyint)) + MAX(ctinyint)) as c3,
         SUM(cfloat) as c4,
         (SUM(cfloat) * ((-(ctinyint)) + MAX(ctinyint))) as c5,
         (-(SUM(cfloat))) as c6,
         (79.553 * cfloat) as c7,
         STDDEV_POP(cfloat) as c8,
         (-(SUM(cfloat))) as c9,
         STDDEV_POP(ctinyint) as c10,
         (((-(ctinyint)) + MAX(ctinyint)) - 10.175) as c11,
         (-((-(SUM(cfloat))))) as c12,
         (-26.28 / (-((-(SUM(cfloat)))))) as c13,
         MAX(cfloat) as c14,
         ((SUM(cfloat) * ((-(ctinyint)) + MAX(ctinyint))) / ctinyint) as c15,
         MIN(ctinyint) as c16
FROM     alltypesorc
WHERE    (((cfloat < 3569)
           AND ((10.175 >= cdouble)
                AND (cboolean1 != 1)))
          OR ((ctimestamp1 > -1.388)
              AND ((ctimestamp2 != -1.3359999999999999)
                   AND (ctinyint < 9763215.5639))))
GROUP BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1
ORDER BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16
LIMIT 40;

SELECT   cboolean1,
         ctinyint,
         ctimestamp1,
         cfloat,
         cstring1,
         (-(ctinyint)) as c1,
         MAX(ctinyint) as c2,
         ((-(ctinyint)) + MAX(ctinyint)) as c3,
         SUM(cfloat) as c4,
         (SUM(cfloat) * ((-(ctinyint)) + MAX(ctinyint))) as c5,
         (-(SUM(cfloat))) as c6,
         (79.553 * cfloat) as c7,
         STDDEV_POP(cfloat) as c8,
         (-(SUM(cfloat))) as c9,
         STDDEV_POP(ctinyint) as c10,
         (((-(ctinyint)) + MAX(ctinyint)) - 10.175) as c11,
         (-((-(SUM(cfloat))))) as c12,
         (-26.28 / (-((-(SUM(cfloat)))))) as c13,
         MAX(cfloat) as c14,
         ((SUM(cfloat) * ((-(ctinyint)) + MAX(ctinyint))) / ctinyint) as c15,
         MIN(ctinyint) as c16
FROM     alltypesorc
WHERE    (((cfloat < 3569)
           AND ((10.175 >= cdouble)
                AND (cboolean1 != 1)))
          OR ((ctimestamp1 > -1.388)
              AND ((ctimestamp2 != -1.3359999999999999)
                   AND (ctinyint < 9763215.5639))))
GROUP BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1
ORDER BY cboolean1, ctinyint, ctimestamp1, cfloat, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16
LIMIT 40;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT   ctimestamp1,
         cfloat,
         cstring1,
         cboolean1,
         cdouble,
         (-26.28 + cdouble),
         (-((-26.28 + cdouble))),
         STDDEV_SAMP((-((-26.28 + cdouble)))),
         (cfloat * -26.28),
         MAX(cfloat),
         (-(cfloat)),
         (-(MAX(cfloat))),
         ((-((-26.28 + cdouble))) / 10.175),
         STDDEV_POP(cfloat),
         COUNT(cfloat),
         (-(((-((-26.28 + cdouble))) / 10.175))),
         (-1.389 % STDDEV_SAMP((-((-26.28 + cdouble))))),
         (cfloat - cdouble),
         VAR_POP(cfloat),
         (VAR_POP(cfloat) % 10.175),
         VAR_SAMP(cfloat),
         (-((cfloat - cdouble)))
FROM     alltypesorc
WHERE    (((ctinyint <= cbigint)
           AND ((cint <= cdouble)
                OR (ctimestamp2 < ctimestamp1)))
          AND ((cdouble < ctinyint)
              AND ((cbigint > -257)
                  OR (cfloat < cint))))
GROUP BY ctimestamp1, cfloat, cstring1, cboolean1, cdouble
ORDER BY cstring1, cfloat, cdouble, ctimestamp1;

SELECT   ctimestamp1,
         cfloat,
         cstring1,
         cboolean1,
         cdouble,
         (-26.28 + cdouble),
         (-((-26.28 + cdouble))),
         STDDEV_SAMP((-((-26.28 + cdouble)))),
         (cfloat * -26.28),
         MAX(cfloat),
         (-(cfloat)),
         (-(MAX(cfloat))),
         ((-((-26.28 + cdouble))) / 10.175),
         STDDEV_POP(cfloat),
         COUNT(cfloat),
         (-(((-((-26.28 + cdouble))) / 10.175))),
         (-1.389 % STDDEV_SAMP((-((-26.28 + cdouble))))),
         (cfloat - cdouble),
         VAR_POP(cfloat),
         (VAR_POP(cfloat) % 10.175),
         VAR_SAMP(cfloat),
         (-((cfloat - cdouble)))
FROM     alltypesorc
WHERE    (((ctinyint <= cbigint)
           AND ((cint <= cdouble)
                OR (ctimestamp2 < ctimestamp1)))
          AND ((cdouble < ctinyint)
              AND ((cbigint > -257)
                  OR (cfloat < cint))))
GROUP BY ctimestamp1, cfloat, cstring1, cboolean1, cdouble
ORDER BY cstring1, cfloat, cdouble, ctimestamp1;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT   cfloat,
         cboolean1,
         cdouble,
         cstring1,
         ctinyint,
         cint,
         ctimestamp1,
         STDDEV_SAMP(cfloat),
         (-26.28 - cint),
         MIN(cdouble),
         (cdouble * 79.553),
         (33 % cfloat),
         STDDEV_SAMP(ctinyint),
         VAR_POP(ctinyint),
         (-23 % cdouble),
         (-(ctinyint)),
         VAR_SAMP(cint),
         (cint - cfloat),
         (-23 % ctinyint),
         (-((-26.28 - cint))),
         STDDEV_POP(cint)
FROM     alltypesorc
WHERE    (((cstring2 LIKE '%ss%')
           OR (cstring1 LIKE '10%'))
          OR ((cint >= -75)
              AND ((ctinyint = csmallint)
                   AND (cdouble >= -3728))))
GROUP BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1
ORDER BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1;

SELECT   cfloat,
         cboolean1,
         cdouble,
         cstring1,
         ctinyint,
         cint,
         ctimestamp1,
         STDDEV_SAMP(cfloat),
         (-26.28 - cint),
         MIN(cdouble),
         (cdouble * 79.553),
         (33 % cfloat),
         STDDEV_SAMP(ctinyint),
         VAR_POP(ctinyint),
         (-23 % cdouble),
         (-(ctinyint)),
         VAR_SAMP(cint),
         (cint - cfloat),
         (-23 % ctinyint),
         (-((-26.28 - cint))),
         STDDEV_POP(cint)
FROM     alltypesorc
WHERE    (((cstring2 LIKE '%ss%')
           OR (cstring1 LIKE '10%'))
          OR ((cint >= -75)
              AND ((ctinyint = csmallint)
                   AND (cdouble >= -3728))))
GROUP BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1
ORDER BY cfloat, cboolean1, cdouble, cstring1, ctinyint, cint, ctimestamp1;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT   cstring1,
         cdouble,
         ctimestamp1,
         (cdouble - 9763215.5639),
         (-((cdouble - 9763215.5639))),
         COUNT(cdouble),
         STDDEV_SAMP(cdouble),
         (-(STDDEV_SAMP(cdouble))),
         (STDDEV_SAMP(cdouble) * COUNT(cdouble)),
         MIN(cdouble),
         (9763215.5639 / cdouble),
         (COUNT(cdouble) / -1.389),
         STDDEV_SAMP(cdouble)
FROM     alltypesorc
WHERE    ((cstring2 LIKE '%b%')
          AND ((cdouble >= -1.389)
              OR (cstring1 < 'a')))
GROUP BY cstring1, cdouble, ctimestamp1;

SELECT   cstring1,
         cdouble,
         ctimestamp1,
         (cdouble - 9763215.5639),
         (-((cdouble - 9763215.5639))),
         COUNT(cdouble),
         STDDEV_SAMP(cdouble),
         (-(STDDEV_SAMP(cdouble))),
         (STDDEV_SAMP(cdouble) * COUNT(cdouble)),
         MIN(cdouble),
         (9763215.5639 / cdouble),
         (COUNT(cdouble) / -1.389),
         STDDEV_SAMP(cdouble)
FROM     alltypesorc
WHERE    ((cstring2 LIKE '%b%')
          AND ((cdouble >= -1.389)
              OR (cstring1 < 'a')))
GROUP BY cstring1, cdouble, ctimestamp1;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT   cfloat,
         cstring1,
         cint,
         ctimestamp1,
         cdouble,
         cbigint,
         (cfloat / ctinyint),
         (cint % cbigint),
         (-(cdouble)),
         (cdouble + (cfloat / ctinyint)),
         (cdouble / cint),
         (-((-(cdouble)))),
         (9763215.5639 % cbigint),
         (2563.58 + (-((-(cdouble)))))
FROM     alltypesorc
WHERE    (((cbigint > -23)
           AND ((cdouble != 988888)
                OR (cint > -863.257)))
          AND ((ctinyint >= 33)
              OR ((csmallint >= cbigint)
                  OR (cfloat = cdouble))))
ORDER BY cbigint, cfloat;

SELECT   cfloat,
         cstring1,
         cint,
         ctimestamp1,
         cdouble,
         cbigint,
         (cfloat / ctinyint),
         (cint % cbigint),
         (-(cdouble)),
         (cdouble + (cfloat / ctinyint)),
         (cdouble / cint),
         (-((-(cdouble)))),
         (9763215.5639 % cbigint),
         (2563.58 + (-((-(cdouble)))))
FROM     alltypesorc
WHERE    (((cbigint > -23)
           AND ((cdouble != 988888)
                OR (cint > -863.257)))
          AND ((ctinyint >= 33)
              OR ((csmallint >= cbigint)
                  OR (cfloat = cdouble))))
ORDER BY cbigint, cfloat;SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT AVG(csmallint),
       (AVG(csmallint) % -563),
       (AVG(csmallint) + 762),
       SUM(cfloat),
       VAR_POP(cbigint),
       (-(VAR_POP(cbigint))),
       (SUM(cfloat) - AVG(csmallint)),
       COUNT(*),
       (-((SUM(cfloat) - AVG(csmallint)))),
       (VAR_POP(cbigint) - 762),
       MIN(ctinyint),
       ((-(VAR_POP(cbigint))) + MIN(ctinyint)),
       AVG(cdouble),
       (((-(VAR_POP(cbigint))) + MIN(ctinyint)) - SUM(cfloat))
FROM   alltypesorc
WHERE  (((ctimestamp1 < ctimestamp2)
         AND ((cstring2 LIKE 'b%')
              AND (cfloat <= -5638.15)))
        OR ((cdouble < ctinyint)
            AND ((-10669 != ctimestamp2)
                 OR (359 > cint))));

set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT STDDEV_SAMP(csmallint),
       (STDDEV_SAMP(csmallint) - 10.175),
       STDDEV_POP(ctinyint),
       (STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)),
       (-(STDDEV_POP(ctinyint))),
       (STDDEV_SAMP(csmallint) % 79.553),
       (-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))),
       STDDEV_SAMP(cfloat),
       (-(STDDEV_SAMP(csmallint))),
       SUM(cfloat),
       ((-((STDDEV_SAMP(csmallint) * (STDDEV_SAMP(csmallint) - 10.175)))) / (STDDEV_SAMP(csmallint) - 10.175)),
       (-((STDDEV_SAMP(csmallint) - 10.175))),
       AVG(cint),
       (-3728 - STDDEV_SAMP(csmallint)),
       STDDEV_POP(cint),
       (AVG(cint) / STDDEV_SAMP(cfloat))
FROM   alltypesorc
WHERE  (((cint <= cfloat)
         AND ((79.553 != cbigint)
              AND (ctimestamp2 = -29071)))
        OR ((cbigint > cdouble)
            AND ((79.553 <= csmallint)
                 AND (ctimestamp1 > ctimestamp2))));

SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT SUM(cint),
       (SUM(cint) * -563),
       (-3728 + SUM(cint)),
       STDDEV_POP(cdouble),
       (-(STDDEV_POP(cdouble))),
       AVG(cdouble),
       ((SUM(cint) * -563) % SUM(cint)),
       (((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)),
       VAR_POP(cdouble),
       (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))),
       ((-3728 + SUM(cint)) - (SUM(cint) * -563)),
       MIN(ctinyint),
       MIN(ctinyint),
       (MIN(ctinyint) * (-((((SUM(cint) * -563) % SUM(cint)) / AVG(cdouble)))))
FROM   alltypesorc
WHERE  (((csmallint >= cint)
         OR ((-89010 >= ctinyint)
             AND (cdouble > 79.553)))
        OR ((-563 != cbigint)
            AND ((ctinyint != cbigint)
                 OR (-3728 >= cdouble))));

SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT MAX(csmallint),
       (MAX(csmallint) * -75),
       COUNT(*),
       ((MAX(csmallint) * -75) / COUNT(*)),
       (6981 * MAX(csmallint)),
       MIN(csmallint),
       (-(MIN(csmallint))),
       (197 % ((MAX(csmallint) * -75) / COUNT(*))),
       SUM(cint),
       MAX(ctinyint),
       (-(MAX(ctinyint))),
       ((-(MAX(ctinyint))) + MAX(ctinyint))
FROM   alltypesorc
WHERE  (((cboolean2 IS NOT NULL)
         AND (cstring1 LIKE '%b%'))
        OR ((ctinyint = cdouble)
            AND ((ctimestamp2 IS NOT NULL)
                 AND (cstring2 LIKE 'a'))));

SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

SELECT cboolean1,
       cfloat,
       cstring1,
       (988888 * csmallint),
       (-(csmallint)),
       (-(cfloat)),
       (-26.28 / cfloat),
       (cfloat * 359),
       (cint % ctinyint),
       (-(cdouble)),
       (ctinyint - -75),
       (762 * (cint % ctinyint))
FROM   alltypesorc
WHERE  ((ctinyint != 0)
        AND ((((cboolean1 <= 0)
          AND (cboolean2 >= cboolean1))
          OR ((cbigint IS NOT NULL)
              AND ((cstring2 LIKE '%a')
                   OR (cfloat <= -257))))));

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT cboolean1,
       cbigint,
       csmallint,
       ctinyint,
       ctimestamp1,
       cstring1,
       (cbigint + cbigint) as c1,
       (csmallint % -257) as c2,
       (-(csmallint)) as c3,
       (-(ctinyint)) as c4,
       ((-(ctinyint)) + 17) as c5,
       (cbigint * (-(csmallint))) as c6,
       (cint % csmallint) as c7,
       (-(ctinyint)) as c8,
       ((-(ctinyint)) % ctinyint) as c9
FROM   alltypesorc
WHERE  ((ctinyint != 0)
        AND (((ctimestamp1 <= 0)
          OR ((ctinyint = cint)
               OR (cstring2 LIKE 'ss')))
          AND ((988888 < cdouble)
              OR ((ctimestamp2 > -15)
                  AND (3569 >= cdouble)))))
ORDER BY cboolean1, cbigint, csmallint, ctinyint, ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9
LIMIT 25;

SELECT cboolean1,
       cbigint,
       csmallint,
       ctinyint,
       ctimestamp1,
       cstring1,
       (cbigint + cbigint) as c1,
       (csmallint % -257) as c2,
       (-(csmallint)) as c3,
       (-(ctinyint)) as c4,
       ((-(ctinyint)) + 17) as c5,
       (cbigint * (-(csmallint))) as c6,
       (cint % csmallint) as c7,
       (-(ctinyint)) as c8,
       ((-(ctinyint)) % ctinyint) as c9
FROM   alltypesorc
WHERE  ((ctinyint != 0)
        AND (((ctimestamp1 <= 0)
          OR ((ctinyint = cint)
               OR (cstring2 LIKE 'ss')))
          AND ((988888 < cdouble)
              OR ((ctimestamp2 > -15)
                  AND (3569 >= cdouble)))))
ORDER BY cboolean1, cbigint, csmallint, ctinyint, ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9
LIMIT 25;


-- double compare timestamp
EXPLAIN
SELECT cboolean1,
       cbigint,
       csmallint,
       ctinyint,
       ctimestamp1,
       cstring1,
       (cbigint + cbigint) as c1,
       (csmallint % -257) as c2,
       (-(csmallint)) as c3,
       (-(ctinyint)) as c4,
       ((-(ctinyint)) + 17) as c5,
       (cbigint * (-(csmallint))) as c6,
       (cint % csmallint) as c7,
       (-(ctinyint)) as c8,
       ((-(ctinyint)) % ctinyint) as c9
FROM   alltypesorc
WHERE  ((ctinyint != 0)
        AND (((ctimestamp1 <= 0.0)
          OR ((ctinyint = cint)
               OR (cstring2 LIKE 'ss')))
          AND ((988888 < cdouble)
              OR ((ctimestamp2 > 7.6850000000000005)
                  AND (3569 >= cdouble)))))
ORDER BY cboolean1, cbigint, csmallint, ctinyint, ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9
LIMIT 25;

SELECT cboolean1,
       cbigint,
       csmallint,
       ctinyint,
       ctimestamp1,
       cstring1,
       (cbigint + cbigint) as c1,
       (csmallint % -257) as c2,
       (-(csmallint)) as c3,
       (-(ctinyint)) as c4,
       ((-(ctinyint)) + 17) as c5,
       (cbigint * (-(csmallint))) as c6,
       (cint % csmallint) as c7,
       (-(ctinyint)) as c8,
       ((-(ctinyint)) % ctinyint) as c9
FROM   alltypesorc
WHERE  ((ctinyint != 0)
        AND (((ctimestamp1 <= 0.0)
          OR ((ctinyint = cint)
               OR (cstring2 LIKE 'ss')))
          AND ((988888 < cdouble)
              OR ((ctimestamp2 > 7.6850000000000005)
                  AND (3569 >= cdouble)))))
ORDER BY cboolean1, cbigint, csmallint, ctinyint, ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9
LIMIT 25;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT ctimestamp1,
       cdouble,
       cboolean1,
       cstring1,
       cfloat,
       (-(cdouble)) as c1,
       (-5638.15 - cdouble) as c2,
       (cdouble * -257) as c3,
       (cint + cfloat) as c4,
       ((-(cdouble)) + cbigint) as c5,
       (-(cdouble)) as c6,
       (-1.389 - cfloat) as c7,
       (-(cfloat)) as c8,
       ((-5638.15 - cdouble) + (cint + cfloat)) as c9
FROM   alltypesorc
WHERE  (((cstring2 IS NOT NULL)
         AND ((ctimestamp1 <= 10)
             AND (ctimestamp2 != 16)))
        OR ((cfloat < -6432)
            OR ((cboolean1 IS NOT NULL)
                AND (cdouble = 988888))))
ORDER BY ctimestamp1, cdouble, cboolean1, cstring1, cfloat, c1, c2, c3, c4, c5, c6, c7, c8, c9
LIMIT 20;

SELECT ctimestamp1,
       cdouble,
       cboolean1,
       cstring1,
       cfloat,
       (-(cdouble)) as c1,
       (-5638.15 - cdouble) as c2,
       (cdouble * -257) as c3,
       (cint + cfloat) as c4,
       ((-(cdouble)) + cbigint) as c5,
       (-(cdouble)) as c6,
       (-1.389 - cfloat) as c7,
       (-(cfloat)) as c8,
       ((-5638.15 - cdouble) + (cint + cfloat)) as c9
FROM   alltypesorc
WHERE  (((cstring2 IS NOT NULL)
         AND ((ctimestamp1 <= 10)
             AND (ctimestamp2 != 16)))
        OR ((cfloat < -6432)
            OR ((cboolean1 IS NOT NULL)
                AND (cdouble = 988888))))
ORDER BY ctimestamp1, cdouble, cboolean1, cstring1, cfloat, c1, c2, c3, c4, c5, c6, c7, c8, c9
LIMIT 20;


-- double compare timestamp
EXPLAIN
SELECT ctimestamp1,
       cdouble,
       cboolean1,
       cstring1,
       cfloat,
       (-(cdouble)) as c1,
       (-5638.15 - cdouble) as c2,
       (cdouble * -257) as c3,
       (cint + cfloat) as c4,
       ((-(cdouble)) + cbigint) as c5,
       (-(cdouble)) as c6,
       (-1.389 - cfloat) as c7,
       (-(cfloat)) as c8,
       ((-5638.15 - cdouble) + (cint + cfloat)) as c9
FROM   alltypesorc
WHERE  (((cstring2 IS NOT NULL)
         AND ((ctimestamp1 <= 12.503)
             AND (ctimestamp2 != 11.998)))
        OR ((cfloat < -6432)
            OR ((cboolean1 IS NOT NULL)
                AND (cdouble = 988888))))
ORDER BY ctimestamp1, cdouble, cboolean1, cstring1, cfloat, c1, c2, c3, c4, c5, c6, c7, c8, c9
LIMIT 20;

SELECT ctimestamp1,
       cdouble,
       cboolean1,
       cstring1,
       cfloat,
       (-(cdouble)) as c1,
       (-5638.15 - cdouble) as c2,
       (cdouble * -257) as c3,
       (cint + cfloat) as c4,
       ((-(cdouble)) + cbigint) as c5,
       (-(cdouble)) as c6,
       (-1.389 - cfloat) as c7,
       (-(cfloat)) as c8,
       ((-5638.15 - cdouble) + (cint + cfloat)) as c9
FROM   alltypesorc
WHERE  (((cstring2 IS NOT NULL)
         AND ((ctimestamp1 <= 12.503)
             AND (ctimestamp2 != 11.998)))
        OR ((cfloat < -6432)
            OR ((cboolean1 IS NOT NULL)
                AND (cdouble = 988888))))
ORDER BY ctimestamp1, cdouble, cboolean1, cstring1, cfloat, c1, c2, c3, c4, c5, c6, c7, c8, c9
LIMIT 20;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

EXPLAIN
SELECT   cstring1,
         cdouble,
         ctimestamp1,
         (cdouble - 9763215.5639),
         (-((cdouble - 9763215.5639))),
         COUNT(cdouble),
         STDDEV_SAMP(cdouble),
         (-(STDDEV_SAMP(cdouble))),
         (STDDEV_SAMP(cdouble) * COUNT(cdouble)),
         MIN(cdouble),
         (9763215.5639 / cdouble),
         (COUNT(cdouble) / -1.389),
         STDDEV_SAMP(cdouble)
FROM     alltypesorc
WHERE    ((cstring2 LIKE '%b%')
          AND ((cdouble >= -1.389)
              OR (cstring1 < 'a')))
GROUP BY cstring1, cdouble, ctimestamp1;

SELECT   cstring1,
         cdouble,
         ctimestamp1,
         (cdouble - 9763215.5639),
         (-((cdouble - 9763215.5639))),
         COUNT(cdouble),
         STDDEV_SAMP(cdouble),
         (-(STDDEV_SAMP(cdouble))),
         (STDDEV_SAMP(cdouble) * COUNT(cdouble)),
         MIN(cdouble),
         (9763215.5639 / cdouble),
         (COUNT(cdouble) / -1.389),
         STDDEV_SAMP(cdouble)
FROM     alltypesorc
WHERE    ((cstring2 LIKE '%b%')
          AND ((cdouble >= -1.389)
              OR (cstring1 < 'a')))
GROUP BY cstring1, cdouble, ctimestamp1;

set hive.explain.user=false;
CREATE TABLE date_decimal_test STORED AS ORC AS SELECT cint, cdouble, CAST (CAST (cint AS TIMESTAMP) AS DATE) AS cdate, CAST (((cdouble*22.1)/37) AS DECIMAL(20,10)) AS cdecimal FROM alltypesorc;
SET hive.vectorized.execution.enabled=true;
EXPLAIN SELECT cdate, cdecimal from date_decimal_test where cint IS NOT NULL AND cdouble IS NOT NULL LIMIT 10;
SELECT cdate, cdecimal from date_decimal_test where cint IS NOT NULL AND cdouble IS NOT NULL LIMIT 10;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled = true;

-- TODO: add more stuff here after HIVE-5918 is fixed, such as cbigint and constants
explain
select cdouble / 0.0 from alltypesorc limit 100;
select cdouble / 0.0 from alltypesorc limit 100;

-- There are no zeros in the table, but there is 988888, so use it as zero

-- TODO: add more stuff here after HIVE-5918 is fixed, such as cbigint and constants as numerators
explain
select (cbigint - 988888L) as s1, cdouble / (cbigint - 988888L) as s2, 1.2 / (cbigint - 988888L)
from alltypesorc where cbigint > 0 and cbigint < 100000000 order by s1, s2 limit 100;
select (cbigint - 988888L) as s1, cdouble / (cbigint - 988888L) as s2, 1.2 / (cbigint - 988888L)
from alltypesorc where cbigint > 0 and cbigint < 100000000 order by s1, s2 limit 100;

-- There are no zeros in the table, but there is -200.0, so use it as zero

explain
select (cdouble + 200.0) as s1, cbigint / (cdouble + 200.0) as s2, (cdouble + 200.0) / (cdouble + 200.0), cbigint / (cdouble + 200.0), 3 / (cdouble + 200.0), 1.2 / (cdouble + 200.0)
from alltypesorc where cdouble >= -500 and cdouble < -199 order by s1, s2 limit 100;
select (cdouble + 200.0) as s1, cbigint / (cdouble + 200.0) as s2, (cdouble + 200.0) / (cdouble + 200.0), cbigint / (cdouble + 200.0), 3 / (cdouble + 200.0), 1.2 / (cdouble + 200.0)
from alltypesorc where cdouble >= -500 and cdouble < -199 order by s1, s2 limit 100;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
explain SELECT cbigint, cdouble FROM alltypesorc WHERE cbigint < cdouble and cint > 0 limit 7;
SELECT cbigint, cdouble FROM alltypesorc WHERE cbigint < cdouble and cint > 0 limit 7;

set hive.optimize.reducededuplication.min.reducer=1;
set hive.limit.pushdown.memory.usage=0.3f;

-- HIVE-3562 Some limit can be pushed down to map stage - c/p parts from limit_pushdown

explain
select ctinyint,cdouble,csmallint from alltypesorc where ctinyint is not null order by ctinyint,cdouble limit 20;
select ctinyint,cdouble,csmallint from alltypesorc where ctinyint is not null order by ctinyint,cdouble limit 20;

-- deduped RS
explain
select ctinyint,avg(cdouble + 1) from alltypesorc group by ctinyint order by ctinyint limit 20;
select ctinyint,avg(cdouble + 1) from alltypesorc group by ctinyint order by ctinyint limit 20;

-- distincts
explain
select distinct(ctinyint) from alltypesorc limit 20;
select distinct(ctinyint) from alltypesorc limit 20;

explain
select ctinyint, count(distinct(cdouble)) from alltypesorc group by ctinyint order by ctinyint limit 20;
select ctinyint, count(distinct(cdouble)) from alltypesorc group by ctinyint order by ctinyint limit 20;

-- limit zero
explain
select ctinyint,cdouble from alltypesorc order by ctinyint limit 0;
select ctinyint,cdouble from alltypesorc order by ctinyint limit 0;

-- 2MR (applied to last RS)
explain
select cdouble, sum(ctinyint) as sum from alltypesorc where ctinyint is not null group by cdouble order by sum, cdouble limit 20;
select cdouble, sum(ctinyint) as sum from alltypesorc where ctinyint is not null group by cdouble order by sum, cdouble limit 20;

SET hive.vectorized.execution.enabled=true;
SELECT SUM(abs(ctinyint)) from alltypesorc;

set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
SELECT AVG(cbigint),
       (-(AVG(cbigint))),
       (-6432 + AVG(cbigint)),
       STDDEV_POP(cbigint),
       (-((-6432 + AVG(cbigint)))),
       ((-((-6432 + AVG(cbigint)))) + (-6432 + AVG(cbigint))),
       VAR_SAMP(cbigint),
       (-((-6432 + AVG(cbigint)))),
       (-6432 + (-((-6432 + AVG(cbigint))))),
       (-((-6432 + AVG(cbigint)))),
       ((-((-6432 + AVG(cbigint)))) / (-((-6432 + AVG(cbigint))))),
       COUNT(*),
       SUM(cfloat),
       (VAR_SAMP(cbigint) % STDDEV_POP(cbigint)),
       (-(VAR_SAMP(cbigint))),
       ((-((-6432 + AVG(cbigint)))) * (-(AVG(cbigint)))),
       MIN(ctinyint),
       (-(MIN(ctinyint)))
FROM   alltypesorc
WHERE  (((cstring2 LIKE '%b%')
         OR ((79.553 != cint)
             OR (NOT(cbigint >= cdouble))))
        OR ((ctinyint >= csmallint)
            AND (NOT ((cboolean2 != 1)
                 OR (3569 != ctinyint)))));

set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.mapred.mode=nonstrict;

explain SELECT cbigint, cdouble FROM alltypesorc WHERE cbigint < cdouble and cint > 0 limit 3,2;
SELECT cbigint, cdouble FROM alltypesorc WHERE cbigint < cdouble and cint > 0 limit 3,2;

explain
select ctinyint,cdouble,csmallint from alltypesorc where ctinyint is not null order by ctinyint,cdouble limit 10,3;
select ctinyint,cdouble,csmallint from alltypesorc where ctinyint is not null order by ctinyint,cdouble limit 10,3;set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
CREATE TABLE alltypesorc_part(ctinyint tinyint, csmallint smallint, cint int, cbigint bigint, cfloat float, cdouble double, cstring1 string, cstring2 string, ctimestamp1 timestamp, ctimestamp2 timestamp, cboolean1 boolean, cboolean2 boolean) partitioned by (ds string) STORED AS ORC;
insert overwrite table alltypesorc_part partition (ds='2011') select * from alltypesorc limit 100;
insert overwrite table alltypesorc_part partition (ds='2012') select * from alltypesorc limit 100;

select count(cdouble), cint from alltypesorc_part where ds='2011' group by cint limit 10;
select count(*) from alltypesorc_part A join alltypesorc_part B on A.ds=B.ds;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
CREATE TABLE alltypesorc_part(ctinyint tinyint, csmallint smallint, cint int, cbigint bigint, cfloat float, cdouble double, cstring1 string, cstring2 string, ctimestamp1 timestamp, ctimestamp2 timestamp, cboolean1 boolean, cboolean2 boolean) partitioned by (ds string) STORED AS ORC;
insert overwrite table alltypesorc_part partition (ds='2011') select * from alltypesorc order by ctinyint, cint, cbigint limit 100;
insert overwrite table alltypesorc_part partition (ds='2012') select * from alltypesorc order by ctinyint, cint, cbigint limit 100;

explain select (cdouble+2) c1 from alltypesorc_part order by c1 limit 10;
select (cdouble+2) c1 from alltypesorc_part order by c1 limit 10;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
CREATE TABLE alltypesorc_part_varchar(ctinyint tinyint, csmallint smallint, cint int, cbigint bigint, cfloat float, cdouble double, cstring1 string, cstring2 string, ctimestamp1 timestamp, ctimestamp2 timestamp, cboolean1 boolean, cboolean2 boolean) partitioned by (ds varchar(4)) STORED AS ORC;
insert overwrite table alltypesorc_part_varchar partition (ds='2011') select * from alltypesorc limit 100;
insert overwrite table alltypesorc_part_varchar partition (ds='2012') select * from alltypesorc limit 100;

select count(cdouble), cint from alltypesorc_part_varchar where ds='2011' group by cint limit 10;
select count(*) from alltypesorc_part_varchar A join alltypesorc_part_varchar B on A.ds=B.ds;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.optimize.index.filter=true;
explain SELECT AVG(cbigint) FROM alltypesorc WHERE cbigint < cdouble;
SELECT AVG(cbigint) FROM alltypesorc WHERE cbigint < cdouble;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

-- SORT_QUERY_RESULTS

-- If you look at ql/src/test/org/apache/hadoop/hive/ql/exec/vector/util/OrcFileGenerator.java
-- which is the data generation class you'll see that those values are specified in the
-- initializeFixedPointValues for each data type. When I created the queries I usedthose values
-- where I needed scalar values to ensure that when the queries executed their predicates would be
-- filtering on values that are guaranteed to exist.

-- Beyond those values, all the other data in the alltypesorc file is random, but there is a
-- specific pattern to the data that is important for coverage. In orc and subsequently
-- vectorization there are a number of optimizations for certain data patterns: AllValues, NoNulls,
-- RepeatingValue, RepeatingNull. The data in alltypesorc is generated such that each column has
-- exactly 3 batches of each data pattern. This gives us coverage for the vector expression
-- optimizations and ensure the metadata in appropriately set on the row batch object which are
-- reused across batches.

-- For the queries themselves in order to efficiently cover as much of the new vectorization
-- functionality as I could I used a number of different techniques to create the
-- vectorization_short_regress.q test suite, primarily equivalence classes, and pairwise
-- combinations.

-- First I divided the search space into a number of dimensions such as type, aggregate function,
-- filter operation, arithmetic operation, etc. The types were explored as equivalence classes of
-- long, double, time, string, and bool. Also, rather than creating a very large number of small
-- queries the resulting vectors were grouped by compatible dimensions to reduce the number of
-- queries.

-- TargetTypeClasses: Long, Timestamp, Double, String, Bool
-- Functions: Avg, Sum, StDevP, StDev, Var, Min, Count
-- ArithmeticOps: Add, Multiply, Subtract, Divide
-- FilterOps: Equal, NotEqual, GreaterThan, LessThan, LessThanOrEqual
-- GroupBy: NoGroupByProjectAggs
EXPLAIN SELECT AVG(cint),
       (AVG(cint) + -3728),
       (-((AVG(cint) + -3728))),
       (-((-((AVG(cint) + -3728))))),
       ((-((-((AVG(cint) + -3728))))) * (AVG(cint) + -3728)),
       SUM(cdouble),
       (-(AVG(cint))),
       STDDEV_POP(cint),
       (((-((-((AVG(cint) + -3728))))) * (AVG(cint) + -3728)) * (-((-((AVG(cint) + -3728)))))),
       STDDEV_SAMP(csmallint),
       (-(STDDEV_POP(cint))),
       (STDDEV_POP(cint) - (-((-((AVG(cint) + -3728)))))),
       ((STDDEV_POP(cint) - (-((-((AVG(cint) + -3728)))))) * STDDEV_POP(cint)),
       VAR_SAMP(cint),
       AVG(cfloat),
       (10.175 - VAR_SAMP(cint)),
       (-((10.175 - VAR_SAMP(cint)))),
       ((-(STDDEV_POP(cint))) / -563),
       STDDEV_SAMP(cint),
       (-(((-(STDDEV_POP(cint))) / -563))),
       (AVG(cint) / SUM(cdouble)),
       MIN(ctinyint),
       COUNT(csmallint),
       (MIN(ctinyint) / ((-(STDDEV_POP(cint))) / -563)),
       (-((AVG(cint) / SUM(cdouble))))
FROM   alltypesorc
WHERE  ((762 = cbigint)
        OR ((csmallint < cfloat)
            AND ((ctimestamp2 > -5)
                 AND (cdouble != cint)))
        OR (cstring1 = 'a')
           OR ((cbigint <= -1.389)
               AND ((cstring2 != 'a')
                    AND ((79.553 != cint)
                         AND (cboolean2 != cboolean1)))));
SELECT AVG(cint),
       (AVG(cint) + -3728),
       (-((AVG(cint) + -3728))),
       (-((-((AVG(cint) + -3728))))),
       ((-((-((AVG(cint) + -3728))))) * (AVG(cint) + -3728)),
       SUM(cdouble),
       (-(AVG(cint))),
       STDDEV_POP(cint),
       (((-((-((AVG(cint) + -3728))))) * (AVG(cint) + -3728)) * (-((-((AVG(cint) + -3728)))))),
       STDDEV_SAMP(csmallint),
       (-(STDDEV_POP(cint))),
       (STDDEV_POP(cint) - (-((-((AVG(cint) + -3728)))))),
       ((STDDEV_POP(cint) - (-((-((AVG(cint) + -3728)))))) * STDDEV_POP(cint)),
       VAR_SAMP(cint),
       AVG(cfloat),
       (10.175 - VAR_SAMP(cint)),
       (-((10.175 - VAR_SAMP(cint)))),
       ((-(STDDEV_POP(cint))) / -563),
       STDDEV_SAMP(cint),
       (-(((-(STDDEV_POP(cint))) / -563))),
       (AVG(cint) / SUM(cdouble)),
       MIN(ctinyint),
       COUNT(csmallint),
       (MIN(ctinyint) / ((-(STDDEV_POP(cint))) / -563)),
       (-((AVG(cint) / SUM(cdouble))))
FROM   alltypesorc
WHERE  ((762 = cbigint)
        OR ((csmallint < cfloat)
            AND ((ctimestamp2 > -5)
                 AND (cdouble != cint)))
        OR (cstring1 = 'a')
           OR ((cbigint <= -1.389)
               AND ((cstring2 != 'a')
                    AND ((79.553 != cint)
                         AND (cboolean2 != cboolean1)))));

-- TargetTypeClasses: Long, Bool, Double, String, Timestamp
-- Functions: Max, VarP, StDevP, Avg, Min, StDev, Var
-- ArithmeticOps: Divide, Multiply, Remainder, Subtract
-- FilterOps: LessThan, LessThanOrEqual, GreaterThan, GreaterThanOrEqual, Like, RLike
-- GroupBy: NoGroupByProjectAggs
EXPLAIN SELECT MAX(cint),
       (MAX(cint) / -3728),
       (MAX(cint) * -3728),
       VAR_POP(cbigint),
       (-((MAX(cint) * -3728))),
       STDDEV_POP(csmallint),
       (-563 % (MAX(cint) * -3728)),
       (VAR_POP(cbigint) / STDDEV_POP(csmallint)),
       (-(STDDEV_POP(csmallint))),
       MAX(cdouble),
       AVG(ctinyint),
       (STDDEV_POP(csmallint) - 10.175),
       MIN(cint),
       ((MAX(cint) * -3728) % (STDDEV_POP(csmallint) - 10.175)),
       (-(MAX(cdouble))),
       MIN(cdouble),
       (MAX(cdouble) % -26.28),
       STDDEV_SAMP(csmallint),
       (-((MAX(cint) / -3728))),
       ((-((MAX(cint) * -3728))) % (-563 % (MAX(cint) * -3728))),
       ((MAX(cint) / -3728) - AVG(ctinyint)),
       (-((MAX(cint) * -3728))),
       VAR_SAMP(cint)
FROM   alltypesorc
WHERE  (((cbigint <= 197)
         AND (cint < cbigint))
        OR ((cdouble >= -26.28)
            AND (csmallint > cdouble))
        OR ((ctinyint > cfloat)
            AND (cstring1 RLIKE '.*ss.*'))
           OR ((cfloat > 79.553)
               AND (cstring2 LIKE '10%')));
SELECT MAX(cint),
       (MAX(cint) / -3728),
       (MAX(cint) * -3728),
       VAR_POP(cbigint),
       (-((MAX(cint) * -3728))),
       STDDEV_POP(csmallint),
       (-563 % (MAX(cint) * -3728)),
       (VAR_POP(cbigint) / STDDEV_POP(csmallint)),
       (-(STDDEV_POP(csmallint))),
       MAX(cdouble),
       AVG(ctinyint),
       (STDDEV_POP(csmallint) - 10.175),
       MIN(cint),
       ((MAX(cint) * -3728) % (STDDEV_POP(csmallint) - 10.175)),
       (-(MAX(cdouble))),
       MIN(cdouble),
       (MAX(cdouble) % -26.28),
       STDDEV_SAMP(csmallint),
       (-((MAX(cint) / -3728))),
       ((-((MAX(cint) * -3728))) % (-563 % (MAX(cint) * -3728))),
       ((MAX(cint) / -3728) - AVG(ctinyint)),
       (-((MAX(cint) * -3728))),
       VAR_SAMP(cint)
FROM   alltypesorc
WHERE  (((cbigint <= 197)
         AND (cint < cbigint))
        OR ((cdouble >= -26.28)
            AND (csmallint > cdouble))
        OR ((ctinyint > cfloat)
            AND (cstring1 RLIKE '.*ss.*'))
           OR ((cfloat > 79.553)
               AND (cstring2 LIKE '10%')));

-- TargetTypeClasses: String, Long, Bool, Double, Timestamp
-- Functions: VarP, Count, Max, StDevP, StDev, Avg
-- ArithmeticOps: Subtract, Remainder, Multiply, Add
-- FilterOps: Equal, LessThanOrEqual, GreaterThan, Like, LessThan
-- GroupBy: NoGroupByProjectAggs
EXPLAIN SELECT VAR_POP(cbigint),
       (-(VAR_POP(cbigint))),
       (VAR_POP(cbigint) - (-(VAR_POP(cbigint)))),
       COUNT(*),
       (COUNT(*) % 79.553),
       MAX(ctinyint),
       (COUNT(*) - (-(VAR_POP(cbigint)))),
       (-((-(VAR_POP(cbigint))))),
       (-1 % (-(VAR_POP(cbigint)))),
       COUNT(*),
       (-(COUNT(*))),
       STDDEV_POP(csmallint),
       (-((-((-(VAR_POP(cbigint))))))),
       (762 * (-(COUNT(*)))),
       MAX(cint),
       (MAX(ctinyint) + (762 * (-(COUNT(*))))),
       ((-(VAR_POP(cbigint))) + MAX(cint)),
       STDDEV_SAMP(cdouble),
       ((-(COUNT(*))) % COUNT(*)),
       COUNT(ctinyint),
       AVG(ctinyint),
       (-3728 % (MAX(ctinyint) + (762 * (-(COUNT(*))))))
FROM   alltypesorc
WHERE  ((ctimestamp1 = ctimestamp2)
        OR (762 = cfloat)
        OR (cstring1 = 'ss')
           OR ((csmallint <= cbigint)
               AND (1 = cboolean2))
              OR ((cboolean1 IS NOT NULL)
                  AND ((ctimestamp2 IS NOT NULL)
                       AND (cstring2 > 'a'))));
SELECT VAR_POP(cbigint),
       (-(VAR_POP(cbigint))),
       (VAR_POP(cbigint) - (-(VAR_POP(cbigint)))),
       COUNT(*),
       (COUNT(*) % 79.553),
       MAX(ctinyint),
       (COUNT(*) - (-(VAR_POP(cbigint)))),
       (-((-(VAR_POP(cbigint))))),
       (-1 % (-(VAR_POP(cbigint)))),
       COUNT(*),
       (-(COUNT(*))),
       STDDEV_POP(csmallint),
       (-((-((-(VAR_POP(cbigint))))))),
       (762 * (-(COUNT(*)))),
       MAX(cint),
       (MAX(ctinyint) + (762 * (-(COUNT(*))))),
       ((-(VAR_POP(cbigint))) + MAX(cint)),
       STDDEV_SAMP(cdouble),
       ((-(COUNT(*))) % COUNT(*)),
       COUNT(ctinyint),
       AVG(ctinyint),
       (-3728 % (MAX(ctinyint) + (762 * (-(COUNT(*))))))
FROM   alltypesorc
WHERE  ((ctimestamp1 = ctimestamp2)
        OR (762 = cfloat)
        OR (cstring1 = 'ss')
           OR ((csmallint <= cbigint)
               AND (1 = cboolean2))
              OR ((cboolean1 IS NOT NULL)
                  AND ((ctimestamp2 IS NOT NULL)
                       AND (cstring2 > 'a'))));

-- TargetTypeClasses: String, Bool, Timestamp, Long, Double
-- Functions: Avg, Max, StDev, VarP
-- ArithmeticOps: Add, Divide, Remainder, Multiply
-- FilterOps: LessThanOrEqual, NotEqual, GreaterThanOrEqual, LessThan, Equal
-- GroupBy: NoGroupByProjectAggs
EXPLAIN SELECT AVG(ctinyint),
       (AVG(ctinyint) + 6981),
       ((AVG(ctinyint) + 6981) + AVG(ctinyint)),
       MAX(cbigint),
       (((AVG(ctinyint) + 6981) + AVG(ctinyint)) / AVG(ctinyint)),
       (-((AVG(ctinyint) + 6981))),
       STDDEV_SAMP(cint),
       (AVG(ctinyint) % (-((AVG(ctinyint) + 6981)))),
       VAR_POP(cint),
       VAR_POP(cbigint),
       (-(MAX(cbigint))),
       ((-(MAX(cbigint))) / STDDEV_SAMP(cint)),
       MAX(cfloat),
       (VAR_POP(cbigint) * -26.28)
FROM   alltypesorc
WHERE  (((ctimestamp2 <= ctimestamp1)
         AND ((cbigint != cdouble)
              AND ('ss' <= cstring1)))
        OR ((csmallint < ctinyint)
            AND (ctimestamp1 >= 0))
           OR (cfloat = 17));
SELECT AVG(ctinyint),
       (AVG(ctinyint) + 6981),
       ((AVG(ctinyint) + 6981) + AVG(ctinyint)),
       MAX(cbigint),
       (((AVG(ctinyint) + 6981) + AVG(ctinyint)) / AVG(ctinyint)),
       (-((AVG(ctinyint) + 6981))),
       STDDEV_SAMP(cint),
       (AVG(ctinyint) % (-((AVG(ctinyint) + 6981)))),
       VAR_POP(cint),
       VAR_POP(cbigint),
       (-(MAX(cbigint))),
       ((-(MAX(cbigint))) / STDDEV_SAMP(cint)),
       MAX(cfloat),
       (VAR_POP(cbigint) * -26.28)
FROM   alltypesorc
WHERE  (((ctimestamp2 <= ctimestamp1)
         AND ((cbigint != cdouble)
              AND ('ss' <= cstring1)))
        OR ((csmallint < ctinyint)
            AND (ctimestamp1 >= 0))
           OR (cfloat = 17));

-- TargetTypeClasses: Timestamp, String, Long, Double, Bool
-- Functions: Max, Avg, Min, Var, StDev, Count, StDevP, Sum
-- ArithmeticOps: Multiply, Subtract, Add, Divide
-- FilterOps: Like, NotEqual, LessThan, GreaterThanOrEqual, GreaterThan, RLike
-- GroupBy: NoGroupByProjectColumns
EXPLAIN SELECT cint,
       cdouble,
       ctimestamp2,
       cstring1,
       cboolean2,
       ctinyint,
       cfloat,
       ctimestamp1,
       csmallint,
       cbigint,
       (-3728 * cbigint) as c1,
       (-(cint)) as c2,
       (-863.257 - cint) as c3,
       (-(csmallint)) as c4,
       (csmallint - (-(csmallint))) as c5,
       ((csmallint - (-(csmallint))) + (-(csmallint))) as c6,
       (cint / cint) as c7,
       ((-863.257 - cint) - -26.28) as c8,
       (-(cfloat)) as c9,
       (cdouble * -89010) as c10,
       (ctinyint / 988888) as c11,
       (-(ctinyint)) as c12,
       (79.553 / ctinyint) as c13
FROM   alltypesorc
WHERE  (((cstring1 RLIKE 'a.*')
         AND (cstring2 LIKE '%ss%'))
        OR ((1 != cboolean2)
            AND ((csmallint < 79.553)
                 AND (-257 != ctinyint)))
        OR ((cdouble > ctinyint)
            AND (cfloat >= cint))
           OR ((cint < cbigint)
               AND (ctinyint > cbigint)))
ORDER BY cint, cdouble, ctimestamp2, cstring1, cboolean2, ctinyint, cfloat, ctimestamp1, csmallint, cbigint, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13
LIMIT 50;

SELECT cint,
       cdouble,
       ctimestamp2,
       cstring1,
       cboolean2,
       ctinyint,
       cfloat,
       ctimestamp1,
       csmallint,
       cbigint,
       (-3728 * cbigint) as c1,
       (-(cint)) as c2,
       (-863.257 - cint) as c3,
       (-(csmallint)) as c4,
       (csmallint - (-(csmallint))) as c5,
       ((csmallint - (-(csmallint))) + (-(csmallint))) as c6,
       (cint / cint) as c7,
       ((-863.257 - cint) - -26.28) as c8,
       (-(cfloat)) as c9,
       (cdouble * -89010) as c10,
       (ctinyint / 988888) as c11,
       (-(ctinyint)) as c12,
       (79.553 / ctinyint) as c13
FROM   alltypesorc
WHERE  (((cstring1 RLIKE 'a.*')
         AND (cstring2 LIKE '%ss%'))
        OR ((1 != cboolean2)
            AND ((csmallint < 79.553)
                 AND (-257 != ctinyint)))
        OR ((cdouble > ctinyint)
            AND (cfloat >= cint))
           OR ((cint < cbigint)
               AND (ctinyint > cbigint)))
ORDER BY cint, cdouble, ctimestamp2, cstring1, cboolean2, ctinyint, cfloat, ctimestamp1, csmallint, cbigint, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13
LIMIT 50;


-- TargetTypeClasses: Long, String, Double, Bool, Timestamp
-- Functions: VarP, Var, StDev, StDevP, Max, Sum
-- ArithmeticOps: Divide, Remainder, Subtract, Multiply
-- FilterOps: Equal, LessThanOrEqual, LessThan, Like, GreaterThanOrEqual, NotEqual, GreaterThan
-- GroupBy: NoGroupByProjectColumns
EXPLAIN SELECT cint,
       cbigint,
       cstring1,
       cboolean1,
       cfloat,
       cdouble,
       ctimestamp2,
       csmallint,
       cstring2,
       cboolean2,
       (cint / cbigint) as c1,
       (cbigint % 79.553) as c2,
       (-((cint / cbigint))) as c3,
       (10.175 % cfloat) as c4,
       (-(cfloat)) as c5,
       (cfloat - (-(cfloat))) as c6,
       ((cfloat - (-(cfloat))) % -6432) as c7,
       (cdouble * csmallint) as c8,
       (-(cdouble)) as c9,
       (-(cbigint)) as c10,
       (cfloat - (cint / cbigint)) as c11,
       (-(csmallint)) as c12,
       (3569 % cbigint) as c13,
       (359 - cdouble) as c14,
       (-(csmallint)) as c15
FROM   alltypesorc
WHERE  (((197 > ctinyint)
         AND (cint = cbigint))
        OR (cbigint = 359)
        OR (cboolean1 < 0)
           OR ((cstring1 LIKE '%ss')
               AND (cfloat <= ctinyint)))
ORDER BY cint, cbigint, cstring1, cboolean1, cfloat, cdouble, ctimestamp2, csmallint, cstring2, cboolean2, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15
LIMIT 25;

SELECT cint,
       cbigint,
       cstring1,
       cboolean1,
       cfloat,
       cdouble,
       ctimestamp2,
       csmallint,
       cstring2,
       cboolean2,
       (cint / cbigint) as c1,
       (cbigint % 79.553) as c2,
       (-((cint / cbigint))) as c3,
       (10.175 % cfloat) as c4,
       (-(cfloat)) as c5,
       (cfloat - (-(cfloat))) as c6,
       ((cfloat - (-(cfloat))) % -6432) as c7,
       (cdouble * csmallint) as c8,
       (-(cdouble)) as c9,
       (-(cbigint)) as c10,
       (cfloat - (cint / cbigint)) as c11,
       (-(csmallint)) as c12,
       (3569 % cbigint) as c13,
       (359 - cdouble) as c14,
       (-(csmallint)) as c15
FROM   alltypesorc
WHERE  (((197 > ctinyint)
         AND (cint = cbigint))
        OR (cbigint = 359)
        OR (cboolean1 < 0)
           OR ((cstring1 LIKE '%ss')
               AND (cfloat <= ctinyint)))
ORDER BY cint, cbigint, cstring1, cboolean1, cfloat, cdouble, ctimestamp2, csmallint, cstring2, cboolean2, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15
LIMIT 25;

-- TargetTypeClasses: String, Bool, Double, Long, Timestamp
-- Functions: Sum, Max, Avg, Var, StDevP, VarP
-- ArithmeticOps: Add, Subtract, Divide, Multiply, Remainder
-- FilterOps: NotEqual, GreaterThanOrEqual, Like, LessThanOrEqual, Equal, GreaterThan
-- GroupBy: NoGroupByProjectColumns
EXPLAIN SELECT   cint,
         cstring1,
         cboolean2,
         ctimestamp2,
         cdouble,
         cfloat,
         cbigint,
         csmallint,
         cboolean1,
         (cint + csmallint) as c1,
         (cbigint - ctinyint) as c2,
         (-(cbigint)) as c3,
         (-(cfloat)) as c4,
         ((cbigint - ctinyint) + cbigint) as c5,
         (cdouble / cdouble) as c6,
         (-(cdouble)) as c7,
         ((cint + csmallint) * (-(cbigint))) as c8,
         ((-(cdouble)) + cbigint) as c9,
         (-1.389 / ctinyint) as c10,
         (cbigint % cdouble) as c11,
         (-(csmallint)) as c12,
         (csmallint + (cint + csmallint)) as c13
FROM     alltypesorc
WHERE    (((csmallint > -26.28)
           AND (cstring2 LIKE 'ss'))
          OR ((cdouble <= cbigint)
              AND ((cstring1 >= 'ss')
                   AND (cint != cdouble)))
          OR (ctinyint = -89010)
             OR ((cbigint <= cfloat)
                 AND (-26.28 <= csmallint)))
ORDER BY cboolean1, cstring1, ctimestamp2, cfloat, cbigint, cstring1, cdouble, cint, csmallint, cdouble, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13
LIMIT 75;

SELECT   cint,
         cstring1,
         cboolean2,
         ctimestamp2,
         cdouble,
         cfloat,
         cbigint,
         csmallint,
         cboolean1,
         (cint + csmallint) as c1,
         (cbigint - ctinyint) as c2,
         (-(cbigint)) as c3,
         (-(cfloat)) as c4,
         ((cbigint - ctinyint) + cbigint) as c5,
         (cdouble / cdouble) as c6,
         (-(cdouble)) as c7,
         ((cint + csmallint) * (-(cbigint))) as c8,
         ((-(cdouble)) + cbigint) as c9,
         (-1.389 / ctinyint) as c10,
         (cbigint % cdouble) as c11,
         (-(csmallint)) as c12,
         (csmallint + (cint + csmallint)) as c13
FROM     alltypesorc
WHERE    (((csmallint > -26.28)
           AND (cstring2 LIKE 'ss'))
          OR ((cdouble <= cbigint)
              AND ((cstring1 >= 'ss')
                   AND (cint != cdouble)))
          OR (ctinyint = -89010)
             OR ((cbigint <= cfloat)
                 AND (-26.28 <= csmallint)))
ORDER BY cboolean1, cstring1, ctimestamp2, cfloat, cbigint, cstring1, cdouble, cint, csmallint, cdouble, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13
LIMIT 75;

-- TargetTypeClasses: Long, String, Double, Timestamp
-- Functions: Avg, Min, StDevP, Sum, Var
-- ArithmeticOps: Divide, Subtract, Multiply, Remainder
-- FilterOps: GreaterThan, LessThan, LessThanOrEqual, GreaterThanOrEqual, Like
-- GroupBy: NoGroupByProjectColumns
EXPLAIN SELECT   ctimestamp1,
         cstring2,
         cdouble,
         cfloat,
         cbigint,
         csmallint,
         (cbigint / 3569) as c1,
         (-257 - csmallint) as c2,
         (-6432 * cfloat) as c3,
         (-(cdouble)) as c4,
         (cdouble * 10.175) as c5,
         ((-6432 * cfloat) / cfloat) as c6,
         (-(cfloat)) as c7,
         (cint % csmallint) as c8,
         (-(cdouble)) as c9,
         (cdouble * (-(cdouble))) as c10
FROM     alltypesorc
WHERE    (((-1.389 >= cint)
           AND ((csmallint < ctinyint)
                AND (-6432 > csmallint)))
          OR ((cdouble >= cfloat)
              AND (cstring2 <= 'a'))
             OR ((cstring1 LIKE 'ss%')
                 AND (10.175 > cbigint)))
ORDER BY csmallint, cstring2, cdouble, cfloat, cbigint, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10
LIMIT 45;

SELECT   ctimestamp1,
         cstring2,
         cdouble,
         cfloat,
         cbigint,
         csmallint,
         (cbigint / 3569) as c1,
         (-257 - csmallint) as c2,
         (-6432 * cfloat) as c3,
         (-(cdouble)) as c4,
         (cdouble * 10.175) as c5,
         ((-6432 * cfloat) / cfloat) as c6,
         (-(cfloat)) as c7,
         (cint % csmallint) as c8,
         (-(cdouble)) as c9,
         (cdouble * (-(cdouble))) as c10
FROM     alltypesorc
WHERE    (((-1.389 >= cint)
           AND ((csmallint < ctinyint)
                AND (-6432 > csmallint)))
          OR ((cdouble >= cfloat)
              AND (cstring2 <= 'a'))
             OR ((cstring1 LIKE 'ss%')
                 AND (10.175 > cbigint)))
ORDER BY csmallint, cstring2, cdouble, cfloat, cbigint, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10
LIMIT 45;

-- TargetTypeClasses: Double, String, Long
-- Functions: StDev, Sum, VarP, Count
-- ArithmeticOps: Remainder, Divide, Subtract
-- FilterOps: GreaterThanOrEqual, Equal, LessThanOrEqual
-- GroupBy: GroupBy
EXPLAIN SELECT   csmallint,
         (csmallint % -75) as c1,
         STDDEV_SAMP(csmallint) as c2,
         (-1.389 / csmallint) as c3,
         SUM(cbigint) as c4,
         ((csmallint % -75) / SUM(cbigint)) as c5,
         (-((csmallint % -75))) as c6,
         VAR_POP(ctinyint) as c7,
         (-((-((csmallint % -75))))) as c8,
         COUNT(*) as c9,
         (COUNT(*) - -89010) as c10
FROM     alltypesorc
WHERE    (((csmallint >= -257))
          AND ((-6432 = csmallint)
               OR ((cint >= cdouble)
                   AND (ctinyint <= cint))))
GROUP BY csmallint
ORDER BY csmallint, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10
LIMIT 20;

SELECT   csmallint,
         (csmallint % -75) as c1,
         STDDEV_SAMP(csmallint) as c2,
         (-1.389 / csmallint) as c3,
         SUM(cbigint) as c4,
         ((csmallint % -75) / SUM(cbigint)) as c5,
         (-((csmallint % -75))) as c6,
         VAR_POP(ctinyint) as c7,
         (-((-((csmallint % -75))))) as c8,
         COUNT(*) as c9,
         (COUNT(*) - -89010) as c10
FROM     alltypesorc
WHERE    (((csmallint >= -257))
          AND ((-6432 = csmallint)
               OR ((cint >= cdouble)
                   AND (ctinyint <= cint))))
GROUP BY csmallint
ORDER BY csmallint, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10
LIMIT 20;

-- TargetTypeClasses: Long, Double, Timestamp
-- Functions: Var, Count, Sum, VarP, StDevP
-- ArithmeticOps: Multiply, Add, Subtract, Remainder
-- FilterOps: GreaterThan, LessThan, Equal, LessThanOrEqual, GreaterThanOrEqual
-- GroupBy: GroupBy
EXPLAIN SELECT   cdouble,
         VAR_SAMP(cdouble),
         (2563.58 * VAR_SAMP(cdouble)),
         (-(VAR_SAMP(cdouble))),
         COUNT(cfloat),
         ((2563.58 * VAR_SAMP(cdouble)) + -5638.15),
         ((-(VAR_SAMP(cdouble))) * ((2563.58 * VAR_SAMP(cdouble)) + -5638.15)),
         SUM(cfloat),
         VAR_POP(cdouble),
         (cdouble - (-(VAR_SAMP(cdouble)))),
         STDDEV_POP(cdouble),
         (cdouble + VAR_SAMP(cdouble)),
         (cdouble * 762),
         SUM(cdouble),
         (-863.257 % (cdouble * 762)),
         SUM(cdouble)
FROM     alltypesorc
WHERE    (((cdouble > 2563.58))
          AND (((cbigint >= cint)
                AND ((csmallint < cint)
                     AND (cfloat < -5638.15)))
               OR (2563.58 = ctinyint)
                  OR ((cdouble <= cbigint)
                      AND (-5638.15 > cbigint))))
GROUP BY cdouble
ORDER BY cdouble;
SELECT   cdouble,
         VAR_SAMP(cdouble),
         (2563.58 * VAR_SAMP(cdouble)),
         (-(VAR_SAMP(cdouble))),
         COUNT(cfloat),
         ((2563.58 * VAR_SAMP(cdouble)) + -5638.15),
         ((-(VAR_SAMP(cdouble))) * ((2563.58 * VAR_SAMP(cdouble)) + -5638.15)),
         SUM(cfloat),
         VAR_POP(cdouble),
         (cdouble - (-(VAR_SAMP(cdouble)))),
         STDDEV_POP(cdouble),
         (cdouble + VAR_SAMP(cdouble)),
         (cdouble * 762),
         SUM(cdouble),
         (-863.257 % (cdouble * 762)),
         SUM(cdouble)
FROM     alltypesorc
WHERE    (((cdouble > 2563.58))
          AND (((cbigint >= cint)
                AND ((csmallint < cint)
                     AND (cfloat < -5638.15)))
               OR (2563.58 = ctinyint)
                  OR ((cdouble <= cbigint)
                      AND (-5638.15 > cbigint))))
GROUP BY cdouble
ORDER BY cdouble;

-- TargetTypeClasses: Bool, Timestamp, String, Double, Long
-- Functions: StDevP, Avg, Count, Min, Var, VarP, Sum
-- ArithmeticOps: Multiply, Subtract, Add, Divide, Remainder
-- FilterOps: NotEqual, LessThan, Like, Equal, RLike
-- GroupBy: GroupBy
EXPLAIN SELECT   ctimestamp1,
         cstring1,
         STDDEV_POP(cint) as c1,
         (STDDEV_POP(cint) * 10.175) as c2,
         (-(STDDEV_POP(cint))) as c3,
         AVG(csmallint) as c4,
         (-(STDDEV_POP(cint))) as c5,
         (-26.28 - STDDEV_POP(cint)) as c6,
         COUNT(*) as c7,
         (-(COUNT(*))) as c8,
         ((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) as c9,
         MIN(ctinyint) as c10,
         (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*)))) as c11,
         (-((STDDEV_POP(cint) * 10.175))) as c12,
         VAR_SAMP(csmallint) as c13,
         (VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) as c14,
         (-((-(STDDEV_POP(cint))))) as c15,
         ((-(COUNT(*))) / STDDEV_POP(cint)) as c16,
         VAR_POP(cfloat) as c17,
         (10.175 / AVG(csmallint)) as c18,
         AVG(cint) as c19,
         VAR_SAMP(cfloat) as c20,
         ((VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) - (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) as c21,
         (-((-((STDDEV_POP(cint) * 10.175))))) as c22,
         AVG(cfloat) as c23,
         (((VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) - (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) * 10.175) as c24,
         (10.175 % (10.175 / AVG(csmallint))) as c25,
         (-(MIN(ctinyint))) as c26,
         MIN(cdouble) as c27,
         VAR_POP(csmallint) as c28,
         (-(((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))))) as c29,
         ((-(STDDEV_POP(cint))) % AVG(cfloat)) as c30,
         (-26.28 / (-(MIN(ctinyint)))) as c31,
         STDDEV_POP(ctinyint) as c32,
         SUM(cint) as c33,
         ((VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) / VAR_POP(cfloat)) as c34,
         (-((-(COUNT(*))))) as c35,
         COUNT(*) as c36,
         ((VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) % -26.28) as c37
FROM     alltypesorc
WHERE    (((ctimestamp1 != 0))
          AND ((((-257 != ctinyint)
                 AND (cboolean2 IS NOT NULL))
                AND ((cstring1 RLIKE '.*ss')
                     AND (-3 < ctimestamp1)))
               OR (ctimestamp2 = -5)
               OR ((ctimestamp1 < 0)
                   AND (cstring2 LIKE '%b%'))
                  OR (cdouble = cint)
                     OR ((cboolean1 IS NULL)
                         AND (cfloat < cint))))
GROUP BY ctimestamp1, cstring1
ORDER BY ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17, c18, c19, c20, c21, c22, c23, c24, c25, c26, c27, c28, c29, c30, c31, c32, c33, c34, c35, c36, c37
LIMIT 50;

SELECT   ctimestamp1,
         cstring1,
         STDDEV_POP(cint) as c1,
         (STDDEV_POP(cint) * 10.175) as c2,
         (-(STDDEV_POP(cint))) as c3,
         AVG(csmallint) as c4,
         (-(STDDEV_POP(cint))) as c5,
         (-26.28 - STDDEV_POP(cint)) as c6,
         COUNT(*) as c7,
         (-(COUNT(*))) as c8,
         ((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) as c9,
         MIN(ctinyint) as c10,
         (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*)))) as c11,
         (-((STDDEV_POP(cint) * 10.175))) as c12,
         VAR_SAMP(csmallint) as c13,
         (VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) as c14,
         (-((-(STDDEV_POP(cint))))) as c15,
         ((-(COUNT(*))) / STDDEV_POP(cint)) as c16,
         VAR_POP(cfloat) as c17,
         (10.175 / AVG(csmallint)) as c18,
         AVG(cint) as c19,
         VAR_SAMP(cfloat) as c20,
         ((VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) - (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) as c21,
         (-((-((STDDEV_POP(cint) * 10.175))))) as c22,
         AVG(cfloat) as c23,
         (((VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) - (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) * 10.175) as c24,
         (10.175 % (10.175 / AVG(csmallint))) as c25,
         (-(MIN(ctinyint))) as c26,
         MIN(cdouble) as c27,
         VAR_POP(csmallint) as c28,
         (-(((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))))) as c29,
         ((-(STDDEV_POP(cint))) % AVG(cfloat)) as c30,
         (-26.28 / (-(MIN(ctinyint)))) as c31,
         STDDEV_POP(ctinyint) as c32,
         SUM(cint) as c33,
         ((VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) / VAR_POP(cfloat)) as c34,
         (-((-(COUNT(*))))) as c35,
         COUNT(*) as c36,
         ((VAR_SAMP(csmallint) + (((-26.28 - STDDEV_POP(cint)) * (-(STDDEV_POP(cint)))) * (-(COUNT(*))))) % -26.28) as c37
FROM     alltypesorc
WHERE    (((ctimestamp1 != 0))
          AND ((((-257 != ctinyint)
                 AND (cboolean2 IS NOT NULL))
                AND ((cstring1 RLIKE '.*ss')
                     AND (-3 < ctimestamp1)))
               OR (ctimestamp2 = -5)
               OR ((ctimestamp1 < 0)
                   AND (cstring2 LIKE '%b%'))
                  OR (cdouble = cint)
                     OR ((cboolean1 IS NULL)
                         AND (cfloat < cint))))
GROUP BY ctimestamp1, cstring1
ORDER BY ctimestamp1, cstring1, c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14, c15, c16, c17, c18, c19, c20, c21, c22, c23, c24, c25, c26, c27, c28, c29, c30, c31, c32, c33, c34, c35, c36, c37
LIMIT 50;

-- TargetTypeClasses: Double, Long, String, Timestamp, Bool
-- Functions: Max, Sum, Var, Avg, Min, VarP, StDev, StDevP
-- ArithmeticOps: Divide, Subtract, Remainder, Add, Multiply
-- FilterOps: GreaterThan, LessThanOrEqual, Equal, LessThan, GreaterThanOrEqual, NotEqual, Like, RLike
-- GroupBy: GroupBy
EXPLAIN SELECT   cboolean1,
         MAX(cfloat),
         (-(MAX(cfloat))),
         (-26.28 / MAX(cfloat)),
         SUM(cbigint),
         (SUM(cbigint) - 10.175),
         VAR_SAMP(cint),
         (VAR_SAMP(cint) % MAX(cfloat)),
         (10.175 + (-(MAX(cfloat)))),
         AVG(cdouble),
         ((SUM(cbigint) - 10.175) + VAR_SAMP(cint)),
         MIN(cbigint),
         VAR_POP(cbigint),
         (-((10.175 + (-(MAX(cfloat)))))),
         (79.553 / VAR_POP(cbigint)),
         (VAR_SAMP(cint) % (79.553 / VAR_POP(cbigint))),
         (-((10.175 + (-(MAX(cfloat)))))),
         SUM(cint),
         STDDEV_SAMP(ctinyint),
         (-1.389 * MIN(cbigint)),
         (SUM(cint) - (-1.389 * MIN(cbigint))),
         STDDEV_POP(csmallint),
         (-((SUM(cint) - (-1.389 * MIN(cbigint))))),
         AVG(cint),
         (-(AVG(cint))),
         (AVG(cint) * SUM(cint))
FROM     alltypesorc
WHERE    (((cboolean1 IS NOT NULL))
          AND (((cdouble < csmallint)
                AND ((cboolean2 = cboolean1)
                     AND (cbigint <= -863.257)))
               OR ((cint >= -257)
                   AND ((cstring1 IS NOT NULL)
                        AND (cboolean1 >= 1)))
               OR (cstring2 RLIKE 'b')
                  OR ((csmallint >= ctinyint)
                      AND (ctimestamp2 IS NULL))))
GROUP BY cboolean1
ORDER BY cboolean1;
SELECT   cboolean1,
         MAX(cfloat),
         (-(MAX(cfloat))),
         (-26.28 / MAX(cfloat)),
         SUM(cbigint),
         (SUM(cbigint) - 10.175),
         VAR_SAMP(cint),
         (VAR_SAMP(cint) % MAX(cfloat)),
         (10.175 + (-(MAX(cfloat)))),
         AVG(cdouble),
         ((SUM(cbigint) - 10.175) + VAR_SAMP(cint)),
         MIN(cbigint),
         VAR_POP(cbigint),
         (-((10.175 + (-(MAX(cfloat)))))),
         (79.553 / VAR_POP(cbigint)),
         (VAR_SAMP(cint) % (79.553 / VAR_POP(cbigint))),
         (-((10.175 + (-(MAX(cfloat)))))),
         SUM(cint),
         STDDEV_SAMP(ctinyint),
         (-1.389 * MIN(cbigint)),
         (SUM(cint) - (-1.389 * MIN(cbigint))),
         STDDEV_POP(csmallint),
         (-((SUM(cint) - (-1.389 * MIN(cbigint))))),
         AVG(cint),
         (-(AVG(cint))),
         (AVG(cint) * SUM(cint))
FROM     alltypesorc
WHERE    (((cboolean1 IS NOT NULL))
          AND (((cdouble < csmallint)
                AND ((cboolean2 = cboolean1)
                     AND (cbigint <= -863.257)))
               OR ((cint >= -257)
                   AND ((cstring1 IS NOT NULL)
                        AND (cboolean1 >= 1)))
               OR (cstring2 RLIKE 'b')
                  OR ((csmallint >= ctinyint)
                      AND (ctimestamp2 IS NULL))))
GROUP BY cboolean1
ORDER BY cboolean1;

-- These tests verify COUNT on empty or null colulmns work correctly.
create table test_count(i int) stored as orc;

explain
select count(*) from test_count;

select count(*) from test_count;

explain
select count(i) from test_count;

select count(i) from test_count;

CREATE TABLE alltypesnull(
    ctinyint TINYINT,
    csmallint SMALLINT,
    cint INT,
    cbigint BIGINT,
    cfloat FLOAT,
    cdouble DOUBLE,
    cstring1 STRING,
    cstring2 STRING,
    ctimestamp1 TIMESTAMP,
    ctimestamp2 TIMESTAMP,
    cboolean1 BOOLEAN,
    cboolean2 BOOLEAN);

insert into table alltypesnull select null, null, null, null, null, null, null, null, null, null, null, null from alltypesorc;

create table alltypesnullorc stored as orc as select * from alltypesnull;

explain
select count(*) from alltypesnullorc;

select count(*) from alltypesnullorc;

explain
select count(ctinyint) from alltypesnullorc;

select count(ctinyint) from alltypesnullorc;

explain
select count(cint) from alltypesnullorc;

select count(cint) from alltypesnullorc;

explain
select count(cfloat) from alltypesnullorc;

select count(cfloat) from alltypesnullorc;

explain
select count(cstring1) from alltypesnullorc;

select count(cstring1) from alltypesnullorc;

explain
select count(cboolean1) from alltypesnullorc;

select count(cboolean1) from alltypesnullorc;
set hive.explain.user=false;
create table vsmb_bucket_1(key int, value string)
  CLUSTERED BY (key)
  SORTED BY (key) INTO 1 BUCKETS
  STORED AS ORC;
create table vsmb_bucket_2(key int, value string)
  CLUSTERED BY (key)
  SORTED BY (key) INTO 1 BUCKETS
  STORED AS ORC;

create table vsmb_bucket_RC(key int, value string)
  CLUSTERED BY (key)
  SORTED BY (key) INTO 1 BUCKETS
  STORED AS RCFILE;

create table vsmb_bucket_TXT(key int, value string)
  CLUSTERED BY (key)
  SORTED BY (key) INTO 1 BUCKETS
  STORED AS TEXTFILE;

insert into table vsmb_bucket_1 select cint, cstring1 from alltypesorc limit 2;
insert into table vsmb_bucket_2 select cint, cstring1 from alltypesorc limit 2;
insert into table vsmb_bucket_RC select cint, cstring1 from alltypesorc limit 2;
insert into table vsmb_bucket_TXT select cint, cstring1 from alltypesorc limit 2;

set hive.vectorized.execution.enabled=true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

explain
select /*+MAPJOIN(a)*/ * from vsmb_bucket_1 a join vsmb_bucket_2 b on a.key = b.key;
select /*+MAPJOIN(a)*/ * from vsmb_bucket_1 a join vsmb_bucket_2 b on a.key = b.key;

explain
select /*+MAPJOIN(b)*/ * from vsmb_bucket_1 a join vsmb_bucket_RC b on a.key = b.key;
select /*+MAPJOIN(b)*/ * from vsmb_bucket_1 a join vsmb_bucket_RC b on a.key = b.key;

-- RC file does not yet provide the vectorized CommonRCFileformat out-of-the-box
-- explain
-- select /*+MAPJOIN(b)*/ * from vsmb_bucket_RC a join vsmb_bucket_2 b on a.key = b.key;
-- select /*+MAPJOIN(b)*/ * from vsmb_bucket_RC a join vsmb_bucket_2 b on a.key = b.key;

explain
select /*+MAPJOIN(b)*/ * from vsmb_bucket_1 a join vsmb_bucket_TXT b on a.key = b.key;
select /*+MAPJOIN(b)*/ * from vsmb_bucket_1 a join vsmb_bucket_TXT b on a.key = b.key;
set hive.explain.user=false;
set hive.fetch.task.conversion=none;
set hive.vectorized.execution.enabled = true
;
explain
select
  csmallint,
  case
    when csmallint = 418 then "a"
    when csmallint = 12205 then "b"
    else "c"
  end,
  case csmallint
    when 418 then "a"
    when 12205 then "b"
    else "c"
  end
from alltypesorc
where csmallint = 418
or csmallint = 12205
or csmallint = 10583
;
select
  csmallint,
  case
    when csmallint = 418 then "a"
    when csmallint = 12205 then "b"
    else "c"
  end,
  case csmallint
    when 418 then "a"
    when 12205 then "b"
    else "c"
  end
from alltypesorc
where csmallint = 418
or csmallint = 12205
or csmallint = 10583
;
explain
select
  csmallint,
  case
    when csmallint = 418 then "a"
    when csmallint = 12205 then "b"
    else null
  end,
  case csmallint
    when 418 then "a"
    when 12205 then null
    else "c"
  end
from alltypesorc
where csmallint = 418
or csmallint = 12205
or csmallint = 10583
;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.fetch.task.conversion=none;
SET hive.vectorized.execution.enabled = true;

-- SORT_QUERY_RESULTS

-- Currently, vectorization is not supported in fetch task (hive.fetch.task.conversion=none)
-- Test type casting in vectorized mode to verify end-to-end functionality.

explain
select
-- to boolean
   cast (ctinyint as boolean)
  ,cast (csmallint as boolean)
  ,cast (cint as boolean)
  ,cast (cbigint as boolean)
  ,cast (cfloat as boolean)
  ,cast (cdouble as boolean)
  ,cast (cboolean1 as boolean)
  ,cast (cbigint * 0 as boolean)
  ,cast (ctimestamp1 as boolean)
  ,cast (cstring1 as boolean)
-- to int family
  ,cast (ctinyint as int)
  ,cast (csmallint as int)
  ,cast (cint as int)
  ,cast (cbigint as int)
  ,cast (cfloat as int)
  ,cast (cdouble as int)
  ,cast (cboolean1 as int)
  ,cast (ctimestamp1 as int)
  ,cast (cstring1 as int)
  ,cast (substr(cstring1, 1, 1) as int)
  ,cast (cfloat as tinyint)
  ,cast (cfloat as smallint)
  ,cast (cfloat as bigint)
-- to float family
  ,cast (ctinyint as double)
  ,cast (csmallint as double)
  ,cast (cint as double)
  ,cast (cbigint as double)
  ,cast (cfloat as double)
  ,cast (cdouble as double)
  ,cast (cboolean1 as double)
  ,cast (ctimestamp1 as double)
  ,cast (cstring1 as double)
  ,cast (substr(cstring1, 1, 1) as double)
  ,cast (cint as float)
  ,cast (cdouble as float)
-- to timestamp
  ,cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
-- to string
  ,cast (ctinyint as string)
  ,cast (csmallint as string)
  ,cast (cint as string)
  ,cast (cbigint as string)
  ,cast (cfloat as string)
  ,cast (cdouble as string)
  ,cast (cboolean1 as string)
  ,cast (cbigint * 0 as string)
  ,cast (ctimestamp1 as string)
  ,cast (cstring1 as string)
  ,cast (cast (cstring1 as char(10)) as string)
  ,cast (cast (cstring1 as varchar(10)) as string)
-- nested and expression arguments
  ,cast (cast (cfloat as int) as float)
  ,cast (cint * 2 as double)
  ,cast (sin(cfloat) as string)
  ,cast (cint as float) + cast(cboolean1 as double)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;


select
-- to boolean
   cast (ctinyint as boolean)
  ,cast (csmallint as boolean)
  ,cast (cint as boolean)
  ,cast (cbigint as boolean)
  ,cast (cfloat as boolean)
  ,cast (cdouble as boolean)
  ,cast (cboolean1 as boolean)
  ,cast (cbigint * 0 as boolean)
  ,cast (ctimestamp1 as boolean)
  ,cast (cstring1 as boolean)
-- to int family
  ,cast (ctinyint as int)
  ,cast (csmallint as int)
  ,cast (cint as int)
  ,cast (cbigint as int)
  ,cast (cfloat as int)
  ,cast (cdouble as int)
  ,cast (cboolean1 as int)
  ,cast (ctimestamp1 as int)
  ,cast (cstring1 as int)
  ,cast (substr(cstring1, 1, 1) as int)
  ,cast (cfloat as tinyint)
  ,cast (cfloat as smallint)
  ,cast (cfloat as bigint)
-- to float family
  ,cast (ctinyint as double)
  ,cast (csmallint as double)
  ,cast (cint as double)
  ,cast (cbigint as double)
  ,cast (cfloat as double)
  ,cast (cdouble as double)
  ,cast (cboolean1 as double)
  ,cast (ctimestamp1 as double)
  ,cast (cstring1 as double)
  ,cast (substr(cstring1, 1, 1) as double)
  ,cast (cint as float)
  ,cast (cdouble as float)
-- to timestamp
  ,cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
-- to string
  ,cast (ctinyint as string)
  ,cast (csmallint as string)
  ,cast (cint as string)
  ,cast (cbigint as string)
  ,cast (cfloat as string)
  ,cast (cdouble as string)
  ,cast (cboolean1 as string)
  ,cast (cbigint * 0 as string)
  ,cast (ctimestamp1 as string)
  ,cast (cstring1 as string)
  ,cast (cast (cstring1 as char(10)) as string)
  ,cast (cast (cstring1 as varchar(10)) as string)
-- nested and expression arguments
  ,cast (cast (cfloat as int) as float)
  ,cast (cint * 2 as double)
  ,cast (sin(cfloat) as string)
  ,cast (cint as float) + cast(cboolean1 as double)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;


set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
create table store(s_store_sk int, s_city string)
stored as orc;
insert overwrite table store
select cint, cstring1
from alltypesorc
where cint not in (
-3728, -563, 762, 6981, 253665376, 528534767, 626923679);
create table store_sales(ss_store_sk int, ss_hdemo_sk int, ss_net_profit double)
stored as orc;
insert overwrite table store_sales
select cint, cint, cdouble
from alltypesorc
where cint not in (
-3728, -563, 762, 6981, 253665376, 528534767, 626923679);
create table household_demographics(hd_demo_sk int)
stored as orc;
insert overwrite table household_demographics
select cint
from alltypesorc
where cint not in (
-3728, -563, 762, 6981, 253665376, 528534767, 626923679);
set hive.auto.convert.join=true;
set hive.vectorized.execution.enabled=true;

set hive.mapjoin.hybridgrace.hashtable=false;

explain
select store.s_city, ss_net_profit
from store_sales
JOIN store ON store_sales.ss_store_sk = store.s_store_sk
JOIN household_demographics ON store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk
limit 100
;

select store.s_city, ss_net_profit
from store_sales
JOIN store ON store_sales.ss_store_sk = store.s_store_sk
JOIN household_demographics ON store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk
limit 100
;

set hive.auto.convert.join=false;
set hive.vectorized.execution.enabled=false;

drop table store;
drop table store_sales;
drop table household_demographics;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled = true;

-- SORT_QUERY_RESULTS

-- Test timestamp functions in vectorized mode to verify they run correctly end-to-end.

CREATE TABLE date_udf_flight (
  origin_city_name STRING,
  dest_city_name STRING,
  fl_date DATE,
  arr_delay FLOAT,
  fl_num INT
);
LOAD DATA LOCAL INPATH '../../data/files/flights_tiny.txt.1' OVERWRITE INTO TABLE date_udf_flight;

CREATE TABLE date_udf_flight_orc (
  fl_date DATE,
  fl_time TIMESTAMP
) STORED AS ORC;

INSERT INTO TABLE date_udf_flight_orc SELECT fl_date, to_utc_timestamp(fl_date, 'America/Los_Angeles') FROM date_udf_flight;

SELECT * FROM date_udf_flight_orc;

EXPLAIN SELECT
  to_unix_timestamp(fl_time),
  year(fl_time),
  month(fl_time),
  day(fl_time),
  dayofmonth(fl_time),
  weekofyear(fl_time),
  date(fl_time),
  to_date(fl_time),
  date_add(fl_time, 2),
  date_sub(fl_time, 2),
  datediff(fl_time, "2000-01-01")
FROM date_udf_flight_orc;

SELECT
  to_unix_timestamp(fl_time),
  year(fl_time),
  month(fl_time),
  day(fl_time),
  dayofmonth(fl_time),
  weekofyear(fl_time),
  date(fl_time),
  to_date(fl_time),
  date_add(fl_time, 2),
  date_sub(fl_time, 2),
  datediff(fl_time, "2000-01-01")
FROM date_udf_flight_orc;

EXPLAIN SELECT
  to_unix_timestamp(fl_date),
  year(fl_date),
  month(fl_date),
  day(fl_date),
  dayofmonth(fl_date),
  weekofyear(fl_date),
  date(fl_date),
  to_date(fl_date),
  date_add(fl_date, 2),
  date_sub(fl_date, 2),
  datediff(fl_date, "2000-01-01")
FROM date_udf_flight_orc;

SELECT
  to_unix_timestamp(fl_date),
  year(fl_date),
  month(fl_date),
  day(fl_date),
  dayofmonth(fl_date),
  weekofyear(fl_date),
  date(fl_date),
  to_date(fl_date),
  date_add(fl_date, 2),
  date_sub(fl_date, 2),
  datediff(fl_date, "2000-01-01")
FROM date_udf_flight_orc;

EXPLAIN SELECT
  year(fl_time) = year(fl_date),
  month(fl_time) = month(fl_date),
  day(fl_time) = day(fl_date),
  dayofmonth(fl_time) = dayofmonth(fl_date),
  weekofyear(fl_time) = weekofyear(fl_date),
  date(fl_time) = date(fl_date),
  to_date(fl_time) = to_date(fl_date),
  date_add(fl_time, 2) = date_add(fl_date, 2),
  date_sub(fl_time, 2) = date_sub(fl_date, 2),
  datediff(fl_time, "2000-01-01") = datediff(fl_date, "2000-01-01")
FROM date_udf_flight_orc;

-- Should all be true or NULL
SELECT
  year(fl_time) = year(fl_date),
  month(fl_time) = month(fl_date),
  day(fl_time) = day(fl_date),
  dayofmonth(fl_time) = dayofmonth(fl_date),
  weekofyear(fl_time) = weekofyear(fl_date),
  date(fl_time) = date(fl_date),
  to_date(fl_time) = to_date(fl_date),
  date_add(fl_time, 2) = date_add(fl_date, 2),
  date_sub(fl_time, 2) = date_sub(fl_date, 2),
  datediff(fl_time, "2000-01-01") = datediff(fl_date, "2000-01-01")
FROM date_udf_flight_orc;

EXPLAIN SELECT
  fl_date,
  to_date(date_add(fl_date, 2)),
  to_date(date_sub(fl_date, 2)),
  datediff(fl_date, date_add(fl_date, 2)),
  datediff(fl_date, date_sub(fl_date, 2)),
  datediff(date_add(fl_date, 2), date_sub(fl_date, 2))
FROM date_udf_flight_orc LIMIT 10;

SELECT
  fl_date,
  to_date(date_add(fl_date, 2)),
  to_date(date_sub(fl_date, 2)),
  datediff(fl_date, date_add(fl_date, 2)),
  datediff(fl_date, date_sub(fl_date, 2)),
  datediff(date_add(fl_date, 2), date_sub(fl_date, 2))
FROM date_udf_flight_orc LIMIT 10;

-- Test extracting the date part of expression that includes time
SELECT to_date('2009-07-30 04:17:52') FROM date_udf_flight_orc LIMIT 1;

EXPLAIN SELECT
  min(fl_date) AS c1,
  max(fl_date),
  count(fl_date),
  count(*)
FROM date_udf_flight_orc
ORDER BY c1;

SELECT
  min(fl_date) AS c1,
  max(fl_date),
  count(fl_date),
  count(*)
FROM date_udf_flight_orc
ORDER BY c1;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

SET hive.map.groupby.sorted=true;

create table dtest(a int, b int) clustered by (a) sorted by (a) into 1 buckets stored as orc;
insert into table dtest select c,b from (select array(300,300,300,300,300) as a, 1 as b from src order by a limit 1) y lateral view  explode(a) t1 as c;

explain select sum(distinct a), count(distinct a) from dtest;
select sum(distinct a), count(distinct a) from dtest;

explain select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc;
select sum(distinct cint), count(distinct cint), avg(distinct cint), std(distinct cint) from alltypesorc;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.optimize.ppd=true;
set hive.ppd.remove.duplicatefilters=true;
set hive.tez.dynamic.partition.pruning=true;
set hive.optimize.metadataonly=false;
set hive.optimize.index.filter=true;
set hive.vectorized.execution.enabled=true;


select distinct ds from srcpart;
select distinct hr from srcpart;

EXPLAIN create table srcpart_date as select ds as ds, ds as `date` from srcpart group by ds;
create table srcpart_date stored as orc as select ds as ds, ds as `date` from srcpart group by ds;
create table srcpart_hour stored as orc as select hr as hr, hr as hour from srcpart group by hr;
create table srcpart_date_hour stored as orc as select ds as ds, ds as `date`, hr as hr, hr as hour from srcpart group by ds, hr;
create table srcpart_double_hour stored as orc as select (hr*2) as hr, hr as hour from srcpart group by hr;

-- single column, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = '2008-04-08';

-- multiple sources, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where hr = 11 and ds = '2008-04-08';

-- multiple columns single source
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = '2008-04-08' and hr = 11;

-- empty set
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where ds = 'I DONT EXIST';

-- expressions
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=false;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (cast(srcpart.hr*2 as string) = cast(srcpart_double_hour.hr as string)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (cast(srcpart.hr*2 as string) = cast(srcpart_double_hour.hr as string)) where srcpart_double_hour.hour = 11;
set hive.tez.dynamic.partition.pruning=true;
select count(*) from srcpart where cast(hr as string) = 11;


-- parent is reduce tasks
EXPLAIN select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- non-equi join
EXPLAIN select count(*) from srcpart, srcpart_date_hour where (srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11) and (srcpart.ds = srcpart_date_hour.ds or srcpart.hr = srcpart_date_hour.hr);
select count(*) from srcpart, srcpart_date_hour where (srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11) and (srcpart.ds = srcpart_date_hour.ds or srcpart.hr = srcpart_date_hour.hr);

-- old style join syntax
EXPLAIN select count(*) from srcpart, srcpart_date_hour where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11 and srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr;
select count(*) from srcpart, srcpart_date_hour where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11 and srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr;

-- left join
EXPLAIN select count(*) from srcpart left join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
EXPLAIN select count(*) from srcpart_date left join srcpart on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- full outer
EXPLAIN select count(*) from srcpart full outer join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- with static pruning
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;

-- union + subquery
EXPLAIN select count(*) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select count(*) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
EXPLAIN select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
EXPLAIN select ds from (select distinct(ds) as ds from srcpart union all select distinct(ds) as ds from srcpart) s where s.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select ds from (select distinct(ds) as ds from srcpart union all select distinct(ds) as ds from srcpart) s where s.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);

set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask = true;
set hive.auto.convert.join.noconditionaltask.size = 10000000;

-- single column, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- multiple sources, single key
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11;
select count(*) from srcpart where hr = 11 and ds = '2008-04-08';

-- multiple columns single source
EXPLAIN select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart join srcpart_date_hour on (srcpart.ds = srcpart_date_hour.ds and srcpart.hr = srcpart_date_hour.hr) where srcpart_date_hour.`date` = '2008-04-08' and srcpart_date_hour.hour = 11;
select count(*) from srcpart where ds = '2008-04-08' and hr = 11;

-- empty set
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';
-- Disabled until TEZ-1486 is fixed
-- select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = 'I DONT EXIST';

-- expressions
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr = cast(srcpart_double_hour.hr/2 as int)) where srcpart_double_hour.hour = 11;
EXPLAIN select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart join srcpart_double_hour on (srcpart.hr*2 = srcpart_double_hour.hr) where srcpart_double_hour.hour = 11;
select count(*) from srcpart where hr = 11;

-- parent is reduce tasks
EXPLAIN select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart join (select ds as ds, ds as `date` from srcpart group by ds) s on (srcpart.ds = s.ds) where s.`date` = '2008-04-08';
select count(*) from srcpart where ds = '2008-04-08';

-- left join
EXPLAIN select count(*) from srcpart left join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';
EXPLAIN select count(*) from srcpart_date left join srcpart on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- full outer
EXPLAIN select count(*) from srcpart full outer join srcpart_date on (srcpart.ds = srcpart_date.ds) where srcpart_date.`date` = '2008-04-08';

-- with static pruning
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart_hour.hour = 11 and srcpart.hr = 11;
EXPLAIN select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;
-- Disabled until TEZ-1486 is fixed
-- select count(*) from srcpart join srcpart_date on (srcpart.ds = srcpart_date.ds) join srcpart_hour on (srcpart.hr = srcpart_hour.hr)
-- where srcpart_date.`date` = '2008-04-08' and srcpart.hr = 13;

-- union + subquery
EXPLAIN select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);
select distinct(ds) from srcpart where srcpart.ds in (select max(srcpart.ds) from srcpart union all select min(srcpart.ds) from srcpart);


-- different file format
create table srcpart_orc (key int, value string) partitioned by (ds string, hr int) stored as orc;


set hive.exec.dynamic.partition.mode=nonstrict;
set hive.vectorized.execution.enabled=false;
set hive.exec.max.dynamic.partitions=1000;

insert into table srcpart_orc partition (ds, hr) select key, value, ds, hr from srcpart;
EXPLAIN select count(*) from srcpart_orc join srcpart_date_hour on (srcpart_orc.ds = srcpart_date_hour.ds and srcpart_orc.hr = srcpart_date_hour.hr) where srcpart_date_hour.hour = 11 and (srcpart_date_hour.`date` = '2008-04-08' or srcpart_date_hour.`date` = '2008-04-09');
select count(*) from srcpart_orc join srcpart_date_hour on (srcpart_orc.ds = srcpart_date_hour.ds and srcpart_orc.hr = srcpart_date_hour.hr) where srcpart_date_hour.hour = 11 and (srcpart_date_hour.`date` = '2008-04-08' or srcpart_date_hour.`date` = '2008-04-09');
select count(*) from srcpart where (ds = '2008-04-08' or ds = '2008-04-09') and hr = 11;

drop table srcpart_orc;
drop table srcpart_date;
drop table srcpart_hour;
drop table srcpart_date_hour;
drop table srcpart_double_hour;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;

-- SORT_QUERY_RESULTS

EXPLAIN SELECT COUNT(t1.cint), MAX(t2.cint), MIN(t1.cint), AVG(t1.cint+t2.cint)
  FROM alltypesorc t1
  JOIN alltypesorc t2 ON t1.cint = t2.cint;

SELECT COUNT(t1.cint), MAX(t2.cint), MIN(t1.cint), AVG(t1.cint+t2.cint)
  FROM alltypesorc t1
  JOIN alltypesorc t2 ON t1.cint = t2.cint;  set hive.explain.user=false;
SET hive.vectorized.execution.enabled = true;

-- Test math functions in vectorized mode to verify they run correctly end-to-end.

explain
select
   cdouble
  ,Round(cdouble, 2)
  ,Floor(cdouble)
  ,Ceil(cdouble)
  ,Rand()
  ,Rand(98007)
  ,Exp(ln(cdouble))
  ,Ln(cdouble)
  ,Ln(cfloat)
  ,Log10(cdouble)
  -- Use log2 as a representative function to test all input types.
  ,Log2(cdouble)
  -- Use 15601.0 to test zero handling, as there are no zeroes in the table
  ,Log2(cdouble - 15601.0)
  ,Log2(cfloat)
  ,Log2(cbigint)
  ,Log2(cint)
  ,Log2(csmallint)
  ,Log2(ctinyint)
  ,Log(2.0, cdouble)
  ,Pow(log2(cdouble), 2.0)
  ,Power(log2(cdouble), 2.0)
  ,Sqrt(cdouble)
  ,Sqrt(cbigint)
  ,Bin(cbigint)
  ,Hex(cdouble)
  ,Conv(cbigint, 10, 16)
  ,Abs(cdouble)
  ,Abs(ctinyint)
  ,Pmod(cint, 3)
  ,Sin(cdouble)
  ,Asin(cdouble)
  ,Cos(cdouble)
  ,ACos(cdouble)
  ,Atan(cdouble)
  ,Degrees(cdouble)
  ,Radians(cdouble)
  ,Positive(cdouble)
  ,Positive(cbigint)
  ,Negative(cdouble)
  ,Sign(cdouble)
  ,Sign(cbigint)
  -- Test nesting
  ,cos(-sin(log(cdouble)) + 3.14159)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 500 = 0
-- test use of a math function in the WHERE clause
and sin(cfloat) >= -1.0;

select
   cdouble
  ,Round(cdouble, 2)
  ,Floor(cdouble)
  ,Ceil(cdouble)
  -- Omit rand() from runtime test because it's nondeterministic.
  -- ,Rand()
  ,Rand(98007)
  ,Exp(ln(cdouble))
  ,Ln(cdouble)
  ,Ln(cfloat)
  ,Log10(cdouble)
  -- Use log2 as a representative function to test all input types.
  ,Log2(cdouble)
  -- Use 15601.0 to test zero handling, as there are no zeroes in the table
  ,Log2(cdouble - 15601.0)
  ,Log2(cfloat)
  ,Log2(cbigint)
  ,Log2(cint)
  ,Log2(csmallint)
  ,Log2(ctinyint)
  ,Log(2.0, cdouble)
  ,Pow(log2(cdouble), 2.0)
  ,Power(log2(cdouble), 2.0)
  ,Sqrt(cdouble)
  ,Sqrt(cbigint)
  ,Bin(cbigint)
  ,Hex(cdouble)
  ,Conv(cbigint, 10, 16)
  ,Abs(cdouble)
  ,Abs(ctinyint)
  ,Pmod(cint, 3)
  ,Sin(cdouble)
  ,Asin(cdouble)
  ,Cos(cdouble)
  ,ACos(cdouble)
  ,Atan(cdouble)
  ,Degrees(cdouble)
  ,Radians(cdouble)
  ,Positive(cdouble)
  ,Positive(cbigint)
  ,Negative(cdouble)
  ,Sign(cdouble)
  ,Sign(cbigint)
  -- Test nesting
  ,cos(-sin(log(cdouble)) + 3.14159)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 500 = 0
-- test use of a math function in the WHERE clause
and sin(cfloat) >= -1.0;

select
   cdouble
  ,Round(cdouble, 2)
  ,Floor(cdouble)
  ,Ceil(cdouble)
  ,Rand(98007) as rnd
  ,Exp(ln(cdouble))
  ,Ln(cdouble)
  ,Ln(cfloat)
  ,Log10(cdouble)
  -- Use log2 as a representative function to test all input types.
  ,Log2(cdouble)
  ,Log2(cfloat)
  ,Log2(cbigint)
  ,Log2(cint)
  ,Log2(csmallint)
  ,Log2(ctinyint)
  ,Log(2.0, cdouble)
  ,Pow(log2(cdouble), 2.0)
  ,Power(log2(cdouble), 2.0)
  ,Sqrt(cdouble)
  ,Sqrt(cbigint)
  ,Bin(cbigint)
  ,Hex(cdouble)
  ,Conv(cbigint, 10, 16)
  ,Abs(cdouble)
  ,Abs(ctinyint)
  ,Pmod(cint, 3)
  ,Sin(cdouble)
  ,Asin(cdouble)
  ,Cos(cdouble)
  ,ACos(cdouble)
  ,Atan(cdouble)
  ,Degrees(cdouble)
  ,Radians(cdouble)
  ,Positive(cdouble)
  ,Positive(cbigint)
  ,Negative(cdouble)
  ,Sign(cdouble)
  ,Sign(cbigint)
from alltypesorc order by rnd limit 400;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;

-- SORT_QUERY_RESULTS

explain select sum(t1.td) from (select  v1.csmallint as tsi, v1.cdouble as td from alltypesorc v1, alltypesorc v2 where v1.ctinyint=v2.ctinyint) t1 join alltypesorc v3 on t1.tsi=v3.csmallint;

select sum(t1.td) from (select  v1.csmallint as tsi, v1.cdouble as td from alltypesorc v1, alltypesorc v2 where v1.ctinyint=v2.ctinyint) t1 join alltypesorc v3 on t1.tsi=v3.csmallint;
set hive.explain.user=false;
set hive.exec.submitviachild=true;
set hive.exec.submit.local.task.via.child=true;

create table if not exists alltypes_parquet (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string) stored as parquet;

insert overwrite table alltypes_parquet
  select cint,
    ctinyint,
    csmallint,
    cfloat,
    cdouble,
    cstring1
  from alltypesorc;

SET hive.vectorized.execution.enabled=true;

explain select *
  from alltypes_parquet
  where cint = 528534767
  limit 10;
select *
  from alltypes_parquet
  where cint = 528534767
  limit 10;

explain select ctinyint,
  max(cint),
  min(csmallint),
  count(cstring1),
  avg(cfloat),
  stddev_pop(cdouble)
  from alltypes_parquet
  group by ctinyint;
select ctinyint,
  max(cint),
  min(csmallint),
  count(cstring1),
  avg(cfloat),
  stddev_pop(cdouble)
  from alltypes_parquet
  group by ctinyint;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;

DROP TABLE parquet_types_staging;
DROP TABLE parquet_types;

-- init
CREATE TABLE parquet_types_staging (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary string,
  m1 map<string, varchar(3)>,
  l1 array<int>,
  st1 struct<c1:int, c2:char(1)>,
  d date,
  cdecimal decimal(4,2)
) ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|'
COLLECTION ITEMS TERMINATED BY ','
MAP KEYS TERMINATED BY ':';

CREATE TABLE parquet_types (
  cint int,
  ctinyint tinyint,
  csmallint smallint,
  cfloat float,
  cdouble double,
  cstring1 string,
  t timestamp,
  cchar char(5),
  cvarchar varchar(10),
  cbinary binary,
  cdecimal decimal(4,2)
) STORED AS PARQUET;

LOAD DATA LOCAL INPATH '../../data/files/parquet_types.txt' OVERWRITE INTO TABLE parquet_types_staging;

INSERT OVERWRITE TABLE parquet_types
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
unhex(cbinary), cdecimal FROM parquet_types_staging;

-- select
explain
SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_types;

SELECT cint, ctinyint, csmallint, cfloat, cdouble, cstring1, t, cchar, cvarchar,
hex(cbinary), cdecimal FROM parquet_types;

explain
SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_types;

SELECT cchar, LENGTH(cchar), cvarchar, LENGTH(cvarchar), cdecimal, SIGN(cdecimal) FROM parquet_types;

explain
SELECT ctinyint,
  MAX(cint),
  MIN(csmallint),
  COUNT(cstring1),
  AVG(cfloat),
  STDDEV_POP(cdouble),
  MAX(cdecimal)
FROM parquet_types
GROUP BY ctinyint
ORDER BY ctinyint;

SELECT ctinyint,
  MAX(cint),
  MIN(csmallint),
  COUNT(cstring1),
  AVG(cfloat),
  STDDEV_POP(cdouble),
  MAX(cdecimal)
FROM parquet_types
GROUP BY ctinyint
ORDER BY ctinyint;SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

DROP TABLE part_staging;
DROP TABLE part_orc;

-- NOTE: This test is a copy of ptf.
-- NOTE: We cannot vectorize "pure" table functions (e.g. NOOP) -- given their blackbox nature. So only queries without table functions and
-- NOTE: with windowing will be vectorized.

-- data setup
CREATE TABLE part_staging(
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
);

LOAD DATA LOCAL INPATH '../../data/files/part_tiny.txt' overwrite into table part_staging;

CREATE TABLE part_orc(
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice DOUBLE,
    p_comment STRING
) STORED AS ORC;

DESCRIBE EXTENDED part_orc;

insert into table part_orc select * from part_staging;

--1. test1

explain extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_orc
  partition by p_mfgr
  order by p_name
  );

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_orc
  partition by p_mfgr
  order by p_name
  );

-- 2. testJoinWithNoop

explain extended
select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part_orc p1 join part_orc p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
;

select p_mfgr, p_name,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop (on (select p1.* from part_orc p1 join part_orc p2 on p1.p_partkey = p2.p_partkey) j
distribute by j.p_mfgr
sort by j.p_name)
;

-- 3. testOnlyPTF

explain extended
select p_mfgr, p_name, p_size
from noop(on part_orc
partition by p_mfgr
order by p_name);

select p_mfgr, p_name, p_size
from noop(on part_orc
partition by p_mfgr
order by p_name);

-- 4. testPTFAlias

explain extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_orc
  partition by p_mfgr
  order by p_name
  ) abc;

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_orc
  partition by p_mfgr
  order by p_name
  ) abc;

-- 5. testPTFAndWhereWithWindowing

explain extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part_orc
          partition by p_mfgr
          order by p_name
          )
;

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part_orc
          partition by p_mfgr
          order by p_name
          )
;

-- 6. testSWQAndPTFAndGBy

explain extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part_orc
          partition by p_mfgr
          order by p_name
          )
group by p_mfgr, p_name, p_size
;

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over (partition by p_mfgr order by p_name) as deltaSz
from noop(on part_orc
          partition by p_mfgr
          order by p_name
          )
group by p_mfgr, p_name, p_size
;

-- 7. testJoin

explain extended
select abc.*
from noop(on part_orc
partition by p_mfgr
order by p_name
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey;

select abc.*
from noop(on part_orc
partition by p_mfgr
order by p_name
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey;

-- 8. testJoinRight

explain extended
select abc.*
from part_orc p1 join noop(on part_orc
partition by p_mfgr
order by p_name
) abc on abc.p_partkey = p1.p_partkey;

select abc.*
from part_orc p1 join noop(on part_orc
partition by p_mfgr
order by p_name
) abc on abc.p_partkey = p1.p_partkey;

-- 9. testNoopWithMap

explain extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part_orc
partition by p_mfgr
order by p_name, p_size desc);

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name, p_size desc) as r
from noopwithmap(on part_orc
partition by p_mfgr
order by p_name, p_size desc);

-- 10. testNoopWithMapWithWindowing

explain extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopwithmap(on part_orc
  partition by p_mfgr
  order by p_name);

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noopwithmap(on part_orc
  partition by p_mfgr
  order by p_name);

-- 11. testHavingWithWindowingPTFNoGBY

explain extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_orc
partition by p_mfgr
order by p_name)
;

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row) as s1
from noop(on part_orc
partition by p_mfgr
order by p_name)
;

-- 12. testFunctionChain

explain extended
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on noopwithmap(on noop(on part_orc
partition by p_mfgr
order by p_mfgr, p_name
)));

select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on noopwithmap(on noop(on part_orc
partition by p_mfgr
order by p_mfgr, p_name
)));

-- 13. testPTFAndWindowingInSubQ

explain extended
select p_mfgr, p_name,
sub1.cd, sub1.s1
from (select p_mfgr, p_name,
count(p_size) over (partition by p_mfgr order by p_name) as cd,
p_retailprice,
sum(p_retailprice) over w1  as s1
from noop(on part_orc
partition by p_mfgr
order by p_name)
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following)
) sub1 ;

select p_mfgr, p_name,
sub1.cd, sub1.s1
from (select p_mfgr, p_name,
count(p_size) over (partition by p_mfgr order by p_name) as cd,
p_retailprice,
sum(p_retailprice) over w1  as s1
from noop(on part_orc
partition by p_mfgr
order by p_name)
window w1 as (partition by p_mfgr order by p_name rows between 2 preceding and 2 following)
) sub1 ;

-- 14. testPTFJoinWithWindowingWithCount

explain extended
select abc.p_mfgr, abc.p_name,
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr,
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noop(on part_orc
partition by p_mfgr
order by p_name
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
;

select abc.p_mfgr, abc.p_name,
rank() over (distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over (distribute by abc.p_mfgr sort by abc.p_name) as dr,
count(abc.p_name) over (distribute by abc.p_mfgr sort by abc.p_name) as cd,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over (distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noop(on part_orc
partition by p_mfgr
order by p_name
) abc join part_orc p1 on abc.p_partkey = p1.p_partkey
;

-- 15. testDistinctInSelectWithPTF

explain extended
select DISTINCT p_mfgr, p_name, p_size
from noop(on part_orc
partition by p_mfgr
order by p_name);

select DISTINCT p_mfgr, p_name, p_size
from noop(on part_orc
partition by p_mfgr
order by p_name);


-- 16. testViewAsTableInputToPTF
create view IF NOT EXISTS mfgr_price_view as
select p_mfgr, p_brand,
sum(p_retailprice) as s
from part_orc
group by p_mfgr, p_brand;

explain extended
select p_mfgr, p_brand, s,
sum(s) over w1  as s1
from noop(on mfgr_price_view
partition by p_mfgr
order by p_mfgr)
window w1 as ( partition by p_mfgr order by p_brand rows between 2 preceding and current row);

select p_mfgr, p_brand, s,
sum(s) over w1  as s1
from noop(on mfgr_price_view
partition by p_mfgr
order by p_mfgr)
window w1 as ( partition by p_mfgr order by p_brand rows between 2 preceding and current row);

-- 17. testMultipleInserts2SWQsWithPTF
CREATE TABLE part_4(
p_mfgr STRING,
p_name STRING,
p_size INT,
r INT,
dr INT,
s DOUBLE);

CREATE TABLE part_5(
p_mfgr STRING,
p_name STRING,
p_size INT,
s2 INT,
r INT,
dr INT,
cud DOUBLE,
fv1 INT);

explain extended
from noop(on part_orc
partition by p_mfgr
order by p_name)
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size,
rank() over (distribute by p_mfgr sort by p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr,
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud,
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

from noop(on part_orc
partition by p_mfgr
order by p_name)
INSERT OVERWRITE TABLE part_4 select p_mfgr, p_name, p_size,
rank() over (distribute by p_mfgr sort by p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s
INSERT OVERWRITE TABLE part_5 select  p_mfgr,p_name, p_size,
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as r,
dense_rank() over (distribute by p_mfgr sort by p_mfgr, p_name) as dr,
cume_dist() over (distribute by p_mfgr sort by p_mfgr, p_name) as cud,
first_value(p_size, true) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

select * from part_4;

select * from part_5;

-- 18. testMulti2OperatorsFunctionChainWithMap

explain extended
select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noopwithmap(on
          noop(on
              noop(on part_orc
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr,p_name
        order by p_mfgr,p_name) ;

select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noopwithmap(on
          noop(on
              noop(on part_orc
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr,p_name
        order by p_mfgr,p_name) ;

-- 19. testMulti3OperatorsFunctionChain

explain extended
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noop(on
          noop(on
              noop(on part_orc
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr
        order by p_mfgr ) ;

select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name rows between unbounded preceding and current row)  as s1
from noop(on
        noop(on
          noop(on
              noop(on part_orc
              partition by p_mfgr
              order by p_mfgr)
            )
          partition by p_mfgr,p_name
          order by p_mfgr,p_name)
        partition by p_mfgr
        order by p_mfgr ) ;

-- 20. testMultiOperatorChainWithNoWindowing

explain extended
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1
from noop(on
        noop(on
          noop(on
              noop(on part_orc
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr));

select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr order by p_name) as s1
from noop(on
        noop(on
          noop(on
              noop(on part_orc
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr));


-- 21. testMultiOperatorChainEndsWithNoopMap

explain extended
select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noopwithmap(on
        noop(on
          noop(on
              noop(on part_orc
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr)
          partition by p_mfgr,p_name
          order by p_mfgr,p_name);

select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name) as dr,
p_size, sum(p_size) over (partition by p_mfgr,p_name rows between unbounded preceding and current row)  as s1
from noopwithmap(on
        noop(on
          noop(on
              noop(on part_orc
              partition by p_mfgr,p_name
              order by p_mfgr,p_name)
            )
          partition by p_mfgr
          order by p_mfgr)
          partition by p_mfgr,p_name
          order by p_mfgr,p_name);

-- 22. testMultiOperatorChainWithDiffPartitionForWindow1

explain extended
select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row)  as s2
from noop(on
        noopwithmap(on
              noop(on part_orc
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          partition by p_mfgr
          order by p_mfgr
          ));

select p_mfgr, p_name,
rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as r,
dense_rank() over (partition by p_mfgr,p_name order by p_mfgr,p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr,p_name order by p_mfgr,p_name rows between unbounded preceding and current row)  as s2
from noop(on
        noopwithmap(on
              noop(on part_orc
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          partition by p_mfgr
          order by p_mfgr
          ));

-- 23. testMultiOperatorChainWithDiffPartitionForWindow2

explain extended
select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmap(on
        noop(on
              noop(on part_orc
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          ));

select p_mfgr, p_name,
rank() over (partition by p_mfgr order by p_name) as r,
dense_rank() over (partition by p_mfgr order by p_name) as dr,
p_size,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row) as s1,
sum(p_size) over (partition by p_mfgr order by p_name range between unbounded preceding and current row)  as s2
from noopwithmap(on
        noop(on
              noop(on part_orc
              partition by p_mfgr, p_name
              order by p_mfgr, p_name)
          ));

set hive.mapred.mode=nonstrict;
--This query must pass even when vectorized reader is not available for
--RC files. The query must fall back to the non-vector mode and run successfully.

CREATE table columnTable (key STRING, value STRING)
ROW FORMAT SERDE
  'org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'
STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.RCFileOutputFormat';

FROM src
INSERT OVERWRITE TABLE columnTable SELECT src.key, src.value ORDER BY src.key, src.value LIMIT 10;
describe columnTable;

SET hive.vectorized.execution.enabled=true;

SELECT key, value FROM columnTable ORDER BY key;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=false;

-- SORT_QUERY_RESULTS

EXPLAIN SELECT COUNT(t1.cint) AS CNT, MAX(t2.cint) , MIN(t1.cint), AVG(t1.cint+t2.cint)
  FROM alltypesorc t1
  JOIN alltypesorc t2 ON t1.cint = t2.cint order by CNT;

SELECT COUNT(t1.cint), MAX(t2.cint) AS CNT, MIN(t1.cint), AVG(t1.cint+t2.cint)
  FROM alltypesorc t1
  JOIN alltypesorc t2 ON t1.cint = t2.cint order by CNT;set hive.explain.user=false;
SET hive.vectorized.execution.enabled = true;

-- Test string functions in vectorized mode to verify end-to-end functionality.

explain
select
   substr(cstring1, 1, 2)
  ,substr(cstring1, 2)
  ,lower(cstring1)
  ,upper(cstring1)
  ,ucase(cstring1)
  ,length(cstring1)
  ,trim(cstring1)
  ,ltrim(cstring1)
  ,rtrim(cstring1)
  ,concat(cstring1, cstring2)
  ,concat('>', cstring1)
  ,concat(cstring1, '<')
  ,concat(substr(cstring1, 1, 2), substr(cstring2, 1, 2))
from alltypesorc
-- Limit the number of rows of output to a reasonable amount.
where cbigint % 237 = 0
-- Test function use in the WHERE clause.
and length(substr(cstring1, 1, 2)) <= 2
and cstring1 like '%';

select
   substr(cstring1, 1, 2)
  ,substr(cstring1, 2)
  ,lower(cstring1)
  ,upper(cstring1)
  ,ucase(cstring1)
  ,length(cstring1)
  ,trim(cstring1)
  ,ltrim(cstring1)
  ,rtrim(cstring1)
  ,concat(cstring1, cstring2)
  ,concat('>', cstring1)
  ,concat(cstring1, '<')
  ,concat(substr(cstring1, 1, 2), substr(cstring2, 1, 2))
from alltypesorc
-- Limit the number of rows of output to a reasonable amount.
where cbigint % 237 = 0
-- Test function use in the WHERE clause.
and length(substr(cstring1, 1, 2)) <= 2
and cstring1 like '%';
set hive.fetch.task.conversion=none;

DROP TABLE IF EXISTS test;
CREATE TABLE test(ts TIMESTAMP) STORED AS ORC;
INSERT INTO TABLE test VALUES ('0001-01-01 00:00:00.000000000'), ('9999-12-31 23:59:59.999999999');

SET hive.vectorized.execution.enabled = false;
EXPLAIN
SELECT ts FROM test;

SELECT ts FROM test;

EXPLAIN
SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test;

SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test;

SET hive.vectorized.execution.enabled = true;
EXPLAIN
SELECT ts FROM test;

SELECT ts FROM test;

EXPLAIN
SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test;

SELECT MIN(ts), MAX(ts), MAX(ts) - MIN(ts) FROM test;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
-- Test timestamp functions in vectorized mode to verify they run correctly end-to-end.
-- Turning on vectorization has been temporarily moved after filling the test table
-- due to bug HIVE-8197.


CREATE TABLE alltypesorc_string(ctimestamp1 timestamp, stimestamp1 string) STORED AS ORC;

INSERT OVERWRITE TABLE alltypesorc_string
SELECT
  to_utc_timestamp(ctimestamp1, 'America/Los_Angeles') AS toutc,
  CAST(to_utc_timestamp(ctimestamp1, 'America/Los_Angeles') AS STRING) as cst
FROM alltypesorc
ORDER BY toutc, cst
LIMIT 40;

SET hive.vectorized.execution.enabled = true;

CREATE TABLE alltypesorc_wrong(stimestamp1 string) STORED AS ORC;

INSERT INTO TABLE alltypesorc_wrong SELECT 'abcd' FROM alltypesorc LIMIT 1;
INSERT INTO TABLE alltypesorc_wrong SELECT '2000:01:01 00-00-00' FROM alltypesorc LIMIT 1;
INSERT INTO TABLE alltypesorc_wrong SELECT '0000-00-00 99:99:99' FROM alltypesorc LIMIT 1;

EXPLAIN SELECT
  to_unix_timestamp(ctimestamp1) AS c1,
  year(ctimestamp1),
  month(ctimestamp1),
  day(ctimestamp1),
  dayofmonth(ctimestamp1),
  weekofyear(ctimestamp1),
  hour(ctimestamp1),
  minute(ctimestamp1),
  second(ctimestamp1)
FROM alltypesorc_string
ORDER BY c1;

SELECT
  to_unix_timestamp(ctimestamp1) AS c1,
  year(ctimestamp1),
  month(ctimestamp1),
  day(ctimestamp1),
  dayofmonth(ctimestamp1),
  weekofyear(ctimestamp1),
  hour(ctimestamp1),
  minute(ctimestamp1),
  second(ctimestamp1)
FROM alltypesorc_string
ORDER BY c1;

EXPLAIN SELECT
  to_unix_timestamp(stimestamp1) AS c1,
  year(stimestamp1),
  month(stimestamp1),
  day(stimestamp1),
  dayofmonth(stimestamp1),
  weekofyear(stimestamp1),
  hour(stimestamp1),
  minute(stimestamp1),
  second(stimestamp1)
FROM alltypesorc_string
ORDER BY c1;

SELECT
  to_unix_timestamp(stimestamp1) AS c1,
  year(stimestamp1),
  month(stimestamp1),
  day(stimestamp1),
  dayofmonth(stimestamp1),
  weekofyear(stimestamp1),
  hour(stimestamp1),
  minute(stimestamp1),
  second(stimestamp1)
FROM alltypesorc_string
ORDER BY c1;

EXPLAIN SELECT
  to_unix_timestamp(ctimestamp1) = to_unix_timestamp(stimestamp1) AS c1,
  year(ctimestamp1) = year(stimestamp1),
  month(ctimestamp1) = month(stimestamp1),
  day(ctimestamp1) = day(stimestamp1),
  dayofmonth(ctimestamp1) = dayofmonth(stimestamp1),
  weekofyear(ctimestamp1) = weekofyear(stimestamp1),
  hour(ctimestamp1) = hour(stimestamp1),
  minute(ctimestamp1) = minute(stimestamp1),
  second(ctimestamp1) = second(stimestamp1)
FROM alltypesorc_string
ORDER BY c1;

-- Should all be true or NULL
SELECT
  to_unix_timestamp(ctimestamp1) = to_unix_timestamp(stimestamp1) AS c1,
  year(ctimestamp1) = year(stimestamp1),
  month(ctimestamp1) = month(stimestamp1),
  day(ctimestamp1) = day(stimestamp1),
  dayofmonth(ctimestamp1) = dayofmonth(stimestamp1),
  weekofyear(ctimestamp1) = weekofyear(stimestamp1),
  hour(ctimestamp1) = hour(stimestamp1),
  minute(ctimestamp1) = minute(stimestamp1),
  second(ctimestamp1) = second(stimestamp1)
FROM alltypesorc_string
ORDER BY c1;

-- Wrong format. Should all be NULL.
EXPLAIN SELECT
  to_unix_timestamp(stimestamp1) AS c1,
  year(stimestamp1),
  month(stimestamp1),
  day(stimestamp1),
  dayofmonth(stimestamp1),
  weekofyear(stimestamp1),
  hour(stimestamp1),
  minute(stimestamp1),
  second(stimestamp1)
FROM alltypesorc_wrong
ORDER BY c1;

SELECT
  to_unix_timestamp(stimestamp1) AS c1,
  year(stimestamp1),
  month(stimestamp1),
  day(stimestamp1),
  dayofmonth(stimestamp1),
  weekofyear(stimestamp1),
  hour(stimestamp1),
  minute(stimestamp1),
  second(stimestamp1)
FROM alltypesorc_wrong
ORDER BY c1;

EXPLAIN SELECT
  min(ctimestamp1),
  max(ctimestamp1),
  count(ctimestamp1),
  count(*)
FROM alltypesorc_string;

SELECT
  min(ctimestamp1),
  max(ctimestamp1),
  count(ctimestamp1),
  count(*)
FROM alltypesorc_string;

-- SUM of timestamps are not vectorized reduce-side because they produce a double instead of a long (HIVE-8211)...
EXPLAIN SELECT
  round(sum(ctimestamp1), 3)
FROM alltypesorc_string;

SELECT
 round(sum(ctimestamp1), 3)
FROM alltypesorc_string;

EXPLAIN SELECT
  round(avg(ctimestamp1), 0),
  variance(ctimestamp1) between 8.97077295279421E19 and 8.97077295279422E19,
  var_pop(ctimestamp1) between 8.97077295279421E19 and 8.97077295279422E19,
  var_samp(ctimestamp1) between 9.20684592523616E19 and 9.20684592523617E19,
  round(std(ctimestamp1), 3),
  round(stddev(ctimestamp1), 3),
  round(stddev_pop(ctimestamp1), 3),
  round(stddev_samp(ctimestamp1), 3)
FROM alltypesorc_string;

SELECT
  round(avg(ctimestamp1), 0),
  variance(ctimestamp1) between 8.97077295279421E19 and 8.97077295279422E19,
  var_pop(ctimestamp1) between 8.97077295279421E19 and 8.97077295279422E19,
  var_samp(ctimestamp1) between 9.20684592523616E19 and 9.20684592523617E19,
  round(std(ctimestamp1), 3),
  round(stddev(ctimestamp1), 3),
  round(stddev_pop(ctimestamp1), 3),
  round(stddev_samp(ctimestamp1), 3)
FROM alltypesorc_string;set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled = true;
SET hive.int.timestamp.conversion.in.seconds=false;

explain
select
-- to timestamp
  cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;


select
-- to timestamp
  cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;

SET hive.int.timestamp.conversion.in.seconds=true;

explain
select
-- to timestamp
  cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;


select
-- to timestamp
  cast (ctinyint as timestamp)
  ,cast (csmallint as timestamp)
  ,cast (cint as timestamp)
  ,cast (cbigint as timestamp)
  ,cast (cfloat as timestamp)
  ,cast (cdouble as timestamp)
  ,cast (cboolean1 as timestamp)
  ,cast (cbigint * 0 as timestamp)
  ,cast (ctimestamp1 as timestamp)
  ,cast (cstring1 as timestamp)
  ,cast (substr(cstring1, 1, 1) as timestamp)
from alltypesorc
-- limit output to a reasonably small number of rows
where cbigint % 250 = 0;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.exec.dynamic.partition=true;
set hive.vectorized.execution.enabled=true;

drop table if exists testacid1;

create table testacid1(id int) clustered by (id) into 2 buckets stored as orc tblproperties("transactional"="true");

insert into table testacid1 values (1),(2),(3),(4);

set hive.compute.query.using.stats=false;

set hive.vectorized.execution.enabled;

select count(1) from testacid1;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

explain
select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc;

-- SORT_QUERY_RESULTS

select min(dc), max(dc), sum(dc), avg(dc) from vectortab2korc;set hive.mapred.mode=nonstrict;
create table testvec(id int, dt int, greg_dt string) stored as orc;
insert into table testvec
values
(1,20150330, '2015-03-30'),
(2,20150301, '2015-03-01'),
(3,20150502, '2015-05-02'),
(4,20150401, '2015-04-01'),
(5,20150313, '2015-03-13'),
(6,20150314, '2015-03-14'),
(7,20150404, '2015-04-04');
set hive.vectorized.execution.enabled=true;
set hive.map.aggr=true;
explain select max(dt), max(greg_dt) from testvec where id=5;
select max(dt), max(greg_dt) from testvec where id=5;set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
;

set hive.exec.reducers.max = 1;

-- SORT_QUERY_RESULTS

CREATE TABLE tbl1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS ORC;
CREATE TABLE tbl2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS ORC;

insert overwrite table tbl1
select * from src where key < 10;

insert overwrite table tbl2
select * from src where key < 10;

set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;

set hive.auto.convert.sortmerge.join=true;

-- The join is being performed as part of sub-query. It should be converted to a sort-merge join
explain
select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

select count(*) from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1;

-- The join is being performed as part of more than one sub-query. It should be converted to a sort-merge join
explain
select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

select count(*) from
(
  select key, count(*) from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1
  group by key
) subq2;

-- A join is being performed across different sub-queries, where a join is being performed in each of them.
-- Each sub-query should be converted to a sort-merge join.
explain
select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

select src1.key, src1.cnt1, src2.cnt1 from
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq1 group by key
) src1
join
(
  select key, count(*) as cnt1 from
  (
    select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
  ) subq2 group by key
) src2
on src1.key = src2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters, it should
-- be converted to a sort-merge join, although there is more than one level of sub-query
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join tbl2 b
  on subq2.key = b.key;

-- Both the tables are nested sub-queries i.e more then 1 level of sub-query.
-- The join should be converted to a sort-merge join
explain
select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

select count(*) from
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
  where key < 6
  ) subq2
  join
  (
  select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq3
  where key < 6
  ) subq4
  on subq2.key = subq4.key;

-- The subquery itself is being joined. Since the sub-query only contains selects and filters and the join key
-- is not getting modified, it should be converted to a sort-merge join. Note that the sub-query modifies one
-- item, but that is not part of the join key.
explain
select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key as key, concat(a.value, a.value) as value from tbl1 a where key < 8) subq1
    join
  (select a.key as key, concat(a.value, a.value) as value from tbl2 a where key < 8) subq2
  on subq1.key = subq2.key;

-- Since the join key is modified by the sub-query, neither sort-merge join not bucketized map-side
-- join should be performed
explain
select count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

select count(*) from
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl1 a) subq1
    join
  (select a.key +1 as key, concat(a.value, a.value) as value from tbl2 a) subq2
  on subq1.key = subq2.key;

-- One of the tables is a sub-query and the other is not.
-- It should be converted to a sort-merge join.
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join tbl2 a on subq1.key = a.key;

-- There are more than 2 inputs to the join, all of them being sub-queries.
-- It should be converted to to a sort-merge join
explain
select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on (subq1.key = subq2.key)
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

select count(*) from
  (select a.key as key, a.value as value from tbl1 a where key < 6) subq1
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq2
  on subq1.key = subq2.key
    join
  (select a.key as key, a.value as value from tbl2 a where key < 6) subq3
  on (subq1.key = subq3.key);

-- The join is being performed on a nested sub-query, and an aggregation is performed after that.
-- The join should be converted to a sort-merge join
explain
select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

select count(*) from (
  select subq2.key as key, subq2.value as value1, b.value as value2 from
  (
    select * from
    (
      select a.key as key, a.value as value from tbl1 a where key < 8
    ) subq1
    where key < 6
  ) subq2
join tbl2 b
on subq2.key = b.key) a;

CREATE TABLE dest1(key int, value string);
CREATE TABLE dest2(key int, val1 string, val2 string);

-- The join is followed by a multi-table insert. It should be converted to
-- a sort-merge join
explain
from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
insert overwrite table dest1 select key, val1
insert overwrite table dest2 select key, val1, val2;

from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
insert overwrite table dest1 select key, val1
insert overwrite table dest2 select key, val1, val2;

select * from dest1;
select * from dest2;

DROP TABLE dest2;
CREATE TABLE dest2(key int, cnt int);

-- The join is followed by a multi-table insert, and one of the inserts involves a reducer.
-- It should be converted to a sort-merge join
explain
from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
insert overwrite table dest1 select key, val1
insert overwrite table dest2 select key, count(*) group by key;

from (
  select a.key as key, a.value as val1, b.value as val2 from tbl1 a join tbl2 b on a.key = b.key
) subq1
insert overwrite table dest1 select key, val1
insert overwrite table dest2 select key, count(*) group by key;

select * from dest1;
select * from dest2;
set hive.cli.print.header=true;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
set hive.fetch.task.conversion=none;
set hive.mapred.mode=nonstrict;

-- SORT_QUERY_RESULTS

create table if not exists TSINT_txt ( RNUM int , CSINT smallint )
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';

create table if not exists TINT_txt ( RNUM int , CINT int )
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';

load data local inpath '../../data/files/TSINT' into table TSINT_txt;

load data local inpath '../../data/files/TINT' into table TINT_txt;

create table TSINT stored as orc AS SELECT * FROM TSINT_txt;

create table TINT stored as orc AS SELECT * FROM TINT_txt;

-- We DO NOT expect the following to vectorized because the BETWEEN range expressions
-- are not constants.  We currently do not support the range expressions being columns.
explain
select tint.rnum, tsint.rnum from tint , tsint where tint.cint between tsint.csint and tsint.csint;

select tint.rnum, tsint.rnum from tint , tsint where tint.cint between tsint.csint and tsint.csint;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

CREATE TABLE decimal_date_test STORED AS ORC AS SELECT cdouble, CAST (((cdouble*22.1)/37) AS DECIMAL(20,10)) AS cdecimal1, CAST (((cdouble*9.3)/13) AS DECIMAL(23,14)) AS cdecimal2, CAST(CAST((CAST(cint AS BIGINT) *ctinyint) AS TIMESTAMP) AS DATE) AS cdate FROM alltypesorc ORDER BY cdate;

EXPLAIN SELECT cdate FROM decimal_date_test WHERE cdate IN (CAST("1969-10-26" AS DATE), CAST("1969-07-14" AS DATE)) ORDER BY cdate;

EXPLAIN SELECT COUNT(*) FROM decimal_date_test WHERE cdate NOT IN (CAST("1969-10-26" AS DATE), CAST("1969-07-14" AS DATE), CAST("1970-01-21" AS DATE));

EXPLAIN SELECT cdecimal1 FROM decimal_date_test WHERE cdecimal1 IN (2365.8945945946, 881.0135135135, -3367.6517567568) ORDER BY cdecimal1;

EXPLAIN SELECT COUNT(*) FROM decimal_date_test WHERE cdecimal1 NOT IN (2365.8945945946, 881.0135135135, -3367.6517567568);

EXPLAIN SELECT cdate FROM decimal_date_test WHERE cdate BETWEEN CAST("1969-12-30" AS DATE) AND CAST("1970-01-02" AS DATE) ORDER BY cdate;

EXPLAIN SELECT cdate FROM decimal_date_test WHERE cdate NOT BETWEEN CAST("1968-05-01" AS DATE) AND CAST("1971-09-01" AS DATE) ORDER BY cdate;

EXPLAIN SELECT cdecimal1 FROM decimal_date_test WHERE cdecimal1 BETWEEN -20 AND 45.9918918919 ORDER BY cdecimal1;

EXPLAIN SELECT COUNT(*) FROM decimal_date_test WHERE cdecimal1 NOT BETWEEN -2000 AND 4390.1351351351;

SELECT cdate FROM decimal_date_test WHERE cdate IN (CAST("1969-10-26" AS DATE), CAST("1969-07-14" AS DATE)) ORDER BY cdate;

SELECT COUNT(*) FROM decimal_date_test WHERE cdate NOT IN (CAST("1969-10-26" AS DATE), CAST("1969-07-14" AS DATE), CAST("1970-01-21" AS DATE));

SELECT cdecimal1 FROM decimal_date_test WHERE cdecimal1 IN (2365.8945945946, 881.0135135135, -3367.6517567568) ORDER BY cdecimal1;

SELECT COUNT(*) FROM decimal_date_test WHERE cdecimal1 NOT IN (2365.8945945946, 881.0135135135, -3367.6517567568);

SELECT cdate FROM decimal_date_test WHERE cdate BETWEEN CAST("1969-12-30" AS DATE) AND CAST("1970-01-02" AS DATE) ORDER BY cdate;

SELECT cdate FROM decimal_date_test WHERE cdate NOT BETWEEN CAST("1968-05-01" AS DATE) AND CAST("1971-09-01" AS DATE) ORDER BY cdate;

SELECT cdecimal1 FROM decimal_date_test WHERE cdecimal1 BETWEEN -20 AND 45.9918918919 ORDER BY cdecimal1;

SELECT COUNT(*) FROM decimal_date_test WHERE cdecimal1 NOT BETWEEN -2000 AND 4390.1351351351;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;
SET hive.vectorized.execution.enabled=true;

DROP TABLE over1k;
DROP TABLE hundredorc;

-- data setup
CREATE TABLE over1k(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k;

CREATE TABLE hundredorc(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS ORC;

INSERT INTO TABLE hundredorc SELECT * FROM over1k LIMIT 100;

EXPLAIN
SELECT sum(hash(*))
FROM hundredorc t1 JOIN hundredorc t2 ON t1.bin = t2.bin;

SELECT sum(hash(*))
FROM hundredorc t1 JOIN hundredorc t2 ON t2.bin = t2.bin;

EXPLAIN
SELECT count(*), bin
FROM hundredorc
GROUP BY bin;

SELECT count(*), bin
FROM hundredorc
GROUP BY bin;
set hive.mapred.mode=nonstrict;
create table test_vector_bround(v0 double, v1 double) stored as orc;
insert into table test_vector_bround
values
(2.5, 1.25),
(3.5, 1.35),
(-2.5, -1.25),
(-3.5, -1.35),
(2.49, 1.249),
(3.49, 1.349),
(2.51, 1.251),
(3.51, 1.351);
set hive.vectorized.execution.enabled=true;
explain select bround(v0), bround(v1, 1) from test_vector_bround;
select bround(v0), bround(v1, 1) from test_vector_bround;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.support.concurrency=true;


CREATE TABLE non_orc_table(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS sequencefile;


explain
insert into table non_orc_table values(1, 'one'),(1, 'one'), (2, 'two'),(3, 'three'); select a, b from non_orc_table order by a;

insert into table non_orc_table values(1, 'one'),(1, 'one'), (2, 'two'),(3, 'three'); select a, b from non_orc_table order by a;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- JAVA_VERSION_SPECIFIC_OUTPUT

DROP TABLE over1k;
DROP TABLE over1korc;

-- data setup
CREATE TABLE over1k(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k;

CREATE TABLE over1korc(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS ORC;

INSERT INTO TABLE over1korc SELECT * FROM over1k;

EXPLAIN SELECT
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc GROUP BY i ORDER BY i LIMIT 10;

SELECT
  i,
  AVG(CAST(50 AS INT)) AS `avg_int_ok`,
  AVG(CAST(50 AS DOUBLE)) AS `avg_double_ok`,
  AVG(CAST(50 AS DECIMAL)) AS `avg_decimal_ok`
  FROM over1korc GROUP BY i ORDER BY i LIMIT 10;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
drop table char_2;

create table char_2 (
  key char(10),
  value char(20)
) stored as orc;

insert overwrite table char_2 select * from src;

select value, sum(cast(key as int)), count(*) numrows
from src
group by value
order by value asc
limit 5;

explain select value, sum(cast(key as int)), count(*) numrows
from char_2
group by value
order by value asc
limit 5;

-- should match the query from src
select value, sum(cast(key as int)), count(*) numrows
from char_2
group by value
order by value asc
limit 5;

select value, sum(cast(key as int)), count(*) numrows
from src
group by value
order by value desc
limit 5;

explain select value, sum(cast(key as int)), count(*) numrows
from char_2
group by value
order by value desc
limit 5;

-- should match the query from src
select value, sum(cast(key as int)), count(*) numrows
from char_2
group by value
order by value desc
limit 5;

drop table char_2;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

drop table if exists vectortab2k;
drop table if exists vectortab2korc;

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

drop table if exists char_lazy_binary_columnar;
create table char_lazy_binary_columnar(ct char(10), csi char(10), ci char(20), cb char(30), cf char(20), cd char(20), cs char(50)) row format serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe' stored as rcfile;

explain
insert overwrite table char_lazy_binary_columnar select t, si, i, b, f, d, s from vectortab2korc;

-- insert overwrite table char_lazy_binary_columnar select t, si, i, b, f, d, s from vectortab2korc;

-- select count(*) as cnt from char_lazy_binary_columnar group by cs order by cnt asc;create table s1(id smallint) stored as orc;

insert into table s1 values (1000),(1001),(1002),(1003),(1000);

set hive.vectorized.execution.enabled=true;
select count(1) from s1 where cast(id as char(4))='1000';

set hive.vectorized.execution.enabled=false;
select count(1) from s1 where cast(id as char(4))='1000';set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;

drop table if exists char_join1_vc1;
drop table if exists char_join1_vc2;
drop table if exists char_join1_str;
drop table if exists char_join1_vc1_orc;
drop table if exists char_join1_vc2_orc;
drop table if exists char_join1_str_orc;

create table  char_join1_vc1 (
  c1 int,
  c2 char(10)
);

create table  char_join1_vc2 (
  c1 int,
  c2 char(20)
);

create table  char_join1_str (
  c1 int,
  c2 string
);

load data local inpath '../../data/files/vc1.txt' into table char_join1_vc1;
load data local inpath '../../data/files/vc1.txt' into table char_join1_vc2;
load data local inpath '../../data/files/vc1.txt' into table char_join1_str;

create table char_join1_vc1_orc stored as orc as select * from char_join1_vc1;
create table char_join1_vc2_orc stored as orc as select * from char_join1_vc2;
create table char_join1_str_orc stored as orc as select * from char_join1_str;

-- Join char with same length char
explain select * from char_join1_vc1_orc a join char_join1_vc1_orc b on (a.c2 = b.c2) order by a.c1;

-- SORT_QUERY_RESULTS

select * from char_join1_vc1_orc a join char_join1_vc1_orc b on (a.c2 = b.c2) order by a.c1;

-- Join char with different length char
explain select * from char_join1_vc1_orc a join char_join1_vc2_orc b on (a.c2 = b.c2) order by a.c1;

-- SORT_QUERY_RESULTS

select * from char_join1_vc1_orc a join char_join1_vc2_orc b on (a.c2 = b.c2) order by a.c1;

-- Join char with string
explain select * from char_join1_vc1_orc a join char_join1_str_orc b on (a.c2 = b.c2) order by a.c1;

-- SORT_QUERY_RESULTS

select * from char_join1_vc1_orc a join char_join1_str_orc b on (a.c2 = b.c2) order by a.c1;

drop table char_join1_vc1;
drop table char_join1_vc2;
drop table char_join1_str;
drop table char_join1_vc1_orc;
drop table char_join1_vc2_orc;
drop table char_join1_str_orc;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
drop table char_2;

create table char_2 (
  key char(10),
  value char(20)
) stored as orc;

insert overwrite table char_2 select * from src;

select key, value
from src
order by key asc
limit 5;

explain select key, value
from char_2
order by key asc
limit 5;

-- should match the query from src
select key, value
from char_2
order by key asc
limit 5;

select key, value
from src
order by key desc
limit 5;

explain select key, value
from char_2
order by key desc
limit 5;

-- should match the query from src
select key, value
from char_2
order by key desc
limit 5;

drop table char_2;


-- Implicit conversion.  Occurs in reduce-side under Tez.
create table char_3 (
  field char(12)
) stored as orc;

explain
insert into table char_3 select cint from alltypesorc limit 10;

insert into table char_3 select cint from alltypesorc limit 10;

drop table char_3;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

EXPLAIN SELECT cdouble, cstring1, cint, cfloat, csmallint, coalesce(cdouble, cstring1, cint, cfloat, csmallint) as c
FROM alltypesorc
WHERE (cdouble IS NULL)
ORDER BY cdouble, cstring1, cint, cfloat, csmallint, c
LIMIT 10;

SELECT cdouble, cstring1, cint, cfloat, csmallint, coalesce(cdouble, cstring1, cint, cfloat, csmallint) as c
FROM alltypesorc
WHERE (cdouble IS NULL)
ORDER BY cdouble, cstring1, cint, cfloat, csmallint, c
LIMIT 10;

EXPLAIN SELECT ctinyint, cdouble, cint, coalesce(ctinyint+10, (cdouble+log2(cint)), 0) as c
FROM alltypesorc
WHERE (ctinyint IS NULL)
ORDER BY ctinyint, cdouble, cint, c
LIMIT 10;

SELECT ctinyint, cdouble, cint, coalesce(ctinyint+10, (cdouble+log2(cint)), 0) as c
FROM alltypesorc
WHERE (ctinyint IS NULL)
ORDER BY ctinyint, cdouble, cint, c
LIMIT 10;

EXPLAIN SELECT cfloat, cbigint, coalesce(cfloat, cbigint, 0) as c
FROM alltypesorc
WHERE (cfloat IS NULL AND cbigint IS NULL)
ORDER BY cfloat, cbigint, c
LIMIT 10;

SELECT cfloat, cbigint, coalesce(cfloat, cbigint, 0) as c
FROM alltypesorc
WHERE (cfloat IS NULL AND cbigint IS NULL)
ORDER BY cfloat, cbigint, c
LIMIT 10;

EXPLAIN SELECT ctimestamp1, ctimestamp2, coalesce(ctimestamp1, ctimestamp2) as c
FROM alltypesorc
WHERE ctimestamp1 IS NOT NULL OR ctimestamp2 IS NOT NULL
ORDER BY ctimestamp1, ctimestamp2, c
LIMIT 10;

SELECT ctimestamp1, ctimestamp2, coalesce(ctimestamp1, ctimestamp2) as c
FROM alltypesorc
WHERE ctimestamp1 IS NOT NULL OR ctimestamp2 IS NOT NULL
ORDER BY ctimestamp1, ctimestamp2, c
LIMIT 10;

EXPLAIN SELECT cfloat, cbigint, coalesce(cfloat, cbigint) as c
FROM alltypesorc
WHERE (cfloat IS NULL AND cbigint IS NULL)
ORDER BY cfloat, cbigint, c
LIMIT 10;

SELECT cfloat, cbigint, coalesce(cfloat, cbigint) as c
FROM alltypesorc
WHERE (cfloat IS NULL AND cbigint IS NULL)
ORDER BY cfloat, cbigint, c
LIMIT 10;

EXPLAIN SELECT cbigint, ctinyint, coalesce(cbigint, ctinyint) as c
FROM alltypesorc
WHERE cbigint IS NULL
LIMIT 10;

SELECT cbigint, ctinyint, coalesce(cbigint, ctinyint) as c
FROM alltypesorc
WHERE cbigint IS NULL
LIMIT 10;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=false;
set hive.fetch.task.conversion=none;

create table str_str_orc (str1 string, str2 string) stored as orc;

insert into table str_str_orc values (null, "X"), ("0", "X"), ("1", "X"), (null, "y");

EXPLAIN
SELECT
   str2, ROUND(sum(cast(COALESCE(str1, 0) as int))/60, 2) as result
from str_str_orc
GROUP BY str2;

SELECT
   str2, ROUND(sum(cast(COALESCE(str1, 0) as int))/60, 2) as result
from str_str_orc
GROUP BY str2;

EXPLAIN
SELECT COALESCE(str1, 0) as result
from str_str_orc;

SELECT COALESCE(str1, 0) as result
from str_str_orc;

SET hive.vectorized.execution.enabled=true;

EXPLAIN
SELECT
   str2, ROUND(sum(cast(COALESCE(str1, 0) as int))/60, 2) as result
from str_str_orc
GROUP BY str2;

SELECT
   str2, ROUND(sum(cast(COALESCE(str1, 0) as int))/60, 2) as result
from str_str_orc
GROUP BY str2;

EXPLAIN
SELECT COALESCE(str1, 0) as result
from str_str_orc;

SELECT COALESCE(str1, 0) as result
from str_str_orc;
set hive.cli.print.header=true;
set hive.explain.user=false;
set hive.fetch.task.conversion=none;
SET hive.vectorized.execution.enabled=true;

CREATE TABLE orc_create_staging (
  str STRING,
  mp  MAP<STRING,STRING>,
  lst ARRAY<STRING>,
  strct STRUCT<A:STRING,B:STRING>
) ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '|'
    COLLECTION ITEMS TERMINATED BY ','
    MAP KEYS TERMINATED BY ':';

LOAD DATA LOCAL INPATH '../../data/files/orc_create.txt' OVERWRITE INTO TABLE orc_create_staging;

CREATE TABLE orc_create_complex (
  str STRING,
  mp  MAP<STRING,STRING>,
  lst ARRAY<STRING>,
  strct STRUCT<A:STRING,B:STRING>
) STORED AS ORC;

INSERT OVERWRITE TABLE orc_create_complex SELECT * FROM orc_create_staging;

-- Since complex types are not supported, this query should not vectorize.
EXPLAIN
SELECT * FROM orc_create_complex;

SELECT * FROM orc_create_complex;

-- However, since this query is not referencing the complex fields, it should vectorize.
EXPLAIN
SELECT COUNT(*) FROM orc_create_complex;

SELECT COUNT(*) FROM orc_create_complex;

-- Also, since this query is not referencing the complex fields, it should vectorize.
EXPLAIN
SELECT str FROM orc_create_complex ORDER BY str;

SELECT str FROM orc_create_complex ORDER BY str;set hive.cli.print.header=true;
set hive.explain.user=false;
set hive.fetch.task.conversion=none;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
set hive.fetch.task.conversion=none;

-- From HIVE-10729.  Not expected to vectorize this query.
--
CREATE TABLE test (a INT, b MAP<INT, STRING>) STORED AS ORC;
INSERT OVERWRITE TABLE test SELECT 199408978, MAP(1, "val_1", 2, "val_2") FROM src LIMIT 1;

explain
select * from alltypesorc join test where alltypesorc.cint=test.a;

select * from alltypesorc join test where alltypesorc.cint=test.a;



CREATE TABLE test2a (a ARRAY<INT>) STORED AS ORC;
INSERT OVERWRITE TABLE test2a SELECT ARRAY(1, 2) FROM src LIMIT 1;

CREATE TABLE test2b (a INT) STORED AS ORC;
INSERT OVERWRITE TABLE test2b VALUES (2), (3), (4);

explain
select *  from test2b join test2a on test2b.a = test2a.a[1];

select *  from test2b join test2a on test2b.a = test2a.a[1];set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

create table web_sales_txt
(
    ws_sold_date_sk           int,
    ws_sold_time_sk           int,
    ws_ship_date_sk           int,
    ws_item_sk                int,
    ws_bill_customer_sk       int,
    ws_bill_cdemo_sk          int,
    ws_bill_hdemo_sk          int,
    ws_bill_addr_sk           int,
    ws_ship_customer_sk       int,
    ws_ship_cdemo_sk          int,
    ws_ship_hdemo_sk          int,
    ws_ship_addr_sk           int,
    ws_web_page_sk            int,
    ws_web_site_sk            int,
    ws_ship_mode_sk           int,
    ws_warehouse_sk           int,
    ws_promo_sk               int,
    ws_order_number           int,
    ws_quantity               int,
    ws_wholesale_cost         decimal(7,2),
    ws_list_price             decimal(7,2),
    ws_sales_price            decimal(7,2),
    ws_ext_discount_amt       decimal(7,2),
    ws_ext_sales_price        decimal(7,2),
    ws_ext_wholesale_cost     decimal(7,2),
    ws_ext_list_price         decimal(7,2),
    ws_ext_tax                decimal(7,2),
    ws_coupon_amt             decimal(7,2),
    ws_ext_ship_cost          decimal(7,2),
    ws_net_paid               decimal(7,2),
    ws_net_paid_inc_tax       decimal(7,2),
    ws_net_paid_inc_ship      decimal(7,2),
    ws_net_paid_inc_ship_tax  decimal(7,2),
    ws_net_profit             decimal(7,2)
)
row format delimited fields terminated by '|'
stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/web_sales_2k' OVERWRITE INTO TABLE web_sales_txt;

------------------------------------------------------------------------------------------

create table web_sales
(
    ws_sold_date_sk           int,
    ws_sold_time_sk           int,
    ws_ship_date_sk           int,
    ws_item_sk                int,
    ws_bill_customer_sk       int,
    ws_bill_cdemo_sk          int,
    ws_bill_hdemo_sk          int,
    ws_bill_addr_sk           int,
    ws_ship_customer_sk       int,
    ws_ship_cdemo_sk          int,
    ws_ship_hdemo_sk          int,
    ws_ship_addr_sk           int,
    ws_web_page_sk            int,
    ws_ship_mode_sk           int,
    ws_warehouse_sk           int,
    ws_promo_sk               int,
    ws_order_number           int,
    ws_quantity               int,
    ws_wholesale_cost         decimal(7,2),
    ws_list_price             decimal(7,2),
    ws_sales_price            decimal(7,2),
    ws_ext_discount_amt       decimal(7,2),
    ws_ext_sales_price        decimal(7,2),
    ws_ext_wholesale_cost     decimal(7,2),
    ws_ext_list_price         decimal(7,2),
    ws_ext_tax                decimal(7,2),
    ws_coupon_amt             decimal(7,2),
    ws_ext_ship_cost          decimal(7,2),
    ws_net_paid               decimal(7,2),
    ws_net_paid_inc_tax       decimal(7,2),
    ws_net_paid_inc_ship      decimal(7,2),
    ws_net_paid_inc_ship_tax  decimal(7,2),
    ws_net_profit             decimal(7,2)
)
partitioned by
(
    ws_web_site_sk            int
)
stored as orc
tblproperties ("orc.stripe.size"="33554432", "orc.compress.size"="16384");

set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table web_sales
partition (ws_web_site_sk)
select ws_sold_date_sk, ws_sold_time_sk, ws_ship_date_sk, ws_item_sk,
       ws_bill_customer_sk, ws_bill_cdemo_sk, ws_bill_hdemo_sk, ws_bill_addr_sk,
       ws_ship_customer_sk, ws_ship_cdemo_sk, ws_ship_hdemo_sk, ws_ship_addr_sk,
       ws_web_page_sk, ws_ship_mode_sk, ws_warehouse_sk, ws_promo_sk, ws_order_number,
       ws_quantity, ws_wholesale_cost, ws_list_price, ws_sales_price, ws_ext_discount_amt,
       ws_ext_sales_price, ws_ext_wholesale_cost, ws_ext_list_price, ws_ext_tax,
       ws_coupon_amt, ws_ext_ship_cost, ws_net_paid, ws_net_paid_inc_tax, ws_net_paid_inc_ship,
       ws_net_paid_inc_ship_tax, ws_net_profit, ws_web_site_sk from web_sales_txt;

------------------------------------------------------------------------------------------

explain
select count(distinct ws_order_number) from web_sales;

select count(distinct ws_order_number) from web_sales;set hive.fetch.task.conversion=none;

create temporary function UDFHelloTest as 'org.apache.hadoop.hive.ql.exec.vector.UDFHelloTest';

create table testorc1(id int, name string) stored as orc;
insert into table testorc1 values(1, 'a1'), (2,'a2');

set hive.vectorized.execution.enabled=true;
explain
select id, UDFHelloTest(name) from testorc1;
select id, UDFHelloTest(name) from testorc1;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;

DROP TABLE over1k;
DROP TABLE over1korc;

-- data setup
CREATE TABLE over1k(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k;

CREATE TABLE over1korc(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS ORC;

INSERT INTO TABLE over1korc SELECT * FROM over1k;

SET hive.vectorized.execution.enabled=false;

EXPLAIN SELECT t, si, i, b, f, d, bo, s, ts, dec, bin FROM over1korc ORDER BY t, si, i LIMIT 20;

SELECT t, si, i, b, f, d, bo, s, ts, dec, bin FROM over1korc ORDER BY t, si, i LIMIT 20;

SELECT SUM(HASH(*))
FROM (SELECT t, si, i, b, f, d, bo, s, ts, dec, bin FROM over1korc ORDER BY t, si, i) as q;

SET hive.vectorized.execution.enabled=true;

EXPLAIN select t, si, i, b, f, d, bo, s, ts, dec, bin FROM over1korc ORDER BY t, si, i LIMIT 20;

SELECT t, si, i, b, f, d, bo, s, ts, dec, bin FROM over1korc ORDER BY t, si, i LIMIT 20;

SELECT SUM(HASH(*))
FROM (SELECT t, si, i, b, f, d, bo, s, ts, dec, bin FROM over1korc ORDER BY t, si, i) as q;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;

set hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

drop table if exists vector_date_1;
create table vector_date_1 (dt1 date, dt2 date) stored as orc;

insert into table vector_date_1
  select null, null from src limit 1;
insert into table vector_date_1
  select date '1999-12-31', date '2000-01-01' from src limit 1;
insert into table vector_date_1
  select date '2001-01-01', date '2001-06-01' from src limit 1;

-- column-to-column comparison in select clause
explain
select
  dt1, dt2,
  -- should be all true
  dt1 = dt1,
  dt1 != dt2,
  dt1 <= dt1,
  dt1 <= dt2,
  dt1 < dt2,
  dt2 >= dt2,
  dt2 >= dt1,
  dt2 > dt1
from vector_date_1 order by dt1;

select
  dt1, dt2,
  -- should be all true
  dt1 = dt1,
  dt1 != dt2,
  dt1 <= dt1,
  dt1 <= dt2,
  dt1 < dt2,
  dt2 >= dt2,
  dt2 >= dt1,
  dt2 > dt1
from vector_date_1 order by dt1;

explain
select
  dt1, dt2,
  -- should be all false
  dt1 != dt1,
  dt1 = dt2,
  dt1 < dt1,
  dt1 >= dt2,
  dt1 > dt2,
  dt2 > dt2,
  dt2 <= dt1,
  dt2 < dt1
from vector_date_1 order by dt1;

select
  dt1, dt2,
  -- should be all false
  dt1 != dt1,
  dt1 = dt2,
  dt1 < dt1,
  dt1 >= dt2,
  dt1 > dt2,
  dt2 > dt2,
  dt2 <= dt1,
  dt2 < dt1
from vector_date_1 order by dt1;

-- column-to-literal/literal-to-column comparison in select clause
explain
select
  dt1,
  -- should be all true
  dt1 != date '1970-01-01',
  dt1 >= date '1970-01-01',
  dt1 > date '1970-01-01',
  dt1 <= date '2100-01-01',
  dt1 < date '2100-01-01',
  date '1970-01-01' != dt1,
  date '1970-01-01' <= dt1,
  date '1970-01-01' < dt1
from vector_date_1 order by dt1;

select
  dt1,
  -- should be all true
  dt1 != date '1970-01-01',
  dt1 >= date '1970-01-01',
  dt1 > date '1970-01-01',
  dt1 <= date '2100-01-01',
  dt1 < date '2100-01-01',
  date '1970-01-01' != dt1,
  date '1970-01-01' <= dt1,
  date '1970-01-01' < dt1
from vector_date_1 order by dt1;

explain
select
  dt1,
  -- should all be false
  dt1 = date '1970-01-01',
  dt1 <= date '1970-01-01',
  dt1 < date '1970-01-01',
  dt1 >= date '2100-01-01',
  dt1 > date '2100-01-01',
  date '1970-01-01' = dt1,
  date '1970-01-01' >= dt1,
  date '1970-01-01' > dt1
from vector_date_1 order by dt1;

select
  dt1,
  -- should all be false
  dt1 = date '1970-01-01',
  dt1 <= date '1970-01-01',
  dt1 < date '1970-01-01',
  dt1 >= date '2100-01-01',
  dt1 > date '2100-01-01',
  date '1970-01-01' = dt1,
  date '1970-01-01' >= dt1,
  date '1970-01-01' > dt1
from vector_date_1 order by dt1;


-- column-to-column comparisons in predicate
-- all rows with non-null dt1 should be returned
explain
select
  dt1, dt2
from vector_date_1
where
  dt1 = dt1
  and dt1 != dt2
  and dt1 < dt2
  and dt1 <= dt2
  and dt2 > dt1
  and dt2 >= dt1
order by dt1;

select
  dt1, dt2
from vector_date_1
where
  dt1 = dt1
  and dt1 != dt2
  and dt1 < dt2
  and dt1 <= dt2
  and dt2 > dt1
  and dt2 >= dt1
order by dt1;

-- column-to-literal/literal-to-column comparison in predicate
-- only a single row should be returned
explain
select
  dt1, dt2
from vector_date_1
where
  dt1 = date '2001-01-01'
  and date '2001-01-01' = dt1
  and dt1 != date '1970-01-01'
  and date '1970-01-01' != dt1
  and dt1 > date '1970-01-01'
  and dt1 >= date '1970-01-01'
  and date '1970-01-01' < dt1
  and date '1970-01-01' <= dt1
order by dt1;

select
  dt1, dt2
from vector_date_1
where
  dt1 = date '2001-01-01'
  and date '2001-01-01' = dt1
  and dt1 != date '1970-01-01'
  and date '1970-01-01' != dt1
  and dt1 > date '1970-01-01'
  and dt1 >= date '1970-01-01'
  and date '1970-01-01' < dt1
  and date '1970-01-01' <= dt1
order by dt1;

drop table vector_date_1;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

drop table if exists decimal_1;

create table decimal_1 (t decimal(4,2), u decimal(5), v decimal) stored as orc;

desc decimal_1;

insert overwrite table decimal_1
  select cast('17.29' as decimal(4,2)), 3.1415926BD, 3115926.54321BD from src tablesample (1 rows);

explain
select cast(t as boolean) from decimal_1 order by t;

select cast(t as boolean) from decimal_1 order by t;

explain
select cast(t as tinyint) from decimal_1 order by t;

select cast(t as tinyint) from decimal_1 order by t;

explain
select cast(t as smallint) from decimal_1 order by t;

select cast(t as smallint) from decimal_1 order by t;

explain
select cast(t as int) from decimal_1 order by t;

select cast(t as int) from decimal_1 order by t;

explain
select cast(t as bigint) from decimal_1 order by t;

select cast(t as bigint) from decimal_1 order by t;

explain
select cast(t as float) from decimal_1 order by t;

select cast(t as float) from decimal_1 order by t;

explain
select cast(t as double) from decimal_1 order by t;

select cast(t as double) from decimal_1 order by t;

explain
select cast(t as string) from decimal_1 order by t;

select cast(t as string) from decimal_1 order by t;

explain
select cast(t as timestamp) from decimal_1 order by t;

select cast(t as timestamp) from decimal_1 order by t;

drop table decimal_1;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS decimal_txt;
DROP TABLE IF EXISTS `decimal`;

CREATE TABLE decimal_txt (dec decimal);

LOAD DATA LOCAL INPATH '../../data/files/decimal_10_0.txt' OVERWRITE INTO TABLE decimal_txt;

CREATE TABLE `DECIMAL` STORED AS ORC AS SELECT * FROM decimal_txt;

EXPLAIN
SELECT dec FROM `DECIMAL` order by dec;

SELECT dec FROM `DECIMAL` order by dec;

DROP TABLE DECIMAL_txt;
DROP TABLE `DECIMAL`;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

drop table decimal_2;

create table decimal_2 (t decimal(18,9)) stored as orc;

insert overwrite table decimal_2
  select cast('17.29' as decimal(4,2)) from src tablesample (1 rows);

explain
select cast(t as boolean) from decimal_2 order by t;

select cast(t as boolean) from decimal_2 order by t;

explain
select cast(t as tinyint) from decimal_2 order by t;

select cast(t as tinyint) from decimal_2 order by t;

explain
select cast(t as smallint) from decimal_2 order by t;

select cast(t as smallint) from decimal_2 order by t;

explain
select cast(t as int) from decimal_2 order by t;

select cast(t as int) from decimal_2 order by t;

explain
select cast(t as bigint) from decimal_2 order by t;

select cast(t as bigint) from decimal_2 order by t;

explain
select cast(t as float) from decimal_2 order by t;

select cast(t as float) from decimal_2 order by t;

explain
select cast(t as double) from decimal_2 order by t;

select cast(t as double) from decimal_2 order by t;

explain
select cast(t as string) from decimal_2 order by t;

select cast(t as string) from decimal_2 order by t;

insert overwrite table decimal_2
  select cast('3404045.5044003' as decimal(18,9)) from src tablesample (1 rows);

explain
select cast(t as boolean) from decimal_2 order by t;

select cast(t as boolean) from decimal_2 order by t;

explain
select cast(t as tinyint) from decimal_2 order by t;

select cast(t as tinyint) from decimal_2 order by t;

explain
select cast(t as smallint) from decimal_2 order by t;

select cast(t as smallint) from decimal_2 order by t;

explain
select cast(t as int) from decimal_2 order by t;

select cast(t as int) from decimal_2 order by t;

explain
select cast(t as bigint) from decimal_2 order by t;

select cast(t as bigint) from decimal_2 order by t;

explain
select cast(t as float) from decimal_2 order by t;

select cast(t as float) from decimal_2 order by t;

explain
select cast(t as double) from decimal_2 order by t;

select cast(t as double) from decimal_2 order by t;

explain
select cast(t as string) from decimal_2 order by t;

select cast(t as string) from decimal_2 order by t;

explain
select cast(3.14 as decimal(4,2)) as c from decimal_2 order by c;

select cast(3.14 as decimal(4,2)) as c from decimal_2 order by c;

explain
select cast(cast(3.14 as float) as decimal(4,2)) as c from decimal_2 order by c;

select cast(cast(3.14 as float) as decimal(4,2)) as c from decimal_2 order by c;

explain
select cast(cast('2012-12-19 11:12:19.1234567' as timestamp) as decimal(30,8)) as c from decimal_2 order by c;

select cast(cast('2012-12-19 11:12:19.1234567' as timestamp) as decimal(30,8)) as c from decimal_2 order by c;

explain
select cast(true as decimal) as c from decimal_2 order by c;

explain
select cast(true as decimal) as c from decimal_2 order by c;

select cast(true as decimal) as c from decimal_2 order by c;

explain
select cast(3Y as decimal) as c from decimal_2 order by c;

select cast(3Y as decimal) as c from decimal_2 order by c;

explain
select cast(3S as decimal) as c from decimal_2 order by c;

select cast(3S as decimal) as c from decimal_2 order by c;

explain
select cast(cast(3 as int) as decimal) as c from decimal_2 order by c;

select cast(cast(3 as int) as decimal) as c from decimal_2 order by c;

explain
select cast(3L as decimal) as c from decimal_2 order by c;

select cast(3L as decimal) as c from decimal_2 order by c;

explain
select cast(0.99999999999999999999 as decimal(20,19)) as c from decimal_2 order by c;

select cast(0.99999999999999999999 as decimal(20,19)) as c from decimal_2 order by c;

explain
select cast('0.99999999999999999999' as decimal(20,20)) as c from decimal_2 order by c;

select cast('0.99999999999999999999' as decimal(20,20)) as c from decimal_2 order by c;
drop table decimal_2;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS DECIMAL_3_txt;
DROP TABLE IF EXISTS DECIMAL_3;

CREATE TABLE DECIMAL_3_txt(key decimal(38,18), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_3_txt;

CREATE TABLE DECIMAL_3 STORED AS ORC AS SELECT * FROM DECIMAL_3_txt;

SELECT * FROM DECIMAL_3 ORDER BY key, value;

SELECT * FROM DECIMAL_3 ORDER BY key DESC, value DESC;

SELECT * FROM DECIMAL_3 ORDER BY key, value;

SELECT DISTINCT key FROM DECIMAL_3 ORDER BY key;

SELECT key, sum(value) FROM DECIMAL_3 GROUP BY key ORDER BY key;

SELECT value, sum(key) FROM DECIMAL_3 GROUP BY value ORDER BY value;

SELECT * FROM DECIMAL_3 a JOIN DECIMAL_3 b ON (a.key = b.key) ORDER BY a.key, a.value, b.value;

SELECT * FROM DECIMAL_3 WHERE key=3.14 ORDER BY key, value;

SELECT * FROM DECIMAL_3 WHERE key=3.140 ORDER BY key, value;

DROP TABLE DECIMAL_3_txt;
DROP TABLE DECIMAL_3;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS DECIMAL_4_1;
DROP TABLE IF EXISTS DECIMAL_4_2;

CREATE TABLE DECIMAL_4_1(key decimal(35,25), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

CREATE TABLE DECIMAL_4_2(key decimal(35,25), value decimal(35,25))
STORED AS ORC;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_4_1;

INSERT OVERWRITE TABLE DECIMAL_4_2 SELECT key, key * 3 FROM DECIMAL_4_1;

SELECT * FROM DECIMAL_4_1 ORDER BY key, value;

SELECT * FROM DECIMAL_4_2 ORDER BY key, value;

SELECT * FROM DECIMAL_4_2 ORDER BY key;

SELECT * FROM DECIMAL_4_2 ORDER BY key, value;

DROP TABLE DECIMAL_4_1;
DROP TABLE DECIMAL_4_2;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS DECIMAL_5_txt;
DROP TABLE IF EXISTS DECIMAL_5;

CREATE TABLE DECIMAL_5_txt(key decimal(10,5), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_5_txt;

CREATE TABLE DECIMAL_5(key decimal(10,5), value int)
STORED AS ORC;

INSERT OVERWRITE TABLE DECIMAL_5 SELECT * FROM DECIMAL_5_txt;

SELECT key FROM DECIMAL_5 ORDER BY key;

SELECT DISTINCT key FROM DECIMAL_5 ORDER BY key;

SELECT cast(key as decimal) FROM DECIMAL_5;

SELECT cast(key as decimal(6,3)) FROM DECIMAL_5;

DROP TABLE DECIMAL_5_txt;
DROP TABLE DECIMAL_5;set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS DECIMAL_6_1_txt;
DROP TABLE IF EXISTS DECIMAL_6_1;
DROP TABLE IF EXISTS DECIMAL_6_2_txt;
DROP TABLE IF EXISTS DECIMAL_6_2;
DROP TABLE IF EXISTS DECIMAL_6_3_txt;
DROP TABLE IF EXISTS DECIMAL_6_3;

CREATE TABLE DECIMAL_6_1_txt(key decimal(10,5), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

CREATE TABLE DECIMAL_6_2_txt(key decimal(17,4), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv9.txt' INTO TABLE DECIMAL_6_1_txt;
LOAD DATA LOCAL INPATH '../../data/files/kv9.txt' INTO TABLE DECIMAL_6_2_txt;

CREATE TABLE DECIMAL_6_1(key decimal(10,5), value int)
STORED AS ORC;

CREATE TABLE DECIMAL_6_2(key decimal(17,4), value int)
STORED AS ORC;

INSERT OVERWRITE TABLE DECIMAL_6_1 SELECT * FROM DECIMAL_6_1_txt;
INSERT OVERWRITE TABLE DECIMAL_6_2 SELECT * FROM DECIMAL_6_2_txt;

SELECT * FROM DECIMAL_6_1 ORDER BY key, value;

SELECT * FROM DECIMAL_6_2 ORDER BY key, value;

SELECT T.key from (
  SELECT key, value from DECIMAL_6_1
  UNION ALL
  SELECT key, value from DECIMAL_6_2
) T order by T.key;

CREATE TABLE DECIMAL_6_3 STORED AS ORC AS SELECT key + 5.5 AS k, value * 11 AS v from DECIMAL_6_1 ORDER BY v;

desc DECIMAL_6_3;

SELECT * FROM DECIMAL_6_3 ORDER BY k, v;

set hive.explain.user=false;
CREATE TABLE decimal_vgby STORED AS ORC AS
    SELECT cdouble, CAST (((cdouble*22.1)/37) AS DECIMAL(20,10)) AS cdecimal1,
    CAST (((cdouble*9.3)/13) AS DECIMAL(23,14)) AS cdecimal2,
    cint
    FROM alltypesorc;

SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

-- First only do simple aggregations that output primitives only
EXPLAIN SELECT cint,
    COUNT(cdecimal1), MAX(cdecimal1), MIN(cdecimal1), SUM(cdecimal1),
    COUNT(cdecimal2), MAX(cdecimal2), MIN(cdecimal2), SUM(cdecimal2)
    FROM decimal_vgby
    GROUP BY cint
    HAVING COUNT(*) > 1;
SELECT cint,
    COUNT(cdecimal1), MAX(cdecimal1), MIN(cdecimal1), SUM(cdecimal1),
    COUNT(cdecimal2), MAX(cdecimal2), MIN(cdecimal2), SUM(cdecimal2)
    FROM decimal_vgby
    GROUP BY cint
    HAVING COUNT(*) > 1;

-- Now add the others...
EXPLAIN SELECT cint,
    COUNT(cdecimal1), MAX(cdecimal1), MIN(cdecimal1), SUM(cdecimal1), AVG(cdecimal1), STDDEV_POP(cdecimal1), STDDEV_SAMP(cdecimal1),
    COUNT(cdecimal2), MAX(cdecimal2), MIN(cdecimal2), SUM(cdecimal2), AVG(cdecimal2), STDDEV_POP(cdecimal2), STDDEV_SAMP(cdecimal2)
    FROM decimal_vgby
    GROUP BY cint
    HAVING COUNT(*) > 1;
SELECT cint,
    COUNT(cdecimal1), MAX(cdecimal1), MIN(cdecimal1), SUM(cdecimal1), AVG(cdecimal1), STDDEV_POP(cdecimal1), STDDEV_SAMP(cdecimal1),
    COUNT(cdecimal2), MAX(cdecimal2), MIN(cdecimal2), SUM(cdecimal2), AVG(cdecimal2), STDDEV_POP(cdecimal2), STDDEV_SAMP(cdecimal2)
    FROM decimal_vgby
    GROUP BY cint
    HAVING COUNT(*) > 1;set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

EXPLAIN SELECT cdouble, cint, cboolean1, ctimestamp1, CAST(cdouble AS DECIMAL(20,10)), CAST(cint AS DECIMAL(23,14)), CAST(cboolean1 AS DECIMAL(5,2)), CAST(ctimestamp1 AS DECIMAL(15,0)) FROM alltypesorc WHERE cdouble IS NOT NULL AND cint IS NOT NULL AND cboolean1 IS NOT NULL AND ctimestamp1 IS NOT NULL LIMIT 10;

SELECT cdouble, cint, cboolean1, ctimestamp1, CAST(cdouble AS DECIMAL(20,10)), CAST(cint AS DECIMAL(23,14)), CAST(cboolean1 AS DECIMAL(5,2)), CAST(ctimestamp1 AS DECIMAL(15,0)) FROM alltypesorc WHERE cdouble IS NOT NULL AND cint IS NOT NULL AND cboolean1 IS NOT NULL AND ctimestamp1 IS NOT NULL LIMIT 10;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;

-- SORT_QUERY_RESULTS

CREATE TABLE decimal_test STORED AS ORC AS SELECT cdouble, CAST (((cdouble*22.1)/37) AS DECIMAL(20,10)) AS cdecimal1, CAST (((cdouble*9.3)/13) AS DECIMAL(23,14)) AS cdecimal2 FROM alltypesorc;
SET hive.vectorized.execution.enabled=true;
EXPLAIN SELECT cdecimal1 + cdecimal2 as c1, cdecimal1 - (2*cdecimal2) as c2, ((cdecimal1+2.34)/cdecimal2) as c3, (cdecimal1 * (cdecimal2/3.4)) as c4, cdecimal1 % 10 as c5, CAST(cdecimal1 AS INT) as c6, CAST(cdecimal2 AS SMALLINT) as c7, CAST(cdecimal2 AS TINYINT) as c8, CAST(cdecimal1 AS BIGINT) as c9, CAST (cdecimal1 AS BOOLEAN) as c10, CAST(cdecimal2 AS DOUBLE) as c11, CAST(cdecimal1 AS FLOAT) as c12, CAST(cdecimal2 AS STRING) as c13, CAST(cdecimal1 AS TIMESTAMP) as c14 FROM decimal_test WHERE cdecimal1 > 0 AND cdecimal1 < 12345.5678 AND cdecimal2 != 0 AND cdecimal2 > 1000 AND cdouble IS NOT NULL
ORDER BY c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14
LIMIT 10;

SELECT cdecimal1 + cdecimal2 as c1, cdecimal1 - (2*cdecimal2) as c2, ((cdecimal1+2.34)/cdecimal2) as c3, (cdecimal1 * (cdecimal2/3.4)) as c4, cdecimal1 % 10 as c5, CAST(cdecimal1 AS INT) as c6, CAST(cdecimal2 AS SMALLINT) as c7, CAST(cdecimal2 AS TINYINT) as c8, CAST(cdecimal1 AS BIGINT) as c9, CAST (cdecimal1 AS BOOLEAN) as c10, CAST(cdecimal2 AS DOUBLE) as c11, CAST(cdecimal1 AS FLOAT) as c12, CAST(cdecimal2 AS STRING) as c13, CAST(cdecimal1 AS TIMESTAMP) as c14 FROM decimal_test WHERE cdecimal1 > 0 AND cdecimal1 < 12345.5678 AND cdecimal2 != 0 AND cdecimal2 > 1000 AND cdouble IS NOT NULL
ORDER BY c1, c2, c3, c4, c5, c6, c7, c8, c9, c10, c11, c12, c13, c14
LIMIT 10;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;
SET hive.vectorized.execution.enabled=true;

CREATE TABLE over1k(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k;

CREATE TABLE t1(dec decimal(4,2)) STORED AS ORC;
INSERT INTO TABLE t1 select dec from over1k;
CREATE TABLE t2(dec decimal(4,0)) STORED AS ORC;
INSERT INTO TABLE t2 select dec from over1k;

explain
select t1.dec, t2.dec from t1 join t2 on (t1.dec=t2.dec);

-- SORT_QUERY_RESULTS

select t1.dec, t2.dec from t1 join t2 on (t1.dec=t2.dec);
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
CREATE TABLE decimal_test STORED AS ORC AS SELECT cbigint, cdouble, CAST (((cdouble*22.1)/37) AS DECIMAL(20,10)) AS cdecimal1, CAST (((cdouble*9.3)/13) AS DECIMAL(23,14)) AS cdecimal2 FROM alltypesorc;
SET hive.vectorized.execution.enabled=true;

-- Test math functions in vectorized mode to verify they run correctly end-to-end.

explain
select
   cdecimal1
  ,Round(cdecimal1, 2)
  ,Round(cdecimal1)
  ,Floor(cdecimal1)
  ,Ceil(cdecimal1)
  ,round(Exp(cdecimal1), 58)
  ,Ln(cdecimal1)
  ,Log10(cdecimal1)
  -- Use log2 as a representative function to test all input types.
  ,Log2(cdecimal1)
  -- Use 15601.0 to test zero handling, as there are no zeroes in the table
  ,Log2(cdecimal1 - 15601.0)
  ,Log(2.0, cdecimal1)
  ,Pow(log2(cdecimal1), 2.0)
  ,Power(log2(cdecimal1), 2.0)
  ,Sqrt(cdecimal1)
  ,Abs(cdecimal1)
  ,Sin(cdecimal1)
  ,Asin(cdecimal1)
  ,Cos(cdecimal1)
  ,ACos(cdecimal1)
  ,Atan(cdecimal1)
  ,Degrees(cdecimal1)
  ,Radians(cdecimal1)
  ,Positive(cdecimal1)
  ,Negative(cdecimal1)
  ,Sign(cdecimal1)
  -- Test nesting
  ,cos(-sin(log(cdecimal1)) + 3.14159)
from decimal_test
-- limit output to a reasonably small number of rows
where cbigint % 500 = 0
-- test use of a math function in the WHERE clause
and sin(cdecimal1) >= -1.0;

select
   cdecimal1
  ,Round(cdecimal1, 2)
  ,Round(cdecimal1)
  ,Floor(cdecimal1)
  ,Ceil(cdecimal1)
  ,round(Exp(cdecimal1), 58)
  ,Ln(cdecimal1)
  ,Log10(cdecimal1)
  -- Use log2 as a representative function to test all input types.
  ,Log2(cdecimal1)
  -- Use 15601.0 to test zero handling, as there are no zeroes in the table
  ,Log2(cdecimal1 - 15601.0)
  ,Log(2.0, cdecimal1)
  ,Pow(log2(cdecimal1), 2.0)
  ,Power(log2(cdecimal1), 2.0)
  ,Sqrt(cdecimal1)
  ,Abs(cdecimal1)
  ,Sin(cdecimal1)
  ,Asin(cdecimal1)
  ,Cos(cdecimal1)
  ,ACos(cdecimal1)
  ,Atan(cdecimal1)
  ,Degrees(cdecimal1)
  ,Radians(cdecimal1)
  ,Positive(cdecimal1)
  ,Negative(cdecimal1)
  ,Sign(cdecimal1)
  -- Test nesting
  ,cos(-sin(log(cdecimal1)) + 3.14159)
from decimal_test
-- limit output to a reasonably small number of rows
where cbigint % 500 = 0
-- test use of a math function in the WHERE clause
and sin(cdecimal1) >= -1.0;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS DECIMAL_PRECISION_txt;
DROP TABLE IF EXISTS DECIMAL_PRECISION;

CREATE TABLE DECIMAL_PRECISION_txt(dec decimal(20,10))
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv8.txt' INTO TABLE DECIMAL_PRECISION_txt;

CREATE TABLE DECIMAL_PRECISION(dec decimal(20,10))
STORED AS ORC;

INSERT OVERWRITE TABLE DECIMAL_PRECISION SELECT * FROM DECIMAL_PRECISION_txt;

SELECT * FROM DECIMAL_PRECISION ORDER BY dec;

SELECT dec, dec + 1, dec - 1 FROM DECIMAL_PRECISION ORDER BY dec;
SELECT dec, dec * 2, dec / 3  FROM DECIMAL_PRECISION ORDER BY dec;
SELECT dec, dec / 9 FROM DECIMAL_PRECISION ORDER BY dec;
SELECT dec, dec / 27 FROM DECIMAL_PRECISION ORDER BY dec;
SELECT dec, dec * dec FROM DECIMAL_PRECISION ORDER BY dec;

EXPLAIN SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION;
SELECT avg(dec), sum(dec) FROM DECIMAL_PRECISION;

SELECT dec * cast('12345678901234567890.12345678' as decimal(38,18)) FROM DECIMAL_PRECISION LIMIT 1;
SELECT * from DECIMAL_PRECISION WHERE dec > cast('1234567890123456789012345678.12345678' as decimal(38,18)) LIMIT 1;
SELECT dec * 12345678901234567890.12345678 FROM DECIMAL_PRECISION LIMIT 1;

SELECT MIN(cast('12345678901234567890.12345678' as decimal(38,18))) FROM DECIMAL_PRECISION;
SELECT COUNT(cast('12345678901234567890.12345678' as decimal(38,18))) FROM DECIMAL_PRECISION;

DROP TABLE DECIMAL_PRECISION_txt;
DROP TABLE DECIMAL_PRECISION;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

create table decimal_tbl_txt (dec decimal(10,0))
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

insert into table decimal_tbl_txt values(101);

select * from decimal_tbl_txt;

explain
select dec, round(dec, -1) from decimal_tbl_txt order by dec;

select dec, round(dec, -1) from decimal_tbl_txt order by dec;

explain
select dec, round(dec, -1) from decimal_tbl_txt order by round(dec, -1);

select dec, round(dec, -1) from decimal_tbl_txt order by round(dec, -1);

create table decimal_tbl_rc (dec decimal(10,0))
row format serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe' stored as rcfile;

insert into table decimal_tbl_rc values(101);

select * from decimal_tbl_rc;

explain
select dec, round(dec, -1) from decimal_tbl_rc order by dec;

select dec, round(dec, -1) from decimal_tbl_rc order by dec;

explain
select dec, round(dec, -1) from decimal_tbl_rc order by round(dec, -1);

select dec, round(dec, -1) from decimal_tbl_rc order by round(dec, -1);

create table decimal_tbl_orc (dec decimal(10,0))
stored as orc;

insert into table decimal_tbl_orc values(101);

select * from decimal_tbl_orc;

explain
select dec, round(dec, -1) from decimal_tbl_orc order by dec;

select dec, round(dec, -1) from decimal_tbl_orc order by dec;

explain
select dec, round(dec, -1) from decimal_tbl_orc order by round(dec, -1);

select dec, round(dec, -1) from decimal_tbl_orc order by round(dec, -1);set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

create table decimal_tbl_1_orc (dec decimal(38,18))
STORED AS ORC;

insert into table decimal_tbl_1_orc values(55555);

select * from decimal_tbl_1_orc;

-- EXPLAIN
-- SELECT dec, round(null), round(null, 0), round(125, null),
-- round(1.0/0.0, 0), round(power(-1.0,0.5), 0)
-- FROM decimal_tbl_1_orc ORDER BY dec;

-- SELECT dec, round(null), round(null, 0), round(125, null),
-- round(1.0/0.0, 0), round(power(-1.0,0.5), 0)
-- FROM decimal_tbl_1_orc ORDER BY dec;

EXPLAIN
SELECT
  round(dec) as d, round(dec, 0), round(dec, 1), round(dec, 2), round(dec, 3),
  round(dec, -1), round(dec, -2), round(dec, -3), round(dec, -4),
  round(dec, -5), round(dec, -6), round(dec, -7), round(dec, -8)
FROM decimal_tbl_1_orc ORDER BY d;

SELECT
  round(dec) as d, round(dec, 0), round(dec, 1), round(dec, 2), round(dec, 3),
  round(dec, -1), round(dec, -2), round(dec, -3), round(dec, -4),
  round(dec, -5), round(dec, -6), round(dec, -7), round(dec, -8)
FROM decimal_tbl_1_orc ORDER BY d;

create table decimal_tbl_2_orc (pos decimal(38,18), neg decimal(38,18))
STORED AS ORC;

insert into table decimal_tbl_2_orc values(125.315, -125.315);

select * from decimal_tbl_2_orc;

EXPLAIN
SELECT
  round(pos) as p, round(pos, 0),
  round(pos, 1), round(pos, 2), round(pos, 3), round(pos, 4),
  round(pos, -1), round(pos, -2), round(pos, -3), round(pos, -4),
  round(neg), round(neg, 0),
  round(neg, 1), round(neg, 2), round(neg, 3), round(neg, 4),
  round(neg, -1), round(neg, -2), round(neg, -3), round(neg, -4)
FROM decimal_tbl_2_orc ORDER BY p;

SELECT
  round(pos) as p, round(pos, 0),
  round(pos, 1), round(pos, 2), round(pos, 3), round(pos, 4),
  round(pos, -1), round(pos, -2), round(pos, -3), round(pos, -4),
  round(neg), round(neg, 0),
  round(neg, 1), round(neg, 2), round(neg, 3), round(neg, 4),
  round(neg, -1), round(neg, -2), round(neg, -3), round(neg, -4)
FROM decimal_tbl_2_orc ORDER BY p;

create table decimal_tbl_3_orc (dec decimal(38,18))
STORED AS ORC;

insert into table decimal_tbl_3_orc values(3.141592653589793);

select * from decimal_tbl_3_orc;

EXPLAIN
SELECT
  round(dec, -15) as d, round(dec, -16),
  round(dec, -13), round(dec, -14),
  round(dec, -11), round(dec, -12),
  round(dec, -9), round(dec, -10),
  round(dec, -7), round(dec, -8),
  round(dec, -5), round(dec, -6),
  round(dec, -3), round(dec, -4),
  round(dec, -1), round(dec, -2),
  round(dec, 0), round(dec, 1),
  round(dec, 2), round(dec, 3),
  round(dec, 4), round(dec, 5),
  round(dec, 6), round(dec, 7),
  round(dec, 8), round(dec, 9),
  round(dec, 10), round(dec, 11),
  round(dec, 12), round(dec, 13),
  round(dec, 13), round(dec, 14),
  round(dec, 15), round(dec, 16)
FROM decimal_tbl_3_orc ORDER BY d;

SELECT
  round(dec, -15) as d, round(dec, -16),
  round(dec, -13), round(dec, -14),
  round(dec, -11), round(dec, -12),
  round(dec, -9), round(dec, -10),
  round(dec, -7), round(dec, -8),
  round(dec, -5), round(dec, -6),
  round(dec, -3), round(dec, -4),
  round(dec, -1), round(dec, -2),
  round(dec, 0), round(dec, 1),
  round(dec, 2), round(dec, 3),
  round(dec, 4), round(dec, 5),
  round(dec, 6), round(dec, 7),
  round(dec, 8), round(dec, 9),
  round(dec, 10), round(dec, 11),
  round(dec, 12), round(dec, 13),
  round(dec, 13), round(dec, 14),
  round(dec, 15), round(dec, 16)
FROM decimal_tbl_3_orc ORDER BY d;

create table decimal_tbl_4_orc (pos decimal(38,18), neg decimal(38,18))
STORED AS ORC;

insert into table decimal_tbl_4_orc values(1809242.3151111344, -1809242.3151111344);

select * from decimal_tbl_4_orc;

EXPLAIN
SELECT round(pos, 9) as p, round(neg, 9), round(1809242.3151111344BD, 9), round(-1809242.3151111344BD, 9)
FROM decimal_tbl_4_orc ORDER BY p;

SELECT round(pos, 9) as p, round(neg, 9), round(1809242.3151111344BD, 9), round(-1809242.3151111344BD, 9)
FROM decimal_tbl_4_orc ORDER BY p;
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS DECIMAL_TRAILING_txt;
DROP TABLE IF EXISTS DECIMAL_TRAILING;

CREATE TABLE DECIMAL_TRAILING_txt (
  id int,
  a decimal(10,4),
  b decimal(15,8)
  )
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv10.txt' INTO TABLE DECIMAL_TRAILING_txt;

CREATE TABLE DECIMAL_TRAILING (
  id int,
  a decimal(10,4),
  b decimal(15,8)
  )
STORED AS ORC;

INSERT OVERWRITE TABLE DECIMAL_TRAILING SELECT * FROM DECIMAL_TRAILING_txt;

SELECT * FROM DECIMAL_TRAILING ORDER BY id;

DROP TABLE DECIMAL_TRAILING_txt;
DROP TABLE DECIMAL_TRAILING;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS DECIMAL_UDF_txt;
DROP TABLE IF EXISTS DECIMAL_UDF;

CREATE TABLE DECIMAL_UDF_txt (key decimal(20,10), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt;

CREATE TABLE DECIMAL_UDF (key decimal(20,10), value int)
STORED AS ORC;

INSERT OVERWRITE TABLE DECIMAL_UDF SELECT * FROM DECIMAL_UDF_txt;

-- addition
EXPLAIN SELECT key + key FROM DECIMAL_UDF;
SELECT key + key FROM DECIMAL_UDF;

EXPLAIN SELECT key + value FROM DECIMAL_UDF;
SELECT key + value FROM DECIMAL_UDF;

EXPLAIN SELECT key + (value/2) FROM DECIMAL_UDF;
SELECT key + (value/2) FROM DECIMAL_UDF;

EXPLAIN SELECT key + '1.0' FROM DECIMAL_UDF;
SELECT key + '1.0' FROM DECIMAL_UDF;

-- substraction
EXPLAIN SELECT key - key FROM DECIMAL_UDF;
SELECT key - key FROM DECIMAL_UDF;

EXPLAIN SELECT key - value FROM DECIMAL_UDF;
SELECT key - value FROM DECIMAL_UDF;

EXPLAIN SELECT key - (value/2) FROM DECIMAL_UDF;
SELECT key - (value/2) FROM DECIMAL_UDF;

EXPLAIN SELECT key - '1.0' FROM DECIMAL_UDF;
SELECT key - '1.0' FROM DECIMAL_UDF;

-- multiplication
EXPLAIN SELECT key * key FROM DECIMAL_UDF;
SELECT key * key FROM DECIMAL_UDF;

EXPLAIN SELECT key, value FROM DECIMAL_UDF where key * value > 0;
SELECT key, value FROM DECIMAL_UDF where key * value > 0;

EXPLAIN SELECT key * value FROM DECIMAL_UDF;
SELECT key * value FROM DECIMAL_UDF;

EXPLAIN SELECT key * (value/2) FROM DECIMAL_UDF;
SELECT key * (value/2) FROM DECIMAL_UDF;

EXPLAIN SELECT key * '2.0' FROM DECIMAL_UDF;
SELECT key * '2.0' FROM DECIMAL_UDF;

-- division
EXPLAIN SELECT key / 0 FROM DECIMAL_UDF limit 1;
SELECT key / 0 FROM DECIMAL_UDF limit 1;

EXPLAIN SELECT key / NULL FROM DECIMAL_UDF limit 1;
SELECT key / NULL FROM DECIMAL_UDF limit 1;

EXPLAIN SELECT key / key FROM DECIMAL_UDF WHERE key is not null and key <> 0;
SELECT key / key FROM DECIMAL_UDF WHERE key is not null and key <> 0;

EXPLAIN SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0;
SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0;

EXPLAIN SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0;
SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0;

EXPLAIN SELECT 1 + (key / '2.0') FROM DECIMAL_UDF;
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF;

-- abs
EXPLAIN SELECT abs(key) FROM DECIMAL_UDF;
SELECT abs(key) FROM DECIMAL_UDF;

-- avg
EXPLAIN SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF GROUP BY value ORDER BY value;
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF GROUP BY value ORDER BY value;

-- negative
EXPLAIN SELECT -key FROM DECIMAL_UDF;
SELECT -key FROM DECIMAL_UDF;

-- positive
EXPLAIN SELECT +key FROM DECIMAL_UDF;
SELECT +key FROM DECIMAL_UDF;

-- ceiling
EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF;
SELECT CEIL(key) FROM DECIMAL_UDF;

-- floor
EXPLAIN SELECT FLOOR(key) FROM DECIMAL_UDF;
SELECT FLOOR(key) FROM DECIMAL_UDF;

-- round
EXPLAIN SELECT ROUND(key, 2) FROM DECIMAL_UDF;
SELECT ROUND(key, 2) FROM DECIMAL_UDF;

-- power
EXPLAIN SELECT POWER(key, 2) FROM DECIMAL_UDF;
SELECT POWER(key, 2) FROM DECIMAL_UDF;

-- modulo
EXPLAIN SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF;
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF;

-- stddev, var
EXPLAIN SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF GROUP BY value;
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF GROUP BY value;

-- stddev_samp, var_samp
EXPLAIN SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value;
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value;

-- histogram
EXPLAIN SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF;
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF;

-- min
EXPLAIN SELECT MIN(key) FROM DECIMAL_UDF;
SELECT MIN(key) FROM DECIMAL_UDF;

-- max
EXPLAIN SELECT MAX(key) FROM DECIMAL_UDF;
SELECT MAX(key) FROM DECIMAL_UDF;

-- count
EXPLAIN SELECT COUNT(key) FROM DECIMAL_UDF;
SELECT COUNT(key) FROM DECIMAL_UDF;

DROP TABLE IF EXISTS DECIMAL_UDF_txt;
DROP TABLE IF EXISTS DECIMAL_UDF;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

DROP TABLE IF EXISTS DECIMAL_UDF2_txt;
DROP TABLE IF EXISTS DECIMAL_UDF2;

CREATE TABLE DECIMAL_UDF2_txt (key decimal(20,10), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF2_txt;

CREATE TABLE DECIMAL_UDF2 (key decimal(20,10), value int)
STORED AS ORC;

INSERT OVERWRITE TABLE DECIMAL_UDF2 SELECT * FROM DECIMAL_UDF2_txt;

EXPLAIN
SELECT acos(key), asin(key), atan(key), cos(key), sin(key), tan(key), radians(key)
FROM DECIMAL_UDF2 WHERE key = 10;

SELECT acos(key), asin(key), atan(key), cos(key), sin(key), tan(key), radians(key)
FROM DECIMAL_UDF2 WHERE key = 10;

EXPLAIN
SELECT
  exp(key), ln(key),
  log(key), log(key, key), log(key, value), log(value, key),
  log10(key), sqrt(key)
FROM DECIMAL_UDF2 WHERE key = 10;

SELECT
  exp(key), ln(key),
  log(key), log(key, key), log(key, value), log(value, key),
  log10(key), sqrt(key)
FROM DECIMAL_UDF2 WHERE key = 10;

DROP TABLE IF EXISTS DECIMAL_UDF2_txt;
DROP TABLE IF EXISTS DECIMAL_UDF2;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

explain
select distinct s, t from vectortab2korc;

select distinct s, t from vectortab2korc;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

EXPLAIN SELECT (ctinyint % 2) + 1, cstring1, cint, elt((ctinyint % 2) + 1, cstring1, cint)
FROM alltypesorc
WHERE ctinyint > 0 LIMIT 10;

SELECT (ctinyint % 2) + 1, cstring1, cint, elt((ctinyint % 2) + 1, cstring1, cint)
FROM alltypesorc
WHERE ctinyint > 0 LIMIT 10;

EXPLAIN
SELECT elt(2, 'abc', 'defg'),
       elt(3, 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg'),
       elt('1', 'abc', 'defg'),
       elt(2, 'aa', CAST('2' AS TINYINT)),
       elt(2, 'aa', CAST('12345' AS SMALLINT)),
       elt(2, 'aa', CAST('123456789012' AS BIGINT)),
       elt(2, 'aa', CAST(1.25 AS FLOAT)),
       elt(2, 'aa', CAST(16.0 AS DOUBLE)),
       elt(0, 'abc', 'defg'),
       elt(3, 'abc', 'defg')
FROM alltypesorc LIMIT 1;

SELECT elt(2, 'abc', 'defg'),
       elt(3, 'aa', 'bb', 'cc', 'dd', 'ee', 'ff', 'gg'),
       elt('1', 'abc', 'defg'),
       elt(2, 'aa', CAST('2' AS TINYINT)),
       elt(2, 'aa', CAST('12345' AS SMALLINT)),
       elt(2, 'aa', CAST('123456789012' AS BIGINT)),
       elt(2, 'aa', CAST(1.25 AS FLOAT)),
       elt(2, 'aa', CAST(16.0 AS DOUBLE)),
       elt(0, 'abc', 'defg'),
       elt(3, 'abc', 'defg')
FROM alltypesorc LIMIT 1;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

-- SORT_QUERY_RESULTS

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

explain
select s, t, max(b) from vectortab2korc group by s, t;

select s, t, max(b) from vectortab2korc group by s, t;
set hive.mapred.mode=nonstrict;
set hive.explain.user=true;
SET hive.vectorized.execution.enabled = true;
set hive.fetch.task.conversion=none;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;
set hive.exec.dynamic.partition.mode=nonstrict;

-- HIVE-12738 -- We are checking if a MapJoin after a GroupBy will work properly.
explain
select *
from src
where not key in
(select key from src)
order by key;

select *
from src
where not key in
(select key from src)
order by key;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

create table store_sales_txt
(
    ss_sold_date_sk           int,
    ss_sold_time_sk           int,
    ss_item_sk                int,
    ss_customer_sk            int,
    ss_cdemo_sk               int,
    ss_hdemo_sk               int,
    ss_addr_sk                int,
    ss_store_sk               int,
    ss_promo_sk               int,
    ss_ticket_number          int,
    ss_quantity               int,
    ss_wholesale_cost         float,
    ss_list_price             float,
    ss_sales_price            float,
    ss_ext_discount_amt       float,
    ss_ext_sales_price        float,
    ss_ext_wholesale_cost     float,
    ss_ext_list_price         float,
    ss_ext_tax                float,
    ss_coupon_amt             float,
    ss_net_paid               float,
    ss_net_paid_inc_tax       float,
    ss_net_profit             float
)
row format delimited fields terminated by '|'
stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/store_sales.txt' OVERWRITE INTO TABLE store_sales_txt;

create table store_sales
(
    ss_sold_date_sk           int,
    ss_sold_time_sk           int,
    ss_item_sk                int,
    ss_customer_sk            int,
    ss_cdemo_sk               int,
    ss_hdemo_sk               int,
    ss_addr_sk                int,
    ss_store_sk               int,
    ss_promo_sk               int,
    ss_ticket_number          int,
    ss_quantity               int,
    ss_wholesale_cost         float,
    ss_list_price             float,
    ss_sales_price            float,
    ss_ext_discount_amt       float,
    ss_ext_sales_price        float,
    ss_ext_wholesale_cost     float,
    ss_ext_list_price         float,
    ss_ext_tax                float,
    ss_coupon_amt             float,
    ss_net_paid               float,
    ss_net_paid_inc_tax       float,
    ss_net_profit             float
)
stored as orc
tblproperties ("orc.stripe.size"="33554432", "orc.compress.size"="16384");

set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table store_sales
select
ss_sold_date_sk           ,
    ss_sold_time_sk       ,
    ss_item_sk            ,
    ss_customer_sk        ,
    ss_cdemo_sk           ,
    ss_hdemo_sk           ,
    ss_addr_sk            ,
    ss_store_sk           ,
    ss_promo_sk           ,
    ss_ticket_number      ,
    ss_quantity           ,
    ss_wholesale_cost     ,
    ss_list_price         ,
    ss_sales_price        ,
    ss_ext_discount_amt   ,
    ss_ext_sales_price    ,
    ss_ext_wholesale_cost ,
    ss_ext_list_price     ,
    ss_ext_tax            ,
    ss_coupon_amt         ,
    ss_net_paid           ,
    ss_net_paid_inc_tax   ,
    ss_net_profit
 from store_sales_txt;

explain
select
  ss_ticket_number
from
  store_sales
group by ss_ticket_number
limit 20;

select
  ss_ticket_number
from
  store_sales
group by ss_ticket_number
limit 20;



explain
select
    min(ss_ticket_number) m
from
    (select
        ss_ticket_number
    from
        store_sales
    group by ss_ticket_number) a
group by ss_ticket_number
order by m;

select
    min(ss_ticket_number) m
from
    (select
        ss_ticket_number
    from
        store_sales
    group by ss_ticket_number) a
group by ss_ticket_number
order by m;



explain
select
    ss_ticket_number, sum(ss_item_sk), sum(q)
from
    (select
        ss_ticket_number, ss_item_sk, min(ss_quantity) q
    from
        store_sales
    group by ss_ticket_number, ss_item_sk) a
group by ss_ticket_number
order by ss_ticket_number;

select
    ss_ticket_number, sum(ss_item_sk), sum(q)
from
    (select
        ss_ticket_number, ss_item_sk, min(ss_quantity) q
    from
        store_sales
    group by ss_ticket_number, ss_item_sk) a
group by ss_ticket_number
order by ss_ticket_number;


explain
select
    ss_ticket_number, ss_item_sk, sum(q)
from
    (select
        ss_ticket_number, ss_item_sk, min(ss_quantity) q
    from
        store_sales
    group by ss_ticket_number, ss_item_sk) a
group by ss_ticket_number, ss_item_sk
order by ss_ticket_number, ss_item_sk;

select
    ss_ticket_number, ss_item_sk, sum(q)
from
    (select
        ss_ticket_number, ss_item_sk, min(ss_quantity) q
    from
        store_sales
    group by ss_ticket_number, ss_item_sk) a
group by ss_ticket_number, ss_item_sk
order by ss_ticket_number, ss_item_sk;

set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;

-- We are not expecting either query to vectorize due to use of pruneing grouping set id,
-- or use of GROUPING__ID virtual column.
create table store_txt
(
    s_store_sk                int,
    s_store_id                string,
    s_rec_start_date          string,
    s_rec_end_date            string,
    s_closed_date_sk          int,
    s_store_name              string,
    s_number_employees        int,
    s_floor_space             int,
    s_hours                   string,
    s_manager                 string,
    s_market_id               int,
    s_geography_class         string,
    s_market_desc             string,
    s_market_manager          string,
    s_division_id             int,
    s_division_name           string,
    s_company_id              int,
    s_company_name            string,
    s_street_number           string,
    s_street_name             string,
    s_street_type             string,
    s_suite_number            string,
    s_city                    string,
    s_county                  string,
    s_state                   string,
    s_zip                     string,
    s_country                 string,
    s_gmt_offset              decimal(5,2),
    s_tax_precentage          decimal(5,2)
)
row format delimited fields terminated by '|'
stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/store_200' OVERWRITE INTO TABLE store_txt;

create table store
stored as orc as
select * from store_txt;

explain
select s_store_id
 from store
 group by s_store_id with rollup;

select s_store_id
 from store
 group by s_store_id with rollup;

explain
select s_store_id, GROUPING__ID
 from store
 group by s_store_id with rollup;

select s_store_id, GROUPING__ID
 from store
 group by s_store_id with rollup;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

EXPLAIN
SELECT cboolean1, IF (cboolean1, 'first', 'second') FROM alltypesorc WHERE cboolean1 IS NOT NULL AND cboolean1 ORDER BY cboolean1;

SELECT cboolean1, IF (cboolean1, 'first', 'second') FROM alltypesorc WHERE cboolean1 IS NOT NULL AND cboolean1 ORDER BY cboolean1 LIMIT 5;

SELECT cboolean1, IF (cboolean1, 'first', 'second') FROM alltypesorc WHERE cboolean1 IS NOT NULL AND NOT cboolean1 ORDER BY cboolean1 LIMIT 5;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;

CREATE TABLE orc_table_1a(a INT) STORED AS ORC;
CREATE TABLE orc_table_2a(c INT) STORED AS ORC;

insert into table orc_table_1a values(1),(1), (2),(3);
insert into table orc_table_2a values(0),(2), (3),(null),(4);

explain
select t1.a from orc_table_2a t2 join orc_table_1a t1 on t1.a = t2.c where t1.a > 2;

select t1.a from orc_table_2a t2 join orc_table_1a t1 on t1.a = t2.c where t1.a > 2;

explain
select t2.c from orc_table_2a t2 left semi join orc_table_1a t1 on t1.a = t2.c where t2.c > 2;

select t2.c from orc_table_2a t2 left semi join orc_table_1a t1 on t1.a = t2.c where t2.c > 2;


CREATE TABLE orc_table_1b(v1 STRING, a INT) STORED AS ORC;
CREATE TABLE orc_table_2b(c INT, v2 STRING) STORED AS ORC;

insert into table orc_table_1b values("one", 1),("one", 1), ("two", 2),("three", 3);
insert into table orc_table_2b values(0, "ZERO"),(2, "TWO"), (3, "THREE"),(null, "<NULL>"),(4, "FOUR");

explain
select t1.v1, t1.a from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

select t1.v1, t1.a from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;


explain
select t1.v1, t1.a, t2.c, t2.v2 from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

select t1.v1, t1.a, t2.c, t2.v2 from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

explain
select t1.v1, t1.a*2, t2.c*5, t2.v2 from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

select t1.v1, t1.a*2, t2.c*5, t2.v2 from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

explain
select t1.v1, t2.v2, t2.c from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

select t1.v1, t2.v2, t2.c from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

explain
select t1.a, t1.v1, t2.v2 from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

select t1.a, t1.v1, t2.v2 from orc_table_2b t2 join orc_table_1b t1 on t1.a = t2.c where t1.a > 2;

explain
select t1.v1, t2.v2, t2.c from orc_table_1b t1 join orc_table_2b t2 on t1.a = t2.c where t1.a > 2;

select t1.v1, t2.v2, t2.c from orc_table_1b t1 join orc_table_2b t2 on t1.a = t2.c where t1.a > 2;

explain
select t1.a, t1.v1, t2.v2 from orc_table_1b t1 join orc_table_2b t2 on t1.a = t2.c where t1.a > 2;

select t1.a, t1.v1, t2.v2 from orc_table_1b t1 join orc_table_2b t2 on t1.a = t2.c where t1.a > 2;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;

set hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

drop table if exists vector_interval_1;
create table vector_interval_1 (ts timestamp, dt date, str1 string, str2 string) stored as orc;

insert into vector_interval_1
  select timestamp '2001-01-01 01:02:03', date '2001-01-01', '1-2', '1 2:3:4' from src limit 1;
insert into vector_interval_1
  select null, null, null, null from src limit 1;

-- constants/cast from string
explain
select
  str1,
  interval '1-2' year to month, interval_year_month(str1),
  interval '1 2:3:4' day to second, interval_day_time(str2)
from vector_interval_1 order by str1;

select
  str1,
  interval '1-2' year to month, interval_year_month(str1),
  interval '1 2:3:4' day to second, interval_day_time(str2)
from vector_interval_1 order by str1;


-- interval arithmetic
explain
select
  dt,
  interval '1-2' year to month + interval '1-2' year to month,
  interval_year_month(str1) + interval_year_month(str1),
  interval '1-2' year to month + interval_year_month(str1),
  interval '1-2' year to month - interval '1-2' year to month,
  interval_year_month(str1) - interval_year_month(str1),
  interval '1-2' year to month - interval_year_month(str1)
from vector_interval_1 order by dt;

select
  dt,
  interval '1-2' year to month + interval '1-2' year to month,
  interval_year_month(str1) + interval_year_month(str1),
  interval '1-2' year to month + interval_year_month(str1),
  interval '1-2' year to month - interval '1-2' year to month,
  interval_year_month(str1) - interval_year_month(str1),
  interval '1-2' year to month - interval_year_month(str1)
from vector_interval_1 order by dt;

explain
select
  dt,
  interval '1 2:3:4' day to second + interval '1 2:3:4' day to second,
  interval_day_time(str2) + interval_day_time(str2),
  interval '1 2:3:4' day to second + interval_day_time(str2),
  interval '1 2:3:4' day to second - interval '1 2:3:4' day to second,
  interval_day_time(str2) - interval_day_time(str2),
  interval '1 2:3:4' day to second - interval_day_time(str2)
from vector_interval_1 order by dt;

select
  dt,
  interval '1 2:3:4' day to second + interval '1 2:3:4' day to second,
  interval_day_time(str2) + interval_day_time(str2),
  interval '1 2:3:4' day to second + interval_day_time(str2),
  interval '1 2:3:4' day to second - interval '1 2:3:4' day to second,
  interval_day_time(str2) - interval_day_time(str2),
  interval '1 2:3:4' day to second - interval_day_time(str2)
from vector_interval_1 order by dt;


-- date-interval arithmetic
explain
select
  dt,
  dt + interval '1-2' year to month,
  dt + interval_year_month(str1),
  interval '1-2' year to month + dt,
  interval_year_month(str1) + dt,
  dt - interval '1-2' year to month,
  dt - interval_year_month(str1),
  dt + interval '1 2:3:4' day to second,
  dt + interval_day_time(str2),
  interval '1 2:3:4' day to second + dt,
  interval_day_time(str2) + dt,
  dt - interval '1 2:3:4' day to second,
  dt - interval_day_time(str2)
from vector_interval_1 order by dt;

select
  dt,
  dt + interval '1-2' year to month,
  dt + interval_year_month(str1),
  interval '1-2' year to month + dt,
  interval_year_month(str1) + dt,
  dt - interval '1-2' year to month,
  dt - interval_year_month(str1),
  dt + interval '1 2:3:4' day to second,
  dt + interval_day_time(str2),
  interval '1 2:3:4' day to second + dt,
  interval_day_time(str2) + dt,
  dt - interval '1 2:3:4' day to second,
  dt - interval_day_time(str2)
from vector_interval_1 order by dt;


-- timestamp-interval arithmetic
explain
select
  ts,
  ts + interval '1-2' year to month,
  ts + interval_year_month(str1),
  interval '1-2' year to month + ts,
  interval_year_month(str1) + ts,
  ts - interval '1-2' year to month,
  ts - interval_year_month(str1),
  ts + interval '1 2:3:4' day to second,
  ts + interval_day_time(str2),
  interval '1 2:3:4' day to second + ts,
  interval_day_time(str2) + ts,
  ts - interval '1 2:3:4' day to second,
  ts - interval_day_time(str2)
from vector_interval_1 order by ts;

select
  ts,
  ts + interval '1-2' year to month,
  ts + interval_year_month(str1),
  interval '1-2' year to month + ts,
  interval_year_month(str1) + ts,
  ts - interval '1-2' year to month,
  ts - interval_year_month(str1),
  ts + interval '1 2:3:4' day to second,
  ts + interval_day_time(str2),
  interval '1 2:3:4' day to second + ts,
  interval_day_time(str2) + ts,
  ts - interval '1 2:3:4' day to second,
  ts - interval_day_time(str2)
from vector_interval_1 order by ts;


-- timestamp-timestamp arithmetic
explain
select
  ts,
  ts - ts,
  timestamp '2001-01-01 01:02:03' - ts,
  ts - timestamp '2001-01-01 01:02:03'
from vector_interval_1 order by ts;

select
  ts,
  ts - ts,
  timestamp '2001-01-01 01:02:03' - ts,
  ts - timestamp '2001-01-01 01:02:03'
from vector_interval_1 order by ts;


-- date-date arithmetic
explain
select
  dt,
  dt - dt,
  date '2001-01-01' - dt,
  dt - date '2001-01-01'
from vector_interval_1 order by dt;

select
  dt,
  dt - dt,
  date '2001-01-01' - dt,
  dt - date '2001-01-01'
from vector_interval_1 order by dt;


-- date-timestamp arithmetic
explain
select
  dt,
  ts - dt,
  timestamp '2001-01-01 01:02:03' - dt,
  ts - date '2001-01-01',
  dt - ts,
  dt - timestamp '2001-01-01 01:02:03',
  date '2001-01-01' - ts
from vector_interval_1 order by dt;

select
  dt,
  ts - dt,
  timestamp '2001-01-01 01:02:03' - dt,
  ts - date '2001-01-01',
  dt - ts,
  dt - timestamp '2001-01-01 01:02:03',
  date '2001-01-01' - ts
from vector_interval_1 order by dt;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

drop table if exists vector_interval_2;
create table vector_interval_2 (ts timestamp, dt date, str1 string, str2 string, str3 string, str4 string) stored as orc;

insert into vector_interval_2
  select timestamp '2001-01-01 01:02:03', date '2001-01-01', '1-2', '1-3', '1 2:3:4', '1 2:3:5' from src limit 1;
insert into vector_interval_2
  select null, null, null, null, null, null from src limit 1;


-- interval comparisons in select clause

explain
select
  str1,
  -- Should all be true
  interval_year_month(str1) = interval_year_month(str1),
  interval_year_month(str1) <= interval_year_month(str1),
  interval_year_month(str1) <= interval_year_month(str2),
  interval_year_month(str1) < interval_year_month(str2),
  interval_year_month(str1) >= interval_year_month(str1),
  interval_year_month(str2) >= interval_year_month(str1),
  interval_year_month(str2) > interval_year_month(str1),
  interval_year_month(str1) != interval_year_month(str2),

  interval_year_month(str1) = interval '1-2' year to month,
  interval_year_month(str1) <= interval '1-2' year to month,
  interval_year_month(str1) <= interval '1-3' year to month,
  interval_year_month(str1) < interval '1-3' year to month,
  interval_year_month(str1) >= interval '1-2' year to month,
  interval_year_month(str2) >= interval '1-2' year to month,
  interval_year_month(str2) > interval '1-2' year to month,
  interval_year_month(str1) != interval '1-3' year to month,

  interval '1-2' year to month = interval_year_month(str1),
  interval '1-2' year to month <= interval_year_month(str1),
  interval '1-2' year to month <= interval_year_month(str2),
  interval '1-2' year to month < interval_year_month(str2),
  interval '1-2' year to month >= interval_year_month(str1),
  interval '1-3' year to month >= interval_year_month(str1),
  interval '1-3' year to month > interval_year_month(str1),
  interval '1-2' year to month != interval_year_month(str2)
from vector_interval_2 order by str1;

select
  str1,
  -- Should all be true
  interval_year_month(str1) = interval_year_month(str1),
  interval_year_month(str1) <= interval_year_month(str1),
  interval_year_month(str1) <= interval_year_month(str2),
  interval_year_month(str1) < interval_year_month(str2),
  interval_year_month(str1) >= interval_year_month(str1),
  interval_year_month(str2) >= interval_year_month(str1),
  interval_year_month(str2) > interval_year_month(str1),
  interval_year_month(str1) != interval_year_month(str2),

  interval_year_month(str1) = interval '1-2' year to month,
  interval_year_month(str1) <= interval '1-2' year to month,
  interval_year_month(str1) <= interval '1-3' year to month,
  interval_year_month(str1) < interval '1-3' year to month,
  interval_year_month(str1) >= interval '1-2' year to month,
  interval_year_month(str2) >= interval '1-2' year to month,
  interval_year_month(str2) > interval '1-2' year to month,
  interval_year_month(str1) != interval '1-3' year to month,

  interval '1-2' year to month = interval_year_month(str1),
  interval '1-2' year to month <= interval_year_month(str1),
  interval '1-2' year to month <= interval_year_month(str2),
  interval '1-2' year to month < interval_year_month(str2),
  interval '1-2' year to month >= interval_year_month(str1),
  interval '1-3' year to month >= interval_year_month(str1),
  interval '1-3' year to month > interval_year_month(str1),
  interval '1-2' year to month != interval_year_month(str2)
from vector_interval_2 order by str1;

explain
select
  str1,
  -- Should all be false
  interval_year_month(str1) != interval_year_month(str1),
  interval_year_month(str1) >= interval_year_month(str2),
  interval_year_month(str1) > interval_year_month(str2),
  interval_year_month(str2) <= interval_year_month(str1),
  interval_year_month(str2) < interval_year_month(str1),
  interval_year_month(str1) != interval_year_month(str1),

  interval_year_month(str1) != interval '1-2' year to month,
  interval_year_month(str1) >= interval '1-3' year to month,
  interval_year_month(str1) > interval '1-3' year to month,
  interval_year_month(str2) <= interval '1-2' year to month,
  interval_year_month(str2) < interval '1-2' year to month,
  interval_year_month(str1) != interval '1-2' year to month,

  interval '1-2' year to month != interval_year_month(str1),
  interval '1-2' year to month >= interval_year_month(str2),
  interval '1-2' year to month > interval_year_month(str2),
  interval '1-3' year to month <= interval_year_month(str1),
  interval '1-3' year to month < interval_year_month(str1),
  interval '1-2' year to month != interval_year_month(str1)
from vector_interval_2 order by str1;

select
  str1,
  -- Should all be false
  interval_year_month(str1) != interval_year_month(str1),
  interval_year_month(str1) >= interval_year_month(str2),
  interval_year_month(str1) > interval_year_month(str2),
  interval_year_month(str2) <= interval_year_month(str1),
  interval_year_month(str2) < interval_year_month(str1),
  interval_year_month(str1) != interval_year_month(str1),

  interval_year_month(str1) != interval '1-2' year to month,
  interval_year_month(str1) >= interval '1-3' year to month,
  interval_year_month(str1) > interval '1-3' year to month,
  interval_year_month(str2) <= interval '1-2' year to month,
  interval_year_month(str2) < interval '1-2' year to month,
  interval_year_month(str1) != interval '1-2' year to month,

  interval '1-2' year to month != interval_year_month(str1),
  interval '1-2' year to month >= interval_year_month(str2),
  interval '1-2' year to month > interval_year_month(str2),
  interval '1-3' year to month <= interval_year_month(str1),
  interval '1-3' year to month < interval_year_month(str1),
  interval '1-2' year to month != interval_year_month(str1)
from vector_interval_2 order by str1;

explain
select
  str3,
  -- Should all be true
  interval_day_time(str3) = interval_day_time(str3),
  interval_day_time(str3) <= interval_day_time(str3),
  interval_day_time(str3) <= interval_day_time(str4),
  interval_day_time(str3) < interval_day_time(str4),
  interval_day_time(str3) >= interval_day_time(str3),
  interval_day_time(str4) >= interval_day_time(str3),
  interval_day_time(str4) > interval_day_time(str3),
  interval_day_time(str3) != interval_day_time(str4),

  interval_day_time(str3) = interval '1 2:3:4' day to second,
  interval_day_time(str3) <= interval '1 2:3:4' day to second,
  interval_day_time(str3) <= interval '1 2:3:5' day to second,
  interval_day_time(str3) < interval '1 2:3:5' day to second,
  interval_day_time(str3) >= interval '1 2:3:4' day to second,
  interval_day_time(str4) >= interval '1 2:3:4' day to second,
  interval_day_time(str4) > interval '1 2:3:4' day to second,
  interval_day_time(str3) != interval '1 2:3:5' day to second,

  interval '1 2:3:4' day to second = interval_day_time(str3),
  interval '1 2:3:4' day to second <= interval_day_time(str3),
  interval '1 2:3:4' day to second <= interval_day_time(str4),
  interval '1 2:3:4' day to second < interval_day_time(str4),
  interval '1 2:3:4' day to second >= interval_day_time(str3),
  interval '1 2:3:5' day to second >= interval_day_time(str3),
  interval '1 2:3:5' day to second > interval_day_time(str3),
  interval '1 2:3:4' day to second != interval_day_time(str4)
from vector_interval_2 order by str3;

select
  str3,
  -- Should all be true
  interval_day_time(str3) = interval_day_time(str3),
  interval_day_time(str3) <= interval_day_time(str3),
  interval_day_time(str3) <= interval_day_time(str4),
  interval_day_time(str3) < interval_day_time(str4),
  interval_day_time(str3) >= interval_day_time(str3),
  interval_day_time(str4) >= interval_day_time(str3),
  interval_day_time(str4) > interval_day_time(str3),
  interval_day_time(str3) != interval_day_time(str4),

  interval_day_time(str3) = interval '1 2:3:4' day to second,
  interval_day_time(str3) <= interval '1 2:3:4' day to second,
  interval_day_time(str3) <= interval '1 2:3:5' day to second,
  interval_day_time(str3) < interval '1 2:3:5' day to second,
  interval_day_time(str3) >= interval '1 2:3:4' day to second,
  interval_day_time(str4) >= interval '1 2:3:4' day to second,
  interval_day_time(str4) > interval '1 2:3:4' day to second,
  interval_day_time(str3) != interval '1 2:3:5' day to second,

  interval '1 2:3:4' day to second = interval_day_time(str3),
  interval '1 2:3:4' day to second <= interval_day_time(str3),
  interval '1 2:3:4' day to second <= interval_day_time(str4),
  interval '1 2:3:4' day to second < interval_day_time(str4),
  interval '1 2:3:4' day to second >= interval_day_time(str3),
  interval '1 2:3:5' day to second >= interval_day_time(str3),
  interval '1 2:3:5' day to second > interval_day_time(str3),
  interval '1 2:3:4' day to second != interval_day_time(str4)
from vector_interval_2 order by str3;

explain
select
  str3,
  -- Should all be false
  interval_day_time(str3) != interval_day_time(str3),
  interval_day_time(str3) >= interval_day_time(str4),
  interval_day_time(str3) > interval_day_time(str4),
  interval_day_time(str4) <= interval_day_time(str3),
  interval_day_time(str4) < interval_day_time(str3),
  interval_day_time(str3) != interval_day_time(str3),

  interval_day_time(str3) != interval '1 2:3:4' day to second,
  interval_day_time(str3) >= interval '1 2:3:5' day to second,
  interval_day_time(str3) > interval '1 2:3:5' day to second,
  interval_day_time(str4) <= interval '1 2:3:4' day to second,
  interval_day_time(str4) < interval '1 2:3:4' day to second,
  interval_day_time(str3) != interval '1 2:3:4' day to second,

  interval '1 2:3:4' day to second != interval_day_time(str3),
  interval '1 2:3:4' day to second >= interval_day_time(str4),
  interval '1 2:3:4' day to second > interval_day_time(str4),
  interval '1 2:3:5' day to second <= interval_day_time(str3),
  interval '1 2:3:5' day to second < interval_day_time(str3),
  interval '1 2:3:4' day to second != interval_day_time(str3)
from vector_interval_2 order by str3;

select
  str3,
  -- Should all be false
  interval_day_time(str3) != interval_day_time(str3),
  interval_day_time(str3) >= interval_day_time(str4),
  interval_day_time(str3) > interval_day_time(str4),
  interval_day_time(str4) <= interval_day_time(str3),
  interval_day_time(str4) < interval_day_time(str3),
  interval_day_time(str3) != interval_day_time(str3),

  interval_day_time(str3) != interval '1 2:3:4' day to second,
  interval_day_time(str3) >= interval '1 2:3:5' day to second,
  interval_day_time(str3) > interval '1 2:3:5' day to second,
  interval_day_time(str4) <= interval '1 2:3:4' day to second,
  interval_day_time(str4) < interval '1 2:3:4' day to second,
  interval_day_time(str3) != interval '1 2:3:4' day to second,

  interval '1 2:3:4' day to second != interval_day_time(str3),
  interval '1 2:3:4' day to second >= interval_day_time(str4),
  interval '1 2:3:4' day to second > interval_day_time(str4),
  interval '1 2:3:5' day to second <= interval_day_time(str3),
  interval '1 2:3:5' day to second < interval_day_time(str3),
  interval '1 2:3:4' day to second != interval_day_time(str3)
from vector_interval_2 order by str3;


-- interval expressions in predicates
explain
select ts from vector_interval_2
where
  interval_year_month(str1) = interval_year_month(str1)
  and interval_year_month(str1) != interval_year_month(str2)
  and interval_year_month(str1) <= interval_year_month(str2)
  and interval_year_month(str1) < interval_year_month(str2)
  and interval_year_month(str2) >= interval_year_month(str1)
  and interval_year_month(str2) > interval_year_month(str1)

  and interval_year_month(str1) = interval '1-2' year to month
  and interval_year_month(str1) != interval '1-3' year to month
  and interval_year_month(str1) <= interval '1-3' year to month
  and interval_year_month(str1) < interval '1-3' year to month
  and interval_year_month(str2) >= interval '1-2' year to month
  and interval_year_month(str2) > interval '1-2' year to month

  and interval '1-2' year to month = interval_year_month(str1)
  and interval '1-2' year to month != interval_year_month(str2)
  and interval '1-2' year to month <= interval_year_month(str2)
  and interval '1-2' year to month < interval_year_month(str2)
  and interval '1-3' year to month >= interval_year_month(str1)
  and interval '1-3' year to month > interval_year_month(str1)
order by ts;

select ts from vector_interval_2
where
  interval_year_month(str1) = interval_year_month(str1)
  and interval_year_month(str1) != interval_year_month(str2)
  and interval_year_month(str1) <= interval_year_month(str2)
  and interval_year_month(str1) < interval_year_month(str2)
  and interval_year_month(str2) >= interval_year_month(str1)
  and interval_year_month(str2) > interval_year_month(str1)

  and interval_year_month(str1) = interval '1-2' year to month
  and interval_year_month(str1) != interval '1-3' year to month
  and interval_year_month(str1) <= interval '1-3' year to month
  and interval_year_month(str1) < interval '1-3' year to month
  and interval_year_month(str2) >= interval '1-2' year to month
  and interval_year_month(str2) > interval '1-2' year to month

  and interval '1-2' year to month = interval_year_month(str1)
  and interval '1-2' year to month != interval_year_month(str2)
  and interval '1-2' year to month <= interval_year_month(str2)
  and interval '1-2' year to month < interval_year_month(str2)
  and interval '1-3' year to month >= interval_year_month(str1)
  and interval '1-3' year to month > interval_year_month(str1)
order by ts;

explain
select ts from vector_interval_2
where
  interval_day_time(str3) = interval_day_time(str3)
  and interval_day_time(str3) != interval_day_time(str4)
  and interval_day_time(str3) <= interval_day_time(str4)
  and interval_day_time(str3) < interval_day_time(str4)
  and interval_day_time(str4) >= interval_day_time(str3)
  and interval_day_time(str4) > interval_day_time(str3)

  and interval_day_time(str3) = interval '1 2:3:4' day to second
  and interval_day_time(str3) != interval '1 2:3:5' day to second
  and interval_day_time(str3) <= interval '1 2:3:5' day to second
  and interval_day_time(str3) < interval '1 2:3:5' day to second
  and interval_day_time(str4) >= interval '1 2:3:4' day to second
  and interval_day_time(str4) > interval '1 2:3:4' day to second

  and interval '1 2:3:4' day to second = interval_day_time(str3)
  and interval '1 2:3:4' day to second != interval_day_time(str4)
  and interval '1 2:3:4' day to second <= interval_day_time(str4)
  and interval '1 2:3:4' day to second < interval_day_time(str4)
  and interval '1 2:3:5' day to second >= interval_day_time(str3)
  and interval '1 2:3:5' day to second > interval_day_time(str3)
order by ts;

select ts from vector_interval_2
where
  interval_day_time(str3) = interval_day_time(str3)
  and interval_day_time(str3) != interval_day_time(str4)
  and interval_day_time(str3) <= interval_day_time(str4)
  and interval_day_time(str3) < interval_day_time(str4)
  and interval_day_time(str4) >= interval_day_time(str3)
  and interval_day_time(str4) > interval_day_time(str3)

  and interval_day_time(str3) = interval '1 2:3:4' day to second
  and interval_day_time(str3) != interval '1 2:3:5' day to second
  and interval_day_time(str3) <= interval '1 2:3:5' day to second
  and interval_day_time(str3) < interval '1 2:3:5' day to second
  and interval_day_time(str4) >= interval '1 2:3:4' day to second
  and interval_day_time(str4) > interval '1 2:3:4' day to second

  and interval '1 2:3:4' day to second = interval_day_time(str3)
  and interval '1 2:3:4' day to second != interval_day_time(str4)
  and interval '1 2:3:4' day to second <= interval_day_time(str4)
  and interval '1 2:3:4' day to second < interval_day_time(str4)
  and interval '1 2:3:5' day to second >= interval_day_time(str3)
  and interval '1 2:3:5' day to second > interval_day_time(str3)
order by ts;

explain
select ts from vector_interval_2
where
  date '2002-03-01' = dt + interval_year_month(str1)
  and date '2002-03-01' <= dt + interval_year_month(str1)
  and date '2002-03-01' >= dt + interval_year_month(str1)
  and dt + interval_year_month(str1) = date '2002-03-01'
  and dt + interval_year_month(str1) <= date '2002-03-01'
  and dt + interval_year_month(str1) >= date '2002-03-01'
  and dt != dt + interval_year_month(str1)

  and date '2002-03-01' = dt + interval '1-2' year to month
  and date '2002-03-01' <= dt + interval '1-2' year to month
  and date '2002-03-01' >= dt + interval '1-2' year to month
  and dt + interval '1-2' year to month = date '2002-03-01'
  and dt + interval '1-2' year to month <= date '2002-03-01'
  and dt + interval '1-2' year to month >= date '2002-03-01'
  and dt != dt + interval '1-2' year to month
order by ts;

select ts from vector_interval_2
where
  date '2002-03-01' = dt + interval_year_month(str1)
  and date '2002-03-01' <= dt + interval_year_month(str1)
  and date '2002-03-01' >= dt + interval_year_month(str1)
  and dt + interval_year_month(str1) = date '2002-03-01'
  and dt + interval_year_month(str1) <= date '2002-03-01'
  and dt + interval_year_month(str1) >= date '2002-03-01'
  and dt != dt + interval_year_month(str1)

  and date '2002-03-01' = dt + interval '1-2' year to month
  and date '2002-03-01' <= dt + interval '1-2' year to month
  and date '2002-03-01' >= dt + interval '1-2' year to month
  and dt + interval '1-2' year to month = date '2002-03-01'
  and dt + interval '1-2' year to month <= date '2002-03-01'
  and dt + interval '1-2' year to month >= date '2002-03-01'
  and dt != dt + interval '1-2' year to month
order by ts;

explain
select ts from vector_interval_2
where
  timestamp '2002-03-01 01:02:03' = ts + interval '1-2' year to month
  and timestamp '2002-03-01 01:02:03' <= ts + interval '1-2' year to month
  and timestamp '2002-03-01 01:02:03' >= ts + interval '1-2' year to month
  and timestamp '2002-04-01 01:02:03' != ts + interval '1-2' year to month
  and timestamp '2002-02-01 01:02:03' < ts + interval '1-2' year to month
  and timestamp '2002-04-01 01:02:03' > ts + interval '1-2' year to month

  and ts + interval '1-2' year to month = timestamp '2002-03-01 01:02:03'
  and ts + interval '1-2' year to month >= timestamp '2002-03-01 01:02:03'
  and ts + interval '1-2' year to month <= timestamp '2002-03-01 01:02:03'
  and ts + interval '1-2' year to month != timestamp '2002-04-01 01:02:03'
  and ts + interval '1-2' year to month > timestamp '2002-02-01 01:02:03'
  and ts + interval '1-2' year to month < timestamp '2002-04-01 01:02:03'

  and ts = ts + interval '0' year
  and ts != ts + interval '1' year
  and ts <= ts + interval '1' year
  and ts < ts + interval '1' year
  and ts >= ts - interval '1' year
  and ts > ts - interval '1' year
order by ts;

select ts from vector_interval_2
where
  timestamp '2002-03-01 01:02:03' = ts + interval '1-2' year to month
  and timestamp '2002-03-01 01:02:03' <= ts + interval '1-2' year to month
  and timestamp '2002-03-01 01:02:03' >= ts + interval '1-2' year to month
  and timestamp '2002-04-01 01:02:03' != ts + interval '1-2' year to month
  and timestamp '2002-02-01 01:02:03' < ts + interval '1-2' year to month
  and timestamp '2002-04-01 01:02:03' > ts + interval '1-2' year to month

  and ts + interval '1-2' year to month = timestamp '2002-03-01 01:02:03'
  and ts + interval '1-2' year to month >= timestamp '2002-03-01 01:02:03'
  and ts + interval '1-2' year to month <= timestamp '2002-03-01 01:02:03'
  and ts + interval '1-2' year to month != timestamp '2002-04-01 01:02:03'
  and ts + interval '1-2' year to month > timestamp '2002-02-01 01:02:03'
  and ts + interval '1-2' year to month < timestamp '2002-04-01 01:02:03'

  and ts = ts + interval '0' year
  and ts != ts + interval '1' year
  and ts <= ts + interval '1' year
  and ts < ts + interval '1' year
  and ts >= ts - interval '1' year
  and ts > ts - interval '1' year
order by ts;

-- day to second expressions in predicate
explain
select ts from vector_interval_2
where
  timestamp '2001-01-01 01:02:03' = dt + interval '0 1:2:3' day to second
  and timestamp '2001-01-01 01:02:03' != dt + interval '0 1:2:4' day to second
  and timestamp '2001-01-01 01:02:03' <= dt + interval '0 1:2:3' day to second
  and timestamp '2001-01-01 01:02:03' < dt + interval '0 1:2:4' day to second
  and timestamp '2001-01-01 01:02:03' >= dt - interval '0 1:2:3' day to second
  and timestamp '2001-01-01 01:02:03' > dt - interval '0 1:2:4' day to second

  and dt + interval '0 1:2:3' day to second = timestamp '2001-01-01 01:02:03'
  and dt + interval '0 1:2:4' day to second != timestamp '2001-01-01 01:02:03'
  and dt + interval '0 1:2:3' day to second >= timestamp '2001-01-01 01:02:03'
  and dt + interval '0 1:2:4' day to second > timestamp '2001-01-01 01:02:03'
  and dt - interval '0 1:2:3' day to second <= timestamp '2001-01-01 01:02:03'
  and dt - interval '0 1:2:4' day to second < timestamp '2001-01-01 01:02:03'

  and ts = dt + interval '0 1:2:3' day to second
  and ts != dt + interval '0 1:2:4' day to second
  and ts <= dt + interval '0 1:2:3' day to second
  and ts < dt + interval '0 1:2:4' day to second
  and ts >= dt - interval '0 1:2:3' day to second
  and ts > dt - interval '0 1:2:4' day to second
order by ts;

select ts from vector_interval_2
where
  timestamp '2001-01-01 01:02:03' = dt + interval '0 1:2:3' day to second
  and timestamp '2001-01-01 01:02:03' != dt + interval '0 1:2:4' day to second
  and timestamp '2001-01-01 01:02:03' <= dt + interval '0 1:2:3' day to second
  and timestamp '2001-01-01 01:02:03' < dt + interval '0 1:2:4' day to second
  and timestamp '2001-01-01 01:02:03' >= dt - interval '0 1:2:3' day to second
  and timestamp '2001-01-01 01:02:03' > dt - interval '0 1:2:4' day to second

  and dt + interval '0 1:2:3' day to second = timestamp '2001-01-01 01:02:03'
  and dt + interval '0 1:2:4' day to second != timestamp '2001-01-01 01:02:03'
  and dt + interval '0 1:2:3' day to second >= timestamp '2001-01-01 01:02:03'
  and dt + interval '0 1:2:4' day to second > timestamp '2001-01-01 01:02:03'
  and dt - interval '0 1:2:3' day to second <= timestamp '2001-01-01 01:02:03'
  and dt - interval '0 1:2:4' day to second < timestamp '2001-01-01 01:02:03'

  and ts = dt + interval '0 1:2:3' day to second
  and ts != dt + interval '0 1:2:4' day to second
  and ts <= dt + interval '0 1:2:3' day to second
  and ts < dt + interval '0 1:2:4' day to second
  and ts >= dt - interval '0 1:2:3' day to second
  and ts > dt - interval '0 1:2:4' day to second
order by ts;

explain
select ts from vector_interval_2
where
  timestamp '2001-01-01 01:02:03' = ts + interval '0' day
  and timestamp '2001-01-01 01:02:03' != ts + interval '1' day
  and timestamp '2001-01-01 01:02:03' <= ts + interval '1' day
  and timestamp '2001-01-01 01:02:03' < ts + interval '1' day
  and timestamp '2001-01-01 01:02:03' >= ts - interval '1' day
  and timestamp '2001-01-01 01:02:03' > ts - interval '1' day

  and ts + interval '0' day = timestamp '2001-01-01 01:02:03'
  and ts + interval '1' day != timestamp '2001-01-01 01:02:03'
  and ts + interval '1' day >= timestamp '2001-01-01 01:02:03'
  and ts + interval '1' day > timestamp '2001-01-01 01:02:03'
  and ts - interval '1' day <= timestamp '2001-01-01 01:02:03'
  and ts - interval '1' day < timestamp '2001-01-01 01:02:03'

  and ts = ts + interval '0' day
  and ts != ts + interval '1' day
  and ts <= ts + interval '1' day
  and ts < ts + interval '1' day
  and ts >= ts - interval '1' day
  and ts > ts - interval '1' day
order by ts;

select ts from vector_interval_2
where
  timestamp '2001-01-01 01:02:03' = ts + interval '0' day
  and timestamp '2001-01-01 01:02:03' != ts + interval '1' day
  and timestamp '2001-01-01 01:02:03' <= ts + interval '1' day
  and timestamp '2001-01-01 01:02:03' < ts + interval '1' day
  and timestamp '2001-01-01 01:02:03' >= ts - interval '1' day
  and timestamp '2001-01-01 01:02:03' > ts - interval '1' day

  and ts + interval '0' day = timestamp '2001-01-01 01:02:03'
  and ts + interval '1' day != timestamp '2001-01-01 01:02:03'
  and ts + interval '1' day >= timestamp '2001-01-01 01:02:03'
  and ts + interval '1' day > timestamp '2001-01-01 01:02:03'
  and ts - interval '1' day <= timestamp '2001-01-01 01:02:03'
  and ts - interval '1' day < timestamp '2001-01-01 01:02:03'

  and ts = ts + interval '0' day
  and ts != ts + interval '1' day
  and ts <= ts + interval '1' day
  and ts < ts + interval '1' day
  and ts >= ts - interval '1' day
  and ts > ts - interval '1' day
order by ts;

drop table vector_interval_2;
set hive.cli.print.header=true;
set hive.explain.user=false;
set hive.fetch.task.conversion=none;

create table unique_timestamps (tsval timestamp) STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/timestamps.txt' OVERWRITE INTO TABLE unique_timestamps;

create table interval_arithmetic_1 (dateval date, tsval timestamp) stored as orc;
insert overwrite table interval_arithmetic_1
  select cast(tsval as date), tsval from unique_timestamps;

SET hive.vectorized.execution.enabled=true;

-- interval year-month arithmetic
explain
select
  dateval,
  dateval - interval '2-2' year to month,
  dateval - interval '-2-2' year to month,
  dateval + interval '2-2' year to month,
  dateval + interval '-2-2' year to month,
  - interval '2-2' year to month + dateval,
  interval '2-2' year to month + dateval
from interval_arithmetic_1
order by dateval;

select
  dateval,
  dateval - interval '2-2' year to month,
  dateval - interval '-2-2' year to month,
  dateval + interval '2-2' year to month,
  dateval + interval '-2-2' year to month,
  - interval '2-2' year to month + dateval,
  interval '2-2' year to month + dateval
from interval_arithmetic_1
order by dateval;

explain
select
  dateval,
  dateval - date '1999-06-07',
  date '1999-06-07' - dateval,
  dateval - dateval
from interval_arithmetic_1
order by dateval;

select
  dateval,
  dateval - date '1999-06-07',
  date '1999-06-07' - dateval,
  dateval - dateval
from interval_arithmetic_1
order by dateval;

explain
select
  tsval,
  tsval - interval '2-2' year to month,
  tsval - interval '-2-2' year to month,
  tsval + interval '2-2' year to month,
  tsval + interval '-2-2' year to month,
  - interval '2-2' year to month + tsval,
  interval '2-2' year to month + tsval
from interval_arithmetic_1
order by tsval;

select
  tsval,
  tsval - interval '2-2' year to month,
  tsval - interval '-2-2' year to month,
  tsval + interval '2-2' year to month,
  tsval + interval '-2-2' year to month,
  - interval '2-2' year to month + tsval,
  interval '2-2' year to month + tsval
from interval_arithmetic_1
order by tsval;

explain
select
  interval '2-2' year to month + interval '3-3' year to month,
  interval '2-2' year to month - interval '3-3' year to month
from interval_arithmetic_1
order by interval '2-2' year to month + interval '3-3' year to month
limit 2;

select
  interval '2-2' year to month + interval '3-3' year to month,
  interval '2-2' year to month - interval '3-3' year to month
from interval_arithmetic_1
order by interval '2-2' year to month + interval '3-3' year to month
limit 2;


-- interval day-time arithmetic
explain
select
  dateval,
  dateval - interval '99 11:22:33.123456789' day to second,
  dateval - interval '-99 11:22:33.123456789' day to second,
  dateval + interval '99 11:22:33.123456789' day to second,
  dateval + interval '-99 11:22:33.123456789' day to second,
  -interval '99 11:22:33.123456789' day to second + dateval,
  interval '99 11:22:33.123456789' day to second + dateval
from interval_arithmetic_1
order by dateval;

select
  dateval,
  dateval - interval '99 11:22:33.123456789' day to second,
  dateval - interval '-99 11:22:33.123456789' day to second,
  dateval + interval '99 11:22:33.123456789' day to second,
  dateval + interval '-99 11:22:33.123456789' day to second,
  -interval '99 11:22:33.123456789' day to second + dateval,
  interval '99 11:22:33.123456789' day to second + dateval
from interval_arithmetic_1
order by dateval;

explain
select
  dateval,
  tsval,
  dateval - tsval,
  tsval - dateval,
  tsval - tsval
from interval_arithmetic_1
order by dateval;

select
  dateval,
  tsval,
  dateval - tsval,
  tsval - dateval,
  tsval - tsval
from interval_arithmetic_1
order by dateval;

explain
select
  tsval,
  tsval - interval '99 11:22:33.123456789' day to second,
  tsval - interval '-99 11:22:33.123456789' day to second,
  tsval + interval '99 11:22:33.123456789' day to second,
  tsval + interval '-99 11:22:33.123456789' day to second,
  -interval '99 11:22:33.123456789' day to second + tsval,
  interval '99 11:22:33.123456789' day to second + tsval
from interval_arithmetic_1
order by tsval;

select
  tsval,
  tsval - interval '99 11:22:33.123456789' day to second,
  tsval - interval '-99 11:22:33.123456789' day to second,
  tsval + interval '99 11:22:33.123456789' day to second,
  tsval + interval '-99 11:22:33.123456789' day to second,
  -interval '99 11:22:33.123456789' day to second + tsval,
  interval '99 11:22:33.123456789' day to second + tsval
from interval_arithmetic_1
order by tsval;

explain
select
  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
from interval_arithmetic_1
limit 2;

select
  interval '99 11:22:33.123456789' day to second + interval '10 9:8:7.123456789' day to second,
  interval '99 11:22:33.123456789' day to second - interval '10 9:8:7.123456789' day to second
from interval_arithmetic_1
limit 2;

drop table interval_arithmetic_1;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;

create table vectortab_a_1k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab_a_1k' OVERWRITE INTO TABLE vectortab_a_1k;

CREATE TABLE vectortab_a_1korc STORED AS ORC AS SELECT * FROM vectortab_a_1k;

create table vectortab_b_1k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab_b_1k' OVERWRITE INTO TABLE vectortab_b_1k;

CREATE TABLE vectortab_b_1korc STORED AS ORC AS SELECT * FROM vectortab_b_1k;

explain
select
   v1.s,
   v2.s,
   v1.intrvl1
from
   ( select
      s,
      (cast(dt as date) - cast(ts as date)) as intrvl1
   from
      vectortab_a_1korc ) v1
join
   (
      select
         s ,
         (cast(dt as date) - cast(ts as date)) as intrvl2
      from
         vectortab_b_1korc
   ) v2
      on v1.intrvl1 = v2.intrvl2
      and v1.s = v2.s;

select
   v1.s,
   v2.s,
   v1.intrvl1
from
   ( select
      s,
      (cast(dt as date) - cast(ts as date)) as intrvl1
   from
      vectortab_a_1korc ) v1
join
   (
      select
         s ,
         (cast(dt as date) - cast(ts as date)) as intrvl2
      from
         vectortab_b_1korc
   ) v2
      on v1.intrvl1 = v2.intrvl2
      and v1.s = v2.s;set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;
set hive.fetch.task.conversion=none;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;

-- SORT_QUERY_RESULTS

CREATE TABLE orcsrc STORED AS ORC AS SELECT * FROM src;

explain
FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
RIGHT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
RIGHT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
RIGHT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
LEFT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
RIGHT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

explain
FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
RIGHT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
RIGHT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));

FROM
(SELECT orcsrc.* FROM orcsrc sort by key) x
RIGHT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Y
ON (x.key = Y.key)
RIGHT OUTER JOIN
(SELECT orcsrc.* FROM orcsrc sort by value) Z
ON (x.key = Z.key)
select sum(hash(Y.key,Y.value));
set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;
set hive.fetch.task.conversion=none;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;

-- SORT_QUERY_RESULTS

CREATE TABLE myinput1_txt(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in3.txt' INTO TABLE myinput1_txt;
CREATE TABLE myinput1 STORED AS ORC AS SELECT * FROM myinput1_txt;

SELECT sum(hash(a.key,a.value,b.key,b.value))  FROM myinput1 a JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value))  FROM myinput1 a LEFT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value))  FROM myinput1 a RIGHT OUTER JOIN myinput1 b on a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key=b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key and a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key=b.key and a.value = b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.value=c.value AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.value = c.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) RIGHT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value) LEFT OUTER JOIN myinput1 c ON (b.key=c.key AND c.key > 40 AND c.value > 50 AND c.key = c.value AND b.key > 40 AND b.value > 50 AND b.key = b.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.key = c.key AND a.key > 40 AND a.value > 50 AND a.key = a.value AND b.key > 40 AND b.value > 50 AND b.key = b.value AND c.key > 40 AND c.value > 50 AND c.key = c.value;set hive.mapred.mode=nonstrict;
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;
set hive.fetch.task.conversion=none;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;

-- SORT_QUERY_RESULTS

CREATE TABLE myinput1_txt(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in1.txt' INTO TABLE myinput1_txt;
CREATE TABLE myinput1 STORED AS ORC AS SELECT * FROM myinput1_txt;

SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.key = b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a JOIN myinput1 b ON a.value = b.value and a.key=b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.value = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key = b.key and a.value=b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key = b.key;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.value = b.value;
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key=b.key and a.value = b.value;

SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a LEFT OUTER JOIN myinput1 b ON (a.value=b.value) RIGHT OUTER JOIN myinput1 c ON (b.value=c.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) from myinput1 a RIGHT OUTER JOIN myinput1 b ON (a.value=b.value) LEFT OUTER JOIN myinput1 c ON (b.value=c.value);
SELECT sum(hash(a.key,a.value,b.key,b.value)) FROM myinput1 a LEFT OUTER JOIN myinput1 b RIGHT OUTER JOIN myinput1 c ON a.value = b.value and b.value = c.value;

set hive.mapred.mode=nonstrict;
set hive.support.concurrency=true;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
set hive.vectorized.execution.enabled=true;

drop table if exists char_part_tbl1 ;
drop table if exists char_part_tbl2;

create table studenttab(name string, age int, gpa double) clustered by (age) into 2 buckets stored as orc tblproperties('transactional'='true');
insert into table studenttab values ('calvin garcia',56,2.50), ('oscar miller',66,3.00), ('(yuri xylophone',30,2.74),('alice underhill',46,3.50);

create table char_tbl1(name string, age int) partitioned  by(gpa char(50)) stored as orc;
create table char_tbl2(name string, age int) partitioned by(gpa char(5)) stored as orc;

insert into table char_tbl1 partition(gpa='3.5') select name, age from studenttab where gpa = 3.5;
insert into table char_tbl1 partition(gpa='2.5') select name, age from studenttab where gpa = 2.5;
insert into table char_tbl2 partition(gpa='3.5') select name, age from studenttab where gpa = 3.5;
insert into table char_tbl2 partition(gpa='3') select name, age from studenttab where gpa = 3;

show partitions char_tbl1;
show partitions char_tbl2;

explain select c1.name, c1.age, c1.gpa, c2.name, c2.age, c2.gpa from char_tbl1 c1 join char_tbl2 c2 on (c1.gpa = c2.gpa);
select c1.name, c1.age, c1.gpa, c2.name, c2.age, c2.gpa from char_tbl1 c1 join char_tbl2 c2 on (c1.gpa = c2.gpa);

set hive.vectorized.execution.enabled=false;
select c1.name, c1.age, c1.gpa, c2.name, c2.age, c2.gpa from char_tbl1 c1 join char_tbl2 c2 on (c1.gpa = c2.gpa);
set hive.explain.user=false;
set hive.fetch.task.conversion=none;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- SORT_QUERY_RESULTS

create table t1 stored as orc as select cast(key as int) key, value from src where key <= 10;

select * from t1 sort by key;

create table t2 stored as orc as select cast(2*key as int) key, value from t1;

select * from t2 sort by key;

create table t3 stored as orc as select * from (select * from t1 union all select * from t2) b;
select * from t3 sort by key, value;

analyze table t3 compute statistics;

create table t4 (key int, value string) stored as orc;
select * from t4;


set hive.vectorized.execution.enabled=false;
set hive.mapjoin.hybridgrace.hashtable=false;

explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;

explain select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;
select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;

explain select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;
select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;

explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;
select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;

explain select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;

explain select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;
select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;

explain select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;
select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;

explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;
select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;

explain select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;
select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;

explain select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;

explain select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;
select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;

explain select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;
select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;

explain select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;
select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;

set hive.vectorized.execution.enabled=false;
set hive.mapjoin.hybridgrace.hashtable=true;

explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;

explain select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;
select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;

explain select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;
select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;

explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;
select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;

explain select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;

explain select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;
select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;

explain select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;
select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;

explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;
select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;

explain select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;
select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;

explain select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;

explain select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;
select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;

explain select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;
select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;

explain select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;
select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;

set hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=false;
SET hive.vectorized.execution.mapjoin.native.enabled=false;

explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;

explain select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;
select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;

explain select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;
select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;

explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;
select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;

explain select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;

explain select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;
select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;

explain select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;
select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;

explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;
select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;

explain select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;
select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;

explain select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;

explain select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;
select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;

explain select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;
select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;

explain select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;
select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;

set hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=true;
SET hive.vectorized.execution.mapjoin.native.enabled=false;

explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;

explain select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;
select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;

explain select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;
select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;

explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;
select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;

explain select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;

explain select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;
select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;

explain select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;
select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;

explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;
select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;

explain select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;
select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;

explain select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;

explain select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;
select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;

explain select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;
select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;

explain select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;
select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;

set hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=false;
SET hive.vectorized.execution.mapjoin.native.enabled=true;

explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;

explain select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;
select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;

explain select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;
select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;

explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;
select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;

explain select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;

explain select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;
select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;

explain select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;
select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;

explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;
select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;

explain select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;
select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;

explain select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;

explain select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;
select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;

explain select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;
select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;

explain select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;
select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;

set hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;

explain select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key=b.key sort by a.key, a.value;

explain select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;
select * from t2 a left semi join t1 b on b.key=a.key sort by a.key, a.value;

explain select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;
select * from t1 a left semi join t4 b on b.key=a.key sort by a.key, a.value;

explain select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;
select a.value from t1 a left semi join t3 b on (b.key = a.key and b.key < '15') sort by a.value;

explain select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = b.key and b.value < "val_10" sort by a.key, a.value;

explain select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;
select a.value from t1 a left semi join (select key from t3 where key > 5) b on a.key = b.key sort by a.value;

explain select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;
select a.value from t1 a left semi join (select key , value from t2 where key > 5) b on a.key = b.key and b.value <= 'val_20' sort by a.value ;

explain select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;
select * from t2 a left semi join (select key , value from t1 where key > 2) b on a.key = b.key sort by a.key, a.value;

explain select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;
select /*+ mapjoin(b) */ a.key from t3 a left semi join t1 b on a.key = b.key sort by a.key;

explain select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;
select * from t1 a left semi join t2 b on a.key = 2*b.key sort by a.key, a.value;

explain select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;
select * from t1 a join t2 b on a.key = b.key left semi join t3 c on b.key = c.key sort by a.key, a.value;

explain select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;
select * from t3 a left semi join t1 b on a.key = b.key and a.value=b.value sort by a.key, a.value;

explain select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;
select /*+ mapjoin(b, c) */ a.key from t3 a left semi join t1 b on a.key = b.key left semi join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t3 a left outer join t1 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;
select a.key from t1 a full outer join t3 b on a.key = b.key left semi join t2 c on b.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key right outer join t1 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;
select a.key from t3 a left semi join t1 b on a.key = b.key full outer join t2 c on a.key = c.key sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;
select a.key from t3 a left semi join t2 b on a.key = b.key left outer join t1 c on a.value = c.value sort by a.key;

explain select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
select a.key from t3 a left semi join t2 b on a.value = b.value where a.key > 100;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.vectorized.execution.enabled=true;
set hive.auto.convert.join=true;
set hive.mapjoin.hybridgrace.hashtable=false;
explain
select count(*) from (select c.ctinyint
from alltypesorc c
left outer join alltypesorc cd
  on cd.cint = c.cint
left outer join alltypesorc hd
  on hd.ctinyint = c.ctinyint
) t1
;
select count(*) from (select c.ctinyint
from alltypesorc c
left outer join alltypesorc cd
  on cd.cint = c.cint
left outer join alltypesorc hd
  on hd.ctinyint = c.ctinyint
) t1;

set hive.auto.convert.join=false;
set hive.vectorized.execution.enabled=false;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.fetch.task.conversion=none;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- SORT_QUERY_RESULTS

drop table if exists TJOIN1;
drop table if exists TJOIN2;
create table if not exists TJOIN1 (RNUM int , C1 int, C2 int) STORED AS orc;
create table if not exists TJOIN2 (RNUM int , C1 int, C2 char(2)) STORED AS orc;
create table if not exists TJOIN1STAGE (RNUM int , C1 int, C2 char(2)) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' STORED AS TEXTFILE ;
create table if not exists TJOIN2STAGE (RNUM int , C1 int, C2 char(2)) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' STORED AS TEXTFILE ;
LOAD DATA LOCAL INPATH '../../data/files/tjoin1.txt' OVERWRITE INTO TABLE TJOIN1STAGE;
LOAD DATA LOCAL INPATH '../../data/files/tjoin2.txt' OVERWRITE INTO TABLE TJOIN2STAGE;
INSERT INTO TABLE TJOIN1 SELECT * from TJOIN1STAGE;
INSERT INTO TABLE TJOIN2 SELECT * from TJOIN2STAGE;

set hive.vectorized.execution.enabled=false;
set hive.mapjoin.hybridgrace.hashtable=false;
explain
select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

set hive.vectorized.execution.enabled=false;
set hive.mapjoin.hybridgrace.hashtable=true;
explain
select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );


set hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=false;
SET hive.vectorized.execution.mapjoin.native.enabled=false;
explain
select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

set hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=true;
SET hive.vectorized.execution.mapjoin.native.enabled=false;
explain
select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

set hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=false;
SET hive.vectorized.execution.mapjoin.native.enabled=true;
explain
select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

set hive.vectorized.execution.enabled=true;
set hive.mapjoin.hybridgrace.hashtable=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;
explain
select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );

select tjoin1.rnum, tjoin1.c1, tjoin1.c2, tjoin2.c2 as c2j2 from tjoin1 left outer join tjoin2 on ( tjoin1.c1 = tjoin2.c1 and tjoin1.c2 > 15 );set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;

-- SORT_QUERY_RESULTS

-- Verify HIVE-8097 with a query that has a Vectorized MapJoin in the Reducer.
-- Query copied from subquery_in.q

-- non agg, non corr, with join in Parent Query
explain
select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR')
;

select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR')
;

-- non agg, corr, with join in Parent Query
explain
select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR' and l_linenumber = li.l_linenumber)
;

select p.p_partkey, li.l_suppkey
from (select distinct l_partkey as p_partkey from lineitem) p join lineitem li on p.p_partkey = li.l_partkey
where li.l_linenumber = 1 and
 li.l_orderkey in (select l_orderkey from lineitem where l_shipmode = 'AIR' and l_linenumber = li.l_linenumber)
;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

create table date_dim
(
    d_date_sk                 int,
    d_date_id                 string,
    d_date                    string,
    d_month_seq               int,
    d_week_seq                int,
    d_quarter_seq             int,
    d_year                    int,
    d_dow                     int,
    d_moy                     int,
    d_dom                     int,
    d_qoy                     int,
    d_fy_year                 int,
    d_fy_quarter_seq          int,
    d_fy_week_seq             int,
    d_day_name                string,
    d_quarter_name            string,
    d_holiday                 string,
    d_weekend                 string,
    d_following_holiday       string,
    d_first_dom               int,
    d_last_dom                int,
    d_same_day_ly             int,
    d_same_day_lq             int,
    d_current_day             string,
    d_current_week            string,
    d_current_month           string,
    d_current_quarter         string,
    d_current_year            string
)
stored as orc;

create table store_sales
(
    ss_sold_date_sk           int,
    ss_sold_time_sk           int,
    ss_item_sk                int,
    ss_customer_sk            int,
    ss_cdemo_sk               int,
    ss_hdemo_sk               int,
    ss_addr_sk                int,
    ss_promo_sk               int,
    ss_ticket_number          int,
    ss_quantity               int,
    ss_wholesale_cost         decimal(7,2),
    ss_list_price             decimal(7,2),
    ss_sales_price            decimal(7,2),
    ss_ext_discount_amt       decimal(7,2),
    ss_ext_sales_price        decimal(7,2),
    ss_ext_wholesale_cost     decimal(7,2),
    ss_ext_list_price         decimal(7,2),
    ss_ext_tax                decimal(7,2),
    ss_coupon_amt             decimal(7,2),
    ss_net_paid               decimal(7,2),
    ss_net_paid_inc_tax       decimal(7,2),
    ss_net_profit             decimal(7,2)
)
partitioned by
(
    ss_store_sk               int
)
stored as orc
tblproperties ("orc.stripe.size"="33554432", "orc.compress.size"="16384");

create table store
(
    s_store_sk                int,
    s_store_id                string,
    s_rec_start_date          string,
    s_rec_end_date            string,
    s_closed_date_sk          int,
    s_store_name              string,
    s_number_employees        int,
    s_floor_space             int,
    s_hours                   string,
    s_manager                 string,
    s_market_id               int,
    s_geography_class         string,
    s_market_desc             string,
    s_market_manager          string,
    s_division_id             int,
    s_division_name           string,
    s_company_id              int,
    s_company_name            string,
    s_street_number           string,
    s_street_name             string,
    s_street_type             string,
    s_suite_number            string,
    s_city                    string,
    s_county                  string,
    s_state                   string,
    s_zip                     string,
    s_country                 string,
    s_gmt_offset              decimal(5,2),
    s_tax_precentage          decimal(5,2)
)
stored as orc;

-- For MR, we are verifying this query DOES NOT vectorize the Map vertex with
-- the 2 TableScanOperators that have different schema.

explain select
        s_state, count(1)
 from store_sales,
 store,
 date_dim
 where store_sales.ss_sold_date_sk = date_dim.d_date_sk and
       store_sales.ss_store_sk = store.s_store_sk and
       store.s_state in ('KS','AL', 'MN', 'AL', 'SC', 'VT')
 group by s_state
 order by s_state
 limit 100;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

create table orc1
  stored as orc
  tblproperties("orc.compress"="ZLIB")
  as
    select rn
    from
    (
      select * from (select cast(1 as int) as rn from src limit 1)a
      union all
      select * from (select cast(100 as int) as rn from src limit 1)b
      union all
      select * from (select cast(10000 as int) as rn from src limit 1)c
    ) t;

create table orc_rn1 (rn int);
create table orc_rn2 (rn int);
create table orc_rn3 (rn int);

analyze table orc1 compute statistics;

explain from orc1 a
insert overwrite table orc_rn1 select a.* where a.rn < 100
insert overwrite table orc_rn2 select a.* where a.rn >= 100 and a.rn < 1000
insert overwrite table orc_rn3 select a.* where a.rn >= 1000;

from orc1 a
insert overwrite table orc_rn1 select a.* where a.rn < 100
insert overwrite table orc_rn2 select a.* where a.rn >= 100 and a.rn < 1000
insert overwrite table orc_rn3 select a.* where a.rn >= 1000;

select * from orc_rn1;
select * from orc_rn2;
select * from orc_rn3;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
CREATE TABLE non_string_part(cint INT, cstring1 STRING, cdouble DOUBLE, ctimestamp1 TIMESTAMP) PARTITIONED BY (ctinyint tinyint) STORED AS ORC;
SET hive.exec.dynamic.partition.mode=nonstrict;
SET hive.exec.dynamic.partition=true;

INSERT OVERWRITE TABLE non_string_part PARTITION(ctinyint) SELECT cint, cstring1, cdouble, ctimestamp1, ctinyint fROM alltypesorc
WHERE ctinyint IS NULL AND cdouble IS NOT NULL ORDER BY cdouble;

SHOW PARTITIONS non_string_part;

EXPLAIN SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10;

SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10;

EXPLAIN SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10;

SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;

-- SORT_QUERY_RESULTS
--
-- We currently do not support null safes (i.e the <=> operator) in native vector map join.
-- The explain output will show vectorized execution for both.  We verify the query
-- results are the same (HIVE-10628 shows native will produce the wrong results
-- otherwise).
--
-- This query for "HIVE-3315 join predicate transitive" triggers HIVE-10640-
-- explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.key is NULL;
-- select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.key is NULL;
--
CREATE TABLE myinput1_txt(key int, value int);
LOAD DATA LOCAL INPATH '../../data/files/in8.txt' INTO TABLE myinput1_txt;
CREATE TABLE myinput1 STORED AS ORC AS SELECT * FROM myinput1_txt;

SET hive.vectorized.execution.mapjoin.native.enabled=false;

-- merging
explain select * from myinput1 a join myinput1 b on a.key<=>b.value;
select * from myinput1 a join myinput1 b on a.key<=>b.value;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key=c.key;
select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key=c.key;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key<=>c.key;
select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key<=>c.key;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value=b.key join myinput1 c on a.key<=>c.key AND a.value=c.value;
select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value=b.key join myinput1 c on a.key<=>c.key AND a.value=c.value;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value<=>b.key join myinput1 c on a.key<=>c.key AND a.value<=>c.value;
select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value<=>b.key join myinput1 c on a.key<=>c.key AND a.value<=>c.value;

-- outer joins
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key<=>b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key<=>b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key<=>b.value;

-- map joins
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key<=>b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key<=>b.value;

SET hive.vectorized.execution.mapjoin.native.enabled=true;

-- merging
explain select * from myinput1 a join myinput1 b on a.key<=>b.value;
select * from myinput1 a join myinput1 b on a.key<=>b.value;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key=c.key;
select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key=c.key;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key<=>c.key;
select * from myinput1 a join myinput1 b on a.key<=>b.value join myinput1 c on a.key<=>c.key;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value=b.key join myinput1 c on a.key<=>c.key AND a.value=c.value;
select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value=b.key join myinput1 c on a.key<=>c.key AND a.value=c.value;

explain select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value<=>b.key join myinput1 c on a.key<=>c.key AND a.value<=>c.value;
select * from myinput1 a join myinput1 b on a.key<=>b.value AND a.value<=>b.key join myinput1 c on a.key<=>c.key AND a.value<=>c.value;

-- outer joins
SELECT * FROM myinput1 a LEFT OUTER JOIN myinput1 b ON a.key<=>b.value;
SELECT * FROM myinput1 a RIGHT OUTER JOIN myinput1 b ON a.key<=>b.value;
SELECT * FROM myinput1 a FULL OUTER JOIN myinput1 b ON a.key<=>b.value;

-- map joins
SELECT /*+ MAPJOIN(a) */ * FROM myinput1 a JOIN myinput1 b ON a.key<=>b.value;
SELECT /*+ MAPJOIN(b) */ * FROM myinput1 a JOIN myinput1 b ON a.key<=>b.value;

set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;

-- SORT_QUERY_RESULTS

create table a(s string) stored as orc;
create table b(s string) stored as orc;
insert into table a values('aaa');
insert into table b values('aaa');

-- We expect no vectorization due to NULL (void) projection type.
explain
select NULL from a;

select NULL from a;

explain
select NULL as x from a union distinct select NULL as x from b;

select NULL as x from a union distinct select NULL as x from b;set hive.cli.print.header=true;
set hive.explain.user=false;
SET hive.auto.convert.join=true;
set hive.fetch.task.conversion=none;
set hive.mapred.mode=nonstrict;

-- SORT_QUERY_RESULTS

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

CREATE TABLE scratch AS SELECT t, si, i, b, f, d, dc FROM vectortab2k;
INSERT INTO TABLE scratch VALUES (NULL, NULL, NULL, NULL, NULL, NULL, NULL);

CREATE TABLE vectortab2k_orc STORED AS ORC AS SELECT * FROM scratch;

SET hive.vectorized.execution.enabled=true;

--
-- Projection LongCol<Compare>LongScalar
--
EXPLAIN
SELECT sum(hash(*)) FROM
    (SELECT t, si, i, (t < 0) as compare1, (si <= 0) as compare2, (i = 0) as compare3 from vectortab2k_orc
        order by t, si, i) as q;

SELECT sum(hash(*)) FROM
    (SELECT t, si, i, (t < 0) as compare1, (si <= 0) as compare2, (i = 0) as compare3 from vectortab2k_orc
        order by t, si, i) as q;

EXPLAIN
SELECT sum(hash(*)) FROM
    (SELECT t, si, i, b, (t > 0) as compare1, (si >= 0) as compare2, (i != 0) as compare3, (b > 0) as compare4 from vectortab2k_orc
        order by t, si, i, b) as q;

SELECT sum(hash(*)) FROM
    (SELECT t, si, i, b, (t > 0) as compare1, (si >= 0) as compare2, (i != 0) as compare3, (b > 0) as compare4 from vectortab2k_orc
        order by t, si, i, b) as q;

-- With some filtering
SELECT sum(hash(*)) FROM
    (SELECT t, si, i, (t < 0) as compare1, (si <= 0) as compare2, (i = 0) as compare3 from vectortab2k_orc
        where pmod(t, 4) > 1
        order by t, si, i) as q;
SELECT sum(hash(*)) FROM
    (SELECT t, si, i, b, (t > 0) as compare1, (si >= 0) as compare2, (i != 0) as compare3, (b > 0) as compare4 from vectortab2k_orc
        where pmod(t, 4) < 2
        order by t, si, i, b) as q;


--
-- Projection LongScalar<Compare>LongColumn
--
SELECT sum(hash(*)) FROM
    (SELECT t, si, i, (0 < t) as compare1, (0 <= si) as compare2, (0 = i) as compare3 from vectortab2k_orc
        order by t, si, i) as q;

SELECT sum(hash(*)) FROM
    (SELECT t, si, i, b, (0 > t) as compare1, (0 >= si) as compare2, (0 != i) as compare3, (0 > b) as compare4 from vectortab2k_orc
        order by t, si, i, b) as q;

-- With some filtering
SELECT sum(hash(*)) FROM
    (SELECT t, si, i, (0 < t) as compare1, (0 <= si) as compare2, (0 = i) as compare3 from vectortab2k_orc
        where pmod(t, 4) > 1
        order by t, si, i) as q;
SELECT sum(hash(*)) FROM
    (SELECT t, si, i, b, (0 > t) as compare1, (0 >= si) as compare2, (0 != i) as compare3, (0 > b) as compare4 from vectortab2k_orc
        where pmod(t, 4) < 2
        order by t, si, i, b) as q;

SET hive.vectorized.execution.enabled=false;

CREATE TABLE scratch_repeat AS SELECT t, si, i, b, bo, 20 as t_repeat,
     9000 as si_repeat, 9233320 as i_repeat, -823823999339992 as b_repeat, false as bo_repeat_false, true as bo_repeat_true FROM vectortab2k;

-- The repeated columns ought to create repeated VectorizedRowBatch for those columns.
-- And then when we do a comparison, we should generate a repeated boolean result.
CREATE TABLE vectortab2k_orc_repeat STORED AS ORC AS SELECT * FROM scratch_repeat;

SET hive.vectorized.execution.enabled=true;

--
-- Projection LongCol<Compare>LongScalar
--
SELECT sum(hash(*)) FROM
    (SELECT t_repeat, si_repeat, i_repeat, (t_repeat > 0) as compare1, (si_repeat >= 0) as compare2, (i_repeat = 0) as compare3 from vectortab2k_orc_repeat
        order by t_repeat, si_repeat, i_repeat) as q;

SELECT sum(hash(*)) FROM
    (SELECT t_repeat, si_repeat, i_repeat, (t_repeat < 0) as compare1, (si_repeat <=0) as compare2, (i_repeat != 0) as compare3 from vectortab2k_orc_repeat
        order by t_repeat, si_repeat, i_repeat) as q;

-- With some filtering
SELECT sum(hash(*)) FROM
    (SELECT t_repeat, si_repeat, i_repeat, (t_repeat > 0) as compare1, (si_repeat >= 0) as compare2, (i_repeat = 0) as compare3 from vectortab2k_orc_repeat
        where pmod(si, 4) = 0
        order by t_repeat, si_repeat, i_repeat) as q;
SELECT sum(hash(*)) FROM
    (SELECT t_repeat, si_repeat, i_repeat, (t_repeat < 0) as compare1, (si_repeat <=0) as compare2, (i_repeat != 0) as compare3 from vectortab2k_orc_repeat
        where pmod(si, 4) = 3
        order by t_repeat, si_repeat, i_repeat) as q;

--
-- Projection LongScalar<Compare>LongColumn
--
SELECT sum(hash(*)) FROM
    (SELECT t_repeat, si_repeat, i_repeat, (0 > t_repeat) as compare1, (0 >= si_repeat) as compare2, (0 = i_repeat) as compare3 from vectortab2k_orc_repeat
        order by t_repeat, si_repeat, i_repeat) as q;

SELECT sum(hash(*)) FROM
    (SELECT t_repeat, si_repeat, i_repeat, (0 < t_repeat) as compare1, (0 <= si_repeat) as compare2, (0 != i_repeat) as compare3 from vectortab2k_orc_repeat
        order by t_repeat, si_repeat, i_repeat) as q;

-- With some filtering
SELECT sum(hash(*)) FROM
    (SELECT t_repeat, si_repeat, i_repeat, (0 > t_repeat) as compare1, (0 >= si_repeat) as compare2, (0 = i_repeat) as compare3 from vectortab2k_orc_repeat
        where pmod(si, 4) = 0
        order by t_repeat, si_repeat, i_repeat) as q;
SELECT sum(hash(*)) FROM
    (SELECT t_repeat, si_repeat, i_repeat, (0 < t_repeat) as compare1, (0 <= si_repeat) as compare2, (0 != i_repeat) as compare3 from vectortab2k_orc_repeat
        where pmod(si, 4) = 3
        order by t_repeat, si_repeat, i_repeat) as q;

SET hive.vectorized.execution.enabled=false;

CREATE TABLE scratch_null AS SELECT t, si, i, b, bo,
     cast(null as tinyint) as t_null, cast(null as smallint) as si_null, cast(null as int) as i_null, cast(null as bigint) as b_null, cast(null as boolean) as bo_null FROM vectortab2k;

-- The nulled columns ought to create repeated null VectorizedRowBatch for those columns.
CREATE TABLE vectortab2k_orc_null STORED AS ORC AS SELECT * FROM scratch_null;

SET hive.vectorized.execution.enabled=true;

--
-- Projection LongCol<Compare>LongScalar
--
SELECT sum(hash(*)) FROM
    (SELECT t_null, si_null, i_null, (t_null > 0) as compare1, (si_null >= 0) as compare2, (i_null = 0) as compare3 from vectortab2k_orc_null
        order by t_null, si_null, i_null) as q;

SELECT sum(hash(*)) FROM
    (SELECT t_null, si_null, i_null, (t_null < 0) as compare1, (si_null <=0) as compare2, (i_null != 0) as compare3 from vectortab2k_orc_null
        order by t_null, si_null, i_null) as q;

-- With some filtering
SELECT sum(hash(*)) FROM
    (SELECT t_null, si_null, i_null, (t_null > 0) as compare1, (si_null >= 0) as compare2, (i_null = 0) as compare3 from vectortab2k_orc_null
        where pmod(si, 4) = 0
        order by t_null, si_null, i_null) as q;
SELECT sum(hash(*)) FROM
    (SELECT t_null, si_null, i_null, (t_null < 0) as compare1, (si_null <=0) as compare2, (i_null != 0) as compare3 from vectortab2k_orc_null
        where pmod(si, 4) = 3
        order by t_null, si_null, i_null) as q;

--
-- Projection LongScalar<Compare>LongColumn
--
SELECT sum(hash(*)) FROM
    (SELECT t_null, si_null, i_null, (0 > t_null) as compare1, (0 >= si_null) as compare2, (0 = i_null) as compare3 from vectortab2k_orc_null
        order by t_null, si_null, i_null) as q;

SELECT sum(hash(*)) FROM
    (SELECT t_null, si_null, i_null, (0 < t_null) as compare1, (0 <= si_null) as compare2, (0 != i_null) as compare3 from vectortab2k_orc_null
        order by t_null, si_null, i_null) as q;

-- With some filtering
SELECT sum(hash(*)) FROM
    (SELECT t_null, si_null, i_null, (0 > t_null) as compare1, (0 >= si_null) as compare2, (0 = i_null) as compare3 from vectortab2k_orc_null
        where pmod(si, 4) = 0
        order by t_null, si_null, i_null) as q;
SELECT sum(hash(*)) FROM
    (SELECT t_null, si_null, i_null, (0 < t_null) as compare1, (0 <= si_null) as compare2, (0 != i_null) as compare3 from vectortab2k_orc_null
        where pmod(si, 4) = 3
        order by t_null, si_null, i_null) as q;

SET hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

EXPLAIN SELECT cdouble, nvl(cdouble, 100) as n
FROM alltypesorc
WHERE (cdouble IS NULL)
LIMIT 10;

SELECT cdouble, nvl(cdouble, 100) as n
FROM alltypesorc
WHERE (cdouble IS NULL)
LIMIT 10;

EXPLAIN SELECT cfloat, nvl(cfloat, 1) as n
FROM alltypesorc
LIMIT 10;

SELECT cfloat, nvl(cfloat, 1) as n
FROM alltypesorc
LIMIT 10;

EXPLAIN SELECT nvl(null, 10) as n
FROM alltypesorc
LIMIT 10;

SELECT nvl(null, 10) as n
FROM alltypesorc
LIMIT 10;

EXPLAIN SELECT nvl(null, null) as n
FROM alltypesorc
LIMIT 10;

SELECT nvl(null, null) as n
FROM alltypesorc
LIMIT 10;
create table orcstr (vcol varchar(20)) stored as orc;

insert overwrite table orcstr select null from src;

SET hive.fetch.task.conversion=none;

SET hive.vectorized.execution.enabled=false;
select vcol from orcstr limit 1;

SET hive.vectorized.execution.enabled=true;
select vcol from orcstr limit 1;

insert overwrite table orcstr select "" from src;

SET hive.vectorized.execution.enabled=false;
select vcol from orcstr limit 1;

SET hive.vectorized.execution.enabled=true;
select vcol from orcstr limit 1;

set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

explain
select bo, max(b) from vectortab2korc group by bo order by bo desc;

select bo, max(b) from vectortab2korc group by bo order by bo desc;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;

CREATE TABLE orc_table_1(v1 STRING, a INT) STORED AS ORC;
CREATE TABLE orc_table_2(c INT, v2 STRING) STORED AS ORC;

insert into table orc_table_1 values ("<null1>", null),("one", 1),("one", 1),("two", 2),("three", 3),("<null2>", null);
insert into table orc_table_2 values (0, "ZERO"),(2, "TWO"), (3, "THREE"),(null, "<NULL1>"),(4, "FOUR"),(null, "<NULL2>");

select * from orc_table_1;
select * from orc_table_2;

explain
select t1.v1, t1.a, t2.c, t2.v2 from orc_table_1 t1 left outer join orc_table_2 t2 on t1.a = t2.c;

-- SORT_QUERY_RESULTS

select t1.v1, t1.a, t2.c, t2.v2 from orc_table_1 t1 left outer join orc_table_2 t2 on t1.a = t2.c;

explain
select t1.v1, t1.a, t2.c, t2.v2 from orc_table_1 t1 right outer join orc_table_2 t2 on t1.a = t2.c;

-- SORT_QUERY_RESULTS

select t1.v1, t1.a, t2.c, t2.v2 from orc_table_1 t1 right outer join orc_table_2 t2 on t1.a = t2.c;set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;

-- Using cint and ctinyint in test queries
create table small_alltypesorc1a as select * from alltypesorc where cint is not null and ctinyint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc2a as select * from alltypesorc where cint is null and ctinyint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc3a as select * from alltypesorc where cint is not null and ctinyint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc4a as select * from alltypesorc where cint is null and ctinyint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;

select * from small_alltypesorc1a;
select * from small_alltypesorc2a;
select * from small_alltypesorc3a;
select * from small_alltypesorc4a;

create table small_alltypesorc_a stored as orc as select * from
(select * from (select * from small_alltypesorc1a) sq1
 union all
 select * from (select * from small_alltypesorc2a) sq2
 union all
 select * from (select * from small_alltypesorc3a) sq3
 union all
 select * from (select * from small_alltypesorc4a) sq4) q;

ANALYZE TABLE small_alltypesorc_a COMPUTE STATISTICS;
ANALYZE TABLE small_alltypesorc_a COMPUTE STATISTICS FOR COLUMNS;

select * from small_alltypesorc_a;

explain
select *
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cint = c.cint;

-- SORT_QUERY_RESULTS

select *
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cint = c.cint;

explain
select c.ctinyint
from small_alltypesorc_a c
left outer join small_alltypesorc_a hd
  on hd.ctinyint = c.ctinyint;

-- SORT_QUERY_RESULTS

select c.ctinyint
from small_alltypesorc_a c
left outer join small_alltypesorc_a hd
  on hd.ctinyint = c.ctinyint;

explain
select count(*), sum(t1.c_ctinyint) from (select c.ctinyint as c_ctinyint
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cint = c.cint
left outer join small_alltypesorc_a hd
  on hd.ctinyint = c.ctinyint
) t1;

-- SORT_QUERY_RESULTS

select count(*), sum(t1.c_ctinyint) from (select c.ctinyint as c_ctinyint
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cint = c.cint
left outer join small_alltypesorc_a hd
  on hd.ctinyint = c.ctinyint
) t1;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;

-- Using cint and cbigint in test queries
create table small_alltypesorc1a as select * from alltypesorc where cint is not null and cbigint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc2a as select * from alltypesorc where cint is null and cbigint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc3a as select * from alltypesorc where cint is not null and cbigint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc4a as select * from alltypesorc where cint is null and cbigint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;

select * from small_alltypesorc1a;
select * from small_alltypesorc2a;
select * from small_alltypesorc3a;
select * from small_alltypesorc4a;

create table small_alltypesorc_a stored as orc as select * from
(select * from (select * from small_alltypesorc1a) sq1
 union all
 select * from (select * from small_alltypesorc2a) sq2
 union all
 select * from (select * from small_alltypesorc3a) sq3
 union all
 select * from (select * from small_alltypesorc4a) sq4) q;

ANALYZE TABLE small_alltypesorc_a COMPUTE STATISTICS;
ANALYZE TABLE small_alltypesorc_a COMPUTE STATISTICS FOR COLUMNS;

select * from small_alltypesorc_a;

explain
select count(*), sum(t1.c_cbigint) from (select c.cbigint as c_cbigint
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cint = c.cint
left outer join small_alltypesorc_a hd
  on hd.cbigint = c.cbigint
) t1;

-- SORT_QUERY_RESULTS

select count(*), sum(t1.c_cbigint) from (select c.cbigint as c_cbigint
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cint = c.cint
left outer join small_alltypesorc_a hd
  on hd.cbigint = c.cbigint
) t1;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;

-- Using cint and cstring1 in test queries
create table small_alltypesorc1a as select * from alltypesorc where cint is not null and cstring1 is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc2a as select * from alltypesorc where cint is null and cstring1 is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc3a as select * from alltypesorc where cint is not null and cstring1 is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;
create table small_alltypesorc4a as select * from alltypesorc where cint is null and cstring1 is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 5;

select * from small_alltypesorc1a;
select * from small_alltypesorc2a;
select * from small_alltypesorc3a;
select * from small_alltypesorc4a;

create table small_alltypesorc_a stored as orc as select * from
(select * from (select * from small_alltypesorc1a) sq1
 union all
 select * from (select * from small_alltypesorc2a) sq2
 union all
 select * from (select * from small_alltypesorc3a) sq3
 union all
 select * from (select * from small_alltypesorc4a) sq4) q;

ANALYZE TABLE small_alltypesorc_a COMPUTE STATISTICS;
ANALYZE TABLE small_alltypesorc_a COMPUTE STATISTICS FOR COLUMNS;

select * from small_alltypesorc_a;
explain
select count(*) from (select c.cstring1
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cint = c.cint
left outer join small_alltypesorc_a hd
  on hd.cstring1 = c.cstring1
) t1
;

-- SORT_QUERY_RESULTS

select count(*) from (select c.cstring1
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cint = c.cint
left outer join small_alltypesorc_a hd
  on hd.cstring1 = c.cstring1
) t1;

explain
select count(*) from (select c.cstring1
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cstring2 = c.cstring2
left outer join small_alltypesorc_a hd
  on hd.cstring1 = c.cstring1
) t1
;

-- SORT_QUERY_RESULTS

select count(*) from (select c.cstring1
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cstring2 = c.cstring2
left outer join small_alltypesorc_a hd
  on hd.cstring1 = c.cstring1
) t1;

explain
select count(*) from (select c.cstring1
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cstring2 = c.cstring2 and cd.cbigint = c.cbigint
left outer join small_alltypesorc_a hd
  on hd.cstring1 = c.cstring1 and hd.cint = c.cint
) t1
;

-- SORT_QUERY_RESULTS

select count(*) from (select c.cstring1
from small_alltypesorc_a c
left outer join small_alltypesorc_a cd
  on cd.cstring2 = c.cstring2 and cd.cbigint = c.cbigint
left outer join small_alltypesorc_a hd
  on hd.cstring1 = c.cstring1 and hd.cint = c.cint
) t1;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;

-- Using cint and ctinyint in test queries
create table small_alltypesorc1b as select * from alltypesorc where cint is not null and ctinyint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 10;
create table small_alltypesorc2b as select * from alltypesorc where cint is null and ctinyint is not null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 10;
create table small_alltypesorc3b as select * from alltypesorc where cint is not null and ctinyint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 10;
create table small_alltypesorc4b as select * from alltypesorc where cint is null and ctinyint is null order by ctinyint, csmallint, cint, cbigint, cfloat, cdouble, cstring1, cstring2, ctimestamp1, ctimestamp2, cboolean1, cboolean2 limit 10;

select * from small_alltypesorc1b;
select * from small_alltypesorc2b;
select * from small_alltypesorc3b;
select * from small_alltypesorc4b;

create table small_alltypesorc_b stored as orc as select * from
(select * from (select * from small_alltypesorc1b) sq1
 union all
 select * from (select * from small_alltypesorc2b) sq2
 union all
 select * from (select * from small_alltypesorc3b) sq3
 union all
 select * from (select * from small_alltypesorc4b) sq4) q;

ANALYZE TABLE small_alltypesorc_b COMPUTE STATISTICS;
ANALYZE TABLE small_alltypesorc_b COMPUTE STATISTICS FOR COLUMNS;

select * from small_alltypesorc_b;

explain
select *
from small_alltypesorc_b c
left outer join small_alltypesorc_b cd
  on cd.cint = c.cint;

-- SORT_QUERY_RESULTS

select *
from small_alltypesorc_b c
left outer join small_alltypesorc_b cd
  on cd.cint = c.cint;

explain
select c.ctinyint
from small_alltypesorc_b c
left outer join small_alltypesorc_b hd
  on hd.ctinyint = c.ctinyint;

-- SORT_QUERY_RESULTS

select c.ctinyint
from small_alltypesorc_b c
left outer join small_alltypesorc_b hd
  on hd.ctinyint = c.ctinyint;

explain
select count(*) from (select c.ctinyint
from small_alltypesorc_b c
left outer join small_alltypesorc_b cd
  on cd.cint = c.cint
left outer join small_alltypesorc_b hd
  on hd.ctinyint = c.ctinyint
) t1
;

-- SORT_QUERY_RESULTS

select count(*) from (select c.ctinyint
from small_alltypesorc_b c
left outer join small_alltypesorc_b cd
  on cd.cint = c.cint
left outer join small_alltypesorc_b hd
  on hd.ctinyint = c.ctinyint
) t1;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;
set hive.auto.convert.join=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=10000;

-- SORT_QUERY_RESULTS

create table sorted_mod_4 stored as orc
as select ctinyint, pmod(cint, 4) as cmodint from alltypesorc
where cint is not null and ctinyint is not null
order by ctinyint;

ANALYZE TABLE sorted_mod_4 COMPUTE STATISTICS;
ANALYZE TABLE sorted_mod_4 COMPUTE STATISTICS FOR COLUMNS;

create table small_table stored
as orc as select ctinyint, cbigint from alltypesorc limit 100;

ANALYZE TABLE small_table COMPUTE STATISTICS;
ANALYZE TABLE small_table COMPUTE STATISTICS FOR COLUMNS;

explain
select count(*) from (select s.*, st.*
from sorted_mod_4 s
left outer join small_table st
on s.ctinyint = st.ctinyint
) t1;

select count(*) from (select s.*, st.*
from sorted_mod_4 s
left outer join small_table st
on s.ctinyint = st.ctinyint
) t1;

explain
select count(*) from (select s.ctinyint, s.cmodint, sm.cbigint
from sorted_mod_4 s
left outer join small_table sm
on s.ctinyint = sm.ctinyint and s.cmodint = 2
) t1;

select count(*) from (select s.ctinyint, s.cmodint, sm.cbigint
from sorted_mod_4 s
left outer join small_table sm
on s.ctinyint = sm.ctinyint and s.cmodint = 2
) t1;

explain
select count(*) from (select s.ctinyint, s.cmodint, sm.cbigint
from sorted_mod_4 s
left outer join small_table sm
on s.ctinyint = sm.ctinyint and pmod(s.ctinyint, 4) = s.cmodint
) t1;

select count(*) from (select s.ctinyint, s.cmodint, sm.cbigint
from sorted_mod_4 s
left outer join small_table sm
on s.ctinyint = sm.ctinyint and pmod(s.ctinyint, 4) = s.cmodint
) t1;

explain
select count(*) from (select s.ctinyint, s.cmodint, sm.cbigint
from sorted_mod_4 s
left outer join small_table sm
on s.ctinyint = sm.ctinyint and s.ctinyint < 100
) t1;

select count(*) from (select s.ctinyint, s.cmodint, sm.cbigint
from sorted_mod_4 s
left outer join small_table sm
on s.ctinyint = sm.ctinyint and s.ctinyint < 100
) t1;

explain
select count(*) from (select s.*, sm.*, s2.*
from sorted_mod_4 s
left outer join small_table sm
  on pmod(sm.cbigint, 8) = s.cmodint
left outer join sorted_mod_4 s2
  on s2.ctinyint = s.ctinyint
) t1;

select count(*) from (select s.*, sm.*, s2.*
from sorted_mod_4 s
left outer join small_table sm
  on pmod(sm.cbigint, 8) = s.cmodint
left outer join sorted_mod_4 s2
  on s2.ctinyint = s.ctinyint
) t1;


create table mod_8_mod_4 stored as orc
as select pmod(ctinyint, 8) as cmodtinyint, pmod(cint, 4) as cmodint from alltypesorc
where cint is not null and ctinyint is not null;

ANALYZE TABLE mod_8_mod_4 COMPUTE STATISTICS;
ANALYZE TABLE mod_8_mod_4 COMPUTE STATISTICS FOR COLUMNS;

create table small_table2 stored
as orc as select pmod(ctinyint, 16) as cmodtinyint, cbigint from alltypesorc limit 100;

ANALYZE TABLE small_table2 COMPUTE STATISTICS;
ANALYZE TABLE small_table2 COMPUTE STATISTICS FOR COLUMNS;

explain
select count(*) from (select s.*, st.*
from mod_8_mod_4 s
left outer join small_table2 st
on s.cmodtinyint = st.cmodtinyint
) t1;

select count(*) from (select s.*, st.*
from mod_8_mod_4 s
left outer join small_table2 st
on s.cmodtinyint = st.cmodtinyint
) t1;

explain
select count(*) from (select s.cmodtinyint, s.cmodint, sm.cbigint
from mod_8_mod_4 s
left outer join small_table2 sm
on s.cmodtinyint = sm.cmodtinyint and s.cmodint = 2
) t1;

select count(*) from (select s.cmodtinyint, s.cmodint, sm.cbigint
from mod_8_mod_4 s
left outer join small_table2 sm
on s.cmodtinyint = sm.cmodtinyint and s.cmodint = 2
) t1;

explain
select count(*) from (select s.cmodtinyint, s.cmodint, sm.cbigint
from mod_8_mod_4 s
left outer join small_table2 sm
on s.cmodtinyint = sm.cmodtinyint and pmod(s.cmodtinyint, 4) = s.cmodint
) t1;

select count(*) from (select s.cmodtinyint, s.cmodint, sm.cbigint
from mod_8_mod_4 s
left outer join small_table2 sm
on s.cmodtinyint = sm.cmodtinyint and pmod(s.cmodtinyint, 4) = s.cmodint
) t1;

explain
select count(*) from (select s.cmodtinyint, s.cmodint, sm.cbigint
from mod_8_mod_4 s
left outer join small_table2 sm
on s.cmodtinyint = sm.cmodtinyint and s.cmodtinyint < 3
) t1;

select count(*) from (select s.cmodtinyint, s.cmodint, sm.cbigint
from mod_8_mod_4 s
left outer join small_table2 sm
on s.cmodtinyint = sm.cmodtinyint and s.cmodtinyint < 3
) t1;

explain
select count(*) from (select s.*, sm.*, s2.*
from mod_8_mod_4 s
left outer join small_table2 sm
  on pmod(sm.cbigint, 8) = s.cmodint
left outer join mod_8_mod_4 s2
  on s2.cmodtinyint = s.cmodtinyint
) t1;

select count(*) from (select s.*, sm.*, s2.*
from mod_8_mod_4 s
left outer join small_table2 sm
  on pmod(sm.cbigint, 8) = s.cmodint
left outer join mod_8_mod_4 s2
  on s2.cmodtinyint = s.cmodtinyint
) t1;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.mapjoin.native.enabled=true;
SET hive.auto.convert.join=true;

-- SORT_QUERY_RESULTS

create table TJOIN1_txt (RNUM int , C1 int, C2 int)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';

create table TJOIN2_txt (RNUM int , C1 int, C2 char(2))
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';

create table if not exists TJOIN3_txt (RNUM int , C1 int, C2 char(2))
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';

create table TJOIN4_txt (RNUM int , C1 int, C2 char(2))
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n';

load data local inpath '../../data/files/TJOIN1' into table TJOIN1_txt;
load data local inpath '../../data/files/TJOIN2' into table TJOIN2_txt;
load data local inpath '../../data/files/TJOIN3' into table TJOIN3_txt;
load data local inpath '../../data/files/TJOIN4' into table TJOIN4_txt;

create table TJOIN1 stored as orc AS SELECT * FROM TJOIN1_txt;
create table TJOIN2 stored as orc AS SELECT * FROM TJOIN2_txt;
create table TJOIN3 stored as orc AS SELECT * FROM TJOIN3_txt;
create table TJOIN4 stored as orc AS SELECT * FROM TJOIN4_txt;

explain
select tj1rnum, tj2rnum, tjoin3.rnum as rnumt3 from
   (select tjoin1.rnum tj1rnum, tjoin2.rnum tj2rnum, tjoin2.c1 tj2c1 from tjoin1 left outer join tjoin2 on tjoin1.c1 = tjoin2.c1 ) tj left outer join tjoin3 on tj2c1 = tjoin3.c1;

select tj1rnum, tj2rnum, tjoin3.rnum as rnumt3 from
   (select tjoin1.rnum tj1rnum, tjoin2.rnum tj2rnum, tjoin2.c1 tj2c1 from tjoin1 left outer join tjoin2 on tjoin1.c1 = tjoin2.c1 ) tj left outer join tjoin3 on tj2c1 = tjoin3.c1;

explain
select tj1rnum, tj2rnum as rnumt3 from
   (select tjoin1.rnum tj1rnum, tjoin2.rnum tj2rnum, tjoin2.c1 tj2c1 from tjoin1 left outer join tjoin2 on tjoin1.c1 = tjoin2.c1 ) tj left outer join tjoin3 on tj2c1 = tjoin3.c1;

select tj1rnum, tj2rnum as rnumt3 from
   (select tjoin1.rnum tj1rnum, tjoin2.rnum tj2rnum, tjoin2.c1 tj2c1 from tjoin1 left outer join tjoin2 on tjoin1.c1 = tjoin2.c1 ) tj left outer join tjoin3 on tj2c1 = tjoin3.c1;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
set hive.fetch.task.conversion=minimal;

-- Exclude test on Windows due to space character being escaped in Hive paths on Windows.
-- EXCLUDE_OS_WINDOWS

-- Check if vectorization code is handling partitioning on DATE and the other data types.


CREATE TABLE flights_tiny (
  origin_city_name STRING,
  dest_city_name STRING,
  fl_date DATE,
  arr_delay FLOAT,
  fl_num INT
);

LOAD DATA LOCAL INPATH '../../data/files/flights_tiny.txt.1' OVERWRITE INTO TABLE flights_tiny;

CREATE TABLE flights_tiny_orc STORED AS ORC AS
SELECT origin_city_name, dest_city_name, fl_date, to_utc_timestamp(fl_date, 'America/Los_Angeles') as fl_time, arr_delay, fl_num
FROM flights_tiny;

SELECT * FROM flights_tiny_orc;

SET hive.vectorized.execution.enabled=false;

select * from flights_tiny_orc sort by fl_num, fl_date limit 25;

select fl_date, count(*) from flights_tiny_orc group by fl_date;

SET hive.vectorized.execution.enabled=true;

explain
select * from flights_tiny_orc sort by fl_num, fl_date limit 25;

select * from flights_tiny_orc sort by fl_num, fl_date limit 25;

explain
select fl_date, count(*) from flights_tiny_orc group by fl_date;

select fl_date, count(*) from flights_tiny_orc group by fl_date;


SET hive.vectorized.execution.enabled=false;

CREATE TABLE flights_tiny_orc_partitioned_date (
  origin_city_name STRING,
  dest_city_name STRING,
  fl_time TIMESTAMP,
  arr_delay FLOAT,
  fl_num INT
)
PARTITIONED BY (fl_date DATE)
STORED AS ORC;

set hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE flights_tiny_orc_partitioned_date
PARTITION (fl_date)
SELECT  origin_city_name, dest_city_name, fl_time, arr_delay, fl_num, fl_date
FROM flights_tiny_orc;


select * from flights_tiny_orc_partitioned_date;

select * from flights_tiny_orc_partitioned_date sort by fl_num, fl_date limit 25;

select fl_date, count(*) from flights_tiny_orc_partitioned_date group by fl_date;

SET hive.vectorized.execution.enabled=true;

explain
select * from flights_tiny_orc_partitioned_date;

select * from flights_tiny_orc_partitioned_date;

explain
select * from flights_tiny_orc_partitioned_date sort by fl_num, fl_date limit 25;

select * from flights_tiny_orc_partitioned_date sort by fl_num, fl_date limit 25;

explain
select fl_date, count(*) from flights_tiny_orc_partitioned_date group by fl_date;

select fl_date, count(*) from flights_tiny_orc_partitioned_date group by fl_date;


SET hive.vectorized.execution.enabled=false;

CREATE TABLE flights_tiny_orc_partitioned_timestamp (
  origin_city_name STRING,
  dest_city_name STRING,
  fl_date DATE,
  arr_delay FLOAT,
  fl_num INT
)
PARTITIONED BY (fl_time TIMESTAMP)
STORED AS ORC;

set hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE flights_tiny_orc_partitioned_timestamp
PARTITION (fl_time)
SELECT  origin_city_name, dest_city_name, fl_date, arr_delay, fl_num, fl_time
FROM flights_tiny_orc;


select * from flights_tiny_orc_partitioned_timestamp;

select * from flights_tiny_orc_partitioned_timestamp sort by fl_num, fl_time limit 25;

select fl_time, count(*) from flights_tiny_orc_partitioned_timestamp group by fl_time;

SET hive.vectorized.execution.enabled=true;

explain
select * from flights_tiny_orc_partitioned_timestamp;

select * from flights_tiny_orc_partitioned_timestamp;

explain
select * from flights_tiny_orc_partitioned_timestamp sort by fl_num, fl_time limit 25;

select * from flights_tiny_orc_partitioned_timestamp sort by fl_num, fl_time limit 25;

explain
select fl_time, count(*) from flights_tiny_orc_partitioned_timestamp group by fl_time;

select fl_time, count(*) from flights_tiny_orc_partitioned_timestamp group by fl_time;
set hive.fetch.task.conversion=minimal;

-- Windows-specific test due to space character being escaped in Hive paths on Windows.
-- INCLUDE_OS_WINDOWS

-- Check if vectorization code is handling partitioning on DATE and the other data types.


CREATE TABLE flights_tiny (
  origin_city_name STRING,
  dest_city_name STRING,
  fl_date DATE,
  arr_delay FLOAT,
  fl_num INT
);

LOAD DATA LOCAL INPATH '../../data/files/flights_tiny.txt.1' OVERWRITE INTO TABLE flights_tiny;

CREATE TABLE flights_tiny_orc STORED AS ORC AS
SELECT origin_city_name, dest_city_name, fl_date, to_utc_timestamp(fl_date, 'America/Los_Angeles') as fl_time, arr_delay, fl_num
FROM flights_tiny;

SELECT * FROM flights_tiny_orc;

SET hive.vectorized.execution.enabled=false;

select * from flights_tiny_orc sort by fl_num, fl_date limit 25;

select fl_date, count(*) from flights_tiny_orc group by fl_date;

SET hive.vectorized.execution.enabled=true;

explain
select * from flights_tiny_orc sort by fl_num, fl_date limit 25;

select * from flights_tiny_orc sort by fl_num, fl_date limit 25;

explain
select fl_date, count(*) from flights_tiny_orc group by fl_date;

select fl_date, count(*) from flights_tiny_orc group by fl_date;


SET hive.vectorized.execution.enabled=false;

CREATE TABLE flights_tiny_orc_partitioned_date (
  origin_city_name STRING,
  dest_city_name STRING,
  fl_time TIMESTAMP,
  arr_delay FLOAT,
  fl_num INT
)
PARTITIONED BY (fl_date DATE)
STORED AS ORC;

set hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE flights_tiny_orc_partitioned_date
PARTITION (fl_date)
SELECT  origin_city_name, dest_city_name, fl_time, arr_delay, fl_num, fl_date
FROM flights_tiny_orc;


select * from flights_tiny_orc_partitioned_date;

select * from flights_tiny_orc_partitioned_date sort by fl_num, fl_date limit 25;

select fl_date, count(*) from flights_tiny_orc_partitioned_date group by fl_date;

SET hive.vectorized.execution.enabled=true;

explain
select * from flights_tiny_orc_partitioned_date;

select * from flights_tiny_orc_partitioned_date;

explain
select * from flights_tiny_orc_partitioned_date sort by fl_num, fl_date limit 25;

select * from flights_tiny_orc_partitioned_date sort by fl_num, fl_date limit 25;

explain
select fl_date, count(*) from flights_tiny_orc_partitioned_date group by fl_date;

select fl_date, count(*) from flights_tiny_orc_partitioned_date group by fl_date;


SET hive.vectorized.execution.enabled=false;

CREATE TABLE flights_tiny_orc_partitioned_timestamp (
  origin_city_name STRING,
  dest_city_name STRING,
  fl_date DATE,
  arr_delay FLOAT,
  fl_num INT
)
PARTITIONED BY (fl_time TIMESTAMP)
STORED AS ORC;

set hive.exec.dynamic.partition.mode=nonstrict;

INSERT INTO TABLE flights_tiny_orc_partitioned_timestamp
PARTITION (fl_time)
SELECT  origin_city_name, dest_city_name, fl_date, arr_delay, fl_num, fl_time
FROM flights_tiny_orc;


select * from flights_tiny_orc_partitioned_timestamp;

select * from flights_tiny_orc_partitioned_timestamp sort by fl_num, fl_time limit 25;

select fl_time, count(*) from flights_tiny_orc_partitioned_timestamp group by fl_time;

SET hive.vectorized.execution.enabled=true;

explain
select * from flights_tiny_orc_partitioned_timestamp;

select * from flights_tiny_orc_partitioned_timestamp;

explain
select * from flights_tiny_orc_partitioned_timestamp sort by fl_num, fl_time limit 25;

select * from flights_tiny_orc_partitioned_timestamp sort by fl_num, fl_time limit 25;

explain
select fl_time, count(*) from flights_tiny_orc_partitioned_timestamp group by fl_time;

select fl_time, count(*) from flights_tiny_orc_partitioned_timestamp group by fl_time;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=minimal;

create table inventory_txt
(
    inv_date_sk                int,
    inv_item_sk                int,
    inv_warehouse_sk           int,
    inv_quantity_on_hand       int
)
row format delimited fields terminated by '|'
stored as textfile;

LOAD DATA LOCAL INPATH '../../data/files/inventory' OVERWRITE INTO TABLE inventory_txt;

-- No column change case

create table inventory_part_0(
    inv_date_sk             int,
    inv_item_sk             int,
    inv_warehouse_sk        int,
    inv_quantity_on_hand    int)
partitioned by (par string) stored as orc;

insert into table inventory_part_0 partition(par='1') select * from inventory_txt;
insert into table inventory_part_0 partition(par='2') select * from inventory_txt;

explain
select sum(inv_quantity_on_hand) from inventory_part_0;

select sum(inv_quantity_on_hand) from inventory_part_0;

-- Additional column for 2nd partition...

create table inventory_part_1(
    inv_date_sk             int,
    inv_item_sk             int,
    inv_warehouse_sk        int,
    inv_quantity_on_hand    int)
partitioned by (par string) stored as orc;

insert into table inventory_part_1 partition(par='4cols') select * from inventory_txt;

alter table inventory_part_1 add columns (fifthcol string);

insert into table inventory_part_1 partition(par='5cols') select *, '5th' as fifthcol from inventory_txt;

explain
select sum(inv_quantity_on_hand) from inventory_part_1;

select sum(inv_quantity_on_hand) from inventory_part_1;

-- Verify we do not vectorize when a partition column name is different.
-- Currently, we do not attempt the actual select because non-vectorized ORC table reader gets a cast exception.

create table inventory_part_2a(
    inv_date_sk             int,
    inv_item_sk             int,
    inv_warehouse_sk        int,
    inv_quantity_on_hand    int)
partitioned by (par string) stored as orc;

insert into table inventory_part_2a partition(par='1') select * from inventory_txt;
insert into table inventory_part_2a partition(par='2') select * from inventory_txt;
alter table inventory_part_2a partition (par='2') change inv_item_sk other_name int;

explain
select sum(inv_quantity_on_hand) from inventory_part_2a;

create table inventory_part_2b(
    inv_date_sk             int,
    inv_item_sk             int,
    inv_warehouse_sk        int,
    inv_quantity_on_hand    int)
partitioned by (par1 string, par2 int) stored as orc;

insert into table inventory_part_2b partition(par1='1',par2=4) select * from inventory_txt;
insert into table inventory_part_2b partition(par1='2',par2=3) select * from inventory_txt;
alter table inventory_part_2b partition (par1='2',par2=3) change inv_quantity_on_hand other_name int;

explain
select sum(inv_quantity_on_hand) from inventory_part_2b;

-- Verify we do not vectorize when a partition column type is different.
-- Currently, we do not attempt the actual select because non-vectorized ORC table reader gets a cast exception.

create table inventory_part_3(
    inv_date_sk             int,
    inv_item_sk             int,
    inv_warehouse_sk        int,
    inv_quantity_on_hand    int)
partitioned by (par string) stored as orc;

insert into table inventory_part_3 partition(par='1') select * from inventory_txt;
insert into table inventory_part_3 partition(par='2') select * from inventory_txt;
alter table inventory_part_3 partition (par='2') change inv_warehouse_sk inv_warehouse_sk bigint;

explain
select sum(inv_quantity_on_hand) from inventory_part_3;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.reducesink.new.enabled=true;

-- SORT_QUERY_RESULTS

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

explain
select b from vectortab2korc order by b;

select b from vectortab2korc order by b;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.reducesink.new.enabled=true;

-- SORT_QUERY_RESULTS

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

explain
select s, i, s2 from vectortab2korc order by s, i, s2;

select s, i, s2 from vectortab2korc order by s, i, s2;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.vectorized.execution.reducesink.new.enabled=true;

-- SORT_QUERY_RESULTS

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

explain
select s from vectortab2korc order by s;

select s from vectortab2korc order by s;
set hive.explain.user=false;
CREATE TABLE decimal_test STORED AS ORC AS SELECT cint, cdouble, CAST (((cdouble*22.1)/37) AS DECIMAL(20,10)) AS cdecimal1, CAST (((cdouble*9.3)/13) AS DECIMAL(23,14)) AS cdecimal2 FROM alltypesorc
WHERE cint is not null and cdouble is not null;

SET hive.vectorized.execution.enabled=true;

EXPLAIN
SELECT cint, cdouble, cdecimal1, cdecimal2, min(cdecimal1) as min_decimal1 FROM decimal_test
WHERE cdecimal1 is not null and cdecimal2 is not null
GROUP BY cint, cdouble, cdecimal1, cdecimal2
ORDER BY cint, cdouble, cdecimal1, cdecimal2
LIMIT 50;

SELECT cint, cdouble, cdecimal1, cdecimal2, min(cdecimal1) as min_decimal1 FROM decimal_test
WHERE cdecimal1 is not null and cdecimal2 is not null
GROUP BY cint, cdouble, cdecimal1, cdecimal2
ORDER BY cint, cdouble, cdecimal1, cdecimal2
LIMIT 50;

SET hive.vectorized.execution.enabled=false;

SELECT sum(hash(*))
  FROM (SELECT cint, cdouble, cdecimal1, cdecimal2, min(cdecimal1) as min_decimal1 FROM decimal_test
        WHERE cdecimal1 is not null and cdecimal2 is not null
        GROUP BY cint, cdouble, cdecimal1, cdecimal2
        ORDER BY cint, cdouble, cdecimal1, cdecimal2
        LIMIT 50) as q;

SET hive.vectorized.execution.enabled=true;

SELECT sum(hash(*))
  FROM (SELECT cint, cdouble, cdecimal1, cdecimal2, min(cdecimal1) as min_decimal1 FROM decimal_test
        WHERE cdecimal1 is not null and cdecimal2 is not null
        GROUP BY cint, cdouble, cdecimal1, cdecimal2
        ORDER BY cint, cdouble, cdecimal1, cdecimal2
        LIMIT 50) as q;
set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

DROP TABLE over1k;
DROP TABLE over1korc;

-- data setup
CREATE TABLE over1k(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/over1k' OVERWRITE INTO TABLE over1k;

CREATE TABLE over1korc(t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
STORED AS ORC;

INSERT INTO TABLE over1korc SELECT * FROM over1k;

EXPLAIN SELECT s AS `string`,
       CONCAT(CONCAT('      ',s),'      ') AS `none_padded_str`,
       CONCAT(CONCAT('|',RTRIM(CONCAT(CONCAT('      ',s),'      '))),'|') AS `none_z_rtrim_str`
       FROM over1korc LIMIT 20;

SELECT s AS `string`,
       CONCAT(CONCAT('      ',s),'      ') AS `none_padded_str`,
       CONCAT(CONCAT('|',RTRIM(CONCAT(CONCAT('      ',s),'      '))),'|') AS `none_z_rtrim_str`
       FROM over1korc LIMIT 20;

------------------------------------------------------------------------------------------

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

EXPLAIN
SELECT CONCAT(CONCAT(CONCAT('Quarter ',CAST(CAST((MONTH(dt) - 1) / 3 + 1 AS INT) AS STRING)),'-'),CAST(YEAR(dt) AS STRING)) AS `field`
    FROM vectortab2korc
    GROUP BY CONCAT(CONCAT(CONCAT('Quarter ',CAST(CAST((MONTH(dt) - 1) / 3 + 1 AS INT) AS STRING)),'-'),CAST(YEAR(dt) AS STRING))
    ORDER BY `field`
    LIMIT 50;

SELECT CONCAT(CONCAT(CONCAT('Quarter ',CAST(CAST((MONTH(dt) - 1) / 3 + 1 AS INT) AS STRING)),'-'),CAST(YEAR(dt) AS STRING)) AS `field`
    FROM vectortab2korc
    GROUP BY CONCAT(CONCAT(CONCAT('Quarter ',CAST(CAST((MONTH(dt) - 1) / 3 + 1 AS INT) AS STRING)),'-'),CAST(YEAR(dt) AS STRING))
    ORDER BY `field`
    LIMIT 50;
set hive.cbo.enable=false;
set hive.tez.dynamic.partition.pruning=false;
set hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;

-- SORT_QUERY_RESULTS

-- 2 Strings
create table test_1 (`id` string, `lineid` string) stored as orc;

insert into table test_1 values ('one','1'), ('seven','1');

explain
select * from test_1 where struct(`id`, `lineid`)
IN (
struct('two','3'),
struct('three','1'),
struct('one','1'),
struct('five','2'),
struct('six','1'),
struct('eight','1'),
struct('seven','1'),
struct('nine','1'),
struct('ten','1')
);

select * from test_1 where struct(`id`, `lineid`)
IN (
struct('two','3'),
struct('three','1'),
struct('one','1'),
struct('five','2'),
struct('six','1'),
struct('eight','1'),
struct('seven','1'),
struct('nine','1'),
struct('ten','1')
);

explain
select `id`, `lineid`, struct(`id`, `lineid`)
IN (
struct('two','3'),
struct('three','1'),
struct('one','1'),
struct('five','2'),
struct('six','1'),
struct('eight','1'),
struct('seven','1'),
struct('nine','1'),
struct('ten','1')
) as b from test_1 ;

select `id`, `lineid`, struct(`id`, `lineid`)
IN (
struct('two','3'),
struct('three','1'),
struct('one','1'),
struct('five','2'),
struct('six','1'),
struct('eight','1'),
struct('seven','1'),
struct('nine','1'),
struct('ten','1')
) as b from test_1 ;


-- 2 Integers
create table test_2 (`id` int, `lineid` int) stored as orc;

insert into table test_2 values (1,1), (7,1);

explain
select * from test_2 where struct(`id`, `lineid`)
IN (
struct(2,3),
struct(3,1),
struct(1,1),
struct(5,2),
struct(6,1),
struct(8,1),
struct(7,1),
struct(9,1),
struct(10,1)
);

select * from test_2 where struct(`id`, `lineid`)
IN (
struct(2,3),
struct(3,1),
struct(1,1),
struct(5,2),
struct(6,1),
struct(8,1),
struct(7,1),
struct(9,1),
struct(10,1)
);

explain
select `id`, `lineid`, struct(`id`, `lineid`)
IN (
struct(2,3),
struct(3,1),
struct(1,1),
struct(5,2),
struct(6,1),
struct(8,1),
struct(7,1),
struct(9,1),
struct(10,1)
) as b from test_2;

select `id`, `lineid`, struct(`id`, `lineid`)
IN (
struct(2,3),
struct(3,1),
struct(1,1),
struct(5,2),
struct(6,1),
struct(8,1),
struct(7,1),
struct(9,1),
struct(10,1)
) as b from test_2;

-- 1 String and 1 Integer
create table test_3 (`id` string, `lineid` int) stored as orc;

insert into table test_3 values ('one',1), ('seven',1);

explain
select * from test_3 where struct(`id`, `lineid`)
IN (
struct('two',3),
struct('three',1),
struct('one',1),
struct('five',2),
struct('six',1),
struct('eight',1),
struct('seven',1),
struct('nine',1),
struct('ten',1)
);

select * from test_3 where struct(`id`, `lineid`)
IN (
struct('two',3),
struct('three',1),
struct('one',1),
struct('five',2),
struct('six',1),
struct('eight',1),
struct('seven',1),
struct('nine',1),
struct('ten',1)
);

explain
select `id`, `lineid`, struct(`id`, `lineid`)
IN (
struct('two',3),
struct('three',1),
struct('one',1),
struct('five',2),
struct('six',1),
struct('eight',1),
struct('seven',1),
struct('nine',1),
struct('ten',1)
) as b from test_3;

select `id`, `lineid`, struct(`id`, `lineid`)
IN (
struct('two',3),
struct('three',1),
struct('one',1),
struct('five',2),
struct('six',1),
struct('eight',1),
struct('seven',1),
struct('nine',1),
struct('ten',1)
) as b from test_3;

-- 1 Integer and 1 String and 1 Double
create table test_4 (`my_bigint` bigint, `my_string` string, `my_double` double) stored as orc;

insert into table test_4 values (1, "b", 1.5), (1, "a", 0.5), (2, "b", 1.5);

explain
select * from test_4 where struct(`my_bigint`, `my_string`, `my_double`)
IN (
struct(1L, "a", 1.5),
struct(1L, "b", -0.5),
struct(3L, "b", 1.5),
struct(1L, "d", 1.5),
struct(1L, "c", 1.5),
struct(1L, "b", 2.5),
struct(1L, "b", 0.5),
struct(5L, "b", 1.5),
struct(1L, "a", 0.5),
struct(3L, "b", 1.5)
);

select * from test_4 where struct(`my_bigint`, `my_string`, `my_double`)
IN (
struct(1L, "a", 1.5),
struct(1L, "b", -0.5),
struct(3L, "b", 1.5),
struct(1L, "d", 1.5),
struct(1L, "c", 1.5),
struct(1L, "b", 2.5),
struct(1L, "b", 0.5),
struct(5L, "b", 1.5),
struct(1L, "a", 0.5),
struct(3L, "b", 1.5)
);

explain
select `my_bigint`, `my_string`, `my_double`, struct(`my_bigint`, `my_string`, `my_double`)
IN (
struct(1L, "a", 1.5),
struct(1L, "b", -0.5),
struct(3L, "b", 1.5),
struct(1L, "d", 1.5),
struct(1L, "c", 1.5),
struct(1L, "b", 2.5),
struct(1L, "b", 0.5),
struct(5L, "b", 1.5),
struct(1L, "a", 0.5),
struct(3L, "b", 1.5)
) as b from test_4;

select `my_bigint`, `my_string`, `my_double`, struct(`my_bigint`, `my_string`, `my_double`)
IN (
struct(1L, "a", 1.5),
struct(1L, "b", -0.5),
struct(3L, "b", 1.5),
struct(1L, "d", 1.5),
struct(1L, "c", 1.5),
struct(1L, "b", 2.5),
struct(1L, "b", 0.5),
struct(5L, "b", 1.5),
struct(1L, "a", 0.5),
struct(3L, "b", 1.5)
) as b from test_4;set hive.cli.print.header=true;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;
set hive.mapred.mode=nonstrict;

explain
select 'key1', 'value1' from alltypesorc tablesample (1 rows);

select 'key1', 'value1' from alltypesorc tablesample (1 rows);


create table decimal_2 (t decimal(18,9)) stored as orc;

explain
insert overwrite table decimal_2
  select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows);

insert overwrite table decimal_2
  select cast('17.29' as decimal(4,2)) from alltypesorc tablesample (1 rows);

select count(*) from decimal_2;

drop table decimal_2;


-- Dummy tables HIVE-13190
explain
select count(1) from (select * from (Select 1 a) x order by x.a) y;

select count(1) from (select * from (Select 1 a) x order by x.a) y;

explain
create temporary table dual as select 1;

create temporary table dual as select 1;

select * from dual;SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;

drop table varchar_udf_1;

create table varchar_udf_1 (c1 string, c2 string, c3 varchar(10), c4 varchar(20)) STORED AS ORC;
insert overwrite table varchar_udf_1
  select key, value, key, value from src where key = '238' limit 1;

-- UDFs with varchar support
explain
select
  concat(c1, c2),
  concat(c3, c4),
  concat(c1, c2) = concat(c3, c4)
from varchar_udf_1 limit 1;

select
  concat(c1, c2),
  concat(c3, c4),
  concat(c1, c2) = concat(c3, c4)
from varchar_udf_1 limit 1;

explain
select
  upper(c2),
  upper(c4),
  upper(c2) = upper(c4)
from varchar_udf_1 limit 1;

select
  upper(c2),
  upper(c4),
  upper(c2) = upper(c4)
from varchar_udf_1 limit 1;

explain
select
  lower(c2),
  lower(c4),
  lower(c2) = lower(c4)
from varchar_udf_1 limit 1;

select
  lower(c2),
  lower(c4),
  lower(c2) = lower(c4)
from varchar_udf_1 limit 1;

-- Scalar UDFs
explain
select
  ascii(c2),
  ascii(c4),
  ascii(c2) = ascii(c4)
from varchar_udf_1 limit 1;

select
  ascii(c2),
  ascii(c4),
  ascii(c2) = ascii(c4)
from varchar_udf_1 limit 1;

explain
select
  concat_ws('|', c1, c2),
  concat_ws('|', c3, c4),
  concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
from varchar_udf_1 limit 1;

select
  concat_ws('|', c1, c2),
  concat_ws('|', c3, c4),
  concat_ws('|', c1, c2) = concat_ws('|', c3, c4)
from varchar_udf_1 limit 1;

explain
select
  decode(encode(c2, 'US-ASCII'), 'US-ASCII'),
  decode(encode(c4, 'US-ASCII'), 'US-ASCII'),
  decode(encode(c2, 'US-ASCII'), 'US-ASCII') = decode(encode(c4, 'US-ASCII'), 'US-ASCII')
from varchar_udf_1 limit 1;

select
  decode(encode(c2, 'US-ASCII'), 'US-ASCII'),
  decode(encode(c4, 'US-ASCII'), 'US-ASCII'),
  decode(encode(c2, 'US-ASCII'), 'US-ASCII') = decode(encode(c4, 'US-ASCII'), 'US-ASCII')
from varchar_udf_1 limit 1;

explain
select
  instr(c2, '_'),
  instr(c4, '_'),
  instr(c2, '_') = instr(c4, '_')
from varchar_udf_1 limit 1;

select
  instr(c2, '_'),
  instr(c4, '_'),
  instr(c2, '_') = instr(c4, '_')
from varchar_udf_1 limit 1;

explain
select
  length(c2),
  length(c4),
  length(c2) = length(c4)
from varchar_udf_1 limit 1;

select
  length(c2),
  length(c4),
  length(c2) = length(c4)
from varchar_udf_1 limit 1;

explain
select
  locate('a', 'abcdabcd', 3),
  locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3),
  locate('a', 'abcdabcd', 3) = locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3)
from varchar_udf_1 limit 1;

select
  locate('a', 'abcdabcd', 3),
  locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3),
  locate('a', 'abcdabcd', 3) = locate(cast('a' as varchar(1)), cast('abcdabcd' as varchar(10)), 3)
from varchar_udf_1 limit 1;

explain
select
  lpad(c2, 15, ' '),
  lpad(c4, 15, ' '),
  lpad(c2, 15, ' ') = lpad(c4, 15, ' ')
from varchar_udf_1 limit 1;

select
  lpad(c2, 15, ' '),
  lpad(c4, 15, ' '),
  lpad(c2, 15, ' ') = lpad(c4, 15, ' ')
from varchar_udf_1 limit 1;

explain
select
  ltrim(c2),
  ltrim(c4),
  ltrim(c2) = ltrim(c4)
from varchar_udf_1 limit 1;

select
  ltrim(c2),
  ltrim(c4),
  ltrim(c2) = ltrim(c4)
from varchar_udf_1 limit 1;

explain
select
  c2 regexp 'val',
  c4 regexp 'val',
  (c2 regexp 'val') = (c4 regexp 'val')
from varchar_udf_1 limit 1;

select
  c2 regexp 'val',
  c4 regexp 'val',
  (c2 regexp 'val') = (c4 regexp 'val')
from varchar_udf_1 limit 1;

explain
select
  regexp_extract(c2, 'val_([0-9]+)', 1),
  regexp_extract(c4, 'val_([0-9]+)', 1),
  regexp_extract(c2, 'val_([0-9]+)', 1) = regexp_extract(c4, 'val_([0-9]+)', 1)
from varchar_udf_1 limit 1;

select
  regexp_extract(c2, 'val_([0-9]+)', 1),
  regexp_extract(c4, 'val_([0-9]+)', 1),
  regexp_extract(c2, 'val_([0-9]+)', 1) = regexp_extract(c4, 'val_([0-9]+)', 1)
from varchar_udf_1 limit 1;

explain
select
  regexp_replace(c2, 'val', 'replaced'),
  regexp_replace(c4, 'val', 'replaced'),
  regexp_replace(c2, 'val', 'replaced') = regexp_replace(c4, 'val', 'replaced')
from varchar_udf_1 limit 1;

select
  regexp_replace(c2, 'val', 'replaced'),
  regexp_replace(c4, 'val', 'replaced'),
  regexp_replace(c2, 'val', 'replaced') = regexp_replace(c4, 'val', 'replaced')
from varchar_udf_1 limit 1;

explain
select
  reverse(c2),
  reverse(c4),
  reverse(c2) = reverse(c4)
from varchar_udf_1 limit 1;

select
  reverse(c2),
  reverse(c4),
  reverse(c2) = reverse(c4)
from varchar_udf_1 limit 1;

explain
select
  rpad(c2, 15, ' '),
  rpad(c4, 15, ' '),
  rpad(c2, 15, ' ') = rpad(c4, 15, ' ')
from varchar_udf_1 limit 1;

select
  rpad(c2, 15, ' '),
  rpad(c4, 15, ' '),
  rpad(c2, 15, ' ') = rpad(c4, 15, ' ')
from varchar_udf_1 limit 1;

explain
select
  rtrim(c2),
  rtrim(c4),
  rtrim(c2) = rtrim(c4)
from varchar_udf_1 limit 1;

select
  rtrim(c2),
  rtrim(c4),
  rtrim(c2) = rtrim(c4)
from varchar_udf_1 limit 1;

explain
select
  sentences('See spot run.  See jane run.'),
  sentences(cast('See spot run.  See jane run.' as varchar(50)))
from varchar_udf_1 limit 1;

select
  sentences('See spot run.  See jane run.'),
  sentences(cast('See spot run.  See jane run.' as varchar(50)))
from varchar_udf_1 limit 1;

explain
select
  split(c2, '_'),
  split(c4, '_')
from varchar_udf_1 limit 1;

select
  split(c2, '_'),
  split(c4, '_')
from varchar_udf_1 limit 1;

explain
select
  str_to_map('a:1,b:2,c:3',',',':'),
  str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
from varchar_udf_1 limit 1;

select
  str_to_map('a:1,b:2,c:3',',',':'),
  str_to_map(cast('a:1,b:2,c:3' as varchar(20)),',',':')
from varchar_udf_1 limit 1;

explain
select
  substr(c2, 1, 3),
  substr(c4, 1, 3),
  substr(c2, 1, 3) = substr(c4, 1, 3)
from varchar_udf_1 limit 1;

select
  substr(c2, 1, 3),
  substr(c4, 1, 3),
  substr(c2, 1, 3) = substr(c4, 1, 3)
from varchar_udf_1 limit 1;

explain
select
  trim(c2),
  trim(c4),
  trim(c2) = trim(c4)
from varchar_udf_1 limit 1;

select
  trim(c2),
  trim(c4),
  trim(c2) = trim(c4)
from varchar_udf_1 limit 1;


-- Aggregate Functions
explain
select
  compute_stats(c2, 16),
  compute_stats(c4, 16)
from varchar_udf_1;

select
  compute_stats(c2, 16),
  compute_stats(c4, 16)
from varchar_udf_1;

explain
select
  min(c2),
  min(c4)
from varchar_udf_1;

select
  min(c2),
  min(c4)
from varchar_udf_1;

explain
select
  max(c2),
  max(c4)
from varchar_udf_1;

select
  max(c2),
  max(c4)
from varchar_udf_1;

drop table varchar_udf_1;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;

drop table if exists vectortab2k;
drop table if exists vectortab2korc;

create table vectortab2k(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '|'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/vectortab2k' OVERWRITE INTO TABLE vectortab2k;

create table vectortab2korc(
            t tinyint,
            si smallint,
            i int,
            b bigint,
            f float,
            d double,
            dc decimal(38,18),
            bo boolean,
            s string,
            s2 string,
            ts timestamp,
            ts2 timestamp,
            dt date)
STORED AS ORC;

INSERT INTO TABLE vectortab2korc SELECT * FROM vectortab2k;

drop table if exists varchar_lazy_binary_columnar;
create table varchar_lazy_binary_columnar(vt varchar(10), vsi varchar(10), vi varchar(20), vb varchar(30), vf varchar(20),vd varchar(20),vs varchar(50)) row format serde 'org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe' stored as rcfile;

explain
insert overwrite table varchar_lazy_binary_columnar select t, si, i, b, f, d, s from vectortab2korc;

-- insert overwrite table varchar_lazy_binary_columnar select t, si, i, b, f, d, s from vectortab2korc;

-- select count(*) as cnt from varchar_lazy_binary_columnar group by vs order by cnt asc;set hive.mapred.mode=nonstrict;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
set hive.fetch.task.conversion=none;
SET hive.auto.convert.join=true;
SET hive.auto.convert.join.noconditionaltask=true;
SET hive.auto.convert.join.noconditionaltask.size=1000000000;

drop table if exists varchar_join1_vc1;
drop table if exists varchar_join1_vc2;
drop table if exists varchar_join1_str;
drop table if exists varchar_join1_vc1_orc;
drop table if exists varchar_join1_vc2_orc;
drop table if exists varchar_join1_str_orc;

create table  varchar_join1_vc1 (
  c1 int,
  c2 varchar(10)
);

create table  varchar_join1_vc2 (
  c1 int,
  c2 varchar(20)
);

create table  varchar_join1_str (
  c1 int,
  c2 string
);

load data local inpath '../../data/files/vc1.txt' into table varchar_join1_vc1;
load data local inpath '../../data/files/vc1.txt' into table varchar_join1_vc2;
load data local inpath '../../data/files/vc1.txt' into table varchar_join1_str;

create table varchar_join1_vc1_orc stored as orc as select * from varchar_join1_vc1;
create table varchar_join1_vc2_orc stored as orc as select * from varchar_join1_vc2;
create table varchar_join1_str_orc stored as orc as select * from varchar_join1_str;

-- Join varchar with same length varchar
explain select * from varchar_join1_vc1_orc a join varchar_join1_vc1_orc b on (a.c2 = b.c2) order by a.c1;
select * from varchar_join1_vc1_orc a join varchar_join1_vc1_orc b on (a.c2 = b.c2) order by a.c1;

-- Join varchar with different length varchar
explain select * from varchar_join1_vc1_orc a join varchar_join1_vc2_orc b on (a.c2 = b.c2) order by a.c1;
select * from varchar_join1_vc1_orc a join varchar_join1_vc2_orc b on (a.c2 = b.c2) order by a.c1;

-- Join varchar with string
explain select * from varchar_join1_vc1_orc a join varchar_join1_str_orc b on (a.c2 = b.c2) order by a.c1;
select * from varchar_join1_vc1_orc a join varchar_join1_str_orc b on (a.c2 = b.c2) order by a.c1;

drop table varchar_join1_vc1;
drop table varchar_join1_vc2;
drop table varchar_join1_str;
drop table varchar_join1_vc1_orc;
drop table varchar_join1_vc2_orc;
drop table varchar_join1_str_orc;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
drop table varchar_2;

create table varchar_2 (
  key varchar(10),
  value varchar(20)
) stored as orc;

insert overwrite table varchar_2 select * from src;

select key, value
from src
order by key asc
limit 5;

explain select key, value
from varchar_2
order by key asc
limit 5;

-- should match the query from src
select key, value
from varchar_2
order by key asc
limit 5;

select key, value
from src
order by key desc
limit 5;

explain select key, value
from varchar_2
order by key desc
limit 5;

-- should match the query from src
select key, value
from varchar_2
order by key desc
limit 5;

drop table varchar_2;

-- Implicit conversion.  Occurs in reduce-side under Tez.
create table varchar_3 (
  field varchar(25)
) stored as orc;

explain
insert into table varchar_3 select cint from alltypesorc limit 10;

insert into table varchar_3 select cint from alltypesorc limit 10;

drop table varchar_3;
set hive.explain.user=false;
SET hive.vectorized.execution.enabled=true;
SET hive.auto.convert.join=true;
set hive.fetch.task.conversion=none;

-- SORT_QUERY_RESULTS

create table count_case_groupby (key string, bool boolean) STORED AS orc;
insert into table count_case_groupby values ('key1', true),('key2', false),('key3', NULL),('key4', false),('key5',NULL);

explain
SELECT key, COUNT(CASE WHEN bool THEN 1 WHEN NOT bool THEN 0 ELSE NULL END) AS cnt_bool0_ok FROM count_case_groupby GROUP BY key;

SELECT key, COUNT(CASE WHEN bool THEN 1 WHEN NOT bool THEN 0 ELSE NULL END) AS cnt_bool0_ok FROM count_case_groupby GROUP BY key;CREATE DATABASE db1;
USE db1;

CREATE TABLE table1 (key STRING, value STRING)
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv1.txt'
OVERWRITE INTO TABLE table1;

CREATE TABLE table2 (key STRING, value STRING)
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '../../data/files/kv1.txt'
OVERWRITE INTO TABLE table2;

-- relative reference, no alias
CREATE VIEW v1 AS SELECT * FROM table1;

-- relative reference, aliased
CREATE VIEW v2 AS SELECT t1.* FROM table1 t1;

-- relative reference, multiple tables
CREATE VIEW v3 AS SELECT t1.*, t2.key k FROM table1 t1 JOIN table2 t2 ON t1.key = t2.key;

-- absolute reference, no alias
CREATE VIEW v4 AS SELECT * FROM db1.table1;

-- absolute reference, aliased
CREATE VIEW v5 AS SELECT t1.* FROM db1.table1 t1;

-- absolute reference, multiple tables
CREATE VIEW v6 AS SELECT t1.*, t2.key k FROM db1.table1 t1 JOIN db1.table2 t2 ON t1.key = t2.key;

-- relative reference, explicit column
CREATE VIEW v7 AS SELECT key from table1;

-- absolute reference, explicit column
CREATE VIEW v8 AS SELECT key from db1.table1;

CREATE DATABASE db2;
USE db2;

SELECT * FROM db1.v1;
SELECT * FROM db1.v2;
SELECT * FROM db1.v3;
SELECT * FROM db1.v4;
SELECT * FROM db1.v5;
SELECT * FROM db1.v6;
SELECT * FROM db1.v7;
SELECT * FROM db1.v8;

DROP TABLE IF EXISTS atab;
CREATE TABLE IF NOT EXISTS atab (ks_uid BIGINT, sr_uid STRING, sr_id STRING, tstamp STRING, m_id STRING, act STRING, at_sr_uid STRING, tstamp_type STRING, original_m_id STRING, original_tstamp STRING, registered_flag TINYINT, at_ks_uid BIGINT) PARTITIONED BY (dt STRING,nt STRING);
LOAD DATA LOCAL INPATH '../../data/files/v1.txt' INTO TABLE atab PARTITION (dt='20130312', nt='tw');
LOAD DATA LOCAL INPATH '../../data/files/v1.txt' INTO TABLE atab PARTITION (dt='20130311', nt='tw');

DROP TABLE IF EXISTS  mstab;
CREATE TABLE  mstab(ks_uid INT, csc INT) PARTITIONED BY (dt STRING);
LOAD DATA LOCAL INPATH '../../data/files/v2.txt' INTO TABLE mstab PARTITION (dt='20130311');

DROP VIEW IF EXISTS aa_view_tw;
CREATE VIEW aa_view_tw AS SELECT ks_uid, sr_id, act, at_ks_uid, at_sr_uid, from_unixtime(CAST(CAST( tstamp as BIGINT)/1000 AS BIGINT),'yyyyMMdd') AS act_date, from_unixtime(CAST(CAST( original_tstamp AS BIGINT)/1000 AS BIGINT),'yyyyMMdd') AS content_creation_date FROM atab WHERE dt='20130312' AND nt='tw' AND ks_uid != at_ks_uid;

DROP VIEW IF EXISTS joined_aa_view_tw;
CREATE VIEW joined_aa_view_tw AS SELECT aa.ks_uid, aa.sr_id, aa.act, at_sr_uid, aa.act_date, aa.at_ks_uid, aa.content_creation_date, coalesce( other.ksc, 10.0) AS at_ksc, coalesce( self.ksc , 10.0 ) AS self_ksc FROM aa_view_tw aa LEFT OUTER JOIN ( SELECT ks_uid, csc AS ksc FROM mstab WHERE dt='20130311' ) self ON ( CAST(aa.ks_uid AS BIGINT) = CAST(self.ks_uid AS BIGINT) ) LEFT OUTER JOIN ( SELECT ks_uid, csc AS ksc FROM mstab WHERE dt='20130311' ) other ON ( CAST(aa.at_ks_uid AS BIGINT) = CAST(other.ks_uid AS BIGINT) );

SELECT * FROM joined_aa_view_tw;
-- Tests that selecting from a view and another view that selects from that same view

CREATE VIEW test_view1 AS SELECT * FROM src;

CREATE VIEW test_view2 AS SELECT * FROM test_view1;

SELECT COUNT(*) FROM test_view1 a JOIN test_view2 b ON a.key = b.key;
set hive.mapred.mode=nonstrict;
select INPUT__FILE__NAME, key, BLOCK__OFFSET__INSIDE__FILE from src;

select key, count(INPUT__FILE__NAME) from src group by key order by key;

select INPUT__FILE__NAME, key, collect_set(BLOCK__OFFSET__INSIDE__FILE) from src group by INPUT__FILE__NAME, key order by key;

select * from src where BLOCK__OFFSET__INSIDE__FILE > 12000 order by key;

select * from src where BLOCK__OFFSET__INSIDE__FILE > 5800 order by key;


CREATE TABLE src_index_test_rc (key int, value string) STORED AS RCFILE;

set hive.io.rcfile.record.buffer.size = 1024;
INSERT OVERWRITE TABLE src_index_test_rc SELECT * FROM src;
select INPUT__FILE__NAME, key, BLOCK__OFFSET__INSIDE__FILE from src_index_test_rc order by key;
set hive.mapred.mode=nonstrict;
set mapred.reduce.tasks=4;
-- SORT_QUERY_RESULTS

-- 1. testWindowing
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1
from part
;

-- 2. testGroupByWithPartitioning
select p_mfgr, p_name, p_size,
min(p_retailprice),
rank() over(distribute by p_mfgr sort by p_name)as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
group by p_mfgr, p_name, p_size
;

-- 3. testGroupByHavingWithSWQ
select p_mfgr, p_name, p_size, min(p_retailprice),
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
group by p_mfgr, p_name, p_size
having p_size > 0
;

-- 4. testCount
select p_mfgr, p_name,
count(p_size) over(distribute by p_mfgr sort by p_name) as cd
from part
;

-- 5. testCountWithWindowingUDAF
select p_mfgr, p_name,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
count(p_size) over(distribute by p_mfgr sort by p_name) as cd,
p_retailprice, sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
;

-- 6. testCountInSubQ
select sub1.r, sub1.dr, sub1.cd, sub1.s1, sub1.deltaSz
from (select p_mfgr, p_name,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
count(p_size) over(distribute by p_mfgr sort by p_name) as cd,
p_retailprice, sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
) sub1;

-- 7. testJoinWithWindowingAndPTF
select abc.p_mfgr, abc.p_name,
rank() over(distribute by abc.p_mfgr sort by abc.p_name) as r,
dense_rank() over(distribute by abc.p_mfgr sort by abc.p_name) as dr,
abc.p_retailprice, sum(abc.p_retailprice) over (distribute by abc.p_mfgr sort by abc.p_name rows between unbounded preceding and current row) as s1,
abc.p_size, abc.p_size - lag(abc.p_size,1,abc.p_size) over(distribute by abc.p_mfgr sort by abc.p_name) as deltaSz
from noop(on part
partition by p_mfgr
order by p_name
) abc join part p1 on abc.p_partkey = p1.p_partkey
;

-- 8. testMixedCaseAlias
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name, p_size desc) as R
from part
;

-- 9. testHavingWithWindowingNoGBY
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s1
from part
;

-- 10. testHavingWithWindowingCondRankNoGBY
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1
from part
;

-- 11. testFirstLast
select  p_mfgr,p_name, p_size,
sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2,
first_value(p_size) over w1  as f,
last_value(p_size, false) over w1  as l
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);

-- 12. testFirstLastWithWhere
select  p_mfgr,p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2,
first_value(p_size) over w1 as f,
last_value(p_size, false) over w1 as l
from part
where p_mfgr = 'Manufacturer#3'
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);

-- 13. testSumWindow
select  p_mfgr,p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over (distribute by p_mfgr  sort by p_name rows between current row and current row)  as s2
from part
window w1 as (distribute by p_mfgr  sort by p_name rows between 2 preceding and 2 following);

-- 14. testNoSortClause
select  p_mfgr,p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r, dense_rank() over(distribute by p_mfgr sort by p_name) as dr
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);

-- 15. testExpressions
select  p_mfgr,p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
cume_dist() over(distribute by p_mfgr sort by p_name) as cud,
percent_rank() over(distribute by p_mfgr sort by p_name) as pr,
ntile(3) over(distribute by p_mfgr sort by p_name) as nt,
count(p_size) over(distribute by p_mfgr sort by p_name) as ca,
avg(p_size) over(distribute by p_mfgr sort by p_name) as avg,
stddev(p_size) over(distribute by p_mfgr sort by p_name) as st,
first_value(p_size % 5) over(distribute by p_mfgr sort by p_name) as fv,
last_value(p_size) over(distribute by p_mfgr sort by p_name) as lv,
first_value(p_size) over w1  as fvW1
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 16. testMultipleWindows
select  p_mfgr,p_name, p_size,
  rank() over(distribute by p_mfgr sort by p_name) as r,
  dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
cume_dist() over(distribute by p_mfgr sort by p_name) as cud,
sum(p_size) over (distribute by p_mfgr sort by p_name range between unbounded preceding and current row) as s1,
sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row) as s2,
first_value(p_size) over w1  as fv1
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 17. testCountStar
select  p_mfgr,p_name, p_size,
count(*) over(distribute by p_mfgr sort by p_name ) as c,
count(p_size) over(distribute by p_mfgr sort by p_name) as ca,
first_value(p_size) over w1  as fvW1
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 18. testUDAFs
select  p_mfgr,p_name, p_size,
sum(p_retailprice) over w1 as s,
min(p_retailprice) over w1 as mi,
max(p_retailprice) over w1 as ma,
avg(p_retailprice) over w1 as ag
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 19. testUDAFsWithGBY
select  p_mfgr,p_name, p_size, p_retailprice,
sum(p_retailprice) over w1 as s,
min(p_retailprice) as mi ,
max(p_retailprice) as ma ,
avg(p_retailprice) over w1 as ag
from part
group by p_mfgr,p_name, p_size, p_retailprice
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 20. testSTATs
select  p_mfgr,p_name, p_size,
stddev(p_retailprice) over w1 as sdev,
stddev_pop(p_retailprice) over w1 as sdev_pop,
collect_set(p_size) over w1 as uniq_size,
variance(p_retailprice) over w1 as var,
corr(p_size, p_retailprice) over w1 as cor,
covar_pop(p_size, p_retailprice) over w1 as covarp
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 21. testDISTs
select  p_mfgr,p_name, p_size,
histogram_numeric(p_retailprice, 5) over w1 as hist,
percentile(p_partkey, 0.5) over w1 as per,
row_number() over(distribute by p_mfgr sort by p_mfgr, p_name) as rn
from part
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

-- 22. testViewAsTableInputWithWindowing
create view IF NOT EXISTS mfgr_price_view as
select p_mfgr, p_brand,
round(sum(p_retailprice),2) as s
from part
group by p_mfgr, p_brand;

select *
from (
select p_mfgr, p_brand, s,
round(sum(s) over w1 , 2)  as s1
from mfgr_price_view
window w1 as (distribute by p_mfgr sort by p_mfgr )
) sq
order by p_mfgr, p_brand;

select p_mfgr, p_brand, s,
round(sum(s) over w1 ,2)  as s1
from mfgr_price_view
window w1 as (distribute by p_mfgr sort by p_brand rows between 2 preceding and current row);

-- 23. testCreateViewWithWindowingQuery
create view IF NOT EXISTS mfgr_brand_price_view as
select p_mfgr, p_brand,
sum(p_retailprice) over w1  as s
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and current row);

select * from mfgr_brand_price_view;

-- 24. testLateralViews
select p_mfgr, p_name,
lv_col, p_size, sum(p_size) over w1   as s
from (select p_mfgr, p_name, p_size, array(1,2,3) arr from part) p
lateral view explode(arr) part_lv as lv_col
window w1 as (distribute by p_mfgr sort by p_size, lv_col rows between 2 preceding and current row);

-- 25. testMultipleInserts3SWQs
CREATE TABLE part_1(
p_mfgr STRING,
p_name STRING,
p_size INT,
r INT,
dr INT,
s DOUBLE);

CREATE TABLE part_2(
p_mfgr STRING,
p_name STRING,
p_size INT,
r INT,
dr INT,
cud INT,
s2 DOUBLE,
fv1 INT);

CREATE TABLE part_3(
p_mfgr STRING,
p_name STRING,
p_size INT,
c INT,
ca INT,
fv INT);

from part
INSERT OVERWRITE TABLE part_1
select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name ) as r,
dense_rank() over(distribute by p_mfgr sort by p_name ) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row)  as s
INSERT OVERWRITE TABLE part_2
select  p_mfgr,p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
cume_dist() over(distribute by p_mfgr sort by p_name) as cud,
round(sum(p_size) over (distribute by p_mfgr sort by p_size range between 5 preceding and current row),1) as s2,
first_value(p_size) over w1  as fv1
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following)
INSERT OVERWRITE TABLE part_3
select  p_mfgr,p_name, p_size,
count(*) over(distribute by p_mfgr sort by p_name) as c,
count(p_size) over(distribute by p_mfgr sort by p_name) as ca,
first_value(p_size) over w1  as fv
window w1 as (distribute by p_mfgr sort by p_mfgr, p_name rows between 2 preceding and 2 following);

select * from part_1;

select * from part_2;

select * from part_3;

-- 26. testGroupByHavingWithSWQAndAlias
select p_mfgr, p_name, p_size, min(p_retailprice) as mi,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
p_size, p_size - lag(p_size,1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
group by p_mfgr, p_name, p_size
having p_size > 0
;

-- 27. testMultipleRangeWindows
select  p_mfgr,p_name, p_size,
sum(p_size) over (distribute by p_mfgr sort by p_size range between 10 preceding and current row) as s2,
sum(p_size) over (distribute by p_mfgr sort by p_size range between current row and 10 following )  as s1
from part
window w1 as (rows between 2 preceding and 2 following);

-- 28. testPartOrderInUDAFInvoke
select p_mfgr, p_name, p_size,
sum(p_size) over (partition by p_mfgr  order by p_name  rows between 2 preceding and 2 following) as s
from part;

-- 29. testPartOrderInWdwDef
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s
from part
window w1 as (partition by p_mfgr  order by p_name  rows between 2 preceding and 2 following);

-- 30. testDefaultPartitioningSpecRules
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s,
sum(p_size) over w2 as s2
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following),
       w2 as (partition by p_mfgr order by p_name);

-- 31. testWindowCrossReference
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over w2 as s2
from part
window w1 as (partition by p_mfgr order by p_name range between 2 preceding and 2 following),
       w2 as w1;


-- 32. testWindowInheritance
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over w2 as s2
from part
window w1 as (partition by p_mfgr order by p_name range between 2 preceding and 2 following),
       w2 as (w1 rows between unbounded preceding and current row);


-- 33. testWindowForwardReference
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over w2 as s2,
sum(p_size) over w3 as s3
from part
window w1 as (distribute by p_mfgr sort by p_name range between 2 preceding and 2 following),
       w2 as w3,
       w3 as (distribute by p_mfgr sort by p_name range between unbounded preceding and current row);


-- 34. testWindowDefinitionPropagation
select p_mfgr, p_name, p_size,
sum(p_size) over w1 as s1,
sum(p_size) over w2 as s2,
sum(p_size) over (w3 rows between 2 preceding and 2 following)  as s3
from part
window w1 as (distribute by p_mfgr sort by p_name range between 2 preceding and 2 following),
       w2 as w3,
       w3 as (distribute by p_mfgr sort by p_name range between unbounded preceding and current row);

-- 35. testDistinctWithWindowing
select DISTINCT p_mfgr, p_name, p_size,
sum(p_size) over w1 as s
from part
window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);

-- 36. testRankWithPartitioning
select p_mfgr, p_name, p_size,
rank() over (partition by p_mfgr order by p_name )  as r
from part;

-- 37. testPartitioningVariousForms
select p_mfgr,
round(sum(p_retailprice) over (partition by p_mfgr order by p_mfgr),2) as s1,
min(p_retailprice) over (partition by p_mfgr) as s2,
max(p_retailprice) over (distribute by p_mfgr sort by p_mfgr) as s3,
round(avg(p_retailprice) over (distribute by p_mfgr),2) as s4,
count(p_retailprice) over (cluster by p_mfgr ) as s5
from part;

-- 38. testPartitioningVariousForms2
select p_mfgr, p_name, p_size,
sum(p_retailprice) over (partition by p_mfgr, p_name order by p_mfgr, p_name rows between unbounded preceding and current row) as s1,
min(p_retailprice) over (distribute by p_mfgr, p_name sort by p_mfgr, p_name rows between unbounded preceding and current row) as s2,
max(p_retailprice) over (partition by p_mfgr, p_name order by p_name) as s3
from part;

-- 39. testUDFOnOrderCols
select p_mfgr, p_type, substr(p_type, 2) as short_ptype,
rank() over (partition by p_mfgr order by substr(p_type, 2))  as r
from part;

-- 40. testNoBetweenForRows
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows unbounded preceding) as s1
     from part ;

-- 41. testNoBetweenForRange
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (distribute by p_mfgr sort by p_size range unbounded preceding) as s1
     from part ;

-- 42. testUnboundedFollowingForRows
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between current row and unbounded following) as s1
    from part ;

-- 43. testUnboundedFollowingForRange
select p_mfgr, p_name, p_size,
    sum(p_retailprice) over (distribute by p_mfgr sort by p_size range between current row and unbounded following) as s1
    from part ;

-- 44. testOverNoPartitionSingleAggregate
select p_name, p_retailprice,
round(avg(p_retailprice) over(),2)
from part
order by p_name;

-- 45. empty partition test
select p_mfgr,
  sum(p_size) over (partition by p_mfgr order by p_size rows between unbounded preceding and current row)
from part
where p_mfgr = 'Manufacturer#6'
;

-- 46. window sz is same as partition sz
select p_retailprice, avg(p_retailprice) over (partition by p_mfgr order by p_name rows between current row and 6 following),
sum(p_retailprice) over (partition by p_mfgr order by p_name rows between current row and 6 following)
from part
where p_mfgr='Manufacturer#1';

-- 47. empty partition
select sum(p_size) over (partition by p_mfgr )
from part where p_mfgr = 'm1';
set hive.join.cache.size=1;

select p_mfgr, p_name, p_size,
rank() over(distribute by p_mfgr sort by p_name) as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1
from part
;

set hive.join.cache.size=25000;-- 1. testQueryLevelPartitionColsNotInSelect
select p_size,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1
from part
 ;

-- 2. testWindowPartitionColsNotInSelect
select p_size,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1
from part;

-- 3. testHavingColNotInSelect
select p_mfgr,
sum(p_retailprice) over (distribute by p_mfgr sort by p_name rows between unbounded preceding and current row) as s1
from part;
create table part_dec(
    p_partkey INT,
    p_name STRING,
    p_mfgr STRING,
    p_brand STRING,
    p_type STRING,
    p_size INT,
    p_container STRING,
    p_retailprice decimal(10,3),
    p_comment STRING
);

insert overwrite table part_dec select * from part;

select p_mfgr, p_retailprice,
first_value(p_retailprice) over(partition by p_mfgr order by p_retailprice) ,
sum(p_retailprice) over(partition by p_mfgr order by p_retailprice)
from part_dec;

select p_mfgr, p_retailprice,
first_value(p_retailprice) over(partition by p_mfgr order by p_retailprice range between 5 preceding and current row) ,
sum(p_retailprice) over(partition by p_mfgr order by p_retailprice range between 5 preceding and current row)
from part_dec;drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select p_mfgr, p_retailprice, p_size,
round(sum(p_retailprice) over w1 , 2) = round(sum(lag(p_retailprice,1,0.0)) over w1 + last_value(p_retailprice) over w1 , 2),
max(p_retailprice) over w1 - min(p_retailprice) over w1 = last_value(p_retailprice) over w1 - first_value(p_retailprice) over w1
from part
window w1 as (distribute by p_mfgr sort by p_retailprice)
;
select p_mfgr, p_retailprice, p_size,
rank() over (distribute by p_mfgr sort by p_retailprice) as r,
sum(p_retailprice) over (distribute by p_mfgr sort by p_retailprice rows between unbounded preceding and current row) as s2,
sum(p_retailprice) over (distribute by p_mfgr sort by p_retailprice rows between unbounded preceding and current row) -5 as s1
from part
;

select s, si, f, si - lead(f, 3) over (partition by t order by bo,s,si,f desc) from over10k limit 100;
select s, i, i - lead(i, 3, 0) over (partition by si order by i,s) from over10k limit 100;
select s, si, d, si - lag(d, 3) over (partition by b order by si,s,d) from over10k limit 100;
select s, lag(s, 3, 'fred') over (partition by f order by b) from over10k limit 100;

select p_mfgr, avg(p_retailprice) over(partition by p_mfgr, p_type order by p_mfgr) from part;

select p_mfgr, avg(p_retailprice) over(partition by p_mfgr order by p_type,p_mfgr rows between unbounded preceding and current row) from part;

-- multi table insert test
create table t1 (a1 int, b1 string);
create table t2 (a1 int, b1 string);
from (select sum(i) over (partition by ts order by i), s from over10k) tt insert overwrite table t1 select * insert overwrite table t2 select * ;
select * from t1 limit 3;
select * from t2 limit 3;

select p_mfgr, p_retailprice, p_size,
round(sum(p_retailprice) over w1 , 2) + 50.0 = round(sum(lag(p_retailprice,1,50.0)) over w1 + (last_value(p_retailprice) over w1),2)
from part
window w1 as (distribute by p_mfgr sort by p_retailprice)
limit 11;
set hive.mapred.mode=nonstrict;
explain
       select rank() over (order by return_ratio) as return_rank from
       (select sum(wr.cint)/sum(ws.c_int)  as return_ratio
                 from cbo_t3  ws join alltypesorc wr on ws.value = wr.cstring1
                  group by ws.c_boolean ) in_web
;
select nonexistfunc(key) over () from src limit 1;
select sum(lead(p_retailprice,1)) as s1  from part;
select p_mfgr, p_name, p_size,
min(p_retailprice),
rank() over(distribute by p_mfgr sort by p_name)as r,
dense_rank() over(distribute by p_mfgr sort by p_name) as dr,
p_size, p_size - lag(p_size,-1,p_size) over(distribute by p_mfgr sort by p_name) as deltaSz
from part
group by p_mfgr, p_name, p_size
;
select p_mfgr,
lead(p_retailprice,1) as s1
from part;
drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select s, rank() over (partition by s order by si), sum(b) over (partition by s order by si) from over10k limit 100;

select s,
rank() over (partition by s order by dec desc),
sum(b) over (partition by s order by ts desc)
from over10k
where s = 'tom allen' or s = 'bob steinbeck';

select s, sum(i) over (partition by s), sum(f) over (partition by si) from over10k where s = 'tom allen' or s = 'bob steinbeck' ;

select s, rank() over (partition by s order by bo), rank() over (partition by si order by bin desc) from over10k
where s = 'tom allen' or s = 'bob steinbeck';

select s, sum(f) over (partition by i), row_number() over (order by f) from over10k where s = 'tom allen' or s = 'bob steinbeck';

select s, rank() over w1,
rank() over w2
from over10k
where s = 'tom allen' or s = 'bob steinbeck'
window
w1 as (partition by s order by dec),
w2 as (partition by si order by f)
;
drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
           ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select row_number() over()  from src where key = '238';

select s, row_number() over (partition by d order by dec) from over10k limit 100;

select i, lead(s) over (partition by bin order by d,i desc) from over10k limit 100;

select i, lag(dec) over (partition by i order by s,i,dec) from over10k limit 100;

select s, last_value(t) over (partition by d order by f) from over10k limit 100;

select s, first_value(s) over (partition by bo order by s) from over10k limit 100;

select t, s, i, last_value(i) over (partition by t order by s)
from over10k where (s = 'oscar allen' or s = 'oscar carson') and t = 10;

drop table if exists wtest;
create table wtest as
select a, b
from
(
SELECT explode(
   map(
   3, array(1,2,3,4,5),
   1, array(int(null),int(null),int(null), int(null), int(null)),
   2, array(1,null,2, null, 3)
   )
  ) as (a,barr) FROM (select * from src limit 1) s
  ) s1 lateral view explode(barr) arr as b;

select a, b,
first_value(b) over (partition by a order by b rows between 1 preceding and 1 following ) ,
first_value(b, true) over (partition by a order by b rows between 1 preceding and 1 following ) ,
first_value(b) over (partition by a order by b rows between unbounded preceding and 1 following ) ,
first_value(b, true) over (partition by a order by b rows between unbounded preceding and 1 following )
from wtest;


select a, b,
first_value(b) over (partition by a order by b desc  rows between 1 preceding and 1 following ) ,
first_value(b, true) over (partition by a order by b desc rows between 1 preceding and 1 following ) ,
first_value(b) over (partition by a order by b desc rows between unbounded preceding and 1 following ) ,
first_value(b, true) over (partition by a order by b desc rows between unbounded preceding and 1 following )
from wtest;

select a, b,
last_value(b) over (partition by a order by b rows between 1 preceding and 1 following ) ,
last_value(b, true) over (partition by a order by b rows between 1 preceding and 1 following ) ,
last_value(b) over (partition by a order by b rows between unbounded preceding and 1 following ) ,
last_value(b, true) over (partition by a order by b rows between unbounded preceding and 1 following )
from wtest;

select a, b,
last_value(b) over (partition by a order by b desc  rows between 1 preceding and 1 following ) ,
last_value(b, true) over (partition by a order by b desc rows between 1 preceding and 1 following ) ,
last_value(b) over (partition by a order by b desc rows between unbounded preceding and 1 following ) ,
last_value(b, true) over (partition by a order by b desc rows between unbounded preceding and 1 following )
from wtest;
drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select i, ntile(10) over (partition by s order by i) from over10k limit 100;

select s, ntile(100) over (partition by i order by s) from over10k limit 100;

select f, ntile(4) over (partition by d order by f) from over10k limit 100;

select d, ntile(1000) over (partition by dec order by d) from over10k limit 100;


drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select s, rank() over (partition by f order by t) from over10k limit 100;

select s, dense_rank() over (partition by ts order by i,s desc) from over10k limit 100;

select s, cume_dist() over (partition by bo order by b,s) from over10k limit 100;

select s, percent_rank() over (partition by dec order by f) from over10k limit 100;

-- If following tests fail, look for the comments in class PTFPPD::process()

select ts, dec, rnk
from
  (select ts, dec,
          rank() over (partition by ts order by dec)  as rnk
          from
            (select other.ts, other.dec
             from over10k other
             join over10k on (other.b = over10k.b)
            ) joined
  ) ranked
where rnk =  1 limit 10;

select ts, dec, rnk
from
  (select ts, dec,
          rank() over (partition by ts)  as rnk
          from
            (select other.ts, other.dec
             from over10k other
             join over10k on (other.b = over10k.b)
            ) joined
  ) ranked
where dec = 89.5 limit 10;

select ts, dec, rnk
from
  (select ts, dec,
          rank() over (partition by ts order by dec)  as rnk
          from
            (select other.ts, other.dec
             from over10k other
             join over10k on (other.b = over10k.b)
             where other.t < 10
            ) joined
  ) ranked
where rnk = 1 limit 10;

drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal(4,2),
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

set hive.limit.pushdown.memory.usage=.8;

-- part tests
explain
select *
from ( select p_mfgr, rank() over(partition by p_mfgr order by p_name) r from part) a
;

explain
select *
from ( select p_mfgr, rank() over(partition by p_mfgr order by p_name) r from part) a
where r < 4;

select *
from ( select p_mfgr, rank() over(partition by p_mfgr order by p_name) r from part) a
where r < 4;

select *
from ( select p_mfgr, rank() over(partition by p_mfgr order by p_name) r from part) a
where r < 2;

-- over10k tests
select *
from (select t, f, rank() over(partition by t order by f) r from over10k) a
where r < 6 and t < 5;

set hive.vectorized.execution.enabled=false;
set hive.limit.pushdown.memory.usage=0.8;

explain
select * from (select ctinyint, cdouble, rank() over(partition by ctinyint order by cdouble) r from  alltypesorc) a where r < 5;

drop table if exists sB;
create table sB ROW FORMAT DELIMITED FIELDS TERMINATED BY ','  STORED AS TEXTFILE as
select * from (select ctinyint, cdouble, rank() over(partition by ctinyint order by cdouble) r from  alltypesorc) a where r < 5;

select * from sB
where ctinyint is null;

set hive.vectorized.execution.enabled=true;
set hive.limit.pushdown.memory.usage=0.8;
drop table if exists sD;
create table sD ROW FORMAT DELIMITED FIELDS TERMINATED BY ','  STORED AS TEXTFILE as
select * from (select ctinyint, cdouble, rank() over(partition by ctinyint order by cdouble) r from  alltypesorc) a where r < 5;

select * from sD
where ctinyint is null;drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal,
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select s, min(i) over (partition by s) from over10k limit 100;

select s, avg(f) over (partition by si order by s) from over10k limit 100;

select s, avg(i) over (partition by t, b order by s) from over10k limit 100;

select max(i) over w from over10k window w as (partition by f) limit 100;

select s, avg(d) over (partition by t order by f) from over10k limit 100;

select key, max(value) over
  (order by key rows between 10 preceding and 20 following)
from src1 where length(key) > 10;-- user-added aggregates should be usable as windowing functions
create temporary function mysum as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum';

select sum(key) over (), mysum(key) over () from src limit 1;
drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal,
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

select s, sum(b) over (partition by i order by s,b rows unbounded preceding) from over10k limit 100;

select s, sum(f) over (partition by d order by s,f rows unbounded preceding) from over10k limit 100;

select s, sum(f) over (partition by ts order by f range between current row and unbounded following) from over10k limit 100;

select s, avg(f) over (partition by ts order by s,f rows between current row and 5 following) from over10k limit 100;

select s, avg(d) over (partition by t order by s,d desc rows between 5 preceding and 5 following) from over10k limit 100;

select s, sum(i) over(partition by ts order by s) from over10k limit 100;

select f, sum(f) over (partition by ts order by f range between unbounded preceding and current row) from over10k limit 100;

select s, i, round(avg(d) over (partition by s order by i) / 10.0 , 2) from over10k limit 7;

select s, i, round((avg(d) over  w1 + 10.0) - (avg(d) over w1 - 10.0),2) from over10k window w1 as (partition by s order by i) limit 7;

set hive.cbo.enable=false;
-- HIVE-9228
select s, i from ( select s, i, round((avg(d) over  w1 + 10.0) - (avg(d) over w1 - 10.0),2) from over10k window w1 as (partition by s order by i)) X limit 7;
drop table over10k;

create table over10k(
           t tinyint,
           si smallint,
           i int,
           b bigint,
           f float,
           d double,
           bo boolean,
           s string,
	   ts timestamp,
           dec decimal,
           bin binary)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/over10k' into table over10k;

-- sum
select ts, f, sum(f) over (partition by ts order by f rows between 2 preceding and 1 preceding) from over10k limit 100;
select ts, f, sum(f) over (partition by ts order by f rows between unbounded preceding and 1 preceding) from over10k limit 100;
select ts, f, sum(f) over (partition by ts order by f rows between 1 following and 2 following) from over10k limit 100;
select ts, f, sum(f) over (partition by ts order by f rows between unbounded preceding and 1 following) from over10k limit 100;

-- avg
select ts, f, avg(f) over (partition by ts order by f rows between 2 preceding and 1 preceding) from over10k limit 100;
select ts, f, avg(f) over (partition by ts order by f rows between unbounded preceding and 1 preceding) from over10k limit 100;
select ts, f, avg(f) over (partition by ts order by f rows between 1 following and 2 following) from over10k limit 100;
select ts, f, avg(f) over (partition by ts order by f rows between unbounded preceding and 1 following) from over10k limit 100;

-- count
select ts, f, count(f) over (partition by ts order by f rows between 2 preceding and 1 preceding) from over10k limit 100;
select ts, f, count(f) over (partition by ts order by f rows between unbounded preceding and 1 preceding) from over10k limit 100;
select ts, f, count(f) over (partition by ts order by f rows between 1 following and 2 following) from over10k limit 100;
select ts, f, count(f) over (partition by ts order by f rows between unbounded preceding and 1 following) from over10k limit 100;

-- max
select ts, f, max(f) over (partition by ts order by t,f rows between 2 preceding and 1 preceding) from over10k limit 100;
select ts, f, max(f) over (partition by ts order by t,f rows between unbounded preceding and 1 preceding) from over10k limit 100;
select ts, f, max(f) over (partition by ts order by t,f rows between 1 following and 2 following) from over10k limit 100;
select ts, f, max(f) over (partition by ts order by t,f rows between unbounded preceding and 1 following) from over10k limit 100;

-- min
select ts, f, min(f) over (partition by ts order by t,f rows between 2 preceding and 1 preceding) from over10k limit 100;
select ts, f, min(f) over (partition by ts order by t,f rows between unbounded preceding and 1 preceding) from over10k limit 100;
select ts, f, min(f) over (partition by ts order by t,f rows between 1 following and 2 following) from over10k limit 100;
select ts, f, min(f) over (partition by ts order by t,f rows between unbounded preceding and 1 following) from over10k limit 100;

-- first_value
select ts, f, first_value(f) over (partition by ts order by f rows between 2 preceding and 1 preceding) from over10k limit 100;
select ts, f, first_value(f) over (partition by ts order by f rows between unbounded preceding and 1 preceding) from over10k limit 100;
select ts, f, first_value(f) over (partition by ts order by f rows between 1 following and 2 following) from over10k limit 100;
select ts, f, first_value(f) over (partition by ts order by f rows between unbounded preceding and 1 following) from over10k limit 100;

-- last_value
select ts, f, last_value(f) over (partition by ts order by f rows between 2 preceding and 1 preceding) from over10k limit 100;
select ts, f, last_value(f) over (partition by ts order by f rows between unbounded preceding and 1 preceding) from over10k limit 100;
select ts, f, last_value(f) over (partition by ts order by f rows between 1 following and 2 following) from over10k limit 100;
select ts, f, last_value(f) over (partition by ts order by f rows between unbounded preceding and 1 following) from over10k limit 100;
-- Test value based windowing spec

drop table if exists emp;

create table emp(empno smallint,
           ename varchar(10),
           job varchar(10),
           manager smallint,
           hiredate date,
           hirets timestamp,
           salary double,
           bonus double,
           stock decimal(10,2),
           deptno tinyint)
       row format delimited
       fields terminated by '|';

load data local inpath '../../data/files/emp2.txt' into table emp;

-- No order by
select hirets, salary, sum(salary) over (partition by hirets range between current row and unbounded following) from emp;


-- Support date datatype
select deptno, empno, hiredate, salary,
    sum(salary) over (partition by deptno order by hiredate range 90 preceding),
    sum(salary) over (partition by deptno order by hiredate range between 90 preceding and 90 following),
    sum(salary) over (partition by deptno order by hiredate range between 90 preceding and 10 preceding),
    sum(salary) over (partition by deptno order by hiredate range between 10 following and 90 following),
    sum(salary) over (partition by deptno order by hiredate range between 10 following and unbounded following),
    sum(salary) over (partition by deptno order by hiredate range between unbounded preceding and 10 following)
from emp;

-- Support timestamp datatype. Value in seconds (90days = 90 * 24 * 3600 seconds)
select deptno, empno, hirets, salary,
    sum(salary) over (partition by deptno order by hirets range 7776000 preceding),
    sum(salary) over (partition by deptno order by hirets range between 7776000 preceding and 7776000 following),
    sum(salary) over (partition by deptno order by hirets range between 7776000 preceding and 864000 preceding),
    sum(salary) over (partition by deptno order by hirets range between 864000 following and 7776000 following),
    sum(salary) over (partition by deptno order by hirets range between 864000 following and unbounded following),
    sum(salary) over (partition by deptno order by hirets range between unbounded preceding and 864000 following)
from emp;

-- Support double datatype
select deptno, empno, bonus,
    avg(bonus) over (partition by deptno order by bonus range 200 preceding),
    avg(bonus) over (partition by deptno order by bonus range between 200 preceding and 200 following),
    avg(bonus) over (partition by deptno order by bonus range between 200 preceding and 100 preceding),
    avg(bonus) over (partition by deptno order by bonus range between 100 following and 200 following),
    avg(bonus) over (partition by deptno order by bonus range between 200 following and unbounded following),
    avg(bonus) over (partition by deptno order by bonus range between unbounded preceding and 200 following)
from emp;

-- Support Decimal datatype
select deptno, empno, stock, salary,
    avg(salary) over (partition by deptno order by stock range 200 preceding),
    avg(salary) over (partition by deptno order by stock range between 200 preceding and 200 following),
    avg(salary) over (partition by deptno order by stock range between 200 preceding and 100 preceding),
    avg(salary) over (partition by deptno order by stock range between 100 following and 200 following),
    avg(salary) over (partition by deptno order by stock range between 200 following and unbounded following),
    avg(salary) over (partition by deptno order by stock range between unbounded preceding and 200 following)
from emp;
--Test small dataset with larger windowing

drop table if exists smalltable_windowing;

create table smalltable_windowing(
      i int,
      type string);
insert into smalltable_windowing values(3, 'a'), (1, 'a'), (2, 'a');

select type, i,
max(i) over (partition by type order by i rows between 1 preceding and 7 following),
min(i) over (partition by type order by i rows between 1 preceding and 7 following),
first_value(i) over (partition by type order by i rows between 1 preceding and 7 following),
last_value(i) over (partition by type order by i rows between 1 preceding and 7 following),
avg(i) over (partition by type order by i rows between 1 preceding and 7 following),
sum(i) over (partition by type order by i rows between 1 preceding and 7 following),
collect_set(i) over (partition by type order by i rows between 1 preceding and 7 following),
count(i) over (partition by type order by i rows between 1 preceding and 7 following)
from smalltable_windowing;
CREATE TABLE dest1(a float);

INSERT OVERWRITE TABLE dest1
SELECT array(1.0,2.0) FROM src;
FROM src
INSERT OVERWRITE TABLE dest1 SELECT DISTINCT src.key, substr(src.value,4,1) GROUP BY src.key
FROM src
INSERT OVERWRITE TABLE dest1 SELECT src.key, DISTINCT substr(src.value,4,1) GROUP BY src.key
