1.2　Spark简介 8
1.2.1　Spark背景 9
1.2.2　Spark的用途 9
1.2.3　Spark编程接口 9
1.2.4　Spark程序的提交类型 10
1.2.5　Spark应用程序的输入/输出类型 11
1.2.6　Spark中的RDD 11
1.2.7　Spark与Hadoop 11
1.3　Python函数式编程 12
1.3.1　Python函数式编程中的数据结构 12
1.3.2　Python对象序列化 15
1.3.3　Python函数式编程基础 17
2.1　Spark部署模式 20
2.1.1　本地模式 21
2.1.2　Spark独立集群 21
2.1.3　基于YARN运行Spark 22
2.1.4　基于Mesos运行Spark 22
2.2　准备安装Spark 23
2.3　获取Spark 23
2.4　在Linux或Mac OS X上安装Spark 25
2.5　在Windows上安装Spark 26
2.6　探索Spark安装目录 28
2.7　部署多节点的Spark独立集群 29
2.8　在云上部署Spark 30
2.8.1　AWS 30
2.8.2　GCP 32
2.8.3　Databricks 32
第3章　理解Spark集群架构 35
3.1　Spark应用中的术语 35
3.1.1　Spark驱动器 36
3.1.2　Spark工作节点与执行器 38
3.1.3　Spark主进程与集群管理器 40
3.2　使用独立集群的Spark应用 41
3.3　在YARN上运行Spark应用 42
3.3.1　ResourceManager作为集群管理器 42
3.3.2　ApplicationMaster作为Spark主进程 42
3.4　在YARN上运行Spark应用的部署模式 42
3.4.1　客户端模式 42
3.4.2　集群模式 43
3.4.3　回顾本地模式 45
3.5　本章小结 45
第4章　Spark编程基础 46
4.1　RDD简介 46
4.2　加载数据到RDD 48
4.2.1　从文件创建RDD 48
4.2.2　从文本文件创建RDD 49
4.2.3　从对象文件创建RDD 52
4.2.4　从数据源创建RDD 52
4.2.5　从JSON文件创建RDD 54
4.2.6　通过编程创建RDD 56
4.3　RDD操作 57
4.3.1　RDD核心概念 57
4.3.2　基本的RDD转化操作 61
4.3.3　基本的RDD行动操作 65
4.3.4　键值对RDD的转化操作 69
4.3.5　MapReduce与单词计数练习 75
4.3.6　连接操作 78
4.3.7　在Spark中连接数据集 82
4.3.8　集合操作 85
4.3.9　数值型RDD的操作 87
4.4　本章小结 89
第5章　Spark核心API高级编程 92
5.1　Spark中的共享变量 92
5.1.1　广播变量 92
5.1.2　累加器 96
5.1.3　练习：使用广播变量和累加器 99
5.2　Spark中的数据分区 100
5.2.1　分区概述 100
5.2.2　掌控分区 101
5.2.3　重分区函数 102
5.2.4　针对分区的API方法 104
5.3　RDD的存储选项 106
5.3.1　回顾RDD谱系 106
5.3.2　RDD存储选项 107
5.3.3　RDD缓存 109
5.3.4　持久化RDD 109
5.3.5　选择何时持久化或缓存RDD 112
5.3.6　保存RDD检查点 112
5.3.7　练习：保存RDD检查点 114
5.4　使用外部程序处理RDD 115
5.5　使用Spark进行数据采样 117
5.6　理解Spark应用与集群配置 118
5.6.1　Spark环境变量 118
5.6.2　Spark配置属性 121

5.7　Spark优化 124

5.7.1　早过滤，勤过滤 124

5.7.2　优化满足结合律的操作 124

5.7.3　理解函数和闭包的影响 126

5.7.4　收集数据的注意事项 127

5.7.5　使用配置参数调节和优化应用 127

5.7.6　避免低效的分区 128

5.7.7　应用性能问题诊断 130


第6章　使用Spark进行SQL与NoSQL编程 134
6.1　Spark SQL简介 134
6.1.1　Hive简介 134
6.1.2　Spark SQL架构 138
6.1.3　DataFrame入门 141
6.1.4　使用DataFrame 150
6.1.5　DataFrame缓存、持久化与重新分区 157
6.1.6　保存DataFrame输出 158
6.1.7　访问Spark SQL 161
6.1.8　练习：使用Spark SQL 163
6.2　在Spark中使用NoSQL系统 165
6.2.1　NoSQL简介 165
6.2.2　在Spark中使用HBase 166
6.2.3　练习：在Spark中使用HBase 169
6.2.4　在Spark中使用Cassandra 170
6.2.5　在Spark中使用DynamoDB 172
6.2.6　其他NoSQL平台 174
6.3　本章小结 174
第7章　使用Spark处理流数据与消息 175
7.1　Spark Streaming简介 175
7.1.1　Spark Streaming架构 176
7.1.2　DStream简介 177
7.1.3　练习：Spark Streaming入门 183
7.1.4　状态操作 184
7.1.5　滑动窗口操作 185
7.2　结构化流处理 188
7.2.1　结构化流处理数据源 188
7.2.2　结构化流处理的数据输出池 189
7.2.3　输出模式 190
7.2.4　结构化流处理操作 190
7.3　在Spark中使用消息系统 192
7.3.1　Apache Kafka 192
7.3.2　KafkaUtils 195
7.3.3　练习：在Spark中使用Kafka 196
7.3.4　亚马逊Kinesis 199
7.4　本章小结 203
第8章　Spark数据科学与机器学习简介 204
8.1　Spark与R语言 204
8.1.1　R语言简介 204
8.1.2　通过R语言使用Spark 210


1．2　一个大一统的软件栈　　2
1．2．1　Spark Core　　2
1．2．2　Spark SQL　　3
1．2．3　Spark Streaming　　3
1．2．4　MLlib　　3
1．2．5　GraphX　　3
1．2．6　集群管理器　　4
1．3．1　数据科学任务　　4
1．3．2　数据处理应用　　5
1．4　Spark简史　　5
1．5　Spark的版本和发布　　6
1．6　Spark的存储层次　　6
2．1　下载Spark　　7
2．2　Spark中Python和Scala的shell　　9
2．3　Spark 核心概念简介　　12
2．4　独立应用　　14
2．4．1　初始化SparkContext　　15
2．4．2　构建独立应用　　16
2．5　总结　　19
3．1　RDD基础　　21
3．2　创建RDD　　23
3．3　RDD操作　　24
3．3．1　转化操作　　24
3．3．2　行动操作　　26
3．3．3　惰性求值　　27
3．4　向Spark传递函数　　27
3．4．1　Python　　27
3．4．2　Scala　　28
3．4．3　Java　　29
3．5　常见的转化操作和行动操作　　30
3．5．1　基本RDD　　30
3．5．2　在不同RDD类型间转换　　37
3．6　持久化( 缓存)　　39
3．7　总结　　40
第4章　键值对操作　　41
4．1　动机　　41
4．2　创建Pair RDD　　42
4．3　Pair RDD的转化操作　　42
4．3．1　聚合操作　　45
4．3．2　数据分组　　49
4．3．3　连接　　50
4．3．4　数据排序　　51
4．4　Pair RDD的行动操作　　52
4．5　数据分区（进阶）　　52
4．5．1　获取RDD的分区方式　　55
4．5．2　从分区中获益的操作　　56
4．5．3　影响分区方式的操作　　57
4．5．4　示例：PageRank　　57
4．5．5　自定义分区方式　　59
4．6　总结　　61
第5章　数据读取与保存　　63
5．1　动机　　63
5．2　文件格式　　64
5．2．1　文本文件　　64
5．2．2　JSON　　66
5．2．3　逗号分隔值与制表符分隔值　　68
5．2．4　SequenceFile　　71
5．2．5　对象文件　　73
5．2．6　Hadoop输入输出格式　　73
5．2．7　文件压缩　　77
5．3　文件系统　　78
5．3．1　本地/“常规”文件系统　　78
5．3．2　Amazon S3　　78
5．3．3　HDFS　　79
5．4　Spark SQL中的结构化数据　　79
5．4．1　Apache Hive　　80
5．4．2　JSON　　80
5．5　数据库　　81
5．5．1　Java数据库连接　　81
5．5．2　Cassandra　　82
5．5．3　HBase　　84
5．5．4　Elasticsearch　　85
5．6　总结　　86
第6章　Spark编程进阶　　87
6．1　简介　　87
6．2　累加器　　88
6．2．1　累加器与容错性　　90
6．2．2　自定义累加器　　91
6．3　广播变量　　91
6．4　基于分区进行操作　　94
6．5　与外部程序间的管道　　96
6．6　数值RDD 的操作　　99
6．7　总结　　100
第7章　在集群上运行Spark　　101
7．1　简介　　101
7．2　Spark运行时架构　　101
7．2．1　驱动器节点　　102
7．2．2　执行器节点　　103
7．2．3　集群管理器　　103
7．2．4　启动一个程序　　104
7．2．5　小结　　104
7．3　使用spark-submit 部署应用　　105
7．4　打包代码与依赖　　107
7．4．1　使用Maven构建的用Java编写的Spark应用　　108
7．4．2　使用sbt构建的用Scala编写的Spark应用　　109
7．4．3　依赖冲突　　 111
7．5　Spark应用内与应用间调度　　111
7．6　集群管理器　　112
7．6．1　独立集群管理器　　112
7．6．2　Hadoop YARN　　115
7．6．3　Apache Mesos　　116
7．6．4　Amazon EC2　　117
7．7　选择合适的集群管理器　　120
7．8　总结　　121
第8章　Spark调优与调试　　123
8．1　使用SparkConf配置Spark　　123
8．2　Spark执行的组成部分：作业、任务和步骤　　127
8．3　查找信息　　131
8．3．1　Spark网页用户界面　　131
8．3．2　驱动器进程和执行器进程的日志　　134
8．4　关键性能考量　　135
8．4．1　并行度　　135
8．4．2　序列化格式　　136
8．4．3　内存管理　　137
8．4．4　硬件供给　　138
8．5　总结　　139
第9章　Spark SQL　　141
9．1　连接Spark SQL　　142
9．2　在应用中使用Spark SQL　　144
9．2．1　初始化Spark SQL　　144
9．2．2　基本查询示例　　145
9．2．3　SchemaRDD　　146
9．2．4　缓存　　148
9．3　读取和存储数据　　149
9．3．1　Apache Hive　　149
9．3．2　Parquet　　150
9．3．3　JSON　　150
9．3．4　基于RDD　　152
9．4　JDBC/ODBC服务器　　153
9．4．1　使用Beeline　　155
9．4．2　长生命周期的表与查询　　156
9．5　用户自定义函数　　156
9．5．1　Spark SQL UDF　　156
9．5．2　Hive UDF　　157
9．6　Spark SQL性能　　158
9．7　总结　　159


6.1 Spark概述 107
6.1.1 批量数据处理 107
6.1.2 实时数据处理 108
6.1.3 一站式解决方案Apache Spark 110
6.1.4 何时应用Spark—实际用例 112
6.2.1 高级架构 114
6.2.2 Spark扩展/库 116
6.2.3 Spark的封装结构和API 117
6.2.4 Spark的执行模型—主管-工作者视图 119
6.3 弹性分布式数据集（RDD） 122
6.4 编写执行第一个Spark程序 124
6.4.1 硬件需求 125
6.4.2 基本软件安装 125
6.4.3 配置Spark集群 127
6.4.4 用Scala编写Spark作业 129
6.4.5 用Java编写Spark作业 132
6.5 故障排除提示和技巧 133
6.5.1 Spark所用的端口数目 134
6.5.2 类路径问题—类未找到异常 134
6.5.3 其他常见异常 134
6.6 本章小结 135
7.1 理解Spark转换及操作 136
7.1.1 RDD API 137
7.1.2 RDD转换操作 139
7.1.3 RDD功能操作 141
7.2 编程Spark转换及操作 142
7.3 Spark中的持久性 157
7.4 本章小结 159
第8章 Spark的SQL查询引擎——Spark SQL 160
8.1 Spark SQL的体系结构 161
8.1.1 Spark SQL的出现 161
8.1.2 Spark SQL的组件 162
8.1.3 Catalyst Optimizer 164
8.1.4 SQL/Hive context 165
8.2 编写第一个Spark SQL作业 166
8.2.1 用Scala编写Spark SQL作业 166
8.2.2 用Java编写Spark SQL作业 170
8.3 将RDD转换为DataFrame 173
8.3.1 自动化过程 174
8.3.2 手动过程 176
8.4 使用Parquet 179
8.4.1 在HDFS中持久化Parquet数据 182
8.4.2 数据分区和模式演化/合并 185
8.5 Hive表的集成 186
8.6 性能调优和最佳实践 190
8.6.1 分区和并行性 191
8.6.2 序列化 191
8.6.3 缓存 192
8.6.4 内存调优 192
8.7 本章小结 194
第9章 用Spark Streaming分析流数据 195
9.1 高级架构 195
9.1.1 Spark Streaming的组件 196
9.1.2 Spark Streaming的封装结构 198
9.2 编写第一个Spark Streaming作业 200
9.2.1 创建流生成器 201
9.2.2 用Scala编写Spark Streaming作业 202
9.2.3 用Java编写Spark Streaming作业 205
9.2.4 执行Spark Streaming作业 207
9.3 实时查询流数据 209
9.3.1 作业的高级架构 209
9.3.2 编写Crime生产者 210
9.3.3 编写Stream消费者和转换器 212
9.3.4 执行SQL Streaming Crime分析器 214
9.4 部署和监测 216
9.4.1 用于Spark Streaming的集群管理器 216
9.4.2 监测Spark Streaming应用程序 218
9.5 本章小结 219
第10章 介绍Lambda架构 220
10.1 什么是Lambda架构 220
10.1.1 Lambda架构的需求 220
10.1.2 Lambda架构的层/组件 222
10.2 Lambda架构的技术矩阵 226
10.3 Lambda架构的实现 228
10.3.1 高级架构 229
10.3.2 配置Apache Cassandra和Spark 230
10.3.3 编写自定义生产者程序 233
10.3.4 编写实时层代码 235
10.3.5 编写批处理层代码 238
10.3.6 编写服务层代码 239
10.3.7 执行所有层代码 241

1.1　运行环境准备2
1.1.1　安装JDK3
1.1.2　安装Scala3
1.1.3　安装Spark4
1.2　Spark初体验4
1.2.1　运行spark-shell4
1.2.2　执行word count5
1.2.3　剖析spark-shell7
1.3　阅读环境准备11
1.4　Spark源码编译与调试13
1.5　小结17
2.1　初识Spark18
2.1.1　Hadoop MRv1的局限18
2.1.2　Spark使用场景20
2.1.3　Spark的特点20
2.2　Spark基础知识20
2.3　Spark基本设计思想22
2.3.1　Spark模块设计22
2.3.2　Spark模型设计24
2.4　Spark基本架构25
2.5　小结26
第3章　SparkContext的初始化28
3.1　SparkContext概述28
3.2　创建执行环境SparkEnv30
3.2.1　安全管理器SecurityManager31
3.2.2　基于Akka的分布式消息系统ActorSystem31
3.2.3　map任务输出跟踪器mapOutputTracker32
3.2.4　实例化ShuffleManager34
3.2.5　shuffle线程内存管理器ShuffleMemoryManager34
3.2.6　块传输服务BlockTransferService35
3.2.7　BlockManagerMaster介绍35
3.2.8　创建块管理器BlockManager36
3.2.9　创建广播管理器Broadcast-Manager36
3.2.10　创建缓存管理器CacheManager37
3.2.11　HTTP文件服务器HttpFile-Server37
3.2.12　创建测量系统MetricsSystem39
3.2.13　创建SparkEnv40
3.3　创建metadataCleaner41
3.4　SparkUI详解42
3.4.1　listenerBus详解43
3.4.2　构造JobProgressListener46
3.4.3　SparkUI的创建与初始化47
3.4.4　Spark UI的页面布局与展示49
3.4.5　SparkUI的启动54
3.5　Hadoop相关配置及Executor环境变量54
3.5.1　Hadoop相关配置信息54
3.5.2　Executor环境变量54
3.6　创建任务调度器TaskScheduler55
3.6.1　创建TaskSchedulerImpl55
3.6.2　TaskSchedulerImpl的初始化57
3.7　创建和启动DAGScheduler57
3.8　TaskScheduler的启动60
3.8.1　创建LocalActor60
3.8.2　ExecutorSource的创建与注册62
3.8.3　ExecutorActor的构建与注册64
3.8.4　Spark自身ClassLoader的创建64
3.8.5　启动Executor的心跳线程66
3.9　启动测量系统MetricsSystem69
3.9.1　注册Sources70
3.9.2　注册Sinks70
3.9.3　给Sinks增加Jetty的Servlet-ContextHandler71
3.10　创建和启动ExecutorAllocation-Manager72
3.11　ContextCleaner的创建与启动73
3.12　Spark环境更新74
3.13　创建DAGSchedulerSource和BlockManagerSource76
3.14　将SparkContext标记为激活77
3.15　小结78
第4章　存储体系79
4.1　存储体系概述79
4.1.1　块管理器BlockManager的实现79
4.1.2　Spark存储体系架构81
4.2　shuffle服务与客户端83
4.2.1　Block的RPC服务84
4.2.2　构造传输上下文Transpor-tContext85
4.2.3　RPC客户端工厂Transport-ClientFactory86
4.2.4　Netty服务器TransportServer87
4.2.5　获取远程shuffle文件88
4.2.6　上传shuffle文件89
4.3　BlockManagerMaster对Block-Manager的管理90
4.3.1　BlockManagerMasterActor90
4.3.2　询问Driver并获取回复方法92
4.3.3　向BlockManagerMaster注册BlockManagerId93
4.4　磁盘块管理器DiskBlockManager94
4.4.1　DiskBlockManager的构造过程94
4.4.2　获取磁盘文件方法getFile96
4.4.3　创建临时Block方法create-TempShuffleBlock96
4.5　磁盘存储DiskStore97
4.5.1　NIO读取方法getBytes97
4.5.2　NIO写入方法putBytes98
4.5.3　数组写入方法putArray98
4.5.4　Iterator写入方法putIterator98
4.6　内存存储MemoryStore99
4.6.1　数据存储方法putBytes101
4.6.2　Iterator写入方法putIterator详解101
4.6.3　安全展开方法unrollSafely102
4.6.4　确认空闲内存方法ensureFreeSpace105
4.6.5　内存写入方法putArray107
4.6.6　尝试写入内存方法tryToPut108
4.6.7　获取内存数据方法getBytes109
4.6.8　获取数据方法getValues110
4.7　Tachyon存储TachyonStore110
4.7.1　Tachyon简介111
4.7.2　TachyonStore的使用112
4.7.3　写入Tachyon内存的方法putIntoTachyonStore113
4.7.4　获取序列化数据方法getBytes113
4.8　块管理器BlockManager114
4.8.1　移出内存方法dropFrom-Memory114
4.8.2　状态报告方法reportBlockStatus116
4.8.3　单对象块写入方法putSingle117
4.8.4　序列化字节块写入方法putBytes118
4.8.5　数据写入方法doPut118
4.8.6　数据块备份方法replicate121
4.8.7　创建DiskBlockObjectWriter的方法getDiskWriter125
4.8.8　获取本地Block数据方法getBlockData125
4.8.9　获取本地shuffle数据方法doGetLocal126
4.8.10　获取远程Block数据方法doGetRemote127
4.8.11　获取Block数据方法get128
4.8.12　数据流序列化方法dataSerializeStream129
4.9　metadataCleaner和broadcastCleaner129
4.10　缓存管理器CacheManager130
4.11　压缩算法133
4.12　磁盘写入实现DiskBlockObjectWriter133
4.13　块索引shuffle管理器IndexShuffleBlockManager135
4.14　shuffle内存管理器ShuffleMemoryManager137
4.15　小结138
第5章　任务提交与执行139
5.1　任务概述139
5.2　广播Hadoop的配置信息142
5.3　RDD转换及DAG构建144
5.3.1　为什么需要RDD144
5.3.2　RDD实现分析146
5.4　任务提交152
5.4.1　任务提交的准备152
5.4.2　finalStage的创建与Stage的划分157
5.4.3　创建Job163
5.4.4　提交Stage164
5.4.5　提交Task165
5.5　执行任务176
5.5.1　状态更新176
5.5.2　任务还原177
5.5.3　任务运行178
5.6　任务执行后续处理179
5.6.1　计量统计与执行结果序列化179
5.6.2　内存回收180
5.6.3　执行结果处理181
5.7　小结187
第6章　计算引擎188
6.1　迭代计算188
6.2　什么是shuffle192
6.3　map端计算结果缓存处理194
6.3.1　map端计算结果缓存聚合195
6.3.2　map端计算结果简单缓存200
6.3.3　容量限制201
6.4　map端计算结果持久化204
6.4.1　溢出分区文件205
6.4.2排序与分区分组207
6.4.3　分区索引文件209
6.5　reduce端读取中间计算结果210
6.5.1　获取map任务状态213
6.5.2　划分本地与远程Block215
6.5.3　获取远程Block217
6.5.4　获取本地Block218
6.6　reduce端计算219
6.6.1　如何同时处理多个map任务的中间结果219
6.6.2　reduce端在缓存中对中间计算结果执行聚合和排序220
6.7　map端与reduce端组合分析221
6.7.1　在map端溢出分区文件，在reduce端合并组合221
6.7.2　在map端简单缓存、排序分组，在reduce端合并组合222
6.7.3　在map端缓存中聚合、排序分组，在reduce端组合222
6.8　小结223
第7章　部署模式224
7.1　local部署模式225
7.2　local-cluster部署模式225
7.2.1　LocalSparkCluster的启动226
7.2.2　CoarseGrainedSchedulerBackend的启动236
7.2.3　启动AppClient237
7.2.4　资源调度242
7.2.5　local-cluster模式的任务执行253
7.3　Standalone部署模式255
7.3.1　启动Standalone模式255
7.3.2　启动Master分析257
7.3.3　启动Worker分析259
7.3.4　启动Driver Application分析261
7.3.5　Standalone模式的任务执行263
7.3.6　资源回收263
7.4　容错机制266
7.4.1　Executor异常退出266
7.4.2　Worker异常退出268
7.4.3　Master异常退出269
7.5　其他部署方案276
7.5.1　YARN277
7.5.2　Mesos280
7.6　小结282
扩　展　篇
第8章　Spark SQL284
8.1　Spark SQL总体设计284
8.1.1　传统关系型数据库SQL运行原理285
8.1.2　Spark SQL运行架构286
8.2　字典表Catalog288
8.3　Tree和TreeNode289
8.4　词法解析器Parser的设计与实现293
8.4.1　SQL语句解析的入口294
8.4.2　建表语句解析器DDLParser295
8.4.3　SQL语句解析器SqlParser296
8.4.4　Spark代理解析器SparkSQLParser299
8.5　Rule和RuleExecutor300
8.6　Analyzer与Optimizer的设计与实现302
8.6.1　语法分析器Analyzer304
8.6.2　优化器Optimizer305
8.7　生成物理执行计划306
8.8　执行物理执行计划308
8.9　Hive311
8.9.1　Hive SQL语法解析器311
8.9.2　Hive SQL元数据分析313
8.9.3　Hive SQL物理执行计划314
8.10　应用举例：JavaSparkSQL314
8.11　小结320
第9章　流式计算321
9.1　Spark Streaming总体设计321
9.2　StreamingContext初始化323
9.3　输入流接收器规范Receiver324
9.4　数据流抽象DStream325
9.4.1　Dstream的离散化326
9.4.2　数据源输入流InputDStream327
9.4.3　Dstream转换及构建DStream Graph329
9.5　流式计算执行过程分析330
9.5.1　流式计算例子CustomReceiver331
9.5.2　Spark Streaming执行环境构建335
9.5.3　任务生成过程347
9.6　窗口操作355
9.7　应用举例357
9.7.1　安装mosquitto358
9.7.2　启动mosquitto358
9.7.3　MQTTWordCount359
9.8　小结361
第10章　图计算362
10.1　Spark GraphX总体设计362
10.1.1　图计算模型363
10.1.2　属性图365
10.1.3　GraphX的类继承体系367
10.2　图操作368
10.2.1　属性操作368
10.2.2　结构操作368
10.2.3　连接操作369
10.2.4　聚合操作370
10.3　Pregel API371
10.3.1　Dijkstra算法373
10.3.2　Dijkstra的实现376
10.4　Graph的构建377
10.4.1　从边的列表加载Graph377
10.4.2　在Graph中创建图的方法377
10.5　顶点集合抽象VertexRDD378
10.6　边集合抽象EdgeRDD379
10.7　图分割380
10.8　常用算法382
10.8.1　网页排名382
10.8.2　Connected Components的应用386
10.8.3　三角关系统计388
10.9　应用举例390
10.10　小结391
第11章　机器学习392
11.1机器学习概论392
11.2　Spark MLlib总体设计394
11.3　数据类型394
11.3.1　局部向量394
11.3.2标记点395
11.3.3局部矩阵396
11.3.4分布式矩阵396
11.4基础统计398
11.4.1摘要统计398
11.4.2相关统计399
11.4.3分层抽样401
11.4.4假设检验401
11.4.5随机数生成402
11.5分类和回归405
11.5.1数学公式405
11.5.2线性回归407
11.5.3分类407
11.5.4回归410
11.6决策树411
11.6.1基本算法411
11.6.2使用例子412
11.7随机森林413
11.7.1基本算法414
11.7.2使用例子414
11.8梯度提升决策树415
11.8.1基本算法415
11.8.2使用例子416
11.9朴素贝叶斯416
11.9.1算法原理416
11.9.2使用例子418
11.10保序回归418
11.10.1算法原理418
11.10.2使用例子419
11.11协同过滤419
11.12聚类420
11.12.1K-means420
11.12.2高斯混合422
11.12.3快速迭代聚类422
11.12.4latent Dirichlet allocation422
11.12.5流式K-means423
11.13维数减缩424
11.13.1奇异值分解424
11.13.2主成分分析425
11.14特征提取与转型425
11.14.1术语频率反转425
11.14.2单词向量转换426
11.14.3标准尺度427
11.14.4正规化尺度428
11.14.5卡方特征选择器428
11.14.6Hadamard积429
11.15频繁模式挖掘429
11.16预言模型标记语言430
11.17管道431
11.17.1管道工作原理432
11.17.2管道API介绍433
11.17.3交叉验证435
11.18小结436
附录A　Utils437
附录B　Akka446
附录C　Jetty450
附录D　Metrics453
附录E　Hadoop word count456
附录F　CommandUtils458
附录G　Netty461
附录H　源码编译错误465



1．1．1 什么是Spark
1．1．2 Spark与MapReduce比较
1．1．3 Spark的演进路线图
1．2 Spark生态系统
1．2．1 Spark Core
1．2．2 Spark Streaming
1．2．3 Spark SQL
1．2．4 BlinkDB
1．2．5 MLBase/MLlib
1．2．6 GraphX
1．2．7 SparkR
1．2．8 Alluxio
2．1 基础环境搭建
2．1．1 搭建集群样板机
2．1．2 配置集群环境
2．2 编译Spark源代码
2．2．1 配置Spark编译环境
2．2．2 使用Maven编译Spark
2．2．3 使用SBT编译Spark
2．2．4 生成Spark部署包
2．3 搭建Spark运行集群
2．3．1 修改配置文件
2．3．2 启动Spark
2．3．3 验证启动
2．3．4 第一个实例
2．4 搭建Spark实战开发环境
2．4．1 CentOS中部署IDEA
2．4．2 使用IDEA开发程序
2．4．3 使用IDEA阅读源代码
2．5 小结

第二篇 核心篇
第3章 Spark编程模型
3．1 RDD概述
3．1．1 背景
3．1．2 RDD简介
3．1．3 RDD的类型
3．2 RDD的实现
3．2．1 作业调度
3．2．2 解析器集成
3．2．3 内存管理
3．2．4 检查点支持
3．2．5 多用户管理
3．3 编程接口
3．3．1 RDD分区（Partitions）
3．3．2 RDD首选位置（PreferredLocations）
3．3．3 RDD依赖关系（Dependencies）
3．3．4 RDD分区计算（Iterator）
3．3．5 RDD分区函数（Partitioner）
3．4 创建操作
3．4．1 并行化集合创建操作
3．4．2 外部存储创建操作
3．5 转换操作
3．5．1 基础转换操作
3．5．2 键值转换操作
3．6 控制操作
3．7 行动操作
3．7．1 集合标量行动操作
3．7．2 存储行动操作
3．8 小结
第4章 Spark核心原理
4．1 消息通信原理
4．1．1 Spark消息通信架构
4．1．2 Spark启动消息通信
4．1．3 Spark运行时消息通信
4．2 作业执行原理
4．2．1 概述
4．2．2 提交作业
4．2．3 划分调度阶段
4．2．4 提交调度阶段
4．2．5 提交任务
4．2．6 执行任务
4．2．7 获取执行结果
4．3 调度算法
4．3．1 应用程序之间
4．3．2 作业及调度阶段之间
4．3．3 任务之间
4．4 容错及HA
4．4．1 Executor异常
4．4．2 Worker异常
4．4．3 Master异常
4．5 监控管理
4．5．1 UI监控
4．5．2 Metrics
4．5．3 REST
4．6 实例演示
4．6．1 计算年降水实例
4．6．2 HA配置实例
4．7 小结
第5章 Spark存储原理
5．1 存储分析
5．1．1 整体架构
5．1．2 存储级别
5．1．3 RDD存储调用
5．1．4 读数据过程
5．1．5 写数据过程
5．2 Shuffle分析
5．2．1 Shuffle简介
5．2．2 Shuffle的写操作
5．2．3 Shuffle的读操作
5．3 序列化和压缩
5．3．1 序列化
5．3．2 压缩
5．4 共享变量
5．4．1 广播变量
5．4．2 累加器
5．5 实例演示
5．6 小结
第6章 Spark运行架构
6．1 运行架构总体介绍
6．1．1 总体介绍
6．1．2 重要类介绍
6．2 本地（Local）运行模式
6．2．1 运行模式介绍
6．2．2 实现原理
6．3 伪分布（Local-Cluster）运行模式
6．3．1 运行模式介绍
6．3．2 实现原理
6．4 独立（Standalone）运行模式
6．4．1 运行模式介绍
6．4．2 实现原理
6．5 YARN运行模式
6．5．1 YARN运行框架
6．5．2 YARN-Client运行模式介绍
6．5．3 YARN-Client 运行模式实现原理
6．5．4 YARN-Cluster运行模式介绍
6．5．5 YARN-Cluster 运行模式实现原理
6．5．6 YARN-Client与YARN-Cluster对比
6．6 Mesos运行模式
6．6．1 Mesos介绍
6．6．2 粗粒度运行模式介绍
6．6．3 粗粒度实现原理
6．6．4 细粒度运行模式介绍
6．6．5 细粒度实现原理
6．6．6 Mesos粗粒度和Mesos细粒度对比
6．7 实例演示
6．7．1 独立运行模式实例
6．7．2 YARN-Client实例
6．7．3 YARN-Cluster实例
6．8 小结

第三篇 组件篇
第7章 Spark SQL
7．1 Spark SQL简介
7．1．1 Spark SQL发展历史
7．1．2 DataFrame/Dataset介绍
7．2 Spark SQL运行原理
7．2．1 通用SQL执行原理
7．2．2 SparkSQL运行架构
7．2．3 SQLContext运行原理分析
7．2．4 HiveContext介绍
7．3 使用Hive-Console
7．3．1 编译Hive-Console
7．3．2 查看执行计划
7．3．3 应用Hive-Console
7．4 使用SQLConsole
7．4．1 启动HDFS和Spark Shell
7．4．2 与RDD交互操作
7．4．3 读取JSON格式数据
7．4．4 读取Parquet格式数据
7．4．5 缓存演示
7．4．6 DSL演示
7．5 使用Spark SQL CLI
7．5．1 配置并启动Spark SQL CLI
7．5．2 实战Spark SQL CLI
7．6 使用Thrift Server
7．6．1 配置并启动Thrift Server
7．6．2 基本操作
7．6．3 交易数据实例
7．6．4 使用IDEA开发实例
7．7 实例演示
7．7．1 销售数据分类实例
7．7．2 网店销售数据统计
7．8 小结
第8章 Spark Streaming
8．1 Spark Streaming简介
8．1．1 术语定义
8．1．2 Spark Streaming特点
8．2 Spark Streaming编程模型
8．2．1 DStream的输入源
8．2．2 DStream的操作
8．3 Spark Streaming运行架构
8．3．1 运行架构
8．3．2 消息通信
8．3．3 Receiver分发
8．3．4 容错性
8．4 Spark Streaming运行原理
8．4．1 启动流处理引擎
8．4．2 接收及存储流数据
8．4．3 数据处理
8．5 实例演示
8．5．1 流数据模拟器
8．5．2 销售数据统计实例
8．5．3 Spark Streaming+Kafka实例
8．6 小结
第9章 Spark MLlib
9．1 Spark MLlib简介
9．1．1 Spark MLlib介绍
9．1．2 Spark MLlib数据类型
9．1．3 Spark MLlib基本统计方法
9．1．4 预言模型标记语言
9．2 线性模型
9．2．1 数学公式
9．2．2 线性回归
9．2．3 线性支持向量机
9．2．4 逻辑回归
9．2．5 线性最小二乘法、Lasso和岭回归
9．2．6 流式线性回归
9．3 决策树
9．4 决策模型组合
9．4．1 随机森林
9．4．2 梯度提升决策树
9．5 朴素贝叶斯
9．6 协同过滤
9．7 聚类
9．7．1 K-means
9．7．2 高斯混合
9．7．3 快速迭代聚类
9．7．4 LDA
9．7．5 二分K-means
9．7．6 流式K-means
9．8 降维
9．8．1 奇异值分解降维
9．8．2 主成分分析降维
9．9 特征提取和变换
9．9．1 词频―逆文档频率
9．9．2 词向量化工具
9．9．3 标准化
9．9．4 范数化
9．10 频繁模式挖掘
9．10．1 频繁模式增长
9．10．2 关联规则挖掘
9．10．3 PrefixSpan
9．11 实例演示
9．11．1 K-means聚类算法实例
9．11．2 手机短信分类实例
9．12 小结
第10章 Spark GraphX
10．1 GraphX介绍
10．1．1 图计算
10．1．2 GraphX介绍
10．1．3 发展历程
10．2 GraphX实现分析
10．2．1 GraphX图数据模型
10．2．2 GraphX图数据存储
10．2．3 GraphX图切分策略
10．2．4 GraphX图操作
10．3 实例演示
10．3．1 图例演示
10．3．2 社区发现演示
10．4 小结
第11章 SparkR
11．1 概述
11．1．1 R语言介绍
11．1．2 SparkR介绍
11．2 SparkR与DataFrame
11．2．1 DataFrames介绍
11．2．2 与DataFrame的相关操作
11．3 编译安装SparkR
11．3．1 编译安装R语言
11．3．2 安装SparkR运行环境
11．3．3 安装SparkR
11．3．4 启动并验证安装
11．4 实例演示
11．5 小结
第12章 Alluxio
12．1 Alluxio简介
12．1．1 Alluxio介绍
12．1．2 Alluxio系统架构
12．1．3 HDFS与Alluxio
12．2 Alluxio编译部署
12．2．1 编译Alluxio
12．2．2 单机部署Alluxio
12．2．3 集群模式部署Alluxio
12．3 Alluxio命令行使用
12．3．1 接口说明
12．3．2 接口操作示例
12．4 实例演示
12．4．1 启动环境
12．4．2 Alluxio上运行Spark
12．4．3 Alluxio上运行MapReduce
12．5 小结


第1章　环境准备 1
1.1　运行环境准备 2
1.1.1　安装JDK 2
1.1.2　安装Scala 2
1.1.3　安装Spark 3
1.2　Spark初体验 4
1.2.1　运行spark-shell 4
1.2.2　执行word count 5
1.2.3　剖析spark-shell 9
1.3　阅读环境准备 14
1.3.1　安装SBT 15
1.3.2　安装Git 15
1.3.3　安装Eclipse Scala IDE插件 15
1.4　Spark源码编译与调试 17
1.5　小结 23
第2章　设计理念与基本架构 24
2.1　初识Spark 25
2.1.1　Hadoop MRv1的局限25
2.1.2　Spark的特点 26
2.1.3 Spark使用场景 28
2.2　Spark基础知识 29
2.3　Spark基本设计思想 31
2.3.1　Spark模块设计 32
2.3.2　Spark模型设计 34
2.4　Spark基本架构 36
2.5　小结 38
第3章　Spark基础设施 39
3.1　Spark配置 40
3.1.1　系统属性中的配置 40
3.1.2　使用SparkConf配置的API 41
3.1.3　克隆SparkConf配置 42
3.2　Spark内置RPC框架 42
3.2.1　RPC配置TransportConf 45
3.2.2　RPC客户端工厂Transport- ClientFactory 47
3.2.3　RPC服务端TransportServer 53
3.2.4　管道初始化 56
3.2.5　TransportChannelHandler详解 57
3.2.6　服务端RpcHandler详解 63
3.2.7　服务端引导程序Transport-ServerBootstrap 68
3.2.8　客户端TransportClient详解 71
3.3　事件总线 78
3.3.1　ListenerBus的继承体系 79
3.3.2　SparkListenerBus详解 80
3.3.3　LiveListenerBus详解 83
3.4　度量系统 87
3.4.1　Source继承体系 87
3.4.2　Sink继承体系 89
3.5　小结 92
第4章　SparkContext的初始化 93
4.1　SparkContext概述 94
4.2　创建Spark环境 97
4.3　SparkUI的实现 100
4.3.1　SparkUI概述 100
4.3.2　WebUI框架体系 102
4.3.3　创建SparkUI 107
4.4　创建心跳接收器 111
4.5　创建和启动调度系统112
4.6　初始化块管理器BlockManager 114
4.7　启动度量系统 114
4.8　创建事件日志监听器115
4.9　创建和启动ExecutorAllocation-Manager 116
4.10　ContextCleaner的创建与启动 120
4.10.1　创建ContextCleaner 120
4.10.2　启动ContextCleaner 120
4.11　额外的SparkListener与启动事件总线 122
4.12　Spark环境更新 123
4.13　SparkContext初始化的收尾 127
4.14　SparkContext提供的常用方法 128
4.15　SparkContext的伴生对象130
4.16　小结 131
第5章　Spark执行环境 132
5.1　SparkEnv概述 133
5.2　安全管理器SecurityManager 133
5.3　RPC环境 135
5.3.1　RPC端点RpcEndpoint 136
5.3.2　RPC端点引用RpcEndpointRef 139
5.3.3　创建传输上下文TransportConf 142
5.3.4　消息调度器Dispatcher 142
5.3.5　创建传输上下文Transport-Context 154
5.3.6　创建传输客户端工厂Transport-ClientFactory 159
5.3.7　创建TransportServer 160
5.3.8　客户端请求发送 162
5.3.9　NettyRpcEnv中的常用方法 173
5.4　序列化管理器SerializerManager 175
5.5　广播管理器BroadcastManager 178
5.6　map任务输出跟踪器 185
5.6.1　MapOutputTracker的实现 187
5.6.2　MapOutputTrackerMaster的实现原理 191
5.7　构建存储体系 199
5.8　创建度量系统 201
5.8.1　MetricsCon?g详解 203
5.8.2　MetricsSystem中的常用方法 207
5.8.3　启动MetricsSystem 209
5.9　输出提交协调器 211
5.9.1　OutputCommitCoordinator-Endpoint的实现 211
5.9.2　OutputCommitCoordinator的实现 212
5.9.3　OutputCommitCoordinator的工作原理 216
5.10　创建SparkEnv 217
5.11　小结 217
第6章　存储体系 219
6.1　存储体系概述 220
6.1.1　存储体系架构 220
6.1.2　基本概念 222
6.2　Block信息管理器 227
6.2.1　Block锁的基本概念 227
6.2.2　Block锁的实现 229
6.3　磁盘Block管理器 234
6.3.1　本地目录结构 234
6.3.2　DiskBlockManager提供的方法 236
6.4　磁盘存储DiskStore 239
6.5　内存管理器 242
6.5.1　内存池模型 243
6.5.2　StorageMemoryPool详解 244
6.5.3　MemoryManager模型 247
6.5.4　Uni?edMemoryManager详解 250
6.6　内存存储MemoryStore 252
6.6.1　MemoryStore的内存模型 253
6.6.2　MemoryStore提供的方法 255
6.7　块管理器BlockManager 265
6.7.1　BlockManager的初始化 265
6.7.2　BlockManager提供的方法 266
6.8　BlockManagerMaster对Block-Manager的管理 285
6.8.1　BlockManagerMaster的职责 285
6.8.2　BlockManagerMasterEndpoint详解 286
6.8.3　BlockManagerSlaveEndpoint详解 289
6.9　Block传输服务 290
6.9.1　初始化NettyBlockTransfer-Service 291
6.9.2　NettyBlockRpcServer详解 292
6.9.3　Shuf?e客户端 296
6.10　DiskBlockObjectWriter详解 305
6.11　小结 308
第7章　调度系统 309
7.1　调度系统概述 310
7.2　RDD详解 312
7.2.1　为什么需要RDD 312
7.2.2　RDD

第一部分　Spark MLlib基础
第1章　Spark机器学习简介 2
1．1　机器学习介绍 2
1．2　Spark介绍 3
1．3　Spark MLlib介绍 4
第2章　Spark数据操作 6
2．1　Spark RDD操作 6
2．1．1　Spark RDD创建操作 6
2．1．2　Spark RDD转换操作 7
2．1．3　Spark RDD行动操作 14
2．2　MLlib Statistics统计操作 15
2．2．1　列统计汇总 15
2．2．2　相关系数 16
2．2．3　假设检验 18
2．3　MLlib数据格式 18
2．3．1　数据处理 18
2．3．2　生成样本 22
第3章　Spark MLlib矩阵向量 26
3．1　Breeze介绍 26
3．1．1　Breeze创建函数 27
3．1．2　Breeze元素访问及操作函数 29
3．1．3　Breeze数值计算函数 34
3．1．4　Breeze求和函数 35
3．1．5　Breeze布尔函数 36
3．1．6　Breeze线性代数函数 37
3．1．7　Breeze取整函数 39
3．1．8　Breeze常量函数 40
3．1．9　Breeze复数函数 40
3．1．10　Breeze三角函数 40
3．1．11　Breeze对数和指数函数 40
3．2　BLAS介绍 41
3．2．1　BLAS向量-向量运算 42
3．2．2　BLAS矩阵-向量运算 42
3．2．3　BLAS矩阵-矩阵运算 43
3．3　MLlib向量 43
3．3．1　MLlib向量介绍 43
3．3．2　MLlib Vector接口 44
3．3．3　MLlib DenseVector类 46
3．3．4　MLlib SparseVector类 49
3．3．5　MLlib Vectors伴生对象 50
3．4　MLlib矩阵 57
3．4．1　MLlib矩阵介绍 57
3．4．2　MLlib Matrix接口 57
3．4．3　MLlib DenseMatrix类 59
3．4．4　MLlib SparseMatrix类 64
3．4．5　MLlib Matrix伴生对象 71
3．5　MLlib BLAS 77
3．6　MLlib分布式矩阵 93
3．6．1　MLlib分布式矩阵介绍 93
3．6．2　行矩阵（RowMatrix） 94
3．6．3　行索引矩阵（IndexedRowMatrix） 96
3．6．4　坐标矩阵（CoordinateMatrix） 97
3．6．5　分块矩阵（BlockMatrix） 98

第二部分　Spark MLlib回归算法
第4章　Spark MLlib线性回归算法 102
4．1　线性回归算法 102
4．1．1　数学模型 102
4．1．2　最小二乘法 105
4．1．3　梯度下降算法 105
4．2　源码分析 106
4．2．1　建立线性回归 108
4．2．2　模型训练run方法 111
4．2．3　权重优化计算 114
4．2．4　线性回归模型 121
4．3　实例 123
4．3．1　训练数据 123
4．3．2　实例代码 123
第5章　Spark MLlib逻辑回归算法 126
5．1　逻辑回归算法 126
5．1．1　数学模型 126
5．1．2 梯度下降算法 128
5．1．3　正则化 129
5．2　源码分析 132
5．2．1　建立逻辑回归 134
5．2．2　模型训练run方法 137
5．2．3　权重优化计算 137
5．2．4　逻辑回归模型 144
5．3　实例 148
5．3．1　训练数据 148
5．3．2　实例代码 148
第6章　Spark MLlib保序回归算法 151
6．1　保序回归算法 151
6．1．1　数学模型 151
6．1．2　L2保序回归算法 153
6．2　源码分析 153
6．2．1　建立保序回归 154
6．2．2　模型训练run方法 156
6．2．3　并行PAV计算 156
6．2．4　PAV计算 157
6．2．5　保序回归模型 159
6．3　实例 164
6．3．1　训练数据 164
6．3．2　实例代码 164

第三部分　Spark MLlib分类算法
第7章　Spark MLlib贝叶斯分类算法 170
7．1　贝叶斯分类算法 170
7．1．1　贝叶斯定理 170
7．1．2　朴素贝叶斯分类 171
7．2　源码分析 173
7．2．1　建立贝叶斯分类 173
7．2．2　模型训练run方法 176
7．2．3　贝叶斯分类模型 179
7．3　实例 181
7．3．1　训练数据 181
7．3．2　实例代码 182
第8章　Spark MLlib SVM支持向量机算法 184
8．1　SVM支持向量机算法 184
8．1．1　数学模型 184
8．1．2　拉格朗日 186
8．2　源码分析 189
8．2．1　建立线性SVM分类 191
8．2．2　模型训练run方法 194
8．2．3　权重优化计算 194
8．2．4　线性SVM分类模型 196
8．3　实例 199
8．3．1　训练数据 199
8．3．2　实例代码 199
第9章　Spark MLlib决策树算法 202
9．1　决策树算法 202
9．1．1　决策树 202
9．1．2　特征选择 203
9．1．3　决策树生成 205
9．1．4　决策树生成实例 206
9．1．5　决策树的剪枝 208
9．2　源码分析 209
9．2．1　建立决策树 211
9．2．2　建立随机森林 216
9．2．3　建立元数据 220
9．2．4　查找特征的分裂及划分 223
9．2．5　查找最好的分裂顺序 228
9．2．6　决策树模型 231
9．3　实例 234
9．3．1　训练数据 234
9．3．2　实例代码 234

第四部分　Spark MLlib聚类算法
第10章　Spark MLlib KMeans聚类算法 238
10．1　KMeans聚类算法 238
10．1．1　KMeans算法 238
10．1．2　演示KMeans算法 239
10．1．3　初始化聚类中心点 239
10．2　源码分析 240
10．2．1　建立KMeans聚类 242
10．2．2　模型训练run方法 247
10．2．3　聚类中心点计算 248
10．2．4　中心点初始化 251
10．2．5　快速距离计算 254
10．2．6　KMeans聚类模型 255
10．3　实例 258
10．3．1　训练数据 258
10．3．2　实例代码 259
第11章　Spark MLlib LDA主题模型算法 261
11．1　LDA主题模型算法 261
11．1．1　LDA概述 261
11．1．2　LDA概率统计基础 262
11．1．3　LDA数学模型 264
11．2　GraphX基础 267
11．3　源码分析 270
11．3．1　建立LDA主题模型 272
11．3．2　优化计算 279
11．3．3　LDA模型 283
11．4　实例 288
11．4．1　训练数据 288
11．4．2　实例代码 288

第五部分　Spark MLlib关联规则挖掘算法
第12章　Spark MLlib FPGrowth关联规则算法 292
12．1　FPGrowth关联规则算法 292
12．1．1　基本概念 292
12．1．2　FPGrowth算法 293
12．1．3　演示FP树构建 294
12．1．4　演示FP树挖掘 296
12．2　源码分析 298
12．2．1　FPGrowth类 298
12．2．2　关联规则挖掘 300
12．2．3　FPTree类 303
12．2．4　FPGrowthModel类 306
12．3　实例 306
12．3．1　训练数据 306
12．3．2　实例代码 306

第六部分　Spark MLlib推荐算法
第13章　Spark MLlib ALS交替最小二乘算法 310
13．1　ALS交替最小二乘算法 310
13．2　源码分析 312
13．2．1　建立ALS 314
13．2．2　矩阵分解计算 322
13．2．3　ALS模型 329
13．3　实例 334
13．3．1　训练数据 334
13．3．2　实例代码 334
第14章　Spark MLlib协同过滤推荐算法 337
14．1　协同过滤推荐算法 337
14．1．1　协同过滤推荐概述 337
14．1．2　用户评分 338
14．1．3　相似度计算 338
14．1．4　推荐计算 340
14．2　协同推荐算法实现 341
14．2．1　相似度计算 344
14．2．2　协同推荐计算 348
14．3　实例 350
14．3．1　训练数据 350
14．3．2　实例代码 350

第七部分　Spark MLlib神经网络算法
第15章　Spark MLlib神经网络算法综述 354
15．1　人工神经网络算法 354
15．1．1　神经元 354
15．1．2　神经网络模型 355
15．1．3 信号前向传播 356
15．1．4　误差反向传播 357
15．1．5　其他参数 360
15．2　神经网络算法实现 361
15．2．1　神经网络类 363
15．2．2　训练准备 370
15．2．3　前向传播 375
15．2．4　误差反向传播 377
15．2．5　权重更新 381
15．2．6　ANN模型 382
15．3　实例 384
15．3．1　测试数据 384
15．3．2　测试函数代码 387
15．3．3　实例代码 388


第1章PythonSpark机器学习与Hadoop大数据1
1.1机器学习的介绍2
1.2Spark的介绍5
1.3Spark数据处理RDD、DataFrame、SparkSQL7
1.4使用Python开发Spark机器学习与大数据应用8
1.5PythonSpark机器学习9
1.6SparkMLPipeline机器学习流程介绍10
1.7Spark2.0的介绍12
1.8大数据定义13
1.9Hadoop简介14
1.10HadoopHDFS分布式文件系统14
1.11HadoopMapReduce的介绍17
1.12结论18
第2章VirtualBox虚拟机软件的安装19
2.1VirtualBox的下载和安装20
2.2设置VirtualBox存储文件夹23
2.3在VirtualBox创建虚拟机25
2.4结论29
第3章UbuntuLinux操作系统的安装30
3.1UbuntuLinux操作系统的安装31
3.2在Virtual设置Ubuntu虚拟光盘文件33
3.3开始安装Ubuntu35
3.4启动Ubuntu40
3.5安装增强功能41
3.6设置默认输入法45
3.7设置“终端”程序48
3.8设置“终端”程序为白底黑字49
3.9设置共享剪贴板50
3.10设置最佳下载服务器52
3.11结论56
第4章HadoopSingleNodeCluster的安装57
4.1安装JDK58
4.2设置SSH无密码登录61
4.3下载安装Hadoop64
4.4设置Hadoop环境变量67
4.5修改Hadoop配置设置文件69
4.6创建并格式化HDFS目录73
4.7启动Hadoop74
4.8打开HadoopResource-ManagerWeb界面76
4.9NameNodeHDFSWeb界面78
4.10结论79
第5章HadoopMultiNodeCluster的安装80
5.1把SingleNodeCluster复制到data183
5.2设置VirtualBox网卡84
5.3设置data1服务器87
5.4复制data1服务器到data2、data3、master94
5.5设置data2服务器97
5.6设置data3服务器100
5.7设置master服务器102
5.8master连接到data1、data2、data3创建HDFS目录107
5.9创建并格式化NameNodeHDFS目录110
5.10启动HadoopMultiNodeCluster112
5.11打开HadoopResourceManagerWeb界面114
5.12打开NameNodeWeb界面115
5.13停止HadoopMultiNodeCluster116
5.14结论116
第6章HadoopHDFS命令117
6.1启动HadoopMulti-NodeCluster118
6.2创建与查看HDFS目录120
6.3从本地计算机复制文件到HDFS122
6.4将HDFS上的文件复制到本地计算机127
6.5复制与删除HDFS文件129
6.6在HadoopHDFSWeb用户界面浏览HDFS131
6.7结论134
第7章HadoopMapReduce135
7.1简单介绍WordCount.java136
7.2编辑WordCount.java137
7.3编译WordCount.java141
7.4创建测试文本文件143
7.5运行WordCount.java145
7.6查看运行结果146
7.7结论147
第8章PythonSpark的介绍与安装148
8.1Scala的介绍与安装150
8.2安装Spark153
8.3启动pyspark交互式界面156
8.4设置pyspark显示信息157
8.5创建测试用的文本文件159
8.6本地运行pyspark程序161
8.7在HadoopYARN运行pyspark163
8.8构建SparkStandaloneCluster运行环境165
8.9在SparkStandalone运行pyspark171
8.10SparkWebUI界面173
8.11结论175
目录
第1章1
1.1引言1
1.2基本术2
1.3假设空间4
1.4归纳偏好6
1.5发展历程10
1.6应用现状13
1.7阅读材料16
习题19
参考文献20
休息一会儿22
第2章模型评估与选择23
2.1经验误差与过拟合23
2.2评估方法24
2.2.1留出法25
2.2.2交叉验证法26
2.2.3自助法27
2.2.4调参与最终模型28
2.3性能度量28
2.3.1错误率与精度29
2.3.2查准率、查全率与F130
2.3.3ROC与AUC33
2.3.4代价敏感错误率与代价曲线35
2.4比较检验37
2.4.1假设检验37
2.4.2交叉验证t检验40
2.4.3McNemar检验41
2.4.4Friedman检验与后续检验42
2.5偏差与方差44
2.6阅读材料46
习题48
参考文献49
休息一会儿51
第3章线性模型53
3.1基本形式53
3.2线性回归53
3.3对数几率回归57
3.4线性判别分析60
3.5多分类学习63
3.6类别不平衡问题66
3.7阅读材料67
习题69
参考文献70
休息一会儿72
第4章决策树73
4.1基本流程73
4.2划分选择75
4.2.1信息增益75
4.2.2增益率77
4.2.3基尼指数79
4.3剪枝处理79
4.3.1预剪枝80
4.3.2后剪枝82
4.4连续与缺失值83
4.4.1连续值处理83
4.4.2缺失值处理85
4.5多变量决策树88
4.6阅读材料92
习题93
参考文献94
休息一会儿95
第5章神经网络97
5.1神经元模型97
5.2感知机与多层网络98
5.3误差逆传播算法101
5.4全局最小与局部极小106
5.5其他常见神经网络108
5.5.1RBF网络108
5.5.2ART网络108
5.5.3SOM网络109
5.5.4级联相关网络110
5.5.5Elman网络111
5.5.6Boltzmann机111
5.6深度学习113
5.7阅读材料115
习题116
参考文献117
休息一会儿120
第6章支持向量机121
6.1间隔与支持向量121
6.2对偶问题123
6.3核函数126
6.4软间隔与正则化129
6.5支持向量回归133
6.6核方法137
6.7阅读材料139
习题141
参考文献142
休息一会儿145
第7章贝叶斯分类器147
7.1贝叶斯决策论147
7.2极大似然估计149
7.3朴素贝叶斯分类器150
7.4半朴素贝叶斯分类器154
7.5贝叶斯网156
7.5.1结构157
7.5.2学习159
7.5.3推断161
7.6EM算法162
7.7阅读材料164
习题166
参考文献167
休息一会儿169
第8章集成学习171
8.1个体与集成171
8.2Boosting173
8.3Bagging与随机森林178
8.3.1Bagging178
8.3.2随机森林179
8.4结合策略181
8.4.1平均法181
8.4.2投票法182
8.4.3学习法183
8.5多样性185
8.5.1误差--分歧分解185
8.5.2多样性度量186
8.5.3多样性增强188
8.6阅读材料190
习题192
参考文献193
休息一会儿196
第9章聚类197
9.1聚类任务197
9.2性能度量197
9.3距离计算199
9.4原型聚类202
9.4.1k均值算法202
9.4.2学习向量量化204
9.4.3高斯混合聚类206
9.5密度聚类211
9.6层次聚类214
9.7阅读材料217
习题220
参考文献221
休息一会儿224
第10章降维与度量学习225
10.1k近邻学习225
10.2低维嵌入226
10.3主成分分析229
10.4核化线性降维232
10.5流形学习234
10.5.1等度量映射234
10.5.2局部线性嵌入235
10.6度量学习237
10.7阅读材料240
习题242
参考文献243
休息一会儿246
第11章特征选择与稀疏学习247
11.1子集搜索与评价247
11.2过滤式选择249
11.3包裹式选择250
11.4嵌入式选择与L$_1$正则化252
11.5稀疏表示与字典学习254
11.6压缩感知257
11.7阅读材料260
习题262
参考文献263
休息一会儿266
第12章计算学习理论267
12.1基础知识267
12.2PAC学习268
12.3有限假设空间270
12.3.1可分情形270
12.3.2不可分情形272
12.4VC维273
12.5Rademacher复杂度279
12.6稳定性284
12.7阅读材料287
习题289
参考文献290
休息一会儿292
第13章半监督学习293
13.1未标记样本293
13.2生成式方法295
13.3半监督SVM298
13.4图半监督学习300
13.5基于分歧的方法304
13.6半监督聚类307
13.7阅读材料311
习题313
参考文献314
休息一会儿317
第14章概率图模型319
14.1隐马尔可夫模型319
14.2马尔可夫随机场322
14.3条件随机场325
14.4学习与推断328
14.4.1变量消去328
14.4.2信念传播330
14.5近似推断331
14.5.1MCMC采样331
14.5.2变分推断334
14.6话题模型337
14.7阅读材料339
习题341
参考文献342
休息一会儿345
第15章规则学习347
15.1基本概念347
15.2序贯覆盖349
15.3剪枝优化352
15.4一阶规则学习354
15.5归纳逻辑程序设计357
15.5.1最小一般泛化358
15.5.2逆归结359
15.6阅读材料363
习题365
参考文献366
休息一会儿369
第16章强化学习371
16.1任务与奖赏371
16.2$K$-摇臂赌博机373
16.2.1探索与利用373
16.2.2$\epsilon$-贪心374
16.2.3Softmax375
16.3有模型学习377
16.3.1策略评估377
16.3.2策略改进379
16.3.3策略迭代与值迭代381
16.4免模型学习382
16.4.1蒙特卡罗强化学习383
16.4.2时序差分学习386
16.5值函数近似388
16.6模仿学习390
16.6.1直接模仿学习391
16.6.2逆强化学习391
16.7阅读材料393
习题394
参考文献395
休息一会儿397
附录399
A矩阵399
B优化403
C概率分布409
后记417
索引419






<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.pingan.spark2</groupId>
  <artifactId>sparksql</artifactId>
  <version>1.0</version>
  <inceptionYear>2008</inceptionYear>
  <properties>
    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
    <spark.version>2.2.1</spark.version>
    <scala.version>2.11.8</scala.version>
  </properties>

  <build>
    <plugins>
      <plugin>
        <artifactId>maven-assembly-plugin</artifactId>
        <configuration>
          <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
          </descriptorRefs>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-compiler-plugin</artifactId>
        <version>3.1</version>
        <configuration>
          <source>1.8</source>
          <target>1.8</target>
        </configuration>
      </plugin>
      <!--<plugin>-->
      <!--<groupId>org.apache.maven.plugins</groupId>-->
      <!--<artifactId>maven-dependency-plugin</artifactId>-->
      <!--<version>3.1.1</version>-->
      <!--<executions>-->
      <!--<execution>-->
      <!--<id>copy-dependencies</id>-->
      <!--<phase>package</phase>-->
      <!--<goals>-->
      <!--<goal>copy-dependencies</goal>-->
      <!--</goals>-->
      <!--<configuration>-->
      <!--<outputDirectory>target/lib</outputDirectory>-->
      <!--<excludeArtifactIds>-->
      <!--spring-boot-devtools,junit-->
      <!--</excludeArtifactIds>-->
      <!--<overWriteSnapshots>true</overWriteSnapshots>-->
      <!--</configuration>-->
      <!--</execution>-->
      <!--</executions>-->
      <!--</plugin>-->
      <!--<plugin>-->
      <!--<groupId>org.apache.maven.plugins</groupId>-->
      <!--<artifactId>maven-jar-plugin</artifactId>-->
      <!--<version>2.3.1</version>-->
      <!--<configuration>-->
      <!--<outputDirectory>src/hdp/LBDP-COREPUB1.26.4/sx_hx_safe/lrs/jar</outputDirectory>-->
      <!--</configuration>-->
      <!--</plugin>-->
      <!--<plugin>-->
      <!--<groupId>org.apache.maven.plugins</groupId>-->
      <!--<artifactId>maven-assembly-plugin</artifactId>-->
      <!--<version>2.6</version>-->
      <!--<configuration>-->
      <!--<appendAssemblyId>false</appendAssemblyId>-->
      <!--<outputDirectory>src/hdp/LBDP-COREPUB1.26.4</outputDirectory>-->
      <!--<!–<finalName>${project.name}</finalName>–>-->
      <!--<finalName>lrs</finalName>-->
      <!--<descriptors>-->
      <!--<descriptor>assembly.xml</descriptor>-->
      <!--</descriptors>-->
      <!--</configuration>-->
      <!--<executions>-->
      <!--<execution>-->
      <!--<id>make-assembly-tar</id>-->
      <!--<phase>package</phase>-->
      <!--<goals>-->
      <!--<goal>single</goal>-->
      <!--</goals>-->
      <!--</execution>-->
      <!--</executions>-->
      <!--</plugin>-->
    </plugins>
  </build>


  <dependencies>
    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-library</artifactId>
      <version>${scala.version}</version>
    </dependency>
    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-reflect</artifactId>
      <version>${scala.version}</version>
    </dependency>
    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-compiler</artifactId>
      <version>${scala.version}</version>
    </dependency>

    <dependency>
      <groupId>org.scala-lang</groupId>
      <artifactId>scala-actors</artifactId>
      <version>${scala.version}</version>
    </dependency>



    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>4.4</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.specs</groupId>
      <artifactId>specs</artifactId>
      <version>1.2.5</version>
      <scope>test</scope>
    </dependency>
    <!--<dependency>-->
    <!--<groupId>org.apache.spark</groupId>-->
    <!--<artifactId>spark-core_2.11</artifactId>-->
    <!--<version>1.6.2</version>-->
    <!--<scope>provided</scope>-->
    <!--</dependency>-->
    <!--<dependency>-->
    <!--<groupId>org.apache.spark</groupId>-->
    <!--<artifactId>spark-hive_2.11</artifactId>-->
    <!--<version>1.6.1</version>-->
    <!--<scope>provided</scope>-->
    <!--</dependency>-->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_2.11</artifactId>
      <version>${spark.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_2.11</artifactId>
      <version>${spark.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_2.11</artifactId>
      <version>${spark.version}</version>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_2.11</artifactId>
      <version>${spark.version}</version>
    </dependency>

    <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-mllib -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-mllib_2.11</artifactId>
      <version>${spark.version}</version>
      <!--<scope>runtime</scope>-->
    </dependency>


    <dependency>
      <groupId>com.databricks</groupId>
      <artifactId>spark-csv_2.11</artifactId>
      <version>1.5.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.commons</groupId>
      <artifactId>commons-jexl</artifactId>
      <version>2.1.1</version>
    </dependency>
    <dependency>
      <groupId>dom4j</groupId>
      <artifactId>dom4j</artifactId>
      <version>1.6.1</version>
    </dependency>
    <!-- log4j -->
    <!--<dependency>-->
    <!--<groupId>org.slf4j</groupId>-->
    <!--<artifactId>slf4j-log4j12</artifactId>-->
    <!--<version>1.7.2</version>-->
    <!--</dependency>-->
    <!-- oracle -->
    <!--<dependency>-->
    <!--<groupId>com.oracle</groupId>-->
    <!--<artifactId>ojdbc14</artifactId>-->
    <!--<version>10.2.0.1.0</version>-->
    <!--</dependency>-->
    <!--        <dependency>
                <groupId>com.pingan.lcloud.shield</groupId>
                <artifactId>shield-spark-common</artifactId>
                <version>1.0.0-SNAPSHOT</version>
            </dependency>-->
    <!--<dependency>-->
    <!--<groupId>org.mongodb.mongo-hadoop</groupId>-->
    <!--<artifactId>mongo-hadoop-core</artifactId>-->
    <!--<version>1.4.2</version>-->
    <!--</dependency>-->
    <!--<dependency>-->
    <!--<groupId>org.mongodb</groupId>-->
    <!--<artifactId>mongo-java-driver</artifactId>-->
    <!--<version>2.13.0</version>-->
    <!--</dependency>-->
    <!--<!– 阿里 FastJson依赖 –>-->
    <!--<dependency>-->
    <!--<groupId>com.alibaba</groupId>-->
    <!--<artifactId>fastjson</artifactId>-->
    <!--<version>1.2.22</version>-->
    <!--</dependency>-->
    <!--<!– jexl解析字符串表达式 –>-->
    <!--<dependency>-->
    <!--<groupId>org.apache.commons</groupId>-->
    <!--<artifactId>commons-jexl3</artifactId>-->
    <!--<version>3.1</version>-->
    <!--</dependency>-->
    <!--<dependency>-->
    <!--<groupId>org.apache.commons</groupId>-->
    <!--<artifactId>commons-jexl</artifactId>-->
    <!--<version>2.1.1</version>-->
    <!--</dependency>-->
    <dependency>
      <groupId>org.jfree</groupId>
      <artifactId>jfreechart</artifactId>
      <version>1.0.19</version>
    </dependency>
    <dependency>
      <groupId>org.jblas</groupId>
      <artifactId>jblas</artifactId>
      <version>1.2.3</version>
    </dependency>
  </dependencies>
</project>

========

Spark SQL的Parquet那些事儿.docx
原创： 深圳浪尖  Spark学习技巧  4月28日
Parquet是一种列式存储格式，很多种处理引擎都支持这种存储格式，也是sparksql的默认存储格式。Spark SQL支持灵活的读和写Parquet文件，并且对parquet文件的schema可以自动解析。当Spark SQL需要写成Parquet文件时，处于兼容的原因所有的列都被自动转化为了nullable。

1
读写Parquet文件
// Encoders for most common types are automatically provided by importing spark.implicits._
import spark.implicits._

val peopleDF = spark.read.json("examples/src/main/resources/people.json")
// DataFrames can be saved as Parquet files, maintaining the schema information
peopleDF.write.parquet("people.parquet")


// Read in the parquet file created above
// Parquet files are self-describing so the schema is preserved
// The result of loading a Parquet file is also a DataFrame
val parquetFileDF = spark.read.parquet("people.parquet")


// Parquet files can also be used to create a temporary view and then used in SQL statements
parquetFileDF.createOrReplaceTempView("parquetFile")
val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")
namesDF.map(attributes => "Name: " + attributes(0)).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

2
分区发现
分区表时很多系统支持的，比如hive，对于一个分区表，往往是采用表中的某一或多个列去作为分区的依据，分区是以文件目录的形式体现。所有内置的文件源(Text/CSV/JSON/ORC/Parquet)都支持自动的发现和推测分区信息。例如，我们想取两个分区列，gender和country，先按照性别分区，再按照国家分区：

path
└── to
   └── table
       ├── gender=male
       │   ├── ...
       │   │
       │   ├── country=US
       │   │   └── data.parquet
       │   ├── country=CN
       │   │   └── data.parquet
       │   └── ...
       └── gender=female
           ├── ...
           │
           ├── country=US
           │   └── data.parquet
           ├── country=CN
           │   └── data.parquet
           └── ...

SparkSession.read.parquet 或者 SparkSession.read.load读取的目录为path/to/table的时候，会自动从路径下抽取分区信息。返回DataFrame的表结构为：

root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true)

细细分析一下你也会发现分区列的数据类型也是自动推断的。当前支持的数据类型有，数字类型，date，timestamp和string类型。有时候用户可能不希望自动推断分区列的类型，这时候只需要将spark.sql.sources.partitionColumnTypeInference.enabled配置为false即可。如果分区列的类型推断这个参数设置为了false，那么分区列的类型会被认为是string。

从spark 1.6开始，分区发现默认情况只会发现给定路径下的分区。比如，上面的分区表，假如你讲路径path/to/table/gender=male传递给SparkSession.read.parquet 或者 SparkSession.read.load 那么gender不会被认为是分区列。如果想检测到该分区，传给spark的路径应该是其父路径也即是path/to/table/，这样gender就会被认为是分区列。

3
schema合并
跟protocol buffer，avro，thrift一样，parquet也支持schema演变升级。用户可以在刚开始的时候创建简单的schema，然后根据需要随时扩展新的列。

spark sql 用Parquet 数据源支持自动检测新增列并且会合并schema。

由于合并schema是一个相当耗费性能的操作，而且很多情况下都是不必要的，所以从spark 1.5开始就默认关闭掉该功能。有两种配置开启方式：

1.通过数据源option设置mergeSchema为true。

2.在全局sql配置中设置spark.sql.parquet.mergeSchema 为true.

// This is used to implicitly convert an RDD to a DataFrame.
import spark.implicits._


// Create a simple DataFrame, store into a partition directory
val squaresDF = spark.sparkContext.makeRDD(1 to 5).map(i => (i, i * i)).toDF("value", "square")
squaresDF.write.parquet("data/test_table/key=1")


// Create another DataFrame in a new partition directory,
// adding a new column and dropping an existing column
val cubesDF = spark.sparkContext.makeRDD(6 to 10).map(i => (i, i * i * i)).toDF("value", "cube")
cubesDF.write.parquet("data/test_table/key=2")


// Read the partitioned table
val mergedDF = spark.read.option("mergeSchema", "true").parquet("data/test_table")
mergedDF.printSchema()


// The final schema consists of all 3 columns in the Parquet files together
// with the partitioning column appeared in the partition directory paths
// root
//  |-- value: int (nullable = true)
//  |-- square: int (nullable = true)
//  |-- cube: int (nullable = true)
//  |-- key: int (nullable = true)

4
hive metastore Parquet表转换
当读写hive metastore parquet格式表的时候，Spark SQL为了较好的性能会使用自己默认的parquet格式而不是采用hive SerDe。该行为是通过参数spark.sql.hive.convertMetastoreParquet空值，默认是true。

5
Hive和parquet兼容性
从表schema处理角度讲hive和parquet有两个主要的区别

hive是大小写敏感的，但是parquet不是。

hive会讲所有列视为nullable，但是nullability在parquet里有独特的意义。

由于上面的原因，在将hive metastore parquet转化为spark parquet表的时候，需要处理兼容一下hive的schema和parquet的schema。兼容处理的原则是：

有相同名字的字段必须要有相同的数据类型，忽略nullability。兼容处理的字段应该保持parquet侧的数据类型，这样就可以处理到nullability类型了。

兼容处理的schema应直接包含在hive元数据里的schema信息：

任何仅仅出现在parquet schema的字段将会被删除

任何仅仅出现在hive 元数据里的字段将会被视为nullable。

6
元数据刷新
Spark SQL为了更好的性能会缓存parquet的元数据。当spark 读取hive表的时候，schema一旦从hive转化为spark sql的，就会被spark sql缓存，如果此时表的schema被hive或者其他外部工具更新，必须要手动的去刷新元数据，才能保证元数据的一致性。

spark.catalog.refreshTable("my_table")
7
配置
parquet的相关的参数可以通过setconf或者set key=value的形式配置。

spark.sql.parquet.binaryAsString 默认值是false。一些parquet生产系统，尤其是impala，hive和老版本的spark sql，不区分binary和string类型。该参数告诉spark 讲binary数据当作字符串处理。

spark.sql.parquet.int96AsTimestamp 默认是true。有些parquet生产系统，尤其是parquet和hive，将timestamp翻译成INT96.该参数会提示Spark SQL讲INT96翻译成timestamp。

spark.sql.parquet.compression.codec 默认是snappy。当写parquet文件的时候设置压缩格式。如果在option或者properties里配置了compression或者parquet.compression优先级依次是：compression，parquet.compression，spark.sql.parquet.compression.codec。支持的配置类型有:none，uncompressed，snappy，gzip，lzo，brotli，lz4，zstd。在hadoop2.9.0之前，zstd需要安装ZstandardCodec，brotli需要安装BrotliCodec。

spark.sql.parquet.filterPushdown 默认是true。设置为true代表开启parquet下推执行优化。

spark.sql.hive.convertMetastoreParquet 默认是true。假如设置为false，spark sql会读取hive parquet表的时候使用Hive SerDe，替代内置的。

spark.sql.parquet.mergeSchema 默认是false。当设置为true的时候，parquet数据源会合并读取所有的parquet文件的schema，否则会从summary文件或者假如没有summary文件的话随机的选一些数据文件来合并schema。

spark.sql.parquet.writeLegacyFormat 默认是false。如果设置为true 数据会以spark 1.4和更早的版本的格式写入。比如，decimal类型的值会被以apache parquet的fixed-length byte array格式写出，该格式是其他系统例如hive，impala等使用的。如果是false，会使用parquet的新版格式。例如，decimals会以int-based格式写出。如果spark sql要以parquet输出并且结果会被不支持新格式的其他系统使用的话，需要设置为true。


-----------------
Spark调优系列之硬件要求
原创： 浪尖  Spark学习技巧  2017-07-25
估计所有的spark开发者都很关心spark的硬件要求。恰当的硬件配置需要具体情况具体分析，浪尖在这里给出以下建议。

一，存储系统

因为因为大多数Spark工作可能需要从外部存储系统（例如Hadoop文件系统或HBase）中读取输入数据，所以将其尽可能靠近该系统很重要。所以，有如下建议：

1，如果可能，在与HDFS相同的节点上运行Spark。最简单的方式是将spark 的Standalone集群和hadoop集群安装在相同的节点，同时配置好Spark和hadoop的内存使用，避免相互干扰(对于hadoop，每个task的内存配置参数是mapred.child.java.opts； mapreduce.tasktracker.map.tasks.maximum 和mapreduce.tasktracker.reduce.tasks.maximum 决定了task的数目)。也可以将hadoop和spark运行在共同的集群管理器上，如mesos和 yarn。

2，如果不可能，请在与HDFS相同的局域网中的不同节点上运行Spark。

3，对于低延迟数据存储（如HBase），可能优先在与存储系统不同的节点上运行计算任务以避免干扰。

二，本地磁盘

虽然Spark可以在内存中执行大量的计算，但它仍然使用本地磁盘来存储不适合RAM的数据，以及在stage之间，也即shuffle的中间结果。我们建议每个节点至少有4-8块磁盘，并且不需要RAID，仅仅是独立的磁盘挂在节点。在Linux中，使用noatime选项安装磁盘，以减少不必要的写入。在spark任务中，spark.local.dir配置可以十多个磁盘目录，以逗号分开。如果你运行在hdfs上，与hdfs保持一致就很好。

使用noatime选项安装磁盘，要求当挂载文件系统时，可以指定标准Linux安装选项（noatime），这将禁用该文件系统上的atime更新。磁盘挂在命令：

mount -t gfs BlockDevice MountPoint -o noatime

BlockDevice 指定GFS文件系统驻留的块设备。

MountPoint 指定GFS文件系统应安装的目录。

例子：

mount -t gfs /dev/vg01/lvol0 /gfs1 -o noatime

三，内存

单台机器内存从8GB到数百GB，spark都能运行良好。在所有情况下，我们建议仅为Spark分配最多75％的内存;留下其余的操作系统和缓冲区缓存。

需要多少内存取决于你的应用程序。要确定你的应用的特定数据集需要多大内存，请加载部分数据集到内存，然后在Spark UI的Storage界面去看它的内存占用量。

请注意，内存使用受到存储级别和序列化格式的极大影响 - 有关如何减少内存使用的技巧，请参阅另一篇调优的文章。

最后，请注意，对于超过200GB的内存的机器JAVA VM运行状态并不一直表现良好。如果你买的机器内存超过了200GB，那么可以在一个节点上运行多个worker。Spark Standalone模式下，你可以在配置文件 conf/spark-env.sh中设置SPARK_WORKER_INSTANCES的值来设置单节点worker的数目。也可以设置SPARK_WORKER_CORES参数来设置每个Worker的cpu数目。

四，网络

根据以往的经验，假如数据是在内存中，那么spark的应用的瓶颈往往就在网络。用10 Gigabit或者更高的网络，是使spark应用跑的最更快的最佳方式。特别是针对“distributed reduce”应用，如group-bys,reduce-bys和sql joins，就表现的更加明显。在任何给定的应用程序中，你可以通过spark ui查看spark shuffle过程夸网络传输了多少数据。

五，cpu

即使每台机器几十个cpu，spark也可以很好的扩展，因为他在线程之间执行最小的共享cpu。你应该每台机器至少配置8-16个内核。根据cpu负载，可能需要更多的cpu：一旦数据在内存中，大多数应用程序的瓶颈就在CPU和内存。

-----------
Spark2.4.0屏障调度器
原创： 浪尖  Spark学习技巧  2018-11-14
前几天，浪尖发了一篇文章，讲了Spark 2.4发布更新情况：

Spark2.4.0发布了！

其中，就有一项说到Spark 为了支持深度学习而引入的屏障调度器。本文就详细讲讲。

基于消息传递结构的计算模型和Spark计算模型是有很大区别。在Spark 内部，每个Stage的某一个一个task不会依赖于相同Stage任何其他的task，因此，Spark的task 可以被独立进行调度执行。为了在Spark中嵌入MPI功能，需要引入一个新的调度模型，暂时命名为“屏障调度”（浪尖直译自barrier scheduling），该调度模型会同时启动任务，并为用户提供足够的信息和工具，将分布式DL训练嵌入到Spark Pipeline中。 Spark还为MPI任务引入了一种新的容错机制。当任何MPI任务在中间失败时，Spark将中止所有任务并重新启动该stage。

1. 要求

概述

每个job中单个barrier stage。

每个job中多个barrier stage。

多job且每个job都带有barrier stage。

Barrier stage 请求的slot比可用的slot多(无动态资源申请)。

Barrier stage请求的slot比可用的slot多(有动态资源申请)。(Spark 2.4就不要想了)

目标

支持barrier调度：对于同一个barrierstage同时启动所有task，并且提供给用户足够的信息和工具，以便用户可以嵌入分布式DL训练模型。

正确的处理失败的场景。

Barrier执行模式支持运行与Standalone模式

使用yarn/mesos/k8s的用户可以再有BarrierStage的时候设置MPI。

安全

用户使用外部线程启动MPI任务的时候，存在外部进行不被杀死而导致内存泄漏的风险。Barrier tasks会使用远程客户端相互交流，但是不会影响Spark当前的安全模型。

API变化

class RDD[T] {
 /** Indicates that Spark must launch the tasks together for the current stage. */
 def barrier(): RDDBarrier[T] = ???
}
/** A [[TaskContext]] with extra info and tooling for a barrier stage. */
class BarrierTaskContext extends TaskContext {
 /** Sets a global barrier and waits until all tasks in this stage hit this
barrier. */
 def barrier(): Unit = ???
 /** Returns the all task infos in this stage. */
 def getTaskInfos(): Array[BarrierTaskInfo]
}
/** Represents an RDD barrier, which forces Spark to launch tasks of this stage
together. */
class RDDBarrier[T] {
 /** Maps partitions together with a provided [[BarrierTaskContext]]. */
 def mapPartitions[S](f: Iterator[T] => Iterator[S]): RDD[S] = ???
 /** TODO extra conf(e.g. timeout) */
}
使用案例

rdd.barrier().mapPartitions { iter =>
   // Write iter to disk.
   ???
   // Fetch TaskContext
   val context = BarrierTaskContext.get()
   // Wait until all tasks finished writing.
   context.barrier()
   // The 0-th task launches an MPI job.
   if (context.partitionId() == 0) {
     val hosts = context.getTaskInfos().map(_.address)
        // Set up MPI machine file using host infos.
     ???
     // Launch the MPI job by calling mpirun.
??? }
   // Wait until the MPI job finished.
   context.barrier()
   // Collect output and return.
??? }
3. 架构

设计提议

为了使spark支持屏障调度(barrier scheduling)，在Spark内部增加了RDDBarrier和BarrierTaskContext。

BarrierStage

如果没有充足的slot资源，barrier stage不会被拉起(也即是空闲的core 必须能够拉起该barrier所有tasks)，这样设计使为了满足一次拉起所有task的目标。

同时当任意的task执行失败的时候，总是重启整个barrier stage。

判断一个stage是否是Barrier stage的一种方式是跟踪Stage所包含的RDD，如果该stage包含RDDBarrier 或者至少一个父RDD是RDDBarrier，那么该stage是一个barrier stage，当然要以shuffleDependency为界限。

调度Barrier Tasks

目前，TaskScheduler会尽可能的在可用的slot上调度task，所以通常不会同时启动同一个stage的所有task。因此需要在barrier stage 的task在调度之前加上资源可用性判断。由于任务的局部性问题，仍然可能仅启动整个barrier stage的部分tasks，因此必须在启动任务之前在此检查确认同一个barrier stage的所有task同时被启动。

Barrier tasks预计比常规tasks具有更长的生命周期，因此barriertasks可能会在相对长的时间范围内占用集群资源，后续提交的任务估计会延迟运行或者仅使用更少的slot运行。建议使用Fair调度策略进行调度，而不是默认的FIFO调度策略，并将barrier任务独立运行，这样至少可以保证普通任务可以在配置给定最少的集群资源上运行。

另一个问题是barrier stage可以提交，但是集群当前没有足够的slot来同时启动所有barrier tasks。如果启用了动态资源分配，则在等待一段时间后，可能会或可能不会满足要求（取决于允许的最大节点）。对于Spark 2.4，提出了一个简单的解决方案，它只检查当前运行的slot的总数，如果数量不足以同时启动同一个stage的所有屏障任务，则取消该job。目标是在3.0的时候可以更好地与动态资源分配集成。对于Spark 2.4，在启用动态资源分配时，job会立即失败，或者job无法连续提交，因为它试图提交一个barrier stage，该stage需要比集群中当前可用的slot更多的slot。

Task Barrier

Barrier tasks应允许用户在task执行过程中插入同步操作，这可以通过在BarrierTaskContext中引入全局barrier操作来实现，这使得当前任务等待直到同一stage中的所有task都达到此barrier。将为BarrierTaskContext.barrier（）提交单独的设计文档。

关注公众号，bigdatatip，回复barrier 即可获得该文档。

失败容错

为确保正确性，当任何task失败时，barrier stage始终会重试整个stage。因此，将要求杀死失败stage的所有正在运行的任务，并且还保证每个单个stage最多只能运行一个taskset (没有zombie task)，这是非常简单的。理想情况下，除了在zombie taskset中杀死正在运行的任务需要一段时间，每个单一stage只应运行一个taskset，必须将失败的taskset标记为zombie 并正确处理TaskKilled消息。

推测任务(Speculativetask)

在barrier 执行模式中，要求每个barrier task必须仅有一个唯一的task ID，目的是其他的tasks 可以直接使用该ID和它交互。这也就意味着每个task只能尝试启动一次，因此必须禁止推测执行。

此外，3.0的时候可能会将Spark任务推测执行设置为单个stage的配置而不是全局配置。

SparkContext.runJob()/PartitionPruningRDD

SparkContext.runJob()执行的时候可以仅是所有分区的子集，其中一个用例是RDD.first()，不会执行所有分区。这种是与barrer执行模式冲突的，可能无法启动某些barrier tasks。在barrier stage检测到这种用法，会由于不支持该操作而抛出异常。

ParititionPruningRDD的情况类似，它只在满足`partitionFilterFunc`的分区上启动任务。我们将在barrierstage 检测PartitionPruningRDD并抛出显式异常。

以上问题都与父RDD与生成的RDD具有不同分区数的问题有关（例如union()/ coalesce()/ first()/ take()/ PartitionPruningRDD），因此可以检测RDD的血统链条，然后在job 提交的时候立即停止。

如果RDD依赖于多个barrier RDD（例如，barrierRdd1.zip（barrierRdd2）），也将立即停止，如果发生这种情况，则无法确保`barrier()`调用的正确行为。

针对Spark 3.0，可以进一步调查上述用例并提出支持它们的方法。

----------
扩展RDD API三部曲之第二部自定义操作算子
原创： 浪尖  Spark学习技巧  2018-12-12


扩展RDD API三部曲，主要是帮助大家掌握如下三个内容：

1). 回顾一下RDD的基础

2). 扩展Action，也即是自定义RDD算子

3). 扩展 transform及自定义RDD

本文主要是将自定义Spark RDD算子中的Action 类型操作。

1. 准备阶段

讲到自定义RDD的action操作，大家首先应该想到的就是那些RDD到key-value算子的隐式转换，具体一点也就是PairRDDFunctions这个类里包含的算子，比如reducebykey等操作算子。

具体实现肯定是要比较了解scala的隐式转换操作，这个浪尖也发过文章了，可以点击下文阅读：

Scala语法基础之隐式转换

首先，我们要进行准备操作，首先定义一个case class

class SalesRecord(val transactionId: String,
                 val customerId: String,
                 val itemId: String,
                 val itemValue: Double) extends Comparable[SalesRecord]
 with Serializable {

 override def compareTo(o: SalesRecord): Int = {
   return this.transactionId.compareTo(o.transactionId)
 }

 override def toString: String = {
   transactionId+","+customerId+","+itemId+","+itemValue
 }
}

然后，定义我们的主要函数：

val sparkConf = new SparkConf().setAppName(this.getClass.getName).setMaster("local[*]")
     .set("yarn.resourcemanager.hostname", "mt-mdh.local")
     .set("spark.executor.instances","2")
     .set("spark.default.parallelism","4")
     .set("spark.sql.shuffle.partitions","4")
     .setJars(List("/opt/sparkjar/bigdata.jar"
       ,"/opt/jars/spark-streaming-kafka-0-10_2.11-2.3.1.jar"
       ,"/opt/jars/kafka-clients-0.10.2.2.jar"
       ,"/opt/jars/kafka_2.11-0.10.2.2.jar"))
   val sc = new SparkContext(sparkConf)
   val dataRDD = sc.textFile("file:///opt/bigdata/src/main/data/sales.csv")
   val salesRecordRDD = dataRDD.map(row => {
     val colValues = row.split(",")
     new SalesRecord(colValues(0),colValues(1),colValues(2),colValues(3).toDouble)
   })

这个时候加入我们需要对itemValue字段求和，常见的做法是

salesRecordRDD.map(_.itemValue).sum

其实，sum就是DoubleRDDFunctions内部的算子，也是通过隐式转换实现的。

2. 自定义算子实现

然后就是要定义RDD的操作算子本身，也即是一个工具类，我们叫他为CustomFunctions，内部包含求和函数如下：

import org.apache.spark.rdd.RDD

class CustomFunctions(rdd:RDD[SalesRecord]) {

 def totalSales = rdd.map(_.itemValue).sum

 def discount(discountPercentage:Double) = new DiscountRDD(rdd,discountPercentage)

}

这个仔细读一下上面已有的隐式转换算子，可以发现还不行，需要为自定义RDD的操作算子，自定义一个隐士转换的算子工具，内容如下：

object CustomFunctions {

  implicit def addCustomFunctions(rdd: RDD[SalesRecord]) = new CustomFunctions(rdd)
}

3. 使用算子

调用我们的转换方法：

println("Spark RDD API : "+salesRecordRDD.map(_.itemValue).sum)

import CustomFunctions._
println("Cunstom RDD API : "+salesRecordRDD.totalSales)
输出结果：



这就是自定义RDD的action操作。

下篇文章为自定义RDD和转换操作，这个就只会在星球里分享了欢迎加入浪尖的知识星球，与近420好友一起学习进步。



|spark的重分区及排序
原创： 浪尖  Spark学习技巧  2018-08-14

前几天，有人在星球里，问了一个有趣的算子，也即是RepartitionAndSortWithinPartitions。当时浪尖也在星球里讲了一下，整个关于分区排序的内容。今天，在这里给大家分享一下。



更多大数据小技巧及调优，spark的源码文章，原理文章及源码视频请加入知识星球。扫描，底部二维码，或者点击阅读原文。




昨天说了，mapPartitions 的使用技巧。大家应该都知道mapPartitions值针对整个分区执行map操作。而且对于PairRDD的分区默认是基于hdfs的物理块，当然不可分割的话就是hdfs的文件个数。但是我们也可以给partitionBy 算子传入HashPartitioner，来给RDD进行重新分区，而且会使得key的hashcode相同的数据落到同一个分区。

spark 1.2之后引入了一个高质量的算子repartitionAndSortWithinPartitions 。该算子为spark的Shuffle增加了sort。假如，后面再跟mapPartitions算子的话，其算子就是针对已经按照key排序的分区，这就有点像mr的意思了。与groupbykey不同的是，数据不会一次装入内存，而是使用迭代器一次一条记录从磁盘加载。这种方式最小化了内存压力。

repartitionAndSortWithinPartitions 也可以用于二次排序。

下面举个简单的例子。

import org.apache.spark.Partitioner
 class KeyBasePartitioner(partitions: Int) extends Partitioner {

   override def numPartitions: Int = partitions

   override def getPartition(key: Any): Int = {
     val k = key.asInstanceOf[Int]
     Math.abs(k.hashCode() % numPartitions)
   }
 }

 import org.apache.spark.SparkContext._
     sc.textFile("file:///opt/hadoop/spark-2.3.1/README.md").flatMap(_.split("\\s+")).map((_,1)).reduceByKey(_+_).map(each=>(each._2,each._1))
     implicit val caseInsensitiveOrdering = new Ordering[Int] {
      override def compare(a: Int, b: Int) = b.compareTo(a)
     }
     // Sort by key, using
 res7.repartitionAndSortWithinPartitions(new KeyBasePartitioner(3)).saveAsTextFile("file:///opt/output/")


结果,可以看到每个分区都是有效的。

mdhdeMacBook-Pro-3:output mdh$ pwd
/opt/output
mdhdeMacBook-Pro-3:output mdh$ ls
_SUCCESS        part-00000      part-00001      part-00002
mdhdeMacBook-Pro-3:output mdh$ head -n 10 part-00000
(24,the)
(12,for)
(9,##)
(9,and)
(6,is)
(6,in)
(3,general)
(3,documentation)
(3,example)
(3,how)
mdhdeMacBook-Pro-3:output mdh$ head -n 10 part-00001
(16,Spark)
(7,can)
(7,run)
(7,on)
(4,build)
(4,Please)
(4,with)
(4,also)
(4,if)
(4,including)
mdhdeMacBook-Pro-3:output mdh$ head -n 10 part-00002
(47,)
(17,to)
(8,a)
(5,using)
(5,of)
(2,Python)
(2,locally)
(2,This)
(2,Hive)
(2,SparkPi)
mdhdeMacBook-Pro-3:output mdh$


上面只是一个简单的使用，关于二次排序及高效结合mapPartitions的例子，浪尖会在这两天更新到星球里。


spark源码系列之内部通讯的三种机制
原创： 浪尖  Spark学习技巧  2017-07-09
本文是以spark1.6.0的源码为例讲解。

Spark为协调各个组件完成任务及内部任务处理采用了多种方式进行了各个组件之间的通讯。总共三个部分牵涉的功能是：

1，DAG相关的DAGSchedulerEventProcessLoop。

2，sparkUI相关的SparkListener

3，RPC相关netty RPC流程。本文只讲流程，后面会详细介绍。

一，单个部件自己消息处理方式

DAGSchedulerEventProcessLoop该类继承自EventLoop。是一个典型的生产消费模型。

A),生产者

通过调用

DAGSchedulerEventProcessLoop.post(event: E)

来将消息进行发布。

B),消费者

Eventloop内部维护了一个线程，循环的消费消息eventQueue.take()，调用onReceive(event)进行处理。DAGSchedulerEventProcessLoop内部实现了doOnReceive，对事件进行模式匹配然后交给具体的消息处理函数。

private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
  case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)

  case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =>
    dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)

  case StageCancelled(stageId) =>
    dagScheduler.handleStageCancellation(stageId)

  case JobCancelled(jobId) =>
    dagScheduler.handleJobCancellation(jobId)

  case JobGroupCancelled(groupId) =>
    dagScheduler.handleJobGroupCancelled(groupId)

  case AllJobsCancelled =>
    dagScheduler.doCancelAllJobs()

  case ExecutorAdded(execId, host) =>
    dagScheduler.handleExecutorAdded(execId, host)

  case ExecutorLost(execId) =>
    dagScheduler.handleExecutorLost(execId, fetchFailed = false)

  case BeginEvent(task, taskInfo) =>
    dagScheduler.handleBeginEvent(task, taskInfo)

  case GettingResultEvent(taskInfo) =>
    dagScheduler.handleGetTaskResult(taskInfo)

  case completion @ CompletionEvent(task, reason, _, _, taskInfo, taskMetrics) =>
    dagScheduler.handleTaskCompletion(completion)

  case TaskSetFailed(taskSet, reason, exception) =>
    dagScheduler.handleTaskSetFailed(taskSet, reason, exception)

  case ResubmitFailedStages =>
    dagScheduler.resubmitFailedStages()
}

C),消息缓存

消息最终是存储于EventLoop的new LinkedBlockingDeque[E]()里。

二，SparkListeners和ListenerBus

SparkUI的各个监控指标都是，由ListenerBus最为生产者将消息，推送到消息缓存出默认支持1万，然后推送给各个Listener进行处理，然后我们的Spark的webUIPage去获取各个Listener的数据，进行展示。

A)，生产者

LiveListenerBus/StreamingListenerBus调用其父类AsynchronousListenerBus的post方法将消息加入 new LinkedBlockingQueue[E](EVENT_QUEUE_CAPACITY)，容量1万。

val eventAdded = eventQueue.offer(event)

B),消费者

AsynchronousListenerBus内部维护了一个消费者线程，线程内部有while(true)进行消息处理。

val event = eventQueue.poll

postToAll(event)

C),消息的具体处理

ListenerBus的postToAll方法，会遍历所有注册了的Listener。

final def postToAll(event: E): Unit = {

  val iter = listeners.iterator
  while (iter.hasNext) {
    val listener = iter.next()
    try {
 onPostEvent(listener, event)
    } catch {
      case NonFatal(e) =>
        logError(s"Listener ${Utils.getFormattedClassName(listener)} threw an exception", e)
    }
  }
}

最终在onPostEvent方法中将消息进行了处理。onPostEvent在源码中的两个重要现:

SparkListenerBus和StreamingListenerBus内部的onPostEvent。

private[spark] trait SparkListenerBus extends ListenerBus[SparkListener, SparkListenerEvent] {

  override def onPostEvent(listener: SparkListener, event: SparkListenerEvent): Unit = {
    event match {
      case stageSubmitted: SparkListenerStageSubmitted =>
        listener.onStageSubmitted(stageSubmitted)
      case stageCompleted: SparkListenerStageCompleted =>
        listener.onStageCompleted(stageCompleted)
      case jobStart: SparkListenerJobStart =>
        listener.onJobStart(jobStart)
      case jobEnd: SparkListenerJobEnd =>
        listener.onJobEnd(jobEnd)
      case taskStart: SparkListenerTaskStart =>
        listener.onTaskStart(taskStart)
      case taskGettingResult: SparkListenerTaskGettingResult =>
        listener.onTaskGettingResult(taskGettingResult)
      case taskEnd: SparkListenerTaskEnd =>
        listener.onTaskEnd(taskEnd)
      case environmentUpdate: SparkListenerEnvironmentUpdate =>
        listener.onEnvironmentUpdate(environmentUpdate)
      case blockManagerAdded: SparkListenerBlockManagerAdded =>
        listener.onBlockManagerAdded(blockManagerAdded)
      case blockManagerRemoved: SparkListenerBlockManagerRemoved =>
        listener.onBlockManagerRemoved(blockManagerRemoved)
      case unpersistRDD: SparkListenerUnpersistRDD =>
        listener.onUnpersistRDD(unpersistRDD)
      case applicationStart: SparkListenerApplicationStart =>
        listener.onApplicationStart(applicationStart)
      case applicationEnd: SparkListenerApplicationEnd =>
        listener.onApplicationEnd(applicationEnd)
      case metricsUpdate: SparkListenerExecutorMetricsUpdate =>
        listener.onExecutorMetricsUpdate(metricsUpdate)
      case executorAdded: SparkListenerExecutorAdded =>
        listener.onExecutorAdded(executorAdded)
      case executorRemoved: SparkListenerExecutorRemoved =>
        listener.onExecutorRemoved(executorRemoved)
      case blockUpdated: SparkListenerBlockUpdated =>
        listener.onBlockUpdated(blockUpdated)
      case logStart: SparkListenerLogStart => // ignore event log metadata
    }
  }

}



private[spark] class StreamingListenerBus
  extends AsynchronousListenerBus[StreamingListener, StreamingListenerEvent]("StreamingListenerBus")
  with Logging {

  private val logDroppedEvent = new AtomicBoolean(false)

  override def onPostEvent(listener: StreamingListener, event: StreamingListenerEvent): Unit = {
    event match {
      case receiverStarted: StreamingListenerReceiverStarted =>
        listener.onReceiverStarted(receiverStarted)
      case receiverError: StreamingListenerReceiverError =>
        listener.onReceiverError(receiverError)
      case receiverStopped: StreamingListenerReceiverStopped =>
        listener.onReceiverStopped(receiverStopped)
      case batchSubmitted: StreamingListenerBatchSubmitted =>
        listener.onBatchSubmitted(batchSubmitted)
      case batchStarted: StreamingListenerBatchStarted =>
        listener.onBatchStarted(batchStarted)
      case batchCompleted: StreamingListenerBatchCompleted =>
        listener.onBatchCompleted(batchCompleted)
      case outputOperationStarted: StreamingListenerOutputOperationStarted =>
        listener.onOutputOperationStarted(outputOperationStarted)
      case outputOperationCompleted: StreamingListenerOutputOperationCompleted =>
        listener.onOutputOperationCompleted(outputOperationCompleted)
      case _ =>
    }
  }

  override def onDropEvent(event: StreamingListenerEvent): Unit = {
    if (logDroppedEvent.compareAndSet(false, true)) {
      // Only log the following message once to avoid duplicated annoying logs.
      logError("Dropping StreamingListenerEvent because no remaining room in event queue. " +
        "This likely means one of the StreamingListeners is too slow and cannot keep up with the " +
        "rate at which events are being started by the scheduler.")
    }
  }
}



D),消息的缓存

消息是缓存在AsynchronousListenerBus

val eventQueue = new LinkedBlockingQueue[E](EVENT_QUEUE_CAPACITY)

EVENT_QUEUE_CAPACITY=10000

三，Spark多进程之间的通讯RPC

Spark的内部rpc老版本是用akka实现的，spark1.6以后虽然保留akka，但是默认实现已经是netty。

其实，rpc采用netty之前，rpc是通过akka，而文件传输是通过netty。现在相当于全部采用了netty的实现的。



四，总结

本篇文章主要是将内部spark的内部事件通知的机制。希望通过这篇文章，大家对spark内部事件通知流程有所了解。

这三种模型是我们现在编程最常见的三种模型，希望能对大家编写自己的代码提供一些有益的思路。

=======================

生产常用Spark累加器剖析之一
原创： 若泽数据-阿春  若泽大数据  4月19日


由于最近在项目中需要用到Spark的累加器，同时需要自己去自定义实现Spark的累加器，从而满足生产上的需求。对此，对Spark的累加器实现机制进行了追踪学习。
本系列文章，将从以下几个方面入手，对Spark累加器进行剖析：

1、Spark累加器的基本概念
2、累加器的重点类构成
3、累加器的源码解析
4、累加器的执行过程
5、累加器使用中的坑
6、自定义累加器的实现

Spark累加器基本概念
Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，只能累加，不能减少累加器只能在Driver端构建，并只能从Driver端读取结果，在Task端只能进行累加
至于这里为什么只能在Task累加呢？下面的内容将会进行详细的介绍，先简单介绍下：
在Task节点，准确的就是说在executor上；每个Task都会有一个累加器的变量，被序列化传输到executor端运行之后再返回过来都是独立运行的；如果在Task端去获取值的话，只能获取到当前Task的，Task与Task之间不会有影响

累加器不会改变Spark lazy计算的特点，只会在Job触发的时候进行相关的累加操作

现有累加器类型:



累加器的重点类介绍
class Accumulator extends Accumulable
源码（源码中已经对这个类的作用做了十分详细的解释）：

/**
 * A simpler value of [[Accumulable]] where the result type being accumulated is the same
 * as the types of elements being merged, i.e. variables that are only "added" to through an
 * associative operation and can therefore be efficiently supported in parallel. They can be used
 * to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric
 * value types, and programmers can add support for new types.
 *
 * An accumulator is created from an initial value `v` by calling [[SparkContext#accumulator]].
 * Tasks running on the cluster can then add to it using the [[Accumulable#+=]] operator.
 * However, they cannot read its value. Only the driver program can read the accumulator's value,
 * using its value method.
 *
 * @param initialValue initial value of accumulator
 * @param param helper object defining how to add elements of type `T`
 * @tparam T result type
 */
class Accumulator[T] private[spark] (
    @transient private[spark] val initialValue: T,
    param: AccumulatorParam[T],
    name: Option[String],
    internal: Boolean)
  extends Accumulable[T, T](initialValue, param, name, internal) {
  def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = {
    this(initialValue, param, name, false)
  }
  def this(initialValue: T, param: AccumulatorParam[T]) = {
    this(initialValue, param, None, false)
  }
}
主要实现了累加器的初始化及封装了相关的累加器操作方法
同时在类对象构建的时候向Accumulators注册累加器
累加器的add操作的返回值类型和传入进去的值类型可以不一样
所以一定要定义好两步操作（即add方法）：累加操作/合并操作

object Accumulators
该方法在Driver端管理着累加器，也包含了累加器的聚合操作

trait AccumulatorParam[T] extends AccumulableParam[T, T]
源码：

/**
 * A simpler version of [[org.apache.spark.AccumulableParam]] where the only data type you can add
 * in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be
 * available when you create Accumulators of a specific type.
 *
 * @tparam T type of value to accumulate
 */
trait AccumulatorParam[T] extends AccumulableParam[T, T] {
  def addAccumulator(t1: T, t2: T): T = {
    addInPlace(t1, t2)
  }
}
AccumulatorParam的addAccumulator操作的泛型封装
具体的实现还是需要在具体实现类里面实现addInPlace方法
自定义实现累加器的关键

object AccumulatorParam
源码：

object AccumulatorParam {
  // The following implicit objects were in SparkContext before 1.2 and users had to
  // `import SparkContext._` to enable them. Now we move them here to make the compiler find
  // them automatically. However, as there are duplicate codes in SparkContext for backward
  // compatibility, please update them accordingly if you modify the following implicit objects.
  implicit object DoubleAccumulatorParam extends AccumulatorParam[Double] {
    def addInPlace(t1: Double, t2: Double): Double = t1 + t2
    def zero(initialValue: Double): Double = 0.0
  }
  implicit object IntAccumulatorParam extends AccumulatorParam[Int] {
    def addInPlace(t1: Int, t2: Int): Int = t1 + t2
    def zero(initialValue: Int): Int = 0
  }
  implicit object LongAccumulatorParam extends AccumulatorParam[Long] {
    def addInPlace(t1: Long, t2: Long): Long = t1 + t2
    def zero(initialValue: Long): Long = 0L
  }
  implicit object FloatAccumulatorParam extends AccumulatorParam[Float] {
    def addInPlace(t1: Float, t2: Float): Float = t1 + t2
    def zero(initialValue: Float): Float = 0f
  }
  // TODO: Add AccumulatorParams for other types, e.g. lists and strings
}
从源码中大量的implicit关键词，可以发现该类主要进行隐式类型转换的操作

TaskContextImpl
在Executor端管理着我们的累加器，累加器是通过该类进行返回的

累加器的源码解析
Driver端
accumulator方法
以下列这段代码中的accumulator方法为入口点，进入到相应的源码中去：

val acc = new Accumulator(initialValue, param, Some(name))
源码：

class Accumulator[T] private[spark] (
    @transient private[spark] val initialValue: T,
    param: AccumulatorParam[T],
    name: Option[String],
    internal: Boolean)
  extends Accumulable[T, T](initialValue, param, name, internal) {
  def this(initialValue: T, param: AccumulatorParam[T], name: Option[String]) = {
    this(initialValue, param, name, false)
  }
  def this(initialValue: T, param: AccumulatorParam[T]) = {
    this(initialValue, param, None, false)
  }
}
继承的Accumulable[T, T]
源码：

class Accumulable[R, T] private[spark] (
    initialValue: R,
    param: AccumulableParam[R, T],
    val name: Option[String],
    internal: Boolean)
  extends Serializable {
…
// 这里的_value并不支持序列化
// 注：有@transient的都不会被序列化
@volatile @transient private var value_ : R = initialValue // Current value on master
  …
  // 注册了当前的累加器
  Accumulators.register(this)
  …,
  }
Accumulators.register()
源码：

// 传入参数，注册累加器
def register(a: Accumulable[_, _]): Unit = synchronized {
// 构造成WeakReference
originals(a.id) = new WeakReference[Accumulable[_, _]](a)
}
至此，Driver端的初始化已经完成
Executor端
Executor端的反序列化是一个得到我们的对象的过程
初始化是在反序列化的时候就完成的，同时反序列化的时候还完成了Accumulator向TaskContextImpl的注册

TaskRunner中的run方法
// 在计算的过程中，会将RDD和function经过序列化之后传给Executor端
private[spark] class Executor(
    executorId: String,
    executorHostname: String,
    env: SparkEnv,
    userClassPath: Seq[URL] = Nil,
    isLocal: Boolean = false)
  extends Logging {
...
  class TaskRunner(
      execBackend: ExecutorBackend,
      val taskId: Long,
      val attemptNumber: Int,
      taskName: String,
      serializedTask: ByteBuffer)
    extends Runnable {
…
override def run(): Unit = {
    …
val (value, accumUpdates) = try {
         // 调用TaskRunner中的task.run方法，触发task的运行
         val res = task.run(
           taskAttemptId = taskId,
           attemptNumber = attemptNumber,
           metricsSystem = env.metricsSystem)
         threwException = false
         res
       } finally {
        …
       }
…
}
Task中的collectAccumulators()方法
private[spark] abstract class Task[T](
final def run(
    taskAttemptId: Long,
    attemptNumber: Int,
    metricsSystem: MetricsSystem)
  : (T, AccumulatorUpdates) = {
  …
    try {
      // 返回累加器，并运行task
      // 调用TaskContextImpl的collectAccumulators，返回值的类型为一个Map
      (runTask(context), context.collectAccumulators())
    } finally {
  …
 }
 …
 }
)
ResultTask中的runTask方法
  override def runTask(context: TaskContext): U = {
    // Deserialize the RDD and the func using the broadcast variables.
    val deserializeStartTime = System.currentTimeMillis()
    val ser = SparkEnv.get.closureSerializer.newInstance()
    // 反序列化是在调用ResultTask的runTask方法的时候做的
    // 会反序列化出来RDD和自己定义的function
    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](
      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime
    metrics = Some(context.taskMetrics)
    func(context, rdd.iterator(partition, context))
  }
Accumulable中的readObject方法
  // 在反序列化的过程中会调用Accumulable.readObject方法
  // Called by Java when deserializing an object
  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {
    in.defaultReadObject()
    // value的初始值为zero；该值是会被序列化的
    value_ = zero
    deserialized = true
    // Automatically register the accumulator when it is deserialized with the task closure.
    //
    // Note internal accumulators sent with task are deserialized before the TaskContext is created
    // and are registered in the TaskContext constructor. Other internal accumulators, such SQL
    // metrics, still need to register here.
    val taskContext = TaskContext.get()
    if (taskContext != null) {
      // 当前反序列化所得到的对象会被注册到TaskContext中
      // 这样TaskContext就可以获取到累加器
      // 任务运行结束之后，就可以通过context.collectAccumulators()返回给executor
      taskContext.registerAccumulator(this)
    }
  }
Executor.scala
// 在executor端拿到accumuUpdates值之后，会去构造一个DirectTaskResult
val directResult = new DirectTaskResult(valueBytes, accumUpdates, task.metrics.orNull)
val serializedDirectResult = ser.serialize(directResult)
val resultSize = serializedDirectResult.limit
…
// 最终由ExecutorBackend的statusUpdate方法发送至Driver端
// ExecutorBackend为一个Trait，有多种实现
execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)
CoarseGrainedExecutorBackend中的statusUpdate方法
// 通过ExecutorBackend的一个实现类：CoarseGrainedExecutorBackend 中的statusUpdate方法
// 将数据发送至Driver端
override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) {
    val msg = StatusUpdate(executorId, taskId, state, data)
    driver match {
      case Some(driverRef) => driverRef.send(msg)
      case None => logWarning(s"Drop $msg because has not yet connected to driver")
    }
  }
CoarseGrainedSchedulerBackend中的receive方法
// Driver端在接收到消息之后，会调用CoarseGrainedSchedulerBackend中的receive方法
override def receive: PartialFunction[Any, Unit] = {
      case StatusUpdate(executorId, taskId, state, data) =>
        // 会在DAGScheduler的handleTaskCompletion方法中将结果返回
        scheduler.statusUpdate(taskId, state, data.value)
    …
}
TaskSchedulerImpl的statusUpdate方法
  def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer) {
  …
            if (state == TaskState.FINISHED) {
              taskSet.removeRunningTask(tid)
              // 将成功的Task入队
              taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)
            } else if (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) {
              taskSet.removeRunningTask(tid)
              taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)
            }
  …
}
TaskResultGetter的enqueueSuccessfulTask方法
  def enqueueSuccessfulTask(taskSetManager: TaskSetManager, tid: Long, serializedData: ByteBuffer) {
…
          result.metrics.setResultSize(size)
          scheduler.handleSuccessfulTask(taskSetManager, tid, result)
…
TaskSchedulerImpl的handleSuccessfulTask方法
  def handleSuccessfulTask(
      taskSetManager: TaskSetManager,
      tid: Long,
      taskResult: DirectTaskResult[_]): Unit = synchronized {
    taskSetManager.handleSuccessfulTask(tid, taskResult)
  }
DAGScheduler的taskEnded方法
  def taskEnded(
      task: Task[_],
      reason: TaskEndReason,
      result: Any,
      accumUpdates: Map[Long, Any],
      taskInfo: TaskInfo,
      taskMetrics: TaskMetrics): Unit = {
  eventProcessLoop.post(
      // 给自身的消息循环体发了个CompletionEvent
      // 这个CompletionEvent会被handleTaskCompletion方法所接收到
      CompletionEvent(task, reason, result, accumUpdates, taskInfo, taskMetrics))
  }
DAGScheduler的handleTaskCompletion方法
// 与上述CoarseGrainedSchedulerBackend中的receive方法章节对应
// 在handleTaskCompletion方法中，接收CompletionEvent
// 不论是ResultTask还是ShuffleMapTask都会去调用updateAccumulators方法，更新累加器的值
private[scheduler] def handleTaskCompletion(event: CompletionEvent) {
    …
    event.reason match {
      case Success =>
        listenerBus.post(SparkListenerTaskEnd(stageId, stage.latestInfo.attemptId, taskType,
          event.reason, event.taskInfo, event.taskMetrics))
        stage.pendingPartitions -= task.partitionId
        task match {
          case rt: ResultTask[_, _] =>
            // Cast to ResultStage here because it's part of the ResultTask
            // TODO Refactor this out to a function that accepts a ResultStage
            val resultStage = stage.asInstanceOf[ResultStage]
            resultStage.activeJob match {
              case Some(job) =>
                if (!job.finished(rt.outputId)) {
                  updateAccumulators(event)
          case smt: ShuffleMapTask =>
            val shuffleStage = stage.asInstanceOf[ShuffleMapStage]
            updateAccumulators(event)
}
…
}
DAGScheduler的updateAccumulators方法
  private def updateAccumulators(event: CompletionEvent): Unit = {
    val task = event.task
    val stage = stageIdToStage(task.stageId)
    if (event.accumUpdates != null) {
      try {
        // 调用了累加器的add方法
        Accumulators.add(event.accumUpdates)
Accumulators的add方法
  def add(values: Map[Long, Any]): Unit = synchronized {
    // 遍历传进来的值
    for ((id, value) <- values) {
      if (originals.contains(id)) {
        // Since we are now storing weak references, we must check whether the underlying data
        // is valid.
        // 根据id从注册的Map中取出对应的累加器
        originals(id).get match {
          // 将值给累加起来，最终将结果加到value里面
          // ++=是被重载了
          case Some(accum) => accum.asInstanceOf[Accumulable[Any, Any]] ++= value
          case None =>
            throw new IllegalAccessError("Attempted to access garbage collected Accumulator.")
        }
      } else {
        logWarning(s"Ignoring accumulator update for unknown accumulator id $id")
      }
    }
  }
Accumulators的++=方法
def ++= (term: R) { value_ = param.addInPlace(value_, term)}
Accumulators的value方法
  def value: R = {
    if (!deserialized) {
      value_
    } else {
      throw new UnsupportedOperationException("Can't read accumulator value in task")
    }
  }
此时我们的应用程序就可以通过 .value 的方式去获取计数器的值了
作者：若泽数据——呼呼呼
原文：https://blog.csdn.net/lemonZhaoTao/article/details/81256413



回归原创文章:

若泽数据-高级班&线下班报名

捷报:线下班学员年薪35W的offer及面试题

高级班学员高薪offer32w，你比他高吗？

捷报: 高级班学员月薪22K及上周3家offer的面试题

刚出炉的3家大数据面试题(含高级)，你会吗？

捷报:刚出炉年薪30w的offer和面试题

捷报:高级班学员年薪37.4W的offer及3家面试题

加我，进大数据V群

生产常用Spark累加器剖析之二
原创： 若泽数据-阿春  若泽大数据  4月26日


生产常用Spark累加器剖析之一



综述
Driver端
1、Driver端初始化构建Accumulator并初始化，同时完成了Accumulator注册，Accumulators.register(this)时Accumulator会在序列化后发送到Executor端


2、Driver接收到ResultTask完成的状态更新后，会去更新Value的值 然后在Action操作执行后就可以获取到Accumulator的值了

Executor端
1、Executor端接收到Task之后会进行反序列化操作，反序列化得到RDD和function。同时在反序列化的同时也去反序列化Accumulator(在readObject方法中完成)，同时也会向TaskContext完成注册


2、完成任务计算之后，随着Task结果一起返回给Driver

结合源码分析
Driver端初始化
Driver端主要经过以下步骤，完成初始化操作：

val accum = sparkContext.accumulator(0, “AccumulatorTest”)
val acc = new Accumulator(initialValue, param, Some(name))
Accumulators.register(this)
Executor端反序列化得到Accumulator
反序列化是在调用ResultTask的runTask方式时候做的操作：

// 会反序列化出来RDD和自己定义的function
val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](
   ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
在反序列化的过程中，会调用Accumulable中的readObject方法：

  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {
    in.defaultReadObject()
    // value的初始值为zero；该值是会被序列化的
    value_ = zero
    deserialized = true
    // Automatically register the accumulator when it is deserialized with the task closure.
    //
    // Note internal accumulators sent with task are deserialized before the TaskContext is created
    // and are registered in the TaskContext constructor. Other internal accumulators, such SQL
    // metrics, still need to register here.
    val taskContext = TaskContext.get()
    if (taskContext != null) {
      // 当前反序列化所得到的对象会被注册到TaskContext中
      // 这样TaskContext就可以获取到累加器
      // 任务运行结束之后，就可以通过context.collectAccumulators()返回给executor
      taskContext.registerAccumulator(this)
    }
  }
注意
Accumulable.scala中的value_，是不会被序列化的，@transient关键词修饰了

@volatile @transient private var value_ : R = initialValue // Current value on master
累加器在各个节点的累加操作
针对传入function中不同的操作，对应有不同的调用方法，以下列举几种（在Accumulator.scala中）：

def += (term: T) { value_ = param.addAccumulator(value_, term) }
def add(term: T) { value_ = param.addAccumulator(value_, term) }
def ++= (term: R) { value_ = param.addInPlace(value_, term)}
根据不同的累加器参数，有不同实现的AccumulableParam（在Accumulator.scala中）：

trait AccumulableParam[R, T] extends Serializable {
  /**
  def addAccumulator(r: R, t: T): R
  def addInPlace(r1: R, r2: R): R
  def zero(initialValue: R): R
}
不同的实现如下图所示：


以IntAccumulatorParam为例：

  implicit object IntAccumulatorParam extends AccumulatorParam[Int] {
    def addInPlace(t1: Int, t2: Int): Int = t1 + t2
    def zero(initialValue: Int): Int = 0
  }
我们发现IntAccumulatorParam实现的是trait AccumulatorParam[T]：

trait AccumulatorParam[T] extends AccumulableParam[T, T] {
  def addAccumulator(t1: T, t2: T): T = {
    addInPlace(t1, t2)
  }
}
在各个节点上的累加操作完成之后，就会紧跟着返回更新之后的Accumulators的value_值

聚合操作
在Task.scala中的run方法，会执行如下：

// 返回累加器，并运行task
// 调用TaskContextImpl的collectAccumulators，返回值的类型为一个Map
(runTask(context), context.collectAccumulators())
在Executor端已经完成了一系列操作，需要将它们的值返回到Driver端进行聚合汇总，整个顺序如图累加器执行流程：


根据执行流程，我们可以发现，在执行完collectAccumulators方法之后，最终会在DAGScheduler中调用updateAccumulators(event)，而在该方法中会调用Accumulators的add方法，从而完成聚合操作：

  def add(values: Map[Long, Any]): Unit = synchronized {
    // 遍历传进来的值
    for ((id, value) <- values) {
      if (originals.contains(id)) {
        // Since we are now storing weak references, we must check whether the underlying data
        // is valid.
        // 根据id从注册的Map中取出对应的累加器
        originals(id).get match {
          // 将值给累加起来，最终将结果加到value里面
         // ++=是被重载了
          case Some(accum) => accum.asInstanceOf[Accumulable[Any, Any]] ++= value
          case None =>
            throw new IllegalAccessError("Attempted to access garbage collected Accumulator.")
        }
      } else {
        logWarning(s"Ignoring accumulator update for unknown accumulator id $id")
      }
    }
  }
获取累加器的值
通过accum.value方法可以获取到累加器的值

至此，累加器执行完毕。

作者：若泽数据——呼呼呼
原文：https://blog.csdn.net/lemonZhaoTao/article/details/81273243

回归原创文章:

若泽数据-高级班&线下班报名

捷报:线下班学员年薪35W的offer及面试题

高级班学员高薪offer32w，你比他高吗？

捷报: 高级班学员月薪22K及上周3家offer的面试题

刚出炉的3家大数据面试题(含高级)，你会吗？

捷报:刚出炉年薪30w的offer和面试题

捷报:高级班学员年薪37.4W的offer及3家面试题

加我，进大数据V群


生产常用Spark累加器剖析之三(自定义累加器)
原创： 若泽数据-阿春  若泽大数据  5天前


前面文章

生产常用Spark累加器剖析之一

生产常用Spark累加器剖析之二



思路 & 需求
参考IntAccumulatorParam的实现思路（上述文章中有讲）：

trait AccumulatorParam[T] extends AccumulableParam[T, T] {
  def addAccumulator(t1: T, t2: T): T = {
    // addInPlace有很多具体的实现类
    // 如果想要实现自定义的话，就得实现这个方法
    addInPlace(t1, t2)
  }
}
自定义也可以通过这个方法去实现，从而兼容我们自定义的累加器

需求：这里实现一个简单的案例，用分布式的方法去实现随机数
/**
  * 自定义的AccumulatorParam
  *
  * Created by lemon on 2018/7/28.
  */
object UniqueKeyAccumulator extends AccumulatorParam[Map[Int, Int]] {
  override def addInPlace(r1: Map[Int, Int], r2: Map[Int, Int]): Map[Int, Int] = {
      // ++用于两个集合相加
      r1++r2
    }
    override def zero(initialValue: Map[Int, Int]): Map[Int, Int] = {
      var data: Map[Int, Int] = Map()
      data
    }
}
/**
  * 使用自定义的累加器，实现随机数
  *
  * Created by lemon on 2018/7/28.
  */
object CustomAccumulator {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf().setAppName("CustomAccumulator").setMaster("local[2]")
    val sc = new SparkContext(sparkConf)
    val uniqueKeyAccumulator = sc.accumulable(Map[Int, Int]())(UniqueKeyAccumulator)
    val distData = sc.parallelize(1 to 10)
    val mapCount = distData.map(x => {
      val randomNum = new Random().nextInt(20)
      // 构造一个k-v对
      val map: Map[Int, Int] = Map[Int, Int](randomNum -> randomNum)
      uniqueKeyAccumulator += map
    })
    println(mapCount.count())
    // 获取到累加器的值 中的key值，并进行打印
    uniqueKeyAccumulator.value.keys.foreach(println)
    sc.stop()
  }
}
运行结果如下图：



作者：若泽数据——呼呼呼
原文：https://blog.csdn.net/lemonZhaoTao/article/details/81407782

原创文章，更多关注公众号获取:

Spark内存管理之一 静态内存管理

Spark内存管理之二 统一内存管理及设计理念

Spark内存管理之三 UnifiedMemoryManager分析

人人都应该会的-生产Spark2.4.0如何Debug源代码

生产常用Spark累加器剖析之一

最佳实践之Spark写入Hfile经典案例

实时数仓之Maxwell读取MySQL binlog日志到Kafka


