1．2　一个大一统的软件栈　　2
1．2．1　Spark Core　　2
1．2．2　Spark SQL　　3
1．2．3　Spark Streaming　　3
1．2．4　MLlib　　3
1．2．5　GraphX　　3
1．2．6　集群管理器　　4
1．3．1　数据科学任务　　4
1．3．2　数据处理应用　　5
1．4　Spark简史　　5
1．5　Spark的版本和发布　　6
1．6　Spark的存储层次　　6
2．1　下载Spark　　7
2．2　Spark中Python和Scala的shell　　9
2．3　Spark 核心概念简介　　12
2．4　独立应用　　14
2．4．1　初始化SparkContext　　15
2．4．2　构建独立应用　　16
2．5　总结　　19
3．1　RDD基础　　21
3．2　创建RDD　　23
3．3　RDD操作　　24
3．3．1　转化操作　　24
3．3．2　行动操作　　26
3．3．3　惰性求值　　27
3．4　向Spark传递函数　　27
3．4．1　Python　　27
3．4．2　Scala　　28
3．4．3　Java　　29
3．5　常见的转化操作和行动操作　　30
3．5．1　基本RDD　　30
3．5．2　在不同RDD类型间转换　　37
3．6　持久化( 缓存)　　39
3．7　总结　　40
第4章　键值对操作　　41
4．1　动机　　41
4．2　创建Pair RDD　　42
4．3　Pair RDD的转化操作　　42
4．3．1　聚合操作　　45
4．3．2　数据分组　　49
4．3．3　连接　　50
4．3．4　数据排序　　51
4．4　Pair RDD的行动操作　　52
4．5　数据分区（进阶）　　52
4．5．1　获取RDD的分区方式　　55
4．5．2　从分区中获益的操作　　56
4．5．3　影响分区方式的操作　　57
4．5．4　示例：PageRank　　57
4．5．5　自定义分区方式　　59
4．6　总结　　61
第5章　数据读取与保存　　63
5．1　动机　　63
5．2　文件格式　　64
5．2．1　文本文件　　64
5．2．2　JSON　　66
5．2．3　逗号分隔值与制表符分隔值　　68
5．2．4　SequenceFile　　71
5．2．5　对象文件　　73
5．2．6　Hadoop输入输出格式　　73
5．2．7　文件压缩　　77
5．3　文件系统　　78
5．3．1　本地/“常规”文件系统　　78
5．3．2　Amazon S3　　78
5．3．3　HDFS　　79
5．4　Spark SQL中的结构化数据　　79
5．4．1　Apache Hive　　80
5．4．2　JSON　　80
5．5　数据库　　81
5．5．1　Java数据库连接　　81
5．5．2　Cassandra　　82
5．5．3　HBase　　84
5．5．4　Elasticsearch　　85
5．6　总结　　86
第6章　Spark编程进阶　　87
6．1　简介　　87
6．2　累加器　　88
6．2．1　累加器与容错性　　90
6．2．2　自定义累加器　　91
6．3　广播变量　　91
6．4　基于分区进行操作　　94
6．5　与外部程序间的管道　　96
6．6　数值RDD 的操作　　99
6．7　总结　　100
第7章　在集群上运行Spark　　101
7．1　简介　　101
7．2　Spark运行时架构　　101
7．2．1　驱动器节点　　102
7．2．2　执行器节点　　103
7．2．3　集群管理器　　103
7．2．4　启动一个程序　　104
7．2．5　小结　　104
7．3　使用spark-submit 部署应用　　105
7．4　打包代码与依赖　　107
7．4．1　使用Maven构建的用Java编写的Spark应用　　108
7．4．2　使用sbt构建的用Scala编写的Spark应用　　109
7．4．3　依赖冲突　　 111
7．5　Spark应用内与应用间调度　　111
7．6　集群管理器　　112
7．6．1　独立集群管理器　　112
7．6．2　Hadoop YARN　　115
7．6．3　Apache Mesos　　116
7．6．4　Amazon EC2　　117
7．7　选择合适的集群管理器　　120
7．8　总结　　121
第8章　Spark调优与调试　　123
8．1　使用SparkConf配置Spark　　123
8．2　Spark执行的组成部分：作业、任务和步骤　　127
8．3　查找信息　　131
8．3．1　Spark网页用户界面　　131
8．3．2　驱动器进程和执行器进程的日志　　134
8．4　关键性能考量　　135
8．4．1　并行度　　135
8．4．2　序列化格式　　136
8．4．3　内存管理　　137
8．4．4　硬件供给　　138
8．5　总结　　139
第9章　Spark SQL　　141
9．1　连接Spark SQL　　142
9．2　在应用中使用Spark SQL　　144
9．2．1　初始化Spark SQL　　144
9．2．2　基本查询示例　　145
9．2．3　SchemaRDD　　146
9．2．4　缓存　　148
9．3　读取和存储数据　　149
9．3．1　Apache Hive　　149
9．3．2　Parquet　　150
9．3．3　JSON　　150
9．3．4　基于RDD　　152
9．4　JDBC/ODBC服务器　　153
9．4．1　使用Beeline　　155
9．4．2　长生命周期的表与查询　　156
9．5　用户自定义函数　　156
9．5．1　Spark SQL UDF　　156
9．5．2　Hive UDF　　157
9．6　Spark SQL性能　　158
9．7　总结　　159
第10章　Spark Streaming　　161
10．1　一个简单的例子　　162
10．2　架构与抽象　　164
10．3　转化操作　　167
10．3．1　无状态转化操作　　167
10．3．2　有状态转化操作　　169
10．4　输出操作　　173
10．5　输入源　　175
10．5．1　核心数据源　　175
10．5．2　附加数据源　　176
10．5．3　多数据源与集群规模　　179
10．6　24/7不间断运行　　180
10．6．1　检查点机制　　180
10．6．2　驱动器程序容错　　181
10．6．3　工作节点容错　　182
10．6．4　接收器容错　　182
10．6．5　处理保证　　183
10．7　Streaming用户界面　　183
10．8　性能考量　　184
10．8．1　批次和窗口大小　　184
10．8．2　并行度　　184
10．8．3　垃圾回收和内存使用　　185
10．9　总结　　185
第11章　基于MLlib的机器学习　　187
11．1　概述　　187
11．2　系统要求　　188
11．3　机器学习基础　　189
11．4　数据类型　　192
11．5　算法　　194
11．5．1　特征提取　　194
11．5．2　统计　　196
11．5．3　分类与回归　　197
11．5．4　聚类　　202
11．5．5　协同过滤与推荐　　203
11．5．6　降维　　204
11．5．7　模型评估　　206
11．6　一些提示与性能考量　　206
11．6．1　准备特征　　206
11．6．2　配置算法　　207
11．6．3　缓存RDD以重复使用　　207
11．6．4　识别稀疏程度　　207
11．6．5　并行度　　207
11．7　流水线API　　208
11．8　总结　　209

6.1 Spark概述 107
6.1.1 批量数据处理 107
6.1.2 实时数据处理 108
6.1.3 一站式解决方案Apache Spark 110
6.1.4 何时应用Spark—实际用例 112
6.2.1 高级架构 114
6.2.2 Spark扩展/库 116
6.2.3 Spark的封装结构和API 117
6.2.4 Spark的执行模型—主管-工作者视图 119
6.3 弹性分布式数据集（RDD） 122
6.4 编写执行第一个Spark程序 124
6.4.1 硬件需求 125
6.4.2 基本软件安装 125
6.4.3 配置Spark集群 127
6.4.4 用Scala编写Spark作业 129
6.4.5 用Java编写Spark作业 132
6.5 故障排除提示和技巧 133
6.5.1 Spark所用的端口数目 134
6.5.2 类路径问题—类未找到异常 134
6.5.3 其他常见异常 134
6.6 本章小结 135
7.1 理解Spark转换及操作 136
7.1.1 RDD API 137
7.1.2 RDD转换操作 139
7.1.3 RDD功能操作 141
7.2 编程Spark转换及操作 142
7.3 Spark中的持久性 157
7.4 本章小结 159
第8章 Spark的SQL查询引擎——Spark SQL 160
8.1 Spark SQL的体系结构 161
8.1.1 Spark SQL的出现 161
8.1.2 Spark SQL的组件 162
8.1.3 Catalyst Optimizer 164
8.1.4 SQL/Hive context 165
8.2 编写第一个Spark SQL作业 166
8.2.1 用Scala编写Spark SQL作业 166
8.2.2 用Java编写Spark SQL作业 170
8.3 将RDD转换为DataFrame 173
8.3.1 自动化过程 174
8.3.2 手动过程 176
8.4 使用Parquet 179
8.4.1 在HDFS中持久化Parquet数据 182
8.4.2 数据分区和模式演化/合并 185
8.5 Hive表的集成 186
8.6 性能调优和最佳实践 190
8.6.1 分区和并行性 191
8.6.2 序列化 191
8.6.3 缓存 192
8.6.4 内存调优 192
8.7 本章小结 194
第9章 用Spark Streaming分析流数据 195
9.1 高级架构 195
9.1.1 Spark Streaming的组件 196
9.1.2 Spark Streaming的封装结构 198
9.2 编写第一个Spark Streaming作业 200
9.2.1 创建流生成器 201
9.2.2 用Scala编写Spark Streaming作业 202
9.2.3 用Java编写Spark Streaming作业 205
9.2.4 执行Spark Streaming作业 207
9.3 实时查询流数据 209
9.3.1 作业的高级架构 209
9.3.2 编写Crime生产者 210
9.3.3 编写Stream消费者和转换器 212
9.3.4 执行SQL Streaming Crime分析器 214
9.4 部署和监测 216
9.4.1 用于Spark Streaming的集群管理器 216
9.4.2 监测Spark Streaming应用程序 218
9.5 本章小结 219
第10章 介绍Lambda架构 220
10.1 什么是Lambda架构 220
10.1.1 Lambda架构的需求 220
10.1.2 Lambda架构的层/组件 222
10.2 Lambda架构的技术矩阵 226
10.3 Lambda架构的实现 228
10.3.1 高级架构 229
10.3.2 配置Apache Cassandra和Spark 230
10.3.3 编写自定义生产者程序 233
10.3.4 编写实时层代码 235
10.3.5 编写批处理层代码 238
10.3.6 编写服务层代码 239
10.3.7 执行所有层代码 241

1.1　运行环境准备2
1.1.1　安装JDK3
1.1.2　安装Scala3
1.1.3　安装Spark4
1.2　Spark初体验4
1.2.1　运行spark-shell4
1.2.2　执行word count5
1.2.3　剖析spark-shell7
1.3　阅读环境准备11
1.4　Spark源码编译与调试13
1.5　小结17
2.1　初识Spark18
2.1.1　Hadoop MRv1的局限18
2.1.2　Spark使用场景20
2.1.3　Spark的特点20
2.2　Spark基础知识20
2.3　Spark基本设计思想22
2.3.1　Spark模块设计22
2.3.2　Spark模型设计24
2.4　Spark基本架构25
2.5　小结26
第3章　SparkContext的初始化28
3.1　SparkContext概述28
3.2　创建执行环境SparkEnv30
3.2.1　安全管理器SecurityManager31
3.2.2　基于Akka的分布式消息系统ActorSystem31
3.2.3　map任务输出跟踪器mapOutputTracker32
3.2.4　实例化ShuffleManager34
3.2.5　shuffle线程内存管理器ShuffleMemoryManager34
3.2.6　块传输服务BlockTransferService35
3.2.7　BlockManagerMaster介绍35
3.2.8　创建块管理器BlockManager36
3.2.9　创建广播管理器Broadcast-Manager36
3.2.10　创建缓存管理器CacheManager37
3.2.11　HTTP文件服务器HttpFile-Server37
3.2.12　创建测量系统MetricsSystem39
3.2.13　创建SparkEnv40
3.3　创建metadataCleaner41
3.4　SparkUI详解42
3.4.1　listenerBus详解43
3.4.2　构造JobProgressListener46
3.4.3　SparkUI的创建与初始化47
3.4.4　Spark UI的页面布局与展示49
3.4.5　SparkUI的启动54
3.5　Hadoop相关配置及Executor环境变量54
3.5.1　Hadoop相关配置信息54
3.5.2　Executor环境变量54
3.6　创建任务调度器TaskScheduler55
3.6.1　创建TaskSchedulerImpl55
3.6.2　TaskSchedulerImpl的初始化57
3.7　创建和启动DAGScheduler57
3.8　TaskScheduler的启动60
3.8.1　创建LocalActor60
3.8.2　ExecutorSource的创建与注册62
3.8.3　ExecutorActor的构建与注册64
3.8.4　Spark自身ClassLoader的创建64
3.8.5　启动Executor的心跳线程66
3.9　启动测量系统MetricsSystem69
3.9.1　注册Sources70
3.9.2　注册Sinks70
3.9.3　给Sinks增加Jetty的Servlet-ContextHandler71
3.10　创建和启动ExecutorAllocation-Manager72
3.11　ContextCleaner的创建与启动73
3.12　Spark环境更新74
3.13　创建DAGSchedulerSource和BlockManagerSource76
3.14　将SparkContext标记为激活77
3.15　小结78
第4章　存储体系79
4.1　存储体系概述79
4.1.1　块管理器BlockManager的实现79
4.1.2　Spark存储体系架构81
4.2　shuffle服务与客户端83
4.2.1　Block的RPC服务84
4.2.2　构造传输上下文Transpor-tContext85
4.2.3　RPC客户端工厂Transport-ClientFactory86
4.2.4　Netty服务器TransportServer87
4.2.5　获取远程shuffle文件88
4.2.6　上传shuffle文件89
4.3　BlockManagerMaster对Block-Manager的管理90
4.3.1　BlockManagerMasterActor90
4.3.2　询问Driver并获取回复方法92
4.3.3　向BlockManagerMaster注册BlockManagerId93
4.4　磁盘块管理器DiskBlockManager94
4.4.1　DiskBlockManager的构造过程94
4.4.2　获取磁盘文件方法getFile96
4.4.3　创建临时Block方法create-TempShuffleBlock96
4.5　磁盘存储DiskStore97
4.5.1　NIO读取方法getBytes97
4.5.2　NIO写入方法putBytes98
4.5.3　数组写入方法putArray98
4.5.4　Iterator写入方法putIterator98
4.6　内存存储MemoryStore99
4.6.1　数据存储方法putBytes101
4.6.2　Iterator写入方法putIterator详解101
4.6.3　安全展开方法unrollSafely102
4.6.4　确认空闲内存方法ensureFreeSpace105
4.6.5　内存写入方法putArray107
4.6.6　尝试写入内存方法tryToPut108
4.6.7　获取内存数据方法getBytes109
4.6.8　获取数据方法getValues110
4.7　Tachyon存储TachyonStore110
4.7.1　Tachyon简介111
4.7.2　TachyonStore的使用112
4.7.3　写入Tachyon内存的方法putIntoTachyonStore113
4.7.4　获取序列化数据方法getBytes113
4.8　块管理器BlockManager114
4.8.1　移出内存方法dropFrom-Memory114
4.8.2　状态报告方法reportBlockStatus116
4.8.3　单对象块写入方法putSingle117
4.8.4　序列化字节块写入方法putBytes118
4.8.5　数据写入方法doPut118
4.8.6　数据块备份方法replicate121
4.8.7　创建DiskBlockObjectWriter的方法getDiskWriter125
4.8.8　获取本地Block数据方法getBlockData125
4.8.9　获取本地shuffle数据方法doGetLocal126
4.8.10　获取远程Block数据方法doGetRemote127
4.8.11　获取Block数据方法get128
4.8.12　数据流序列化方法dataSerializeStream129
4.9　metadataCleaner和broadcastCleaner129
4.10　缓存管理器CacheManager130
4.11　压缩算法133
4.12　磁盘写入实现DiskBlockObjectWriter133
4.13　块索引shuffle管理器IndexShuffleBlockManager135
4.14　shuffle内存管理器ShuffleMemoryManager137
4.15　小结138
第5章　任务提交与执行139
5.1　任务概述139
5.2　广播Hadoop的配置信息142
5.3　RDD转换及DAG构建144
5.3.1　为什么需要RDD144
5.3.2　RDD实现分析146
5.4　任务提交152
5.4.1　任务提交的准备152
5.4.2　finalStage的创建与Stage的划分157
5.4.3　创建Job163
5.4.4　提交Stage164
5.4.5　提交Task165
5.5　执行任务176
5.5.1　状态更新176
5.5.2　任务还原177
5.5.3　任务运行178
5.6　任务执行后续处理179
5.6.1　计量统计与执行结果序列化179
5.6.2　内存回收180
5.6.3　执行结果处理181
5.7　小结187
第6章　计算引擎188
6.1　迭代计算188
6.2　什么是shuffle192
6.3　map端计算结果缓存处理194
6.3.1　map端计算结果缓存聚合195
6.3.2　map端计算结果简单缓存200
6.3.3　容量限制201
6.4　map端计算结果持久化204
6.4.1　溢出分区文件205
6.4.2排序与分区分组207
6.4.3　分区索引文件209
6.5　reduce端读取中间计算结果210
6.5.1　获取map任务状态213
6.5.2　划分本地与远程Block215
6.5.3　获取远程Block217
6.5.4　获取本地Block218
6.6　reduce端计算219
6.6.1　如何同时处理多个map任务的中间结果219
6.6.2　reduce端在缓存中对中间计算结果执行聚合和排序220
6.7　map端与reduce端组合分析221
6.7.1　在map端溢出分区文件，在reduce端合并组合221
6.7.2　在map端简单缓存、排序分组，在reduce端合并组合222
6.7.3　在map端缓存中聚合、排序分组，在reduce端组合222
6.8　小结223
第7章　部署模式224
7.1　local部署模式225
7.2　local-cluster部署模式225
7.2.1　LocalSparkCluster的启动226
7.2.2　CoarseGrainedSchedulerBackend的启动236
7.2.3　启动AppClient237
7.2.4　资源调度242
7.2.5　local-cluster模式的任务执行253
7.3　Standalone部署模式255
7.3.1　启动Standalone模式255
7.3.2　启动Master分析257
7.3.3　启动Worker分析259
7.3.4　启动Driver Application分析261
7.3.5　Standalone模式的任务执行263
7.3.6　资源回收263
7.4　容错机制266
7.4.1　Executor异常退出266
7.4.2　Worker异常退出268
7.4.3　Master异常退出269
7.5　其他部署方案276
7.5.1　YARN277
7.5.2　Mesos280
7.6　小结282
扩　展　篇
第8章　Spark SQL284
8.1　Spark SQL总体设计284
8.1.1　传统关系型数据库SQL运行原理285
8.1.2　Spark SQL运行架构286
8.2　字典表Catalog288
8.3　Tree和TreeNode289
8.4　词法解析器Parser的设计与实现293
8.4.1　SQL语句解析的入口294
8.4.2　建表语句解析器DDLParser295
8.4.3　SQL语句解析器SqlParser296
8.4.4　Spark代理解析器SparkSQLParser299
8.5　Rule和RuleExecutor300
8.6　Analyzer与Optimizer的设计与实现302
8.6.1　语法分析器Analyzer304
8.6.2　优化器Optimizer305
8.7　生成物理执行计划306
8.8　执行物理执行计划308
8.9　Hive311
8.9.1　Hive SQL语法解析器311
8.9.2　Hive SQL元数据分析313
8.9.3　Hive SQL物理执行计划314
8.10　应用举例：JavaSparkSQL314
8.11　小结320
第9章　流式计算321
9.1　Spark Streaming总体设计321
9.2　StreamingContext初始化323
9.3　输入流接收器规范Receiver324
9.4　数据流抽象DStream325
9.4.1　Dstream的离散化326
9.4.2　数据源输入流InputDStream327
9.4.3　Dstream转换及构建DStream Graph329
9.5　流式计算执行过程分析330
9.5.1　流式计算例子CustomReceiver331
9.5.2　Spark Streaming执行环境构建335
9.5.3　任务生成过程347
9.6　窗口操作355
9.7　应用举例357
9.7.1　安装mosquitto358
9.7.2　启动mosquitto358
9.7.3　MQTTWordCount359
9.8　小结361
第10章　图计算362
10.1　Spark GraphX总体设计362
10.1.1　图计算模型363
10.1.2　属性图365
10.1.3　GraphX的类继承体系367
10.2　图操作368
10.2.1　属性操作368
10.2.2　结构操作368
10.2.3　连接操作369
10.2.4　聚合操作370
10.3　Pregel API371
10.3.1　Dijkstra算法373
10.3.2　Dijkstra的实现376
10.4　Graph的构建377
10.4.1　从边的列表加载Graph377
10.4.2　在Graph中创建图的方法377
10.5　顶点集合抽象VertexRDD378
10.6　边集合抽象EdgeRDD379
10.7　图分割380
10.8　常用算法382
10.8.1　网页排名382
10.8.2　Connected Components的应用386
10.8.3　三角关系统计388
10.9　应用举例390
10.10　小结391
第11章　机器学习392
11.1机器学习概论392
11.2　Spark MLlib总体设计394
11.3　数据类型394
11.3.1　局部向量394
11.3.2标记点395
11.3.3局部矩阵396
11.3.4分布式矩阵396
11.4基础统计398
11.4.1摘要统计398
11.4.2相关统计399
11.4.3分层抽样401
11.4.4假设检验401
11.4.5随机数生成402
11.5分类和回归405
11.5.1数学公式405
11.5.2线性回归407
11.5.3分类407
11.5.4回归410
11.6决策树411
11.6.1基本算法411
11.6.2使用例子412
11.7随机森林413
11.7.1基本算法414
11.7.2使用例子414
11.8梯度提升决策树415
11.8.1基本算法415
11.8.2使用例子416
11.9朴素贝叶斯416
11.9.1算法原理416
11.9.2使用例子418
11.10保序回归418
11.10.1算法原理418
11.10.2使用例子419
11.11协同过滤419
11.12聚类420
11.12.1K-means420
11.12.2高斯混合422
11.12.3快速迭代聚类422
11.12.4latent Dirichlet allocation422
11.12.5流式K-means423
11.13维数减缩424
11.13.1奇异值分解424
11.13.2主成分分析425
11.14特征提取与转型425
11.14.1术语频率反转425
11.14.2单词向量转换426
11.14.3标准尺度427
11.14.4正规化尺度428
11.14.5卡方特征选择器428
11.14.6Hadamard积429
11.15频繁模式挖掘429
11.16预言模型标记语言430
11.17管道431
11.17.1管道工作原理432
11.17.2管道API介绍433
11.17.3交叉验证435
11.18小结436
附录A　Utils437
附录B　Akka446
附录C　Jetty450
附录D　Metrics453
附录E　Hadoop word count456
附录F　CommandUtils458
附录G　Netty461
附录H　源码编译错误465



1．1．1 什么是Spark
1．1．2 Spark与MapReduce比较
1．1．3 Spark的演进路线图
1．2 Spark生态系统
1．2．1 Spark Core
1．2．2 Spark Streaming
1．2．3 Spark SQL
1．2．4 BlinkDB
1．2．5 MLBase/MLlib
1．2．6 GraphX
1．2．7 SparkR
1．2．8 Alluxio
2．1 基础环境搭建
2．1．1 搭建集群样板机
2．1．2 配置集群环境
2．2 编译Spark源代码
2．2．1 配置Spark编译环境
2．2．2 使用Maven编译Spark
2．2．3 使用SBT编译Spark
2．2．4 生成Spark部署包
2．3 搭建Spark运行集群
2．3．1 修改配置文件
2．3．2 启动Spark
2．3．3 验证启动
2．3．4 第一个实例
2．4 搭建Spark实战开发环境
2．4．1 CentOS中部署IDEA
2．4．2 使用IDEA开发程序
2．4．3 使用IDEA阅读源代码
2．5 小结

第二篇 核心篇
第3章 Spark编程模型
3．1 RDD概述
3．1．1 背景
3．1．2 RDD简介
3．1．3 RDD的类型
3．2 RDD的实现
3．2．1 作业调度
3．2．2 解析器集成
3．2．3 内存管理
3．2．4 检查点支持
3．2．5 多用户管理
3．3 编程接口
3．3．1 RDD分区（Partitions）
3．3．2 RDD首选位置（PreferredLocations）
3．3．3 RDD依赖关系（Dependencies）
3．3．4 RDD分区计算（Iterator）
3．3．5 RDD分区函数（Partitioner）
3．4 创建操作
3．4．1 并行化集合创建操作
3．4．2 外部存储创建操作
3．5 转换操作
3．5．1 基础转换操作
3．5．2 键值转换操作
3．6 控制操作
3．7 行动操作
3．7．1 集合标量行动操作
3．7．2 存储行动操作
3．8 小结
第4章 Spark核心原理
4．1 消息通信原理
4．1．1 Spark消息通信架构
4．1．2 Spark启动消息通信
4．1．3 Spark运行时消息通信
4．2 作业执行原理
4．2．1 概述
4．2．2 提交作业
4．2．3 划分调度阶段
4．2．4 提交调度阶段
4．2．5 提交任务
4．2．6 执行任务
4．2．7 获取执行结果
4．3 调度算法
4．3．1 应用程序之间
4．3．2 作业及调度阶段之间
4．3．3 任务之间
4．4 容错及HA
4．4．1 Executor异常
4．4．2 Worker异常
4．4．3 Master异常
4．5 监控管理
4．5．1 UI监控
4．5．2 Metrics
4．5．3 REST
4．6 实例演示
4．6．1 计算年降水实例
4．6．2 HA配置实例
4．7 小结
第5章 Spark存储原理
5．1 存储分析
5．1．1 整体架构
5．1．2 存储级别
5．1．3 RDD存储调用
5．1．4 读数据过程
5．1．5 写数据过程
5．2 Shuffle分析
5．2．1 Shuffle简介
5．2．2 Shuffle的写操作
5．2．3 Shuffle的读操作
5．3 序列化和压缩
5．3．1 序列化
5．3．2 压缩
5．4 共享变量
5．4．1 广播变量
5．4．2 累加器
5．5 实例演示
5．6 小结
第6章 Spark运行架构
6．1 运行架构总体介绍
6．1．1 总体介绍
6．1．2 重要类介绍
6．2 本地（Local）运行模式
6．2．1 运行模式介绍
6．2．2 实现原理
6．3 伪分布（Local-Cluster）运行模式
6．3．1 运行模式介绍
6．3．2 实现原理
6．4 独立（Standalone）运行模式
6．4．1 运行模式介绍
6．4．2 实现原理
6．5 YARN运行模式
6．5．1 YARN运行框架
6．5．2 YARN-Client运行模式介绍
6．5．3 YARN-Client 运行模式实现原理
6．5．4 YARN-Cluster运行模式介绍
6．5．5 YARN-Cluster 运行模式实现原理
6．5．6 YARN-Client与YARN-Cluster对比
6．6 Mesos运行模式
6．6．1 Mesos介绍
6．6．2 粗粒度运行模式介绍
6．6．3 粗粒度实现原理
6．6．4 细粒度运行模式介绍
6．6．5 细粒度实现原理
6．6．6 Mesos粗粒度和Mesos细粒度对比
6．7 实例演示
6．7．1 独立运行模式实例
6．7．2 YARN-Client实例
6．7．3 YARN-Cluster实例
6．8 小结

第三篇 组件篇
第7章 Spark SQL
7．1 Spark SQL简介
7．1．1 Spark SQL发展历史
7．1．2 DataFrame/Dataset介绍
7．2 Spark SQL运行原理
7．2．1 通用SQL执行原理
7．2．2 SparkSQL运行架构
7．2．3 SQLContext运行原理分析
7．2．4 HiveContext介绍
7．3 使用Hive-Console
7．3．1 编译Hive-Console
7．3．2 查看执行计划
7．3．3 应用Hive-Console
7．4 使用SQLConsole
7．4．1 启动HDFS和Spark Shell
7．4．2 与RDD交互操作
7．4．3 读取JSON格式数据
7．4．4 读取Parquet格式数据
7．4．5 缓存演示
7．4．6 DSL演示
7．5 使用Spark SQL CLI
7．5．1 配置并启动Spark SQL CLI
7．5．2 实战Spark SQL CLI
7．6 使用Thrift Server
7．6．1 配置并启动Thrift Server
7．6．2 基本操作
7．6．3 交易数据实例
7．6．4 使用IDEA开发实例
7．7 实例演示
7．7．1 销售数据分类实例
7．7．2 网店销售数据统计
7．8 小结
第8章 Spark Streaming
8．1 Spark Streaming简介
8．1．1 术语定义
8．1．2 Spark Streaming特点
8．2 Spark Streaming编程模型
8．2．1 DStream的输入源
8．2．2 DStream的操作
8．3 Spark Streaming运行架构
8．3．1 运行架构
8．3．2 消息通信
8．3．3 Receiver分发
8．3．4 容错性
8．4 Spark Streaming运行原理
8．4．1 启动流处理引擎
8．4．2 接收及存储流数据
8．4．3 数据处理
8．5 实例演示
8．5．1 流数据模拟器
8．5．2 销售数据统计实例
8．5．3 Spark Streaming+Kafka实例
8．6 小结
第9章 Spark MLlib
9．1 Spark MLlib简介
9．1．1 Spark MLlib介绍
9．1．2 Spark MLlib数据类型
9．1．3 Spark MLlib基本统计方法
9．1．4 预言模型标记语言
9．2 线性模型
9．2．1 数学公式
9．2．2 线性回归
9．2．3 线性支持向量机
9．2．4 逻辑回归
9．2．5 线性最小二乘法、Lasso和岭回归
9．2．6 流式线性回归
9．3 决策树
9．4 决策模型组合
9．4．1 随机森林
9．4．2 梯度提升决策树
9．5 朴素贝叶斯
9．6 协同过滤
9．7 聚类
9．7．1 K-means
9．7．2 高斯混合
9．7．3 快速迭代聚类
9．7．4 LDA
9．7．5 二分K-means
9．7．6 流式K-means
9．8 降维
9．8．1 奇异值分解降维
9．8．2 主成分分析降维
9．9 特征提取和变换
9．9．1 词频―逆文档频率
9．9．2 词向量化工具
9．9．3 标准化
9．9．4 范数化
9．10 频繁模式挖掘
9．10．1 频繁模式增长
9．10．2 关联规则挖掘
9．10．3 PrefixSpan
9．11 实例演示
9．11．1 K-means聚类算法实例
9．11．2 手机短信分类实例
9．12 小结
第10章 Spark GraphX
10．1 GraphX介绍
10．1．1 图计算
10．1．2 GraphX介绍
10．1．3 发展历程
10．2 GraphX实现分析
10．2．1 GraphX图数据模型
10．2．2 GraphX图数据存储
10．2．3 GraphX图切分策略
10．2．4 GraphX图操作
10．3 实例演示
10．3．1 图例演示
10．3．2 社区发现演示
10．4 小结
第11章 SparkR
11．1 概述
11．1．1 R语言介绍
11．1．2 SparkR介绍
11．2 SparkR与DataFrame
11．2．1 DataFrames介绍
11．2．2 与DataFrame的相关操作
11．3 编译安装SparkR
11．3．1 编译安装R语言
11．3．2 安装SparkR运行环境
11．3．3 安装SparkR
11．3．4 启动并验证安装
11．4 实例演示
11．5 小结
第12章 Alluxio
12．1 Alluxio简介
12．1．1 Alluxio介绍
12．1．2 Alluxio系统架构
12．1．3 HDFS与Alluxio
12．2 Alluxio编译部署
12．2．1 编译Alluxio
12．2．2 单机部署Alluxio
12．2．3 集群模式部署Alluxio
12．3 Alluxio命令行使用
12．3．1 接口说明
12．3．2 接口操作示例
12．4 实例演示
12．4．1 启动环境
12．4．2 Alluxio上运行Spark
12．4．3 Alluxio上运行MapReduce
12．5 小结


第1章　环境准备 1
1.1　运行环境准备 2
1.1.1　安装JDK 2
1.1.2　安装Scala 2
1.1.3　安装Spark 3
1.2　Spark初体验 4
1.2.1　运行spark-shell 4
1.2.2　执行word count 5
1.2.3　剖析spark-shell 9
1.3　阅读环境准备 14
1.3.1　安装SBT 15
1.3.2　安装Git 15
1.3.3　安装Eclipse Scala IDE插件 15
1.4　Spark源码编译与调试 17
1.5　小结 23
第2章　设计理念与基本架构 24
2.1　初识Spark 25
2.1.1　Hadoop MRv1的局限25
2.1.2　Spark的特点 26
2.1.3 Spark使用场景 28
2.2　Spark基础知识 29
2.3　Spark基本设计思想 31
2.3.1　Spark模块设计 32
2.3.2　Spark模型设计 34
2.4　Spark基本架构 36
2.5　小结 38
第3章　Spark基础设施 39
3.1　Spark配置 40
3.1.1　系统属性中的配置 40
3.1.2　使用SparkConf配置的API 41
3.1.3　克隆SparkConf配置 42
3.2　Spark内置RPC框架 42
3.2.1　RPC配置TransportConf 45
3.2.2　RPC客户端工厂Transport- ClientFactory 47
3.2.3　RPC服务端TransportServer 53
3.2.4　管道初始化 56
3.2.5　TransportChannelHandler详解 57
3.2.6　服务端RpcHandler详解 63
3.2.7　服务端引导程序Transport-ServerBootstrap 68
3.2.8　客户端TransportClient详解 71
3.3　事件总线 78
3.3.1　ListenerBus的继承体系 79
3.3.2　SparkListenerBus详解 80
3.3.3　LiveListenerBus详解 83
3.4　度量系统 87
3.4.1　Source继承体系 87
3.4.2　Sink继承体系 89
3.5　小结 92
第4章　SparkContext的初始化 93
4.1　SparkContext概述 94
4.2　创建Spark环境 97
4.3　SparkUI的实现 100
4.3.1　SparkUI概述 100
4.3.2　WebUI框架体系 102
4.3.3　创建SparkUI 107
4.4　创建心跳接收器 111
4.5　创建和启动调度系统112
4.6　初始化块管理器BlockManager 114
4.7　启动度量系统 114
4.8　创建事件日志监听器115
4.9　创建和启动ExecutorAllocation-Manager 116
4.10　ContextCleaner的创建与启动 120
4.10.1　创建ContextCleaner 120
4.10.2　启动ContextCleaner 120
4.11　额外的SparkListener与启动事件总线 122
4.12　Spark环境更新 123
4.13　SparkContext初始化的收尾 127
4.14　SparkContext提供的常用方法 128
4.15　SparkContext的伴生对象130
4.16　小结 131
第5章　Spark执行环境 132
5.1　SparkEnv概述 133
5.2　安全管理器SecurityManager 133
5.3　RPC环境 135
5.3.1　RPC端点RpcEndpoint 136
5.3.2　RPC端点引用RpcEndpointRef 139
5.3.3　创建传输上下文TransportConf 142
5.3.4　消息调度器Dispatcher 142
5.3.5　创建传输上下文Transport-Context 154
5.3.6　创建传输客户端工厂Transport-ClientFactory 159
5.3.7　创建TransportServer 160
5.3.8　客户端请求发送 162
5.3.9　NettyRpcEnv中的常用方法 173
5.4　序列化管理器SerializerManager 175
5.5　广播管理器BroadcastManager 178
5.6　map任务输出跟踪器 185
5.6.1　MapOutputTracker的实现 187
5.6.2　MapOutputTrackerMaster的实现原理 191
5.7　构建存储体系 199
5.8　创建度量系统 201
5.8.1　MetricsCon?g详解 203
5.8.2　MetricsSystem中的常用方法 207
5.8.3　启动MetricsSystem 209
5.9　输出提交协调器 211
5.9.1　OutputCommitCoordinator-Endpoint的实现 211
5.9.2　OutputCommitCoordinator的实现 212
5.9.3　OutputCommitCoordinator的工作原理 216
5.10　创建SparkEnv 217
5.11　小结 217
第6章　存储体系 219
6.1　存储体系概述 220
6.1.1　存储体系架构 220
6.1.2　基本概念 222
6.2　Block信息管理器 227
6.2.1　Block锁的基本概念 227
6.2.2　Block锁的实现 229
6.3　磁盘Block管理器 234
6.3.1　本地目录结构 234
6.3.2　DiskBlockManager提供的方法 236
6.4　磁盘存储DiskStore 239
6.5　内存管理器 242
6.5.1　内存池模型 243
6.5.2　StorageMemoryPool详解 244
6.5.3　MemoryManager模型 247
6.5.4　Uni?edMemoryManager详解 250
6.6　内存存储MemoryStore 252
6.6.1　MemoryStore的内存模型 253
6.6.2　MemoryStore提供的方法 255
6.7　块管理器BlockManager 265
6.7.1　BlockManager的初始化 265
6.7.2　BlockManager提供的方法 266
6.8　BlockManagerMaster对Block-Manager的管理 285
6.8.1　BlockManagerMaster的职责 285
6.8.2　BlockManagerMasterEndpoint详解 286
6.8.3　BlockManagerSlaveEndpoint详解 289
6.9　Block传输服务 290
6.9.1　初始化NettyBlockTransfer-Service 291
6.9.2　NettyBlockRpcServer详解 292
6.9.3　Shuf?e客户端 296
6.10　DiskBlockObjectWriter详解 305
6.11　小结 308
第7章　调度系统 309
7.1　调度系统概述 310
7.2　RDD详解 312
7.2.1　为什么需要RDD 312
7.2.2　RDD

第一部分　Spark MLlib基础
第1章　Spark机器学习简介 2
1．1　机器学习介绍 2
1．2　Spark介绍 3
1．3　Spark MLlib介绍 4
第2章　Spark数据操作 6
2．1　Spark RDD操作 6
2．1．1　Spark RDD创建操作 6
2．1．2　Spark RDD转换操作 7
2．1．3　Spark RDD行动操作 14
2．2　MLlib Statistics统计操作 15
2．2．1　列统计汇总 15
2．2．2　相关系数 16
2．2．3　假设检验 18
2．3　MLlib数据格式 18
2．3．1　数据处理 18
2．3．2　生成样本 22
第3章　Spark MLlib矩阵向量 26
3．1　Breeze介绍 26
3．1．1　Breeze创建函数 27
3．1．2　Breeze元素访问及操作函数 29
3．1．3　Breeze数值计算函数 34
3．1．4　Breeze求和函数 35
3．1．5　Breeze布尔函数 36
3．1．6　Breeze线性代数函数 37
3．1．7　Breeze取整函数 39
3．1．8　Breeze常量函数 40
3．1．9　Breeze复数函数 40
3．1．10　Breeze三角函数 40
3．1．11　Breeze对数和指数函数 40
3．2　BLAS介绍 41
3．2．1　BLAS向量-向量运算 42
3．2．2　BLAS矩阵-向量运算 42
3．2．3　BLAS矩阵-矩阵运算 43
3．3　MLlib向量 43
3．3．1　MLlib向量介绍 43
3．3．2　MLlib Vector接口 44
3．3．3　MLlib DenseVector类 46
3．3．4　MLlib SparseVector类 49
3．3．5　MLlib Vectors伴生对象 50
3．4　MLlib矩阵 57
3．4．1　MLlib矩阵介绍 57
3．4．2　MLlib Matrix接口 57
3．4．3　MLlib DenseMatrix类 59
3．4．4　MLlib SparseMatrix类 64
3．4．5　MLlib Matrix伴生对象 71
3．5　MLlib BLAS 77
3．6　MLlib分布式矩阵 93
3．6．1　MLlib分布式矩阵介绍 93
3．6．2　行矩阵（RowMatrix） 94
3．6．3　行索引矩阵（IndexedRowMatrix） 96
3．6．4　坐标矩阵（CoordinateMatrix） 97
3．6．5　分块矩阵（BlockMatrix） 98

第二部分　Spark MLlib回归算法
第4章　Spark MLlib线性回归算法 102
4．1　线性回归算法 102
4．1．1　数学模型 102
4．1．2　最小二乘法 105
4．1．3　梯度下降算法 105
4．2　源码分析 106
4．2．1　建立线性回归 108
4．2．2　模型训练run方法 111
4．2．3　权重优化计算 114
4．2．4　线性回归模型 121
4．3　实例 123
4．3．1　训练数据 123
4．3．2　实例代码 123
第5章　Spark MLlib逻辑回归算法 126
5．1　逻辑回归算法 126
5．1．1　数学模型 126
5．1．2 梯度下降算法 128
5．1．3　正则化 129
5．2　源码分析 132
5．2．1　建立逻辑回归 134
5．2．2　模型训练run方法 137
5．2．3　权重优化计算 137
5．2．4　逻辑回归模型 144
5．3　实例 148
5．3．1　训练数据 148
5．3．2　实例代码 148
第6章　Spark MLlib保序回归算法 151
6．1　保序回归算法 151
6．1．1　数学模型 151
6．1．2　L2保序回归算法 153
6．2　源码分析 153
6．2．1　建立保序回归 154
6．2．2　模型训练run方法 156
6．2．3　并行PAV计算 156
6．2．4　PAV计算 157
6．2．5　保序回归模型 159
6．3　实例 164
6．3．1　训练数据 164
6．3．2　实例代码 164

第三部分　Spark MLlib分类算法
第7章　Spark MLlib贝叶斯分类算法 170
7．1　贝叶斯分类算法 170
7．1．1　贝叶斯定理 170
7．1．2　朴素贝叶斯分类 171
7．2　源码分析 173
7．2．1　建立贝叶斯分类 173
7．2．2　模型训练run方法 176
7．2．3　贝叶斯分类模型 179
7．3　实例 181
7．3．1　训练数据 181
7．3．2　实例代码 182
第8章　Spark MLlib SVM支持向量机算法 184
8．1　SVM支持向量机算法 184
8．1．1　数学模型 184
8．1．2　拉格朗日 186
8．2　源码分析 189
8．2．1　建立线性SVM分类 191
8．2．2　模型训练run方法 194
8．2．3　权重优化计算 194
8．2．4　线性SVM分类模型 196
8．3　实例 199
8．3．1　训练数据 199
8．3．2　实例代码 199
第9章　Spark MLlib决策树算法 202
9．1　决策树算法 202
9．1．1　决策树 202
9．1．2　特征选择 203
9．1．3　决策树生成 205
9．1．4　决策树生成实例 206
9．1．5　决策树的剪枝 208
9．2　源码分析 209
9．2．1　建立决策树 211
9．2．2　建立随机森林 216
9．2．3　建立元数据 220
9．2．4　查找特征的分裂及划分 223
9．2．5　查找最好的分裂顺序 228
9．2．6　决策树模型 231
9．3　实例 234
9．3．1　训练数据 234
9．3．2　实例代码 234

第四部分　Spark MLlib聚类算法
第10章　Spark MLlib KMeans聚类算法 238
10．1　KMeans聚类算法 238
10．1．1　KMeans算法 238
10．1．2　演示KMeans算法 239
10．1．3　初始化聚类中心点 239
10．2　源码分析 240
10．2．1　建立KMeans聚类 242
10．2．2　模型训练run方法 247
10．2．3　聚类中心点计算 248
10．2．4　中心点初始化 251
10．2．5　快速距离计算 254
10．2．6　KMeans聚类模型 255
10．3　实例 258
10．3．1　训练数据 258
10．3．2　实例代码 259
第11章　Spark MLlib LDA主题模型算法 261
11．1　LDA主题模型算法 261
11．1．1　LDA概述 261
11．1．2　LDA概率统计基础 262
11．1．3　LDA数学模型 264
11．2　GraphX基础 267
11．3　源码分析 270
11．3．1　建立LDA主题模型 272
11．3．2　优化计算 279
11．3．3　LDA模型 283
11．4　实例 288
11．4．1　训练数据 288
11．4．2　实例代码 288

第五部分　Spark MLlib关联规则挖掘算法
第12章　Spark MLlib FPGrowth关联规则算法 292
12．1　FPGrowth关联规则算法 292
12．1．1　基本概念 292
12．1．2　FPGrowth算法 293
12．1．3　演示FP树构建 294
12．1．4　演示FP树挖掘 296
12．2　源码分析 298
12．2．1　FPGrowth类 298
12．2．2　关联规则挖掘 300
12．2．3　FPTree类 303
12．2．4　FPGrowthModel类 306
12．3　实例 306
12．3．1　训练数据 306
12．3．2　实例代码 306

第六部分　Spark MLlib推荐算法
第13章　Spark MLlib ALS交替最小二乘算法 310
13．1　ALS交替最小二乘算法 310
13．2　源码分析 312
13．2．1　建立ALS 314
13．2．2　矩阵分解计算 322
13．2．3　ALS模型 329
13．3　实例 334
13．3．1　训练数据 334
13．3．2　实例代码 334
第14章　Spark MLlib协同过滤推荐算法 337
14．1　协同过滤推荐算法 337
14．1．1　协同过滤推荐概述 337
14．1．2　用户评分 338
14．1．3　相似度计算 338
14．1．4　推荐计算 340
14．2　协同推荐算法实现 341
14．2．1　相似度计算 344
14．2．2　协同推荐计算 348
14．3　实例 350
14．3．1　训练数据 350
14．3．2　实例代码 350

第七部分　Spark MLlib神经网络算法
第15章　Spark MLlib神经网络算法综述 354
15．1　人工神经网络算法 354
15．1．1　神经元 354
15．1．2　神经网络模型 355
15．1．3 信号前向传播 356
15．1．4　误差反向传播 357
15．1．5　其他参数 360
15．2　神经网络算法实现 361
15．2．1　神经网络类 363
15．2．2　训练准备 370
15．2．3　前向传播 375
15．2．4　误差反向传播 377
15．2．5　权重更新 381
15．2．6　ANN模型 382
15．3　实例 384
15．3．1　测试数据 384
15．3．2　测试函数代码 387
15．3．3　实例代码 388


第1章PythonSpark机器学习与Hadoop大数据1
1.1机器学习的介绍2
1.2Spark的介绍5
1.3Spark数据处理RDD、DataFrame、SparkSQL7
1.4使用Python开发Spark机器学习与大数据应用8
1.5PythonSpark机器学习9
1.6SparkMLPipeline机器学习流程介绍10
1.7Spark2.0的介绍12
1.8大数据定义13
1.9Hadoop简介14
1.10HadoopHDFS分布式文件系统14
1.11HadoopMapReduce的介绍17
1.12结论18
第2章VirtualBox虚拟机软件的安装19
2.1VirtualBox的下载和安装20
2.2设置VirtualBox存储文件夹23
2.3在VirtualBox创建虚拟机25
2.4结论29
第3章UbuntuLinux操作系统的安装30
3.1UbuntuLinux操作系统的安装31
3.2在Virtual设置Ubuntu虚拟光盘文件33
3.3开始安装Ubuntu35
3.4启动Ubuntu40
3.5安装增强功能41
3.6设置默认输入法45
3.7设置“终端”程序48
3.8设置“终端”程序为白底黑字49
3.9设置共享剪贴板50
3.10设置最佳下载服务器52
3.11结论56
第4章HadoopSingleNodeCluster的安装57
4.1安装JDK58
4.2设置SSH无密码登录61
4.3下载安装Hadoop64
4.4设置Hadoop环境变量67
4.5修改Hadoop配置设置文件69
4.6创建并格式化HDFS目录73
4.7启动Hadoop74
4.8打开HadoopResource-ManagerWeb界面76
4.9NameNodeHDFSWeb界面78
4.10结论79
第5章HadoopMultiNodeCluster的安装80
5.1把SingleNodeCluster复制到data183
5.2设置VirtualBox网卡84
5.3设置data1服务器87
5.4复制data1服务器到data2、data3、master94
5.5设置data2服务器97
5.6设置data3服务器100
5.7设置master服务器102
5.8master连接到data1、data2、data3创建HDFS目录107
5.9创建并格式化NameNodeHDFS目录110
5.10启动HadoopMultiNodeCluster112
5.11打开HadoopResourceManagerWeb界面114
5.12打开NameNodeWeb界面115
5.13停止HadoopMultiNodeCluster116
5.14结论116
第6章HadoopHDFS命令117
6.1启动HadoopMulti-NodeCluster118
6.2创建与查看HDFS目录120
6.3从本地计算机复制文件到HDFS122
6.4将HDFS上的文件复制到本地计算机127
6.5复制与删除HDFS文件129
6.6在HadoopHDFSWeb用户界面浏览HDFS131
6.7结论134
第7章HadoopMapReduce135
7.1简单介绍WordCount.java136
7.2编辑WordCount.java137
7.3编译WordCount.java141
7.4创建测试文本文件143
7.5运行WordCount.java145
7.6查看运行结果146
7.7结论147
第8章PythonSpark的介绍与安装148
8.1Scala的介绍与安装150
8.2安装Spark153
8.3启动pyspark交互式界面156
8.4设置pyspark显示信息157
8.5创建测试用的文本文件159
8.6本地运行pyspark程序161
8.7在HadoopYARN运行pyspark163
8.8构建SparkStandaloneCluster运行环境165
8.9在SparkStandalone运行pyspark171
8.10SparkWebUI界面173
8.11结论175
目录
第1章1
1.1引言1
1.2基本术2
1.3假设空间4
1.4归纳偏好6
1.5发展历程10
1.6应用现状13
1.7阅读材料16
习题19
参考文献20
休息一会儿22
第2章模型评估与选择23
2.1经验误差与过拟合23
2.2评估方法24
2.2.1留出法25
2.2.2交叉验证法26
2.2.3自助法27
2.2.4调参与最终模型28
2.3性能度量28
2.3.1错误率与精度29
2.3.2查准率、查全率与F130
2.3.3ROC与AUC33
2.3.4代价敏感错误率与代价曲线35
2.4比较检验37
2.4.1假设检验37
2.4.2交叉验证t检验40
2.4.3McNemar检验41
2.4.4Friedman检验与后续检验42
2.5偏差与方差44
2.6阅读材料46
习题48
参考文献49
休息一会儿51
第3章线性模型53
3.1基本形式53
3.2线性回归53
3.3对数几率回归57
3.4线性判别分析60
3.5多分类学习63
3.6类别不平衡问题66
3.7阅读材料67
习题69
参考文献70
休息一会儿72
第4章决策树73
4.1基本流程73
4.2划分选择75
4.2.1信息增益75
4.2.2增益率77
4.2.3基尼指数79
4.3剪枝处理79
4.3.1预剪枝80
4.3.2后剪枝82
4.4连续与缺失值83
4.4.1连续值处理83
4.4.2缺失值处理85
4.5多变量决策树88
4.6阅读材料92
习题93
参考文献94
休息一会儿95
第5章神经网络97
5.1神经元模型97
5.2感知机与多层网络98
5.3误差逆传播算法101
5.4全局最小与局部极小106
5.5其他常见神经网络108
5.5.1RBF网络108
5.5.2ART网络108
5.5.3SOM网络109
5.5.4级联相关网络110
5.5.5Elman网络111
5.5.6Boltzmann机111
5.6深度学习113
5.7阅读材料115
习题116
参考文献117
休息一会儿120
第6章支持向量机121
6.1间隔与支持向量121
6.2对偶问题123
6.3核函数126
6.4软间隔与正则化129
6.5支持向量回归133
6.6核方法137
6.7阅读材料139
习题141
参考文献142
休息一会儿145
第7章贝叶斯分类器147
7.1贝叶斯决策论147
7.2极大似然估计149
7.3朴素贝叶斯分类器150
7.4半朴素贝叶斯分类器154
7.5贝叶斯网156
7.5.1结构157
7.5.2学习159
7.5.3推断161
7.6EM算法162
7.7阅读材料164
习题166
参考文献167
休息一会儿169
第8章集成学习171
8.1个体与集成171
8.2Boosting173
8.3Bagging与随机森林178
8.3.1Bagging178
8.3.2随机森林179
8.4结合策略181
8.4.1平均法181
8.4.2投票法182
8.4.3学习法183
8.5多样性185
8.5.1误差--分歧分解185
8.5.2多样性度量186
8.5.3多样性增强188
8.6阅读材料190
习题192
参考文献193
休息一会儿196
第9章聚类197
9.1聚类任务197
9.2性能度量197
9.3距离计算199
9.4原型聚类202
9.4.1k均值算法202
9.4.2学习向量量化204
9.4.3高斯混合聚类206
9.5密度聚类211
9.6层次聚类214
9.7阅读材料217
习题220
参考文献221
休息一会儿224
第10章降维与度量学习225
10.1k近邻学习225
10.2低维嵌入226
10.3主成分分析229
10.4核化线性降维232
10.5流形学习234
10.5.1等度量映射234
10.5.2局部线性嵌入235
10.6度量学习237
10.7阅读材料240
习题242
参考文献243
休息一会儿246
第11章特征选择与稀疏学习247
11.1子集搜索与评价247
11.2过滤式选择249
11.3包裹式选择250
11.4嵌入式选择与L$_1$正则化252
11.5稀疏表示与字典学习254
11.6压缩感知257
11.7阅读材料260
习题262
参考文献263
休息一会儿266
第12章计算学习理论267
12.1基础知识267
12.2PAC学习268
12.3有限假设空间270
12.3.1可分情形270
12.3.2不可分情形272
12.4VC维273
12.5Rademacher复杂度279
12.6稳定性284
12.7阅读材料287
习题289
参考文献290
休息一会儿292
第13章半监督学习293
13.1未标记样本293
13.2生成式方法295
13.3半监督SVM298
13.4图半监督学习300
13.5基于分歧的方法304
13.6半监督聚类307
13.7阅读材料311
习题313
参考文献314
休息一会儿317
第14章概率图模型319
14.1隐马尔可夫模型319
14.2马尔可夫随机场322
14.3条件随机场325
14.4学习与推断328
14.4.1变量消去328
14.4.2信念传播330
14.5近似推断331
14.5.1MCMC采样331
14.5.2变分推断334
14.6话题模型337
14.7阅读材料339
习题341
参考文献342
休息一会儿345
第15章规则学习347
15.1基本概念347
15.2序贯覆盖349
15.3剪枝优化352
15.4一阶规则学习354
15.5归纳逻辑程序设计357
15.5.1最小一般泛化358
15.5.2逆归结359
15.6阅读材料363
习题365
参考文献366
休息一会儿369
第16章强化学习371
16.1任务与奖赏371
16.2$K$-摇臂赌博机373
16.2.1探索与利用373
16.2.2$\epsilon$-贪心374
16.2.3Softmax375
16.3有模型学习377
16.3.1策略评估377
16.3.2策略改进379
16.3.3策略迭代与值迭代381
16.4免模型学习382
16.4.1蒙特卡罗强化学习383
16.4.2时序差分学习386
16.5值函数近似388
16.6模仿学习390
16.6.1直接模仿学习391
16.6.2逆强化学习391
16.7阅读材料393
习题394
参考文献395
休息一会儿397
附录399
A矩阵399
B优化403
C概率分布409
后记417
索引419