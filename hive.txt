dy 章初识Hadoop3
1.1数据！数据！3
1.2数据的存储与分析5
1.3查询所有数据6
1.4不仅仅是批处理7
1.5相较于其他系统的优势8
1.6ApacheHadoop发展简史12
1.7本书包含的内容16
第2章关于MapReduce19
2.1气象数据集19
2.2使用Unix工具来分析数据21
2.3使用Hadoop来分析数据22
2.4横向扩展31
2.5HadoopStreaming37
第3章Hadoop分布式文件系统42
3.1HDFS的设计42
3.2HDFS的概念44
3.3命令行接口50
3.4Hadoop文件系统52
3.5Java接口56
3.6数据流68
3.7通过distcp并行复制76
第4章关于YARN78
4.1剖析YARN应用运行机制79
4.2YARN与MapReduce1相比82
4.3YARN中的调度85
4.4延伸阅读95
第5章Hadoop的I/O操作96
5.1数据完整性96
5.2压缩99
5.3序列化109
5.4基于文件的数据结构127
第Ⅱ部分关于MapReduce
第6章MapReduce应用开发141
6.1用于配置的API142
6.2配置开发环境144
6.3用MRUnit来写单元测试152
6.4本地运行测试数据156
6.5在集群上运行160
6.6作业调优174
6.7MapReduce的工作流176
第7章MapReduce的工作机制184
7.1剖析MapReduce作业运行
机制184
7.2失败191
7.3shuffle和排序195
7.4任务的执行201
第8章MapReduce的
类型与格式207
8.1MapReduce的类型207
8.2输入格式218
8.3输出格式236
第9章MapReduce的特性243
9.1计数器243
9.2排序252
9.3连接264
9.4边数据分布270
9.5MapReduce库类276
第Ⅲ部分Hadoop的操作
dy 0章构建Hadoop集群279

dy 章 基础知识
1.1 Hadoop和MapReduce综述
1.2 Hadoop生态系统中的Hive
1.2.1 Pig
1.2.2 HBase
1.2.3 Cascading、Crunch及其他
1.3 Java和Hive：词频统计算法
1.4 后续事情
第2章 基础操作
 2.1 安装预先配置好的虚拟机
 2.2 安装详细步骤
 2.2.1 装Java
 2.2.2 安装Hadoop
 2.2.3 本地模式、伪分布式模式和分布式模式
 2.2.4 测试Hadoop
 2.2.5 安装Hive
 2.3 Hive内部是什么
 2.4 启动Hive
 2.5 配置Hadoop环境
 2.5.1 本地模式配置
 2.5.2 分布式模式和伪分布式模式配置
 2.5.3 使用JDBC连接元数据
 2.6 Hive命令
 2.7 命令行界面
 2.7.1 CLI 选项
 2.7.2 变量和属性
 2.7.3 Hive中“一次使用”命令
 2.7.4 从文件中执行Hive查询
 2.7.5 hiverc文件
 2.7.6 使用Hive CLI的更多介绍
 2.7.7 查看操作命令历史
 2.7.8 执行shell命令
 2.7.9 在Hive内使用Hadoop的dfs命令
 2.7.10 Hive脚本中如何进行注释
     2.7.11 显示字段名称

第3章 数据类型和文件格式
 3.1 基本数据类型
 3.2 集合数据类型
 3.3 文本文件数据编码
 3.4 读时模式

第4章 HiveQL：数据定义
 4.1 Hive中的数据库
 4.2 修改数据库
 4.3 创建表
 4.3.1 管理表
 4.3.2 外部表
 4.4 分区表、管理表
 4.4.1 外部分区表
 4.4.2 自定义表的存储格式
 4.5 删除表
 4.6 修改表
 4.6.1 表重命名
 4.6.2 增加、修改和删除表分区
 4.6.3 修改列信息
 4.6.4 增加列
 4.6.5 删除或者替换列
 4.6.6 修改表属性
 4.6.7 修改存储属性
 4.6.8 众多的修改表语句

…………已省略更多目录

Spark快速大数据分析-目录

推荐序 xi
译者序 xiv
序 xvi
前言 xvii
dy 章 Spark数据分析导论 1
1．1 Spark是什么 1
1．2 一个大一统的软件栈 2
1．2．1 Spark Core 2
1．2．2 Spark SQL 3
1．2．3 Spark Streaming 3
1．2．4 MLlib 3
1．2．5 GraphX 3
1．2．6 集群管理器 4
1．3 Spark的用户和用途 4
1．3．1 数据科学任务 4
1．3．2 数据处理应用 5
1．4 Spark简史 5
1．5 Spark的版本和发布 6
1．6 Spark的存储层次 6
第2章 Spark下载与入门 7
2．1 下载Spark 7
2．2 Spark中Python和Scala的shell 9
2．3 Spark 核心概念简介 12
2．4 独立应用 14
2．4．1 初始化SparkContext 15
2．4．2 构建独立应用 16
2．5 总结 19
第3章 RDD编程 21
3．1 RDD基础 21
3．2 创建RDD 23
3．3 RDD操作 24
3．3．1 转化操作 24
3．3．2 行动操作 26
3．3．3 惰性求值 27
3．4 向Spark传递函数 27
3．4．1 Python 27
3．4．2 Scala 28
3．4．3 Java 29
3．5 常见的转化操作和行动操作 30
3．5．1 基本RDD 30
3．5．2 在不同RDD类型间转换 37
3．6 持久化( 缓存) 39
3．7 总结 40
第4章 键值对操作 41
4．1 动机 41
4．2 创建Pair RDD 42
4．3 Pair RDD的转化操作 42
4．3．1 聚合操作 45
4．3．2 数据分组 49
4．3．3 连接 50
4．3．4 数据排序 51
4．4 Pair RDD的行动操作 52
4．5 数据分区（进阶） 52
4．5．1 获取RDD的分区方式 55
4．5．2 从分区中获益的操作 56
4．5．3 影响分区方式的操作 57
4．5．4 示例：PageRank 57
4．5．5 自定义分区方式 59
4．6 总结 61
第5章 数据读取与保存 63
5．1 动机 63
…………已省略更多目录

dy 章 基础知识

1.1 Hadoop和MapReduce综述

1.2 Hadoop生态系统中的Hive

1.2.1 Pig

1.2.2 HBase

1.2.3 Cascading、Crunch及其他

1.3 Java和Hive：词频统计算法

1.4 后续事情



第2章 基础操作

2.1 安装预先配置好的虚拟机

2.2 安装详细步骤

2.2.1 装Java

2.2.2 安装Hadoop

2.2.3 本地模式、伪分布式模式和分布式模式

2.2.4 测试Hadoop

2.2.5 安装Hive

2.3 Hive内部是什么

2.4 启动Hive

2.5 配置Hadoop环境

2.5.1 本地模式配置

2.5.2 分布式模式和伪分布式模式配置

2.5.3 使用JDBC连接元数据

2.6 Hive命令

2.7 命令行界面

2.7.1 CLI 选项

2.7.2 变量和属性

2.7.3 Hive中“一次使用”命令

2.7.4 从文件中执行Hive查询

2.7.5 hiverc文件

2.7.6 使用Hive CLI的更多介绍

2.7.7 查看操作命令历史

2.7.8 执行shell命令

2.7.9 在Hive内使用Hadoop的dfs命令

2.7.10 Hive脚本中如何进行注释

2.7.11 显示字段名称



第3章 数据类型和文件格式

3.1 基本数据类型

3.2 集合数据类型

3.3 文本文件数据编码

3.4 读时模式



第4章 HiveQL：数据定义

4.1 Hive中的数据库

4.2 修改数据库

4.3 创建表

4.3.1 管理表

4.3.2 外部表

4.4 分区表、管理表

4.4.1 外部分区表

4.4.2 自定义表的存储格式

4.5 删除表

4.6 修改表

4.6.1 表重命名

4.6.2 增加、修改和删除表分区

4.6.3 修改列信息

4.6.4 增加列

4.6.5 删除或者替换列

4.6.6 修改表属性

4.6.7 修改存储属性

4.6.8 众多的修改表语句



第5章 HiveQL：数据操作

5.1 向管理表中装载数据

5.2 通过查询语句向表中插入数据

5.3 单个查询语句中创建表并加载数据

5.4 导出数据



第6章 HiveQL：查询

6.1 SELECT…FROM语句

6.1.1 使用正则表达式来指定列

6.1.2 使用列值进行计算

6.1.3 算术运算符

6.1.4 使用函数

6.1.5 LIMIT语句

6.1.6 列别名

6.1.7 嵌套SELECT语句

6.1.8 CASE…WHEN…THEN 句式

6.1.9 什么情况下Hive可以避免进行MapReduce

6.2 WHERE语句

6.2.1 谓词操作符

6.2.2 关于浮点数比较

6.2.3 LIKE和RLIKE

6.3 GROUP BY 语句

6.4 JOIN语句

6.4.1 INNER JOIN

6.4.2 JOIN优化

6.4.3 LEFT OUTER JOIN

6.4.4 OUTER JOIN

6.4.5 RIGHT OUTER JOIN

6.4.6 FULL OUTER JOIN

6.4.7 LEFT SEMI-JOIN

6.4.8 笛卡尔积JOIN

6.4.9 map-side JOIN

6.5 ORDER BY和SORT BY

6.6 含有SORT BY 的DISTRIBUTE BY

6.7 CLUSTER BY

6.8 类型转换

6.9 抽样查询

6.9.1 数据块抽样

6.9.2 分桶表的输入裁剪

6.10 UNION ALL



第7章 HiveQL：视图

7.1 使用视图来降低查询复杂度

7.2 使用视图来限制基于条件过滤的数据

7.3 动态分区中的视图和map类型

7.4 视图零零碎碎相关的事情



第8章 HiveQL：索引

8.1 创建索引

8.2 重建索引

8.3 显示索引

8.4 删除索引

8.5 实现一个定制化的索引处理器



第9章 模式设计

9.1 按天划分的表

9.2 关于分区

9.3 wei一键和标准化

9.4 同一份数据多种处理

9.5 对于每个表的分区

9.6 分桶表数据存储

9.7 为表增加列

9.8 使用列存储表

9.8.1 重复数据

9.8.2 多列

9.9 (几乎)总是使用压缩



dy 0章 调优

10.1 使用EXPLAIN

10.2 EXPLAIN EXTENDED

10.3 限制调整

10.4 JOIN优化

10.5 本地模式

10.6 并行执行

10.7 严格模式

10.8 调整mapper和reducer个数

10.9 JVM重用

10.10 索引

10.11 动态分区调整

10.12 推测执行

10.13 单个MapReduce中多个GROUP BY

10.14 虚拟列



dy 1章 其他文件格式和压缩方法

11.1 确定安装编解码器

11.2 选择一种压缩编/解码器

11.3 开启中间压缩

11.4 终输出结果压缩

11.5 sequence file存储格式

11.6 使用压缩实践

11.7 存档分区

11.8 压缩：包扎



dy 2章 开发

12.1 修改Log4J属性

12.2 连接Java调试器到Hive

12.3 从源码编译Hive

12.3.1 执行Hive测试用例

12.3.2 执行hook

12.4 配置Hive和Eclipse

12.5 Maven工程中使用Hive

12.6 Hive中使用hive_test进行单元测试

12.7 新增的插件开发工具箱(PDK)



dy 3章 函数

13.1 发现和描述函数

13.2 调用函数

13.3 标准函数

13.4 聚合函数

13.5 表生成函数

13.6 一个通过日期计算其星座的UDF

13.7 UDF与GenericUDF

13.8 不变函数

13.9 用户自定义聚合函数

13.10 用户自定义表生成函数

13.10.1 可以产生多行数据的UDTF

13.10.2 可以产生具有多个字段的单行数据的UDTF

13.10.3 可以模拟复杂数据类型的UDTF

13.11 在 UDF中访问分布式缓存

13.12 以函数的方式使用注解

13.12.1 定数性(deterministic)标注

13.12.2 状态性(stateful)标注

13.12.3 wei一性

13.13 宏命令



dy 4章 Streaming

14.1 恒等变换

14.2 改变类型

14.3 投影变换

14.4 操作转换

14.5 使用分布式内存

14.6 由一行产生多行

14.7 使用streaming进行聚合计算

14.8 CLUSTER BY、DISTRIBUTE BY、SORT BY

14.9 GenericMR Tools for Streaming to Java

14.10 计算cogroup



dy 5章 自定义Hive文件和记录格式

15.1 文件和记录格式

15.2 阐明CREATE TABLE句式

15.3 文件格式

15.3.1 SequenceFile

15.3.2 RCfile

15.3.3 示例自定义输入格式：DualInputFormat

15.4 记录格式：SerDe

15.5 CSV和TSV SerDe

15.6 ObjectInspector

15.7 Thing Big Hive Reflection ObjectInspector

15.8 XML UDF

15.9 XPath相关的函数

15.10 JSON SerDe

15.11 Avro Hive SerDe

15.11.1 使用表属性信息定义Avro Schema

15.11.2 从指定URL中定义Schema

15.11.3 进化的模式

15.12 二进制输出



dy 6章 Hive的Thrift服务

16.1 启动Thrift Server

16.2 配置Groovy使用HiveServer

16.3 连接到HiveServer

16.4 获取集群状态信息

16.5 结果集模式

16.6 获取结果

16.7 获取执行计划

16.8 元数据存储方法

16.9 管理HiveServer

16.9.1 生产环境使用HiveServer

16.9.2 清理

16.10 Hive ThriftMetastore

16.10.1 ThriftMetastore 配置

16.10.2 客户端配置



dy 7章 存储处理程序和NoSQL

17.1 Storage Handler Background

17.2 HiveStorageHandler

17.3 HBase

17.4 Cassandra

17.4.1 静态列映射(Static Column Mapping)

17.4.2 为动态列转置列映射

17.4.3 Cassandra SerDe Properties

17.5 DynamoDB



dy 8章 安全

18.1 和Hadoop安全功能相结合

18.2 使用Hive进行验证

18.3 Hive中的权限管理

18.3.1 用户、组和角色

18.3.2 Grant 和 Revoke权限

18.4 分区级别的权限

18.5 自动授权



dy 9章 锁

19.1 Hive结合Zookeeper支持锁功能

19.2 显式锁和独占锁



第20章 Hive和Oozie整合

20.1 Oozie提供的多种动作(Action)

20.2 一个只包含两个查询过程的工作流示例

20.3 Oozie 网页控制台

20.4 工作流中的变量

20.5 获取输出

20.6 获取输出到变量



第21章 Hive和亚马逊网络服务系统(AWS)

21.1 为什么要弹性MapReduce

21.2 实例

21.3 开始前的注意事项

21.4 管理自有EMR Hive集群

21.5 EMR Hive上的Thrift Server服务

21.6 EMR上的实例组

21.7 配置EMR集群

21.7.1 部署hive-site.xml文件

21.7.2 部署.hiverc脚本

21.7.3 建立一个内存密集型配置

21.8 EMR上的持久层和元数据存储

21.9 EMR集群上的HDFS和S3

21.10 在S3上部署资源、配置和辅助程序脚本

21.11 S3上的日志

21.12 现买现卖

21.13 安全组

21.14 EMR和EC2以及Apache Hive的比较

21.15 包装



第22章 HCatalog

22.1 介绍

22.2 MapReduce

22.2.1 读数据

22.2.2 写数据

22.3 命令行

22.4 安全模型

22.5 架构



第23章 案例研究

23.1 m6d.com(Media6Degrees)

23.1.1 M 6D的数据科学，使用Hive和R

23.1.2 M6D UDF伪随机

23.1.3 M6D如何管理多MapReduce集群间的Hive数据访问

23.2 Outbrain

23.2.1 站内线上身份识别

23.2.2 计算复杂度

23.2.3 会话化

23.3 NASA喷气推进实验室

23.3.1 区域气候模型评价系统

23.3.2 我们的经验：为什么使用Hive

23.3.3 解决这些问题我们所面临的挑战

23.4 Photobucket

23.4.1 Photobucket 公司的大数据应用情况

23.4.2 Hive所使用的硬件资源信息

23.4.3 Hive提供了什么

23.4.4 Hive支持的用户有哪些

23.5 SimpleReach

23.6 Experiences and Needs from the Customer Trenches

23.6.1 介绍

23.6.2 Customer Trenches的用例



术语词汇表

前言
第1章 Hive介绍
1．1 Hive工作原理
1．2 Hive的数据类型
1．3 Hive的特点
1．4 本章小结

第2章 Hive架构
2．1 Hive用户接口
2．1．1 Hive CLI
2．1．2 HWI
2．1．3 Thrift服务
2．2 Hive元数据库
2．2．1 Hive元数据表结构
2．2．2 Hive元数据的三种存储模式
2．3 Hive数据存储
2．4 Hive文件格式
2．4．1 TextFile格式
2．4．2 SequenceFile格式
2．4．3 RCFile格式
2．4．4 ORC格式
2．5 本章小结

第3章 HiveQL表操作
3．1 内部表
3．2 外部表
3．3 分区表
3．3．1 静态分区
3．3．2 动态分区
3．4 桶表
3．5 视图
3．5．1 使用视图降低查询复杂度
3．5．2 使用视图来限制基于条件过滤的数据
3．5．3 动态分区中的视图和map类型
3．6 本章小结

第4章 HiveQL数据操作
4．1 装载数据到表中
4．2 通过查询语句向表中插入数据
4．3 单个查询语句中创建并加载数据
4．4 导出数据
4．5 本章小结

第5章 HiveQL查询
5．1 SELECT…FROM语句
5．1．1 使用正则表达式来指定列的
5．1．2 使用列值进行计算
5．1．3 算述运算符
5．1．4 使用函数
5．1．5 LIMIT语句
5．1．6 列别名
5．1．7 嵌套SELECT语句
5．1．8 CASE…WHEN…THEN语句
5．2 WHERE语句
5．2．1 谓词操作符
5．2．2 关于浮点数比较
5．2．3 LIKE和RLIKE
5．3 GROtJPBY语句
5．4．JOIN语句
5．4．1 INNER JOIN
5．4．2 JOIN优化
5．4．3 LEFTOUTER JOIN
5．4．4 R1GHTOUTER JOIN
5．4．5 FULLOUTER JOIN
5．4．6 LEFT SEMI JOIN
5．4．7 笛卡尔积JOIN
5．4．8 mad-side JOIN
5．5 ORDER BY和SOPT BY
5．6 含有SOftT BY的DISTRIBIJTE BY
5．7 CLUSTER BY
5．8 类型转换
5．9 抽样查询
5．9．1 数据块抽样
5．9．2 分桶表的输入裁剪
5．1 0LINIONALL
5．1 1本章小结

第6章 Hive配置与应用
6．1 Hive安装与配置
6．2 Hive访问
6．3 Hive基本操作
6．3．1 Hive CLI命令行操作讲解
6．3．2 Hive的数据类型
6．3．3 Hive表的创建
6．3．4 Hive数据导入
6．3．5 Hive数据导出
6．4 Hive数据定义
6．4．1 内部表与外部表的区别
6．4．2 内部表建立
6．4．3 外部表建立
6．4．4 表的分区与桶的建立
6．4．5 删除表与修改表结构
6．4．6 HiveQL简单查询语句
6．4．7 WHERE语句
6．5 Hive高级查询
6．6 本章小结

第7章Hive自定义函数
7．1 LIDF
7．2 UDTF
7．3 UDAF
7．4 Hive函数综合案例
7．4．1 Row-Sequence实现列自增长
7．4．2 列转行和行转列
7．5 本章小结

第8章Hive综合案例（一）
8．1 项目背景与数据情况
8．2 关键指标KPI
8．3 开发步骤分析
8．4 表结构设计
8．5 数据清洗过程
8．5．1 定期上传日志至HDFS
8．5．2 编写．MapReduce程序清理日志
8．5．3 定期清理日志至HDFS
8．5．4 查询清洗前后的数据
8．6 数据统计分析
8．6．1 借助Hive进行统计
8．6．2 使用HiveQL统计关键指标
8．7 本章小结

第9章Hive综合案例（二）
9．1 项目应用场景
9．2 设计与实现
9．2．1 日志格式分析
9．2．2 建立表
9．2．3 程序设计
9．2．4 编码实现
9．2．5 运行并测试
9．3 本章小结

第10章Hive综合案例（三）
10．1 应用场景
10．2 设计与实现
10．2．1 数据处理
10．2．2 使用Hive对清洗后的数据进行多维分析
10．2．3 在MySQL中建立数据库
10．2．4 使用sqoop把分析结果导入到MySQL中
10．2．5 程序设计与实现
10．2．6 运行并测试


第1章 数据仓库简介

1.1 什么是数据仓库 1

1.1.1 数据仓库的定义 1

1.1.2 建立数据仓库的原因 3

1.2 操作型系统与分析型系统 5

1.2.1 操作型系统 5

1.2.2 分析型系统 8

1.2.3 操作型系统和分析型系统对比 9

1.3 数据仓库架构 10

1.3.1 基本架构 10

1.3.2 主要数据仓库架构 12

1.3.3 操作数据存储 16

1.4 抽取-转换-装载 17

1.4.1 数据抽取 17

1.4.2 数据转换 19

1.4.3 数据装载 20

1.4.4 开发ETL系统的方法 21

1.4.5 常见ETL工具 21

1.5 数据仓库需求 22

1.5.1 基本需求 22

1.5.2 数据需求 23

 1.6 小结 24

第2章 数据仓库设计基础

2.1 关系数据模型 25

2.1.1 关系数据模型中的结构 25

2.1.2 关系完整性 28

2.1.3 规范化 30

2.1.4 关系数据模型与数据仓库 33

2.2 维度数据模型 34

2.2.1 维度数据模型建模过程 35

2.2.2 维度规范化 36

2.2.3 维度数据模型的特点 37

2.2.4 星型模式 38

2.2.5 雪花模式 40

2.3 Data Vault模型 42

2.3.1 Data Vault模型简介 42

2.3.2 Data Vault模型的组成部分 43

2.3.3 Data Vault模型的特点 44

2.3.4 Data Vault模型的构建 44

2.3.5 Data Vault模型实例 46

2.4 数据集市 49

2.4.1 数据集市的概念 50

2.4.2 数据集市与数据仓库的区别 50

2.4.3 数据集市设计 50

2.5 数据仓库实施步骤 51

2.6 小结 54

第3章 Hadoop生态圈与数据仓库

3.1 大数据定义 55

3.2 Hadoop简介 56

3.2.1 Hadoop的构成 57

3.2.2 Hadoop的主要特点 58

3.2.3 Hadoop架构 58

3.3 Hadoop基本组件 59

3.3.1 HDFS 60

3.3.2 MapReduce 65

3.3.3 YARN 72

3.4 Hadoop生态圈的其他组件 77

3.5 Hadoop与数据仓库 81

3.5.1 关系数据库的可扩展性瓶颈 82

3.5.2 CAP理论 84

3.5.3 Hadoop数据仓库工具 85

3.6 小结 88

第4章 安装Hadoop

4.1 Hadoop主要发行版本 89

4.1.1 Cloudera Distribution for Hadoop（CDH） 89

4.1.2 Hortonworks Data Platform（HDP） 90

4.1.3 MapR Hadoop 90

4.2 安装Apache Hadoop 91

4.2.1 安装环境 91

4.2.2 安装前准备 92

4.2.3 安装配置Hadoop 93

4.2.4 安装后配置 97

4.2.5 初始化及运行 97

4.3 配置HDFS Federation 99

4.4 离线安装CDH及其所需的服务 104

4.4.1 CDH安装概述 104

4.4.2 安装环境 106

4.4.3 安装配置 106

4.4.4 Cloudera Manager许可证管理 114

4.5 小结 115.........



Hive编程指南

　　《Hive编程指南》是一本Apache Hive的编程指南，旨在介绍如何使用Hive的SQL方法HiveQL来汇总、查询和分析存储在Hadoop分布式文件系统上的大数据集合。全书通过大量的实例，首先介绍如何在用户环境下安装和配置Hive，并对Hadoop和MapReduce进行详尽阐述，*终演示Hive如何在Hadoop生态系统进行工作。

　　《Hive编程指南》适合对大数据感兴趣的爱好者以及正在使用Hadoop系统的数据库管理员阅读使用。


第1章　基础知识　
1.1　Hadoop和MapReduce综述　
1.2　Hadoop生态系统中的Hive　
1.2.1　Pig　
1.2.2　HBase　
1.2.3　Cascading、Crunch及其他　
1.3　Java和Hive：词频统计算法　
1.4　后续事情　

第2章　基础操作　
2.1　安装预先配置好的虚拟机　
2.2　安装详细步骤　
2.2.1　装Java　
2.2.2　安装Hadoop　
2.2.3　本地模式、伪分布式模式和分布式模式　
2.2.4　测试Hadoop　
2.2.5　安装Hive　
2.3　Hive内部是什么　
2.4　启动Hive　
2.5　配置Hadoop环境　
2.5.1　本地模式配置　
2.5.2　分布式模式和伪分布式模式配置　
2.5.3　使用JDBC连接元数据　
2.6　Hive命令　
2.7　命令行界面　
2.7.1　CLI 选项　
2.7.2　变量和属性　
2.7.3　Hive中“一次使用”命令　
2.7.4　从文件中执行Hive查询　
2.7.5　hiverc文件　
2.7.6　使用Hive CLI的更多介绍　
2.7.7　查看操作命令历史　
2.7.8　执行shell命令　
2.7.9　在Hive内使用Hadoop的dfs命令　
2.7.10　Hive脚本中如何进行注释　
2.7.11　显示字段名称　

第3章　数据类型和文件格式　
3.1　基本数据类型　
3.2　集合数据类型　
3.3　文本文件数据编码　
3.4　读时模式　

第4章　HiveQL：数据定义　
4.1　Hive中的数据库　
4.2　修改数据库　
4.3　创建表　
4.3.1　管理表　
4.3.2　外部表　
4.4　分区表、管理表　
4.4.1　外部分区表　
4.4.2　自定义表的存储格式　
4.5　删除表　
4.6　修改表　
4.6.1　表重命名　
4.6.2　增加、修改和删除表分区　
4.6.3　修改列信息　
4.6.4　增加列　
4.6.5　删除或者替换列　
4.6.6　修改表属性　
4.6.7　修改存储属性　
4.6.8　众多的修改表语句　

第5章　HiveQL：数据操作　
5.1　向管理表中装载数据　
5.2　通过查询语句向表中插入数据　
5.3　单个查询语句中创建表并加载数据　
5.4　导出数据　

第6章　HiveQL：查询　
6.1　SELECT…FROM语句　
6.1.1　使用正则表达式来指定列　
6.1.2　使用列值进行计算　
6.1.3　算术运算符　
6.1.4　使用函数　
6.1.5　LIMIT语句　
6.1.6　列别名　
6.1.7　嵌套SELECT语句　
6.1.8　CASE…WHEN…THEN 句式　
6.1.9　什么情况下Hive可以避免进行MapReduce　
6.2　WHERE语句　
6.2.1　谓词操作符　
6.2.2　关于浮点数比较　
6.2.3　LIKE和RLIKE　
6.3　GROUP BY 语句　
6.4　JOIN语句　
6.4.1　INNER JOIN　
6.4.2　JOIN优化　
6.4.3　LEFT OUTER JOIN　
6.4.4　OUTER JOIN　
6.4.5　RIGHT OUTER JOIN　
6.4.6　FULL OUTER JOIN　
6.4.7　LEFT SEMI-JOIN　
6.4.8　笛卡尔积JOIN　
6.4.9　map-side JOIN　
6.5　ORDER BY和SORT BY　
6.6　含有SORT BY 的DISTRIBUTE BY　
6.7　CLUSTER BY　
6.8　类型转换　
6.9　抽样查询　
6.9.1　数据块抽样　
6.9.2　分桶表的输入裁剪　
6.10　UNION ALL　..........

第 1章 为Hive打好基础：Hadoop 1
1．1　一只小象出生了　2
1．2　Hadoop的结构　3
1．3　数据冗余　6
1．3．1　传统的高可用性　6
1．3．2　Hadoop的高可用性　9
1．4　MapReduce处理　12
1．4．1　超越MapReduce　16
1．4．2　YARN和现代数据架构　17
1．4．3　Hadoop 和开源社区　19
1．4．4　我们身在何处　22
第　2 章 Hive 简介　24
2．1　Hadoop 发行版　25
2．2　集群架构　27
2．3　Hive 的安装　30
2．4　探寻你的方式　32
2．5　Hive CLI　35
第3　章 Hive架构　37
3．1　Hive组件　37
3．2　HCatalog　38
3．3　HiveServer2　40
3．4　客户端工具　42
3．5　执行引擎：Tez　46
第4　章 Hive表DDL　48
4．1　schema-on-read　48
4．2　Hive数据模型　49
4．2．1　模式/数据库　49
4．2．2　为什么使用多个模式/数据库　49
4．2．3　创建数据库　49
4．2．4　更改数据库　50
4．2．5　删除数据库　50
4．2．6　列出数据库　51
4．3　Hive中的数据类型　51
4．3．1　基本数据类型　51
4．3．2　选择数据类型　51
4．3．3　复杂数据类型　52
4．4　表　53
4．4．1　创建表　53
4．4．2　列出表　54
4．4．3　内部表/外部表　54
4．4．4　内部表/受控表　55
4．4．5　内部表/外部表示例　55
4．4．6　表的属性　59
4．4．7　生成已有表的CREATE TABLE命令　60
4．4．8　分区和分桶　61
4．4．9　分区注意事项　63
4．4．10　对日期列进行高效分区　63
4．4．11　分桶的注意事项　65
4．4．12　更改表　66
4．4．13　ORC文件格式　67
4．4．14　更改表分区　68
4．4．15　修改列　72
4．4．16　删除表/分区　72
4．4．17　保护表/分区　73
4．4．18　其他CREATE TABLE命令选项　73
第5　章 数据操作语言　75
5．1　将数据装载到表中　75
5．1．1　使用存储在HDFS中的文件装载数据　75
5．1．2　使用查询装载数据　77
5．1．3　将查询到的数据写入文件系统　80
5．1．4　直接向表插入值　81
5．1．5　直接更新表中数据　83
5．1．6　在表中直接删除数据　84
5．1．7　创建结构相同的表　85
5．2　连接　86
5．2．1　使用等值连接来整合表　86
5．2．2　使用外连接　87
5．2．3　使用左半连接　89
5．2．4　用单次MapReduce实现连接　90
5．2．5　最后使用最大的表　91
5．2．6　事务处理　92
5．2．7　ACID是什么，以及为什么要用到它　92
5．2．8　Hive配置　92
第6章　将数据装载到Hive　94
6．1　装载数据之前的设计注意事项　94
6．2　将数据装载到HDFS　95
6．2．1　Ambari 文件视图　95
6．2．2　Hadoop命令行　97
6．2．3　HDFS的NFS Gateway　97
6．2．4　Sqoop　98
6．2．5　Apache NiFi　101
6．3　用Hive 访问数据　105
6．3．1　外部表　105
6．3．2　LOAD DATA语句　106
6．4　在Hive中装载增量变更数据　107
6．5　Hive流处理　107
6．6　小结　108
第7章　查询半结构化数据　109
7．1　点击流数据　111
7．1．1　摄取数据　113
7．1．2　创建模式　116
7．1．3　装载数据　116
7．1．4　查询数据　116
7．2　摄取JSON数据　119
7．2．1　使用UDF查询JSON　121
7．2．2　使用SerDe访问JSON　122
第8章　Hive分析　125
8．1　构建分析模型　125
8．1．1　使用太阳模型获取需求　125
8．1．2　将太阳模型转换为星型模式　129
8．1．3　构建数据仓库　137
8．2　评估分析模型 ．　140
8．2．1　评估太阳模型　140
8．2．2　评估聚合结果　142
8．2．3　评估数据集市　143
8．3　掌握数据仓库管理　144
8．3．1　必备条件　144
8．3．2　检索数据库　144
8．3．3　评估数据库　147
8．3．4　过程数据库　160
8．3．5　转换数据库　185
8．3．6　你掌握了什么　192
8．3．7　组织数据库　192
8．3．8　报表数据库　196
8．3．9　示例报表　197
8．4　高级分析　199
8．5　接下来学什么　199
第9章　Hive性能调优　200
9．1　Hive性能检查表　200
9．2　执行引擎　201
9．2．1　MapReduce　201
9．2．2　Tez　201
9．3　存储格式　203
9．3．1　ORC格式　203
9．3．2　Parquet格式　205
9．4　矢量化查询执行　206
9．5　查询执行计划　206
9．5．1　基于代价的优化　208
9．5．2　执行计划　210
9．5．3　性能检查表小结　212
第　10章 Hive的安全性　213
10．1　数据安全性的几个方面　213
10．1．1　身份认证　214
10．1．2　授权　214
10．1．3　管理　214
10．1．4　审计　214
10．1．5　数据保护　214
10．2　Hadoop的安全性　215
10．3　Hive的安全性　215
10．3．1　默认授权模式　215
10．3．2　基于存储的授权模式　216
10．3．3　基于SQL标准的授权模式　217
10．3．4　管理通过SQL进行的访问　218
10．4　使用Ranger进行Hive授权　219
10．4．1　访问Ranger用户界面　220
10．4．2　创建Ranger策略　220
10．4．3　使用Ranger审计　222
第　11章 Hive的未来　224
11．1　LLAP　224
11．2　Hive-on-Spark　225
11．3　Hive：ACID 和MERGE　225
11．4　可调隔离等级　225
11．5　ROLAP/基于立方体的分析　226
11．6　HiveServer2的发展　226
11．7　面向不同工作负载的多个HiveServer2实例　226
附录A　建立大数据团队　227
附录B　Hive函数　231

第1章 基础知识
1.1 Hadoop和MapReduce综述
1.2 Hadoop生态系统中的Hive
1.2.1 Pig
1.2.2 HBase
1.2.3 Cascading、Crunch及其他
1.3 Java和Hive：词频统计算法
1.4 后续事情

第2章 基础操作
2.1 安装预先配置好的虚拟机
2.2 安装详细步骤
2.2.1 装Java
2.2.2 安装Hadoop
2.2.3 本地模式、伪分布式模式和分布式模式
2.2.4 测试Hadoop
2.2.5 安装Hive
2.3 Hive内部是什么
2.4 启动Hive
2.5 配置Hadoop环境
2.5.1 本地模式配置
2.5.2 分布式模式和伪分布式模式配置
2.5.3 使用JDBC连接元数据
2.6 Hive命令
2.7 命令行界面
2.7.1 CLI 选项
2.7.2 变量和属性
2.7.3 Hive中“一次使用”命令
2.7.4 从文件中执行Hive查询
2.7.5 hiverc文件
2.7.6 使用Hive CLI的更多介绍
2.7.7 查看操作命令历史
2.7.8 执行shell命令
2.7.9 在Hive内使用Hadoop的dfs命令
2.7.10 Hive脚本中如何进行注释
2.7.11 显示字段名称

第3章 数据类型和文件格式
3.1 基本数据类型
3.2 集合数据类型
3.3 文本文件数据编码
3.4 读时模式

第4章 HiveQL：数据定义
4.1 Hive中的数据库
4.2 修改数据库
4.3 创建表
4.3.1 管理表
4.3.2 外部表
4.4 分区表、管理表
4.4.1 外部分区表
4.4.2 自定义表的存储格式
4.5 删除表
4.6 修改表
4.6.1 表重命名
4.6.2 增加、修改和删除表分区
4.6.3 修改列信息
4.6.4 增加列
4.6.5 删除或者替换列
4.6.6 修改表属性
4.6.7 修改存储属性
4.6.8 众多的修改表语句

第5章 HiveQL：数据操作
5.1 向管理表中装载数据
5.2 通过查询语句向表中插入数据
5.3 单个查询语句中创建表并加载数据
5.4 导出数据

第6章 HiveQL：查询
6.1 SELECT…FROM语句
6.1.1 使用正则表达式来指定列
6.1.2 使用列值进行计算
6.1.3 算术运算符
6.1.4 使用函数
6.1.5 LIMIT语句
6.1.6 列别名
6.1.7 嵌套SELECT语句
6.1.8 CASE…WHEN…THEN 句式
6.1.9 什么情况下Hive可以避免进行MapReduce
6.2 WHERE语句
6.2.1 谓词操作符
6.2.2 关于浮点数比较
6.2.3 LIKE和RLIKE
6.3 GROUP BY 语句
6.4 JOIN语句
6.4.1 INNER JOIN
6.4.2 JOIN优化
6.4.3 LEFT OUTER JOIN
6.4.4 OUTER JOIN
6.4.5 RIGHT OUTER JOIN
6.4.6 FULL OUTER JOIN
6.4.7 LEFT SEMI-JOIN
6.4.8 笛卡尔积JOIN
6.4.9 map-side JOIN
6.5 ORDER BY和SORT BY
6.6 含有SORT BY 的DISTRIBUTE BY
6.7 CLUSTER BY
6.8 类型转换
6.9 抽样查询
6.9.1 数据块抽样
6.9.2 分桶表的输入裁剪
6.10 UNION ALL

第7章 HiveQL：视图
7.1 使用视图来降低查询复杂度
7.2 使用视图来限制基于条件过滤的数据
7.3 动态分区中的视图和map类型
7.4 视图零零碎碎相关的事情

第8章 HiveQL：索引
8.1 创建索引
8.2 重建索引
8.3 显示索引
8.4 删除索引
8.5 实现一个定制化的索引处理器

第9章 模式设计
9.1 按天划分的表
9.2 关于分区
9.3 唯一键和标准化
9.4 同一份数据多种处理
9.5 对于每个表的分区
9.6 分桶表数据存储
9.7 为表增加列
9.8 使用列存储表
9.8.1 重复数据
9.8.2 多列
9.9 （几乎）总是使用压缩

第10章 调优
10.1 使用EXPLAIN
10.2 EXPLAIN EXTENDED
10.3 限制调整
10.4 JOIN优化
10.5 本地模式
10.6 并行执行
10.7 严格模式
10.8 调整mapper和reducer个数
10.9 JVM重用
10.10 索引
10.11 动态分区调整
10.12 推测执行
10.13 单个MapReduce中多个GROUP BY
10.14 虚拟列

第11章 其他文件格式和压缩方法
11.1 确定安装编解码器
11.2 选择一种压缩编/解码器
11.3 开启中间压缩
11.4 最终输出结果压缩
11.5 sequence file存储格式
11.6 使用压缩实践
11.7 存档分区
11.8 压缩：包扎

第12章 开发
12.1 修改Log4J属性
12.2 连接Java调试器到Hive
12.3 从源码编译Hive
12.3.1 执行Hive测试用例
12.3.2 执行hook
12.4 配置Hive和Eclipse
12.5 Maven工程中使用Hive
12.6 Hive中使用hive_test进行单元测试
12.7 新增的插件开发工具箱（PDK）

第13章 函数
13.1 发现和描述函数
13.2 调用函数
13.3 标准函数
13.4 聚合函数
13.5 表生成函数
13.6 一个通过日期计算其星座的UDF
13.7 UDF与GenericUDF
13.8 不变函数
13.9 用户自定义聚合函数
13.10 用户自定义表生成函数
13.10.1 可以产生多行数据的UDTF
13.10.2 可以产生具有多个字段的单行数据的UDTF
13.10.3 可以模拟复杂数据类型的UDTF
13.11 在 UDF中访问分布式缓存
13.12 以函数的方式使用注解
13.12.1 定数性（deterministic）标注
13.12.2 状态性（stateful）标注
13.12.3 唯一性
13.13 宏命令

第14章 Streaming
14.1 恒等变换
14.2 改变类型
14.3 投影变换
14.4 操作转换
14.5 使用分布式内存
14.6 由一行产生多行
14.7 使用streaming进行聚合计算
14.8 CLUSTER BY、DISTRIBUTE BY、SORT BY
14.9 GenericMR Tools for Streaming to Java
14.10 计算cogroup

第15章 自定义Hive文件和记录格式
15.1 文件和记录格式
15.2 阐明CREATE TABLE句式
15.3 文件格式
15.3.1 SequenceFile
15.3.2 RCfile
15.3.3 示例自定义输入格式：DualInputFormat
15.4 记录格式：SerDe
15.5 CSV和TSV SerDe
15.6 ObjectInspector
15.7 Thing Big Hive Reflection ObjectInspector
15.8 XML UDF
15.9 XPath相关的函数
15.10 JSON SerDe
15.11 Avro Hive SerDe
15.11.1 使用表属性信息定义Avro Schema
15.11.2 从指定URL中定义Schema
15.11.3 进化的模式
15.12 二进制输出

第16章 Hive的Thrift服务
16.1 启动Thrift Server
16.2 配置Groovy使用HiveServer
16.3 连接到HiveServer
16.4 获取集群状态信息
16.5 结果集模式
16.6 获取结果
16.7 获取执行计划
16.8 元数据存储方法
16.9 管理HiveServer
16.9.1 生产环境使用HiveServer
16.9.2 清理
16.10 Hive ThriftMetastore
16.10.1 ThriftMetastore 配置
16.10.2 客户端配置

第17章 存储处理程序和NoSQL
17.1 Storage Handler Background
17.2 HiveStorageHandler
17.3 HBase
17.4 Cassandra
17.4.1 静态列映射（Static Column Mapping）
17.4.2 为动态列转置列映射
17.4.3 Cassandra SerDe Properties
17.5 DynamoDB

第18章 安全
18.1 和Hadoop安全功能相结合
18.2 使用Hive进行验证
18.3 Hive中的权限管理
18.3.1 用户、组和角色
18.3.2 Grant 和 Revoke权限
18.4 分区级别的权限
18.5 自动授权

第19章 锁
19.1 Hive结合Zookeeper支持锁功能
19.2 显式锁和独占锁

第20章 Hive和Oozie整合
20.1 Oozie提供的多种动作（Action）
20.2 一个只包含两个查询过程的工作流示例
20.3 Oozie 网页控制台
20.4 工作流中的变量
20.5 获取输出
20.6 获取输出到变量

第21章 Hive和亚马逊网络服务系统（AWS）
21.1 为什么要弹性MapReduce
21.2 实例
21.3 开始前的注意事项
21.4 管理自有EMR Hive集群
21.5 EMR Hive上的Thrift Server服务
21.6 EMR上的实例组
21.7 配置EMR集群
21.7.1 部署hive-site.xml文件
21.7.2 部署.hiverc脚本
21.7.3 建立一个内存密集型配置
21.8 EMR上的持久层和元数据存储
21.9 EMR集群上的HDFS和S
21.10 在S3上部署资源、配置和辅助程序脚本
21.11 S3上的日志
21.12 现买现卖
21.13 安全组
21.14 EMR和EC2以及Apache Hive的比较
21.15 包装

第22章 HCatalog
22.1 介绍
22.2 MapReduce
22.2.1 读数据
22.2.2 写数据
22.3 命令行
22.4 安全模型
22.5 架构

第23章 案例研究
23.1 m6d.com（Media6Degrees）
23.1.1 M 6D的数据科学，使用Hive和R
23.1.2 M6D UDF伪随机
23.1.3 M6D如何管理多MapReduce集群间的Hive数据访问
23.2 Outbrain
23.2.1 站内线上身份识别
23.2.2 计算复杂度
23.2.3 会话化
23.3 NASA喷气推进实验室
23.3.1 区域气候模型评价系统
23.3.2 我们的经验：为什么使用Hive
23.3.3 解决这些问题我们所面临的挑战
23.4 Photobucket
23.4.1 Photobucket 公司的大数据应用情况
23.4.2 Hive所使用的硬件资源信息
23.4.3 Hive提供了什么
23.4.4 Hive支持的用户有哪些
23.5 SimpleReach
23.6 Experiences and Needs from the Customer Trenches
23.6.1 介绍
23.6.2 Customer Trenches的用例
术语词汇表

Hadoop 指南-目录

第Ⅰ部分Hadoop基础知识
dy 章初识Hadoop3
1.1数据！数据！3
1.2数据的存储与分析5
1.3查询所有数据6
1.4不仅仅是批处理7
1.5相较于其他系统的优势8
1.6ApacheHadoop发展简史12
1.7本书包含的内容16
第2章关于MapReduce19
2.1气象数据集19
2.2使用Unix工具来分析数据21
2.3使用Hadoop来分析数据22
2.4横向扩展31
2.5HadoopStreaming37
第3章Hadoop分布式文件系统42
3.1HDFS的设计42
3.2HDFS的概念44
3.3命令行接口50
3.4Hadoop文件系统52
3.5Java接口56
3.6数据流68
3.7通过distcp并行复制76
第4章关于YARN78
4.1剖析YARN应用运行机制79
4.2YARN与MapReduce1相比82
4.3YARN中的调度85
4.4延伸阅读95
第5章Hadoop的I/O操作96
5.1数据完整性96
5.2压缩99
5.3序列化109
5.4基于文件的数据结构127
第Ⅱ部分关于MapReduce
第6章MapReduce应用开发141
6.1用于配置的API142
6.2配置开发环境144
6.3用MRUnit来写单元测试152
6.4本地运行测试数据156
6.5在集群上运行160
6.6作业调优174
6.7MapReduce的工作流176
第7章MapReduce的工作机制184
7.1剖析MapReduce作业运行
机制184
7.2失败191
7.3shuffle和排序195
7.4任务的执行201
第8章MapReduce的
类型与格式207
8.1MapReduce的类型207
8.2输入格式218
8.3输出格式236
第9章MapReduce的特性243
9.1计数器243
9.2排序252
9.3连接264
9.4边数据分布270
9.5MapReduce库类276
第Ⅲ部分Hadoop的操作
dy 0章构建Hadoop集群279
…………已省略更多目录

Hive编程指南-目录

dy 章 基础知识
1.1 Hadoop和MapReduce综述
1.2 Hadoop生态系统中的Hive
1.2.1 Pig
1.2.2 HBase
1.2.3 Cascading、Crunch及其他
1.3 Java和Hive：词频统计算法
1.4 后续事情
第2章 基础操作
 2.1 安装预先配置好的虚拟机
 2.2 安装详细步骤
 2.2.1 装Java
 2.2.2 安装Hadoop
 2.2.3 本地模式、伪分布式模式和分布式模式
 2.2.4 测试Hadoop
 2.2.5 安装Hive
 2.3 Hive内部是什么
 2.4 启动Hive
 2.5 配置Hadoop环境
 2.5.1 本地模式配置
 2.5.2 分布式模式和伪分布式模式配置
 2.5.3 使用JDBC连接元数据
 2.6 Hive命令
 2.7 命令行界面
 2.7.1 CLI 选项
 2.7.2 变量和属性
 2.7.3 Hive中“一次使用”命令
 2.7.4 从文件中执行Hive查询
 2.7.5 hiverc文件
 2.7.6 使用Hive CLI的更多介绍
 2.7.7 查看操作命令历史
 2.7.8 执行shell命令
 2.7.9 在Hive内使用Hadoop的dfs命令
 2.7.10 Hive脚本中如何进行注释
     2.7.11 显示字段名称

第3章 数据类型和文件格式
 3.1 基本数据类型
 3.2 集合数据类型
 3.3 文本文件数据编码
 3.4 读时模式

第4章 HiveQL：数据定义
 4.1 Hive中的数据库
 4.2 修改数据库
 4.3 创建表
 4.3.1 管理表
 4.3.2 外部表
 4.4 分区表、管理表
 4.4.1 外部分区表
 4.4.2 自定义表的存储格式
 4.5 删除表
 4.6 修改表
 4.6.1 表重命名
 4.6.2 增加、修改和删除表分区
 4.6.3 修改列信息
 4.6.4 增加列
 4.6.5 删除或者替换列
 4.6.6 修改表属性
 4.6.7 修改存储属性
 4.6.8 众多的修改表语句

 ====================================================================================================================================

 https://blog.csdn.net/doveyoung8/article/details/80014442

 hive UDTF函数

 之前说过HIVE，UDF(User-Defined-Function)函数的编写和使用，现在来看看UDTF的编写和使用。

 1. UDTF介绍

 UDTF(User-Defined Table-Generating Functions) 用来解决 输入一行输出多行(On-to-many maping) 的需求。

 2. 编写自己需要的UDTF

 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF,实现initialize, process, close三个方法。

 UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）。

 初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward()调用产生一行；如果产生多列可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数。

 最后close()方法调用，对需要清理的方法进行清理。


 下面是我写的一个用来切分”key:value;key:value;”这种字符串，返回结果为key, value两个字段。供参考：


 import java.util.ArrayList;

 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

 public class UDTFExplode extends GenericUDTF {

     @Override
     public void close() throws HiveException {
         // TODO Auto-generated method stub

     }

     @Override
     public void process(Object[] args) throws HiveException {
         // TODO Auto-generated method stub
         String input = args[0].toString();
         String[] test = input.split(";");
         for (int i = 0; i < test.length; i++) {
             try {
                 String[] result = test[i].split(":");
                 forward(result);
             } catch (Exception e) {
                 continue;
             }
         }

     }

     @Override
     public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException {
         if (args.length != 1) {
             throw new UDFArgumentLengthException("ExplodeMap takes only one argument");
         }
         if (args[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
             throw new UDFArgumentException("ExplodeMap takes string as a parameter");
         }

         ArrayList<String> fieldNames = new ArrayList<String>();
         ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
         fieldNames.add("col1");
         fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
         fieldNames.add("col2");
         fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

         return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
     }

 }
 复制代码
 3.使用方法

 将程序打成JAR包，然后上传服务器。添加UDF函数：



 UDTF有两种使用方法，一种直接放到select后面，一种和lateral view一起使用。

 1：直接select中使用



 select split_test('asd:123\;rtrt:3445\;vbvx:6787') as (col1,col2) from finance.dual;
 需要注意的是UDTF不可以添加其他字段使用，不可以嵌套调用，不可以和group by/cluster by/distribute by/sort by一起使用

 2：和lateral view一起使用



 select '1', mytable.col1, mytable.col2 from finance.dual lateral view split_test('asd:123\;rtrt:3445\;vbvx:6787') as (col1,col2) mytable as col1, col2;
 执行过程相当于单独执行了两次抽取，然后union到一个表里。



 在大规模数据量的数据分析及建模任务中，往往针对全量数据进行挖掘分析时会十分耗时和占用集群资源，因此一般情况下只需要抽取一小部分数据进行分析及建模操作。Hive提供了数据取样（SAMPLING）的功能，能够根据一定的规则进行数据抽样，目前支持数据块抽样，分桶抽样和随机抽样，具体如下所示：

 数据块抽样（tablesample()函数）
 1） tablesample(n percent) 根据hive表数据的大小按比例抽取数据，并保存到新的hive表中。如：抽取原hive表中10%的数据
 （注意：测试过程中发现，select语句不能带where条件且不支持子查询，可通过新建中间表或使用随机抽样解决）
 create table xxx_new as select * from xxx tablesample(10 percent)
 2）tablesample(n M) 指定抽样数据的大小，单位为M。
 3）tablesample(n rows) 指定抽样数据的行数，
 其中n代表每个map任务均取n行数据，
 map数量可通过hive表的简单查询语句确认（关键词：number of mappers: x)

 分桶抽样
 hive中分桶其实就是根据某一个字段Hash取模，放入指定数据的桶中，比如将表table_1按照ID分成100个桶，其算法是hash(id) % 100，这样，hash(id) % 100 = 0的数据被放到第一个桶中，hash(id) % 100 = 1的记录被放到第二个桶中。创建分桶表的关键语句为：CLUSTER BY语句。

 分桶抽样语法：
 TABLESAMPLE (BUCKET x OUT OF y [ON colname])
 其中x是要抽样的桶编号，桶编号从1开始，colname表示抽样的列，y表示桶的数量。
 例如：将表随机分成10组，抽取其中的第一个桶的数据
 select * from table_01 tablesample(bucket 1 out of 10 on rand())

 随机抽样（rand()函数）
 1）使用rand()函数进行随机抽样，limit关键字限制抽样返回的数据，其中rand函数前的distribute和sort关键字可以保证数据在mapper和reducer阶段是随机分布的，案例如下：
 select * from table_name where col=xxx distribute by rand() sort by rand() limit num;
 2）使用order 关键词
 案例如下：
 select * from table_name where col=xxx order by rand() limit num;
 经测试对比，千万级数据中进行随机抽样 order by方式耗时更长，大约多30秒左右。



 rand() 函数

 生成一个0-1之间的随机数，可设定随机种子。

 利用这个函数在hive 中进行随机抽样。

 test1  简单随机抽样

 SELECT t.varx,t.a
 FROM(
     SELECT varx,rand() a
     FROM tablename)t
 WHERE t.a BETWEEN 0 AND 0.2
 这样就抽取了五分之一的数据。

 --或者像这样随机抽取100条数据，与limit结合使用

 SELECT distinct a.*
 FROM table a
 ORDER BY rand(222)
 limit 100
 test2  数据块取样（Block Sampling）

 关键字 TABLESAMPLE

 SELECT * FROM table1 TABLESAMPLE (30M)

 SELECT * FROM table1 TABLESAMPLE (15 PERCENT)

 SELECT COUNT(1) FROM (SELECT * FROM lxw1 TABLESAMPLE (200 ROWS)) x --不懂
 SELECT COUTN(2) FROM table1 TABLESAMPLE (BUCKET 1 OUT OF 20 ON RAND()) -- 分桶20抽取第2桶
 test3  系统抽样

 mod,rand() 依照userrid取模，分5组，每组随机抽取100个用户，实现如：

 select *
   from(
       select refund_id,user_id,mod,rank_num from
       (select refund_id,user_id,cast(10+rand()*100 as double) rank_num,
         user_id%5 as mod --依据user_id，取模，获取 mod
         from table1)
       distribute by mod sort by mod,rank_num desc  --根据mod分组，并排序
       ) a
 where row_number(mod)<=20; --从每个mod里面抽取20个
 test4 分层抽样


=======================
Hive源码系列（七）编译模块之词法、语法解析 （中）
这篇主要举实际案例说明怎么使用antlr工具、利用antlr生成的Lexer、Parser、TreeParser代码，获取asttree。这些都是hive获取asttree的过程，理解了这些，再理解hive的asttree就很容易了
程序设计语言入门小案例一般都用“Hello World”，在编译领域的入门往往选择计算器。而我们这次的小案例就更简单：一个只能计算【两】个【整数】相【加】的计算器，比如：计算1+1...
先来考虑一下如果何下手，在我们的计算器中，只接受输入整数和加号，其它的一概不理。这里说的是整数，如果输入了一个字母，我们一定是要拒绝的...对于这一块要写对应的词法规则，这个阶段的过程就叫做词法分析
输入满足词法的规则，并不代表我们就能接受，如果是【加号】【整数】【整数】或者【整数】【整数】【加号】这样的排列，我们是不能接受的，这里接受的合法语法是【整数】【加号】【整数】，因此我们需要在词法规则的基础上再定义语法规则，规则定输入满足这样句式的才算是合法... 我们把这个阶段叫做语法分析
弄清楚了我们的词法、语法规则后，我们需要以antlr的语言把这些写出来。
antlr语法博大精深，我觉得没必要钻的太深，实现这个案例，用不到antlr太多复杂的语法，只用看懂就行

1、使用antlrworks

双击antlrworks-1.5.1.jar启动antlrworks

File-->New







选择ANTLR 3 Grammer  (*.g)  新建Calculator语法文件







输入规则：







ctrl+s 保存







Antlr的语法文件通常会保存在一个 .g的文件中，我们的语法文件叫做 Caculator.g，保存在E:\hive\anltr\calculator 目录 下

在E:\hive\anltr\calculator目录 创建output目录

File-->Preferences设置Output path







运行调试，点击图标中的小甲虫







在弹出来的调试界面中，选择 text

输入 1+2













之后将会在output窗口看到被识别出来的token流，以及具体语法分析树和ASTTree的结果











到此，就是简单使用anltrworks用语法文件来解析输入数据的过程



2、使用eclipse



新建一个java项目，antlr-my

File-->New-->Java Project







在antlr-my项目下创建grammar、lib







下载antlr-3.4-complete.jar   hive中用的是antlr3.4

http://www.java2s.com/Code/Jar/a/Downloadantlr34completejar.htm

将antlr-3.4-complete.jar放在lib下面







创建语法文件：

在grammar上面右键， New -> Other,选择 ANTLR中的Combined Grammar



输入文件名Caculator





设置antlr-my项目相关的antlr插件检查设置：

在antlr-my项目上右键 -->Properties-->ANTLR













以上设置完了之后，点ok

在Calculator.g文件中输入内容：







点击Interpreter,在expr区域输入表达式1+2 点击执行按钮：







我们可以看到下图中具体语法分析树的结果







3、用java来获取ASTTree



antlr自动生成了词法分析器CalculatorLexer.java和语法分析器 CalculatorParser.java的代码







创建测试类：TestCalculator.java







能够获取生成的ASTTree








微信扫一扫
关注该公众号

====================
dy 章 Spark数据分析导论 1
1．1 Spark是什么 1
1．2 一个大一统的软件栈 2
1．2．1 Spark Core 2
1．2．2 Spark SQL 3
1．2．3 Spark Streaming 3
1．2．4 MLlib 3
1．2．5 GraphX 3
1．2．6 集群管理器 4
1．3 Spark的用户和用途 4
1．3．1 数据科学任务 4
1．3．2 数据处理应用 5
1．4 Spark简史 5
1．5 Spark的版本和发布 6
1．6 Spark的存储层次 6
第2章 Spark下载与入门 7
2．1 下载Spark 7
2．2 Spark中Python和Scala的shell 9
2．3 Spark 核心概念简介 12
2．4 独立应用 14
2．4．1 初始化SparkContext 15
2．4．2 构建独立应用 16
2．5 总结 19
第3章 RDD编程 21
3．1 RDD基础 21
3．2 创建RDD 23
3．3 RDD操作 24
3．3．1 转化操作 24
3．3．2 行动操作 26
3．3．3 惰性求值 27
3．4 向Spark传递函数 27
3．4．1 Python 27
3．4．2 Scala 28
3．4．3 Java 29
3．5 常见的转化操作和行动操作 30
3．5．1 基本RDD 30
3．5．2 在不同RDD类型间转换 37
3．6 持久化( 缓存) 39
3．7 总结 40
第4章 键值对操作 41
4．1 动机 41
4．2 创建Pair RDD 42
4．3 Pair RDD的转化操作 42
4．3．1 聚合操作 45
4．3．2 数据分组 49
4．3．3 连接 50
4．3．4 数据排序 51
4．4 Pair RDD的行动操作 52
4．5 数据分区（进阶） 52
4．5．1 获取RDD的分区方式 55
4．5．2 从分区中获益的操作 56
4．5．3 影响分区方式的操作 57
4．5．4 示例：PageRank 57
4．5．5 自定义分区方式 59
4．6 总结 61
第5章 数据读取与保存 63
5．1 动机 63

-----------

文档中心  弹性 MapReduce  EMR 开发教程  Hue 使用指南  Hive 元数据管理
Hive 元数据管理
最近更新时间：2018-07-05 20:30:31

 编辑   查看pdf
使用 Hue 之前，您首先需要通过 EMR 的快捷入口登录Hue，且 Hue 上的用户名建议是 hadoop。

进入 Hue 的 Hive 编辑模式，并选择对应的数据库

选择数据库

在 Hue 的 Hive SQL 编辑框里面输入 hive-SQL 创建表和导入数据

 drop table hive_test;
create table hive_test (a int, b string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ’,’;
load data local inpath "/usr/local/service/hadoop/bin/hive_test.data" into table hive_t
单击执行按钮如下图：

执行

Hue 可以将查询结果以图表的方式展示，执行这个 SQL

 select * from hive_test limit 10;
执行完成后，结果数据如图所示：

执行结果

选择图例模式，在这里选择了"pie"，可以看到结果图

结果图

==============
文档中心  弹性 MapReduce  EMR 开发教程  Hive 开发指南  Hive 最佳实践
Hive 最佳实践
最近更新时间：2017-11-17 10:42:44

 编辑   查看pdf
执行引擎设置
腾讯云 EMR 中的 Hive 目前支持三种执行引擎：

MR
TEZ
Spark
如果需要 TEZ 那么在初始购买集群的时候需要勾选 TEZ，在普通情况下建议执行引擎为 TEZ，这样您会获得更好的计算效率。

存储选择
腾讯云存储介质目前支持本地数据盘、普通云硬盘、SSD 云硬盘以及 COS 对象存储，如果您对成本敏感，那么基于 COS 的数据仓库方式是一个不错的选择

数据格式
腾讯云压缩支持 snappy、lzo 等压缩算法，如果使用 Hive 建议您的数据文件格式使用 ORC 或者 parquet 的格式，这样您会更节省空间以及会获得更好的计算效率

查询引擎如何选择
腾讯云 EMR 目前支持的查询引擎有 Presto、SparkSQL、Hive，如果您想实现多种数据源耦合查询建议您使用 Presto，如果普通数据仓库建议您使用 Hive+TEZ 的模式，如果您对时延比较敏感可以考虑SparkSQL。

数据安全
如果您是使用 COS 作为底层存储，建议您使用外部表的方式，以免误删数据，而如果是存储在 HDFS 那么建议您开启 HDFS 回收站来避免数据误删除。



==============