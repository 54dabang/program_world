dy 章初识Hadoop3
1.1数据！数据！3
1.2数据的存储与分析5
1.3查询所有数据6
1.4不仅仅是批处理7
1.5相较于其他系统的优势8
1.6ApacheHadoop发展简史12
1.7本书包含的内容16
第2章关于MapReduce19
2.1气象数据集19
2.2使用Unix工具来分析数据21
2.3使用Hadoop来分析数据22
2.4横向扩展31
2.5HadoopStreaming37
第3章Hadoop分布式文件系统42
3.1HDFS的设计42
3.2HDFS的概念44
3.3命令行接口50
3.4Hadoop文件系统52
3.5Java接口56
3.6数据流68
3.7通过distcp并行复制76
第4章关于YARN78
4.1剖析YARN应用运行机制79
4.2YARN与MapReduce1相比82
4.3YARN中的调度85
4.4延伸阅读95
第5章Hadoop的I/O操作96
5.1数据完整性96
5.2压缩99
5.3序列化109
5.4基于文件的数据结构127
第Ⅱ部分关于MapReduce
第6章MapReduce应用开发141
6.1用于配置的API142
6.2配置开发环境144
6.3用MRUnit来写单元测试152
6.4本地运行测试数据156
6.5在集群上运行160
6.6作业调优174
6.7MapReduce的工作流176
第7章MapReduce的工作机制184
7.1剖析MapReduce作业运行
机制184
7.2失败191
7.3shuffle和排序195
7.4任务的执行201
第8章MapReduce的
类型与格式207
8.1MapReduce的类型207
8.2输入格式218
8.3输出格式236
第9章MapReduce的特性243
9.1计数器243
9.2排序252
9.3连接264
9.4边数据分布270
9.5MapReduce库类276
第Ⅲ部分Hadoop的操作
dy 0章构建Hadoop集群279

dy 章 基础知识
1.1 Hadoop和MapReduce综述
1.2 Hadoop生态系统中的Hive
1.2.1 Pig
1.2.2 HBase
1.2.3 Cascading、Crunch及其他
1.3 Java和Hive：词频统计算法
1.4 后续事情
第2章 基础操作
 2.1 安装预先配置好的虚拟机
 2.2 安装详细步骤
 2.2.1 装Java
 2.2.2 安装Hadoop
 2.2.3 本地模式、伪分布式模式和分布式模式
 2.2.4 测试Hadoop
 2.2.5 安装Hive
 2.3 Hive内部是什么
 2.4 启动Hive
 2.5 配置Hadoop环境
 2.5.1 本地模式配置
 2.5.2 分布式模式和伪分布式模式配置
 2.5.3 使用JDBC连接元数据
 2.6 Hive命令
 2.7 命令行界面
 2.7.1 CLI 选项
 2.7.2 变量和属性
 2.7.3 Hive中“一次使用”命令
 2.7.4 从文件中执行Hive查询
 2.7.5 hiverc文件
 2.7.6 使用Hive CLI的更多介绍
 2.7.7 查看操作命令历史
 2.7.8 执行shell命令
 2.7.9 在Hive内使用Hadoop的dfs命令
 2.7.10 Hive脚本中如何进行注释
     2.7.11 显示字段名称

第3章 数据类型和文件格式
 3.1 基本数据类型
 3.2 集合数据类型
 3.3 文本文件数据编码
 3.4 读时模式

第4章 HiveQL：数据定义
 4.1 Hive中的数据库
 4.2 修改数据库
 4.3 创建表
 4.3.1 管理表
 4.3.2 外部表
 4.4 分区表、管理表
 4.4.1 外部分区表
 4.4.2 自定义表的存储格式
 4.5 删除表
 4.6 修改表
 4.6.1 表重命名
 4.6.2 增加、修改和删除表分区
 4.6.3 修改列信息
 4.6.4 增加列
 4.6.5 删除或者替换列
 4.6.6 修改表属性
 4.6.7 修改存储属性
 4.6.8 众多的修改表语句

…………已省略更多目录

Spark快速大数据分析-目录

推荐序 xi
译者序 xiv
序 xvi
前言 xvii
dy 章 Spark数据分析导论 1
1．1 Spark是什么 1
1．2 一个大一统的软件栈 2
1．2．1 Spark Core 2
1．2．2 Spark SQL 3
1．2．3 Spark Streaming 3
1．2．4 MLlib 3
1．2．5 GraphX 3
1．2．6 集群管理器 4
1．3 Spark的用户和用途 4
1．3．1 数据科学任务 4
1．3．2 数据处理应用 5
1．4 Spark简史 5
1．5 Spark的版本和发布 6
1．6 Spark的存储层次 6
第2章 Spark下载与入门 7
2．1 下载Spark 7
2．2 Spark中Python和Scala的shell 9
2．3 Spark 核心概念简介 12
2．4 独立应用 14
2．4．1 初始化SparkContext 15
2．4．2 构建独立应用 16
2．5 总结 19
第3章 RDD编程 21
3．1 RDD基础 21
3．2 创建RDD 23
3．3 RDD操作 24
3．3．1 转化操作 24
3．3．2 行动操作 26
3．3．3 惰性求值 27
3．4 向Spark传递函数 27
3．4．1 Python 27
3．4．2 Scala 28
3．4．3 Java 29
3．5 常见的转化操作和行动操作 30
3．5．1 基本RDD 30
3．5．2 在不同RDD类型间转换 37
3．6 持久化( 缓存) 39
3．7 总结 40
第4章 键值对操作 41
4．1 动机 41
4．2 创建Pair RDD 42
4．3 Pair RDD的转化操作 42
4．3．1 聚合操作 45
4．3．2 数据分组 49
4．3．3 连接 50
4．3．4 数据排序 51
4．4 Pair RDD的行动操作 52
4．5 数据分区（进阶） 52
4．5．1 获取RDD的分区方式 55
4．5．2 从分区中获益的操作 56
4．5．3 影响分区方式的操作 57
4．5．4 示例：PageRank 57
4．5．5 自定义分区方式 59
4．6 总结 61
第5章 数据读取与保存 63
5．1 动机 63
…………已省略更多目录

dy 章 基础知识

1.1 Hadoop和MapReduce综述

1.2 Hadoop生态系统中的Hive

1.2.1 Pig

1.2.2 HBase

1.2.3 Cascading、Crunch及其他

1.3 Java和Hive：词频统计算法

1.4 后续事情



第2章 基础操作

2.1 安装预先配置好的虚拟机

2.2 安装详细步骤

2.2.1 装Java

2.2.2 安装Hadoop

2.2.3 本地模式、伪分布式模式和分布式模式

2.2.4 测试Hadoop

2.2.5 安装Hive

2.3 Hive内部是什么

2.4 启动Hive

2.5 配置Hadoop环境

2.5.1 本地模式配置

2.5.2 分布式模式和伪分布式模式配置

2.5.3 使用JDBC连接元数据

2.6 Hive命令

2.7 命令行界面

2.7.1 CLI 选项

2.7.2 变量和属性

2.7.3 Hive中“一次使用”命令

2.7.4 从文件中执行Hive查询

2.7.5 hiverc文件

2.7.6 使用Hive CLI的更多介绍

2.7.7 查看操作命令历史

2.7.8 执行shell命令

2.7.9 在Hive内使用Hadoop的dfs命令

2.7.10 Hive脚本中如何进行注释

2.7.11 显示字段名称



第3章 数据类型和文件格式

3.1 基本数据类型

3.2 集合数据类型

3.3 文本文件数据编码

3.4 读时模式



第4章 HiveQL：数据定义

4.1 Hive中的数据库

4.2 修改数据库

4.3 创建表

4.3.1 管理表

4.3.2 外部表

4.4 分区表、管理表

4.4.1 外部分区表

4.4.2 自定义表的存储格式

4.5 删除表

4.6 修改表

4.6.1 表重命名

4.6.2 增加、修改和删除表分区

4.6.3 修改列信息

4.6.4 增加列

4.6.5 删除或者替换列

4.6.6 修改表属性

4.6.7 修改存储属性

4.6.8 众多的修改表语句



第5章 HiveQL：数据操作

5.1 向管理表中装载数据

5.2 通过查询语句向表中插入数据

5.3 单个查询语句中创建表并加载数据

5.4 导出数据



第6章 HiveQL：查询

6.1 SELECT…FROM语句

6.1.1 使用正则表达式来指定列

6.1.2 使用列值进行计算

6.1.3 算术运算符

6.1.4 使用函数

6.1.5 LIMIT语句

6.1.6 列别名

6.1.7 嵌套SELECT语句

6.1.8 CASE…WHEN…THEN 句式

6.1.9 什么情况下Hive可以避免进行MapReduce

6.2 WHERE语句

6.2.1 谓词操作符

6.2.2 关于浮点数比较

6.2.3 LIKE和RLIKE

6.3 GROUP BY 语句

6.4 JOIN语句

6.4.1 INNER JOIN

6.4.2 JOIN优化

6.4.3 LEFT OUTER JOIN

6.4.4 OUTER JOIN

6.4.5 RIGHT OUTER JOIN

6.4.6 FULL OUTER JOIN

6.4.7 LEFT SEMI-JOIN

6.4.8 笛卡尔积JOIN

6.4.9 map-side JOIN

6.5 ORDER BY和SORT BY

6.6 含有SORT BY 的DISTRIBUTE BY

6.7 CLUSTER BY

6.8 类型转换

6.9 抽样查询

6.9.1 数据块抽样

6.9.2 分桶表的输入裁剪

6.10 UNION ALL



第7章 HiveQL：视图

7.1 使用视图来降低查询复杂度

7.2 使用视图来限制基于条件过滤的数据

7.3 动态分区中的视图和map类型

7.4 视图零零碎碎相关的事情



第8章 HiveQL：索引

8.1 创建索引

8.2 重建索引

8.3 显示索引

8.4 删除索引

8.5 实现一个定制化的索引处理器



第9章 模式设计

9.1 按天划分的表

9.2 关于分区

9.3 wei一键和标准化

9.4 同一份数据多种处理

9.5 对于每个表的分区

9.6 分桶表数据存储

9.7 为表增加列

9.8 使用列存储表

9.8.1 重复数据

9.8.2 多列

9.9 (几乎)总是使用压缩



dy 0章 调优

10.1 使用EXPLAIN

10.2 EXPLAIN EXTENDED

10.3 限制调整

10.4 JOIN优化

10.5 本地模式

10.6 并行执行

10.7 严格模式

10.8 调整mapper和reducer个数

10.9 JVM重用

10.10 索引

10.11 动态分区调整

10.12 推测执行

10.13 单个MapReduce中多个GROUP BY

10.14 虚拟列



dy 1章 其他文件格式和压缩方法

11.1 确定安装编解码器

11.2 选择一种压缩编/解码器

11.3 开启中间压缩

11.4 终输出结果压缩

11.5 sequence file存储格式

11.6 使用压缩实践

11.7 存档分区

11.8 压缩：包扎



dy 2章 开发

12.1 修改Log4J属性

12.2 连接Java调试器到Hive

12.3 从源码编译Hive

12.3.1 执行Hive测试用例

12.3.2 执行hook

12.4 配置Hive和Eclipse

12.5 Maven工程中使用Hive

12.6 Hive中使用hive_test进行单元测试

12.7 新增的插件开发工具箱(PDK)



dy 3章 函数

13.1 发现和描述函数

13.2 调用函数

13.3 标准函数

13.4 聚合函数

13.5 表生成函数

13.6 一个通过日期计算其星座的UDF

13.7 UDF与GenericUDF

13.8 不变函数

13.9 用户自定义聚合函数

13.10 用户自定义表生成函数

13.10.1 可以产生多行数据的UDTF

13.10.2 可以产生具有多个字段的单行数据的UDTF

13.10.3 可以模拟复杂数据类型的UDTF

13.11 在 UDF中访问分布式缓存

13.12 以函数的方式使用注解

13.12.1 定数性(deterministic)标注

13.12.2 状态性(stateful)标注

13.12.3 wei一性

13.13 宏命令



dy 4章 Streaming

14.1 恒等变换

14.2 改变类型

14.3 投影变换

14.4 操作转换

14.5 使用分布式内存

14.6 由一行产生多行

14.7 使用streaming进行聚合计算

14.8 CLUSTER BY、DISTRIBUTE BY、SORT BY

14.9 GenericMR Tools for Streaming to Java

14.10 计算cogroup



dy 5章 自定义Hive文件和记录格式

15.1 文件和记录格式

15.2 阐明CREATE TABLE句式

15.3 文件格式

15.3.1 SequenceFile

15.3.2 RCfile

15.3.3 示例自定义输入格式：DualInputFormat

15.4 记录格式：SerDe

15.5 CSV和TSV SerDe

15.6 ObjectInspector

15.7 Thing Big Hive Reflection ObjectInspector

15.8 XML UDF

15.9 XPath相关的函数

15.10 JSON SerDe

15.11 Avro Hive SerDe

15.11.1 使用表属性信息定义Avro Schema

15.11.2 从指定URL中定义Schema

15.11.3 进化的模式

15.12 二进制输出



dy 6章 Hive的Thrift服务

16.1 启动Thrift Server

16.2 配置Groovy使用HiveServer

16.3 连接到HiveServer

16.4 获取集群状态信息

16.5 结果集模式

16.6 获取结果

16.7 获取执行计划

16.8 元数据存储方法

16.9 管理HiveServer

16.9.1 生产环境使用HiveServer

16.9.2 清理

16.10 Hive ThriftMetastore

16.10.1 ThriftMetastore 配置

16.10.2 客户端配置



dy 7章 存储处理程序和NoSQL

17.1 Storage Handler Background

17.2 HiveStorageHandler

17.3 HBase

17.4 Cassandra

17.4.1 静态列映射(Static Column Mapping)

17.4.2 为动态列转置列映射

17.4.3 Cassandra SerDe Properties

17.5 DynamoDB



dy 8章 安全

18.1 和Hadoop安全功能相结合

18.2 使用Hive进行验证

18.3 Hive中的权限管理

18.3.1 用户、组和角色

18.3.2 Grant 和 Revoke权限

18.4 分区级别的权限

18.5 自动授权



dy 9章 锁

19.1 Hive结合Zookeeper支持锁功能

19.2 显式锁和独占锁



第20章 Hive和Oozie整合

20.1 Oozie提供的多种动作(Action)

20.2 一个只包含两个查询过程的工作流示例

20.3 Oozie 网页控制台

20.4 工作流中的变量

20.5 获取输出

20.6 获取输出到变量



第21章 Hive和亚马逊网络服务系统(AWS)

21.1 为什么要弹性MapReduce

21.2 实例

21.3 开始前的注意事项

21.4 管理自有EMR Hive集群

21.5 EMR Hive上的Thrift Server服务

21.6 EMR上的实例组

21.7 配置EMR集群

21.7.1 部署hive-site.xml文件

21.7.2 部署.hiverc脚本

21.7.3 建立一个内存密集型配置

21.8 EMR上的持久层和元数据存储

21.9 EMR集群上的HDFS和S3

21.10 在S3上部署资源、配置和辅助程序脚本

21.11 S3上的日志

21.12 现买现卖

21.13 安全组

21.14 EMR和EC2以及Apache Hive的比较

21.15 包装



第22章 HCatalog

22.1 介绍

22.2 MapReduce

22.2.1 读数据

22.2.2 写数据

22.3 命令行

22.4 安全模型

22.5 架构



第23章 案例研究

23.1 m6d.com(Media6Degrees)

23.1.1 M 6D的数据科学，使用Hive和R

23.1.2 M6D UDF伪随机

23.1.3 M6D如何管理多MapReduce集群间的Hive数据访问

23.2 Outbrain

23.2.1 站内线上身份识别

23.2.2 计算复杂度

23.2.3 会话化

23.3 NASA喷气推进实验室

23.3.1 区域气候模型评价系统

23.3.2 我们的经验：为什么使用Hive

23.3.3 解决这些问题我们所面临的挑战

23.4 Photobucket

23.4.1 Photobucket 公司的大数据应用情况

23.4.2 Hive所使用的硬件资源信息

23.4.3 Hive提供了什么

23.4.4 Hive支持的用户有哪些

23.5 SimpleReach

23.6 Experiences and Needs from the Customer Trenches

23.6.1 介绍

23.6.2 Customer Trenches的用例



术语词汇表

前言
第1章 Hive介绍
1．1 Hive工作原理
1．2 Hive的数据类型
1．3 Hive的特点
1．4 本章小结

第2章 Hive架构
2．1 Hive用户接口
2．1．1 Hive CLI
2．1．2 HWI
2．1．3 Thrift服务
2．2 Hive元数据库
2．2．1 Hive元数据表结构
2．2．2 Hive元数据的三种存储模式
2．3 Hive数据存储
2．4 Hive文件格式
2．4．1 TextFile格式
2．4．2 SequenceFile格式
2．4．3 RCFile格式
2．4．4 ORC格式
2．5 本章小结

第3章 HiveQL表操作
3．1 内部表
3．2 外部表
3．3 分区表
3．3．1 静态分区
3．3．2 动态分区
3．4 桶表
3．5 视图
3．5．1 使用视图降低查询复杂度
3．5．2 使用视图来限制基于条件过滤的数据
3．5．3 动态分区中的视图和map类型
3．6 本章小结

第4章 HiveQL数据操作
4．1 装载数据到表中
4．2 通过查询语句向表中插入数据
4．3 单个查询语句中创建并加载数据
4．4 导出数据
4．5 本章小结

第5章 HiveQL查询
5．1 SELECT…FROM语句
5．1．1 使用正则表达式来指定列的
5．1．2 使用列值进行计算
5．1．3 算述运算符
5．1．4 使用函数
5．1．5 LIMIT语句
5．1．6 列别名
5．1．7 嵌套SELECT语句
5．1．8 CASE…WHEN…THEN语句
5．2 WHERE语句
5．2．1 谓词操作符
5．2．2 关于浮点数比较
5．2．3 LIKE和RLIKE
5．3 GROtJPBY语句
5．4．JOIN语句
5．4．1 INNER JOIN
5．4．2 JOIN优化
5．4．3 LEFTOUTER JOIN
5．4．4 R1GHTOUTER JOIN
5．4．5 FULLOUTER JOIN
5．4．6 LEFT SEMI JOIN
5．4．7 笛卡尔积JOIN
5．4．8 mad-side JOIN
5．5 ORDER BY和SOPT BY
5．6 含有SOftT BY的DISTRIBIJTE BY
5．7 CLUSTER BY
5．8 类型转换
5．9 抽样查询
5．9．1 数据块抽样
5．9．2 分桶表的输入裁剪
5．1 0LINIONALL
5．1 1本章小结

第6章 Hive配置与应用
6．1 Hive安装与配置
6．2 Hive访问
6．3 Hive基本操作
6．3．1 Hive CLI命令行操作讲解
6．3．2 Hive的数据类型
6．3．3 Hive表的创建
6．3．4 Hive数据导入
6．3．5 Hive数据导出
6．4 Hive数据定义
6．4．1 内部表与外部表的区别
6．4．2 内部表建立
6．4．3 外部表建立
6．4．4 表的分区与桶的建立
6．4．5 删除表与修改表结构
6．4．6 HiveQL简单查询语句
6．4．7 WHERE语句
6．5 Hive高级查询
6．6 本章小结

第7章Hive自定义函数
7．1 LIDF
7．2 UDTF
7．3 UDAF
7．4 Hive函数综合案例
7．4．1 Row-Sequence实现列自增长
7．4．2 列转行和行转列
7．5 本章小结

第8章Hive综合案例（一）
8．1 项目背景与数据情况
8．2 关键指标KPI
8．3 开发步骤分析
8．4 表结构设计
8．5 数据清洗过程
8．5．1 定期上传日志至HDFS
8．5．2 编写．MapReduce程序清理日志
8．5．3 定期清理日志至HDFS
8．5．4 查询清洗前后的数据
8．6 数据统计分析
8．6．1 借助Hive进行统计
8．6．2 使用HiveQL统计关键指标
8．7 本章小结

第9章Hive综合案例（二）
9．1 项目应用场景
9．2 设计与实现
9．2．1 日志格式分析
9．2．2 建立表
9．2．3 程序设计
9．2．4 编码实现
9．2．5 运行并测试
9．3 本章小结

第10章Hive综合案例（三）
10．1 应用场景
10．2 设计与实现
10．2．1 数据处理
10．2．2 使用Hive对清洗后的数据进行多维分析
10．2．3 在MySQL中建立数据库
10．2．4 使用sqoop把分析结果导入到MySQL中
10．2．5 程序设计与实现
10．2．6 运行并测试


第1章 数据仓库简介

1.1 什么是数据仓库 1

1.1.1 数据仓库的定义 1

1.1.2 建立数据仓库的原因 3

1.2 操作型系统与分析型系统 5

1.2.1 操作型系统 5

1.2.2 分析型系统 8

1.2.3 操作型系统和分析型系统对比 9

1.3 数据仓库架构 10

1.3.1 基本架构 10

1.3.2 主要数据仓库架构 12

1.3.3 操作数据存储 16

1.4 抽取-转换-装载 17

1.4.1 数据抽取 17

1.4.2 数据转换 19

1.4.3 数据装载 20

1.4.4 开发ETL系统的方法 21

1.4.5 常见ETL工具 21

1.5 数据仓库需求 22

1.5.1 基本需求 22

1.5.2 数据需求 23

 1.6 小结 24

第2章 数据仓库设计基础

2.1 关系数据模型 25

2.1.1 关系数据模型中的结构 25

2.1.2 关系完整性 28

2.1.3 规范化 30

2.1.4 关系数据模型与数据仓库 33

2.2 维度数据模型 34

2.2.1 维度数据模型建模过程 35

2.2.2 维度规范化 36

2.2.3 维度数据模型的特点 37

2.2.4 星型模式 38

2.2.5 雪花模式 40

2.3 Data Vault模型 42

2.3.1 Data Vault模型简介 42

2.3.2 Data Vault模型的组成部分 43

2.3.3 Data Vault模型的特点 44

2.3.4 Data Vault模型的构建 44

2.3.5 Data Vault模型实例 46

2.4 数据集市 49

2.4.1 数据集市的概念 50

2.4.2 数据集市与数据仓库的区别 50

2.4.3 数据集市设计 50

2.5 数据仓库实施步骤 51

2.6 小结 54

第3章 Hadoop生态圈与数据仓库

3.1 大数据定义 55

3.2 Hadoop简介 56

3.2.1 Hadoop的构成 57

3.2.2 Hadoop的主要特点 58

3.2.3 Hadoop架构 58

3.3 Hadoop基本组件 59

3.3.1 HDFS 60

3.3.2 MapReduce 65

3.3.3 YARN 72

3.4 Hadoop生态圈的其他组件 77

3.5 Hadoop与数据仓库 81

3.5.1 关系数据库的可扩展性瓶颈 82

3.5.2 CAP理论 84

3.5.3 Hadoop数据仓库工具 85

3.6 小结 88

第4章 安装Hadoop

4.1 Hadoop主要发行版本 89

4.1.1 Cloudera Distribution for Hadoop（CDH） 89

4.1.2 Hortonworks Data Platform（HDP） 90

4.1.3 MapR Hadoop 90

4.2 安装Apache Hadoop 91

4.2.1 安装环境 91

4.2.2 安装前准备 92

4.2.3 安装配置Hadoop 93

4.2.4 安装后配置 97

4.2.5 初始化及运行 97

4.3 配置HDFS Federation 99

4.4 离线安装CDH及其所需的服务 104

4.4.1 CDH安装概述 104

4.4.2 安装环境 106

4.4.3 安装配置 106

4.4.4 Cloudera Manager许可证管理 114

4.5 小结 115.........



Hive编程指南

　　《Hive编程指南》是一本Apache Hive的编程指南，旨在介绍如何使用Hive的SQL方法HiveQL来汇总、查询和分析存储在Hadoop分布式文件系统上的大数据集合。全书通过大量的实例，首先介绍如何在用户环境下安装和配置Hive，并对Hadoop和MapReduce进行详尽阐述，*终演示Hive如何在Hadoop生态系统进行工作。

　　《Hive编程指南》适合对大数据感兴趣的爱好者以及正在使用Hadoop系统的数据库管理员阅读使用。


第1章　基础知识　
1.1　Hadoop和MapReduce综述　
1.2　Hadoop生态系统中的Hive　
1.2.1　Pig　
1.2.2　HBase　
1.2.3　Cascading、Crunch及其他　
1.3　Java和Hive：词频统计算法　
1.4　后续事情　

第2章　基础操作　
2.1　安装预先配置好的虚拟机　
2.2　安装详细步骤　
2.2.1　装Java　
2.2.2　安装Hadoop　
2.2.3　本地模式、伪分布式模式和分布式模式　
2.2.4　测试Hadoop　
2.2.5　安装Hive　
2.3　Hive内部是什么　
2.4　启动Hive　
2.5　配置Hadoop环境　
2.5.1　本地模式配置　
2.5.2　分布式模式和伪分布式模式配置　
2.5.3　使用JDBC连接元数据　
2.6　Hive命令　
2.7　命令行界面　
2.7.1　CLI 选项　
2.7.2　变量和属性　
2.7.3　Hive中“一次使用”命令　
2.7.4　从文件中执行Hive查询　
2.7.5　hiverc文件　
2.7.6　使用Hive CLI的更多介绍　
2.7.7　查看操作命令历史　
2.7.8　执行shell命令　
2.7.9　在Hive内使用Hadoop的dfs命令　
2.7.10　Hive脚本中如何进行注释　
2.7.11　显示字段名称　

第3章　数据类型和文件格式　
3.1　基本数据类型　
3.2　集合数据类型　
3.3　文本文件数据编码　
3.4　读时模式　

第4章　HiveQL：数据定义　
4.1　Hive中的数据库　
4.2　修改数据库　
4.3　创建表　
4.3.1　管理表　
4.3.2　外部表　
4.4　分区表、管理表　
4.4.1　外部分区表　
4.4.2　自定义表的存储格式　
4.5　删除表　
4.6　修改表　
4.6.1　表重命名　
4.6.2　增加、修改和删除表分区　
4.6.3　修改列信息　
4.6.4　增加列　
4.6.5　删除或者替换列　
4.6.6　修改表属性　
4.6.7　修改存储属性　
4.6.8　众多的修改表语句　

第5章　HiveQL：数据操作　
5.1　向管理表中装载数据　
5.2　通过查询语句向表中插入数据　
5.3　单个查询语句中创建表并加载数据　
5.4　导出数据　

第6章　HiveQL：查询　
6.1　SELECT…FROM语句　
6.1.1　使用正则表达式来指定列　
6.1.2　使用列值进行计算　
6.1.3　算术运算符　
6.1.4　使用函数　
6.1.5　LIMIT语句　
6.1.6　列别名　
6.1.7　嵌套SELECT语句　
6.1.8　CASE…WHEN…THEN 句式　
6.1.9　什么情况下Hive可以避免进行MapReduce　
6.2　WHERE语句　
6.2.1　谓词操作符　
6.2.2　关于浮点数比较　
6.2.3　LIKE和RLIKE　
6.3　GROUP BY 语句　
6.4　JOIN语句　
6.4.1　INNER JOIN　
6.4.2　JOIN优化　
6.4.3　LEFT OUTER JOIN　
6.4.4　OUTER JOIN　
6.4.5　RIGHT OUTER JOIN　
6.4.6　FULL OUTER JOIN　
6.4.7　LEFT SEMI-JOIN　
6.4.8　笛卡尔积JOIN　
6.4.9　map-side JOIN　
6.5　ORDER BY和SORT BY　
6.6　含有SORT BY 的DISTRIBUTE BY　
6.7　CLUSTER BY　
6.8　类型转换　
6.9　抽样查询　
6.9.1　数据块抽样　
6.9.2　分桶表的输入裁剪　
6.10　UNION ALL　..........

第 1章 为Hive打好基础：Hadoop 1
1．1　一只小象出生了　2
1．2　Hadoop的结构　3
1．3　数据冗余　6
1．3．1　传统的高可用性　6
1．3．2　Hadoop的高可用性　9
1．4　MapReduce处理　12
1．4．1　超越MapReduce　16
1．4．2　YARN和现代数据架构　17
1．4．3　Hadoop 和开源社区　19
1．4．4　我们身在何处　22
第　2 章 Hive 简介　24
2．1　Hadoop 发行版　25
2．2　集群架构　27
2．3　Hive 的安装　30
2．4　探寻你的方式　32
2．5　Hive CLI　35
第3　章 Hive架构　37
3．1　Hive组件　37
3．2　HCatalog　38
3．3　HiveServer2　40
3．4　客户端工具　42
3．5　执行引擎：Tez　46
第4　章 Hive表DDL　48
4．1　schema-on-read　48
4．2　Hive数据模型　49
4．2．1　模式/数据库　49
4．2．2　为什么使用多个模式/数据库　49
4．2．3　创建数据库　49
4．2．4　更改数据库　50
4．2．5　删除数据库　50
4．2．6　列出数据库　51
4．3　Hive中的数据类型　51
4．3．1　基本数据类型　51
4．3．2　选择数据类型　51
4．3．3　复杂数据类型　52
4．4　表　53
4．4．1　创建表　53
4．4．2　列出表　54
4．4．3　内部表/外部表　54
4．4．4　内部表/受控表　55
4．4．5　内部表/外部表示例　55
4．4．6　表的属性　59
4．4．7　生成已有表的CREATE TABLE命令　60
4．4．8　分区和分桶　61
4．4．9　分区注意事项　63
4．4．10　对日期列进行高效分区　63
4．4．11　分桶的注意事项　65
4．4．12　更改表　66
4．4．13　ORC文件格式　67
4．4．14　更改表分区　68
4．4．15　修改列　72
4．4．16　删除表/分区　72
4．4．17　保护表/分区　73
4．4．18　其他CREATE TABLE命令选项　73
第5　章 数据操作语言　75
5．1　将数据装载到表中　75
5．1．1　使用存储在HDFS中的文件装载数据　75
5．1．2　使用查询装载数据　77
5．1．3　将查询到的数据写入文件系统　80
5．1．4　直接向表插入值　81
5．1．5　直接更新表中数据　83
5．1．6　在表中直接删除数据　84
5．1．7　创建结构相同的表　85
5．2　连接　86
5．2．1　使用等值连接来整合表　86
5．2．2　使用外连接　87
5．2．3　使用左半连接　89
5．2．4　用单次MapReduce实现连接　90
5．2．5　最后使用最大的表　91
5．2．6　事务处理　92
5．2．7　ACID是什么，以及为什么要用到它　92
5．2．8　Hive配置　92
第6章　将数据装载到Hive　94
6．1　装载数据之前的设计注意事项　94
6．2　将数据装载到HDFS　95
6．2．1　Ambari 文件视图　95
6．2．2　Hadoop命令行　97
6．2．3　HDFS的NFS Gateway　97
6．2．4　Sqoop　98
6．2．5　Apache NiFi　101
6．3　用Hive 访问数据　105
6．3．1　外部表　105
6．3．2　LOAD DATA语句　106
6．4　在Hive中装载增量变更数据　107
6．5　Hive流处理　107
6．6　小结　108
第7章　查询半结构化数据　109
7．1　点击流数据　111
7．1．1　摄取数据　113
7．1．2　创建模式　116
7．1．3　装载数据　116
7．1．4　查询数据　116
7．2　摄取JSON数据　119
7．2．1　使用UDF查询JSON　121
7．2．2　使用SerDe访问JSON　122
第8章　Hive分析　125
8．1　构建分析模型　125
8．1．1　使用太阳模型获取需求　125
8．1．2　将太阳模型转换为星型模式　129
8．1．3　构建数据仓库　137
8．2　评估分析模型 ．　140
8．2．1　评估太阳模型　140
8．2．2　评估聚合结果　142
8．2．3　评估数据集市　143
8．3　掌握数据仓库管理　144
8．3．1　必备条件　144
8．3．2　检索数据库　144
8．3．3　评估数据库　147
8．3．4　过程数据库　160
8．3．5　转换数据库　185
8．3．6　你掌握了什么　192
8．3．7　组织数据库　192
8．3．8　报表数据库　196
8．3．9　示例报表　197
8．4　高级分析　199
8．5　接下来学什么　199
第9章　Hive性能调优　200
9．1　Hive性能检查表　200
9．2　执行引擎　201
9．2．1　MapReduce　201
9．2．2　Tez　201
9．3　存储格式　203
9．3．1　ORC格式　203
9．3．2　Parquet格式　205
9．4　矢量化查询执行　206
9．5　查询执行计划　206
9．5．1　基于代价的优化　208
9．5．2　执行计划　210
9．5．3　性能检查表小结　212
第　10章 Hive的安全性　213
10．1　数据安全性的几个方面　213
10．1．1　身份认证　214
10．1．2　授权　214
10．1．3　管理　214
10．1．4　审计　214
10．1．5　数据保护　214
10．2　Hadoop的安全性　215
10．3　Hive的安全性　215
10．3．1　默认授权模式　215
10．3．2　基于存储的授权模式　216
10．3．3　基于SQL标准的授权模式　217
10．3．4　管理通过SQL进行的访问　218
10．4　使用Ranger进行Hive授权　219
10．4．1　访问Ranger用户界面　220
10．4．2　创建Ranger策略　220
10．4．3　使用Ranger审计　222
第　11章 Hive的未来　224
11．1　LLAP　224
11．2　Hive-on-Spark　225
11．3　Hive：ACID 和MERGE　225
11．4　可调隔离等级　225
11．5　ROLAP/基于立方体的分析　226
11．6　HiveServer2的发展　226
11．7　面向不同工作负载的多个HiveServer2实例　226
附录A　建立大数据团队　227
附录B　Hive函数　231

第1章 基础知识
1.1 Hadoop和MapReduce综述
1.2 Hadoop生态系统中的Hive
1.2.1 Pig
1.2.2 HBase
1.2.3 Cascading、Crunch及其他
1.3 Java和Hive：词频统计算法
1.4 后续事情

第2章 基础操作
2.1 安装预先配置好的虚拟机
2.2 安装详细步骤
2.2.1 装Java
2.2.2 安装Hadoop
2.2.3 本地模式、伪分布式模式和分布式模式
2.2.4 测试Hadoop
2.2.5 安装Hive
2.3 Hive内部是什么
2.4 启动Hive
2.5 配置Hadoop环境
2.5.1 本地模式配置
2.5.2 分布式模式和伪分布式模式配置
2.5.3 使用JDBC连接元数据
2.6 Hive命令
2.7 命令行界面
2.7.1 CLI 选项
2.7.2 变量和属性
2.7.3 Hive中“一次使用”命令
2.7.4 从文件中执行Hive查询
2.7.5 hiverc文件
2.7.6 使用Hive CLI的更多介绍
2.7.7 查看操作命令历史
2.7.8 执行shell命令
2.7.9 在Hive内使用Hadoop的dfs命令
2.7.10 Hive脚本中如何进行注释
2.7.11 显示字段名称

第3章 数据类型和文件格式
3.1 基本数据类型
3.2 集合数据类型
3.3 文本文件数据编码
3.4 读时模式

第4章 HiveQL：数据定义
4.1 Hive中的数据库
4.2 修改数据库
4.3 创建表
4.3.1 管理表
4.3.2 外部表
4.4 分区表、管理表
4.4.1 外部分区表
4.4.2 自定义表的存储格式
4.5 删除表
4.6 修改表
4.6.1 表重命名
4.6.2 增加、修改和删除表分区
4.6.3 修改列信息
4.6.4 增加列
4.6.5 删除或者替换列
4.6.6 修改表属性
4.6.7 修改存储属性
4.6.8 众多的修改表语句

第5章 HiveQL：数据操作
5.1 向管理表中装载数据
5.2 通过查询语句向表中插入数据
5.3 单个查询语句中创建表并加载数据
5.4 导出数据

第6章 HiveQL：查询
6.1 SELECT…FROM语句
6.1.1 使用正则表达式来指定列
6.1.2 使用列值进行计算
6.1.3 算术运算符
6.1.4 使用函数
6.1.5 LIMIT语句
6.1.6 列别名
6.1.7 嵌套SELECT语句
6.1.8 CASE…WHEN…THEN 句式
6.1.9 什么情况下Hive可以避免进行MapReduce
6.2 WHERE语句
6.2.1 谓词操作符
6.2.2 关于浮点数比较
6.2.3 LIKE和RLIKE
6.3 GROUP BY 语句
6.4 JOIN语句
6.4.1 INNER JOIN
6.4.2 JOIN优化
6.4.3 LEFT OUTER JOIN
6.4.4 OUTER JOIN
6.4.5 RIGHT OUTER JOIN
6.4.6 FULL OUTER JOIN
6.4.7 LEFT SEMI-JOIN
6.4.8 笛卡尔积JOIN
6.4.9 map-side JOIN
6.5 ORDER BY和SORT BY
6.6 含有SORT BY 的DISTRIBUTE BY
6.7 CLUSTER BY
6.8 类型转换
6.9 抽样查询
6.9.1 数据块抽样
6.9.2 分桶表的输入裁剪
6.10 UNION ALL

第7章 HiveQL：视图
7.1 使用视图来降低查询复杂度
7.2 使用视图来限制基于条件过滤的数据
7.3 动态分区中的视图和map类型
7.4 视图零零碎碎相关的事情

第8章 HiveQL：索引
8.1 创建索引
8.2 重建索引
8.3 显示索引
8.4 删除索引
8.5 实现一个定制化的索引处理器

第9章 模式设计
9.1 按天划分的表
9.2 关于分区
9.3 唯一键和标准化
9.4 同一份数据多种处理
9.5 对于每个表的分区
9.6 分桶表数据存储
9.7 为表增加列
9.8 使用列存储表
9.8.1 重复数据
9.8.2 多列
9.9 （几乎）总是使用压缩

第10章 调优
10.1 使用EXPLAIN
10.2 EXPLAIN EXTENDED
10.3 限制调整
10.4 JOIN优化
10.5 本地模式
10.6 并行执行
10.7 严格模式
10.8 调整mapper和reducer个数
10.9 JVM重用
10.10 索引
10.11 动态分区调整
10.12 推测执行
10.13 单个MapReduce中多个GROUP BY
10.14 虚拟列

第11章 其他文件格式和压缩方法
11.1 确定安装编解码器
11.2 选择一种压缩编/解码器
11.3 开启中间压缩
11.4 最终输出结果压缩
11.5 sequence file存储格式
11.6 使用压缩实践
11.7 存档分区
11.8 压缩：包扎

第12章 开发
12.1 修改Log4J属性
12.2 连接Java调试器到Hive
12.3 从源码编译Hive
12.3.1 执行Hive测试用例
12.3.2 执行hook
12.4 配置Hive和Eclipse
12.5 Maven工程中使用Hive
12.6 Hive中使用hive_test进行单元测试
12.7 新增的插件开发工具箱（PDK）

第13章 函数
13.1 发现和描述函数
13.2 调用函数
13.3 标准函数
13.4 聚合函数
13.5 表生成函数
13.6 一个通过日期计算其星座的UDF
13.7 UDF与GenericUDF
13.8 不变函数
13.9 用户自定义聚合函数
13.10 用户自定义表生成函数
13.10.1 可以产生多行数据的UDTF
13.10.2 可以产生具有多个字段的单行数据的UDTF
13.10.3 可以模拟复杂数据类型的UDTF
13.11 在 UDF中访问分布式缓存
13.12 以函数的方式使用注解
13.12.1 定数性（deterministic）标注
13.12.2 状态性（stateful）标注
13.12.3 唯一性
13.13 宏命令

第14章 Streaming
14.1 恒等变换
14.2 改变类型
14.3 投影变换
14.4 操作转换
14.5 使用分布式内存
14.6 由一行产生多行
14.7 使用streaming进行聚合计算
14.8 CLUSTER BY、DISTRIBUTE BY、SORT BY
14.9 GenericMR Tools for Streaming to Java
14.10 计算cogroup

第15章 自定义Hive文件和记录格式
15.1 文件和记录格式
15.2 阐明CREATE TABLE句式
15.3 文件格式
15.3.1 SequenceFile
15.3.2 RCfile
15.3.3 示例自定义输入格式：DualInputFormat
15.4 记录格式：SerDe
15.5 CSV和TSV SerDe
15.6 ObjectInspector
15.7 Thing Big Hive Reflection ObjectInspector
15.8 XML UDF
15.9 XPath相关的函数
15.10 JSON SerDe
15.11 Avro Hive SerDe
15.11.1 使用表属性信息定义Avro Schema
15.11.2 从指定URL中定义Schema
15.11.3 进化的模式
15.12 二进制输出

第16章 Hive的Thrift服务
16.1 启动Thrift Server
16.2 配置Groovy使用HiveServer
16.3 连接到HiveServer
16.4 获取集群状态信息
16.5 结果集模式
16.6 获取结果
16.7 获取执行计划
16.8 元数据存储方法
16.9 管理HiveServer
16.9.1 生产环境使用HiveServer
16.9.2 清理
16.10 Hive ThriftMetastore
16.10.1 ThriftMetastore 配置
16.10.2 客户端配置

第17章 存储处理程序和NoSQL
17.1 Storage Handler Background
17.2 HiveStorageHandler
17.3 HBase
17.4 Cassandra
17.4.1 静态列映射（Static Column Mapping）
17.4.2 为动态列转置列映射
17.4.3 Cassandra SerDe Properties
17.5 DynamoDB

第18章 安全
18.1 和Hadoop安全功能相结合
18.2 使用Hive进行验证
18.3 Hive中的权限管理
18.3.1 用户、组和角色
18.3.2 Grant 和 Revoke权限
18.4 分区级别的权限
18.5 自动授权

第19章 锁
19.1 Hive结合Zookeeper支持锁功能
19.2 显式锁和独占锁

第20章 Hive和Oozie整合
20.1 Oozie提供的多种动作（Action）
20.2 一个只包含两个查询过程的工作流示例
20.3 Oozie 网页控制台
20.4 工作流中的变量
20.5 获取输出
20.6 获取输出到变量

第21章 Hive和亚马逊网络服务系统（AWS）
21.1 为什么要弹性MapReduce
21.2 实例
21.3 开始前的注意事项
21.4 管理自有EMR Hive集群
21.5 EMR Hive上的Thrift Server服务
21.6 EMR上的实例组
21.7 配置EMR集群
21.7.1 部署hive-site.xml文件
21.7.2 部署.hiverc脚本
21.7.3 建立一个内存密集型配置
21.8 EMR上的持久层和元数据存储
21.9 EMR集群上的HDFS和S
21.10 在S3上部署资源、配置和辅助程序脚本
21.11 S3上的日志
21.12 现买现卖
21.13 安全组
21.14 EMR和EC2以及Apache Hive的比较
21.15 包装

第22章 HCatalog
22.1 介绍
22.2 MapReduce
22.2.1 读数据
22.2.2 写数据
22.3 命令行
22.4 安全模型
22.5 架构

第23章 案例研究
23.1 m6d.com（Media6Degrees）
23.1.1 M 6D的数据科学，使用Hive和R
23.1.2 M6D UDF伪随机
23.1.3 M6D如何管理多MapReduce集群间的Hive数据访问
23.2 Outbrain
23.2.1 站内线上身份识别
23.2.2 计算复杂度
23.2.3 会话化
23.3 NASA喷气推进实验室
23.3.1 区域气候模型评价系统
23.3.2 我们的经验：为什么使用Hive
23.3.3 解决这些问题我们所面临的挑战
23.4 Photobucket
23.4.1 Photobucket 公司的大数据应用情况
23.4.2 Hive所使用的硬件资源信息
23.4.3 Hive提供了什么
23.4.4 Hive支持的用户有哪些
23.5 SimpleReach
23.6 Experiences and Needs from the Customer Trenches
23.6.1 介绍
23.6.2 Customer Trenches的用例
术语词汇表

Hadoop 指南-目录

第Ⅰ部分Hadoop基础知识
dy 章初识Hadoop3
1.1数据！数据！3
1.2数据的存储与分析5
1.3查询所有数据6
1.4不仅仅是批处理7
1.5相较于其他系统的优势8
1.6ApacheHadoop发展简史12
1.7本书包含的内容16
第2章关于MapReduce19
2.1气象数据集19
2.2使用Unix工具来分析数据21
2.3使用Hadoop来分析数据22
2.4横向扩展31
2.5HadoopStreaming37
第3章Hadoop分布式文件系统42
3.1HDFS的设计42
3.2HDFS的概念44
3.3命令行接口50
3.4Hadoop文件系统52
3.5Java接口56
3.6数据流68
3.7通过distcp并行复制76
第4章关于YARN78
4.1剖析YARN应用运行机制79
4.2YARN与MapReduce1相比82
4.3YARN中的调度85
4.4延伸阅读95
第5章Hadoop的I/O操作96
5.1数据完整性96
5.2压缩99
5.3序列化109
5.4基于文件的数据结构127
第Ⅱ部分关于MapReduce
第6章MapReduce应用开发141
6.1用于配置的API142
6.2配置开发环境144
6.3用MRUnit来写单元测试152
6.4本地运行测试数据156
6.5在集群上运行160
6.6作业调优174
6.7MapReduce的工作流176
第7章MapReduce的工作机制184
7.1剖析MapReduce作业运行
机制184
7.2失败191
7.3shuffle和排序195
7.4任务的执行201
第8章MapReduce的
类型与格式207
8.1MapReduce的类型207
8.2输入格式218
8.3输出格式236
第9章MapReduce的特性243
9.1计数器243
9.2排序252
9.3连接264
9.4边数据分布270
9.5MapReduce库类276
第Ⅲ部分Hadoop的操作
dy 0章构建Hadoop集群279
…………已省略更多目录

Hive编程指南-目录

dy 章 基础知识
1.1 Hadoop和MapReduce综述
1.2 Hadoop生态系统中的Hive
1.2.1 Pig
1.2.2 HBase
1.2.3 Cascading、Crunch及其他
1.3 Java和Hive：词频统计算法
1.4 后续事情
第2章 基础操作
 2.1 安装预先配置好的虚拟机
 2.2 安装详细步骤
 2.2.1 装Java
 2.2.2 安装Hadoop
 2.2.3 本地模式、伪分布式模式和分布式模式
 2.2.4 测试Hadoop
 2.2.5 安装Hive
 2.3 Hive内部是什么
 2.4 启动Hive
 2.5 配置Hadoop环境
 2.5.1 本地模式配置
 2.5.2 分布式模式和伪分布式模式配置
 2.5.3 使用JDBC连接元数据
 2.6 Hive命令
 2.7 命令行界面
 2.7.1 CLI 选项
 2.7.2 变量和属性
 2.7.3 Hive中“一次使用”命令
 2.7.4 从文件中执行Hive查询
 2.7.5 hiverc文件
 2.7.6 使用Hive CLI的更多介绍
 2.7.7 查看操作命令历史
 2.7.8 执行shell命令
 2.7.9 在Hive内使用Hadoop的dfs命令
 2.7.10 Hive脚本中如何进行注释
     2.7.11 显示字段名称

第3章 数据类型和文件格式
 3.1 基本数据类型
 3.2 集合数据类型
 3.3 文本文件数据编码
 3.4 读时模式

第4章 HiveQL：数据定义
 4.1 Hive中的数据库
 4.2 修改数据库
 4.3 创建表
 4.3.1 管理表
 4.3.2 外部表
 4.4 分区表、管理表
 4.4.1 外部分区表
 4.4.2 自定义表的存储格式
 4.5 删除表
 4.6 修改表
 4.6.1 表重命名
 4.6.2 增加、修改和删除表分区
 4.6.3 修改列信息
 4.6.4 增加列
 4.6.5 删除或者替换列
 4.6.6 修改表属性
 4.6.7 修改存储属性
 4.6.8 众多的修改表语句
========

1. Hive基本概念
1.1 Hive简介
1.1.1 什么是Hive
​ 官网: https://hive.apache.org/ ，建议英语过八级的可以去尝试，不过也有谷歌翻译呢，网址： https://translate.google.cn

Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能，与spark不同的是hive是将sql语句转换为MapReduce任务进行运行的（所以适合于处理大批量的离线数据而且速度极其缓慢，不能作为真正的大型数据库）。

1.1.2 为什么使用Hive
Ø 直接使用hadoop所面临的问题

人员学习成本太高

项目周期要求太短

MapReduce实现复杂查询逻辑开发难度太大

​ 因此，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析（基于已有的规整化的数据的查询操作）。

Ø 为什么要使用Hive

操作接口采用类SQL语法，提供快速开发的能力。

避免了去写MapReduce，减少开发人员的学习成本。

扩展功能很方便。

另外，基于mr的算法会导致众多实现，而且性能不一，而使用hive将可以统一具体实现统一性能方便管理

1.1.3 Hive的特点
Ø 可扩展

Hive可以自由的扩展集群的规模，一般情况下不需要重启服务。

Ø 延展性

Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。

Ø 容错

良好的容错性，节点出现问题SQL仍可完成执行。

1.1.4 安装Hive
三种模式

内嵌模式：元数据保持在内嵌的derby模式(内嵌数据库)，只允许一个会话连接(最简洁)

本地独立模式：在本地安装Mysql，吧元数据放到mySql内

远程模式：元数据放置在远程的Mysql数据库(连接外部数据库服务，实际开发使用)

1、下载Hive安装包

http://hive.apache.org/downloads.html

2、将hive文件上传到HADOOP集群，并解压

​ 将文件上传到：/export/software

tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /export/servers/

cd /export/servers/

ln -s apache-hive-1.2.1-bin hive

3、配置环境变量，编辑/etc/profile

#set hive env

export HIVE_HOME=/export/servers/hive

export PATH=${HIVE_HOME}/bin:$PATH
1
2
3
4
5
#让环境变量生效

source /etc/profile

4、修改hive配置文件

进入配置文件的目录

cd /export/servers/hive/conf/

修改hive-env.sh文件

cp hive-env.sh.template hive-env.sh

将以下内容写入到hive-env.sh文件中

export JAVA_HOME=/export/servers/jdk

export HADOOP_HOME=/export/servers/hadoop

export HIVE_HOME=/export/servers/hive

修改log4j文件

cp hive-log4j.properties.template hive-log4j.properties

将EventCounter修改成org.apache.hadoop.log.metrics.EventCounter

#log4j.appender.EventCounter=org.apache.hadoop.hive.shims.HiveEventCounter

log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter

配置远程登录模式

touch hive-site.xml

将以下信息写入到hive-site.xml文件中

<configuration>
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://hadoop02:3306/hivedb?createDatabaseIfNotExist=true</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.jdbc.Driver</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>root</value>
        </property>
        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>root</value>
        </property>
</configuration>
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
5、安装mysql并配置hive数据库及权限

安装mysql数据库及客户端

yum install mysql-server

yum install mysql

service mysqld start

配置hive元数据库

mysql -u root -p

create database hivedb;

对hive元数据库进行赋权，开放远程连接，开放localhost连接

grant all privileges on . to root@”%” identified by “root” with grant option;

grant all privileges on . to root@”localhost” identified by “root” with grant option;

6、运行hive命令即可启动hive

hive

附录1：如果报错Terminal initialization failed; falling back to unsupported

​ 将/export/servers/hive/lib 里面的jline2.12替换了hadoop 中/export/servers/hadoop/hadoop-2.6.1/share/hadoop/yarn/lib/jline-0.09*.jar

附录2：异常信息

​ Logging initialized using configuration in jar:file:/export/servers/apache-hive-2.0.0-bin/lib/hive-common-2.0.0.jar!/hive-log4j2.properties

Exception in thread “main” java.lang.RuntimeException: Hive metastore database is not initialized. Please use schematool (e.g. ./schematool -initSchema -dbType …) to create the schema. If needed, don’t forget to include the option to auto-create the underlying database in your JDBC connection string (e.g. ?createDatabaseIfNotExist=true for mysql)

处理方法：

​ schematool -dbType mysql -initSchema

1.1.5 java后台使用hive
使用maven管理jar包依赖

链接: mvnrepository.com
搜索hive,复制以下依赖到pom.xml中即可
org.apache.org->
    hive-exec--->自定义UDF
    hive-common
    hive-service
    hive-jdbc
    hadoop-common
然后选择合适的版本即可
1
2
3
4
5
6
7
8
9
1.2 Hive架构
1.2.1 架构图


Jobtracker是hadoop1.x中的组件，它的功能相当于： Resourcemanager+AppMaster

TaskTracker 相当于： Nodemanager + yarnchild

1.2.1.1 业务模型建造
​ 在经过了长期的大数据的实践和总结，可以使用星型模型、雪花模型等

​ 星型模型：以核心表为中心，以其他需求字段以及其组合作为纬度，多层依次展开

1.2.1.2 架构全景
​ 首先需要使用Hive关联到一个YARN管理的HDFS中，假设一个统计的原始文件为access.log，当一个指令发送到hive上执行时，例如说create database db_bic;指令会先发送到类似于complier编译解析器，一方面发送写的操作大元信息(schema)数据库，另一方面会查询mr的模板库，生成真正的执行代码的job runer ,然后会操作到YARN的HDFS中在对应的文件目录下创建一个对应名称的文件夹，例如/user/hive/warehouse/db_bi,在创建了数据库之后再创建一张表，类似sql的形式,先use db_bic;然后create table ods_click_data(ip string,timestamp string,reques_url string,referral_url string,,status int,useragent string)；发送到hive的compiler，依赖操作需要先查询之前的元信息(schema)数据库，然后查询mr模板库，分析产生job runer,到YARN管理中的HDFS中再次依赖创建创建,例如/user/hive/warehouse/db_bi/ods_click_data/，在ods_click_data文件夹中存放着一系列的点击流数据文件

1.2.2 基本组成
​ 用户接口：包括 CLI、JDBC/ODBC、WebGUI。

​ 元数据存储：通常是存储在关系数据库如 mysql , derby中。

​ 解释器、编译器、优化器、执行器。

1.2.3 各组件的基本功能
用户接口主要由三个：CLI、JDBC/ODBC和WebGUI。其中，CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。

元数据存储：Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。

解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。

1.3 Hive与Hadoop的关系
Hive利用HDFS存储数据，利用MapReduce查询数据



1.4 Hive与传统数据库对比


总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析

1.5 Hive的数据存储
1、Hive中所有的数据都存储在 HDFS 中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，RCFILE等）

2、只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据。

3、Hive 中包含以下数据模型：DB、Table，External Table，Partition，Bucket。

db：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹

table：在hdfs中表现所属db目录下一个文件夹

external table：外部表, 与table类似，不过其数据存放位置可以在任意指定路径

普通表: 删除表后, hdfs上的文件都删了

External外部表删除后, hdfs上的文件没有删除, 只是把文件删除了

partition：在hdfs中表现为table目录下的子目录

bucket：桶, 在hdfs中表现为同一个表目录下根据hash散列之后的多个文件, 会根据不同的文件把数据放到不同的文件中
1
2
3
4
5
6
7
8
9
10
11
12
13
1.6 HIVE的安装部署
1.6.1 安装
单机版：

元数据库mysql版：

1.6.2 使用方式
Hive交互shell
bin/hive

Hive thrift服务


启动方式，（假如是在hadoop01上）：

启动为前台：bin/hiveserver2

启动为后台：nohup bin/hiveserver2 1>/var/log/hiveserver.log 2>/var/log/hiveserver.err &

启动成功后，可以在别的节点上用beeline去连接

v 方式（1）

hive/bin/beeline 回车，进入beeline的命令界面

输入命令连接hiveserver2

beeline> !connect jdbc:hive2//mini1:10000

（hadoop01是hiveserver2所启动的那台主机名，端口默认是10000）

v 方式（2）

或者启动就连接：

bin/beeline -u jdbc:hive2://mini1:10000 -n hadoop

接下来就可以做正常sql查询了

Hive命令
[hadoop@hdp-node-02 ~]$ hive -e ‘sql’

2. Hive基本操作
在使用HIVE操作数据之前先总结一下mr的操作模式:

mapreduce在编程的时候，基本上一个固化的模式，没有太多可灵活改变的地方，除了以下几处：

1、输入数据接口：InputFormat   --->     FileInputFormat(文件类型数据读取的通用抽象类)  DBInputFormat （数据库数据读取的通用抽象类）
   默认使用的实现类是： TextInputFormat     job.setInputFormatClass(TextInputFormat.class)
   TextInputFormat的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为key，行内容作为value返回

2、逻辑处理接口： Mapper
   完全需要用户自己去实现其中  map()   setup()   clean()

3、map输出的结果在shuffle阶段会被partition以及sort，此处有两个接口可自定义：
     Partitioner
        有默认实现 HashPartitioner，逻辑是  根据key和numReduces来返回一个分区号； key.hashCode()&Integer.MAXVALUE % numReduces
    通常情况下，用默认的这个HashPartitioner就可以，如果业务上有特别的需求，可以自定义

     Comparable
        当我们用自定义的对象作为key来输出时，就必须要实现WritableComparable接口，override其中的compareTo()方法

4、reduce端的数据分组比较接口 ： Groupingcomparator
    reduceTask拿到输入数据（一个partition的所有数据）后，首先需要对数据进行分组，其分组的默认原则是key相同，然后对每一组kv数据调用一次reduce()方法，并且将这一组kv中的第一个kv的key作为参数传给reduce的key，将这一组数据的value的迭代器传给reduce()的values参数

    利用上述这个机制，我们可以实现一个高效的分组取最大值的逻辑：
    自定义一个bean对象用来封装我们的数据，然后改写其compareTo方法产生倒序排序的效果
    然后自定义一个Groupingcomparator，将bean对象的分组逻辑改成按照我们的业务分组id来分组（比如订单号）
    这样，我们要取的最大值就是reduce()方法中传进来key


5、逻辑处理接口：Reducer
    完全需要用户自己去实现其中  reduce()   setup()   clean()

6、输出数据接口： OutputFormat  ---> 有一系列子类  FileOutputformat  DBoutputFormat  .....
    默认实现类是TextOutputFormat，功能逻辑是：  将每一个KV对向目标文本文件中输出为一行
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
2.1 DDL操作
2.1.1 创建表
建表语法
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name

[(col_name data_type [COMMENT col_comment], …)]

[COMMENT table_comment]

[PARTITIONED BY (col_name data_type [COMMENT col_comment], …)]

[CLUSTERED BY (col_name, col_name, …)

[SORTED BY (col_name [ASC|DESC], …)] INTO num_buckets BUCKETS]

[ROW FORMAT row_format]

[STORED AS file_format]

[LOCATION hdfs_path]

说明：

1、 CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。

2、 EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。

3、LIKE 允许用户复制现有的表结构，但是不复制数据。

4、ROW FORMAT DELIMITED FIELDS TERMINATED BY char MAP KEYS TERMINATED BY char  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,property_name=property_value, ...)]
用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。

5、 STORED AS SEQUENCEFILE|TEXTFILE|RCFILE
如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。

6、CLUSTERED BY
对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。

把表（或者分区）组织成桶（Bucket）有两个理由：
（1）获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。
（2）使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
具体实例
1、 创建内部表mytable。



2、 创建外部表pageview。



内布表区别补充：当试验drop两种表时，内部表的源文件也会被删除，而外部表不会

3、 创建分区表invites。



4、 创建带桶的表student。



补充：在使用带桶的表时需要先设置以下参数:

# 开启分桶
set hive.enforce.bucketing = true;
set mapreduce.job.reduces = 适合的数量;
1
2
3
2.1.2 修改表
增加/删除分区
语法结构

ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION ‘location1’ ] partition_spec [ LOCATION ‘location2’ ] …

partition_spec:
PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, …)

ALTER TABLE table_name DROP partition_spec, partition_spec,…

具体实例





重命名表
语法结构

ALTER TABLE table_name RENAME TO new_table_name

具体实例



增加/更新列
语法结构

ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], …)

注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。

ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]

具体实例



2.1.3 显示命令
show tables;

show databases ;

show partitions table_name;

show functions;

desc extended t_name;

desc formatted table_name;

2.2 DML操作
2.2.1 Load
语法结构

LOAD DATA [LOCAL] INPATH ‘filepath’ [OVERWRITE] INTO

TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 …)]

说明：

1、 Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。

2、 filepath：

相对路径，例如：project/data1

绝对路径，例如：/user/hive/project/data1

包含模式的完整 URI，列如：

hdfs://namenode:9000/user/hive/project/data1

3、 LOCAL关键字

如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。

如果没有指定 LOCAL 关键字，则根据inpath中的uri查找文件

4、 OVERWRITE 关键字

如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。

如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。

具体实例

1、 加载相对路径数据。



2、 加载绝对路径数据。



3、 加载包含模式数据。



4、 OVERWRITE关键字使用。



2.2.2 Insert
将查询结果插入Hive表
语法结构

INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1 FROM from_statement

Multiple inserts:

FROM from_statement

INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 …)] select_statement1

[INSERT OVERWRITE TABLE tablename2 [PARTITION …] select_statement2] …

Dynamic partition inserts:

INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] …) select_statement FROM from_statement

具体实例

1、基本模式插入。



2、多插入模式。



3、自动分区模式。



导出表数据

语法结构

INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT … FROM …

multiple inserts:

FROM from_statement

INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1

[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] …

具体实例

1、导出文件到本地。



说明：

数据写入到文件系统时进行文本序列化，且每列用^A来区分，\n为换行符。用more命令查看时不容易看出分割符，可以使用: sed -e ‘s/\x01/|/g’ filename来查看。

2、导出数据到HDFS。



2.2.3 SELECT
基本的Select操作

语法结构

SELECT [ALL | DISTINCT] select_expr, select_expr, …

FROM table_reference

[WHERE where_condition]

[GROUP BY col_list [HAVING condition]]

[CLUSTER BY col_list

| [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]

]

[LIMIT number]

注：1、order by 会对输入做全局排序，因此当只有一个reducer时，会导致当输入规模较大，需要较长的计算时间。

2、sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks>1，则sort by只保证每个reducer的输出有序，不保证全局有序。

3、distribute by根据distribute by指定的内容将数据分到同一个reducer。

4、Cluster by 除了具有Distribute by的功能外，还会对该字段进行排序。因此，常常认为cluster by = distribute by + sort by

具体实例

1、获取年龄大的3个学生。



2、查询学生信息按年龄，降序排序。







3、按学生名称汇总学生年龄。



2.3 Hive Join
语法结构

join_table:

table_reference JOIN table_factor [join_condition]

| table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition

| table_reference LEFT SEMI JOIN table_reference join_condition

Hive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。

另外，Hive 支持多于 2 个表的连接。

写 join 查询时，需要注意几个关键点：

只支持等值join
例如：

SELECT a.* FROM a JOIN b ON (a.id = b.id)

SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)

是正确的，然而:

SELECT a.* FROM a JOIN b ON (a.id>b.id)

是错误的。

可以 join 多于 2 个表。
例如

SELECT a.val, b.val, c.val FROM a JOIN b

​ ON (a.key = b.key1) JOIN c ON (c.key = b.key2)

如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，例如：

SELECT a.val, b.val, c.val FROM a JOIN b

​ ON (a.key = b.key1) JOIN c

​ ON (c.key = b.key1)

被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key。

SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1)

JOIN c ON (c.key = b.key2)

而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 join。

3．join 时，每次 map/reduce 任务的逻辑：

​ reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如：

SELECT a.val, b.val, c.val FROM a

​ JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)

所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有：

SELECT a.val, b.val, c.val FROM a

​ JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)

这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。

4．LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况

例如：

SELECT a.val, b.val FROM

a LEFT OUTER JOIN b ON (a.key=b.key)

对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出:

a.val, NULL

所以 a 表中的所有记录都被保留了；

“a RIGHT OUTER JOIN b”会保留所有 b 表的记录。

Join 发生在 WHERE 子句之前。如果你想限制 join 的输出，应该在 WHERE 子句中写过滤条件——或是在 join 子句中写。这里面一个容易混淆的问题是表分区的情况：

SELECT a.val, b.val FROM a

LEFT OUTER JOIN b ON (a.key=b.key)

WHERE a.ds=’2009-07-07’ AND b.ds=’2009-07-07’

会 join a 表到 b 表（OUTER JOIN），列出 a.val 和 b.val 的记录。WHERE 从句中可以使用其他列作为过滤条件。但是，如前所述，如果 b 表中找不到对应 a 表的记录，b 表的所有列都会列出 NULL，包括 ds 列。也就是说，join 会过滤 b 表中不能找到匹配 a 表 join key 的所有记录。这样的话，LEFT OUTER 就使得查询结果与 WHERE 子句无关了。解决的办法是在 OUTER JOIN 时使用以下语法：

SELECT a.val, b.val FROM a LEFT OUTER JOIN b

ON (a.key=b.key AND

​ b.ds=’2009-07-07’ AND

​ a.ds=’2009-07-07’)

这一查询的结果是预先在 join 阶段过滤过的，所以不会存在上述问题。这一逻辑也可以应用于 RIGHT 和 FULL 类型的 join 中。

Join 是不能交换位置的。无论是 LEFT 还是 RIGHT join，都是左连接的。

SELECT a.val1, a.val2, b.val, c.val

FROM a

JOIN b ON (a.key = b.key)

LEFT OUTER JOIN c ON (a.key = c.key)

先 join a 表到 b 表，丢弃掉所有 join key 中不匹配的记录，然后用这一中间结果和 c 表做 join。这一表述有一个不太明显的问题，就是当一个 key 在 a 表和 c 表都存在，但是 b 表中不存在的时候：整个记录在第一次 join，即 a JOIN b 的时候都被丢掉了（包括a.val1，a.val2和a.key），然后我们再和 c 表 join 的时候，如果 c.key 与 a.key 或 b.key 相等，就会得到这样的结果：NULL, NULL, NULL, c.val

具体实例

1、 获取已经分配班级的学生姓名。



2、 获取尚未分配班级的学生姓名。



3、 LEFT SEMI JOIN是IN/EXISTS的高效实现。



3 Hive Shell参数
3.1 Hive命令行
 语法结构

hive [-hiveconf x=y] [<-i filename>] [<-f filename>|<-e query-string>] [-S]

说明：

1、 -i 从文件初始化HQL。

2、 -e从命令行执行指定的HQL

3、 -f 执行HQL脚本

4、 -v 输出执行的HQL语句到控制台

5、 -p connect to Hive Server on port number

6、 -hiveconf x=y Use this to set hive/hadoop configuration variables.

 具体实例

1、运行一个查询。



2、运行一个文件。



3、运行参数文件。



3.2 Hive参数配置方式
Hive参数大全：

https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties

开发Hive应用时，不可避免地需要设定Hive的参数。设定Hive的参数可以调优HQL代码的执行效率，或帮助定位问题。然而实践中经常遇到的一个问题是，为什么设定的参数没有起作用？这通常是错误的设定方式导致的。

对于一般参数，有以下三种设定方式：

l 配置文件

l 命令行参数

l 参数声明

配置文件：Hive的配置文件包括

l 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml

l 默认配置文件：$HIVE_CONF_DIR/hive-default.xml

用户自定义配置会覆盖默认配置。

另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。

配置文件的设定对本机启动的所有Hive进程都有效。

命令行参数：启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：

bin/hive -hiveconf hive.root.logger=INFO,console

这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效。

参数声明：可以在HQL中使用SET关键字设定参数，例如：

set mapred.reduce.tasks=100;

这一设定的作用域也是session级的。

上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。

4. Hive函数
4.0 参考资料
函数分类


HIVE CLI命令
显示当前会话有多少函数可用
SHOW FUNCTIONS;

显示函数的描述信息
DESC FUNCTION concat;



显示函数的扩展描述信息
DESC FUNCTION EXTENDED concat;

简单函数
函数的计算粒度为单条记录。
关系运算
数学运算
逻辑运算
数值计算
类型转换
日期函数
条件函数
字符串函数
统计函数

聚合函数
函数处理的数据粒度为多条记录。
sum()—求和
count()—求数据量
avg()—求平均直
distinct—求不同值数
min—求最小值
max—求最人值

集合函数
复合类型构建
复杂类型访问
复杂类型长度

特殊函数
窗口函数
应用场景
用于分区排序
动态Group By
Top N
累计计算
层次查询

Windowing functions

lead

lag

FIRST_VALUE

LAST_VALUE

分析函数
Analytics functions

RANK

ROW_NUMBER

DENSE_RANK

CUME_DIST

PERCENT_RANK

NTILE

混合函数
java_method(class,method [,arg1 [,arg2])reflect(class,method [,arg1 [,arg2..]])hash(a1 [,a2…])

UDTF
lateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (‘,‘ columnAlias)* fromClause: FROM baseTable (lateralView)*

ateral view用于和split, explode等UDTF一起使用，它能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。lateral view首先为原始表的每行调用UDTF，UTDF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。

常用函数Demo：
create table employee(

id string,

money double,

type string)row format delimited

fields terminated by ‘\t‘

lines terminated by ‘\n‘

stored as textfile;load data local inpath ‘/liguodong/hive/data‘ into table employee;select * from employee;

优先级依次为NOT AND ORselect id,money from employee where (id=‘1001‘ or id=‘1002‘) and money=‘100‘;





cast类型转换

select cast(1.5 as int);





if判断

if(con,‘‘,‘‘);

hive (default)> select if(2>1,‘YES‘,‘NO‘);

YES

case when con then ‘‘ when con then ‘‘ else ‘‘ end (‘‘里面类型要一样)

select case when id=‘1001‘ then ‘v1001‘ when id=‘1002‘ then ‘v1002‘ else ‘v1003‘ end from employee;



get_json_object

get_json_object(json 解析函数，用来处理json，必须是json格式)select get_json_object(‘{“name”:”jack”,”age”:”20”}‘,‘$.name‘);



URL解析函数

parse_url(string urlString, string partToExtract [, string keyToExtract])

select parse_url(‘http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1‘, ‘HOST‘) from

employee limit 1;



字符串连接函数： concat
语法: concat(string A, string B…)
返回值: string
说明：返回输入字符串连接后的结果，支持任意个输入字符串
举例：

hive> select concat(‘abc‘,‘def’,‘gh‘) from lxw_dual;

abcdefgh

带分隔符字符串连接函数： concat_ws
语法: concat_ws(string SEP, string A, string B…)
返回值: string
说明：返回输入字符串连接后的结果， SEP 表示各个字符串间的分隔符

concat_ws(string SEP, array)

举例：

hive> select concat_ws(‘,‘,‘abc‘,‘def‘,‘gh‘) from lxw_dual;

abc,def,gh



列出该字段所有不重复的值，相当于去重

collect_set(id) //返回的是数组

列出该字段所有的值，列出来不去重

collect_list(id) //返回的是数组

select collect_set(id) from taborder;





求和

sum(money)

统计列数

count(*)

select sum(num),count(*) from taborder;



窗口函数

first_value(第一行值)

first_value(money) over (partition by id order by money)

select ch,num,first_value(num) over (partition by ch order by num) from taborder;





rows between 1 preceding and 1 following (当前行以及当前行的前一行与后一行)

hive (liguodong)> select ch,num,first_value(num) over (partition by ch order by num ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) from taborder;



last_value 最后一行值

hive (liguodong)> select ch,num,last_value(num) over (partition by ch) from taborder;



lead

去当前行后面的第二行的值

lead(money,2) over (order by money)

lag

去当前行前面的第二行的值

lag(money,2) over (order by money)

“`

“`

select ch, num, lead(num,2) over (order by num) from taborder;



select ch, num, lag(num,2) over (order by num) from taborder;



rank排名

rank() over(partition by id order by money)

select ch, num, rank() over(partition by ch order by num) as rank from taborder;



select ch, num, dense_rank() over(partition by ch order by num) as dense_rank from taborder;



cume_dist

cume_dist (相同值的最大行号/行数)

cume_dist() over (partition by id order by money)

percent_rank (相同值的最小行号-1)/(行数-1)

第一个总是从0开始

percent_rank() over (partition by id order by money)

select ch,num,cume_dist() over (partition by ch order by num) as cume_dist,

percent_rank() over (partition by ch order by num) as percent_rank

from taborder;



ntile分片

ntile(2) over (order by money desc) 分两份

select ch,num,ntile(2) over (order by num desc) from taborder;



混合函数

select id,java_method(“java.lang,Math”,”sqrt”,cast(id as double)) as sqrt from hiveTest;

UDTF

select id,adid

from employee

lateral view explode(split(type,‘B‘)) tt as adid;

explode 把一列转成多行

hive (liguodong)> select id,adid

​ > from hiveDemo

​ > lateral view explode(split(str,‘,‘)) tt as adid;



正则表达式
使用正则表达式的函数
regexp_replace(string subject A,string B,string C)
regexp_extract(string subject,string pattern,int index)

hive> select regexp_replace(‘foobar‘, ‘oo|ar‘, ‘‘) from lxw_dual;

fb

hive> select regexp_replace(‘979|7.10.80|8684‘, ‘.\|(.)‘,1) from hiveDemo limit 1;



hive> select regexp_replace(‘979|7.10.80|8684‘, ‘(.?)\|(.)‘,1) from hiveDemo limit 1;



4.1 内置运算符
内容较多，见《Hive官方文档》

4.2 内置函数
内容较多，见《Hive官方文档》

测试各种内置函数的快捷方法：

1、创建一个dual表

create table dual(id string);

2、load一个文件（一行，一个空格）到dual表

3、select substr(‘angelababy’,2,3) from dual;

4.3 Hive自定义函数和Transform
当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。

4.3.1 自定义函数类别
UDF 作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数）

UDAF（用户定义聚集函数）：接收多个输入数据行，并产生一个输出数据行。（count，max）

4.3.2 UDF开发实例
UDF类源码注解摘抄:

A User-defined function (UDF) for use with Hive.
New UDF classes need to inherit from this UDF class (or from {@link org.apache.hadoop.hive.ql.udf.generic.GenericUDF GenericUDF} which provides more flexibility at the cost of more complexity).
Requirements for all classes extending this UDF are:
Implement one or more methods named {@code evaluate} which will be called by Hive (the exact way in which Hive resolves the method to call can be configured by setting a custom {@link UDFMethodResolver}). The following are some examples:
{@code public int evaluate();}
{@code public int evaluate(int a);}
{@code public double evaluate(int a, double b);}
{@code public String evaluate(String a, int b, Text c);}
{@code public Text evaluate(String a);}
{@code public String evaluate(List a);} (Note that Hive Arrays are represented as {@link java.util.List Lists} in Hive. So an {@code ARRAY} column would be passed in as a {@code List}.)
{@code evaluate} should never be a void method. However it can return {@code null} if needed.
Return types as well as method arguments can be either Java primitives or the corresponding {@link org.apache.hadoop.io.Writable Writable} class.
One instance of this class will be instantiated per JVM and it will not be called concurrently. @see Description @see UDFType
1
2
3
4
5
6
7
8
9
10
11
12
13
注意：需要暴露的evaluate方法的访问修饰符请设置为public

1、先开发一个java类，继承UDF，并以evaluate为方法名书写自定义函数功能：

package cn.it.bigdata.udf
import org.apache.hadoop.hive.ql.exec.UDF;
import org.apache.hadoop.io.Text;
public final class Lower extends UDF{
    public Text evaluate(final Text s){
        if(s==null){return null;}
        return new Text(s.toString().toLowerCase());
    }
}
1
2
3
4
5
6
7
8
9
2、打成jar包上传到服务器

3、将jar包添加到hive的classpath

hive>add JAR /home/hadoop/udf.jar;

4、创建临时函数与开发好的java class关联(调用udf时带上不同的参数时对应着关联中指定的类中以evaluate相同参数的方法)

Hive>create temporary function tolowercase as 'cn.it.bigdata.udf.ToProvince';
1
5、即可在hql中使用自定义的函数strip

Select strip(name),age from t_test;

4.3.3 Transform实现
Hive的 TRANSFORM 关键字提供了在SQL中调用自写脚本的功能

适合实现Hive中没有的功能又不想写UDF的情况

使用示例1：下面这句sql就是借用了weekday_mapper.py对数据进行了处理.

CREATE TABLE u_data_new (
  movieid INT,
  rating INT,
  weekday INT,
  userid INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';

add FILE weekday_mapper.py;

INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (movieid , rate, timestring,uid)
  USING 'python weekday_mapper.py'
  AS (movieid, rating, weekday,userid)
FROM t_rating;
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
其中weekday_mapper.py内容如下

#!/bin/python
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  movieid, rating, unixtime,userid = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print '\t'.join([movieid, rating, str(weekday),userid])
1
2
3
4
5
6
7
8
9
使用示例2：下面的例子则是使用了shell的cat命令来处理数据

CREATE TABLE u_data_new (
  movieid INT,
  rating INT,
  weekday INT,
  userid INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';

add FILE weekday_mapper.py;

INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (movieid , rate, timestring,uid)
  USING 'python weekday_mapper.py'
  AS (movieid, rating, weekday,userid)
FROM t_rating;
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
其中中weekday_mapper.py文件内容如下

#!/bin/python
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  movieid, rating, unixtime,userid = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print '\t'.join([movieid, rating, str(weekday),userid])
1
2
3
4
5
6
7
8
9
5. Hive实战
Hive 实战案例1——数据ETL
需求：
对web点击流日志基础数据表进行etl（按照仓库模型设计）

按各时间维度统计来源域名top10

已有数据表 “t_orgin_weblog” ：

+------------------+------------+----------+--+
|     col_name     | data_type  | comment  |
+------------------+------------+----------+--+
| valid            | string     |          |
| remote_addr      | string     |          |
| remote_user      | string     |          |
| time_local       | string     |          |
| request          | string     |          |
| status           | string     |          |
| body_bytes_sent  | string     |          |
| http_referer     | string     |          |
| http_user_agent  | string     |          |
+------------------+------------+----------+--+
1
2
3
4
5
6
7
8
9
10
11
12
13
数据示例：
| true|1.162.203.134| - | 18/Sep/2013:13:47:35| /images/my.jpg                        | 200| 19939 | "http://www.angularjs.cn/A0d9"                      | "Mozilla/5.0 (Windows   |

| true|1.202.186.37 | - | 18/Sep/2013:15:39:11| /wp-content/uploads/2013/08/windjs.png| 200| 34613 | "http://cnodejs.org/topic/521a30d4bee8d3cb1272ac0f" | "Mozilla/5.0 (Macintosh;|
1
2
3
实现步骤：
1、对原始数据进行抽取转换

–将来访url分离出host ,path,query,query id 字段

drop table if exists t_etl_referurl;
create table t_etl_referurl as
SELECT a.*,b.*
    FROM t_orgin_weblog a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, "\"", ""), 'HOST', 'PATH','QUERY', 'QUERY:id') b
    as host, path, query, query_id
1
2
3
4
5
2、从前述步骤进一步分离出日期时间形成ETL明细表“t_etl_detail” day tm

drop table if exists t_etl_detail;
create table t_etl_detail as
select b.*,substring(time_local,0,11) as daystr,
substring(time_local,13) as tmstr,
substring(time_local,4,3) as month,
substring(time_local,0,2) as day,
substring(time_local,13,2) as hour
from t_etl_referurl b;
1
2
3
4
5
6
7
8
3、对etl数据进行分区(包含所有数据的结构化信息)

drop table t_etl_detail_prt;
create table t_etl_detail_prt(
valid                   string,
remote_addr            string,
remote_user            string,
time_local               string,
request                 string,
status                  string,
body_bytes_sent         string,
http_referer             string,
http_user_agent         string,
host                   string,
path                   string,
query                  string,
query_id               string,
daystr                 string,
tmstr                  string,
month                  string,
day                    string,
hour                   string)
partitioned by (mm string,dd string);
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
导入数据

insert into table t_etl_detail_prt partition(mm='Sep',dd='18')
    select * from t_etl_detail where daystr='18/Sep/2013';
insert into table t_etl_detail_prt partition(mm='Sep',dd='19')
    select * from t_etl_detail where daystr='19/Sep/2013';
1
2
3
4
分成多个时间维度统计各referer_host的访问次数并排序

create table t_refer_host_visit_top_tmp
    as select referer_host,count(*) as counts,mm,dd,hh
            from t_display_referer_counts
            group by hh,dd,mm,referer_host
            order by hh asc,dd asc,mm asc,counts desc;
1
2
3
4
5
4、来源访问次数topn各时间维度URL

​ 取各时间维度的referer_host访问次数topn

select *
    from (
        select referer_host,counts,concat(hh,dd),row_number() over (partition by concat(hh,dd)
            order by concat(hh,dd) asc
        )
    as od from t_refer_host_visit_top_tmp) t
    where od<=3;
1
2
3
4
5
6
7
Hive 实战案例2——访问时长统计
需求：
从web日志中统计每日访客平均停留时间

实现步骤：
1、 由于要从大量请求中分辨出用户的各次访问，逻辑相对复杂，通过hive直接实现有困难，因此编写一个mr程序来求出访客访问信息（详见代码）

启动mr程序获取结果：

[hadoop@hdp-node-01 ~]$ hadoop jar weblog.jar cn.it.bigdata.hive.mr.UserStayTime /weblog/input /weblog/stayout
1
2、 将mr的处理结果导入hive表

drop table t_display_access_info_tmp;
create table t_display_access_info_tmp(
    remote_addr string,firt_req_time string,last_req_time string,stay_long bigint)
row format delimited fields terminated by '\t';
load data inpath '/weblog/stayout4' into table t_display_access_info_tmp;
1
2
3
4
5
3、得出访客访问信息表 “t_display_access_info”

由于有一些访问记录是单条记录，mr程序处理处的结果给的时长是0，所以考虑给单次请求的停留时间一个默认市场30秒

drop table t_display_access_info;
create table t_display_access_info as
select remote_addr,firt_req_time,last_req_time,
case stay_long
when 0 then 30000
else stay_long
end as stay_long
from t_display_access_info_tmp;
1
2
3
4
5
6
7
8
4、统计所有用户停留时间平均值

select avg(stay_long) from t_display_access_info;
1
Hive实战案例3——级联求和
需求：
有如下访客访问次数统计表 t_access_times

访客	月份	访问次数
A	2015-01-02	5
A	2015-01-03	15
B	2015-01-01	5
A	2015-01-04	8
B	2015-01-05	25
A	2015-01-06	5
A	2015-02-02	4
A	2015-02-06	6
B	2015-02-06	10
B	2015-02-07	5
……	……	……
需要输出报表：t_access_times_accumulate

访客	月份	月访问总计	累计访问总计
A	2015-01	33	33
A	2015-02	10	43
…….	…….	…….	…….
B	2015-01	30	30
B	2015-02	15	45
…….	…….	…….	…….
实现步骤
可以用一个hql语句即可实现：

select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulate
from
(select username,month,sum(salary) as salary from t_access_times group by username,month) A
inner join
(select username,month,sum(salary) as salary from t_access_times group by username,month) B
on
A.username=B.username
where B.month <= A.month
group by A.username,A.month
order by A.username,A.month;

 ====================================================================================================================================

 https://blog.csdn.net/doveyoung8/article/details/80014442

 hive UDTF函数

 之前说过HIVE，UDF(User-Defined-Function)函数的编写和使用，现在来看看UDTF的编写和使用。

 1. UDTF介绍

 UDTF(User-Defined Table-Generating Functions) 用来解决 输入一行输出多行(On-to-many maping) 的需求。

 2. 编写自己需要的UDTF

 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF,实现initialize, process, close三个方法。

 UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）。

 初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward()调用产生一行；如果产生多列可以将多个列的值放在一个数组中，然后将该数组传入到forward()函数。

 最后close()方法调用，对需要清理的方法进行清理。


 下面是我写的一个用来切分”key:value;key:value;”这种字符串，返回结果为key, value两个字段。供参考：


 import java.util.ArrayList;

 import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
 import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

 public class UDTFExplode extends GenericUDTF {

     @Override
     public void close() throws HiveException {
         // TODO Auto-generated method stub

     }

     @Override
     public void process(Object[] args) throws HiveException {
         // TODO Auto-generated method stub
         String input = args[0].toString();
         String[] test = input.split(";");
         for (int i = 0; i < test.length; i++) {
             try {
                 String[] result = test[i].split(":");
                 forward(result);
             } catch (Exception e) {
                 continue;
             }
         }

     }

     @Override
     public StructObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException {
         if (args.length != 1) {
             throw new UDFArgumentLengthException("ExplodeMap takes only one argument");
         }
         if (args[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
             throw new UDFArgumentException("ExplodeMap takes string as a parameter");
         }

         ArrayList<String> fieldNames = new ArrayList<String>();
         ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();
         fieldNames.add("col1");
         fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
         fieldNames.add("col2");
         fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

         return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
     }

 }
 复制代码
 3.使用方法

 将程序打成JAR包，然后上传服务器。添加UDF函数：



 UDTF有两种使用方法，一种直接放到select后面，一种和lateral view一起使用。

 1：直接select中使用



 select split_test('asd:123\;rtrt:3445\;vbvx:6787') as (col1,col2) from finance.dual;
 需要注意的是UDTF不可以添加其他字段使用，不可以嵌套调用，不可以和group by/cluster by/distribute by/sort by一起使用

 2：和lateral view一起使用



 select '1', mytable.col1, mytable.col2 from finance.dual lateral view split_test('asd:123\;rtrt:3445\;vbvx:6787') as (col1,col2) mytable as col1, col2;
 执行过程相当于单独执行了两次抽取，然后union到一个表里。



 在大规模数据量的数据分析及建模任务中，往往针对全量数据进行挖掘分析时会十分耗时和占用集群资源，因此一般情况下只需要抽取一小部分数据进行分析及建模操作。Hive提供了数据取样（SAMPLING）的功能，能够根据一定的规则进行数据抽样，目前支持数据块抽样，分桶抽样和随机抽样，具体如下所示：

 数据块抽样（tablesample()函数）
 1） tablesample(n percent) 根据hive表数据的大小按比例抽取数据，并保存到新的hive表中。如：抽取原hive表中10%的数据
 （注意：测试过程中发现，select语句不能带where条件且不支持子查询，可通过新建中间表或使用随机抽样解决）
 create table xxx_new as select * from xxx tablesample(10 percent)
 2）tablesample(n M) 指定抽样数据的大小，单位为M。
 3）tablesample(n rows) 指定抽样数据的行数，
 其中n代表每个map任务均取n行数据，
 map数量可通过hive表的简单查询语句确认（关键词：number of mappers: x)

 分桶抽样
 hive中分桶其实就是根据某一个字段Hash取模，放入指定数据的桶中，比如将表table_1按照ID分成100个桶，其算法是hash(id) % 100，这样，hash(id) % 100 = 0的数据被放到第一个桶中，hash(id) % 100 = 1的记录被放到第二个桶中。创建分桶表的关键语句为：CLUSTER BY语句。

 分桶抽样语法：
 TABLESAMPLE (BUCKET x OUT OF y [ON colname])
 其中x是要抽样的桶编号，桶编号从1开始，colname表示抽样的列，y表示桶的数量。
 例如：将表随机分成10组，抽取其中的第一个桶的数据
 select * from table_01 tablesample(bucket 1 out of 10 on rand())

 随机抽样（rand()函数）
 1）使用rand()函数进行随机抽样，limit关键字限制抽样返回的数据，其中rand函数前的distribute和sort关键字可以保证数据在mapper和reducer阶段是随机分布的，案例如下：
 select * from table_name where col=xxx distribute by rand() sort by rand() limit num;
 2）使用order 关键词
 案例如下：
 select * from table_name where col=xxx order by rand() limit num;
 经测试对比，千万级数据中进行随机抽样 order by方式耗时更长，大约多30秒左右。



 rand() 函数

 生成一个0-1之间的随机数，可设定随机种子。

 利用这个函数在hive 中进行随机抽样。

 test1  简单随机抽样

 SELECT t.varx,t.a
 FROM(
     SELECT varx,rand() a
     FROM tablename)t
 WHERE t.a BETWEEN 0 AND 0.2
 这样就抽取了五分之一的数据。

 --或者像这样随机抽取100条数据，与limit结合使用

 SELECT distinct a.*
 FROM table a
 ORDER BY rand(222)
 limit 100
 test2  数据块取样（Block Sampling）

 关键字 TABLESAMPLE

 SELECT * FROM table1 TABLESAMPLE (30M)

 SELECT * FROM table1 TABLESAMPLE (15 PERCENT)

 SELECT COUNT(1) FROM (SELECT * FROM lxw1 TABLESAMPLE (200 ROWS)) x --不懂
 SELECT COUTN(2) FROM table1 TABLESAMPLE (BUCKET 1 OUT OF 20 ON RAND()) -- 分桶20抽取第2桶
 test3  系统抽样

 mod,rand() 依照userrid取模，分5组，每组随机抽取100个用户，实现如：

 select *
   from(
       select refund_id,user_id,mod,rank_num from
       (select refund_id,user_id,cast(10+rand()*100 as double) rank_num,
         user_id%5 as mod --依据user_id，取模，获取 mod
         from table1)
       distribute by mod sort by mod,rank_num desc  --根据mod分组，并排序
       ) a
 where row_number(mod)<=20; --从每个mod里面抽取20个
 test4 分层抽样


=======================
Hive源码系列（七）编译模块之词法、语法解析 （中）
这篇主要举实际案例说明怎么使用antlr工具、利用antlr生成的Lexer、Parser、TreeParser代码，获取asttree。这些都是hive获取asttree的过程，理解了这些，再理解hive的asttree就很容易了
程序设计语言入门小案例一般都用“Hello World”，在编译领域的入门往往选择计算器。而我们这次的小案例就更简单：一个只能计算【两】个【整数】相【加】的计算器，比如：计算1+1...
先来考虑一下如果何下手，在我们的计算器中，只接受输入整数和加号，其它的一概不理。这里说的是整数，如果输入了一个字母，我们一定是要拒绝的...对于这一块要写对应的词法规则，这个阶段的过程就叫做词法分析
输入满足词法的规则，并不代表我们就能接受，如果是【加号】【整数】【整数】或者【整数】【整数】【加号】这样的排列，我们是不能接受的，这里接受的合法语法是【整数】【加号】【整数】，因此我们需要在词法规则的基础上再定义语法规则，规则定输入满足这样句式的才算是合法... 我们把这个阶段叫做语法分析
弄清楚了我们的词法、语法规则后，我们需要以antlr的语言把这些写出来。
antlr语法博大精深，我觉得没必要钻的太深，实现这个案例，用不到antlr太多复杂的语法，只用看懂就行

1、使用antlrworks

双击antlrworks-1.5.1.jar启动antlrworks

File-->New







选择ANTLR 3 Grammer  (*.g)  新建Calculator语法文件







输入规则：







ctrl+s 保存







Antlr的语法文件通常会保存在一个 .g的文件中，我们的语法文件叫做 Caculator.g，保存在E:\hive\anltr\calculator 目录 下

在E:\hive\anltr\calculator目录 创建output目录

File-->Preferences设置Output path







运行调试，点击图标中的小甲虫







在弹出来的调试界面中，选择 text

输入 1+2













之后将会在output窗口看到被识别出来的token流，以及具体语法分析树和ASTTree的结果











到此，就是简单使用anltrworks用语法文件来解析输入数据的过程



2、使用eclipse



新建一个java项目，antlr-my

File-->New-->Java Project







在antlr-my项目下创建grammar、lib







下载antlr-3.4-complete.jar   hive中用的是antlr3.4

http://www.java2s.com/Code/Jar/a/Downloadantlr34completejar.htm

将antlr-3.4-complete.jar放在lib下面







创建语法文件：

在grammar上面右键， New -> Other,选择 ANTLR中的Combined Grammar



输入文件名Caculator





设置antlr-my项目相关的antlr插件检查设置：

在antlr-my项目上右键 -->Properties-->ANTLR













以上设置完了之后，点ok

在Calculator.g文件中输入内容：







点击Interpreter,在expr区域输入表达式1+2 点击执行按钮：







我们可以看到下图中具体语法分析树的结果







3、用java来获取ASTTree



antlr自动生成了词法分析器CalculatorLexer.java和语法分析器 CalculatorParser.java的代码







创建测试类：TestCalculator.java







能够获取生成的ASTTree








微信扫一扫
关注该公众号

====================
dy 章 Spark数据分析导论 1
1．1 Spark是什么 1
1．2 一个大一统的软件栈 2
1．2．1 Spark Core 2
1．2．2 Spark SQL 3
1．2．3 Spark Streaming 3
1．2．4 MLlib 3
1．2．5 GraphX 3
1．2．6 集群管理器 4
1．3 Spark的用户和用途 4
1．3．1 数据科学任务 4
1．3．2 数据处理应用 5
1．4 Spark简史 5
1．5 Spark的版本和发布 6
1．6 Spark的存储层次 6
第2章 Spark下载与入门 7
2．1 下载Spark 7
2．2 Spark中Python和Scala的shell 9
2．3 Spark 核心概念简介 12
2．4 独立应用 14
2．4．1 初始化SparkContext 15
2．4．2 构建独立应用 16
2．5 总结 19
第3章 RDD编程 21
3．1 RDD基础 21
3．2 创建RDD 23
3．3 RDD操作 24
3．3．1 转化操作 24
3．3．2 行动操作 26
3．3．3 惰性求值 27
3．4 向Spark传递函数 27
3．4．1 Python 27
3．4．2 Scala 28
3．4．3 Java 29
3．5 常见的转化操作和行动操作 30
3．5．1 基本RDD 30
3．5．2 在不同RDD类型间转换 37
3．6 持久化( 缓存) 39
3．7 总结 40
第4章 键值对操作 41
4．1 动机 41
4．2 创建Pair RDD 42
4．3 Pair RDD的转化操作 42
4．3．1 聚合操作 45
4．3．2 数据分组 49
4．3．3 连接 50
4．3．4 数据排序 51
4．4 Pair RDD的行动操作 52
4．5 数据分区（进阶） 52
4．5．1 获取RDD的分区方式 55
4．5．2 从分区中获益的操作 56
4．5．3 影响分区方式的操作 57
4．5．4 示例：PageRank 57
4．5．5 自定义分区方式 59
4．6 总结 61
第5章 数据读取与保存 63
5．1 动机 63

-----------

文档中心  弹性 MapReduce  EMR 开发教程  Hue 使用指南  Hive 元数据管理
Hive 元数据管理
最近更新时间：2018-07-05 20:30:31

 编辑   查看pdf
使用 Hue 之前，您首先需要通过 EMR 的快捷入口登录Hue，且 Hue 上的用户名建议是 hadoop。

进入 Hue 的 Hive 编辑模式，并选择对应的数据库

选择数据库

在 Hue 的 Hive SQL 编辑框里面输入 hive-SQL 创建表和导入数据

 drop table hive_test;
create table hive_test (a int, b string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ’,’;
load data local inpath "/usr/local/service/hadoop/bin/hive_test.data" into table hive_t
单击执行按钮如下图：

执行

Hue 可以将查询结果以图表的方式展示，执行这个 SQL

 select * from hive_test limit 10;
执行完成后，结果数据如图所示：

执行结果

选择图例模式，在这里选择了"pie"，可以看到结果图

结果图

==============
文档中心  弹性 MapReduce  EMR 开发教程  Hive 开发指南  Hive 最佳实践
Hive 最佳实践
最近更新时间：2017-11-17 10:42:44

 编辑   查看pdf
执行引擎设置
腾讯云 EMR 中的 Hive 目前支持三种执行引擎：

MR
TEZ
Spark
如果需要 TEZ 那么在初始购买集群的时候需要勾选 TEZ，在普通情况下建议执行引擎为 TEZ，这样您会获得更好的计算效率。

存储选择
腾讯云存储介质目前支持本地数据盘、普通云硬盘、SSD 云硬盘以及 COS 对象存储，如果您对成本敏感，那么基于 COS 的数据仓库方式是一个不错的选择

数据格式
腾讯云压缩支持 snappy、lzo 等压缩算法，如果使用 Hive 建议您的数据文件格式使用 ORC 或者 parquet 的格式，这样您会更节省空间以及会获得更好的计算效率

查询引擎如何选择
腾讯云 EMR 目前支持的查询引擎有 Presto、SparkSQL、Hive，如果您想实现多种数据源耦合查询建议您使用 Presto，如果普通数据仓库建议您使用 Hive+TEZ 的模式，如果您对时延比较敏感可以考虑SparkSQL。

数据安全
如果您是使用 COS 作为底层存储，建议您使用外部表的方式，以免误删数据，而如果是存储在 HDFS 那么建议您开启 HDFS 回收站来避免数据误删除。



==============