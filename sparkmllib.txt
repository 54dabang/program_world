11．1　概述　　187
11．2　系统要求　　188
11．3　机器学习基础　　189
11．4　数据类型　　192
11．5　算法　　194
11．5．1　特征提取　　194
11．5．2　统计　　196
11．5．3　分类与回归　　197
11．5．4　聚类　　202
11．5．5　协同过滤与推荐　　203
11．5．6　降维　　204
11．5．7　模型评估　　206
11．6　一些提示与性能考量　　206
11．6．1　准备特征　　206
11．6．2　配置算法　　207
11．6．3　缓存RDD以重复使用　　207
11．6．4　识别稀疏程度　　207
11．6．5　并行度　　207
11．7　流水线API　　208
11．8　总结　　209

第一部分　Spark MLlib基础
第1章　Spark机器学习简介 2
1．1　机器学习介绍 2
1．2　Spark介绍 3
1．3　Spark MLlib介绍 4
第2章　Spark数据操作 6
2．1　Spark RDD操作 6
2．1．1　Spark RDD创建操作 6
2．1．2　Spark RDD转换操作 7
2．1．3　Spark RDD行动操作 14
2．2　MLlib Statistics统计操作 15
2．2．1　列统计汇总 15
2．2．2　相关系数 16
2．2．3　假设检验 18
2．3　MLlib数据格式 18
2．3．1　数据处理 18
2．3．2　生成样本 22
第3章　Spark MLlib矩阵向量 26
3．1　Breeze介绍 26
3．1．1　Breeze创建函数 27
3．1．2　Breeze元素访问及操作函数 29
3．1．3　Breeze数值计算函数 34
3．1．4　Breeze求和函数 35
3．1．5　Breeze布尔函数 36
3．1．6　Breeze线性代数函数 37
3．1．7　Breeze取整函数 39
3．1．8　Breeze常量函数 40
3．1．9　Breeze复数函数 40
3．1．10　Breeze三角函数 40
3．1．11　Breeze对数和指数函数 40
3．2　BLAS介绍 41
3．2．1　BLAS向量-向量运算 42
3．2．2　BLAS矩阵-向量运算 42
3．2．3　BLAS矩阵-矩阵运算 43
3．3　MLlib向量 43
3．3．1　MLlib向量介绍 43
3．3．2　MLlib Vector接口 44
3．3．3　MLlib DenseVector类 46
3．3．4　MLlib SparseVector类 49
3．3．5　MLlib Vectors伴生对象 50
3．4　MLlib矩阵 57
3．4．1　MLlib矩阵介绍 57
3．4．2　MLlib Matrix接口 57
3．4．3　MLlib DenseMatrix类 59
3．4．4　MLlib SparseMatrix类 64
3．4．5　MLlib Matrix伴生对象 71
3．5　MLlib BLAS 77
3．6　MLlib分布式矩阵 93
3．6．1　MLlib分布式矩阵介绍 93
3．6．2　行矩阵（RowMatrix） 94
3．6．3　行索引矩阵（IndexedRowMatrix） 96
3．6．4　坐标矩阵（CoordinateMatrix） 97
3．6．5　分块矩阵（BlockMatrix） 98

第二部分　Spark MLlib回归算法
第4章　Spark MLlib线性回归算法 102
4．1　线性回归算法 102
4．1．1　数学模型 102
4．1．2　最小二乘法 105
4．1．3　梯度下降算法 105
4．2　源码分析 106
4．2．1　建立线性回归 108
4．2．2　模型训练run方法 111
4．2．3　权重优化计算 114
4．2．4　线性回归模型 121
4．3　实例 123
4．3．1　训练数据 123
4．3．2　实例代码 123
第5章　Spark MLlib逻辑回归算法 126
5．1　逻辑回归算法 126
5．1．1　数学模型 126
5．1．2 梯度下降算法 128
5．1．3　正则化 129
5．2　源码分析 132
5．2．1　建立逻辑回归 134
5．2．2　模型训练run方法 137
5．2．3　权重优化计算 137
5．2．4　逻辑回归模型 144
5．3　实例 148
5．3．1　训练数据 148
5．3．2　实例代码 148
第6章　Spark MLlib保序回归算法 151
6．1　保序回归算法 151
6．1．1　数学模型 151
6．1．2　L2保序回归算法 153
6．2　源码分析 153
6．2．1　建立保序回归 154
6．2．2　模型训练run方法 156
6．2．3　并行PAV计算 156
6．2．4　PAV计算 157
6．2．5　保序回归模型 159
6．3　实例 164
6．3．1　训练数据 164
6．3．2　实例代码 164

第三部分　Spark MLlib分类算法
第7章　Spark MLlib贝叶斯分类算法 170
7．1　贝叶斯分类算法 170
7．1．1　贝叶斯定理 170
7．1．2　朴素贝叶斯分类 171
7．2　源码分析 173
7．2．1　建立贝叶斯分类 173
7．2．2　模型训练run方法 176
7．2．3　贝叶斯分类模型 179
7．3　实例 181
7．3．1　训练数据 181
7．3．2　实例代码 182
第8章　Spark MLlib SVM支持向量机算法 184
8．1　SVM支持向量机算法 184
8．1．1　数学模型 184
8．1．2　拉格朗日 186
8．2　源码分析 189
8．2．1　建立线性SVM分类 191
8．2．2　模型训练run方法 194
8．2．3　权重优化计算 194
8．2．4　线性SVM分类模型 196
8．3　实例 199
8．3．1　训练数据 199
8．3．2　实例代码 199
第9章　Spark MLlib决策树算法 202
9．1　决策树算法 202
9．1．1　决策树 202
9．1．2　特征选择 203
9．1．3　决策树生成 205
9．1．4　决策树生成实例 206
9．1．5　决策树的剪枝 208
9．2　源码分析 209
9．2．1　建立决策树 211
9．2．2　建立随机森林 216
9．2．3　建立元数据 220
9．2．4　查找特征的分裂及划分 223
9．2．5　查找最好的分裂顺序 228
9．2．6　决策树模型 231
9．3　实例 234
9．3．1　训练数据 234
9．3．2　实例代码 234

第四部分　Spark MLlib聚类算法
第10章　Spark MLlib KMeans聚类算法 238
10．1　KMeans聚类算法 238
10．1．1　KMeans算法 238
10．1．2　演示KMeans算法 239
10．1．3　初始化聚类中心点 239
10．2　源码分析 240
10．2．1　建立KMeans聚类 242
10．2．2　模型训练run方法 247
10．2．3　聚类中心点计算 248
10．2．4　中心点初始化 251
10．2．5　快速距离计算 254
10．2．6　KMeans聚类模型 255
10．3　实例 258
10．3．1　训练数据 258
10．3．2　实例代码 259
第11章　Spark MLlib LDA主题模型算法 261
11．1　LDA主题模型算法 261
11．1．1　LDA概述 261
11．1．2　LDA概率统计基础 262
11．1．3　LDA数学模型 264
11．2　GraphX基础 267
11．3　源码分析 270
11．3．1　建立LDA主题模型 272
11．3．2　优化计算 279
11．3．3　LDA模型 283
11．4　实例 288
11．4．1　训练数据 288
11．4．2　实例代码 288

第五部分　Spark MLlib关联规则挖掘算法
第12章　Spark MLlib FPGrowth关联规则算法 292
12．1　FPGrowth关联规则算法 292
12．1．1　基本概念 292
12．1．2　FPGrowth算法 293
12．1．3　演示FP树构建 294
12．1．4　演示FP树挖掘 296
12．2　源码分析 298
12．2．1　FPGrowth类 298
12．2．2　关联规则挖掘 300
12．2．3　FPTree类 303
12．2．4　FPGrowthModel类 306
12．3　实例 306
12．3．1　训练数据 306
12．3．2　实例代码 306

第六部分　Spark MLlib推荐算法
第13章　Spark MLlib ALS交替最小二乘算法 310
13．1　ALS交替最小二乘算法 310
13．2　源码分析 312
13．2．1　建立ALS 314
13．2．2　矩阵分解计算 322
13．2．3　ALS模型 329
13．3　实例 334
13．3．1　训练数据 334
13．3．2　实例代码 334
第14章　Spark MLlib协同过滤推荐算法 337
14．1　协同过滤推荐算法 337
14．1．1　协同过滤推荐概述 337
14．1．2　用户评分 338
14．1．3　相似度计算 338
14．1．4　推荐计算 340
14．2　协同推荐算法实现 341
14．2．1　相似度计算 344
14．2．2　协同推荐计算 348
14．3　实例 350
14．3．1　训练数据 350
14．3．2　实例代码 350

第七部分　Spark MLlib神经网络算法
第15章　Spark MLlib神经网络算法综述 354
15．1　人工神经网络算法 354
15．1．1　神经元 354
15．1．2　神经网络模型 355
15．1．3 信号前向传播 356
15．1．4　误差反向传播 357
15．1．5　其他参数 360
15．2　神经网络算法实现 361
15．2．1　神经网络类 363
15．2．2　训练准备 370
15．2．3　前向传播 375
15．2．4　误差反向传播 377
15．2．5　权重更新 381
15．2．6　ANN模型 382
15．3　实例 384
15．3．1　测试数据 384
15．3．2　测试函数代码 387
15．3．3　实例代码 388



目录
第1章 星星之火 1
1.1 大数据时代 1
1.2 大数据分析时代 2
1.3 简单、优雅、有效——这就是Spark 3
1.4 核心——MLlib 4
1.5 星星之火，可以燎原 6
1.6 小结 6
第2章 Spark安装和开发环境配置 7
2.1 Windows单机模式Spark安装和配置 7
2.1.1 Windows 7安装Java 7
2.1.2 Windows 7安装Scala 10
2.1.3 Intellij IDEA下载和安装 13
2.1.4 Intellij IDEA中Scala插件的安装 14
2.1.5 HelloJava——使用Intellij IDEA创建Java程序 18
2.1.6 HelloScala——使用Intellij IDEA创建Scala程序 21
2.1.7 *后一脚——Spark单机版安装 26
2.2 经典的WordCount 29
2.2.1 Spark实现WordCount 29
2.2.2 MapReduce实现WordCount 31
2.3 小结 34
第3章 RDD详解 35
3.1 RDD是什么 35
3.1.1 RDD名称的秘密 35
3.1.2 RDD特性 36
3.1.3 与其他分布式共享内存的区别 37
3.1.4 RDD缺陷 37
3.2 RDD工作原理 38
3.2.1 RDD工作原理图 38
3.2.2 RDD的相互依赖 38
3.3 RDD应用API详解 39
3.3.1 使用aggregate方法对给定的数据集进行方法设定 39
3.3.2 提前计算的cache方法 42
3.3.3 笛卡尔操作的cartesian方法 43
3.3.4 分片存储的coalesce方法 44
3.3.5 以value计算的countByValue方法 45
3.3.6 以key计算的countByKey方法 45
3.3.7 除去数据集中重复项的distinct方法 46
3.3.8 过滤数据的filter方法 47
3.3.9 以行为单位操作数据的flatMap方法 47
3.3.10 以单个数据为目标进行操作的map方法 48
3.3.11 分组数据的groupBy方法 48
3.3.12 生成键值对的keyBy方法 49
3.3.13 同时对两个数据进行处理的reduce方法 50
3.3.14 对数据进行重新排序的sortBy方法 51
3.3.15 合并压缩的zip方法 52
3.4 小结 53
第4章 MLlib基本概念 54
4.1 MLlib基本数据类型 54
4.1.1 多种数据类型 54
4.1.2 从本地向量集起步 55
4.1.3 向量标签的使用 56
4.1.4 本地矩阵的使用 58
4.1.5 分布式矩阵的使用 59
4.2 MLlib数理统计基本概念 62
4.2.1 基本统计量 62
4.2.2 统计量基本数据 63
4.2.3 距离计算 64
4.2.4 两组数据相关系数计算 65
4.2.5 分层抽样 67
4.2.6 假设检验 69
4.2.7 随机数 70
4.3 小结 71
第5章 协同过滤算法 72
5.1 协同过滤 72
5.1.1 协同过滤概述 72
5.1.2 基于用户的推荐 73
5.1.3 基于物品的推荐 74
5.1.4 协同过滤算法的不足 75
5.2 相似度度量 75
5.2.1 基于欧几里得距离的相似度计算 75
5.2.2 基于余弦角度的相似度计算 76
5.2.3 欧几里得相似度与余弦相似度的比较 77
5.2.4 *个例子——余弦相似度实战 77
5.3 MLlib中的交替*小二乘法（ALS算法） 80
5.3.1 *小二乘法（LS算法）详解 81
5.3.2 MLlib中交替*小二乘法（ALS算法）详解 82
5.3.3 ALS算法实战 83
5.4 小结 85
第6章 MLlib线性回归理论与实战 86
6.1 随机梯度下降算法详解 86
6.1.1 道士下山的故事 87
6.1.2 随机梯度下降算法的理论基础 88
6.1.3 随机梯度下降算法实战 88
6.2 MLlib回归的过拟合 89
6.2.1 过拟合产生的原因 90
6.2.2 lasso回归与岭回归 91
6.3 MLlib线性回归实战 91
6.3.1 MLlib线性回归基本准备 91
6.3.2 MLlib线性回归实战：商品价格与消费者收入之间的关系 94
6.3.3 对拟合曲线的验证 95
6.4 小结 97
第7章 MLlib分类实战 98
7.1 逻辑回归详解 98
7.1.1 逻辑回归不是回归算法 98
7.1.2 逻辑回归的数学基础 99
7.1.3 一元逻辑回归示例 100
7.1.4 多元逻辑回归示例 101
7.1.5 MLlib逻辑回归验证 103
7.1.6 MLlib逻辑回归实例：肾癌的转移判断 104
7.2 支持向量机详解 106
7.2.1 三角还是圆 106
7.2.2 支持向量机的数学基础 108
7.2.3 支持向量机使用示例 109
7.2.4 使用支持向量机分析肾癌转移 110
7.3 朴素贝叶斯详解 111
7.3.1 穿裤子的男生or女生 111
7.3.2 贝叶斯定理的数学基础和意义 112
7.3.3 朴素贝叶斯定理 113
7.3.4 MLlib朴素贝叶斯使用示例 114
7.3.5 MLlib朴素贝叶斯实战：“僵尸粉”的鉴定 115
7.4 小结 117
第8章 决策树与保序回归 118
8.1 决策树详解 118
8.1.1 水晶球的秘密 119
8.1.2 决策树的算法基础：信息熵 119
8.1.3 决策树的算法基础——ID3算法 121
8.1.4 MLlib中决策树的构建 122
8.1.5 MLlib中决策树示例 123
8.1.6 随机雨林与梯度提升算法（GBT） 125
8.2 保序回归详解 127
8.2.1 何为保序回归 128
8.2.2 保序回归示例 128
8.3 小结 129
第9章 MLlib中聚类详解 130
9.1 聚类与分类 130
9.1.1 什么是分类 130
9.1.2 什么是聚类 131
9.2 MLlib中的Kmeans算法 131
9.2.1 什么是kmeans算法 131
9.2.2 MLlib中Kmeans算法示例 133
9.2.3 Kmeans算法中细节的讨论 134
9.3 高斯混合聚类 135
9.3.1 从高斯分布聚类起步 135
9.3.2 混合高斯聚类 137
9.3.3 MLlib高斯混合模型使用示例 137
9.4 快速迭代聚类 138
9.4.1 快速迭代聚类理论基础 138
9.4.2 快速迭代聚类示例 139
9.5 小结 140
第10章 MLlib中关联规则 141
10.1 Apriori频繁项集算法 141
10.1.1 啤酒与尿布 141
10.1.2 经典的Apriori算法 142
10.1.3 Apriori算法示例 144
10.2 FP-growth算法 145
10.2.1 Apriori算法的局限性 145
10.2.2 FP-growth算法 145
10.2.3 FP树示例 148
10.3 小结 149
第11章 数据降维 150
11.1 奇异值分解（SVD） 150
11.1.1 行矩阵（RowMatrix）详解 150
11.1.2 奇异值分解算法基础 151
11.1.3 MLlib中奇异值分解示例 152
11.2 主成分分析（PCA） 153
11.2.1 主成分分析（PCA）的定义 154
11.2.2 主成分分析（PCA）的数学基础 154
11.2.3 MLlib中主成分分析（PCA）示例 155
11.3 小结 156
第12章 特征提取和转换 157
12.1 TF-IDF 157
12.1.1 如何查找所要的新闻 157
12.1.2 TF-IDF算法的数学计算 158
12.1.3 MLlib中TF-IDF示例 159
12.2 词向量化工具 160
12.2.1 词向量化基础 160
12.2.2 词向量化使用示例 161
12.3 基于卡方检验的特征选择 162
12.3.1 “吃货”的苦恼 162
12.3.2 MLlib中基于卡方检验的特征选择示例 163
12.4 小结 164
第13章 MLlib实战演练——鸢尾花分析 166
13.1 建模说明 166
13.1.1 数据的描述与分析目标 166
13.1.2 建模说明 168
13.2 数据预处理和分析 171
13.2.1 微观分析——均值与方差的对比分析 171
13.2.2 宏观分析——不同种类特性的长度计算 174
13.2.3 去除重复项——相关系数的确定 176
13.3 长与宽之间的关系——数据集的回归分析 180
13.3.1 使用线性回归分析长与宽之间的关系 180
13.3.2 使用逻辑回归分析长与宽之间的关系 183
13.4 使用分类和聚类对鸢尾花数据集进行处理 184
13.4.1 使用聚类分析对数据集进行聚类处理 184
13.4.2 使用分类分析对数据集进行分类处理 187
13.5 *终的判定——决策树测试 188
13.5.1 决定数据集的归类——决策树 188
13.5.2 决定数据集归类的分布式方法——随机雨林 190

第1章 星星之火
1.1 大数据时代
1.2 大数据分析时代
1.3 简单、优雅、有效——这就是Spark
1.4 核心——MLlib
1.5 星星之火，可以燎原
1.6 小结
第2章 Spark安装和开发环境配置
2.1 Windows单机模式Spark安装和配置
2.2 经典的WordCount
2.3 小结
第3章 RDD详解
3.1 RDD是什么
3.2 RDD工作原理
3.3 RDD应用API详解
3.4 小结
第4章 MLlib基本概念
4.1 MLlib基本数据类型
4.2 MLlib数理统计基本概念
4.3 小结
第5章 协同过滤算法
5.1 协同过滤
5.2 相似度度量
5.3 MLlib中的交替最小二乘法（ALS算法）
5.4 小结
第6章 MLlib线性回归理论与实战
6.1 随机梯度下降算法详解
6.2 MLlib回归的过拟合
6.3 MLlib线性回归实战
6.4 小结
第7章 MLlib分类实战
7.1 逻辑回归详解
7.2 支持向量机详解
7.3 朴素贝叶斯详解
7.4 小结
第8章 决策树与保序回归
8.1 决策树详解
8.2 保序回归详解
8.3 小结
第9章 MLlib中聚类详解
9.1 聚类与分类
9.2 MLlib中的Kmeans算法
9.3 高斯混合聚类
9.4 快速迭代聚类
9.5 小结
第10章 MLlib中关联规则
10.1 Apriori频繁项集算法
10.2 FP-growth算法
10.3 小结
第11章 数据降维
11.1 奇异值分解（SVD）
11.2 主成分分析（PCA）
11.3 小结
第12章 特征提取和转换
12.1 TF-IDF
12.2 词向量化工具
12.3 基于卡方检验的特征选择
12.4 小结
第13章 MLlib实战演练——鸢尾花分析
13.1 建模说明
13.2 数据预处理和分析
13.3 长与宽之间的关系——数据集的回归分析
13.4 使用分类和聚类对鸢尾花数据集进行处理
13.5 最终的判定——决策树测试

第1章　Spark机器学习简介 2
1．1　机器学习介绍 2
1．2　Spark介绍 3
1．3　Spark MLlib介绍 4
第2章　Spark数据操作 6
2．1　Spark RDD操作 6
2．1．1　Spark RDD创建操作 6
2．1．2　Spark RDD转换操作 7
2．1．3　Spark RDD行动操作 14
2．2　MLlib Statistics统计操作 15
2．2．1　列统计汇总 15
2．2．2　相关系数 16
2．2．3　假设检验 18
2．3　MLlib数据格式 18
2．3．1　数据处理 18
2．3．2　生成样本 22
第3章　Spark MLlib矩阵向量 26
3．1　Breeze介绍 26
3．1．1　Breeze创建函数 27
3．1．2　Breeze元素访问及操作函数 29
3．1．3　Breeze数值计算函数 34
3．1．4　Breeze求和函数 35
3．1．5　Breeze布尔函数 36
3．1．6　Breeze线性代数函数 37
3．1．7　Breeze取整函数 39
3．1．8　Breeze常量函数 40
3．1．9　Breeze复数函数 40
3．1．10　Breeze三角函数 40
3．1．11　Breeze对数和指数函数 40
3．2　BLAS介绍 41
3．2．1　BLAS向量-向量运算 42
3．2．2　BLAS矩阵-向量运算 42
3．2．3　BLAS矩阵-矩阵运算 43
3．3　MLlib向量 43
3．3．1　MLlib向量介绍 43
3．3．2　MLlib Vector接口 44
3．3．3　MLlib DenseVector类 46
3．3．4　MLlib SparseVector类 49
3．3．5　MLlib Vectors伴生对象 50
3．4　MLlib矩阵 57
3．4．1　MLlib矩阵介绍 57
3．4．2　MLlib Matrix接口 57
3．4．3　MLlib DenseMatrix类 59
3．4．4　MLlib SparseMatrix类 64
3．4．5　MLlib Matrix伴生对象 71
3．5　MLlib BLAS 77
3．6　MLlib分布式矩阵 93
3．6．1　MLlib分布式矩阵介绍 93
3．6．2　行矩阵（RowMatrix） 94
3．6．3　行索引矩阵（IndexedRowMatrix） 96
3．6．4　坐标矩阵（CoordinateMatrix） 97
3．6．5　分块矩阵（BlockMatrix） 98

第二部分　Spark MLlib回归算法
第4章　Spark MLlib线性回归算法 102
4．1　线性回归算法 102
4．1．1　数学模型 102
4．1．2　最小二乘法 105
4．1．3　梯度下降算法 105
4．2　源码分析 106
4．2．1　建立线性回归 108
4．2．2　模型训练run方法 111
4．2．3　权重优化计算 114
4．2．4　线性回归模型 121
4．3　实例 123
4．3．1　训练数据 123
4．3．2　实例代码 123
第5章　Spark MLlib逻辑回归算法 126
5．1　逻辑回归算法 126
5．1．1　数学模型 126
5．1．2 梯度下降算法 128
5．1．3　正则化 129
5．2　源码分析 132
5．2．1　建立逻辑回归 134
5．2．2　模型训练run方法 137
5．2．3　权重优化计算 137
5．2．4　逻辑回归模型 144
5．3　实例 148
5．3．1　训练数据 148
5．3．2　实例代码 148
第6章　Spark MLlib保序回归算法 151
6．1　保序回归算法 151
6．1．1　数学模型 151
6．1．2　L2保序回归算法 153
6．2　源码分析 153
6．2．1　建立保序回归 154
6．2．2　模型训练run方法 156
6．2．3　并行PAV计算 156
6．2．4　PAV计算 157
6．2．5　保序回归模型 159
6．3　实例 164
6．3．1　训练数据 164
6．3．2　实例代码 164

第三部分　Spark MLlib分类算法
第7章　Spark MLlib贝叶斯分类算法 170
7．1　贝叶斯分类算法 170
7．1．1　贝叶斯定理 170
7．1．2　朴素贝叶斯分类 171
7．2　源码分析 173
7．2．1　建立贝叶斯分类 173
7．2．2　模型训练run方法 176
7．2．3　贝叶斯分类模型 179
7．3　实例 181
7．3．1　训练数据 181
7．3．2　实例代码 182
第8章　Spark MLlib SVM支持向量机算法 184
8．1　SVM支持向量机算法 184
8．1．1　数学模型 184
8．1．2　拉格朗日 186
8．2　源码分析 189
8．2．1　建立线性SVM分类 191
8．2．2　模型训练run方法 194
8．2．3　权重优化计算 194
8．2．4　线性SVM分类模型 196
8．3　实例 199
8．3．1　训练数据 199
8．3．2　实例代码 199
第9章　Spark MLlib决策树算法 202
9．1　决策树算法 202
9．1．1　决策树 202
9．1．2　特征选择 203
9．1．3　决策树生成 205
9．1．4　决策树生成实例 206
9．1．5　决策树的剪枝 208
9．2　源码分析 209
9．2．1　建立决策树 211
9．2．2　建立随机森林 216
9．2．3　建立元数据 220
9．2．4　查找特征的分裂及划分 223
9．2．5　查找最好的分裂顺序 228
9．2．6　决策树模型 231
9．3　实例 234
9．3．1　训练数据 234
9．3．2　实例代码 234

第四部分　Spark MLlib聚类算法
第10章　Spark MLlib KMeans聚类算法 238
10．1　KMeans聚类算法 238
10．1．1　KMeans算法 238
10．1．2　演示KMeans算法 239
10．1．3　初始化聚类中心点 239
10．2　源码分析 240
10．2．1　建立KMeans聚类 242
10．2．2　模型训练run方法 247
10．2．3　聚类中心点计算 248
10．2．4　中心点初始化 251
10．2．5　快速距离计算 254
10．2．6　KMeans聚类模型 255
10．3　实例 258
10．3．1　训练数据 258
10．3．2　实例代码 259
第11章　Spark MLlib LDA主题模型算法 261
11．1　LDA主题模型算法 261
11．1．1　LDA概述 261
11．1．2　LDA概率统计基础 262
11．1．3　LDA数学模型 264
11．2　GraphX基础 267
11．3　源码分析 270
11．3．1　建立LDA主题模型 272
11．3．2　优化计算 279
11．3．3　LDA模型 283
11．4　实例 288
11．4．1　训练数据 288
11．4．2　实例代码 288

第五部分　Spark MLlib关联规则挖掘算法
第12章　Spark MLlib FPGrowth关联规则算法 292
12．1　FPGrowth关联规则算法 292
12．1．1　基本概念 292
12．1．2　FPGrowth算法 293
12．1．3　演示FP树构建 294
12．1．4　演示FP树挖掘 296
12．2　源码分析 298
12．2．1　FPGrowth类 298
12．2．2　关联规则挖掘 300
12．2．3　FPTree类 303
12．2．4　FPGrowthModel类 306
12．3　实例 306
12．3．1　训练数据 306
12．3．2　实例代码 306

第六部分　Spark MLlib推荐算法
第13章　Spark MLlib ALS交替最小二乘算法 310
13．1　ALS交替最小二乘算法 310
13．2　源码分析 312
13．2．1　建立ALS 314
13．2．2　矩阵分解计算 322
13．2．3　ALS模型 329
13．3　实例 334
13．3．1　训练数据 334
13．3．2　实例代码 334
第14章　Spark MLlib协同过滤推荐算法 337
14．1　协同过滤推荐算法 337
14．1．1　协同过滤推荐概述 337
14．1．2　用户评分 338
14．1．3　相似度计算 338
14．1．4　推荐计算 340
14．2　协同推荐算法实现 341
14．2．1　相似度计算 344
14．2．2　协同推荐计算 348
14．3　实例 350
14．3．1　训练数据 350
14．3．2　实例代码 350

第七部分　Spark MLlib神经网络算法
第15章　Spark MLlib神经网络算法综述 354
15．1　人工神经网络算法 354
15．1．1　神经元 354
15．1．2　神经网络模型 355
15．1．3 信号前向传播 356
15．1．4　误差反向传播 357
15．1．5　其他参数 360
15．2　神经网络算法实现 361
15．2．1　神经网络类 363
15．2．2　训练准备 370
15．2．3　前向传播 375
15．2．4　误差反向传播 377
15．2．5　权重更新 381
15．2．6　ANN模型 382
15．3　实例 384
15．3．1　测试数据 384
15．3．2　测试函数代码 387
15．3．3　实例代码 388


机器学习中一些基础的数学
原创： 刘同学  Spark推荐系统  2018-11-12
135无圈.png

  Spark推荐系统，干货，心得
  点击上方蓝字关注～



今天整理下学习机器学习相关的文章，下篇打算整理一下spark推荐系统中是如何跟机器学习算法的融合，希望大家一起多多交流并指正~

先简单的回归一下之前的数学基础



机器学习基础

机器学习的分类与一般思路（常用的机器学习算法如有监督，无监督，强化学习，深度学习及应用场景）

微积分基础

泰勒公式、导数与梯度

概率与统计基础

概率公式、常见分布、常见统计量

线性代数基础

矩阵乘法的几何意义

机器学习另一方的名字：数据挖掘、模式识别、统计学习、

机器学习应用领域：计算机视觉、语音识别、自然语言处理、推荐系统等等

推荐书籍:



学习机器学习要了解的数学基础

方向导数 （是标量，没有方向）



梯度 (是向量，有方向)



几何意义：是函数在该点变化最快的方向

考虑一座解析式为z=H(x,y)的山,在(X0,Y0)的梯度是在该点坡度变化最快的方向

梯度下降法。（作用是 求损失函数的最小值）

思考：若下山方向和梯度为函数z=f(x,y) 在点p的梯度，记作grad f(x,y)



标量(Scalar) 是只有大小，没有方向的量，如 1,2,3等

向量(Vector) 是有大小和方向的量，如(1,2)

矩阵(Matrix)是好几个向量排成一排合并而成的一堆数字，如[1,2; 3,4]



凸函数(导数是递增的，由负数变成正数。或者二阶导数大于0)



凸函数的性质：琴生不等式





几何意义：(x,f(x))点和(y,f(y))的期望  大于等于 x,y中点的期望

就是    f(Ex)  （x和y期望的函数值）    <=     Ef(x)  （ f(x)和f(y)的函数值的期望）

期望：在概率论和统计学中，数学期望(mean)（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和，是最基本的数学特征之一。它反映随机变量平均取值的大小

为什么讲琴生不等式

因为：琴生不等式是  机器学习领域 绝大部分不等式的起源

概率与统计

概率公式

条件概率：



在B条件下发生A的概率：A、B两个条件同时发生的概率除以B发生的概率

全概率公式：



事件A的概率是  在所有条件Bi下发生A的概率(及条件概率)，再与Bi的概率乘积  然后各项加和

贝叶斯公式：



在贝叶斯算法的时候在细讲



常见的概率分布有哪些



工程上基本都会遇到这些分布

二项分布：如 二分类问题

泊松分布：如 靠近0值是一个山峰，靠近无穷值 在山脚下的图像(应用场景 某电商卖快餐，大部分价格都在20元左右，少量的商品价格高在40元左右)

松分布一种概率分布，其特点是该分布的均值等于方差

泊松分布其实是二项分布推广到无穷的形式

指数分布： x越靠近0，y值越大，x越靠近正无穷，y值越小





正态分布：正态分布是一个统计学概念，该分布由两个参数——平均值和方差决定。正态分布曲线呈钟型，两头低，中间高，左右对称，平均值决定正态曲线的中心位置；方差决定正态曲线的陡峭或扁平程度







概率与统计的关注点（机器学习思维）



基于已经知道的条件 推出具体事件中出现概率

概率问题其实就是已知模型model预测新样本中lable的思维





通过n个样本反推总体的分布情况，来估计总的均值和方差

数理统计其实这就是根据样本数据特征和lable去训练model的思维

如下图：



所以统计与机器学习是紧密交织在一起的，学习机器学习之前，建议学习下统计学

总结：





重要的统计量

期望 : 均值

方差 : 如 特征1的向量 减去 均值的平方和

协方差 ： 可以描述两个维度的相关性(如特征1与标签的相关性，特征1和特征2的相关性)

所以协方差统计是评估特征与模型相关性和选择特征的方法。如PCA算法

如果两个变量存在非线性关系，协方差是评价不出来的

相关系数是  （x,y）协方差/ |x|  与  (x,y)协方差/ |y| 的夹角的cos值 (相关系数值的区间 为-1 到 1 )

相关概念：去均值化



线性代数与矩阵论

两个矩阵相乘从几何的意义：

左边矩阵第一行每个值乘以第二个矩阵每一个列



2x - y = 1

x +  y =  5

应用：

svd 机器学习中的重要的降维算法



相关概念：奇异值矩阵(对角矩阵)

svd在降维里面细讲



Spark特征工程
原创： 刘同学  Spark推荐系统  2018-11-25
135无圈.png

  Spark推荐系统，干货，心得
  点击上方蓝字关注～



   之前好多同学一直想了解下用户画像的东西，个人理解用户画像算是在特征工程里面一个表现方面吧，学完特征工程，用户画像不在话下。以下也是我在工作中经常用的特征工程的方法，总结给大家。特征工程的应用在哪里？可以参考上一篇的推荐系统部分。互相学习，欢迎交流~



特征是什么=》数据中抽取出来的对结果预测有用的信息(实际工作中可能是hive表里面的column)

特征工程是什么？

==》是使用专业背景知识和技巧处理数据，使得特征能在机器学习算法中发挥更好的作用的过程



特征工程的重要性

==》2周内可以完成的一次特征的迭代，在有效的情况下auc提升约3-5%，可以相当于一个月完成机器模型的小优化



构建简单的用户画像





简单的用户画像可以从用户基本属性，行为属性，统计属性出发。

基本属性：身高，体重，性别，年龄等等

行为属性：购买，消费，观看，点评等等

统计属性：一些统计量如max, min, mean, std等等

这样一个简单的用户画像就构建出来，还是来主要说说特征工程~



特征工程中不同数据类型的特征的处理方法

数值型（连续型）

a)统计值 max, min, mean, std

b)离散化  比如age 1-100  按照年龄段进行分段, 数值型 =》类别型

c)hash分桶

三个方法都很常用

类别型（离散型）

a) one-hot编码( 编码完之后叫做哑变量 )

比如 列：颜色  中所有值包括(红，黄，绿) 经过onthot编码之后

红：1,0,0   黄：0,1,0  绿：0,0,1





这是spark将数据进行onehot编码，其中 categoryVec这个列，他用的是稀疏矩阵表示方法。

(2,[0],[1.0]) 含义为 (矩阵长度为2，有值位置的索引是0，索引对应的值是1.0)

==》转成为稠密矩阵就为  [1.0,0.0]



b) Hash与聚类处理





- 将整个文章所有的词进行合并去重，做成特征维度

- 这里面的三句话分别用向量进行表示

- bucket1 代表 词集( to，likes，movies)

做词集的意义在于：某些词集属于个别领域内的词语



c) Histogram 映射





如上数据，假如对爱好进行预测

先对爱好进行onehot编码

足球:[1,0,0]

散步:[0,1,0]

看剧:[0,0,1]

再对性别 进行histogram映射编码

男:[2/3,1/3,0]   代表含义为  [一共三个男生2个人喜欢足球，3个男生1个人喜好散步，3个男生1个喜好看剧]  即 将特征与预测结果进行关系关联

这样： 女可以编码为 [0,1/3,2/3]



时间型

既可以看成连续型，又可以看做离散值







文本型

a) 词袋

就是上面Hash与聚类处理 中词集的意思

b) n-gram的应用

文本1：我喜欢你    分词=》[我，喜欢，你]

文本2：你喜欢我    分词=》[你，喜欢，我]

这样两句话的向量一样，但其实含义并不一样，这是需要用到n-gram加大切分力度

n-gram意思：比如2-gram，相当于两个两个词语相连 文本1分词可以成转行 [我，喜欢，你，我喜欢，喜欢你] ；文本2分词 就变成了 [你，喜欢，我, 你喜欢，喜欢我]

这样两个文本通过分词+2-gram得出来的向量就不同，从而 就区分开来含义 。







c) TF-IDF （词频-反文档频率）

可以理解成求出某一个词在当前文档的中的权重值  (实际应用场景较多，如jieba，局部求score)





代码展示







d) word2vec

就是将一个词(word)映射成一个指定维度的向量(vector)

作用：比如城市字段(column)，其中值(value)分别有 北京，上海，沈阳....等

经过Word2vec 将北京，上海，沈阳....分别转换成一个向量，但是隐藏属性是北京和上海都属于一线城市，所以二者的关系相对于沈阳可能更相近。有Word2vec的好处 就是可以用向量的距离表示出北京与上海之间的相似性，如果仅仅使用onehot编码就体现不出来了





统计型

是比赛，排序和推荐系统中常常用到的，这也是跟业务相关度很高的处理方法(上面用户画像也用到)









组合特征

a) 简单组合特征：拼接型

由多个特征组合形成一个新的特征(列)

colour&&cate:  黑色&&电脑    组成一个新的特征(column)，如果用户有买 该特征(value)为1，没有买特征(value)为就为0

 点击率预估,转化率预估场景中 ： 正负权重，喜欢&&不喜欢某种类型

b)模型组合特征

   1）用GBDT训练产出特征组合路径





比如说：性别为男，在北京，用手机上网的人 会进行某种决策

组合特征就是： 男_北京_apple

如果这个特征值为1(命中) 就进行某种决策

  2）组合特征和原始特征一起放进LR训练

 这是工业界很常用的方法 GBDT+LR

特征选择

冗余：部分特征的相关度太高 ，存在就浪费计算性能。比如   年龄段 和 年龄 两个特征 冗余度高

噪声：部分特征对预测结果有负影响

特征选择：理解成 比如原本有50个特征，从中挑选出30个特征

降维： 将原本的50个特征进行特征工程处理之后变成上千维度对每个维度进行排序，取topN维



特征选择的方式：

过滤型：评估单个特征和结果值之间的相关度，排序留下top相关的特征部分

Pesrsion相关系数，互信息，距离相关度

缺点：没有考虑到特征之间的关联作用，可能把有用的关联特征误剔除

       在python里面sklearn有现成的方法使用，SekectKBest方法

包裹型：把特征选择看做一个特征子集搜索问题，筛选各种特征子集，用模型评估效果

使用算法：递归特征删除算法





   嵌入型：根据模型来分析特征的重要性

   常用的方式为用正则化来做特征选择 (L1 正则化，比如模型里面某个权重很小，L1之后直接变成      0。截断型效应)

在电商用LR做CTR/CVR预估， 在 3-5 亿维的系数特征上用 L1 正则化的 LR 模型 。 剩余 2-3 千万的 feature，意味着其他的 feature 重要度不够 。

特征工程&归一化
原创： echoy189  Spark推荐系统  4月26日
135无圈.png

  Spark推荐系统，干货，心得
  点击上方蓝字关注～

归一化：将每个要素的重新映射到特定范围（通常为[0，1]）

spark中使用MinMaxScaler（最大-最小规范化）

他需要的参数：

min：默认为0.0，转换后的下限

max：默认为1.0，转换后的上限

MinMaxScaler计算数据集的统计信息，并生成MinMaxScalerModel，然后，模型可以单独转换每个要素，使其在给定的范围内。

特征E的重新缩放值被计算为：



对于情况Emax==Emin,Rescaled(ei)= 0.5(max+min)

请注意，由于零值可能会转换为非零值，即使对于稀疏输入，变压器的输出也将为DenseVector（稠密矩阵）



那么为什么进行特征归一化？

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得

不同指标之间具有可比性。例如，分析一个人的身高和体重对健康的影响，如果

使用米（m）和千克（kg）作为单位，那么身高特征会在1.6～1.8m的数值范围

内，体重特征会在50～100kg的范围内，分析出来的结果显然会倾向于数值差别比

较大的体重特征。想要得到更为准确的结果，就需要进行特征归一化处理，使各指标处于同一数值量级，以便进行分析。

借助随机梯度下降的实例来

说明归一化的重要性。假设有两种数值型特征，x 1 的取值范围为 [0, 10]，x2 的取值

范围为[0, 3]，于是可以构造一个目标函数符合图1.1（a）中的等值图。

在学习速率相同的情况下，x1 的更新速度会大于x 2 ，需要较多的迭代才能找到

最优解。如果将x 1 和x 2 归一化到相同的数值区间后，优化目标的等值图会变成图

1.1（b）中的圆形，x 1 和x 2 的更新速度变得更为一致，容易更快地通过梯度下降找

到最优解。



当然，数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模

型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模

型。但对于决策树模型则并不适用，以C4.5为例，决策树在进行节点分裂时主要

依据数据集D关于特征x的信息增益比，而信息增益比跟特征是否经过归一化是无关的，

因为归一化并不会改变样本在特征x上的信息增益。





object MinMaxScalerExample {
  def main(args: Array[String]): Unit = {
    val spark = CreateSparkSession.creteLocalSparkSession(this.getClass.getName)
    val dataFrame = spark.createDataFrame(Seq(
      (0, Vectors.dense(1.0, 0.5, -1.0)),
      (1, Vectors.dense(2.0, 1.0, 1.0)),
      (2, Vectors.dense(4.0, 10.0, 2.0))
    )).toDF("id", "features")
    val scaler = new MinMaxScaler()
      .setInputCol("features")
      .setOutputCol("scaledFeatures")
    // Compute summary statistics and generate MinMaxScalerModel
    val scalerModel = scaler.fit(dataFrame)
    // rescale each feature to range [min, max].
    val scaledData = scalerModel.transform(dataFrame)
    println(s"Features scaled to range: [${scaler.getMin}, ${scaler.getMax}]")
    scaledData.select("features", "scaledFeatures").show(false)

  }
}


运行结果：







推荐阅读：

决策树之特征选择

Spark特征工程

Spark梯度下降法

ChiSqSelector卡方选择器


特征的选择：ChiSqSelector卡方选择器
原创： echoy189  Spark推荐系统  3月7日


135无圈.png

  Spark推荐系统，干货，心得
  点击上方蓝字关注～

在之前文章中有写决策树特征选择，这里在介绍另一种方法，ChiSqSelector卡方选择器。

先要了解几个概念：

自变量：研究者主动操纵，而引起因变量发生变化的因素或条件，因此自变量被看作是因变量的原因。

应变量：在函数关系式中，某个量会随一个（或几个）变动的量的变动而变动。

自由度：自由度是指当以样本的统计量来估计总体的参数时，样本中独立或能自由变化的数据的个数，称为该统计量的自由度。一般来说，自由度等于独立变量减掉其衍生量数。举例来说，变异数的定义是样本减平均值(一个由样本决定的衍生量)，因此对N个随机样本而言，其自由度为N-1

卡方检验适用与类别变量，如果要对连续型变量做检验，可以将连续型变量分成多个区间，变成类别的形式。

如上所述，卡方检验可以判断两个变量之间是否有显著的相关性（也可以说成是否有显著的独立性）我们通过一个小例子来说明如何做卡方独立性检验。

一个班里面的总人数87人，男53，女34。他们中打游戏与不打游戏的数量如下表：


男

女

总数

打游戏

34

10

44

不打游戏

19

24

43

总数

53

34

87

假设打不打游戏与性别为男或女是独立无关的。且可以得到一个班级随机男生的概率为

53/87 =0.609

理论值四格表

因为前面假设打不打游戏与性别为男或女是独立无关的，所以得到


总数

男

女

打游戏

44

44*0.609=26.8

44*(1-0.609)=17.2

不打游戏

43

43*0.609=26.2

43*(1-0.609)=16.8

卡方检验公式

卡方检验的公式如下，其中A为实际值，也就是第一个四格表中的四个数据。T为理论值，也就是理论四格表中的四格数据



X2值用于衡量实际值与理论值得差异程度，包含了以下两个信息：

l 实际值与理论值偏差的绝对大小（由于平方的存在，差异被放大了）

l 差异值与理论值得相对大小。

   上述场景的CHI = 10.10

卡方分布的临界值

   当通过上述的公式计算得到CHI的值以后，该如何判断我们的原假设是否成立呢？可以通过查询卡方分布的临界值表（这个表是固定的，百科可查）来查看我们的原假设是否成立。

自由度F = （性别类别数-1）* (打不打游戏类别数 -1) =（2-1）* (2 -1 )=1



由于自由度F = 1，所以只需要看分布表的第一行。可以看到，随着CHI的增大，原假设成立的概率就越小。因为CHI = 10.10，所有原假设发生的概率约等于0.1%，反之，也就是说，原假设不成立（即两个分类变量不是独立无关）的概率大于99.9%，所以打游戏与性别有关

如何应用于特征选择

CHI值越大，说明两个变量越不可能是独立无关的，也就是说X2越大，两个变量的相关程序也就越高。对于特征变量x1,x2,...,xn，以及分类变量y。只需要计算CHI(x1, y)、CHI(x2,y)、...、CHI(xn, y)，并按照CHI的值从大到小将特征排序，然后选择阈值，大于阈值的特征留下，小于阈值的特征删除。这样就筛选出一组特征子集了，接着使用这组特征子集去训练分类器，然后评估分类器的性能。


SparkML代码

Spark中的ChiSqSelector 就是使用了卡方独立性检验来决定哪些特征优秀应该被选择。它提供了3中方法：

（1）numTopFeatures，设置固定的提取特征的数量，程序会根据卡方值的高低返回前n个卡方值最高的特征。（预测能力最强的前n个特征）

（2）percentile，与上类似，设置的是一个比例。

（3）fpr，预先设定一个显著性水平α，所有p值低于α的特征将会被选择出来。

在默认情况下，使用numTopFeatures方法，并且默认选择前50个特征。

Examples

假设我们有如下一个dataframe,feature(自变量)是4个特征，clicked是标签（应变量）

 id |features              | clicked
---|-----------------------|---------
 7 | [0.0, 0.0, 18.0, 1.0] | 1.0
 8 | [0.0, 1.0, 12.0, 0.0] | 0.0
 9 | [1.0, 0.0, 15.0, 0.1] | 0.0


如果我们使用ChiSqSelector并设置numTopFeatures=1，根据我们所有的特征，其中最后一列标签clicked是认为最有用的特征：

  id |features              | clicked | selectedFeatures
---|----------------------------------|------------------
 7 | [0.0, 0.0, 18.0, 1.0] | 1.0      | [1.0]
 8 | [0.0, 1.0, 12.0, 0.0] | 0.0      | [0.0]
 9 | [1.0, 0.0, 15.0, 0.1] | 0.0      | [0.1]


scala代码



/**
  * create by liuye10 on 2019/3/7
  */
import com.badou.common.CreateSparkSession
import org.apache.spark.ml.feature.ChiSqSelector
import org.apache.spark.ml.linalg.Vectors
object ChiSqSelectorSelectFea {

  def main(args: Array[String]): Unit = {
    val spark = CreateSparkSession.creteLocalSparkSession(this.getClass.getName)
    val data = Seq(
      (7, Vectors.dense(0.0, 0.0, 18.0, 1.0), 1.0),
      (8, Vectors.dense(0.0, 1.0, 12.0, 0.0), 0.0),
      (9, Vectors.dense(1.0, 0.0, 15.0, 0.1), 0.0)
    )

    val df = spark.createDataset(data).toDF("id", "features", "clicked")

    val selector = new ChiSqSelector()
      .setNumTopFeatures(1)
      .setFeaturesCol("features")
      .setLabelCol("clicked")
      .setOutputCol("selectedFeatures")

    val result = selector.fit(df).transform(df)

    println(s"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected")
    result.show()
  }


}


推荐阅读：

spark协同过滤
决策树之特征选择

Spark特征工程

Spark推荐系统

机器学习中一些基础的数学

spark-streaming 连接kafka

Docker上搭建Spark集群

idea本地调试spark，hive，kafka


决策树之特征选择
原创： echoy189  Spark推荐系统  3月5日
实际工作中，做好训练集/测试集，用训练集跑完一个模型之后，需要不断对该模型进行优化并提高auc，故需要调整模型参数或新增一些维度特征，这样每次都要重新制作训练集数据，工作量庞大。

这里介绍使用spark决策树C4.5提前进行特征选择，减少模型迭代工作量。

下图是原始操作流程，因为每次都会新挖掘出一些特征，需要重复将新挖掘特征与原始特征做成新的训练集，训练出新的模型并与原始model进行auc比对，开发周期性长，工作量大







决策树进行特征选择

使用spark决策树进行特征选择，可以找出原始model阈值特征

(这步需要不断尝试跟经验积累)





特征选择(降维)



1、将阈值特征与每次新挖掘的新特征(n维)组合成n+1维新特征组合

2、使用C4.5(决策树)计算出特征重要性，新增特征与阈值特征重要性进行比对

3、新挖掘特征大于阈值特征的保留(对模型有提高作用)，小于的特征丢弃



spark代码





测试数据

链接：https://pan.baidu.com/s/1aKQdLOr2JO6hUaG1_HtJhg

提取码：ohjk

推荐阅读：

Spark特征工程

Spark推荐系统

机器学习中一些基础的数学

flume 1.8+Hadoop2.0

spark-streaming 连接kafka

Docker上搭建Spark集群

idea本地调试spark，hive，kafka

