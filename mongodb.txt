https://help.aliyun.com/product/26556.html?spm=a2c4g.11186623.6.29.e0d672f4BgZEpj

设置常用的MongoDB监控报警规则
更新时间：2019-01-21 09:45:10

编辑 ·
 · 我的收藏
本页目录
背景信息
操作步骤
云数据库MongoDB提供实例状态监控及报警功能。本文将介绍设置磁盘空间使用率、IOPS使用率、连接数使用率、CPU使用率等常用的监控项目。

背景信息
随着数据量及业务的发展，MongoDB实例的性能资源使用率可能会逐步提升，直至被消耗殆尽。
某些场景下MongoDB实例的性能资源可能被大量地异常消耗。如大量的慢查询引起的CPU使用率上升，大量数据写入导致磁盘空间被急剧消耗等情况。
说明 当磁盘容量不足将导致实例被锁定。如遇到实例被锁定您可以提交工单。实例解锁后您可以通过变更配置来增加磁盘空间。
通过对实例的关键性能指标设置监控报警规则，让您在第一时间得知指标数据发生异常，帮助您迅速定位并处理故障。

操作步骤
登录MongoDB管理控制台。
在页面左上角，选择实例所在的地域。
找到目标实例，单击实例ID。
在左侧导航栏中，单击报警规则。
单击设置报警规则，跳转至云监控控制台页面。
在云监控控制台页面，单击页面右上角的创建报警规则。
在创建报警规则页面，设置关联资源。

设置项目	说明
产品	下拉选择实例类型。
云数据库MongoDB版-副本集
云数据库MongoDB版-分片集群
云数据库MongoDB版-单节点实例
说明 当选择 云数据库MongoDB版-分片集群时，请选择需要监控的 Mongos节点和 Shard节点。
资源范围
资源范围选择全部实例，则产品下任何实例满足报警规则描述时，都会发送报警通知。
选择指定的实例，则选中的实例满足报警规则描述时，才会发送报警通知。
地域	选择实例所属地域。
实例	选择实例ID，可选择多个实例。
设置报警规则，此处先设置磁盘空间使用率，设置完成后单击添加报警规则。

说明
例如规则描述为磁盘使用率5分钟平均值>=80%，则报警服务会5分钟检查一次5分钟内的数据是否满足平均值>=80%。您可以根据您的业务场景微调相关数值。
角色选择为任意角色即代表监控实例的 Primary 节点和 Secondary 节点。
参考上一步骤设置IOPS使用率、连接数使用率、CPU使用率的监控报警规则。


 设置报警规则的其他项目。
设置项目	说明
通道沉默时间	指报警发生后如果未恢复正常，间隔多久重复发送一次报警通知。
连续几次超过阈值后报警
即连续几次报警的探测结果符合您设置的规则描述，才会触发报警，建议设置为3次。

例如规则描述为"CPU使用率 5分钟内平均值>80%,连续3次超过阈值后报警"，则连续出现3次 CPU使用率 5分钟内平均值>80%的情况，才会触发报警。

生效时间	设置报警规则生效的时间。
设置通知方式。
设置项目	说明
通知对象	发送报警的联系人或联系组，详情请参考报警联系人和报警联系组。
报警级别	分为Critical 、Warning、Info三个等级，不同等级对应不同的通知方式。
Critical：电话语音+手机短信+邮件+钉钉机器人
Warning：手机短信+邮件+钉钉机器人
Info：邮件+钉钉机器人
邮件主题	自定义报警邮件的主题，默认为产品名称+监控项名称+实例ID。
邮件备注	自定义报警邮件补充信息。填写邮件备注后，发送报警的邮件通知中会附带您的备注。
报警回调	详情请参考使用报警回调。
设置完成后，单击确认。报警规则将自动生效。

Azure Cosmos DB API for MongoDB 迁移到阿里云
更新时间：2019-02-22 17:34:57

编辑 ·
 · 我的收藏
本页目录
注意事项
数据库账号权限要求
环境准备
迁移步骤
使用MongoDB数据库自带的备份还原工具，您可以将Azure Cosmos DB API for MongoDB迁移至阿里云。

注意事项
该操作为全量迁移，为避免迁移前后数据不一致，迁移开始前请停止数据库写入。
如果您之前使用mongodump命令对数据库进行过备份操作，请将备份在dump文件夹下的文件移动至其他目录。确保默认的dump备份文件夹为空，否则将会覆盖该文件夹下之前备份的文件。
请在安装有MongoDB服务的服务器上执行mongodump和mongorestore命令，并非在mongo shell环境下执行。
数据库账号权限要求
实例类型	账号权限
Azure Cosmos DB	read
目的MongoDB实例	readWrite
环境准备
创建云数据库MongoDB实例，详情请参考创建实例。
说明
实例的存储空间要大于Azure Cosmos DB。
实例的数据库版本选用3.4。
设置阿里云MongoDB数据库的数据库密码，详情请参考设置密码。
在某个服务器上安装MongoDB程序，详情请参考安装MongoDB。
说明
请安装MongoDB3.0以上版本。
该服务器仅作为数据备份与恢复的临时中转平台，迁移操作完成后不再需要。
备份目录所在分区的可用磁盘空间要大于Azure Cosmos DB。
本案例将MongoDB服务安装在Linux服务器上进行演示。

迁移步骤
登录Azure门户。
在左侧导航栏单击Azure Cosmos DB。
在Azure Cosmos DB页面，单击需要迁移的Cosmos DB 账户名称。
在账户详情页，单击Connection String。
单击Read-only Keys页签，查看连接该数据库所需的信息。
图 1. Azure连接信息

说明 迁移数据时使用只读权限的账号密码信息即可。
在安装有MongoDB服务的Linux服务器上执行以下命令进行数据备份，将数据备份至该服务器上。
mongodump --host <HOST>:10255 --authenticationDatabase admin -u <USERNAME> -p <PRIMARY PASSWORD> --ssl --sslAllowInvalidCertificates
说明：将<HOST>、<USERNAME>、<PRIMARY PASSWORD>更换为Azure连接信息图中对应选项的值。

等待备份完成，Azure Cosmos DB的数据库将备份至当前目录下dump文件夹中。

获取阿里云MongoDB数据库的Primary节点连接地址，详情请参考实例连接说明。
在安装有MongoDB服务的Linux服务器上执行以下语句将数据库数据全部导入至阿里云MongoDB数据库。
 mongorestore --host <mongodb_host>:3717 --authenticationDatabase admin -u <username> -p <password> dump
说明：
<mongodb_host>：MongoDB实例的Primary节点连接地址。
<username>：登录MongoDB实例的数据库用户名。
<password>：登录MongoDB实例的数据库密码。
等待数据恢复完成，Azure Cosmos DB API for MongoDB数据库即迁移至阿里云MongoDB数据库中。

设置数据分片以充分利用Shard性能
更新时间：2019-03-11 13:05:10

编辑 ·
 · 我的收藏
本页目录
注意事项
操作示例
后续操作
您可以对分片集群实例中数据库的集合设置数据分片，以充分利用各 Shard 节点的存储空间和计算性能。

注意事项
该操作仅适用于分片集群实例。
进行数据分片操作后，均衡器会对满足条件的现有数据进行分片，这将占用实例的性能，请在业务低峰期操作。
分片的片键一经设置后不可修改。
分片的片键选取将影响分片集群实例性能，片键的选取可参考如何选择Shard Key。
若未进行数据分片，数据写入将被集中在 PrimaryShard 节点中。这将导致其他 Shard 节点的存储空间和计算性能无法被充分利用。

操作示例
本文进行操作演示的示例中，数据库为mongodbtest，集合为customer。

通过Mongo Shell登录分片集群实例。
对集合所在的数据库启用分片功能。
sh.enableSharding("<database>")
说明：<database>：数据库名。

操作示例：
sh.enableSharding("mongodbtest")
说明 您可以通过 sh.status()查看分片状态。
对集合的某个字段建立索引。
db.<collection>.createIndex(<keys>,<options>)
说明：
<collection>：集合名。
<keys>：包含用于建立索引的字段和排序方式。
排序方式设置为1表示按升序来创建索引，设置为-1表示按照降序来创建索引。

<options>：表示接收可选参数，详情请参考db.collection.createIndex()，本操作示例中暂未使用到该字段。
操作示例：
db.customer.createIndex({"name":1})
对集合设置数据分片。
sh.shardCollection("<database>.<collection>",{ "<key>":<value> } )
说明：
<database>：数据库名。
<collection>：集合名。
<key>：分片的键，MongoDB将根据该值进行数据分片。
<value>
1：表示索引升序，通常能很好的支持基于 Shard Key 的范围查询。
-1：表示索引降序，通常能很好的支持基于 Shard Key 的范围查询。
hashed：表示使用Hash分片，通常能将写入均衡分布到各个 Shard 节点中。
操作示例：
sh.shardCollection("mongodbtest.customer",{"name":1})
如数据库中该集合拥有实际数据，等待后台的均衡器自动执行即可，该过程对用户透明。

后续操作
经过一段时间的运行或数据写入后，您可以在Mongo Shell中执行 sh.status()，查看数据分片信息和Shard上的块存储信息。

您也可以通过执行 db.stats()查看该数据库在各Shard节点的数据存储情况。

 上一篇：Azure Cosmos DB API for MongoDB 迁移到阿里云

 整理数据库碎片以提升磁盘利用率
 更新时间：2018-11-14 10:27:11

 编辑 ·
  · 我的收藏
 本页目录
 注意事项
 单节点实例/副本集实例操作示例
 分片集群实例操作示例
 MongoDB数据库在长期频繁地删除、写入数据，将产生很多碎片。这些碎片将占用磁盘空间，降低磁盘利用率。您可以对集合中的所有数据和索引进行重写和碎片整理，释放未使用的空间，提升磁盘利用率和查询性能。

 注意事项
 执行该操作前，建议对数据库进行备份。
 正在进行碎片整理的数据库会被锁定，读写操作将被阻塞。请在业务低峰期操作。
 该操作不宜频繁执行。
 单节点实例/副本集实例操作示例
 通过mongo shell连接云数据库的Primary节点，详情请参考mongoshell连接实例。
 切换至集合所在的数据库。
 use <database_name>
 命令说明：

 <database_name>：数据库名。

 执行命令来对某个集合进行碎片整理。
 db.runCommand({compact:"<collection_name>",force:true})
 命令说明：

 <collection_name>：集合名。

 说明 force为可选参数。
 值为true时，compact命令才可以在副本集中的主节点上运行 。
 如果为false， compact命令在主节点上运行时将返回错误。
 等待执行，返回{ "ok" : 1 }代表执行完成。
 说明 compact操作不会传递给Secondary节点，实例为副本集实例时，请重复上述步骤通过mongo shell连接至Secondary节点，执行碎片整理命令。
 碎片整理完毕后，可通过db.stats()命令查看碎片整理后数据库占用的磁盘空间。

 分片集群实例操作示例
 通过mongo shell连接到分片集群实例中的任一mongos节点，详情请参考mongo shell连接分片集群实例。
 执行命令对Shard节点中的Primary节点合进行集合的碎片整理。
 db.runCommand({runCommandOnShard:"<Shard ID>","command":{compact:"<collection_name>",force:true}})
 命令说明：

 <Shard ID>：Shard节点ID。

 <collection_name>：集合名。

 执行命令对Shard节点中的Secondary节点进行集合的碎片整理。
 db.runCommand({runCommandOnShard:"<Shard ID>","command":{compact:"<collection_name>"},queryOptions: {$readPreference: {mode: 'secondary'}}})
 命令说明：

 <Shard ID>：Shard节点ID。

 <collection_name>：集合名。

 碎片整理完毕后，可通过db.runCommand({dbstats:1}) 命令查看碎片整理后数据库占用的磁盘空间。

 管理MongoDB均衡器Balancer
 更新时间：2019-04-16 10:31:41

 编辑 ·
  · 我的收藏
 本页目录
 注意事项
 关闭 Balancer 功能
 开启 Balancer 功能
 设置 Balancer 的活动窗口
 云数据库支持均衡器 Balancer 管理操作。在一些特殊的业务场景下，您可以启用或者关闭 Balancer 功能，设置活动窗口等操作。

 注意事项
 Balancer 属于分片集群架构中的功能，该操作仅适用于分片集群实例。
 Balancer 的相关操作可能会占用实例的资源，请在业务低峰期操作。
 关闭 Balancer 功能
 云数据库MongoDB的 Balancer 功能默认是开启状态。特殊业务场景下需要关闭，请参考下述操作步骤。

 通过 Mongo Shell 登录数据库。
 在 mongos 节点命令窗口中，切换至 config 数据库。
 use config
 执行如下命令查看 Balancer 运行状态，如返回值为空。
 while( sh.isBalancerRunning() ) {
           print("waiting...");
           sleep(1000);
 }
 返回值为空，表示 Balancer 没有处于执行任务的状态，此时可执行下一步的操作，关闭 Balancer 。
 返回值 waiting 表示 Balancer 正在执行块迁移，此时不能执行关闭 Balancer 的命令，否则可能引起数据不一致。

 确认执行第3步的命令后返回的值为空，可执行关闭 Balancer 命令。
 sh.stopBalancer()
 开启 Balancer 功能
 如果您设置了数据分片，开启 Balancer 功能后可能会立即触发均衡任务。这将占用实例的资源，请在业务低峰期执行该操作。

 通过 Mongo Shell 登录数据库。
 在 mongos 节点命令窗口中，切换至 config 数据库。
 use config
 执行如下命令开启 Balancer功能。
 sh.setBalancerState(true)
 设置 Balancer 的活动窗口
 为避免 Balancer 执行块迁移操作影响您的业务，您可以通过设置 Balancer 的活动窗口，让 Blancer 在指定的时间段工作。

 说明 执行该操作须确保 Balancer 功能处于开启状态。如未开启，请参考开启 Balancer 功能。
 通过 Mongo Shell 登录数据库。
 在 mongos 节点命令窗口中，切换至 config 数据库。
 use config
 执行如下命令设置 Balancer 的活动窗口。
 db.settings.update(
    { _id: "balancer" },
    { $set: { activeWindow : { start : "<start-time>", stop : "<stop-time>" } } },
    { upsert: true }
 )
 说明
 <start-time>：开始时间，时间格式为 HH:MM，HH取值范围为00 - 23，MM取值范围为00 - 59。
 <stop-time>：结束时间，时间格式为 HH:MM，HH取值范围为00 - 23，MM取值范围为00 - 59。
 您可以通过执行 sh.status()命令查看 Balancer 的活动窗口。如下示例中，活动窗口设置为01:00- 03:00。

 相关操作：如您需要 balancer 始终处于运行状态，您可以使用如下命令去除活动窗口的设置。
 db.settings.update({ _id : "balancer" }, { $unset : { activeWindow : true } })

 使用数据镜像保护尚未写入完整的数据
 更新时间：2018-11-23 17:16:10

 编辑 ·
  · 我的收藏
 本页目录
 使用场景
 副本集实例操作方法
 分片集群实例操作方法
 云数据库MongoDB提供数据镜像能力，您可以对副本集实例或分片集群实例创建一个只读数据镜像。其中副本集最高支持3TB数据，集群版本最高支持96TB数据。

 使用场景
 创建数据镜像，可确保在数据大批量写入更新期间，所有读请求从数据镜像获取数据。从而确保数据在完整写入前不会被应用程序读取到。数据镜像的读取性能与先前非镜像数据的读取性能完全保持一致。
 说明 数据更新完成后，可将数据正式同步生效，供应用正常连接读取最新数据。通过阿里云提供的数据镜像操作命令，可实现数据自动同步生效（秒级别同步）。数据同步期间不影响正常数据读取操作。
 副本集实例操作方法
 通过mongo shell连接到需要操作的节点（Primary节点或Secondary节点），连接方法请参考mongo shell 连接副本集实例。
 创建数据镜像。
 db.runCommand({checkpoint:"create"})
 数据镜像功能使用完毕，删除数据镜像。
 db.runCommand({checkpoint:"drop"})
 分片集群实例操作方法
 通过mongo shell连接到分片集群实例中的任意一个mongos，连接方法请参考mongo shell 连接分片集群实例。
 创建数据镜像。
 在所有Shard的Primary节点上创建数据镜像。
 db.runCommand({runCommandOnShard: "all", "command": {checkpoint:"create"}})
 在所有Shard的Secondary节点上创建数据镜像。
 db.runCommand({runCommandOnShard: "all", "command": {checkpoint:"create"}, $queryOptions: {$readPreference: {mode: 'secondary'}}})
 数据镜像功能使用完毕，删除数据镜像。
 在所有Shard的Primary节点上删除数据镜像。
 db.runCommand({runCommandOnShard: "all", "command": {checkpoint:"drop"}})
 在所有Shard的Secondary节点上删除数据镜像。
 db.runCommand({runCommandOnShard: "all", "command": {checkpoint:"drop"}, $queryOptions: {$readPreferenc

 如何连接副本集实例实现读写分离和高可用
 更新时间：2018-12-06 15:55:58

 编辑 ·
  · 我的收藏
 本页目录
 使用前须知
 Connection String 连接说明
 副本集实例 Connection String URI 连接示例
 MongoDB副本集实例通过多个数据副本来保证数据的高可靠，通过自动的主备切换机制来保证服务的高可用。需要注意的是，您需要使用正确的方法连接副本集实例来保障高可用，您也可以通过设置来实现读写分离。

 使用前须知
 副本集实例的Primary节点不是固定的。当遇到副本集轮转升级、Primary节点宕机、网络分区等场景时可能会触发主备切换，副本集可能会选举一个新的Primary节点，原先的Primary节点会降级为Secondary节点。
 若使用Primary节点的地址直接连接Primary节点，所有的读写操作均在Primary节点完成，造成该节点压力较大，且一旦副本集发生主备切换，您连接的Primary会降级为Secondary，您将无法继续执行写操作，将严重影响到您的业务使用。
 Connection String 连接说明
 要正确连接副本集实例，您需要先了解下MongoDB的Connection String URI，所有官方的driver都支持以Connection String的方式来连接MongoDB。

 mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]
 说明：
 mongodb:// ：前缀，代表这是一个Connection String。
 username:password@：登录数据库的用户和密码信息，如果启用了鉴权，需要指定密码。
 hostX:portX：副本集成员的IP地址:端口信息，多个成员以逗号分割。
 /database：鉴权时，用户帐号所属的数据库。
 ?options：指定额外的连接选项。
 说明 更多关于 Connection String 请参考MongoDB文档Connection String URI。
 副本集实例 Connection String URI 连接示例
 云数据库MongoDB提供了 Connection String URI 连接方式。

 获取副本集实例的 Connection String URI 连接信息，详情请参考副本集实例连接说明。

 应用程序设置使用 Connection String URI 来连接实例，详情请参考程序代码连接实例。
 说明
 要实现读写分离，需要在 Connection String URI 的options里添加readPreference=secondaryPreferred，设置读请求为Secondary节点优先。

 更多读选项请参考Read preferences。

 示例：
 mongodb://root:xxxxxxxx@dds-xxxxxxxxxxxx:3717,xxxxxxxxxxxx:3717/admin?replicaSet=mgset-xxxxxx&readPreference=secondaryPreferred
 通过上述 Connection String 来连接MongoDB副本集实例，读请求将优先发给Secondary节点实现读写分离。同时客户端会自动检测节点的主备关系，当主备关系发生变化时，自动将写操作切换到新的Primary节点上，以保证服务的高可用。


 使用 Connection String URI 连接分片集群实例
 更新时间：2019-04-16 10:21:28

 编辑 ·
  · 我的收藏
 本页目录
 背景信息
 Connection String URI 连接说明
 如何正确地连接分片集群实例
 常用连接参数
 MongoDB分片集群实例提供各个 mongos 节点的连接地址，通过连接 mongos 节点，您可以连接至分片集群实例的数据库。需要注意的是，您需要使用正确的方法连接分片集群实例来实现负载均衡及高可用。

 背景信息
 分片集群架构
 MongoDB分片集群（Sharded Cluster）通过将数据分散存储到多个分片（Shard）中，以实现高可扩展性。实现分片集群时，MongoDB 引入 Config Server 来存储集群的元数据，引入 mongos 作为应用访问的入口，mongos 从 Config Server 读取路由信息，并将请求路由到后端对应的 Shard 上。

 用户访问 mongos 跟访问单个 mongod 类似。
 所有 mongos 是对等关系，用户访问分片集群可通过任意一个或多个 mongos 。
 mongos 本身是无状态的，可任意扩展，集群的服务能力为“Shard服务能力之和”与“mongos服务能力之和”的最小值。
 访问分片集群时，最好将应用负载均匀地分散到多个 mongos 上。
 Connection String URI 连接说明
 要正确连接分片集群实例，您需要先了解下MongoDB的Connection String URI，所有官方的driver都支持以 Connection String URI 的方式来连接MongoDB数据库。

 Connection String URI 示例：

 mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]
 说明
 mongodb:// 前缀，代表这是一个Connection String URI 。
 username:password@ 登录数据库的用户和密码信息。
 hostX:portX多个 mongos 的地址列表。
 /database鉴权时，用户帐号所属的数据库。
 ?options 指定额外的连接选项。
 如何正确地连接分片集群实例
 云数据库MongoDB提供了 Connection String URI 连接方式。使用 Connection String URI 连接方式进行连接，可实现负载均衡及高可用。

 获取分片集群实例的 Connection String URI 连接信息，详情请参考分片集群实例连接说明。
 Connection String URI连接信息
 应用程序中设置使用 Connection String URI 来连接实例，详情请参考程序代码连接。
 通过 java 来连接的示例代码如下所示。

 MongoClientURI connectionString = new MongoClientURI("mongodb://:****@s-xxxxxxxx.mongodb.rds.aliyuncs.com:3717,s-xxxxxxxx.mongodb.rds.aliyuncs.com:3717/admin"); // ****替换为root密码
 MongoClient client = new MongoClient(connectionString);
 MongoDatabase database = client.getDatabase("mydb");
 MongoCollection<Document> collection = database.getCollection("mycoll");
 说明
 通过上述方式连接分片集群时，客户端会自动将请求分散到多个 mongos 上，以实现负载均衡。同时，当 URI 里 mongos 数量在2个及以上时，当有 mongos 故障时，客户端能自动进行切换，将请求都分散到状态正常的 mongos 上。

 当 mongos 数量很多时，您可以按应用将 mongos 进行分组。例如有2个应用 A、B，实例有4个 mongos，可以让应用 A 访问 mongos 1-2（URI 里只指定 mongos 1-2 的地址）， 应用 B 来访问 mongos 3-4（URI 里只指定 mongos 3-4 的地址）。根据这种方法来实现应用间的访问隔离。
 说明 应用访问的 mongos 彼此隔离，但后端 Shard 仍然是共享的。
 常用连接参数
 如何实现读写分离
 在 Connection String URI 的options里添加readPreference=secondaryPreferred，设置读请求为Secondary节点优先。

 示例
 mongodb://root:xxxxxxxx@dds-xxxxxxxxxxxx:3717,xxxxxxxxxxxx:3717/admin?replicaSet=mgset-xxxxxx&readPreference=secondaryPreferred
 如何限制连接数
 在 Connection String URI 的options里添加 maxPoolSize=xx ，即可将客户端连接池中的连接数限制在xx以内。

 如何保证数据写入到大多数节点后才返回
 在 Connection String URI 的options里添加 w= majority ，即可保证写请求成功写入大多数节点才向客户端确认。

 使用 MongoDB 存储日志数据
 更新时间：2017-06-28 11:56:31


 本页目录
 模式设计
 写日志
 查询日志
 数据分片
 应对数据增长
 线上运行的服务会产生大量的运行及访问日志，日志里会包含一些错误、警告、及用户行为等信息。通常服务会以文本的形式记录日志信息，这样可读性强，方便于日常定位问题。但当产生大量的日志之后，要想从大量日志里挖掘出有价值的内容，则需要对数据进行进一步的存储和分析。

 本文以存储 web 服务的访问日志为例，介绍如何使用 MongoDB 来存储、分析日志数据，让日志数据发挥最大的价值。本文的内容同样适用于其他的日志存储型应用。

 模式设计
 一个典型的web服务器的访问日志类似如下，包含访问来源、用户、访问的资源地址、访问结果、用户使用的系统及浏览器类型等。

 127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326 "[http://www.example.com/start.html](http://www.example.com/start.html)" "Mozilla/4.08 [en] (Win98; I ;Nav)"
 最简单存储这些日志的方法是，将每行日志存储在一个单独的文档里，每行日志在MongoDB里的存储模式如下所示：

 {
     _id: ObjectId('4f442120eb03305789000000'),
     line: '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326 "[http://www.example.com/start.html](http://www.example.com/start.html)" "Mozilla/4.08 [en] (Win98; I ;Nav)"'
 }
 上述模式虽然能解决日志存储的问题，但这些数据分析起来比较麻烦，因为文本分析并不是MongoDB所擅长的，更好的办法是把一行日志存储到MongoDB的文档里前，先提取出各个字段的值。如下所示，上述的日志被转换为一个包含很多个字段的文档。

 {
      _id: ObjectId('4f442120eb03305789000000'),
      host: "127.0.0.1",
      logname: null,
      user: 'frank',
      time: ISODate("2000-10-10T20:55:36Z"),
      path: "/apache_pb.gif",
      request: "GET /apache_pb.gif HTTP/1.0",
      status: 200,
      response_size: 2326,
      referrer: "[http://www.example.com/start.html](http://www.example.com/start.html)",
      user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"
 }
 同时，在这个过程中，如果您觉得有些字段对数据分析没有任何帮助，则可以直接过滤掉，以减少存储上的消耗。比如数据分析不会关心user信息、request、status信息，这几个字段没必要存储。ObjectId里本身包含了时间信息，没必要再单独存储一个time字段 (当然带上time也有好处，time更能代表请求产生的时间，而且查询语句写起来更方便，尽量选择存储空间占用小的数据类型）基于上述考虑，上述日志最终存储的内容可能类似如下所示：

 {
     _id: ObjectId('4f442120eb03305789000000'),
     host: "127.0.0.1",
     time: ISODate("2000-10-10T20:55:36Z"),
     path: "/apache_pb.gif",
     referer: "[http://www.example.com/start.html](http://www.example.com/start.html)",
     user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"
 }
 写日志
 日志存储服务需要能同时支持大量的日志写入，用户可以定制writeConcern来控制日志写入能力，比如如下定制方式：

 db.events.insert({
         host: "127.0.0.1",
         time: ISODate("2000-10-10T20:55:36Z"),
         path: "/apache_pb.gif",
         referer: "[http://www.example.com/start.html](http://www.example.com/start.html)",
         user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"
     }
 )
 说明：

 如果要想达到最高的写入吞吐，可以指定writeConcern为 {w: 0}。
 如果日志的重要性比较高（比如需要用日志来作为计费凭证），则可以使用更安全的writeConcern级别，比如 {w: 1} 或 {w: “majority”}。
 同时，为了达到最优的写入效率，用户还可以考虑批量的写入方式，一次网络请求写入多条日志。格式如下所示：

 db.events.insert([doc1, doc2, ...])

 查询日志
 当日志按上述方式存储到MongoDB后，就可以按照各种查询需求查询日志了。

 查询所有访问/apache_pb.gif 的请求
 q_events = db.events.find({'path': '/apache_pb.gif'})

 如果这种查询非常频繁，可以针对path字段建立索引，提高查询效率：

 db.events.createIndex({path: 1})

 查询某一天的所有请求
 q_events = db.events.find({'time': { '$gte': ISODate("2016-12-19T00:00:00.00Z"),'$lt': ISODate("2016-12-20T00:00:00.00Z")}})
 通过对time字段建立索引，可加速这类查询：

 db.events.createIndex({time: 1})

 查询某台主机一段时间内的所有请求
 q_events = db.events.find({
     'host': '127.0.0.1',
     'time': {'$gte': ISODate("2016-12-19T00:00:00.00Z"),'$lt': ISODate("2016-12-20T00:00:00.00Z" }
 })
 同样，用户还可以使用MongoDB的aggregation、mapreduce框架来做一些更复杂的查询分析，在使用时应该尽量建立合理的索引以提升查询效率。

 数据分片
 当写日志的服务节点越来越多时，日志存储的服务需要保证可扩展的日志写入能力以及海量的日志存储能力，这时就需要使用MongoDB sharding来扩展，将日志数据分散存储到多个shard，关键的问题就是shard key的选择。

 按时间戳字段分片
 使用时间戳来进行分片（如ObjectId类型的_id，或者time字段），这种分片方式存在如下问题：

 因为时间戳一直顺序增长的特性，新的写入都会分到同一个shard，并不能扩展日志写入能力。
 很多日志查询是针对最新的数据，而最新的数据通常只分散在部分shard上，这样导致查询也只会落到部分shard。
 按随机字段分片
 按照_id字段来进行hash分片，能将数据以及写入都均匀都分散到各个shard，写入能力会随shard数量线性增长。但该方案的问题是，数据分散毫无规律。所有的范围查询（数据分析经常需要用到）都需要在所有的shard上进行查找然后合并查询结果，影响查询效率。

 按均匀分布的key分片
 假设上述场景里 path 字段的分布是比较均匀的，而且很多查询都是按path维度去划分的，那么可以考虑按照path字段对日志数据进行分片，好处是：

 写请求会被均分到各个shard。
 针对path的查询请求会集中落到某个（或多个）shard，查询效率高。
 不足的地方是：

 如果某个path访问特别多，会导致单个chunk特别大，只能存储到单个shard，容易出现访问热点。
 如果path的取值很少，也会导致数据不能很好的分布到各个shard。
 当然上述不足的地方也有办法改进，方法是给分片key里引入一个额外的因子,比如原来的shard key是 {path: 1}，引入额外的因子后变成：

 {path: 1, ssk: 1} 其中ssk可以是一个随机值，比如_id的hash值，或是时间戳，这样相同的path还是根据时间排序的

 这样做的效果是分片key的取值分布丰富，并且不会出现单个值特别多的情况。上述几种分片方式各有优劣，用户可以根据实际需求来选择方案。

 应对数据增长
 分片的方案能提供海量的数据存储支持，但随着数据越来越多，存储的成本会不断的上升。通常很多日志数据有个特性，日志数据的价值随时间递减。比如1年前、甚至3个月前的历史数据完全没有分析价值，这部分可以不用存储，以降低存储成本，而在MongoDB里有很多方法支持这一需求。

 TTL 索引
 MongoDB的TTL索引可以支持文档在一定时间之后自动过期删除。例如上述日志time字段代表了请求产生的时间，针对该字段建立一个TTL索引，则文档会在30小时后自动被删除。

 db.events.createIndex( { time: 1 }, { expireAfterSeconds: 108000 } )

 注意：TTL索引是目前后台用来定期（默认60s一次）删除单线程已过期文档的。如果日志文档被写入很多，会积累大量待过期的文档，那么会导致文档过期一直跟不上而一直占用着存储空间。

 使用Capped集合
 如果对日志保存的时间没有特别严格的要求，只是在总的存储空间上有限制，则可以考虑使用capped collection来存储日志数据。指定一个最大的存储空间或文档数量，当达到阈值时，MongoDB会自动删除capped collection里最老的文档。

 db.createCollection("event", {capped: true, size: 104857600000}

 定期按集合或DB归档
 比如每到月底就将events集合进行重命名，名字里带上当前的月份，然后创建新的events集合用于写入。比如2016年的日志最终会被存储在如下12个集合里：

 events-201601
  events-201602
  events-201603
  events-201604
  ....
  events-201612
 当需要清理历史数据时，直接将对应的集合删除掉：

  db["events-201601"].drop()
  db["events-201602"].drop()
 不足到时候，如果要查询多个月份的数据，查询的语句会稍微复杂些，需要从多个集合里查询结果来合并。

 排查MongoDB CPU使用率高的问题
 更新时间：2019-03-11 13:04:49

 编辑 ·
  · 我的收藏
 本页目录
 分析MongoDB数据库正在执行的请求
 分析MongoDB数据库的慢请求
 服务能力评估
 在使用云数据库MongoDB的时候您可能会遇到MongoDB CPU使用率很高或者CPU使用率接近100%的问题，从而导致数据读写处理异常缓慢，影响正常业务。本文主要帮助您从应用的角度排查MongoDB CPU使用率高的问题。

 分析MongoDB数据库正在执行的请求
 通过Mongo Shell连接实例。
 Mongo Shell连接单节点实例
 Mongo Shell连接副本集实例
 Mongo Shell连接分片集群实例
 执行db.currentOp()命令，查看数据库当前正在执行的操作。
 该命令的输出示例如下。

 {
         "desc" : "conn632530",
         "threadId" : "140298196924160",
         "connectionId" : 632530,
         "client" : "11.192.159.236:57052",
         "active" : true,
         "opid" : 1008837885,
         "secs_running" : 0,
         "microsecs_running" : NumberLong(70),
         "op" : "update",
         "ns" : "mygame.players",
         "query" : {
             "uid" : NumberLong(31577677)
         },
         "numYields" : 0,
         "locks" : {
             "Global" : "w",
             "Database" : "w",
             "Collection" : "w"
         },
         ....
     },
 您需要重点关注以下几个字段。

 字段	返回值说明
 client	该请求是由哪个客户端发起的。
 opid	操作的唯一标识符。
 说明 如果有需要，可以通过 db.killOp(opid)直接终止该操作。
 secs_running	表示该操作已经执行的时间，单位为秒。如果该字段返回的值特别大，需要查看请求是否合理。
 microsecs_running	表示该操作已经执行的时间，单位为毫秒。如果该字段返回的值特别大，需要查看请求是否合理。
 ns	该操作目标集合。
 op	表示操作的类型。通常是查询、插入、更新、删除中的一种。
 locks	跟锁相关的参数，请参考官方文档，本文不做详细介绍。
 说明 db.currentOp 文档请参见db.currentOp。
 通过 db.currentOp()查看正在执行的操作，分析是否有不正常耗时的请求正在执行。比如您的业务平时 CPU 使用率不高，运维管理人员连到MongoDB数据库执行了一些需要全表扫描的操作导致 CPU 使用率非常高，业务响应缓慢，此时需要重点关注执行时间非常耗时的操作。
 说明 如果发现有异常的请求，您可以找到该请求对应的opid，执行 db.killOp(opid)终止该请求。
 如果您的应用刚刚上线，MongoDB实例的CPU使用率马上处于持续很高的状态，执行db.currentOp()，在输出结果中未发现异常请求，您可参考下述小节分析数据库慢请求。

 分析MongoDB数据库的慢请求
 云数据库MongoDB默认开启了慢请求Profiling ，系统自动地将请求时间超过100ms的执行情况记录到对应数据库下的system.profile集合里。

 通过 Mongo Shell 连接实例。
 详情请参考Mongo Shell连接单节点实例、Mongo Shell连接副本集实例、Mongo Shell连接分片集群实例。

 通过use <database>命令进入指定数据库。
 use mongodbtest
 执行如下命令，查看该数据下的慢请求日志。
 db.system.profile.find().pretty()
 分析慢请求日志，查找引起MongoDB CPU使用率升高的原因。
 以下为某个慢请求日志示例，可查看到该请求进行了全表扫描，扫描了11000000个文档，没有通过索引进行查询。
 {
         "op" : "query",
         "ns" : "123.testCollection",
         "command" : {
                 "find" : "testCollection",
                 "filter" : {
                         "name" : "zhangsan"
                 },
                 "$db" : "123"
         },
         "keysExamined" : 0,
         "docsExamined" : 11000000,
         "cursorExhausted" : true,
         "numYield" : 85977,
         "nreturned" : 0,
         "locks" : {
                 "Global" : {
                         "acquireCount" : {
                                 "r" : NumberLong(85978)
                         }
                 },
                 "Database" : {
                         "acquireCount" : {
                                 "r" : NumberLong(85978)
                         }
                 },
                 "Collection" : {
                         "acquireCount" : {
                                 "r" : NumberLong(85978)
                         }
                 }
         },
         "responseLength" : 232,
         "protocol" : "op_command",
         "millis" : 19428,
         "planSummary" : "COLLSCAN",
         "execStats" : {
                 "stage" : "COLLSCAN",
                 "filter" : {
                         "name" : {
                                 "$eq" : "zhangsan"
                         }
                 },
                 "nReturned" : 0,
                 "executionTimeMillisEstimate" : 18233,
                 "works" : 11000002,
                 "advanced" : 0,
                 "needTime" : 11000001,
                 "needYield" : 0,
                 "saveState" : 85977,
                 "restoreState" : 85977,
                 "isEOF" : 1,
                 "invalidates" : 0,
                 "direction" : "forward",
 ....in"
                 }
         ],
         "user" : "root@admin"
 }
 通常在慢请求日志中，您需要重点关注以下几点。

 全表扫描（关键字： COLLSCAN、 docsExamined ）
 全集合（表）扫描COLLSCAN 。
 当一个操作请求（如查询、更新、删除等）需要全表扫描时，将非常占用CPU资源。在查看慢请求日志时发现COLLSCAN关键字，很可能是这些查询占用了你的CPU资源。
 说明 如果这种请求比较频繁，建议对查询的字段建立索引的方式来优化。
 通过查看docsExamined的值，可以查看到一个查询扫描了多少文档。该值越大，请求所占用的CPU开销越大。
 不合理的索引（关键字： IXSCAN、keysExamined ）
 通过查看keysExamined字段，可以查看到一个使用了索引的查询，扫描了多少条索引。该值越大，CPU开销越大。

 如果索引建立的不太合理，或者是匹配的结果很多。这样即使使用索引，请求开销也不会优化很多，执行的速度也会很慢。

 如下所示，假设某个集合的数据，x字段的取值很少（假设只有1、2），而y字段的取值很丰富。

 { x: 1, y: 1 }
 { x: 1, y: 2 }
 { x: 1, y: 3 }
 ......
 { x: 1, y: 100000}
 { x: 2, y: 1 }
 { x: 2, y: 2 }
 { x: 2, y: 3 }
 ......
 { x: 1, y: 100000}
 要实现 {x: 1, y: 2} 这样的查询。

 db.createIndex( {x: 1} )         效果不好，因为x相同取值太多
 db.createIndex( {x: 1, y: 1} )   效果不好，因为x相同取值太多
 db.createIndex( {y: 1 } )        效果好，因为y相同取值很少
 db.createIndex( {y: 1, x: 1 } )  效果好，因为y相同取值少
 关于{y: 1} 与 {y: 1, x: 1} 的区别，可参考MongoDB索引原理及复合索引官方文档。

 大量数据排序（关键字： SORT、hasSortStage ）
 当查询请求里包含排序的时候， system.profile 集合里的 hasSortStage 字段会为 true 。如果排序无法通过索引满足， MongoDB 会在查询结果中进行排序。而排序这个动作将非常消耗CPU资源，这种情况需要对经常排序的字段建立索引的方式进行优化。
 说明 当您在system.profile集合里发现SORT关键字时，可以考虑通过索引来优化排序。
 其他还有诸如建立索引、aggregation（遍历、查询、更新、排序等动作的组合） 等操作也可能非常耗CPU资源，但本质上也是上述几种场景。更多 profiling 的设置请参考profiling官方文档。

 服务能力评估
 经过上述分析数据库正在执行的请求和分析数据库慢请求两轮优化之后，整个数据库的查询相对合理，所有的请求都高效地使用了索引。

 此时在业务环境使用中还经常遇到CPU资源被占满，那么可能是实例的服务能力已经达到上限了。这种情况下您应当查看监控信息以分析实例资源使用状态；同时对MongoDB数据库进行测试，以便了解在您的业务场景下，当前实例是否满足所需要的设备性能和服务能力。

 如您需要升级实例，可以参考变更配置或变更副本集实例节点数进行操作。

 关于MongoDB Sharding，你应该知道的
 更新时间：2017-12-27 14:47:07


 本页目录
 什么情况下使用Sharded cluster？
 如何确定shard、mongos数量？
 如何选择shard key？
 关于jumbo chunk及chunk size
 Tag aware sharding
 关于负载均衡
 moveChunk归档设置
 recoverShardingState设置
 什么情况下使用Sharded cluster？
 当您遇到如下两个问题时，您可以使用Sharded cluster来解决您的问题：

 存储容量受单机限制，即磁盘资源遭遇瓶颈。
 读写能力受单机限制，可能是CPU、内存或者网卡等资源遭遇瓶颈，导致读写能力无法扩展。
 如何确定shard、mongos数量？
 当您决定使用Sharded cluster时，到底应该部署多少个shard、多少个mongos？shard、mongos的数量归根结底是由应用需求决定：

 如果您使用sharding只是解决海量数据存储问题，访问并不多。假设单个shard能存储M， 需要的存储总量是N，那么您可以按照如下公式来计算实际需要的shard、mongos数量：

 numberOfShards = N/M/0.75 （假设容量水位线为75%）
 numberOfMongos = 2+（对访问要求不高，至少部署2个mongos做高可用即可）

 如果您使用sharding是解决高并发写入（或读取）数据的问题，总的数据量其实很小。您要部署的shard、mongos要满足读写性能需求，容量上则不是考量的重点。假设单个shard最大qps为M，单个mongos最大qps为Ms，需要总的qps为Q。那么您可以按照如下公式来计算实际需要的shard、mongos数量：

 numberOfShards = Q/M /0.75 （假设负载水位线为75%）
 numberOfMongos = Q/Ms/0.75

 注：mongos、mongod的服务能力，需要用户根据访问特性来实测得出。

 如果sharding要同时解决上述2个问题，则按需求更高的指标来预估。以上估算是基于sharded cluster里数据及请求都均匀分布的理想情况。但实际情况下，分布可能并不均衡，为了让系统的负载分布尽量均匀，就需要合理的选择shard key。

 如何选择shard key？
 MongoDB Sharded cluster支持2种分片方式：

 范围分片，通常能很好的支持基于shard key的范围查询。
 Hash 分片，通常能将写入均衡分布到各个shard。
 上述2种分片策略都无法解决以下san个问题：

 shard key取值范围太小(low cardinality)，比如将数据中心作为shard key，而数据中心通常不会很多，分片的效果肯定不好。
 shard key某个值的文档特别多，这样导致单个chunk特别大（及 jumbo chunk），会影响chunk迁移及负载均衡。
 根据非shardkey进行查询、更新操作都会变成scatter-gather查询，影响效率。
 好的shard key应该拥有如下特性：

 key分布足够离散（sufficient cardinality）
 写请求均匀分布（evenly distributed write）
 尽量避免scatter-gather查询（targeted read）
 例如某物联网应用使用MongoDB Sharded cluster存储海量设备的工作日志。假设设备数量在百万级别，设备每10s向 MongoDB汇报一次日志数据，日志包含deviceId，timestamp信息。应用最常见的查询请求是查询某个设备某个时间内的日志信息。以下四个方案中前三个不建议使用，第四个为最优方案，主要是为了给客户做个对比。

 方案1： 时间戳作为shard key，范围分片：

 Bad。
 新的写入都是连续的时间戳，都会请求到同一个shard，写分布不均。
 根据deviceId的查询会分散到所有shard上查询，效率低。
 方案2： 时间戳作为shard key，hash分片：

 Bad。
 写入能均分到多个shard。
 根据deviceId的查询会分散到所有shard上查询，效率低。
 方案3：deviceId作为shardKey，hash分片（如果 id 没有明显的规则，范围分片也一样）：

 Bad。
 写入能均分到多个shard。
 同一个deviceId对应的数据无法进一步细分，只能分散到同一个chunk，会造成jumbo chunk根据deviceId的查询只请求到单个shard。不足的是，请求路由到单个shard后，根据时间戳的范围查询需要全表扫描并排序。
 方案4：(deviceId，时间戳)组合起来作为shardKey，范围分片（Better）：

 Good。
 写入能均分到多个shard。
 同一个deviceId的数据能根据时间戳进一步分散到多个chunk。
 根据deviceId查询时间范围的数据，能直接利用（deviceId，时间戳）复合索引来完成。
 关于jumbo chunk及chunk size
 MongoDB默认的chunk size为64MB，如果chunk超过64MB且不能分裂（比如所有文档的shard key都相同），则会被标记为jumbo chunk ，balancer不会迁移这样的chunk，从而可能导致负载不均衡，应尽量避免。

 一旦出现了jumbo chunk，如果对负载均衡要求不高，并不会影响到数据的读写访问。如果一定要处理，可以尝试如下方法：

 对jumbo chunk进行split，一旦split成功，mongos会自动清除jumbo标记。

 对于不可再分的chunk，如果该chunk已不是jumbo chunk，可以尝试手动清除chunk的jumbo标记（注意先备份下config数据库，以免误操作导致config库损坏）。

 调大chunk size，当chunk大小不超过chunk size时，jumbo标记最终会被清理。但是随着数据的写入仍然会再出现 jumbo chunk，根本的解决办法还是合理的规划shard key。

 关于chunk size如何设置，绝大部分情况下可以直接使用默认的chunk size ，以下场景可能需要调整chunk size（取值在1-1024之间）：

 迁移时IO负载太大，可以尝试设置更小的chunk size。

 测试时，为了方便验证效果，设置较小的chunk size。

 初始chunk size设置不合理，导致出现大量jumbo chunk影响负载均衡，此时可以尝试调大chunk size。

 将未分片的集合转换为分片集合，如果集合容量太大，需要（数据量达到T级别才有可能遇到）调大chunk size才能转换成功。具体方法请参考Sharding Existing Collection Data Size。

 Tag aware sharding
 Tag aware sharding是Sharded cluster很有用的一个特性，允许用户自定义一些chunk的分布规则。Tag aware sharding原理如下：

 sh.addShardTag()给shard设置标签A。

 sh.addTagRange()给集合的某个chunk范围设置标签A，最终MongoDB会保证设置标签A的chunk范围（或该范围的超集）分布设置了标签A的shard上。

 Tag aware sharding可应用在如下场景
 将部署在不同机房的shard设置机房标签，将不同chunk范围的数据分布到指定的机房。

 将服务能力不通的shard设置服务等级标签，将更多的chunk分散到服务能力更强的shard上去。

 使用Tag aware sharding需要注意：

 chunk分配到对应标签的shard上无法立即完成，而是在不断insert、update后触发split、moveChunk后逐步完成的并且需要保证balancer是开启的。在设置了tag range一段时间后，写入仍然没有分布到tag相同的shard上去。

 关于负载均衡
 MongoDB Sharded cluster的自动负载均衡目前是由mongos的后台线程来做，并且每个集合同一时刻只能有一个迁移任务。负载均衡主要根据集合在各个shard上chunk的数量来决定的，相差超过一定阈值（跟chunk总数量相关）就会触发chunk迁移。

 负载均衡默认是开启的，为了避免chunk迁移影响到线上业务，可以通过设置迁移执行窗口，比如只允许凌晨2:00-6:00期间进行迁移。

 use config
 db.settings.update(
    { _id: "balancer" },
    { $set: { activeWindow : { start : "02:00", stop : "06:00" } } },
    { upsert: true }
 )
 注意：在进行sharding备份时（通过mongos或者单独备份config server和所有shard），需要停止负载均衡，以免备份出来的数据出现状态不一致问题。

 sh.stopBalancer()
 moveChunk归档设置
 使用3.0及以前版本的Sharded cluster可能会遇到一个问题，停止写入数据后，数据目录里的磁盘空间占用还会一直增加。

 上述行为是由sharding.archiveMovedChunks配置项决定的，该配置项在3.0及以前的版本默认为true。即在move chunk 时，源shard会将迁移的chunk数据归档一份在数据目录里，当出现问题时，可用于恢复。也就是说，chunk发生迁移时，源节点上的空间并没有释放出来，而目标节点又占用了新的空间。

 说明：在3.2版本，该配置项默认值也被设置为false，默认不会对moveChunk的数据在源shard上归档。

 recoverShardingState设置
 使用MongoDB Sharded cluster时，还可能遇到一个问题：

 启动shard后，shard不能正常服务，Primary上调用ismaster时，结果却为true，也无法正常执行其他命令，其状态类似如下：

 mongo-9003:PRIMARY> db.isMaster()
 {
     "hosts" : [
         "host1:9003",
         "host2:9003",
         "host3:9003"
     ],
     "setName" : "mongo-9003",
     "setVersion" : 9,
     "ismaster" : false,  // primary 的 ismaster 为 false？？？
     "secondary" : true,
     "primary" : "host1:9003",
     "me" : "host1:9003",
     "electionId" : ObjectId("57c7e62d218e9216c70aa3cf"),
     "maxBsonObjectSize" : 16777216,
     "maxMessageSizeBytes" : 48000000,
     "maxWriteBatchSize" : 1000,
     "localTime" : ISODate("2016-09-01T12:29:27.113Z"),
     "maxWireVersion" : 4,
     "minWireVersion" : 0,
     "ok" : 1
 }
 查看其错误日志，会发现shard一直无法连接上config server，是由sharding.recoverShardingState选项决定，默认为true。也就是说shard启动时会连接config server进行sharding状态的一些初始化，如果config server连不上，初始化工作就一直无法完成，导致shard 状态不正常。

 在您将Sharded cluster所有节点都迁移到新的主机上时可能会遇到了上述问题，因为config server的信息发生变化，而 shard启动时还会连接之前的config server。通过在启动命令行加上setParameter recoverShardingState=false来启动shard就能恢复正常。


通过数据集成导入导出MongoDB数据
更新时间：2017-08-18 17:50:06


本页目录
操作步骤
数据集成（Data Integration）是阿里集团对外提供的可跨异构数据存储系统的、可靠、安全、低成本、可弹性扩展的数据同步平台，为20+种数据源提供不同网络环境下的离线(全量/增量)数据进出通道。详细的数据源类型列表请参见支持数据源类型。用户可以通过数据集成（Data Integration）对云产品MongoDB进行数据的导入和导出。

数据导入和导出均有以下两种实现方式：

向导模式：向导模式是可视化界面配置同步任务， 一共涉及到五步，选择来源，选择目标，字段映射，通道控制，预览保存。在每个不同的数据源之间，这几步的界面可能有不同的内容，向导模式可以转换成脚本模式。向导模式不支持创建同步任务。

脚本模式：进入脚本界面你可以选择相应的模板，此模板包含了同步任务的主要参数，然后补全剩余的参数也能创建同步任务。但是脚本模式不能转化成向导模式。

本文主要介绍如何将Table Store中的数据导入到MongoDB中，将MongoDB中的数据导出到Table Store中操作步骤与导入类似，因此本文将不再赘述数据如何导出。

注意：

只有项目管理员角色才能够新建数据源，其他角色的成员仅能查看数据源。

如您想用子账号创建数据集成任务，需赋予子账号相应的权限。具体请参考：开通阿里云主账号、设置子账号。

操作步骤
以项目管理员身份进入数加管理控制台，单击项目列表下对应项目操作栏中的进入工作区。如何创建项目请参考创建项目。
进入顶部菜单栏中的数据集成页面，单击左侧导航栏中的数据源。

单击右上角的新增数据源，如下图所示：

数据源

在新增数据源对话框中填写相关配置项，针对MongoDB数据源配置项的具体说明如下：

数据源名称： 由英文字母、数字、下划线组成且需以字符或下划线开头，长度不超过60个字符 。
数据源描述： 对数据源进行简单描述，不得超过80个字符 。
数据源类型：当前选择的数据源类型MongoDB：阿里云数据库和有公网IP的自建数据库。
访问地址：格式为：host:port 。
添加访问地址：添加访问地址，格式：host:port 。
数据库名：该数据源对应的数据库名 。
用户名/密码：数据库对应的用户名和密码 。
完成上述信息项的配置后，单击测试连通性。测试通过单击确定。

新建同步任务，单击数据集成下的同步任务，并选择脚本模式，如下图所示：

10.jpg

在弹出的导入模板中选择自己需要的来源类型和目标类型，如下图所示：

向导模式

单击确认后即进入脚本模式配置页面，可根据自身情况进行配置，如有问题可单击右上方的帮助手册进行查看，如下图所示：

向导配置

单击运行即可。

如下是一个完整的MongoDBReader脚本案例：

{
  "type": "job",
  "configuration": {
    "setting": {
      "speed": {
        "concurrent": "1",//并发数
        "mbps": "1"//同步能达到的最大数率
      },
      "errorLimit": {
        "record": "0"//错误记录数
      }
    },
    "reader": {
      "parameter": {
        "column": [
          {
            "name": "name",
            "type": "string"
          },
          {
            "name": "year",
            "type": "int"
          }
        ],
        "datasource": "px_mongodb_datasource",//数据源名，建议数据源都先添加数据源后再配置同步任务,此配置项填写的内容必须要与添加的数据源名称保持一致
        "collectionName": "px"
      },
      "plugin": "mongodb"
    },
    "writer": {
      "parameter": {
        "writeMode": "insert",//写入模式
        "preSql": [],//导入前准备语句
        "column": [
          "name",
          "year"
        ],
        "table": "person",//
        "datasource": "px_mysql",//数据源名，建议数据源都先添加数据源后再配置同步任务,此配置项填写的内容必须要与添加的数据源名称保持一致
        "postSql": []
      },
      "plugin": "mysql"
    }
  },
  "version": "1.0"
}

MongoDB 复制集原理深度分析
更新时间：2017-07-17 09:49:21


本页目录
复制集简介
Primary 选举（一）
特殊的 Secondary 节点
Primary 选举 （二）
数据同步
复制集的读写设置
复制集简介
Mongodb 复制集由一组 Mongod 实例（进程）组成，包含一个 Primary 节点和多个 Secondary 节点，Mongodb Driver（客户端）的所有数据都写入 Primary，Secondary 从 Primary 同步写入的数据，以保持复制集内所有成员存储相同的数据集，提供数据的高可用。

下图（图片源于 Mongodb 官方文档）是一个典型的 Mongdb 复制集，包含一个 Primary 节点和2个 Secondary 节点。



Primary 选举（一）
复制集通过 replSetInitiate 命令（或 mongo shell 的 rs.initiate()）进行初始化，初始化后各个成员间开始发送心跳消息，并发起 Primary 选举操作，获得大多数成员投票支持的节点，会成为 Primary，其余节点成为 Secondary。

初始化复制集
config = {
    _id : "my_replica_set",
    members : [
         {_id : 0, host : "rs1.example.net:27017"},
         {_id : 1, host : "rs2.example.net:27017"},
         {_id : 2, host : "rs3.example.net:27017"},
   ]
}
rs.initiate(config)
“大多数”的定义
假设复制集内投票成员（后续介绍）数量为 N，则大多数为 N/2 + 1，当复制集内存活成员数量不足大多数时，整个复制集将无法选举出 Primary，复制集将无法提供写服务，处于只读状态。

投票成员数	大多数	容忍失效数
1	1	0
2	2	0
3	2	1
4	3	1
5	3	2
6	4	2
7	4	3
通常建议将复制集成员数量设置为奇数，从上表可以看出3个节点和4个节点的复制集都只能容忍1个节点失效，从服务可用性的角度看，其效果是一样的，但无疑4个节点能提供更可靠的数据存储。

特殊的 Secondary 节点
正常情况下，复制集的 Secondary 会参与 Primary 选举（自身也可能会被选为 Primary），并从 Primary 同步最新写入的数据，以保证与 Primary 存储相同的数据。

Secondary 可以提供读服务，增加 Secondary 节点可以提供复制集的读服务能力，同时提升复制集的可用性。另外，Mongodb 支持对复制集的 Secondary 节点进行灵活的配置，以适应多种场景的需求。

Arbiter
Arbiter 节点只参与投票，不能被选为 Primary，并且不从 Primary 同步数据。

比如你部署了一个2个节点的复制集，1个 Primary，1个 Secondary，任意节点宕机，复制集将不能提供服务了（无法选出 Primary），这时可以给复制集添加一个 Arbiter 节点，即使有节点宕机，仍能选出 Primary。

Arbiter 本身不存储数据，是非常轻量级的服务，当复制集成员为偶数时，最好加入一个 Arbiter 节点，以提升复制集可用性。

Priority0
Priority0节点的选举优先级为0，不会被选举为 Primary。

比如你跨机房 A、B 部署了一个复制集，并且想指定 Primary 必须在 A 机房，这时可以将 B 机房的复制集成员 Priority 设置为0，这样 Primary 就一定会是 A 机房的成员。（注意：如果这样部署，最好将大多数节点部署在 A 机房，否则网络分区时可能无法选出 Primary。）

Vote0
Mongodb 3.0里，复制集成员最多50个，参与 Primary 选举投票的成员最多7个，其他成员（Vote0）的 vote 属性必须设置为0，即不参与投票。

Hidden
Hidden 节点不能被选为主（Priority 为0），并且对 Driver 不可见。

因 Hidden 节点不会接受 Driver 的请求，可使用 Hidden 节点做一些数据备份、离线计算的任务，不会影响复制集的服务。

Delayed
Delayed 节点必须是 Hidden 节点，并且其数据落后与 Primary 一段时间（可配置，比如1个小时）。

因 Delayed 节点的数据比 Primary 落后一段时间，当错误或者无效的数据写入 Primary 时，可通过 Delayed 节点的数据来恢复到之前的时间点。

Primary 选举 （二）
Primary 选举除了在复制集初始化时发生，还有如下场景。

复制集被 reconfig
Secondary 节点检测到 Primary 宕机时，会触发新 Primary 的选举，当有 Primary 节点主动 stepDown（主动降级为 Secondary）时，也会触发新的 Primary 选举。Primary 的选举受节点间心跳、优先级、最新的 oplog 时间等多种因素影响。

节点优先级
每个节点都倾向于投票给优先级最高的节点。优先级为0的节点不会主动发起 Primary 选举。当 Primary 发现有优先级更高 Secondary，并且该 Secondary 的数据落后在10s内，则 Primary 会主动降级，让优先级更高的 Secondary 有成为 Primary 的机会。

Optime
拥有最新 optime（最近一条 oplog 的时间戳）的节点才能被选为 Primary。

网络分区
只有更大多数投票节点间保持网络连通，才有机会被选 Primary；如果 Primary 与大多数的节点断开连接，Primary 会主动降级为 Secondary。当发生网络分区时，可能在短时间内出现多个 Primary，故 Driver 在写入时，最好设置大多数成功的策略，这样即使出现多个 Primary，也只有一个 Primary 能成功写入大多数。

数据同步
Primary 与 Secondary 之间通过 oplog 来同步数据，Primary 上的写操作完成后，会向特殊的 local.oplog.rs 特殊集合写入一条 oplog，Secondary 不断的从 Primary 取新的 oplog 并应用。

因 oplog 的数据会不断增加，local.oplog.rs 被设置成为一个 capped 集合，当容量达到配置上限时，会将最旧的数据删除掉。另外考虑到 oplog 在 Secondary 上可能重复应用，oplog 必须具有幂等性，即重复应用也会得到相同的结果。

如下 oplog 的格式，包含 ts、h、op、ns、o 等字段。

{
  "ts" : Timestamp(1446011584, 2),
  "h" : NumberLong("1687359108795812092"),
  "v" : 2,
  "op" : "i",
  "ns" : "test.nosql",
  "o" : { "_id" : ObjectId("563062c0b085733f34ab4129"), "name" : "mongodb", "score" : "100" }
}
ts：操作时间，当前 timestamp + 计数器，计数器每秒都被重置；
h：操作的全局唯一标识；
v：oplog 版本信息；
op：操作类型；
i：插入操作；
u：更新操作；
d：删除操作；
c：执行命令（如 createDatabase，dropDatabase）；
n：空操作，特殊用途；
ns：操作针对的集合；
o：操作内容，如果是更新操作；
o2：操作查询条件，仅 update 操作包含该字段。
Secondary 初次同步数据时，会先执行 init sync，从 Primary（或其他数据更新的 Secondary）同步全量数据，然后不断通过执行tailable cursor从 Primary 的 local.oplog.rs 集合里查询最新的 oplog 并应用到自身。

init sync 过程
init sync 过程包含如下步骤：

T1时间，从 Primary 同步所有数据库的数据（local 除外），通过 listDatabases+ listCollections + cloneCollection 敏命令组合完成，假设 T2时间完成所有操作。

从 Primary 应用[T1-T2]时间段内的所有 oplog，可能部分操作已经包含在步骤1，但由于 oplog 的幂等性，可重复应用。

根据 Primary 各集合的 index 设置，在 Secondary 上为相应集合创建 index。（每个集合_id 的 index 已在步骤1中完成）。

注意：oplog 集合的大小应根据 DB 规模及应用写入需求合理配置，配置得太大，会造成存储空间的浪费；配置得太小，可能造成 Secondary 的 init sync 一直无法成功。比如在步骤1里由于 DB 数据太多、并且 oplog 配置太小，导致 oplog 不足以存储[T1, T2]时间内的所有 oplog，这就 Secondary 无法从 Primary 上同步完整的数据集。

修改复制集配置
当需要修改复制集时，比如增加成员、删除成员、或者修改成员配置（如 priorty、vote、hidden、delayed 等属性），可通过 replSetReconfig 命令（rs.reconfig()）对复制集进行重新配置。

比如将复制集的第2个成员 Priority 设置为2，可执行如下命令：

cfg = rs.conf();
cfg.members[1].priority = 2;
rs.reconfig(cfg);
异常处理（rollback）
当 Primary 宕机时，如果有数据未同步到 Secondary，当 Primary 重新加入时，如果新的 Primary 上已经发生了写操作，则旧 Primary 需要回滚部分操作，以保证数据集与新的 Primary 一致。

旧 Primary 将回滚的数据写到单独的 rollback 目录下，数据库管理员可根据需要使用 mongorestore 进行恢复。

复制集的读写设置
Read Preference
默认情况下，复制集的所有读请求都发到 Primary，Driver 可通过设置 Read Preference 来将读请求路由到其他的节点。

primary：默认规则，所有读请求发到 Primary；
primaryPreferred：Primary 优先，如果 Primary 不可达，请求 Secondary；
secondary：所有的读请求都发到 secondary；
secondaryPreferred：Secondary 优先，当所有 Secondary 不可达时，请求 Primary；
nearest：读请求发送到最近的可达节点上（通过 ping 探测得出最近的节点）。
Write Concern
默认情况下，Primary 完成写操作即返回，Driver 可通过设置 Write Concern (参见这里)来设置写成功的规则。

如下的 write concern 规则设置写必须在大多数节点上成功，超时时间为5s。

db.products.insert(
  { item: "envelopes", qty : 100, type: "Clasp" },
  { writeConcern: { w: majority, wtimeout: 5000 } }
)
上面的设置方式是针对单个请求的，也可以修改副本集默认的 write concern，这样就不用每个请求单独设置。

cfg = rs.conf()
cfg.settings = {}
cfg.settings.getLastErrorDefaults = { w: "majority", wtimeout: 5000 }
rs.reconfig(cfg)