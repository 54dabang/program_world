第1章 Elasticsearch开发搜索引擎应用1
1.1 搜索引擎开发需求1
1.2 准备开发环境1
1.2.1 Windows命令行cmd1
1.2.2 在Windows下使用Java3
1.2.3 Linux终端5
1.2.4 在Linux下使用Java9
1.2.5 Eclipse集成开发环境10
1.3 了解Elasticsearch10
1.3.1 JSON数据格式11
1.3.2 Elasticsearch基本概念12
1.3.3 HTTP协议13
1.4 Elasticsearch安装和配置16
1.4.1 安装Elasticsearch16
1.4.2 运行Elasticsearch作为服务进程19
1.5 实现一个简单的网站搜索21
1.5.1 定义索引结构23
1.5.2 导入数据26
1.5.3 查询API27
1.5.4 实现搜索界面29
1.6 本章小结35
第2章 开发中文搜索引擎36
2.1 中文分词原理36
2.1.1 最长匹配方法36
2.1.2 自己写分析器42
2.1.3 概率语言模型的分词方法44
2.1.4 中文分词插件原理52
2.1.5 开发中文分词插件54
2.1.6 支持Elasticsearch的插件57
2.1.7 中文分析器提供者59
2.1.8 字词混合索引61
2.2 提高分词准确度63
2.3 本章小结65
第3章 Mapping详解66
3.1 索引模式66
3.1.1 创建模式66
3.1.2 修改模式68
3.2 Mapping数据类型69
3.3 Mapping参数70
3.4 动态Mapping71
3.4.1 使用动态Mapping72
3.4.2 实现原理72
3.5 本章小结74
第4章 深入源码分析75
4.1 Lucene源码分析75
4.1.1 使用Lucene75
4.1.2 Ivy管理依赖项77
4.1.3 源码结构介绍77
4.1.4 并发控制82
4.2 启动搜索服务88
4.3 Guice框架89
4.4 日期和时间库——Joda-Time91
4.5 Transport模块91
4.6 线程池92
4.7 模块93
4.8 Netty通信框架93
4.9 缓存94
4.10 分布式95
4.11 Zen发现机制95
4.12 联合搜索97
4.13 JVM字节码98
4.13.1 编译代码99
4.13.2 同步相关指令99
4.14 本章小结100
第5章 提高搜索相关性102
5.1 向量空间检索模型102
5.2 BM25检索模型105
5.2.1 使用BM25检索模型108
5.2.2 参数调优108
5.3 学习评分109
5.3.1 基本原理109
5.3.2 准备数据110
5.3.3 Elasticsearch学习排名112
5.4 查询意图识别112
5.5 图像特征提升检索体验113
5.6 本章小结116
第6章 搜索界面开发118
6.1 使用Searchkit实现搜索界面118
6.2 Spring Boot入门122
6.2.1 可执行的WAR125
6.2.2 spring-boot-devtools模块实现热部署136
6.3 Java模板引擎Pebble介绍136
6.4 通过Spring-data-elasticsearch 项目访问Elasticsearch141
6.5 REST基本概念149
6.6 使用Vue.js开发搜索界面154
6.7 使用Vue.js Paginator插件实现翻页157
6.8 实现搜索接口161
6.8.1 编码识别161
6.8.2 布尔搜索163
6.8.3 搜索结果重定向164
6.8.4 搜索结果排序165
6.8.5 实现相似文档搜索166
6.9 Suggester搜索词提示167
6.9.1 拼音提示169
6.9.2 部署总结169
6.9.3 相关搜索170
6.9.4 再次查找172
6.9.5 搜索日志172
6.10 Word2vec挖掘相关搜索词174
6.11 部署网站179
6.11.1 部署到Web服务器179
6.11.2 防止攻击181
6.12 使用Rust开发搜索界面184
6.13 本章小结184
第7章 Elastic栈系统监控186
7.1 管理Elasticsearch集群186
7.1.1 写入权限控制187
7.1.2 使用X-Pack188
7.1.3 快照189
7.2 Logstash数据处理工具190
7.2.1 使用Logstash190
7.2.2 插件192
7.2.3 数据库输入插件192
7.2.4 开发插件193
7.3 Filebeat文件收集器193
7.4 消息过期194
7.5 Kibana可视化平台195
7.6 Flume日志收集系统196
7.7 Kafka分布式流平台197
7.8 Graylog日志管理平台198
7.9 本章小结202
第8章 案例分析204
8.1 双语句对搜索204
8.1.1 爬虫抓取双语句对204
8.1.2 英文分词205
8.1.3 句子切分205
8.1.4 标注词性207
8.1.5 词对齐209
8.1.6 索引数据213
8.2 内容管理系统站内检索214
8.2.1 MySQL数据库214
8.2.2 RESTful API管理索引215
8.2.3 自动客服机器人217
8.3 搜索文档225
8.3.1 爬虫抓取信息225
8.3.2 在Linux下使用.NET233
8.3.3 NEST客户端235
8.4 本章小结239
参考文献240


1．1　用Elasticsearch解决搜索问题　3
1．1．1　提供快速查询　3
1．1．2　确保结果的相关性　4
1．1．3　超越精确匹配　5
1．2　探索典型的Elasticsearch使用案例　6
1．2．1　将Elasticsearch作为主要的后端系统　7
1．2．2　将Elasticsearch添加到现有的系统　7
1．2．3　将Elasticsearch和现有工具一同使用　8
1．2．4　Elasticsearch的主要特性　10
1．2．5　扩展Lucene的功能　10
1．2．6　在Elasticsearch中组织数据　12
1．2．7　安装Java语言　12
1．2．8　下载并启动Elasticsearch　13
1．2．9　验证是否工作　14
1．3　小结　16
第2章　深入功能　17
2．1　理解逻辑设计：文档、类型和索引　18
2．1．1　文档　19
2．1．2　类型　20
2．1．3　索引　21
2．2　理解物理设计：节点和分片　21
2．2．1　创建拥有一个或多个节点的集群　22
2．2．2　理解主分片和副本分片　23
2．2．3　在集群中分发分片　25
2．2．4　分布式索引和搜索　26
2．3　索引新数据　27
2．3．1　通过cURL索引一篇文档　28
2．3．2　创建索引和映射类型　30
2．3．3　通过代码样例索引文档　31
2．4　搜索并获取数据　32
2．4．1　在哪里搜索　33
2．4．2　回复的内容　33
2．4．3　如何搜索　36
2．4．4　通过ID获取文档　39
2．5　配置Elasticsearch　40
2．5．1　在elasticsearch．yml中指定集群的名称　40
2．5．2　通过logging．yml指定详细日志记录　41
2．5．3　调整JVM设置　41
2．6　在集群中加入节点　42
2．6．1　启动第二个节点　43
2．6．2　增加额外的节点　44
2．7　小结　45
第3章　索引、更新和删除数据　47
3．1　使用映射来定义各种文档　48
3．1．1　检索和定义映射　49
3．1．2　扩展现有的映射　50
3．2　用于定义文档字段的核心类型　51
3．2．1　字符串类型　52
3．2．2　数值类型　54
3．2．3　日期类型　55
3．2．4　布尔类型　56
3．3　数组和多字段　56
3．3．1　数组　56
3．3．2　多字段　57
3．4　使用预定义字段　58
3．4．1　控制如何存储和搜索文档　59
3．4．2　识别文档　61
3．5　更新现有文档　63
3．5．1　使用更新API　64
3．5．2　通过版本来实现并发控制　66
3．6　删除数据　69
3．6．1　删除文档　70
3．6．2　删除索引　71
3．6．3　关闭索引　72
3．6．4　重新索引样本文档　73
3．7　小结　73
第4章　搜索数据　74
4．1　搜索请求的结构　75
4．1．1　确定搜索范围　75
4．1．2　搜索请求的基本模块　76
4．1．3　基于请求主体的搜索请求　78
4．1．4　理解回复的结构　81
4．2　介绍查询和过滤器DSL　82
4．2．1　match查询和term过滤器　82
4．2．2　常用的基础查询和过滤器　85
4．2．3　match查询和term过滤器　91
4．2．4　phrase_prefix查询　92
4．3　组合查询或复合查询　93
4．3．1　bool查询　93
4．3．2　bool过滤器　96
4．4　超越match和过滤器查询　98
4．4．1　range查询和过滤器　98
4．4．2　prefix查询和过滤器　99
4．4．3　wildcard查询　100
4．5　使用过滤器查询字段的存在性　102
4．5．1　exists过滤器　102
4．5．2　missing过滤器　102
4．5．3　将任何查询转变为过滤器　103
4．6　为任务选择最好的查询　104
4．7　小结　105
第5章　分析数据　106
5．1　什么是分析　106
5．1．1　字符过滤　107
5．1．2　切分为分词　108
5．1．3　分词过滤器　108
5．1．4　分词索引　108
5．2　为文档使用分析器　109
5．2．1　在索引创建时增加分析器　109
5．2．2　在Elasticsearch的配置中添加分析器　111
5．2．3　在映射中指定某个字段的分析器　112
5．3　使用分析API来分析文本　113
5．3．1　选择一个分析器　114
5．3．2　通过组合即兴地创建分析器　115
5．3．3　基于某个字段映射的分析　115
5．3．4　使用词条向量API来学习索引词条　116
5．4　分析器、分词器和分词过滤器　117
5．4．1　内置的分析器　117
5．4．2　分词器　119
5．4．3　分词过滤器　122
5．5　N元语法、侧边N元语法和滑动窗口　128
5．5．1　一元语法过滤器　128
5．5．2　二元语法过滤器　129
5．5．3　三元语法过滤器　129
5．5．4　设置min_gram和max_gram　129
5．5．5　侧边N元语法过滤器　129
5．5．6　N元语法的设置　130
5．5．7　滑动窗口分词过滤器　131
5．6　提取词干　132
5．6．1　算法提取词干　133
5．6．2　使用字典提取词干　133
5．6．3　重写分词过滤器的词干提取　134
5．7　小结　134
第6章　使用相关性进行搜索　136
6．1　Elasticsearch的打分机制　137
6．1．1　文档打分是如何运作的　137
6．1．2　词频　137
6．1．3　逆文档频率　138
6．1．4　Lucene评分公式　138
6．2　其他打分方法　139
6．3　boosting　141
6．3．1　索引期间的boosting　142
6．3．2　查询期间的boosting　142
6．3．3　跨越多个字段的查询　143
6．4　使用“解释”来理解文档是如何被评分的　144
6．5　使用查询再打分来减小评分操作的性能影响　147
6．6　使用function_score来定制得分　148
6．6．1　weight函数　149
6．6．2　合并得分　150
6．6．3　field_value_factor函数　151
6．6．4　脚本　152
6．6．5　随机　152
6．6．6　衰减函数　153
6．6．7　配置选项　155
6．7　尝试一起使用它们吧　156
6．8　使用脚本来排序　157
6．9　字段数据　158
6．9．1　字段数据缓存　158
6．9．2　字段数据用在哪里　159
6．9．3　管理字段数据　160
6．10　小结　163
第7章　使用聚集来探索数据　164
7．1　理解聚集的具体结构　166
7．1．1　理解聚集请求的结构　166
7．1．2　运行在查询结果上的聚集　168
7．1．3　过滤器和聚集　169
7．2　度量聚集　170
7．2．1　统计数据　171
7．2．2　高级统计　172
7．2．3　近似统计　173
7．3　多桶型聚集　176
7．3．1　terms聚集　177
7．3．2　range聚集　183
7．3．3　histogram聚集　185
7．4　嵌套聚集　187
7．4．1　嵌套多桶聚集　189
7．4．2　通过嵌套聚集获得结果分组　190
7．4．3　使用单桶聚集　192
7．5　小结　196
第8章　文档间的关系　197
8．1　定义文档间关系的选项概览　197
8．1．1　对象类型　198
8．1．2　嵌套类型　200
8．1．3　父子关系　200
8．1．4　反规范化　200
8．2　将对象作为字段值　202
8．2．1　映射和索引对象　203
8．2．2　在对象中搜索　204
8．3　嵌套类型：联结嵌套的文档　206
8．3．1　映射并索引嵌套文档　207
8．3．2　搜索和聚集嵌套文档　210
8．4　父子关系：关联分隔的文档　216
8．4．1　子文档的索引、更新和删除　218
8．4．2　在父文档和子文档中搜索　220
8．5　反规范化：使用冗余的数据管理　227
8．5．1　反规范化的使用案例　228
8．5．2　索引、更新和删除反规范化的数据　230
8．5．3　查询反规范化的数据　233
8．6　应用端的连接　234
8．7　小结　235
第二部分
第9章　向外扩展　238
9．1　向Elasticsearch集群加入节点　238
9．2　发现其他Elasticsearch节点　241
9．2．1　通过广播来发现　241
9．2．2　通过单播来发现　242
9．2．3　选举主节点和识别错误　243
9．2．4　错误的识别　244
9．3　删除集群中的节点　245
9．4　升级Elasticsearch的节点　250
9．4．1　进行轮流重启　250
9．4．2　最小化重启后的恢复时间　251
9．5　使用_cat API　252
9．6　扩展策略　254
9．6．1　过度分片　254
9．6．2　将数据切分为索引和分片　255
9．6．3　最大化吞吐量　256
9．7　别名　257
9．7．1　什么是别名　258
9．7．2　别名的创建　259
9．8　路由　261
9．8．1　为什么使用路由　261
9．8．2　路由策略　262
9．8．3　使用_search_shards API来决定搜索在哪里执行　263
9．8．4　配置路由　265
9．8．5　结合路由和别名　265
9．9　小结　267
第10章　提升性能　268
10．1　合并请求　269
10．1．1　批量索引、更新和 删除　269
10．1．2　多条搜索和多条获取 API接口　273
10．2　优化Lucene分段的 处理　276
10．2．1　刷新和冲刷的阈值　276
10．2．2　合并以及合并策略　279
10．2．3　存储和存储限流　282
10．3　充分利用缓存　285
10．3．1　过滤器和过滤器 缓存　285
10．3．2　分片查询缓存　291
10．3．3　JVM堆和操作系统 缓存　293
10．3．4　使用预热器让缓存 热身　296
10．4　其他的性能权衡　297
10．4．1　大规模的索引还是 昂贵的搜索　298
10．4．2　调优脚本，要么 别用它　301
10．4．3　权衡网络开销，更少的 数据和更好的分布式 得分　305
10．4．4　权衡内存，进行深度 分页　308
10．5　小结　310
第11章　管理集群　311
11．1　改善默认的配置　311
11．1．1　索引模板　312
11．1．2　默认的映射　315
11．2　分配的感知　318
11．2．1　基于分片的分配　318
11．2．2　强制性的分配感知　319
11．3　监控瓶颈　320
11．3．1　检查集群的健康 状态　320
11．3．2　CPU：慢日志、热线程和 线程池　322
11．3．3　内存：堆的大小、字段和 过滤器缓存　326
11．3．4　操作系统缓存　330
11．3．5　存储限流　330
11．4　备份你的数据　331
11．4．1　快照API　331
11．4．2　将数据备份到共享的文件系统　332
11．4．3　从备份中恢复　335
11．4．4　使用资料库插件　336
11．5　小结　337
附录A　处理地理空间的数据（网上下载）
附录B　插件（网上下载）
附录C　高亮（网上下载）
附录D　Elasticsearch的监控插件（网上下载）
附录E　使用渗滤器将搜索颠倒过来（网上下载）
附录F　为自动完成和“您是指”功能使用建议器（网上下载）

第1章　走进Elasticsearch
1．1　基本概念和原理
1．1．1　索引结构
1．1．2　分片（shard）
1．1．3　动态更新索引
1．1．4　近实时搜索
1．1．5　段合并
1．2　集群内部原理
1．2．1　集群节点角色
1．2．2　集群健康状态
1．2．3　集群状态
1．2．4　集群扩容
1．3 客户端API
1．4　主要内部模块简介
1．4．1 模块结构
1．4．2 模块管理
第2章　准备编译和调试环境
2．1 编译源码
2．1．1 准备JDK和Gradle
2．1．2 下载源代码
2．1．3 编译项目，打包
2．1．4 将工程导入IntelliJ IDEA
2．2 调试Elasticsearch
2．2．1 本地运行、调试项目
2．2．2 远程调试
2．3 代码书签和断点组
第3章　集群启动流程
3．1 选举主节点
3．2　选举集群元信息
3．3　allocation过程
3．4　index recovery
3．5 集群启动日志
3．6 小结
第4章　节点的启动和关闭
4．1 启动流程做了什么
4．2 启动流程分析
4．2．1 启动脚本
4．2．2 解析命令行参数和配置文件
4．2．3 加载安全配置
4．2．4 检查内部环境
4．2．5 检测外部环境
4．2．6 启动内部模块
4．2．7 启动keepalive线程
4．3 节点关闭流程
4．4 关闭流程分析
4．5 分片读写过程中执行关闭
4．6 主节点被关闭
4．7 小结
第5章　选主流程
5．1 设计思想
5．2 为什么使用主从模式
5．3 选举算法
5．4 相关配置
5．5 流程概述
5．6 流程分析
5．6．1 选举临时Master
5．6．2 投票与得票的实现
5．6．3 确立Master或加入集群
5．7 节点失效检测
5．7．1 NodesFaultDetection事件处理
5．7．2 MasterFaultDetection事件处理
5．8 小结
第6章　数据模型
6．1 PacificA算法
6．1．1 数据副本策略
6．1．2 配置管理
6．1．3 错误检测
6．2 ES的数据副本模型
6．2．1 基本写入模型
6．2．2 写故障处理
6．2．3 基本读取模型
6．2．4 读故障处理
6．2．5 引申的含义
6．2．6 系统异常
6．3 Allocation IDs
6．3．1 安全地分配主分片
6．3．2 将分配标记为陈旧
6．2．3 一个例子
6．3．4 不会丢失全部
6．4 Sequence IDs
6．4．1 Primary Terms和Sequence Numbers
6．4．2 本地及全局检查点
6．4．3 用于快速恢复（Recovery）
6．5 _version
第7章　写流程
7．1 文档操作的定义
7．2 可选参数
7．3 Index/Bulk基本流程
7．4 Index/Bulk详细流程
7．4．1 协调节点流程
7．4．2 主分片节点流程
7．4．3 副分片节点流程
7．5 I/O异常处理
7．5．1 Engine关闭过程
7．5．2 Master的对应处理
7．5．3 异常流程总结
7．6 系统特性
7．7 思考
第8章　GET流程
8．1 可选参数
8．2 GET基本流程
8．3 GET详细分析
8．3．1 协调节点
8．3．2 数据节点
8．4 MGET流程分析
8．5 思考
第9章　Search流程
9．1 索引和搜索
9．1．1 建立索引
9．1．2 执行搜索
9．2 search type
9．3 分布式搜索过程
9．3．1 协调节点流程
9．3．2　执行搜索的数据节点流程
9．4 小结
第10章　索引恢复流程分析
10．1 相关配置
10．2 流程概述
10．3 主分片恢复流程
10．4 副分片恢复流程
10．4．1 流程概述
10．4．2 synced flush机制
10．4．3 副分片节点处理过程
10．4．4 主分片节点处理过程
10．5 recovery速度优化
10．6 如何保证副分片和主分片一致
10．7 recovery相关监控命令
10．8 小结
第11章　gateway模块分析
11．1 元数据
11．2 元数据的持久化
11．3 元数据的恢复
11．4 元数据恢复流程分析
11．4．1 选举集群级和索引级别的元数据
11．4．2 触发allocation
11．5 思考
第12章　allocation模块分析
12．1 什么是allocation
12．2 触发时机
12．3 allocation模块结构概述
12．4 allocators
12．5 deciders
12．5．1 负载均衡类
12．5．2 并发控制类
12．5．3 条件限制类
12．6 核心reroute实现
12．6．1 集群启动时reroute的触发时机
12．6．2 流程分析
12．6．3 gatewayAllocator
12．6．4 shardsAllocator
12．7 从gateway到allocation流程的转换
12．8 从allocation流程到recovery流程的转换
12．9 思考
第13章　Snapshot模块分析
13．1 仓库
13．2 快照
13．2．1 创建快照
13．2．2 获取快照信息
13．2．3 快照status
13．2．4 取消、删除快照和恢复操作
13．3 从快照恢复
13．3．1 部分恢复
13．3．2 恢复过程中更改索引设置
13．3．3 监控恢复进度
13．4 创建快照的实现原理
13．4．1 Lucene文件格式简介
13．4．2 协调节点流程
13．4．3 主节点流程
13．4．4 数据节点流程
13．5 删除快照实现原理
13．5．1 协调节点流程
13．5．2 主节点流程
13．6 思考与总结
第14章　Cluster模块分析
14．1 集群状态
14．2 内部封装和实现
14．2．1 MasterService
14．2．2 ClusterApplierService
14．2．3 线程池
14．3 提交集群任务
14．3．1 内部模块如何提交任务
14．3．2 任务提交过程实现
14．4 集群任务的执行过程
14．5 集群状态的发布过程
14．5．1 增量发布的实现原理
14．5．2 二段提交总流程
14．5．3 发布过程
14．5．4 提交过程
14．5．5 异常处理
14．6 应用集群状态
14．7 查看等待执行的集群任务
14．8 任务管理API
14．8．1 列出运行中的任务
14．8．2 取消任务
14．9 思考与总结
第15章　Transport模块分析
15．1 配置信息
15．1．1 传输模块配置
15．1．2 通用网络配置
15．2 Transport总体架构
15．2．1 网络层
15．2．2 服务层
15．3 REST解析和处理
15．4 RPC实现
15．4．1 RPC的注册和映射
15．4．2 根据Action获取处理类
15．5 思考与总结
第16章　ThreadPool模块分析
16．1 线程池类型
16．1．1 fixed
16．1．2 scaling
16．1．3 direct
16．1．4 fixed_auto_queue_size
16．2 处理器设置
16．3 查看线程池
16．3．1 cat thread pool
16．3．2 nodes info
16．3．3 nodes stats
16．3．4 nodes hot threads
16．3．5 Java的线程池结构
16．4 ES的线程池实现
16．4．1 ThreadPool类结构与初始化
16．4．2 fixed类型线程池构建过程
16．4．3 scaling类型线程池构建过程
16．4．4 direct类型线程池构建过程
16．4．5 fixed_auto_queue_size类型线程池构建过程
16．5 其他线程池
16．6 思考与总结
第17章　Shrink原理分析
17．1 准备源索引
17．2 缩小索引
17．3 Shrink的工作原理
17．3．1 创建新索引
17．3．2 创建硬链接
17．3．3 硬链接过程源码分析
第18章　写入速度优化
18．1 translog flush间隔调整
18．2 索引刷新间隔refresh_interval
18．3 段合并优化
18．4 indexing buffer
18．5 使用bulk请求
18．5．1 bulk线程池和队列
18．5．2 并发执行bulk请求
18．6 磁盘间的任务均衡
18．7 节点间的任务均衡
18．8 索引过程调整和优化
18．8．1 自动生成doc ID
18．8．2 调整字段Mappings
18．8．3 调整_source字段
18．8．4 禁用_all字段
18．8．5 对Analyzed的字段禁用Norms
18．8．6 index_options 设置
18．9 参考配置
18．10 思考与总结
第19章　搜索速度的优化
19．1 为文件系统cache预留足够的内存
19．2 使用更快的硬件
19．3 文档模型
19．4 预索引数据
19．5 字段映射
19．6 避免使用脚本
19．7 优化日期搜索
19．8 为只读索引执行force-merge
19．9 预热全局序号（global ordinals）
19．10 execution hint
19．11 预热文件系统cache
19．12 转换查询表达式
19．13 调节搜索请求中的batched_reduce_size
19．14 使用近似聚合
19．15 深度优先还是广度优先
19．16 限制搜索请求的分片数
19．17 利用自适应副本选择（ARS）提升ES响应速度
第20章　磁盘使用量优化
20．1 预备知识
20．1．1 元数据字段
20．1．2 索引映射参数
20．2 优化措施
20．2．1 禁用对你来说不需要的特性
20．2．2 禁用doc values
20．2．3 不要使用默认的动态字符串映射
20．2．4 观察分片大小
20．2．5 禁用_source
20．2．6 使用best_compression
20．2．7 Fource Merge
20．2．8 Shrink Index
20．2．9 数值类型长度够用就好
20．2．10 使用索引排序来排列类似的文档
20．2．11 在文档中以相同的顺序放置字段
20．3 测试数据
第21章　综合应用实践
21．1 集群层
21．1．1 规划集群规模
21．1．2 单节点还是多节点部署
21．1．3 移除节点
21．1．4 独立部署主节点
21．2 节点层
21．2．1 控制线程池的队列大小
21．2．2 为系统cache保留一半物理内存
21．3 系统层
21．3．1 关闭swap
21．3．2 配置Linux OOM Killer
21．3．3 优化内核参数
21．4 索引层
21．4．1 使用全局模板
21．4．2 索引轮转
21．4．3 避免热索引分片不均
21．4．4 副本数选择
21．4．5 Force Merge
21．4．6 Shrink Index
21．4．7 close索引
21．4．8 延迟分配分片
21．4．9 小心地使用fielddata
21．5 客户端
21．5．1 使用REST API而非Java API
21．5．2 注意429状态码
21．5．3 curl的HEAD请求
21．5．4 了解你的搜索计划
21．5．5 为读写请求设置比较长的超时时间
21．6 读写
21．6．1 避免搜索操作返回巨大的结果集
21．6．2 避免索引巨大的文档
21．6．3 避免使用多个_type
21．6．4 避免使用_all字段
21．6．5 避免将请求发送到同一个协调节点
21．7 控制相关度
第22章　故障诊断
22．1 使用Profile API定位慢查询
22．2 使用Explain API分析未分配的分片（Unassigned Shards）
22．2．1 诊断未分配的主分片
22．2．2 诊断未分配的副分片
22．2．3 诊断已分配的分片
22．3 节点CPU使用率高
22．4 节点内存使用率高
22．5 Slow Logs
22．6 分析工具
22．6．1 I/O信息
22．6．2 内存
22．6．3 CPU信息
22．6．4 网络连接和流量
22．7 小结
附录A　重大版本变化

1.1　Apache Lucene简介1
1.1.1　熟悉Lucene2
1.1.2　Lucene的总体架构2
1.1.3　分析数据4
1.1.4　Lucene查询语言5
1.2　何为Elasticsearch8
1.2.1　Elasticsearch的基本概念8
1.2.2　Elasticsearch架构背后的关键概念10
1.2.3　Elasticsearch的工作流程10
1.3　在线书店示例14
1.4　小结17
第2章　查询DSL进阶18
2.1　Apache Lucene默认评分公式解释18
2.1.1　何时文档被匹配上19
2.1.2　TF/IDF评分公式19
2.1.3　Elasticsearch如何看评分21
2.1.4　一个例子21
2.2　查询改写24
2.2.1　前缀查询示例24
2.2.2　回到Apache Lucene26
2.2.3　查询改写的属性28
2.3　查询模板30
2.3.1　引入查询模板31
2.3.2　Mustache模板引擎33
2.3.3　把查询模板保存到文件35
2.4　过滤器的使用及作用原理36
2.4.1　过滤及查询相关性36
2.4.2　过滤器的工作原理40
2.4.3　性能考量41
2.4.4　后置过滤和过滤查询42
2.4.5　选择正确的过滤方式44
2.5　选择正确的查询方式45
2.5.1　查询方式分类45
2.5.2　使用示例50
2.6　小结65
第3章　不只是文本搜索66
3.1　查询二次评分66
3.1.1　什么是查询二次评分67
3.1.2　一个查询例子67
3.1.3　二次评分查询的结构67
3.1.4　二次评分参数70
3.1.5　总结70
3.2　多匹配控制71
3.3　重要词项聚合78
3.3.1　一个例子79
3.3.2　选择重要词项81
3.3.3　多值分析81
3.3.4　额外的配置84
3.3.5　使用限制89
3.4　文档分组89
3.4.1　top_hits聚合90
3.4.2　一个例子90
3.5　文档关系95
3.5.1　对象类型95
3.5.2　嵌套文档98
3.5.3　parent-child关系99
3.5.4　其他解决方案102
3.6　Elasticsearch各版本中脚本的变化102
3.6.1　脚本变迁102
3.6.2　Groovy简单介绍103
3.6.3　全文检索中的脚本108
3.6.4　Lucene表达式115
3.7　小结118
第4章　改善用户搜索体验119
4.1　改正用户拼写错误119
4.1.1　测试数据120
4.1.2　深入技术细节121
4.1.3　suggester121
4.2　改善查询相关性142
4.2.1　数据142
4.2.2　改善相关性的探索之旅145
4.3　小结157
第5章　分布式索引架构159
5.1　选择合适的分片和副本数159
5.1.1　分片和过度分配160
5.1.2　一个过度分配的正面例子161
5.1.3　多分片与多索引161
5.1.4　副本161
5.2　路由162
5.2.1　分片和数据162
5.2.2　测试路由功能162
5.2.3　索引时使用路由166
5.2.4　别名169
5.2.5　多个路由值169
5.3　调整默认分片的分配行为170
5.3.1　部署意识171
5.3.2　过滤173
5.3.3　运行时更新分配策略174
5.3.4　确定每个节点允许的总分片数175
5.3.5　确定每个物理机器允许的总分片数175
5.4　查询执行偏好179
5.5　小结181
第6章　底层索引控制182
6.1　改变Apache Lucene的评分方式182
6.1.1　可用的相似度模型183
6.1.2　为每字段配置相似度模型183
6.1.3　相似度模型配置184
6.1.4　选择默认的相似度模型185
6.2　选择适当的目录实现—store模块188
6.3　准实时、提交、更新及事务日志191
6.3.1　索引更新及更新提交192
6.3.2　事务日志193
6.3.3　准实时读取194
6.4　控制索引合并195
6.4.1　选择正确的合并策略196
6.4.2　合并策略配置197
6.4.3　调度199
6.5　关于I/O调节200
6.5.1　控制I/O节流200
6.5.2　配置200
6.6　理解Elasticsearch缓存202
6.6.1　过滤器缓存203
6.6.2　字段数据缓存204
6.6.3　查询分片缓存212
6.6.4　使用circuit breaker213
6.6.5　清除缓存214
6.7　小结215
第7章　管理Elasticsearch216
7.1　发现和恢复模块216
7.1.1　发现模块的配置217
7.1.2　主节点218
7.1.3　网关和恢复模块的配置223
7.1.4　索引恢复API226
7.2　使用人类友好的Cat API 229
7.2.1　基础知识230
7.2.2　使用Cat API231
7.2.3　一些例子232
7.3　备份232
7.4　联盟搜索236
7.4.1　测试用的集群236
7.4.2　建立部落节点237
7.4.3　通过部落节点读取数据238
7.4.4　通过部落节点写入数据239
7.4.5　处理索引冲突240
7.4.6　屏蔽写操作241
7.5　小结242
第8章　提高性能243
8.1　使用doc values来优化查询243
8.1.1　字段缓存存在的问题244
8.1.2　使用doc values的例子245
8.2　了解垃圾回收器247
8.2.1　Java内存248
8.2.2　解决垃圾回收问题249
8.2.3　在类UNIX系统上避免内存交换254
8.3　对查询做基准测试255
8.3.1　为基准测试配置集群256
8.3.2　进行基准测试256
8.3.3　控制运行中的基准测试259
8.4　热点线程261
8.4.1　热点线程的使用说明261
8.4.2　热点线程API的响应262
8.5　扩展Elasticsearch263
8.5.1　垂直扩展263
8.5.2　水平扩展264
8.5.3　在高负载的场景下使用Elasticsearch271
8.6　小结283
第9章　开发Elasticsearch插件284
9.1　创建Maven项目284
9.2　了解基本知识285
9.2.1　Maven Java项目的结构285
9.2.2　POM的理念285
9.2.3　执行构建过程286
9.2.4　引入Maven装配插件287
9.3　创建自定义REST行为289
9.3.1　设定289
9.3.2　实现细节289
9.4　创建自定义分析插件295
9.4.1　实现细节295
9.4.2　测试自定义分析插件302


第1章 信息检索模型 1

1.1 信息检索概述 1

1.1.1 信息过载 1

1.1.2 信息检索定义 2

1.1.3 信息检索常用术语 3

1.1.4 信息检索系统 4

1.2 分词算法 5

1.2.1 分词算法概述 5

1.2.2 词典匹配分词法 6

1.2.3 语义理解分词法 6

1.2.4 词频统计分词法 7

1.3 倒排索引 7

1.4 布尔检索模型 9

1.5 tf-idf权重计算 11

1.6 向量空间模型 13

1.7 概率检索模型 16

1.7.1 贝叶斯决策理论 17

1.7.2 二值独立模型 18

1.7.3 Okapi BM25模型 20

1.7.4 BM25F模型 20

1.8 本章小结 21

第2章 Lucene开发入门 22

2.1 Lucene概述 22

2.1.1 Lucene简介 22

2.1.2 Lucene特点 22

2.1.3 Lucene架构 23

2.2 Lucene开发准备 25

2.2.1 下载Lucene文件库 25

2.2.2 工程中引入Lucene 26

2.2.3 下载Luke 27

2.2.4 下载IK分词工具 28

2.2.5 工程搭建 29

2.3 Lucene分词详解 30

2.3.1 Lucene分词系统 30

2.3.2 分词器测试 31

2.3.3 IK分词器配置 34

2.3.4 中文分词器对比 36

2.3.5 扩展停用词词典 38

2.3.6 扩展自定义词典 38

2.4 Lucene索引详解 40

2.4.1 Lucene字段类型 40

2.4.2 索引文档示例 41

2.4.3 Luke中查看索引 46

2.4.4 索引的删除 48

2.4.5 索引的更新 49

2.5 Lucene查询详解 50

2.5.1 搜索入门 51

2.5.2 多域搜索（MultiFieldQueryParser） 52

2.5.3 词项搜索（TermQuery） 53

2.5.4 布尔搜索（BooleanQuery） 53

2.5.5 范围搜索（RangeQuery） 54

2.5.6 前缀搜索（PrefixQuery） 55

2.5.7 多关键字搜索（PhraseQuery） 55

2.5.8 模糊搜索（FuzzyQuery） 55

2.5.9 通配符搜索（WildcardQuery） 56

2.6 Lucene查询高亮 56

2.7 Lucene新闻高频词提取 58

2.7.1 问题提出 58

2.7.2 需求分析 58

2.7.3 编程实现 58

2.8 本章小结 61

第3章 Lucene文件检索项目实战 62

3.1 需求分析 62

3.2 架构设计 63

3.3 文本内容抽取 64

3.3.1 Tika简介 64

3.3.2 Tika下载 64

3.3.3 搭建工程 65

3.3.4 内容抽取 66

3.3.5 自动解析 68

3.4 工程搭建 71

3.5 索引文档 72

3.6 查询界面 75

3.7 文件检索 77

3.8 结果展示 80

3.9 本章小结 85

第4章 从Lucene到Elasticsearch 86

4.1 Elasticsearch概述 86

4.1.1 诞生过程 86

4.1.2 流行度分析 88

4.1.3 架构解读 89

4.1.4 优点 89

4.1.5 应用场景 90

4.1.6 核心概念 92

4.1.7 对比RDMS 94

4.1.8 文档结构 94

4.2 安装Elasticsearch 95

4.2.1 安装Java 96

4.2.2 下载Elasticsearch 97

4.2.3 启动Elasticsearch 97

4.2.4 后台运行Elasticsearch 99

4.2.5 关闭Elasticsearch 99

4.2.6 基本配置 100

4.3 中文分词器配置 101

4.3.1 IK分词器安装 101

4.3.2 扩展本地词库 102

4.3.3 配置远程词库 103

4.4 Head插件使用指南 105

4.4.1 Head插件的安装 105

4.4.2 Head插件的使用 107

4.5 REST命令 109

4.5.1 CURL工具 110

4.5.2 Kibana Dev Tools 111

4.6 本章小结 112

第5章 Elasticsearch集群入门 113

5.1 索引管理 113

5.1.1 新建索引 113

5.1.2 更新副本 115

5.1.3 读写权限 115

5.1.4 查看索引 116

5.1.5 删除索引 117

5.1.6 索引的打开与关闭 118

5.1.7 复制索引 118

5.1.8 收缩索引 119

5.1.9 索引别名 120

5.2 文档管理 123

5.2.1 新建文档 123

5.2.2 获取文档 125

5.2.3 更新文档 127

5.2.4 查询更新 129

5.2.5 删除文档 129

5.2.6 查询删除 130

5.2.7 批量操作 130

5.2.8 版本控制 133

5.2.9 路由机制 136

5.3 映射详解 137

5.3.1 映射分类 137

5.3.2 动态映射 138

5.3.3 日期检测 140

5.3.4 静态映射 141

5.3.5 字段类型 142

5.3.6 元字段 156

5.3.7 映射参数 162

5.3.8 映射模板 180

==========

第1章 搜索的相关性问题

1.1 我们的目标：掌握相关性技术研发的技能

1.2 为什么搜索的相关性如此之难

1.2.1 什么是具备“相关性”的搜索结果

1.2.2 搜索：没有银弹

1.3 来自相关性研究的启示

1.3.1 信息检索

1.3.2 能否利用信息检索解决相关性问题

1.4 如何解决相关性

1.5 不只是技术：管理、协作与反馈

1.6 本章小结

第２章 搜索—幕后揭秘

2.1 搜索101

2.1.1 什么是搜索文档

2.1.2 对内容进行搜索

2.1.3 通过搜索来探索内容

2.1.4 获取进入搜索引擎的内容

2.2 搜索引擎的数据结构

2.2.1 倒排索引

2.2.2 倒排索引的其他内容

2.3 对内容进行索引：提取、充实、分析和索引

2.3.1 将内容提取为文档

2.3.2 充实文档以清理、强化与合并数据

2.3.3 执行分析

2.3.4 索引

2.4 文档的搜索和获取

2.4.1 布尔搜索： AND/OR/NOT

2.4.2 基于 Lucene搜索的布尔查询（MUST/MUST_NOT/SHOULD）

2.4.3 位置和短语匹配

2.4.4 助力用户浏览：过滤、切面和聚合

2.4.5 排序、结果排名，以及相关性

2.5 本章小结

第3章 调试我们的第一个相关性问题

3.1 Solr和Elasticsearch的应用：基于Elasticsearch的例子

3.2 最了不起的数据集：TMDB

3.3 用Python语言编写的例子

3.4 第一个搜索应用

3.4.1 针对 TMDB Elasticsearch索引的第一次搜索

3.5 调试查询匹配

3.5.1 检查底层查询策略

3.5.2 剖析查询解析

3.5.3 调试分析，解决匹配问题

3.5.4 比较查询条件和倒排索引

3.5.5 通过修改分析器来修正我们的匹配

3.6 调试排名

3.6.1 利用 Lucene的解释功能来剖析相关性评价

3.6.2 向量空间模型、相关性解释信息和我们

3.6.3 向量空间模型在实践中的注意事项

3.6.4 通过对匹配的评价来度量相关性

3.6.5 用 TF×IDF计算权重

3.6.6 谎言、该死的谎言和相似度

3.6.7 决定搜索词重要性的因素

3.6.8 解决 Space Jam和 alien的排名问题

3.7 问题解决了？工作永远做不完！

3.8 本章小结

第4章 驾驭token

4.1 将token作为文档特征

4.1.1 匹配的流程

4.1.2 token，不只是单词

4.2 控制查准率和查全率

4.2.1 查准率和查全率的例子

4.2.2 查准率或查全率的分析

4.2.3 一味提高查全率

4.3 查准率和查全率—让鱼和熊掌兼得

4.3.1 评价单一字段中特征的强度

4.3.2 超越 TF × IDF的评价：多搜索词与多字段

4.4 分析策略

4.4.1 处理分隔符

4.4.2 捕获同义词的语义

4.4.3 在搜索中为专指性建模

4.4.4 利用同义词为专指性建模

4.4.5 利用路径为专指性建模

4.4.6 对整个世界分词

4.4.7 对整数分词

4.4.8 对地理数据分词

4.4.9 对歌曲分词

4.5 本章小结

第5章 多字段搜索基础

5.1 信号及信号建模

5.1.1 什么是信号

5.1.2 从源数据模型开始

5.1.3 实现信号

5.1.4 信号建模：为数据的相关性建模

5.2 TMDB—搜索，人类最后的边疆

5.2.1 违反基本法则

5.2.2 让嵌套文档扁平化

5.3 在以字段为中心的搜索中给信号建模

5.3.1 从 best_.elds开始

5.3.2 控制搜索结果中的字段偏好

5.3.3 可以使用信号更精准的 best_.elds吗

5.3.4 让失败者分享荣耀：为 best_.elds校准

5.3.5 利用 most_.elds统计多个信号

5.3.6 在 most_.elds中缩放信号

5.3.7 什么时候其他匹配才无关紧要

5.3.8 有关 most_.elds的结论是什么

5.4 本章小结

第6章 以词为中心的搜索

6.1 什么是以词为中心的搜索

6.2 我们为什么需要以词为中心的搜索

6.2.1 猎寻“白化象”

6.2.2 在“星际迷航”的例子中寻找白化象问题

6.2.3 避免信号冲突

6.2.4 理解信号冲突的机理

6.3 完成第一个以词为中心的搜索

6.3.1 使用以词为中心的排名函数

6.3.2 运行以词为中心的查询解析器（深入底层）

6.3.3 理解字段同步

6.3.4 字段同步和信号建模

6.3.5 查询解析器和信号冲突

6.3.6 对以词为中心的搜索进行调优

6.4 在以词为中心的搜索中解决信号冲突

6.4.1 将字段合并成自定义全字段

6.4.2 利用 cross_.elds解决信号冲突

6.5 结合以字段为中心和以词为中心的策略：鱼与熊掌兼得

6.5.1 将“相似字段”分到一组

6.5.2 理解相似字段的局限

6.5.3 将贪婪的简单搜索和保守的放大器结合起来

6.5.4 以词为中心与以字段为中心，查准率与查全率

6.5.5 考虑过滤、放大，以及重新排名

6.6 本章小结

第7章 调整相关性函数

7.1 何谓评价调整

7.2 放大：通过突出结果来实现调整

7.2.1 放大：最后的边疆

7.2.2 放大时—选择加法运算还是乘法运算，布尔查询还是函数查询？

7.2.3 选择第一扇门：利用布尔查询进行加法放大

7.2.4 选择第二扇门：利用数学运算进行排名的函数查询

7.2.5 函数查询实践：简单的乘法放大

7.2.6 放大处理的基础：信号，处处是信号

7.3 过滤：通过排除的方法对结果进行调整

7.4 满足业务需求的评价调整策略

7.4.1 搜索所有影片

7.4.2 对放大信号进行建模

7.4.3 构造排名函数：增加具有较高价值的层级

7.4.4 利用函数查询对具有较高价值的层级进行评价

7.4.5 忽略 TF × IDF

7.4.6 捕捉综合质量指标

7.4.7 达成用户的时效性目标

7.4.8 结合函数查询

7.4.9 把一切联系起来

7.5 本章小结

第8章 提供相关性反馈

8.1 搜索框中的相关性反馈

8.1.1 利用“即输即搜”提供即时结果

8.1.2 利用“搜索补全”帮助用户找到最佳查询

8.1.3 利用搜索建议来修正输入和拼写错误

8.2 浏览期间的相关性反馈

8.2.1 构建基于切面的浏览

8.2.2 提供面包线导航

8.2.3 选择其他的结果排序方式

8.3 搜索结果清单中的相关性反馈

8.3.1 什么信息应该出现在搜索结果中

8.3.2 通过文本片段与高亮提供相关性反馈

8.3.3 对相似文档分组

8.3.4 在用户搜不到结果时给予帮助

8.4 本章小结

第9章 设计以相关性为核心的搜索应用

9.1 Yowl！一个绝佳的新起点

9.2 信息和需求的收集

9.2.1 理解用户及其信息需求

9.2.2 理解业务需求

9.2.3 找出必要及可用的信息

9.3 搜索应用的设计

9.3.1 将用户体验可视化

9.3.2 定义字段和模型的信号

9.3.3 信号的组合与平衡

9.4 部署、监控和改进

9.4.1 监控

9.4.2 找出问题并解决它们

9.5 知道什么是恰到好处

9.6 本章小结

第10章 以相关性为核心的企业

10.1 反馈：以相关性为核心的企业所依赖的基石

10.2 为什么以用户为中心的文化比数据驱动的文化更重要

10.3 无视相关性的天马行空

10.4 相关性反馈的觉醒：领域专家和专业用户

10.5 相关性反馈的成长：内容管理

10.5.1 内容管理员的角色

10.5.2 与内容管理员缺乏交流的风险

10.6 让相关性更加流畅：工程师/内容管理员的结对

10.7 让相关性加速：测试驱动的相关性

10.7.1 理解测试驱动的相关性

10.7.2 使用带用户行为数据的测试驱动相关性

10.8 超越测试驱动的相关性：学习排序

10.9 本章小结

第11章 语义和个性化搜索

11.1 基于用户概况的个性化搜索

11.1.1 收集用户的概况信息

11.1.2 将概要信息与文档索引紧密关联

11.2 基于用户行为的个性化搜索

11.2.1 引入协同过滤

11.2.2 使用共现计数的基本协同过滤算法

11.2.3 将用户行为信息与文档索引紧密关联

11.3 构建概念性搜索的基本方法

11.3.1 构建概念性信号

11.3.2 利用同义词对内容进行扩充

11.4 利用机器学习来构建概念性搜索

11.4.1 概念性搜索中短语的重要性

11.5 连接个性化搜索与概念性搜索

11.6 推荐是一种广义的搜索

11.6.1 用推荐代替搜索

附录A 直接根据TMDB建立索引

附录B Solr读者指南

1.1　Elasticsearch是什么1
1.1.1　Elasticsearch的历史2
1.1.2　相关产品3
1.2　全文搜索3
1.2.1　Lucene介绍4
1.2.2　Lucene倒排索引4
1.3　基础知识6
1.3.1　Elasticsearch术语及概念6
1.3.2　JSON介绍10
1.4　安装配置12
1.4.1　安装Java12
1.4.2　安装Elasticsearch12
1.4.3　配置13
1.4.4　运行15
1.4.5　停止17
1.4.6　作为服务17
1.4.7　版本升级19
1.5　对外接口21
1.5.1　API约定22
1.5.2　REST介绍25
1.5.3　Head插件安装26
1.5.4　创建库27
1.5.5　插入数据28
1.5.6　修改文档28
1.5.7　查询文档29
1.5.8　删除文档29
1.5.9　删除库30
1.6　Java接口30
1.6.1　Java接口说明30
1.6.2　创建索引文档33
1.6.3　增加文档34
1.6.4　修改文档35
1.6.5　查询文档35
1.6.6　删除文档35
1.7　小结36
第2章　索引37
2.1　索引管理37
2.1.1　创建索引37
2.1.2　删除索引39
2.1.3　获取索引39
2.1.4　打开/关闭索引40
2.2　索引映射管理41
2.2.1　增加映射41
2.2.2　获取映射44
2.2.3　获取字段映射45
2.2.4　判断类型是否存在46
2.3　索引别名46
2.4　索引配置51
2.4.1　更新索引配置51
2.4.2　获取配置52
2.4.3　索引分析52
2.4.4　索引模板54
2.4.5　复制配置55
2.4.6　重建索引56
2.5　索引监控60
2.5.1　索引统计60
2.5.2　索引分片62
2.5.3　索引恢复63
2.5.4　索引分片存储64
2.6　状态管理64
2.6.1　清除缓存64
2.6.2　索引刷新64
2.6.3　冲洗65
2.6.4　合并索引65
2.7　文档管理66
2.7.1　增加文档66
2.7.2　更新删除文档69
2.7.3　查询文档73
2.7.4　多文档操作76
2.7.5　索引词频率80
2.7.6　查询更新接口83
2.8　小结87
第3章　映射88
3.1　概念88
3.2　字段数据类型90
3.2.1　核心数据类型91
3.2.2　复杂数据类型96
3.2.3　地理数据类型100
3.2.4　专门数据类型106
3.3　元字段108
3.3.1　_all字段109
3.3.2　_field_names字段109
3.3.3　_id字段110
3.3.4　_index字段110
3.3.5　_meta字段111
3.3.6　_parent字段111
3.3.7　_routing字段112
3.3.8　_source字段114
3.3.9　_type字段115
3.3.10　_uid字段115
3.4　映射参数116
3.4.1　analyzer参数116
3.4.2　boost参数118
3.4.3　coerce参数119
3.4.4　copy_to参数120
3.4.5　doc_values参数121
3.4.6　dynamic参数122
3.4.7　enabled参数122
3.4.8　fielddata参数123
3.4.9　format参数126
3.4.10　geohash参数128
3.4.11　geohash_precision参数129
3.4.12　geohash_prefix参数130
3.4.13　ignore_above参数131
3.4.14　ignore_malformed参数131
3.4.15　include_in_all参数132
3.4.16　index参数133
3.4.17　index_options参数133
3.4.18　lat_lon参数134
3.4.19　fields参数135
3.4.20　norms参数136
3.4.21　null_value参数137
3.4.22　position_increment_gap参数137
3.4.23　precision_step参数138
3.4.24　properties参数138
3.4.25　search_analyzer参数139
3.4.26　similarity参数140
3.4.27　store参数141
3.4.28　term_vector参数141
3.5　动态映射142
3.5.1　概念142
3.5.2　_default_映射143
3.5.3　动态字段映射143
3.5.4　动态模板145
3.5.5　重写默认模板148
3.6　小结148
第4章　搜索149
4.1　深入搜索149
4.1.1　搜索方式149
4.1.2　重新评分153
4.1.3　滚动查询请求155
4.1.4　隐藏内容查询158
4.1.5　搜索相关函数161
4.1.6　搜索模板164
4.2　查询DSL167
4.2.1　查询和过滤的区别167
4.2.2　全文搜索168
4.2.3　字段查询179
4.2.4　复合查询183
4.2.5　连接查询188
4.2.6　地理查询190
4.2.7　跨度查询197
4.2.8　高亮显示200
4.3　简化查询203
4.4　小结206
第5章　聚合207
5.1　聚合的分类207
5.2　度量聚合209
5.2.1　平均值聚合209
5.2.2　基数聚合211
5.2.3　最大值聚合213
5.2.4　最小值聚合214
5.2.5　和聚合214
5.2.6　值计数聚合215
5.2.7　统计聚合215
5.2.8　百分比聚合215
5.2.9　百分比分级聚合216
5.2.10　最高命中排行聚合217
5.2.11　脚本度量聚合217
5.2.12　地理边界聚合221
5.2.13　地理重心聚合222
5.3　分组聚合223
5.3.1　子聚合224
5.3.2　直方图聚合226
5.3.3　日期直方图聚合230
5.3.4　时间范围聚合233
5.3.5　范围聚合234
5.3.6　过滤聚合235
5.3.7　多重过滤聚合236
5.3.8　空值聚合238
5.3.9　嵌套聚合239
5.3.10　采样聚合240
5.3.11　重要索引词聚合242
5.3.12　索引词聚合245
5.3.13　总体聚合251
5.3.14　地理点距离聚合251
5.3.15　地理散列网格聚合253
5.3.16　IPv4范围聚合255
5.4　管道聚合257
5.4.1　平均分组聚合259
5.4.2　移动平均聚合261
5.4.3　总和分组聚合262
5.4.4　总和累计聚合262
5.4.5　最大分组聚合264
5.4.6　最小分组聚合265
5.4.7　统计分组聚合266
5.4.8　百分位分组聚合268
5.4.9　差值聚合269


1.1 搜索引擎开发需求1
1.2 准备开发环境1
1.2.1 Windows命令行cmd1
1.2.2 在Windows下使用Java3
1.2.3 Linux终端5
1.2.4 在Linux下使用Java9
1.2.5 Eclipse集成开发环境10
1.3 了解Elasticsearch10
1.3.1 JSON数据格式11
1.3.2 Elasticsearch基本概念12
1.3.3 HTTP协议13
1.4 Elasticsearch安装和配置16
1.4.1 安装Elasticsearch16
1.4.2 运行Elasticsearch作为服务进程19
1.5 实现一个简单的网站搜索21
1.5.1 定义索引结构23
1.5.2 导入数据26
1.5.3 查询API27
1.5.4 实现搜索界面29
1.6 本章小结35
第2章 开发中文搜索引擎36
2.1 中文分词原理36
2.1.1 最长匹配方法36
2.1.2 自己写分析器42
2.1.3 概率语言模型的分词方法44
2.1.4 中文分词插件原理52
2.1.5 开发中文分词插件54
2.1.6 支持Elasticsearch的插件57
2.1.7 中文分析器提供者59
2.1.8 字词混合索引61
2.2 提高分词准确度63
2.3 本章小结65
第3章 Mapping详解66
3.1 索引模式66
3.1.1 创建模式66
3.1.2 修改模式68
3.2 Mapping数据类型69
3.3 Mapping参数70
3.4 动态Mapping71
3.4.1 使用动态Mapping72
3.4.2 实现原理72
3.5 本章小结74
第4章 深入源码分析75
4.1 Lucene源码分析75
4.1.1 使用Lucene75
4.1.2 Ivy管理依赖项77
4.1.3 源码结构介绍77
4.1.4 并发控制82
4.2 启动搜索服务88
4.3 Guice框架89
4.4 日期和时间库——Joda-Time91
4.5 Transport模块91
4.6 线程池92
4.7 模块93
4.8 Netty通信框架93
4.9 缓存94
4.10 分布式95
4.11 Zen发现机制95
4.12 联合搜索97
4.13 JVM字节码98
4.13.1 编译代码99
4.13.2 同步相关指令99
4.14 本章小结100
第5章 提高搜索相关性102
5.1 向量空间检索模型102
5.2 BM25检索模型105
5.2.1 使用BM25检索模型108
5.2.2 参数调优108
5.3 学习评分109
5.3.1 基本原理109
5.3.2 准备数据110
5.3.3 Elasticsearch学习排名112
5.4 查询意图识别112
5.5 图像特征提升检索体验113
5.6 本章小结116
第6章 搜索界面开发118
6.1 使用Searchkit实现搜索界面118
6.2 Spring Boot入门122
6.2.1 可执行的WAR125
6.2.2 spring-boot-devtools模块实现热部署136
6.3 Java模板引擎Pebble介绍136
6.4 通过Spring-data-elasticsearch 项目访问Elasticsearch141
6.5 REST基本概念149
6.6 使用Vue.js开发搜索界面154
6.7 使用Vue.js Paginator插件实现翻页157
6.8 实现搜索接口161
6.8.1 编码识别161
6.8.2 布尔搜索163
6.8.3 搜索结果重定向164
6.8.4 搜索结果排序165
6.8.5 实现相似文档搜索166
6.9 Suggester搜索词提示167
6.9.1 拼音提示169
6.9.2 部署总结169
6.9.3 相关搜索170
6.9.4 再次查找172
6.9.5 搜索日志172
6.10 Word2vec挖掘相关搜索词174
6.11 部署网站179
6.11.1 部署到Web服务器179
6.11.2 防止攻击181
6.12 使用Rust开发搜索界面184
6.13 本章小结184
第7章 Elastic栈系统监控186
7.1 管理Elasticsearch集群186
7.1.1 写入权限控制187
7.1.2 使用X-Pack188
7.1.3 快照189
7.2 Logstash数据处理工具190
7.2.1 使用Logstash190
7.2.2 插件192
7.2.3 数据库输入插件192
7.2.4 开发插件193
7.3 Filebeat文件收集器193
7.4 消息过期194
7.5 Kibana可视化平台195
7.6 Flume日志收集系统196
7.7 Kafka分布式流平台197
7.8 Graylog日志管理平台198
7.9 本章小结202
第8章 案例分析204
8.1 双语句对搜索204
8.1.1 爬虫抓取双语句对204
8.1.2 英文分词205
8.1.3 句子切分205
8.1.4 标注词性207
8.1.5 词对齐209
8.1.6 索引数据213
8.2 内容管理系统站内检索214
8.2.1 MySQL数据库214
8.2.2 RESTful API管理索引215
8.2.3 自动客服机器人217
8.3 搜索文档225
8.3.1 爬虫抓取信息225
8.3.2 在Linux下使用.NET233
8.3.3 NEST客户端235

1．1 基本概念 1
1．2 安装 2
1．3 搜索集群 5
1．4 创建索引 6
1．5 使用Java客户端接口 9
1．5．1 创建索引 11
1．5．2 增加、删除与修改数据 14
1．5．3 分析器 16
1．5．4 数据导入 17
1．5．5 通过摄取快速导入数据 17
1．5．6 索引库结构 17
1．5．7 查询 18
1．5．8 区间查询 22
1．5．9 排序 23
1．5．10 分布式搜索 23
1．5．11 过滤器 24
1．5．12 高亮显示 24
1．5．13 分页 25
1．5．14 通过聚合实现分组查询 26
1．5．15 文本列的聚合 27
1．5．16 遍历数据 28
1．5．17 索引文档 29
1．5．18 Percolate 29
1．6 RESTClient 30
1．6．1 使用摄取 31
1．6．2 代码实现摄取 33
1．7 使用Jest 33
1．8 Python客户端 37
1．9 Scala客户端 40
1．10 PHP客户端 43
1．11 SQL支持 44
1．12 本章小结 48
第2章 开发插件 49
2．1 搜索中文 49
2．1．1 中文分词原理 49
2．1．2 中文分词插件原理 51
2．1．3 开发中文分词插件 53
2．1．4 中文AnalyzerProvider 55
2．1．5 字词混合索引 57
2．2 搜索英文 60
2．2．1 句子切分 60
2．2．2 标注词性 62
2．3 使用测试套件 64
2．4 本章小结 68
第3章 管理搜索集群 69
3．1 节点类型 69
3．2 管理集群 69
3．3 写入权限控制 70
3．4 使用X-Pack 71
3．5 快照 72
3．6 Zen发现机制 73
3．7 联合搜索 74
3．8 缓存 74
3．9 本章小结 75
第4章 源码分析 76
4．1 Lucene源码分析 76
4．1．1 Ivy管理依赖项 76
4．1．2 源码结构介绍 76
4．2 Gradle 77
4．3 Guice 77
4．4 Joda-Time 79
4．5 Transport 80
4．6 线程池 80
4．7 模块 80
4．8 Netty 81
4．9 分布式 81
4．10 本章小结 82
第5章 搜索相关性 83
5．1 BM25检索模型 83
5．1．1 使用BM25检索模型 86
5．1．2 参数调优 86
5．2 学习评分 86
5．2．1 基本原理 87
5．2．2 准备数据 87
5．2．3 Elasticsearch学习排名 89
5．3 本章小结 91
第6章 搜索引擎用户界面 92
6．1 JSP实现搜索界面 92
6．1．1 用于显示搜索结果的自定义标签 93
6．1．2 使用Listlib 98
6．1．3 实现翻页 100
6．2 使用Spring实现的搜索界面 102
6．2．1 实现REST搜索界面 102
6．2．2 REST API中的HTTP PUT 104
6．2．3 Spring-data-elasticsearch 106
6．2．4 Spring HATEOAS 112
6．3 实现搜索接口 113
6．3．1 编码识别 113
6．3．2 布尔搜索 116
6．3．3 搜索结果排序 116
6．4 实现相似文档搜索 117
6．5 实现AJAX搜索联想词 119
6．5．1 估计查询词的文档频率 119
6．5．2 搜索联想词总体结构 119
6．5．3 服务器端处理 120
6．5．4 浏览器端处理 125
6．5．5 拼音提示 127
6．5．6 部署总结 127
6．5．7 Suggester 128
6．6 推荐搜索词 129
6．6．1 挖掘相关搜索词 130
6．6．2 使用多线程计算相关搜索词 132
6．7 查询意图理解 133
6．7．1 拼音搜索 133
6．7．2 无结果处理 133
6．8 集成其他功能 134
6．8．1 拼写检查 134
6．8．2 分类统计 135
6．8．3 相关搜索 141
6．8．4 再次查找 144
6．8．5 搜索日志 144
6．9 查询分析 146
6．9．1 历史搜索词记录 146
6．9．2 日志信息过滤 147
6．9．3 信息统计 148
6．9．4 挖掘日志信息 150
6．9．5 查询词意图分析 150
6．10 部署网站 150
6．10．1 部署到Web服务器 151
6．10．2 防止攻击 152
6．11 本章小结 156
第7章　OCR文字识别 157
7．1 Tesseract 157
7．2 使用TensorFlow识别文字 161
7．3 OpenCV 164
7．3．1 预处理 166
7．3．2 文字区域提取 169
7．3．3 纠正偏斜 171
7．3．4 Linux环境支持 172
7．4 JavaCV 172
7．5 本章小结 174
第8章　问答式搜索 176
8．1 生成表示语义的代码 176
8．2 信息整合 181
8．2．1 实体对齐 181
8．2．2 编辑距离 181
8．2．3 Jaro-Winkler距离 187
8．2．4 比较器 189
8．2．5 Cleaner 189
8．2．6 运行过程 190
8．2．7 遗传算法调整参数 192
8．3 自动问答 193
8．3．1 问句处理器 193
8．3．2 自动发现答案 198
8．4 本章小结 199
第9章　Elastic系统监控 201
9．1 Logstash 201
9．1．1 使用Logstash 201
9．1．2 插件 203
9．1．3 数据库输入插件 206
9．2 Filebeat 207
9．3 消息过期 208
9．4 Kibana 208
9．5 Flume 209
9．6 Kafka 210
9．7 Graylog 211
9．8 物联网数据 215

1.1　Apache Lucene简介1
1.1.1　熟悉Lucene2
1.1.2　Lucene的总体架构2
1.1.3　分析你的数据3
1.1.4　Lucene查询语言4
1.2　ElasticSearch简介6
1.2.1　ElasticSearch的基本概念7
1.2.2　ElasticSearch架构背后的关键概念8
1.2.3　ElasticSearch的工作流程9
1.3　小结13
第2章　查询DSL进阶14
2.1　Apache Lucene默认评分公式解释14
2.1.1　何时文档被匹配上15
2.1.2　TF/IDF评分公式15
2.1.3　ElasticSearch如何看评分16
2.2　查询改写17
2.2.1　前缀查询范例17
2.2.2　回顾Apache Lucene19
2.2.3　查询改写的属性20
2.3　二次评分21
2.3.1　理解二次评分21
2.3.2　范例数据21
2.3.3　查询22
2.3.4　二次评分查询的结构22
2.3.5　二次评分参数配置23
2.3.6　小结24
2.4　批量操作24
2.4.1　批量取24
2.4.2　批量查询26
2.5　排序27
2.5.1　基于多值字段的排序28
2.5.2　基于多值geo字段的排序28
2.5.3　基于嵌套对象的排序30
2.6　数据更新API31
2.6.1　简单字段更新31
2.6.2　使用脚本按条件更新32
2.6.3　使用更新 API创建或删除文档33
2.7　使用过滤器优化查询33
2.7.1　过滤器与缓存34
2.7.2　词项查找过滤器36
2.8　ElasticSearch切面机制中的过滤器与作用域40
2.8.1　范例数据40
2.8.2　切面计算和过滤41
2.8.3　过滤器作为查询的一部分42
2.8.4　切面过滤器44
2.8.5　全局作用域45
2.9　小结47
第3章　底层索引控制48
3.1　改变Apache Lucene的评分方式48
3.1.1　可用的相似度模型49
3.1.2　为每字段配置相似度模型49
3.2　相似度模型配置50
3.2.1　选择默认的相似度模型51
3.2.2　配置被选用的相似度模型52
3.3　使用编解码器53
3.3.1　简单使用范例53
3.3.2　工作原理解释54
3.3.3　可用的倒排表格式55
3.3.4　配置编解码器56
3.4　准实时、提交、更新及事务日志58
3.4.1　索引更新及更新提交59
3.4.2　事务日志60
3.4.3　准实时读取62
3.5　深入理解数据处理62
3.5.1　输入并不总是进行文本分析62
3.5.2　范例的使用65
3.5.3　索引期更换分词器67
3.5.4　搜索时更换分析器68
3.5.5　陷阱与默认分析68
3.6　控制索引合并68
3.6.1　选择正确的合并策略69
3.6.2　合并策略配置70
3.6.3　调度72
3.7　小结73
第4章　分布式索引架构74
4.1　选择合适的分片和副本数74
4.1.1　分片和过度分配75
4.1.2　一个过度分配的正面例子75
4.1.3　多分片与多索引76
4.1.4　副本76
4.2　路由76
4.2.1　分片和数据77
4.2.2　测试路由功能77
4.2.3　索引时使用路由80
4.2.4　别名83
4.2.5　多个路由值83
4.3　调整默认的分片分配行为84
4.3.1　分片分配器简介84
4.3.2　even_shard 分片分配器84
4.3.3　balanced分片分配器85
4.3.4　自定义分片分配器85
4.3.5　裁决者86
4.4　调整分片分配88
4.4.1　部署意识89
4.4.2　过滤91
4.4.3　运行时更新分配策略92
4.4.4　确定每个节点允许的总分片数93
4.4.5　更多的分片分配属性96
4.5　查询执行偏好97
4.6　应用我们的知识99
4.6.1　基本假定99
4.6.2　配置100
4.6.3　变化来了104
4.7　小结105
第5章　管理ElasticSearch106
5.1　选择正确的目录实现-存储模块106
5.2　发现模块的配置109
5.2.1　Zen发现109
5.2.2　亚马逊EC2发现111
5.2.3　本地网关114
5.2.4　恢复配置115
5.3　索引段统计116
5.3.1　segments API简介116
5.3.2　索引段信息的可视化118
5.4　理解ElasticSearch缓存119
5.4.1　过滤器缓存119
5.4.2　字段数据缓存121
5.4.3　清除缓存126
5.5　小结127
第6章　故障处理129
6.1　了解垃圾回收器129
6.1.1　Java内存130
6.1.2　处理垃圾回收问题131
6.1.3　在类UNIX系统中避免内存交换135
6.2　关于I/O调节136
6.2.1　控制IO节流136
6.2.2　配置136
6.3　用预热器提升查询速度138
6.3.1　为什么使用预热器138
6.3.2　操作预热器138
6.3.3　测试预热器141
6.4　热点线程144
6.4.1　澄清热点线程API的用法误区145
6.4.2　热点线程API的响应信息145
6.5　现实场景146
6.5.1　越来越差的性能146
6.5.2　混杂的环境和负载不平衡148
6.5.3　我的服务器出故障了149
6.6　小结150
第7章　改善用户搜索体验151
7.1　改正用户拼写错误151
7.1.1　测试数据152
7.1.2　深入技术细节152
7.1.3　completion suggester168
7.2　改善查询相关性172
7.2.1　数据172
7.2.2　改善相关性的探索之旅174
7.3　小结188
第8章　ElasticSearch Java API189
8.1　ElasticSearch Java API简介189
8.2　代码190
8.3　连接到集群191
8.3.1　成为ElasticSearch节点191
8.3.2　使用传输机连接方式192
8.3.3　选择合适的连接方式193
8.4　API剖析194
8.5　CRUD操作195
8.5.1　读取文档195
8.5.2　索引文档197
8.5.3　更新文档199
8.5.4　删除文档201
8.6　ElasticSearch查询203
8.6.1　准备查询请求203
8.6.2　构造查询203
8.6.3　分页206
8.6.4　排序207
8.6.5　过滤207
8.6.6　切面计算208
8.6.7　高亮209
8.6.8　查询建议209
8.6.9　计数210
8.6.10　滚动211
8.7　批量执行多个操作211
8.7.1　批量操作211
8.7.2　根据查询删除文档212
8.7.3　Multi GET212
8.7.4　Multi Search212
8.8　Percolator213
8.9　explain API214
8.10　构造JSON格式的查询和文档214
8.11　管理API216
8.11.1　集群管理API216
8.11.2　索引管理API219
8.12　小结226
第9章　开发ElasticSearch插件227
9.1　建立Apache Maven项目结构227
9.1.1　了解基本知识228
9.1.2　Maven Java项目的结构228
9.1.3　POM的理念228
9.1.4　运行构建过程229
9.1.5　引入Maven装配插件230
9.2　创建一个自定义river插件232
9.2.1　实现细节232
9.2.2　测试river238
9.3　创建自定义分析插件240
9.3.1　实现细节240
9.3.2　测试自定义分析插件247


第1章 Spark Streaming应用概述
1.1 Spark Streaming应用案例
1.2 Spark Streaming应用剖析
第2章 Spark Streaming基本原理
2.1 Spark Core简介
2.2 Spark Streaming设计思想
2.3 Spark Streaming整体架构
2.4 编程接口
第3章 Spark Streaming运行流程详解
3.1 从StreamingContext的初始化到启动
3.2 数据接收
3.3 数据处理
3.4 数据清理
3.5 容错机制
3.6 No Receiver方式
3.7 输出不重复
3.8 消费速率的动态控制
3.9 状态操作
3.10 窗口操作
3.11 页面展示
3.12 Spark Streaming应用程序的停止
第4章 Spark Streaming性能调优机制
4.1 并行度解析
4.2 内存
4.3 序列化
4.4 Batch Interval
4.5 Task
4.6 JVM GC
第5章 Spark 2.0中的流计算
5.1 连续应用程序
5.2 无边界表unbounded table
5.3 增量输出模式
5.4 API简化
5.5 其他改进

第1章　大数据漫游指南 1
1.1　Spark前传 1
1.1.1　Web 2.0时代 2
1.1.2　无处不在的传感器 7
1.2　Spark Streaming：MapReduce和CEP的交集 9
第2章　Spark简介 10
2.1　安装 11
2.2　执行 12
2.2.1　独立集群模式（Standalone Cluster） 12
2.2.2　YARN模式 13
2.3　第一个应用程序 13
2.3.1　构建 16
2.3.2　执行 17
2.4　SparkContext 19
2.4.1　RDDs创建 19
2.4.2　处理依赖关系 20
2.4.3　创建共享变量 21
2.4.4　作业执行 22
2.5　RDD 22
2.5.1　持久化 23
2.5.2　转换 24
2.5.3　行动（Action） 28
小结 29
第3章　实时RDD：DStream 30
3.1　从连续流到离散流 30
3.2　第一个Spark Streaming应用程序 31
3.2.1　构建和执行 34
3.2.2　Streaming Context 34
3.3　DStreams 36
3.3.1　Spark Streaming应用程序剖析 38
3.3.2　转换 42
小结 52
第4章　高速流：并行化及其他 54
4.1　流数据的一大飞跃 54
4.2　并行化 56
4.2.1　Worker 56
4.2.2　执行器（Executor） 57
4.2.3　任务（Task） 59
4.3　批处理间隔 62
4.4　调度 64
4.4.1　应用程序间调度 64
4.4.2　批处理调度 64
4.4.3　作业间调度 65
4.4.4　一个行动，一个作业 65
4.5　内存 66
4.5.1　序列化 67
4.5.2　压缩（Compression） 70
4.5.3　垃圾收集 70
4.6　Shuffle 70
4.6.1　早期投影和过滤 70
4.6.2　经常使用组合器 70
4.6.3　大量运用平行化 70
4.6.4　文件合并（File Consolidation） 71
4.6.5　更多内存 71
小结 71
第5章　链接外部数据源 72
5.1　智慧城市，智慧地球，一切更智慧 72
5.2　ReceiverInputDStream 74
5.3　套接字 76
5.4　MQTT 85
5.5　Flume 89
5.5.1　基于推模式的Flume数据摄取 91
5.5.2　基于拉模式的Flume数据摄取 92
5.6　Kafka 92
5.6.1　基于接收器的Kafka消费者 95
5.6.2　直接Kafka消费者 98
5.7　Twitter 99
5.8　块间隔 100
5.9　自定义接收器 100
小结 104
第6章　边界效应 106
6.1　盘点股市 106
6.2　foreachRDD 108
6.2.1　为每条记录创建一个连接 110
6.2.2　为每个分区创建一个连接 111
6.2.3　静态连接 112
6.2.4　惰性静态连接 113
6.2.5　静态连接池 114
6.3　可扩展流存储 116
6.3.1　HBase 117
6.3.2　股市控制台（Dashboard） 118
6.3.3　SparkOnHBase 120
6.3.4　Cassandra 122
6.3.5　Spark Cassandra连接器 124
6.4　全局状态（Global State） 126
6.4.1　静态变量 126
6.4.2　updateStateByKey() 128
6.4.3　累加器 129
6.4.4　外部解决方案 131
小结 133
第7章　充分准备 134
7.1　每个点击都异乎重要 134
7.2　Tachyon（Alluxio） 135
7.3　Spark Web UI 138
7.3.1　历史分析 151
7.3.2　RESTful度量 152
7.4　日志记录 153
7.5　外部度量 154
7.6　系统度量 156
7.7　监控和报警 157
小结 159
第8章　实时ETL和分析技术 160
8.1　交易数据记录的强大功能 160
8.2　第一个流式Spark SQL应用程序 162
8.3　SQLContext 165
8.3.1　创建数据框 165
8.3.2　执行SQL 168
8.3.3　配置 169
8.3.4　用户自定义函数 169
8.3.5　Catalyst：查询执行和优化 171
8.3.6　HiveContext 171
8.4　数据框（Data Frame） 173
8.4.1　类型 173
8.4.2　查询转换 173
8.4.3　行动 180
8.4.4　RDD操作 182
8.4.5　持久化 182
8.4.6　最佳做法 183
8.5　SparkR 183
8.6　第一个SparkR应用程序 184
8.6.1　执行 185
8.6.2　流式SparkR 185
小结 188

第9章　大规模机器学习 189
9.1　传感器数据风暴 189
9.2　流式MLlib应用程序 191
9.3　MLlib 194
9.3.1　数据类型 194
9.3.2　统计分析 197
9.3.3　预处理 198
9.4　特征选择和提取 199
9.4.1　卡方选择 199
9.4.2　主成分分析 200
9.5　学习算法 201
9.5.1　分类 202
9.5.2　聚类 202
9.5.3　推荐系统 204
9.5.4　频繁模式挖掘 207
9.6　流式ML管道应用程序 208
9.7　ML 211
9.8　管道交叉验证 212
小结 213
第10章　云、Lambda及Python 215
10.1　一条好评胜过一千个广告 216
10.2　Google Dataproc 217
10.3　基于Dataproc应用程序创建的第一个Spark 220
10.4　PySpark 227
10.5　Lambda架构 229
10.6　流式图分析 238

第1章?Spark和Spark Streaming的安装与配置 1
安装Spark 2
硬件需求 2
软件需求 4
安装Spark扩展――Spark Streaming 7
配置和运行Spark集群 8
你的第一个Spark程序 11
用Scala编码Spark作业 12
用Java开发Spark作业 15
管理员/开发者工具 18
集群管理 18
提交Spark作业 19
故障定位 20
配置端口号 20
类路径问题――类没有发现 20
其他常见异常 20
总结 21
第2章?Spark和Spark Streaming的体系结构与组件 23
批处理和实时数据处理的比较 24
批处理 24
实时数据处理 26
Spark的体系结构 28
Spark对比Hadoop 28
Spark的层次化结构 29
Spark Streaming的体系结构 31
Spark Streaming是什么 32
Spark Streaming的上层体系结构 32
你的第一个Spark Streaming程序 34
用Scala编码Spark Streaming作业 34
用Java编码Spark Streaming作业 37
客户端程序 39
打包和部署一个Spark Streaming作业 41
总结 43
第3章?实时处理分布式日志文件 45
Spark的封装结构和客户端API 46
Spark内核 48
Spark库及扩展 54
弹性分布式数据集及离散流 58
弹性分布式数据集 59
离散流 63
从分布的、多样的数据源中加载数据 65
Flume 框架 67
Flume的安装和配置 69
配置Spark以接收Flume事件 73
封装和部署Spark Streaming作业 77
分布式日志文件处理的总体架构 77
总结 78
第4章?在流数据中应用Transformation 79
理解并应用Transformation功能 80
模拟日志流 80
功能操作 82
转换操作 89
窗口操作 91
性能调优 94
分块和并行化 94
序列化 94
Spark内存调优 95
总结 97
第5章?日志分析数据的持久化 99
Spark Streaming的输出操作 100
集成Cassandra 110
安装和配置Apache Cassandra 110
配置Spark 112
通过编写Spark作业将流式网页日志存入Cassandra 113
总结 120
第6章?与Spark高级库集成 121
实时查询流数据 122
了解Spark SQL 122
集成Spark SQL与流数据 129
图的分析――Spark GraphX 135
GraphX API介绍 137
集成Spark Streaming 140
总结 147
第7章?产品部署 149
Spark部署模式 150
部署在Apache Mesos上 151
部署在Hadoop或者YARN上 156
高可用性和容错性 160
单机模式下的高可用性 160
Mesos或者YARN下的高可用性 162
容错性 162
Streaming 作业的监听 166
应用程序UI界面/作业UI界面 166
与其他监控工具的集成 169

第1章 Spark Streaming应用概述 ······1
1.1 Spark Streaming应用案例 ·······2
1.2　Spark Streaming应用剖析 ·····13
第2章 Spark Streaming基本原理 ····15
2.1　Spark Core简介 ··················16
2.2 Spark Streaming设计思想 ·····26
2.3 Spark Streaming整体架构 ·····30
2.4 编程接口 ·························33
第3章 Spark Streaming运行流程详解·············39
3.1 从StreamingContext的初始化到启动 ··········40
3.2 数据接收 ·························54
3.3 数据处理 ·························91
3.4 数据清理 ························115
3.5 容错机制 ························127
3.5.1 容错原理 ·························128
3.5.2 Driver容错机制 ·················152
3.5.3 Executor容错机制 ··············161
3.6 No Receiver方式 ···············167
3.7 输出不重复 ·····················175
3.8 消费速率的动态控制 ·········176
3.9 状态操作 ························189
3.10 窗口操作 ·······················212
3.11 页面展示 ·······················216
3.12 Spark Streaming应用程序的停止··········227
第4章Spark Streaming 性能调优机制···········237
4.1 并行度解析 ·····················238
4.1.1 数据接收的并行度 ·············238
4.1.2 数据处理的并行度 ·············240
4.2 内存······························240
4.3 序列化 ···························240
4.4 Batch Interval ···················241
4.5 Task ·······························242
4.6 JVM GC ·························242
第5章Spark 2.0中的流计算 ··········245
5.1 连续应用程序 ··················246
5.2 无边界表unbounded table ····248
5.3 增量输出模式 ··················249
5.4 API简化 ··························250
5.5 其他改进 ························250

=============

一 概述
1.1 什么是搜索？
  百度：我们比如说想找寻任何的信息的时候，就会上百度去搜索一下，比如说找一部自己喜欢的电影，或者说找一本喜欢的书，或者找一条感兴趣的新闻（提到搜索的第一印象）。百度 != 搜索
  • 1）互联网的搜索：电商网站、招聘网站、新闻网站、各种 app。
  • 2）IT 系统的搜索：OA 软件、办公自动化软件、会议管理、日程管理、项目管理。
  搜索，就是在任何场景下，找寻你想要的信息，这个时候，会输入一段你要搜索的关键字，然后就期望找到这个关键字相关的有些信息

1.2 如果用数据库做搜索会怎么样？

用数据库来实现搜索，是不太靠谱的。通常来说，性能会很差的。
1.3 什么是全文检索和 Lucene？
  1）全文检索，倒排索引
  全文检索是指计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。全文搜索搜索引擎数据库中的数据。
  


  2）lucene
  就是一个 jar 包，里面包含了封装好的各种建立倒排索引，以及进行搜索的代码，包括各种算法。我们就用 java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 进行去进行开发就可以了。


1.4 什么是 Elasticsearch？
  Elasticsearch，基于 lucene，隐藏复杂性，提供简单易用的 restful api 接口、java api 接口（还有其他语言的 api 接口）。
  关于 elasticsearch 的一个传说，有一个程序员失业了，陪着自己老婆去英国伦敦学习厨师课程。程序员在失业期间想给老婆写一个菜谱搜索引擎，觉得 lucene 实在太复杂了，就开发了一个封装了 lucene 的开源项目--Compass。后来程序员找到了工作，是做分布式的高性能项目的，觉得 Compass 不够，就写了 Elasticsearch，让 lucene 变成分布式的系统。
  Elasticsearch 是一个实时分布式搜索和分析引擎。它用于全文搜索、结构化搜索、数据分析。
  全文检索：将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。
  结构化检索：我想搜索商品分类为日化用品的商品都有哪些，如：select * from products where category_id='日化用品'
  数据分析：电商网站，最近7天牙膏这种商品销量排名前10的商家有哪些？新闻网站，最近1个月访问量排名前3的新闻版块是哪些？

1.5 Elasticsearch 的适用场景
• 1）维基百科，类似百度百科，比如：牙膏，牙膏的维基百科，全文检索、高亮、搜索推荐。
• 2）The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击、浏览、收藏、评论）+ 社交网络数据（对某某新闻的相关看法）；数据分析，给到每篇新闻文章的作者，让他知道他的文章的公众反馈（好、坏、热门、垃圾、鄙视、崇拜等）。
• 3）Stack Overflow（国外的程序异常讨论论坛），IT 问题，程序的报错，提交上去，有人会跟你讨论和回答；全文检索，搜索相关问题和答案，程序报错了，就会将报错信息粘贴到里面去，搜索有没有对应的答案。
• 4）GitHub（开源代码管理），搜索上千亿行代码。
• 5）国内：站内搜索（电商、招聘、门户 等等）；IT 系统搜索（OA、CRM、ERP 等等）；数据分析（ES 热门的一个使用场景）。

1.6 Elasticsearch 的特点
• 1）可以作为一个大型分布式集群（数百台服务器）技术，处理 PB 级数据，服务大公司；也可以运行在单机上，服务小公司。
• 2）Elasticsearch 不是什么新技术，主要是将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的 ES；比如：lucene（全文检索），商用的数据分析软件（也是有的），分布式数据库（mycat）。
• 3）对用户而言，是开箱即用的，非常简单，作为中小型的应用，直接 3 分钟部署一下ES，就可以作为生产环境的系统来使用了，数据量不大，操作不是太复杂。
• 4）数据库的功能面对很多领域是不够用的（事务、还有各种联机事务型的操作）；特殊的功能，比如全文检索、同义词处理、相关度排名、复杂数据分析、海量数据的近实时处理；Elasticsearch 作为传统数据库的一个补充，提供了数据库所不能提供的很多功能。

1.7 Elasticsearch 的核心概念
1.7.1 近实时
  近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于 es 执行搜索和分析可以达到秒级。

1.7.2 Cluster（集群）
  集群包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是 elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常。

1.7.3 Node（节点）
  集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为 “elasticsearch” 的集群，如果直接启动一堆节点，那么它们会自动组成一个 elasticsearch 集群，当然一个节点也可以组成一个 elasticsearch 集群。

1.7.4 Index（索引 --> 数据库）
  索引包含一堆有相似结构的文档数据，比如可以有一个客户索引、商品分类索引、订单索引，索引有一个名称。一个 index 包含很多 document，一个 index 就代表了一类类似的或者相同的 document。比如说建立一个 product index(商品索引)，里面可能就存放了所有的商品数据，即所有的商品 document。

1.7.5 Type（类型 --> 表）
  每个索引里都可以有一个或多个 type，type 是 index 中的一个逻辑数据分类，一个 type 下的 document 都有相同的 field。比如博客系统，有一个索引，可以定义用户数据 type、博客数据 type、评论数据 type。
  商品 index，里面存放了所有的商品数据，即商品 document。
  但是商品分很多种类，每个种类的 document 的 field 可能不太一样，比如说电器商品，可能还包含一些诸如售后时间范围这样的特殊 field；生鲜商品，还包含一些诸如生鲜保质期之类的特殊 field。
  例如：type：日化商品type、电器商品 type、生鲜商品 type
  日化商品 type：product_id, product_name, product_desc, category_id, category_name
  电器商品 type：product_id, product_name, product_desc, category_id, category_name, service_period
  生鲜商品 type：product_id, product_name, product_desc, category_id, category_name, eat_period
  每一个 type 里面，都会包含一堆 document。如下：

电器商品 type

{
  "product_id": "1",
  "product_name": "长虹电视机",
  "product_desc": "4k高清",
  "category_id": "3",
  "category_name": "电器",
  "service_period": "1年"
}
生鲜商品 type

{
  "product_id": "2",
  "product_name": "基围虾",
  "product_desc": "纯天然，冰岛产",
  "category_id": "4",
  "category_name": "生鲜",
  "eat_period": "7天"
}
1.7.6 Document（文档 --> 行）
  文档是 es 中的最小数据单元，一个 document 可以是一条客户数据、一条商品分类数据、一条订单数据，通常用 JSON 数据结构表示，每个 index 下的 type 中，都可以去存储多个 document。

1.7.7 Field（字段 --> 列）
  Field 是 Elasticsearch 的最小单位。一个 document 里面有多个 field，每个 field 就是一个数据字段。

1.7.8 Mapping（映射 --> 约束）
  数据如何存放到索引对象上，需要有一个映射配置，包括：数据类型、是否存储、是否分词等。
  这样就创建了一个名为 blog 的 Index。Type 不用单独创建，在创建 Mapping 时指定就可以。Mapping 用来定义 Document 中每个字段的类型，即所使用的 analyzer、是否索引等属性。创建 Mapping 的代码示例如下：

client.indices.putMapping({
    index : 'blog',
    type : 'article',
    body : {
        article: {
            properties: {
                id: {
                    type: 'string',
                    analyzer: 'ik',
                    store: 'yes',
                },
                title: {
                    type: 'string',
                    analyzer: 'ik',
                    store: 'no',
                },
                content: {
                    type: 'string',
                    analyzer: 'ik',
                    store: 'yes',
                }
            }
        }
    }
});
1.7.9 Elasticsearch 与数据库的类比

1.7.10 ES 存入数据和搜索数据机制

回到顶部
二 快速入门
2.1 安装包下载
1）Elasticsearch官网： https://www.elastic.co/products/elasticsearch


点击 Download

点击 past releases 我们选择 5.2.2 Linux 版本的。

补充：ELK简介以及新旧版架构介绍
2.2 安装 Elasticsearch（单节点 Linux 环境）
注意：因为 Elasticsearch 是基于 java 写的，所以它的运行环境中需要 java 的支持，在 Linux 下执行命令：java -version，检查 Jar 包是否安装。安装 java 版本至少是 1.8 以上。

Step0：将 elasticsearch-5.2.2.tar.gz 上传至 Linux 的 /opt/software 目录下

Step1：解压 elasticsearch-5.2.2.tar.gz 到/opt/module 目录下

[atguigu@hadoop102 software]$ tar -zxf elasticsearch-5.2.2.tar.gz -C /opt/module/
Step2：在 /opt/module/elasticsearch-5.2.2 路径下创建 data 和 logs 文件夹

[atguigu@hadoop102 elasticsearch-5.2.2]$ mkdir data
[atguigu@hadoop102 elasticsearch-5.2.2]$ mkdir logs
Step3：修改配置文件 /opt/module/elasticsearch-5.2.2/config/elasticsearch.yml

[atguigu@hadoop102 config]$ pwd
/opt/module/elasticsearch-5.2.2/config
[atguigu@hadoop102 config]$ vim elasticsearch.yml
elasticsearch.yml

# ---------------------------------- Cluster -----------------------------------
cluster.name: my-application
# ------------------------------------ Node ------------------------------------
node.name: node-102
# ----------------------------------- Paths ------------------------------------
path.data: /opt/module/elasticsearch-5.2.2/data
path.logs: /opt/module/elasticsearch-5.2.2/logs
# ----------------------------------- Memory -----------------------------------
bootstrap.memory_lock: false
bootstrap.system_call_filter: false
# ---------------------------------- Network -----------------------------------
network.host: 192.168.25.102
# --------------------------------- Discovery ----------------------------------
discovery.zen.ping.unicast.hosts: ["hadoop102"]
文件详解如下：

(1) cluster.name
    如果要配置集群需要两个节点上的 elasticsearch 配置的 cluster.name 相同，都启动可以自动组成集群。
    这里如果不改 cluster.name 则默认是 cluster.name=my-application。
(2) node.name
    随意取但是集群内的各节点不能相同。
(3) 修改后的每行前面不能有空格，修改后的 “:” 后面必须有一个空格。
(4) bootstrap.memory_lock 和 bootstrap.system_call_filter 需要配置成 false，否则初始化会报错！
Step4：配置 linux 系统环境（参考：http://blog.csdn.net/satiling/article/details/59697916）
(1) 切换到 root 用户，编辑 limits.conf 添加类似如下内容

[root@hadoop102 elasticsearch-5.2.2]# vim /etc/security/limits.conf
添加如下内容:

* soft nofile 65536
* hard nofile 131072
* soft nproc 2048
* hard nproc 4096
(2) 切换到 root 用户，进入 limits.d 目录下修改配置文件。

[root@hadoop102 elasticsearch-5.2.2]# vim /etc/security/limits.d/90-nproc.conf
修改如下内容：

* soft nproc 1024
#修改为
* soft nproc 2048
(3) 切换到 root 用户，修改配置 sysctl.conf

[root@hadoop102 elasticsearch-5.2.2]# vim /etc/sysctl.conf
添加下面配置：

vm.max_map_count=655360

并执行命令：
[root@hadoop102 elasticsearch-5.2.2]# sysctl -p

然后，重新启动 elasticsearch，即可启动成功。
Step5：启动集群，注意：要切回 atguigu 用户启动集群，否则会报错！

[atguigu@hadoop102 elasticsearch-5.2.2]$ bin/elasticsearch
Step6：两种方式测试集群
方式一：重新开启一个会话窗口，输入命令：curl http://hadoop102:9200

[atguigu@hadoop102 elasticsearch-5.2.2]$ curl http://hadoop102:9200
{
  "name" : "node-102",
  "cluster_name" : "my-application",
  "cluster_uuid" : "yb29ijbJQ2mBzCHTOjyUGw",
  "version" : {
    "number" : "5.2.2",
    "build_hash" : "f9d9b74",
    "build_date" : "2017-02-24T17:26:45.835Z",
    "build_snapshot" : false,
    "lucene_version" : "6.4.1"
  },
  "tagline" : "You Know, for Search"
}
方式二：通过浏览器查看：http://hadoop102:9200/


Step7：停止集群

kill -9 进程号
2.3 安装 Elasticsearch（多节点集群 Linux 环境）
略

2.4 Elasticsearch head 插件安装
Step1：下载 elasticsearch-head 插件
  https://github.com/mobz/elasticsearch-head
  elasticsearch-head-master.zip

Step2：nodejs 官网下载安装包
  https://nodejs.org/dist/
  node-v6.9.2-linux-x64.tar.xz

Step3：将 elasticsearch-head-master.zip 和 node-v6.9.2-linux-x64.tar.xz 都上传到 Linux 的 /opt/software 目录下

Step4：安装nodejs

[atguigu@hadoop102 software]$ tar -zxf node-v6.9.2-linux-x64.tar.gz -C /opt/module/
Step5：切换到 root 用户，配置 nodejs 环境变量

[root@hadoop102 software]# vim /etc/profile

#NODE_HOME
export NODE_HOME=/opt/module/node-v6.9.2-linux-x64
export PATH=$PATH:$NODE_HOME/bin

[root@hadoop102 software]# source /etc/profile
Step6：查看 node 和 npm 版本

[root@hadoop102 software]# node -v
v6.9.2

[root@hadoop102 software]# npm -v
3.10.9
Step7：切换到 atguigu 用户，解压 head 插件到 /opt/module 目录下

[atguigu@hadoop102 software]$ unzip elasticsearch-head-master.zip -d /opt/module/
Step8：查看当前 head 插件目录下有无 node_modules/grunt 目录，没有的话，执行命令创建：

[atguigu@hadoop102 elasticsearch-head-master]$ npm install grunt --save
Step9：安装 head 插件：

[atguigu@hadoop102 elasticsearch-head-master]$ npm install -g cnpm --registry=https://registry.npm.taobao.org
Step10：安装 grunt：

[atguigu@hadoop102 elasticsearch-head-master]$ npm install -g grunt-cli
Step11：编辑 Gruntfile.js

[atguigu@hadoop102 elasticsearch-head-master]$ vim Gruntfile.js

在文件93行添加 hostname: '0.0.0.0',
options: {
        hostname: '0.0.0.0',
        port: 9100,
        base: '.',
        keepalive: true
      }
Step12：检查 head 根目录下是否存在 base 文件夹，如果没有，就创建 base 文件夹，然后将 _site 目录下的 base 文件夹及其内容复制到 head 根目录下

[atguigu@hadoop102 elasticsearch-head-master]$ mkdir base
[atguigu@hadoop102 _site]$ cp base/* ../base/
Step13：启动 grunt server：

[atguigu@hadoop102 elasticsearch-head-master]$ grunt server -d
Running "connect:server" (connect) task
[D] Task source: /opt/module/elasticsearch-head-master/node_modules/grunt-contrib-connect/tasks/connect.js
Waiting forever...
Started connect web server on http://localhost:9100

如果提示 grunt 的模块没有安装：
Local Npm module “grunt-contrib-clean” not found. Is it installed?
Local Npm module “grunt-contrib-concat” not found. Is it installed?
Local Npm module “grunt-contrib-watch” not found. Is it installed?
Local Npm module “grunt-contrib-connect” not found. Is it installed?
Local Npm module “grunt-contrib-copy” not found. Is it installed?
Local Npm module “grunt-contrib-jasmine” not found. Is it installed?
Warning: Task “connect:server” not found. Use –force to continue.

执行以下命令：
npm install grunt-contrib-clean -registry=https://registry.npm.taobao.org
npm install grunt-contrib-concat -registry=https://registry.npm.taobao.org
npm install grunt-contrib-watch -registry=https://registry.npm.taobao.org
npm install grunt-contrib-connect -registry=https://registry.npm.taobao.org
npm install grunt-contrib-copy -registry=https://registry.npm.taobao.org
npm install grunt-contrib-jasmine -registry=https://registry.npm.taobao.org

最后一个模块可能安装不成功，但是不影响使用。

再次重新启动 grunt server，启动成功
Step14：浏览器访问 head 插件：http://hadoop102:9100


Step15：启动集群插件后发现【集群健康值：未连接】
在 /opt/module/elasticsearch-5.2.2/config 路径下修改配置文件 elasticsearch.yml，在文件末尾增加：

[atguigu@hadoop102 config]$ pwd
/opt/module/elasticsearch-5.2.2/config
[atguigu@hadoop102 config]$ vim elasticsearch.yml

http.cors.enabled: true
http.cors.allow-origin: "*"
再次重新启动 elasticsearch，然后再次重新启动 grunt server。
浏览器再次访问 head 插件：http://hadoop102:9100，成功截图如下：


Step16：关闭插件服务

ctrl+c
[atguigu@hadoop102 elasticsearch-head-master]$ netstat -lntp | grep 9100
tcp        0      0 192.168.25.102:9100          0.0.0.0:*                   LISTEN      6070/grunt
回到顶部
三 Java API 操作
  Elasticsearch 的 Java 客户端非常强大；它可以建立一个嵌入式实例并在必要时运行管理任务。
  运行一个 Java 应用程序和 Elasticsearch 时，有两种操作模式可供使用。该应用程序可在 Elasticsearch 集群中扮演更加主动或更加被动的角色。在更加主动的情况下（称为 Node Client），应用程序实例将从集群接收请求，确定哪个节点应处理该请求，就像正常节点所做的一样。（应用程序甚至可以托管索引和处理请求。）另一种模式称为 Transport Client，它将所有请求都转发到另一个 Elasticsearch 节点，由后者来确定最终目标。

3.1 API 基本操作
3.1.1 操作环境准备
1）创建 maven 工程（不使用骨架的方式）


2）添加 pom.xml 文件
    <dependencies>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>3.8.1</version>
            <scope>test</scope>
        </dependency>

        <dependency>
            <groupId>org.elasticsearch</groupId>
            <artifactId>elasticsearch</artifactId>
            <version>5.2.2</version>
        </dependency>

        <dependency>
            <groupId>org.elasticsearch.client</groupId>
            <artifactId>transport</artifactId>
            <version>5.2.2</version>
        </dependency>

        <dependency>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-core</artifactId>
            <version>2.9.0</version>
        </dependency>
    </dependencies>
3）等待依赖的 jar 包下载完成
当直接在 Elasticsearch 建立文档对象时，如果索引不存在的，默认会自动创建，映射采用默认方式 。

3.1.2 获取 Transport Client
（1）ElasticSearch 服务默认端口 9300。
（2）ElasticSearch Web 管理平台默认端口 9200。

    private TransportClient client;

    @SuppressWarnings({ "unchecked" })
    @Before // 表示先执行这个方法
    public void getClient() throws UnknownHostException {
        // 1、获取客户端对象，设置连接的集群名称
        Settings settings = Settings.builder().put("cluster.name", "my-application").build();
        client = new PreBuiltTransportClient(settings);
        // 2、连接集群
        client.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName("hadoop102"), 9300));
        // 3、打印集群名称
        System.out.println(client.toString());
    }
3.1.3 创建索引
1）源代码

    @Test
    public void createIndex() {
        // 1、创建索引（indices 指数）
        client.admin().indices().prepareCreate("blog").get();
        // 2、关闭连接
        client.close();
    }
2）查看结果

{"blog":{"aliases":{},"mappings":{},"settings":{"index":{"creation_date":"1507466730030","number_of_shards":"5","number_of_replicas":"1","uuid":"lec0xYiBSmStspGVa6c80Q","version":{"created":"5060299"},"provided_name":"blog"}}}}
3.1.4 删除索引
1）源代码

    @Test
    public void deleteIndex(){
        // 1、删除索引
        client.admin().indices().prepareDelete("blog").get();
        // 2、关闭连接
        client.close();
    }
2）查看结果
浏览器查看http://hadoop102:9200/blog
没有blog索引了。

{"error":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"bolg","index_uuid":"_na_","index":"bolg"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"bolg","index_uuid":"_na_","index":"bolg"},"status":404}
3.1.5 新建文档（源数据是手写的 json 串）
当直接在 ElasticSearch 建立文档对象时，如果索引不存在的，默认会自动创建，映射采用默认方式。
1）源代码

    @Test
    public void createIndexByJson() throws UnknownHostException {
        // 1、文档数据准备
        String json = "{" + "\"id\":\"1\"," + "\"title\":\"基于Lucene的搜索服务器\","
                + "\"content\":\"它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口\"" + "}";
        // 2、创建文档
        IndexResponse indexResponse = client.prepareIndex("blog", "article", "1").setSource(json).execute().actionGet();
        // 3、打印返回的结果
        System.out.println("index:" + indexResponse.getIndex());
        System.out.println("type:" + indexResponse.getType());
        System.out.println("id:" + indexResponse.getId());
        System.out.println("version:" + indexResponse.getVersion());
        System.out.println("result:" + indexResponse.getResult());
        // 4、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果


浏览器结果

3.1.6 新建文档（源数据是以 map 方式添加的键值对）
1）源代码

    @Test
    public void createIndexByMap() {
        // 1、文档数据准备
        Map<String, Object> json = new HashMap<String, Object>();
        json.put("id", "2");
        json.put("title", "基于Lucene的搜索服务器");
        json.put("content", "它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口");
        // 2、创建文档
        IndexResponse indexResponse = client.prepareIndex("blog", "article", "2").setSource(json).execute().actionGet();
        // 3、打印返回的结果
        System.out.println("index:" + indexResponse.getIndex());
        System.out.println("type:" + indexResponse.getType());
        System.out.println("id:" + indexResponse.getId());
        System.out.println("version:" + indexResponse.getVersion());
        System.out.println("result:" + indexResponse.getResult());
        // 4、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果


浏览器结果

3.1.7 新建文档（源数据是通过 es 构建器构建的数据）
1）源代码

    @Test
    public void createIndexByBuilder() throws Exception {
        // 1、通过 es 自带的帮助类，来构建 json 数据
        XContentBuilder builder = XContentFactory.jsonBuilder().startObject()
                .field("id", "3")
                .field("title", "基于Lucene的搜索服务器")
                .field("content", "它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口")
                .endObject();
        // 2、创建文档
        IndexResponse indexResponse = client.prepareIndex("blog", "article", "3").setSource(builder).get();
        // 3、打印返回的结果
        System.out.println("index:" + indexResponse.getIndex());
        System.out.println("type:" + indexResponse.getType());
        System.out.println("id:" + indexResponse.getId());
        System.out.println("version:" + indexResponse.getVersion());
        System.out.println("result:" + indexResponse.getResult());
        // 4、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果


浏览器结果

3.1.8 搜索文档数据（单个索引）
1）源代码

    @Test
    public void getData() throws Exception {
        // 1、查询文档
        GetResponse response = client.prepareGet("blog", "article", "1").get();
        // 2、打印搜索的结果
        System.out.println(response.getSourceAsString());
        // 3、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果


3.1.9 搜索文档数据（多个索引）
1）源代码

    @Test
    public void getMultiData() {
        // 1、查询多个文档
        MultiGetResponse response = client.prepareMultiGet()
                .add("blog", "article", "1")
                .add("blog", "article", "2", "3")
                .add("blog", "article", "2").get();
        // 2、遍历返回的结果
        for (MultiGetItemResponse itemResponse : response) {
            GetResponse getResponse = itemResponse.getResponse();
            // 如果获取到查询结果
            if (getResponse.isExists()) {
                String sourceAsString = getResponse.getSourceAsString();
                System.out.println(sourceAsString);
            }
        }
        // 3、关闭资源
        client.close();
    }
2）结果查看
控制台打印结果


3.1.10 更新文档数据（update）
1）源代码

    @Test
    public void updateData() throws Throwable {
        // 1、创建更新数据的请求对象
        UpdateRequest updateRequest = new UpdateRequest();
        updateRequest.index("blog");
        updateRequest.type("article");
        updateRequest.id("3");
        updateRequest.doc(XContentFactory.jsonBuilder().startObject()
                .field("title", "基于Lucene的搜索服务器") // 对没有的字段进行添加，对已有的字段进行替换
                .field("content", "它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。大数据前景无限")
                .field("createDate", "2017-8-22").endObject());
        // 2、获取更新后的值
        UpdateResponse indexResponse = client.update(updateRequest).get();
        // 3、打印返回的结果
        System.out.println("index:" + indexResponse.getIndex());
        System.out.println("type:" + indexResponse.getType());
        System.out.println("id:" + indexResponse.getId());
        System.out.println("version:" + indexResponse.getVersion());
        System.out.println("result:" + indexResponse.getResult());
        // 4、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果


浏览器结果

3.1.11 更新文档数据（upsert）
设置查询条件，查找不到则添加 IndexRequest 内容，查找到则按照 UpdateRequest 更新。
1）源代码

    @Test
    public void upsertData() throws Exception {
        // 设置查询条件，查找不到则添加 IndexRequest 内容
        IndexRequest indexRequest = new IndexRequest("blog", "article", "5")
                .source(XContentFactory.jsonBuilder().startObject()
                .field("title", "搜索服务器")
                .field("content","Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。").endObject());
        // 设置更新，查找到则按照 UpdateRequest 更新
        UpdateRequest upsert = new UpdateRequest("blog", "article", "5")
                .doc(XContentFactory.jsonBuilder().startObject().field("user", "李四").endObject()).upsert(indexRequest);
        client.update(upsert).get();
        client.close();
    }
2）结果查看
第一次执行，浏览器结果


第二次执行，浏览器结果

3.1.12 删除文档数据（prepareDelete）
1）源代码

    @Test
    public void deleteData() {
        // 1、删除文档数据
        DeleteResponse indexResponse = client.prepareDelete("blog", "article", "5").get();
        // 2、打印返回的结果
        System.out.println("index:" + indexResponse.getIndex());
        System.out.println("type:" + indexResponse.getType());
        System.out.println("id:" + indexResponse.getId());
        System.out.println("version:" + indexResponse.getVersion());
        System.out.println("result:" + indexResponse.getResult());
        // 3、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果


3.2 条件查询 QueryBuilder
3.2.1 查询所有（matchAllQuery）
1）源代码

    @Test
    public void matchAllQuery() {
        // 1、执行查询（查询所有）
        SearchResponse searchResponse = client.prepareSearch("blog").setTypes("article")
                .setQuery(QueryBuilders.matchAllQuery()).get();
        // 2、打印查询结果
        SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象
        System.out.println("查询结果有：" + hits.getTotalHits() + "条");
        Iterator<SearchHit> iterator = hits.iterator();
        while (iterator.hasNext()) {
            SearchHit searchHit = iterator.next(); // 每个查询对象
            System.out.println(searchHit.getSourceAsString()); // 获取字符串格式打印
        }
        // 3、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果


3.2.2 对所有字段分词查询（queryStringQuery）
1）源代码

    @Test
    public void queryStringQuery() {
        // 1、条件查询（对所有字段分词查询）
        SearchResponse searchResponse = client.prepareSearch("blog").setTypes("article")
                .setQuery(QueryBuilders.queryStringQuery("全文")).get();
        // 2、打印查询结果
        SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象
        System.out.println("查询结果有：" + hits.getTotalHits() + "条");
        Iterator<SearchHit> iterator = hits.iterator();
        while (iterator.hasNext()) {
            SearchHit searchHit = iterator.next(); // 每个查询对象
            System.out.println(searchHit.getSourceAsString()); // 获取字符串格式打印
        }
        // 3、关闭连接
        client.close();
    }
2）结果查看
同上--查询所有（matchAllQuery）

3.2.3 通配符查询（wildcardQuery）
*：表示多个字符（任意的字符）
?：表示单个字符
1）源代码

    @Test
    public void wildcardQuery() {
        // 1、通配符查询
        SearchResponse searchResponse = client.prepareSearch("blog").setTypes("article")
                .setQuery(QueryBuilders.wildcardQuery("content", "*全*")).get();
        // 2、打印查询结果
        SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象
        System.out.println("查询结果有：" + hits.getTotalHits() + "条");
        Iterator<SearchHit> iterator = hits.iterator();
        while (iterator.hasNext()) {
            SearchHit searchHit = iterator.next(); // 每个查询对象
            System.out.println(searchHit.getSourceAsString()); // 获取字符串格式打印
        }
        // 3、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果
同上--查询所有（matchAllQuery）

3.2.4 词条查询（TermQuery）
1）源代码

    @Test
    public void termQuery() {
        // 1、词条查询
        SearchResponse searchResponse = client.prepareSearch("blog").setTypes("article")
                .setQuery(QueryBuilders.termQuery("content", "全")).get(); // 因为没有使用 IK 分词器，所有只能一个字一个字的查
        // 2、打印查询结果
        SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象
        System.out.println("查询结果有：" + hits.getTotalHits() + "条");
        Iterator<SearchHit> iterator = hits.iterator();
        while (iterator.hasNext()) {
            SearchHit searchHit = iterator.next(); // 每个查询对象
            System.out.println(searchHit.getSourceAsString()); // 获取字符串格式打印
        }
        // 3、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果
同上--查询所有（matchAllQuery）

3.2.5 模糊查询（fuzzy）
1）源代码

    @Test
    public void fuzzyQuery() {
        // 1、模糊查询
        SearchResponse searchResponse = client.prepareSearch("blog").setTypes("article")
                .setQuery(QueryBuilders.fuzzyQuery("title", "lucene")).get();
        // 2、打印查询结果
        SearchHits hits = searchResponse.getHits(); // 获取命中次数，查询结果有多少对象
        System.out.println("查询结果有：" + hits.getTotalHits() + "条");
        Iterator<SearchHit> iterator = hits.iterator();
        while (iterator.hasNext()) {
            SearchHit searchHit = iterator.next(); // 每个查询对象
            System.out.println(searchHit.getSourceAsString()); // 获取字符串格式打印
        }
        // 3、关闭连接
        client.close();
    }
2）结果查看
控制台打印结果
同上--查询所有（matchAllQuery）

3.3 映射相关操作
注意：执行映射操作之前，需要先创建一个新的没有存在 mapping 的索引，本例子中新的索引是 blog2，如下：
1）源代码

    @Test
    public void createIndex() {
        // 1、创建索引（indices 指数）
        client.admin().indices().prepareCreate("blog2").get();
        // 2、关闭连接
        client.close();
    }

    @Test
    public void createMapping() throws Exception {
        // 1、设置 mapping
        XContentBuilder builder = XContentFactory.jsonBuilder()
                .startObject()
                    .startObject("article2")
                        .startObject("properties")
                            .startObject("id2")
                                .field("type", "string")
                                .field("store", "yes")
                            .endObject()
                            .startObject("title2")
                                .field("type", "string")
                                .field("store", "no")
                            .endObject()
                            .startObject("content2")
                                .field("type", "string")
                                .field("store", "yes")
                            .endObject()
                        .endObject()
                    .endObject()
                .endObject();
        // 2、添加 mapping
        PutMappingRequest mapping = Requests.putMappingRequest("blog2").type("article2").source(builder);
        client.admin().indices().putMapping(mapping).get();
        // 3、关闭资源
        client.close();
    }
2）结果查看
浏览器结果


我的GitHub地址：https://github.com/heizemingjun


============
借助Beats搭建可视化运维系统
更新时间：2019-03-20 11:01:55

编辑 ·
 · 我的收藏
本页目录
背景介绍
背景介绍
Beats平台集合了多种单一用途数据采集器，这些采集器安装后可用作轻量型代理，从成百上千或成千上万台机器向 Logstash 或 Elasticsearch 发送数据。

Metricbeat是一个轻量级的指标采集器，用于从系统和服务收集指标。从 CPU 到内存，从 Redis 到 Nginx，Metricbeat 能够以一种轻量型的方式，输送各种系统和服务统计数据。

这篇文章主要向用户演示，如何使用Metricbeat采集一台Mac电脑的指标信息，投递到阿里云Elasticsearch（以下统称阿里云ES）上，并且在Kibana中生成对应dashborard。

说明 使用Metricbeat采集一台基于Linux系统或windows系统电脑的指标信息，投递到阿里云Elasticsearch（以下统称阿里云ES）上也是类似的操作步骤。
购买并配置阿里云ES
如果没有阿里云ES实例，需要先进行准备工作。可以通过阿里云ES提供的内网地址/公网地址将本地MAC中的数据推送给阿里云ES。

说明
如果是通过阿里云ES公网地址来访问（本篇以此为例），需要先打开阿里云ES实例公网访问的开关，并在网络配置及数据备份界面中配置公网地址访问白名单。

如果是通过阿里云ES内网地址来访问，需要先购买一台与阿里云ES实例相同VPC和Region的阿里云ECS实例进行访问操作。

登陆阿里云ES控制台，单击阿里云ES实例管理，切换到网络配置及数据备份界面，打开公网地址开关。

将自己的MAC机器对外的公网IP配置到公网地址访问白名单中。

注意 如果你使用的是公司或WIFI等网络，需要将公网出口的跳板机IP配置进去。如果获取不到，建议配置 0.0.0.0/1,128.0.0.0/1来开放尽可能多的IP（本篇以此为例）， 需要特别注意这个配置将导致你的阿里云ES基本上完全暴露在公网中，需要先评估下是否可以接受这个风险。
配置完成后，获取阿里云ES的公网地址备用。

修改YML文件配置 允许自动创建索引（默认不允许），该操作会触发重启阿里云ES进行生效，需要一些时间来生效。

下载并配置Metricbeat
MAC系统的Metricbeat安装包下载地址。
32位Linux系统的Metricbeat安装包下载地址。
64位Linux系统的Metricbeat安装包下载地址。
32位Windows系统的Metricbeat安装包下载地址。
64位Windows系统的Metricbeat安装包下载地址。
下载到本地某一个文件夹后，解压缩，进入Metricbeat文件夹。

打开并编辑 metricbeat.yml 中 Elasticsearch output部分内容，需取消对应内容注释状态。

说明
阿里云ES提供了用户名和密码的访问控制。

hosts：为阿里云ES实例的公网/内网地址（本篇以阿里云ES公网地址为例）。
protocol：需要配置为 http。
username：默认是elastic。
password：为购买阿里云ES服务时填写的登陆密码。
启动Metricbeat
启动Metricbeat向阿里云ES推送数据。

./metricbeat -e -c metricbeat.yml

在Kibana中查看dashboard
打开阿里云ES实例中集成的kibana控制台，切换到dashboard界面展示如下。
说明
如果kibana控制台中没有创建过Index Patterns，切换到dashboard界面后可能无法正常展示对应信息，此时可以创建1个Index Patterns，再切换到dashboard界面查看对应内容。

各类相关指标列表。

Metricbeat-cpu指标信息。

说明 数据可以定义成5s刷新一次，并且可以生成对应的report，接入webhook对异常进行告警。


云上数据导入
更新时间：2019-03-20 11:02:15

编辑 ·
 · 我的收藏
本页目录
阿里云上数据导入阿里云ES（离线）
支持数据源
操作步骤
实时数据导入
阿里云上数据导入阿里云ES（离线）
阿里云上拥有丰富的云存储、云数据库产品。如果您希望针对这些产品中的数据进行分析和搜索，可以通过 数据集成（Data Integration）来实现最快 5min 一次的离线数据，同步到Elasticsearch的需求。

支持数据源
阿里云云数据库(MySQL、PG、SQL Server、PPAS、MongoDB、HBase)
阿里云DRDS
阿里云MaxCompute(ODPS)
阿里云OSS
阿里云Table Store
自建HDFS、Oracle、FTP、DB2，及上述云数据库的自建版本
说明 做数据同步时可能会产生公网流量费用，请您知晓。
操作步骤
完成离线数据导入，需要您完成以下几步操作：

您需要有一台可以与 VPC 内的 Elasticsearch 交互的ECS，这台 ECS 将获取数据源数据并执行写ES数据的“Job”(该任务将由“Data Integration系统统一下发”)。
您需要开通 Data Intergration 的服务，并且将ECS作为一个可以执行 Job 的“资源”注册到 Data Intergration 的服务中去。
您需要配置一个“数据同步的脚本”，并且让其可以周期性的执行起来。
详细步骤
购买一台与Elasticsearch服务处于同一个VPC内的ECS服务器，并分配一个公网IP合或开通弹性IP，为了节省您的成本，您可以复用已有的ECS服务器，(如何购买ECS，请参考步骤 2：创建ECS实例文档)。

说明
建议使用 centos6、centos7 或者 aliyunos。
如果您添加的 ECS 需要执行 MaxCompute 任务或者同步任务，需要检查当前 ECS 的 python 版本是否是 python2.6或2.7 的版本（centos5 的版本为 2.4 ，其余 os 自带了 2.6 以上版本）。
请确保 ECS 有公网 IP。
前往Data Integration控制台，并进入工作区。

如果您已经开通过Data Integration 或者 DataWorks 产品，您将会看到如下页面：

如果您未开通过Data Integration 或者 DataWorks 产品，您需要按照步骤进行 Data Integration 的开通，此开通动作 会产生费用，请您按照费用提示进行预算评估。

进入Data Integration的项目管理-调度资源管理页面，将您之前VPC内的ECS配置成为一个调度资源。详细配置参考新增任务资源。

在Data Integration中配置数据同步脚本，具体配置请参考脚本模式配置。Elasticsearch的Config规则请参考 配置Elasticsearch Writer。

说明
同步脚本的配置分为三个部分，Reader是配置您上游数据源（待同步数据的云产品）的config，Writer是配置ES的config，还有一个setting是配置同步中的一些丢包和最大并发等设置
ES Writer中accessId和accessKey需要配置您的Elasticsearch的访问“用户名”和“密码”
脚本配置完成后，将数据同步Job进行提交，按照需求填写相应的周期性执行配置，并点击“确定”完成提交动作。

说明
如果您希望周期性调度，需要配置周期任务，具体的Job开始执行时间，周期间隔，Job生命周期，均需要在这个弹窗中配置
周期任务将于配置任务开始的第二天00:00，按照您的配置规则生效执行
最后一步提交完成后，请务必前往运维中心-周期任务，找到您提交的Job，将其调度资源从 默认修改为 您配置好的调度资源。

实时数据导入


同步 MySQL 数据库到 Elasticsearch 中并进行搜索分析
更新时间：2019-03-20 11:03:36

编辑 ·
 · 我的收藏
本页目录
准备工作
操作步骤
数据搜索分析
常见问题
阿里云上拥有丰富的云存储、云数据库产品。如果您希望针对这些产品中的数据进行分析和搜索，可以通过DataWorks的数据集成服务，将离线数据同步到Elasticsearch中，最快可达到5分钟一次。

说明 做数据同步时可能会产生公网流量费用，请您知晓。
准备工作
完成离线数据的分析与搜索，需要您完成以下几步操作：

创建一个数据库，您可以选择使用阿里云的RDS数据库，也可以在本地服务器上自建数据库。本文档以RDS MySQL数据库为例，数据库字段及数据如下图所示。


购买一台可以与VPC内的Elasticsearch交互的ECS，这台ECS将获取数据源数据并执行写Elasticsearch数据的任务（该任务将由数据集成系统统一下发）。

开通DataWorks的数据集成服务，并且将ECS作为一个可以执行任务的资源，注册到数据集成服务中去。

配置一个数据同步的脚本，并且让其可以周期性的执行起来。

创建一个Elasticsearch实例，用来存储数据集成系统同步成功的数据。

操作步骤
数据同步
搭建IPv4专有网络。

进入Elasticsearch控制台，单击 创建，创建一个Elasticsearch实例。
说明 地域、 专有网络、 虚拟交换机与您第一步中创建的专有网络保持一致。

购买一台与Elasticsearch服务处于同一个VPC内的ECS服务器，并分配一个公网IP合或开通弹性IP，为了节省您的成本，您可以复用已有的ECS服务器。
说明
建议使用 centos6、centos7 或者 aliyunos。
如果您添加的 ECS 需要执行 MaxCompute 任务或者同步任务，需要检查当前 ECS 的 python 版本是否是 python2.6或2.7 的版本（centos5 的版本为 2.4 ，其余 os 自带了 2.6 以上版本）。
请确保 ECS 有公网 IP。
进入DataWorks 控制台，并进入工作区。

如果您已经开通过DataWorks数据集成产品，您将会看到如下页面：


如果您未开通过DataWorks数据集成产品，您将会看到如下页面。您需要按照步骤开通数据集成服务，此开通动作会产生费用，请您按照费用提示进行预算评估。


单击DataWorks项目下方的进入数据集成。

在数据集成页面，选择左侧导航栏中的资源组，单击新增资源组。

根据界面提示，输入资源组名称和服务器信息。此服务器为您已经购买的ECS服务器，服务器信息说明如下：


ECS UUID：步骤 3：连接ECS实例 服务器，执行 dmidecode | grep UUID，取返回值。

机器 IP/机器CPU（核）/机器内存（GB）：您ECS实例的公网IP/CPU/内存。您可以在ECS控制台上单击实例名称，在配置信息模块，找到相关信息。

按照界面提示，完成安装Agent步骤。其中第五步为开通服务器的8000端口，可以跳过，保持系统默认即可。

配置数据库白名单，添加该资源组的IP地址和DataWorks服务器的IP地址，到您的数据库白名单中。配置方法请参见添加白名单。

资源组创建成功后，选择左侧导航栏的数据源，单击新增数据源。

单击MySQL，进入新增MySQL数据源页面，填入数据源信息，如下图所示。


数据源类型：本文档以阿里云数据库（RDS）为例，您也可以选择有公网IP和无公网IP。各配置项的详细信息请参见配置MySQL数据源。

选择左侧导航栏的同步任务，单击新建，选择脚本模式。

在导入模板对话框中，选择数据源类型 > MySQL，数据源为您第10步中新增的数据源名称，目标类型为Elasticsearch，完成后单击确认。


配置数据同步脚本。具体配置请参考脚本模式配置，Elasticsearch 的配置规则请参考配置Elasticsearch Writer。


说明
同步脚本的配置分为三个部分，Reader用来配置您上游数据源（待同步数据的云产品）的config，Writer用来配置 Elasticsearch的config，setting用来配置同步中的一些丢包和最大并发等。
endpoint为Elasticsearch 的内网或外网地址，如果您使用的是内网地址，请在Elasticsearch的集群配置页面，配置Elasticsearch的系统白名单。如果您是用的是外网地址，请在Elasticsearch的网络配置页面，配置 Elasticsearch的公网地址访问白名单（包括DataWorks服务器的IP地址和您所使用的资源组的IP地址）。
Elasticsearch Writer中accessId和accessKey需要配置您的Elasticsearch的访问用户名（默认为elastic）和密码。
index为Elasticsearch实例的索引，您需要使用该索引名称访问Elasticsearch的数据。
同步脚本配置完成后，单击页面右侧的配置任务资源组，选择您第7步创建的资源组名称，完成后单击运行，将MySQL中的数据同步到Elasticsearch中。

数据搜索分析
进入Elasticsearch控制台，单击右上角的kibana控制台，选择Dev Tools。

执行如下命令，查看已经同步过来的数据。

POST /testrds/_search?pretty
{
"query": { "match_all": {}}
}
testrds为您同步数据时，设置的index字段的值。


执行如下命令，按照trans_num字段对文档进行排序。

POST /testrds/_search?pretty
{
"query": { "match_all": {} },
"sort": { "trans_num": { "order": "desc" } }
}
执行如下命令，搜索文档中的category和brand字段。

POST /testrds/_search?pretty
{
"query": { "match_all": {} },
"_source": ["category", "brand"]
}
执行如下命令，搜索category为生的文档。

POST /testrds/_search?pretty
{
"query": { "match": {"category":"生"} }
}

更多命令和访问方式，请参考ES访问测试 和Elastic.co官方帮助中心。

常见问题
同步过程中出现无法连接数据库的相关错误。

解决方法：将您资源组中所使用的ECS服务器的内网IP和外网IP，都添加到您数据库的白名单中。

同步过程中无法连通Elasticsearch实例的相关错误。

解决方法：按照下面步骤进行排查。

检查在运行同步脚本之前，是否在页面右侧的配置任务资源组中选择了您前面步骤创建的资源组。

是，执行下一步。
否，单击页面右侧的配置任务资源组，选择您前面步骤创建的资源组。完成后单击运行。
检查是否在Elasticsearch实例的白名单中，添加了DataWorks服务器的IP地址和您所使用的资源组的IP地址。

是，执行下一步。
否，将DataWorks服务器的IP地址和您所使用的资源组的IP地址，添加到 Elasticsearch 实例的白名单中。
说明 如果您使用的是内网地址，请在Elasticsearch的 集群配置页面，配置Elasticsearch的系统白名单。如果您是用的是外网地址，请在Elasticsearch的 网络配置页面，配置Elasticsearch的公网地址访问白名单（包括DataWorks服务器的IP地址和您所使用的资源组的IP地址）。
检查您的同步脚本配置是否正确。包括endpoint（您 Elasticsearch 实例的内网或外网地址）、accessId（Elasticsearch 实例的访问用户名，默认为elastic）和accessKey（Elasticsearch实例的访问密码）。



RDS for MySQL与阿里云ES实时同步数据
更新时间：2019-04-10 14:04:12

编辑 ·
 · 我的收藏
本页目录
支持实时同步类型
支持SQL操作类型
配置步骤
数据传输服务 DTS （以下简称 DTS）支持RDS for MySQL与阿里云Elasticsearch实时同步数据，通过 DTS 提供的 RDS for MySQL->阿里云Elasticsearch实时同步功能，可以将企业线上RDS for MySQL中的生产数据实时同步到阿里云Elasticsearch中进行搜索。本小节介绍如何使用 DTS 快速创建RDS for MySQL->阿里云Elasticsearch的实时同步作业，实现RDS for MySQL数据到阿里云Elasticsearch的实时同步。

支持实时同步类型
同一个阿里云账号下 RDS for MySQL->阿里云Elasticsearch实例。

支持SQL操作类型
主要支持的SQL操作类型如下：

Insert
Delete
Update
说明 目前暂不支持 DDL同步，如果同步过程中遇到DDL操作，DTS会忽略掉。
如果后续遇到DDL某个表，则对应表的DML操作可能失败，修复方法为：

参考减少同步对象先将这个对象从同步列表中摘除。
删除阿里云Elasticsearch中这个表对应的索引。
参考 新增同步对象， 修改这个同步作业，将这个表重新添加到同步对象中，进行重新初始化。
如果是修改表、新增列的DDL，建议DDL的操作顺序为：

先在阿里云Elasticsearch中手动修改对应表的mapping，新增列。
再在源RDS for MySQL实例中手动修改表结构，新增列。
暂停DTS同步实例，重启DTS同步实例让DTS重新加载阿里云Elasticsearch中修改后的mapping关系。
配置步骤
下面详细介绍创建RDS for MySQL实例到阿里云Elasticsearch实例同步链路的具体步骤。

购买同步链路
进入数据传输服务 DTS控制台，进入数据同步界面，点击控制台右上角创建同步作业先购买一个同步链路，购买完同步链路后返回DTS控制台，进行配置同步链路。

说明 在配置同步链路之前需要先购买一个同步链路，同步链路目前支持 包年包月及 按量付费两种付费模式，可以根据需要选择不同的付费模式。
购买界面参数
功能
选择数据同步。

源实例
选择MySQL。

源实例地域
本示例为RDS for MySQL，需选择RDS for MySQL实例所在地域。
目标实例
选择Elasticsearch。

目标实例地域
阿里云Elasticsearch实例所在地域，订购后不支持更换地域，请谨慎选择。

同步拓扑
选择单项同步。

网络类型
默认为专线，目前仅支持专线模式。

同步链路规格
同步链路规格影响了链路的同步性能，同步链路规格跟性能之间的对应关系详见数据同步规格说明。

订购时长
如果是预付费，默认为1个月，支持勾选开启自动续费功能。
购买数量
默认为1，根据业务实际需要进行选择。

说明 DTS控制台的同步实例按照地域展示，刚才购买的同步实例所属的地域为同步实例的目标地域。例如上面购买的是 杭州RDS for MySQL->杭州阿里云Elasticsearch的同步实例，那么这个同步实例在DTS的杭州地区。进入杭州区域的实例列表，查找刚才购买的同步实例，然后点击新购实例右侧的 配置同步作业开始配置实例。
配置同步链路

同步作业名称

同步作业名称没有唯一性要求，为了更方便识别具体的作业，建议选择一个有业务意义的作业名称，方便后续的链路查找及管理。

源实例信息
本示例采用数据源为 RDS for MySQL，需要配置RDS实例的ID、数据库账号、数据库密码。


目标实例信息
目标实例信息中需要配置阿里云Elasticsearch的实例ID，及访问阿里云ES实例账号密码。


以上内容配置完成后，点击授权白名单并进入下一步进行RDS for MySQL及阿里云Elasticsearch的白名单添加。

授权实例白名单
说明 如果是RDS for MySQL，DTS会自动添加白名单或安全组。
如果源实例为RDS for MySQL，那么DTS将自身的IP段添加到RDS实例的白名单的安全组中，避免因为RDS实例设置了白名单，DTS服务器连接不上数据库导致同步作业创建失败。为了保证同步作业的稳定性，在同步过程中，请勿将这些服务器 IP 从 RDS实例的白名单的安全组中删除。

当白名单授权后，点击下一步，进入同步账号创建。

选择同步对象
当白名单授权完成后，即进入同步对象的选择步骤。在这个步骤可以配置需要同步的表列，以及索引的命名规则。

索引名称命名规则可以选择：表名、库名_表名。

如果选择了表名，那么索引名称同表名。
如果选择了库名表名，那么索引名称的命名格式为：库名表名。例如，库名为：dbtest,表名为：sbtest1，那么这张表同步到阿里云Elasticsearch后，对应的索引名称为：dbtest_sbtest1。
如果需要同步的不同库中存在相同名称的表名，建议索引名称命名规则选择：库名_表名。
选择具体需要同步的库表列，实时同步的同步对象的选择粒度可以支持到表级别，即用户可以选择同步某些库或某几张表。

实时同步的同步对象的选择粒度可以支持到表级别，即用户可以选择同步某些库或某几张表。


默认所有表的docid为表的主键，如果部分表没有主键，那么对于这部分配置docid 对应的源表的列。在右侧-已选择对象 框中，将鼠标挪到对应表上，点击右侧的 编辑 入口，进入这个表的高级设置界面。

在高级配置中可以设置：

索引名称、Type名称、分区列及分区数定义、_id取值列。其中 _id 取值如果选择 业务主键，那么需要选择对应的业务主键列。

配置完同步对象后，进入高级配置步骤。

高级配置
主要配置
同步初始化类型，建议选择 结构初始化+全量数据初始化，由DTS自动进行索引的创建及全量数据的初始化。如果不选择结构初始化，那么需要在同步创建之前，先手动在阿里云Elasticsearch中完成索引mapping的定义。如果不选择全量数据初始化，那么DTS同步增量数据的起始时间点为：启动同步的时间点。

索引分片配置，默认为5个分片，1个副本。可以根据业务需要进行调整，一旦调整后，所有的索引按照这个配置定义分片。

字符串analyzer定义，可以选择字符串的analyzer，默认为Standard Analyzer。取值包括：Standard Analyzer、Simple Analyzer、Whitespace Analyzer、Stop Analyzer、Keyword Analyzer、English Analyzer、Fingerprint Analyzer，所有索引的字符串字段按照这个配置定义Analyzer。


时区，可以配置同步到阿里云Elasticsearch中的时间字段存储的时区，默认为东八区。

预检查
同步作业配置完成后，DTS会进行预检查，当预检查通过后，可以点击 启动 按钮，启动同步作业。

同步作业启动后，即进入同步作业列表，此时刚启动的作业处于同步初始化状态。初始化的时间长度取决于源实例中同步对象的数据量大小，初始化完成后，同步链路即进入同步中的状态，此时源跟目标实例的同步链路才真正建立。

数据效验
以上任务完执行成后，登录阿里云ES控制台，确认对应阿里云ES实例中有无创建对应索引，及同步的数据是否符合预期。

通过ES-Hadoop将Hadoop数据写入阿里云Elasticsearch
更新时间：2019-03-20 11:03:56

编辑 ·
 · 我的收藏
本页目录
支持版本
开通服务
编写EMR写数据到ES的MR作业
EMR中完成作业
结果验证
API分析
基于阿里云Elasticsearch和E-MapReduce，通过ES-Hadoop可直接将数据写入阿里云Elasticsearch。

支持版本
阿里云Elasticsearch 5.5.3 with X-Pack。

说明 不支持阿里云Elasticsearch 6.3.2 with X-Pack 版本。
开通服务
本示例需要用到的阿里云产品如下：

专有网络VPC：由于通过公网访问推送数据安全性较差，为保证阿里云Elasticsearch访问环境安全，对应区域下必须要有VPC和虚拟交换机，因此需开通VPC专有网络。
OSS：在本示例中将E-MapReduce的日志存储在OSS上，在开通配置E-MapReduce前需开通OSS并创建完成Bucket。
Elasticsearch
E-MapReduce
请参考以下步骤开通上述阿里云产品：

开通阿里云VPC

登录阿里云首页后选择产品 > 云计算基础 > 网络 > 创建网络环境 > 专有网络VPC，然后单击立即开通。
进入到VPC管理控制台界面，新建专有网络。
创建完成之后在控制台中可以进行管理。
说明 更多关于专有网络VPC的文档请参专有网络VPC 。
开通专有网络OSS

登录阿里云首页后选择产品 > 云计算基础 > 存储服务 > 云存储 > 对象存储 OSS，然后单击立即开通。
进入到OSS管理控制台界面，单击新建 Bucket。
说明 Bucket的区域要和E-MapReduce集群的区域一致，本示例将区域均选择为华东1区。
根据界面提示完成Bucket创建。
开通阿里云Elasticsearch

登录阿里云首页后选择产品 > 大数据 > 大数据搜索与分析 > Elasticsearch，进入阿里云Elasticsearch产品界面。
说明 新用户可以免费试用30天
购买成功后，在Elasticsearch控制台可以看到新创建的Elasticsearch集群实例。
开通阿里云E-MapReduce

登录阿里云首页后选择产品 > 大数据 > 大数据计算 > E-MapReduce，进入E-MapReduce产品页面。
单击立即购买，根据界面提示完成参数配置。
E-MapReduce集群创建成功后在集群列表中查看，也可通过以下操作验证集群创建结果：
公网IP可以直接访问，远程登录：
ssh root@你的公网IP
使用jps命令查看后台进程：
[root@emr-header-1 ~]# jps
16640 Bootstrap
17988 RunJar
19140 HistoryServer
18981 WebAppProxyServer
14023 Jps
15949 gateway.jar
16621 ZeppelinServer
1133 EmrAgent
15119 RunJar
17519 ResourceManager
1871 Application
19316 JobHistoryServer
1077 WatchDog
17237 SecondaryNameNode
16502 NameNode
16988 ApacheDsTanukiWrapper
18429 ApplicationHistoryServer
编写EMR写数据到ES的MR作业
推荐使用maven来进行项目管理，操作步骤如下：

安装 Maven
首先确保计算机已经正确安装maven。

生成工程框架
在工程根目录处执行如下命令：

mvn archetype:generate -DgroupId=com.aliyun.emrtoes -DartifactId=emrtoes -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
mvn 会自动生成一个空的Sample工程，工程名为emrtoes（和指定的artifactId一致），里面包含一个简单的 pom.xml和App类（类的包路径和指定的groupId一致）。

加入Hadoop和ES-Hadoop依赖
使用任意IDE打开这个工程，编辑pom.xml文件。在dependencies内添加如下内容：

<dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-mapreduce-client-common</artifactId>
      <version>2.7.3</version>
  </dependency>
  <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-common</artifactId>
      <version>2.7.3</version>
  </dependency>
   <dependency>
       <groupId>org.elasticsearch</groupId>
       <artifactId>elasticsearch-hadoop-mr</artifactId>
       <version>5.5.3</version>
   </dependency>
添加打包插件
由于使用了第三方库，需要把第三方库打包到jar文件中，在pom.xml中添加maven-assembly-plugin插件的坐标：

<plugins>
   <plugin>
     <artifactId>maven-assembly-plugin</artifactId>
     <configuration>
       <archive>
         <manifest>
           <mainClass>com.aliyun.emrtoes.EmrToES</mainClass>
         </manifest>
       </archive>
       <descriptorRefs>
         <descriptorRef>jar-with-dependencies</descriptorRef>
       </descriptorRefs>
     </configuration>
     <executions>
       <execution>
         <id>make-assembly</id>
         <phase>package</phase>
         <goals>
           <goal>single</goal>
         </goals>
       </execution>
     </executions>
   </plugin>
   <plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-shade-plugin</artifactId>
     <version>3.1.0</version>
     <executions>
       <execution>
         <phase>package</phase>
         <goals>
           <goal>shade</goal>
         </goals>
         <configuration>
           <transformers>
             <transformer implementation="org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer">
             </transformer>
           </transformers>
         </configuration>
       </execution>
     </executions>
   </plugin>
 </plugins>
编写代码
在com.aliyun.emrtoes包下和App类平行的位置添加新类EmrToES.java，内容如下：

package com.aliyun.emrtoes;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
 import org.apache.hadoop.util.GenericOptionsParser;
 import org.elasticsearch.hadoop.mr.EsOutputFormat;
 import java.io.IOException;
 public class EmrToES {
     public static class MyMapper extends Mapper<Object, Text, NullWritable, Text> {
         private Text line = new Text();
         @Override
         protected void map(Object key, Text value, Context context)
                 throws IOException, InterruptedException {
             if (value.getLength() > 0) {
                 line.set(value);
                 context.write(NullWritable.get(), line);
             }
         }
     }
     public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {
         Configuration conf = new Configuration();
         String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
         //阿里云 Elasticsearch X-PACK用户名和密码
         conf.set("es.net.http.auth.user", "你的X-PACK用户名");
         conf.set("es.net.http.auth.pass", "你的X-PACK密码");
         conf.setBoolean("mapred.map.tasks.speculative.execution", false);
         conf.setBoolean("mapred.reduce.tasks.speculative.execution", false);
         conf.set("es.nodes", "你的Elasticsearch内网地址");
         conf.set("es.port", "9200");
         conf.set("es.nodes.wan.only", "true");
         conf.set("es.resource", "blog/yunqi");
         conf.set("es.mapping.id", "id");
         conf.set("es.input.json", "yes");
         Job job = Job.getInstance(conf, "EmrToES");
         job.setJarByClass(EmrToES.class);
         job.setMapperClass(MyMapper.class);
         job.setInputFormatClass(TextInputFormat.class);
         job.setOutputFormatClass(EsOutputFormat.class);
         job.setMapOutputKeyClass(NullWritable.class);
         job.setMapOutputValueClass(Text.class);
         FileInputFormat.setInputPaths(job, new Path(otherArgs[0]));
         System.exit(job.waitForCompletion(true) ? 0 : 1);
     }
 }
编译并打包
在工程的目录下，执行如下命令：

mvn clean package
执行完毕以后，可在工程目录的target目录下看到一个emrtoes-1.0-SNAPSHOT-jar-with-dependencies.jar，这个就是作业jar包。


EMR中完成作业
测试数据
把下面的数据写入到blog.json中：

{"id":"1","title":"git简介","posttime":"2016-06-11","content":"svn与git的最主要区别..."}
{"id":"2","title":"ava中泛型的介绍与简单使用","posttime":"2016-06-12","content":"基本操作：CRUD ..."}
{"id":"3","title":"SQL基本操作","posttime":"2016-06-13","content":"svn与git的最主要区别..."}
{"id":"4","title":"Hibernate框架基础","posttime":"2016-06-14","content":"Hibernate框架基础..."}
{"id":"5","title":"Shell基本知识","posttime":"2016-06-15","content":"Shell是什么..."}
上传到阿里云E-MapReduce集群，使用scp远程拷贝命令上传文件：

scp blog.json root@你的弹性公网IP:/root
把blog.json上传至HDFS：

hadoop fs -mkdir /work
hadoop fs -put blog.json /work
上传JAR包
把maven工程target目录下的jar包上传至阿里云E-MapReduce集群：

scp target/emrtoes-1.0-SNAPSHOT-jar-with-dependencies.jar root@YourIP:/root
执行MR作业
执行以下命令：

hadoop jar emrtoes-1.0-SNAPSHOT-jar-with-dependencies.jar /work/blog.json
运行成功的话，控制台会输出如下图所示信息：


结果验证
执行以下操作验证数据是否成功写入Elasticsearch。

curl -u elastic -XGET es-cn-v0h0jdp990001rta9.elasticsearch.aliyuncs.com:9200/blog/_search?pretty

也可以通过Kibana查看：

API分析
Map过程按行读入，input kye的类型为Object，input value的类型为Text。输出的key为NullWritable类型，NullWritable是Writable的一个特殊类，实现方法为空实现，不从数据流中读数据，也不写入数据，只充当占位符。

MapReduce中如果不需要使用键或值，就可以将键或值声明为NullWritable，这里把输出的key设置NullWritable类型。输出为BytesWritable类型，把json字符串序列化。

在本示例中只需要写入，因此没有Reduce过程。

参数配置说明
conf.set(“es.net.http.auth.user”, “你的X-PACK用户名”)
设置X-PACK的用户名

conf.set(“es.net.http.auth.pass”, “你的X-PACK密码”)
设置X-PACK的密码

conf.setBoolean(“mapred.map.tasks.speculative.execution”, false)
关闭mapper阶段的执行推测

conf.setBoolean(“mapred.reduce.tasks.speculative.execution”, false)
关闭reducer阶段的执行推测

conf.set(“es.nodes”, “你的Elasticsearch内网地址”)
配置Elasticsearch的IP和端口

conf.set(“es.resource”, “blog/yunqi”)
设置索引到Elasticsearch的索引名和类型名

conf.set(“es.mapping.id”, “id”)
设置文档id，这个参数”id”是文档中的id字段

conf.set(“es.input.json”, “yes”)
指定输入的文件类型为json

job.setInputFormatClass(TextInputFormat.class)
设置输入流为文本类型

job.setOutputFormatClass(EsOutputFormat.class)
设置输出为EsOutputFormat类型

job.setMapOutputKeyClass(NullWritable.class)
设置Map的输出key类型为NullWritable类型

job.setMapOutputValueClass(BytesWritable.class)
设置Map的输出value类型为BytesWritable类型

FileInputFormat.setInputPaths(job, new Path(otherArgs[0]))
传入HDFS上的文件路径

============

logstash部署
更新时间：2019-03-20 11:04:07

编辑 ·
 · 我的收藏
本页目录
依赖环境准备
测试用例
常见问题解决
补充说明
依赖环境准备
购买阿里云ES 以及 能够同时访问自建集群和阿里云ES的ECS实例（已符合条件的ECS不需要重复购买），并准备1.8及以上版本的JDK。
可以是经典网络的ECS实例，前提是该ECS实例能够通过 经典网络问题 访问VPC内的阿里云ES服务。

下载5.5.3版本的 logstash。
在 elastic官网 页面中，找到与ElasticSearch版本一致的 logstash 下载（建议下载5.5.3版）。

对下载的 logstash 压缩包进行解压缩。
tar -xzvf logstash-5.5.3.tar.gz
# ElasticSearch从5.x版本之后，进行了配置文件的严格校验。
测试用例
创建数据接入账号密码
创建角色
curl -XPOST -H "Content-Type: application/json" -u elastic:es-password http://***instanceId***.elasticsearch.aliyuncs.com:9200/_xpack/security/role/***role-name*** -d '{"cluster": ["manage_index_templates", "monitor"],"indices": [{"names": [ "logstash-*" ], "privileges":["write","delete","create_index"]}]}'
# es-password 是您登录kibana的密码
# ***instanceId*** 是您的阿里云ES实例id
# ***role-name*** 您想使用的角色名称
# logstash默认的索引名称以logstash-当前日期命名，所以在添加用户角色的时候，需要有对logstash-*索引读写权限。
创建用户
curl -XPOST -H "Content-Type: application/json" -u elastic:es-password http://***instanceId***.elasticsearch.aliyuncs.com:9200/_xpack/security/user/***user-name*** -d '{"password" : "***logstash-password***","roles" : ["***role-name***"],"full_name" : "***your full name***"}'
# es-password 是您登录kibana的密码
# ***instanceId*** 是您的阿里云ES实例id
# ***user-name*** 是您想创建的数据接入用户名
# ***logstash-password*** 是您创建的数据接入用户的密码
# ***role-name*** 是您之前创建的角色名称
# ***your full name*** 为当前用户名设置一个全名描述
说明 以上创建角色和用户，同样可以在 kibana 页面中进行配置。
添加角色

添加用户

编写conf文件
详情请参见官方 配置文件结构 文档。

样例
在 ECS 中创建 test.conf 文件，添加以下配置：
input {
    file {
        path => "/your/file/path/xxx"
        }
}
filter {
}
output {
  elasticsearch {
    hosts => ["http://***instanceId***.elasticsearch.aliyuncs.com:9200"]
    user => "***user-name***"
    password => "***logstash-password***"
  }
}
# ***instanceId*** 是您的阿里云ES实例id
# ***user-name*** 是您想创建的数据接入用户名
# ***logstash-password*** 是您创建的数据接入用户的密码
# 用户名和密码需要用英文引号引起来防止特殊字符在启动logstash时报错
执行
按照配置的conf文件，执行logstash：
bin/logstash -f path/to/your/test.conf
# 在logstash提供了丰富的 input、filter、output 插件，只需要简单的配置就可是实现数据的流转。
# 该例子展示了通过 logstash 获取 file 中的变化，提交到 elasticsearch 集群。只要监控的 file 文件有新增内容，logstash 就会自动的索引到 elasticsearch 集群中。
常见问题解决
更改集群自动创建索引配置

阿里云Elasticsearch为了保证用户操作数据的时候的安全性，默认把自动创建索引的配置设置为不允许。

logstash在上传数据的时候是采用的提交数据创建索引的方式，并不是采用的create index的 API 创建的索引。所以在使用 logstash 上传数据的时候，需要把集群的自动创建索引的配置先设置为允许。

注意
修改该配置并确认后，阿里云ES集群会自动重启。

没有创建索引没有权限

核对一下自己创建的接入数据的用户拥有的角色，是否具有write、delete、create_index这几个的权限。

内存不足

logstash 默认配置的是1g内存，如果您申请的 ECS 内存不足，可以适当调小 logstash 内存使用。修改 config/jvm.options 中的内存配置。

配置test.conf时用户名密码没有添加引号

如果在配置您的任务文件时，即上文提到的test.conf文件，用户名和密码中有特殊字符但是又没有用引号括起来，就会出现上述的错误信息。

补充说明
如果希望可以监控 logstash 节点，并收集监控日志：

需要您在安装 logstash 的时候同时为 logstash 安装 X-Pack 的插件，详情请参见下载地址 。
在完成下载后，需要对X-Pack进行部署安装。
bin/logstash-plugin install
        file:///path/to/file/x-pack-5.5.3.zip
添加 logstash 监控用户，阿里云ElasticSearch集群默认禁掉 logstash_system 用户，需要创建一个以角色为 logstash_system 的用户名（用户名不可为 logstash_system）。可以根据自己的意愿命名，本文以用户 logstash_system_monitor 为例，推荐以下两种方式创建用户。
通过 kibana 管理模块添加监控用户
登录 kibana 管理页面，参考以下图片操作。

点击 create user 功能按钮。

输入需要的信息保存提交。

通过指令添加用户
curl -u elastic:es-password -XPOST http://***instanceId***.elasticsearch.aliyuncs.com:9200/_xpack/security/user/logstash_system_monitor -d '{"password" : "***logstash-monitor-password***","roles" : ["logstash_system"],"full_name" : "your full name"}'
# es-password 是您登录kibana的密码
# ***instanceId*** 是您的阿里云ES实例id
# ***logstash-monitor-password*** 是您创建的 logstash_system_monitor 的密码

--------
自建ES迁移
KB: 61145 ·
更新时间：2019-04-10 12:35:19

编辑 ·
本页目录
前提条件
执行流程
创建索引
同步速度优化
数据迁移
批量创建老集群索引
前提条件
本文档主要用于将基于阿里云ECS自建的ES迁移至阿里云ES，参考本文档做迁移必须先满足以下条件，如果不满足以下条件需要通过其它迁移方案做迁移，详情请参见logstash部署：

自建ES所在ECS必需是VPC网络（不支持Classiclink方式打通的ECS），并且自建ES所在ECS对应VPC必须与阿里云ES对应VPC相同。
可以通过中控机器（或者任意一台机器）执行该脚本，前提是该中控机器可以同时访问新老ES集群的9200端口。
VPC安全组不能限制 IP白名单，并且需要开启9200端口。
VPC安全组不能限制阿里云ES实例各节点IP（kibana控制台可查看阿里云ES实例各节点IP）。
可以在执行脚本的机器上通过 curl -XGET http://<host>:9200 验证。
执行流程
创建索引。
数据迁移。
创建索引
需要参考老集群中需要迁移的索引配置提前在新集群中创建索引，或者新集群可以使用动态创建索引（需开启自动创建索引）和动态映射（不建议）功能。

附件中提供了python脚本（indiceCreate.py），可以拉取所有的索引并创建，只设置分片数和0副本，其余需要单独调整。

说明 如果执行curl命令提示以下报错，可以在curl命令中再添加 -H "Content-Type: application/json"参数重试。
{"error":"Content-Type header [application/x-www-form-urlencoded] is not supported","status":406}

// 获取老集群中所有索引信息，如果没有权限可去掉"-u user:pass"参数，oldClusterHost为老集群的host，注意替换。
  curl -u user:pass -XGET http://oldClusterHost/_cat/indices | awk '{print $3}'
  // 参考上面返回的索引列表，获取需要迁移的指定用户索引的setting和mapping，注意替换indexName为要查询的用户索引名。
  curl -u user:pass -XGET http://oldClusterHost/indexName/_settings,_mapping?pretty=true
  // 参考上面获取到的对应索引的_settings,_mapping信息，在新集群中创建对应索引，索引副本数可以先设置为0，用于加快数据同步速度，数据迁移完成后再重置副本数为1。
  //其中newClusterHost是新集群的host，testindex是已经创建的索引名，testtype是对应索引的type。
  curl -u user:pass -XPUT http://<newClusterHost>/<testindex> -d '{
    "testindex" : {
        "settings" : {
            "number_of_shards" : "5", //假设老集群中对应索引的shard数是5个
            "number_of_replicas" : "0" //设置索引副本为0
          }
        },
        "mappings" : { //假设老集群中对应索引的mappings配置如下
            "testtype" : {
                "properties" : {
                    "uid" : {
                        "type" : "long"
                    },
                    "name" : {
                        "type" : "text"
                    },
                    "create_time" : {
                      "type" : "long"
                    }
                }
           }
       }
   }
}'
同步速度优化
说明 如果单索引数据量比较大，可以在迁移前将目的索引的副本数设置为 0，刷新时间为 -1。待数据迁移完成后，再更改回来，这样可以加快数据同步速度。
// 迁移索引数据前可以先将索引副本数设为0，不刷新，用于加快数据迁移速度。
curl -u user:password -XPUT 'http://<host:port>/indexName/_settings' -d' {
        "number_of_replicas" : 0,
        "refresh_interval" : "-1"
}'
// 索引数据迁移完成后，可以重置索引副本数为1，刷新时间1s（1s是默认值）。
curl -u user:password -XPUT 'http://<host:port>/indexName/_settings' -d' {
        "number_of_replicas" : 1,
        "refresh_interval" : "1s"
}'
数据迁移
为保证数据迁移结束后一致性，需要上游业务 停止老集群的写操作，读服务可以正常进行。迁移完毕后，直接切换到新集群进行读写。如不停止写操作可能会存在数据最终的不一致！

说明
使用下述方案迁移时，如果是通过 IP + Port访问老集群，则必须在新集群的 yml中配置 reindex 白名单（为老集群的 IP 地址），例如：reindex.remote.whitelist: 1.1.1.1:9200,1.2.*.*:*
如果使用域名访问，则不允许通过 http://host:port/path 这种（带path）形式访问。
数据量小
使用reindex.sh脚本。

#!/bin/bash
# file:reindex.sh
indexName="你的索引名"
newClusterUser="新集群用户名"
newClusterPass="新集群密码"
newClusterHost="新集群host"
oldClusterUser="老集群用户名"
oldClusterPass="老集群密码"
# 老集群host必须是[scheme]://[host]:[port]，例如http://10.37.1.1:9200
oldClusterHost="老集群host"
curl -u ${newClusterUser}:${newClusterPass} -XPOST "http://${newClusterHost}/_reindex?pretty" -H "Content-Type: application/json" -d'{
    "source": {
        "remote": {
            "host": "'${oldClusterHost}'",
            "username": "'${oldClusterUser}'",
            "password": "'${oldClusterPass}'"
        },
        "index": "'${indexName}'",
        "query": {
            "match_all": {}
        }
    },
    "dest": {
       "index": "'${indexName}'"
    }
}'
数据量大、无删除操作、有更新时间
数据量较大且无删除操作时，可以使用滚动迁移的方式，减小停止写服务的时间。滚动迁移需要有一个类似于更新时间的字段代表新数据的写时序。可以在数据迁移完成后，再停止写服务，快速更新一次。即可切换到新集群，恢复读写。

#!/bin/bash
# file: circleReindex.sh
# CONTROLLING STARTUP:
# 这是通过reindex操作远程重建索引的脚本，要求：
# 1. 新集群已经创建完索引，或者支持自动创建和动态映射。
# 2. 新集群必须在yml里配置IP白名单 reindex.remote.whitelist: 172.16.123.*:9200
# 3. host必须是[scheme]://[host]:[port]
USAGE="Usage: sh circleReindex.sh <count>
       count: 执行次数，多次（负数为循环）增量执行或者单次执行
Example:
        sh circleReindex.sh 1
        sh circleReindex.sh 5
        sh circleReindex.sh -1"
indexName="你的索引名"
newClusterUser="新集群用户名"
newClusterPass="新集群密码"
oldClusterUser="老集群用户名"
oldClusterPass="老集群密码"
## http://myescluster.com
newClusterHost="新集群host"
# 老集群host必须是[scheme]://[host]:[port]，例如http://10.37.1.1:9200
oldClusterHost="老集群host"
timeField="更新时间字段"
reindexTimes=0
lastTimestamp=0
curTimestamp=`date +%s`
hasError=false
function reIndexOP() {
    reindexTimes=$[${reindexTimes} + 1]
    curTimestamp=`date +%s`
    ret=`curl -u ${newClusterUser}:${newClusterPass} -XPOST "${newClusterHost}/_reindex?pretty" -H "Content-Type: application/json" -d '{
        "source": {
            "remote": {
                "host": "'${oldClusterHost}'",
                "username": "'${oldClusterUser}'",
                "password": "'${oldClusterPass}'"
            },
            "index": "'${indexName}'",
            "query": {
                "range" : {
                    "'${timeField}'" : {
                        "gte" : '${lastTimestamp}',
                        "lt" : '${curTimestamp}'
                    }
                }
            }
        },
        "dest": {
            "index": "'${indexName}'"
        }
    }'`
    lastTimestamp=${curTimestamp}
    echo "第${reindexTimes}次reIndex，本次更新截止时间 ${lastTimestamp} 结果：${ret}"
    if [[ ${ret} == *error* ]]; then
        hasError=true
        echo "本次执行异常，中断后续执行操作～～，请检查"
    fi
}
function start() {
    ## 负数就不停循环执行
    if [[ $1 -lt 0 ]]; then
        while :
        do
            reIndexOP
        done
    elif [[ $1 -gt 0 ]]; then
        k=0
        while [[ k -lt $1 ]] && [[ ${hasError} == false ]]; do
            reIndexOP
            let ++k
        done
    fi
}
## main
if [ $# -lt 1 ]; then
    echo "$USAGE"
    exit 1
fi
echo "开始执行索引 ${indexName} 的 ReIndex操作"
start $1
echo "总共执行 ${reindexTimes} 次 reIndex 操作"
数据量大、无删除操作、无更新时间
当数据量较大，且索引的 mapping 中没有定义更新时间字段时，需要由上游业务修改代码添加更新时间字段。添加完成后可以先将历史数据迁移完，然后再使用上述的第2种方案。

下面是迁移没有更新时间字段的老数据脚本。

#!/bin/bash
# file:miss.sh
indexName="你的索引名"
newClusterUser="新集群用户名"
newClusterPass="新集群密码"
newClusterHost="新集群host"
oldClusterUser="老集群用户名"
oldClusterPass="老集群密码"
# 老集群host必须是[scheme]://[host]:[port]，例如http://10.37.1.1:9200
oldClusterHost="老集群host"
timeField="updatetime"
curl -u ${newClusterUser}:${newClusterPass} -XPOST "http://${newClusterHost}/_reindex?pretty" -H "Content-Type: application/json" -d '{
    "source": {
        "remote": {
            "host": "'${oldClusterHost}'",
            "username": "'${oldClusterUser}'",
            "password": "'${oldClusterPass}'"
        },
        "index": "'${indexName}'",
        "query": {
            "bool": {
                "must_not": {
                    "exists": {
                        "field": "'${timeField}'"
                    }
                }
            }
        }
    },
    "dest": {
       "index": "'${indexName}'"
    }
}'
不停止写服务
敬请期待。

批量创建老集群索引
下面是一个在新集群中，批量创建老集群索引的python脚本。该脚本默认新创建的索引副本数为0。

#!/usr/bin/python
# -*- coding: UTF-8 -*-
# 文件名：indiceCreate.py
import sys
import base64
import time
import httplib
import json
## 老集群host（ip+port）
oldClusterHost = "old-cluster.com"
## 老集群用户名，可为空
oldClusterUserName = "old-username"
## 老集群密码，可为空
oldClusterPassword = "old-password"
## 新集群host（ip+port）
newClusterHost = "new-cluster.com"
## 新集群用户名，可为空
newClusterUser = "new-username"
## 新集群密码，可为空
newClusterPassword = "new-password"
DEFAULT_REPLICAS = 0
def httpRequest(method, host, endpoint, params="", username="", password=""):
    conn = httplib.HTTPConnection(host)
    headers = {}
    if (username != "") :
        'Hello {name}, your age is {age} !'.format(name = 'Tom', age = '20')
        base64string = base64.encodestring('{username}:{password}'.format(username = username, password = password)).replace('\n', '')
        headers["Authorization"] = "Basic %s" % base64string;
    if "GET" == method:
        headers["Content-Type"] = "application/x-www-form-urlencoded"
        conn.request(method=method, url=endpoint, headers=headers)
    else :
        headers["Content-Type"] = "application/json"
        conn.request(method=method, url=endpoint, body=params, headers=headers)
    response = conn.getresponse()
    res = response.read()
    return res
def httpGet(host, endpoint, username="", password=""):
    return httpRequest("GET", host, endpoint, "", username, password)
def httpPost(host, endpoint, params, username="", password=""):
    return httpRequest("POST", host, endpoint, params, username, password)
def httpPut(host, endpoint, params, username="", password=""):
    return httpRequest("PUT", host, endpoint, params, username, password)
def getIndices(host, username="", password=""):
    endpoint = "/_cat/indices"
    indicesResult = httpGet(oldClusterHost, endpoint, oldClusterUserName, oldClusterPassword)
    indicesList = indicesResult.split("\n")
    indexList = []
    for indices in indicesList:
        if (indices.find("open") > 0):
            indexList.append(indices.split()[2])
    return indexList
def getSettings(index, host, username="", password=""):
    endpoint = "/" + index + "/_settings"
    indexSettings = httpGet(host, endpoint, username, password)
    print index + "  原始settings如下：\n" + indexSettings
    settingsDict = json.loads(indexSettings)
    ## 分片数默认和老集群索引保持一致
    number_of_shards = settingsDict[index]["settings"]["index"]["number_of_shards"]
    ## 副本数默认为0
    number_of_replicas = DEFAULT_REPLICAS
    newSetting = "\"settings\": {\"number_of_shards\": %s, \"number_of_replicas\": %s}" % (number_of_shards, number_of_replicas)
    return newSetting
def getMapping(index, host, username="", password=""):
    endpoint = "/" + index + "/_mapping"
    indexMapping = httpGet(host, endpoint, username, password)
    print index + " 原始mapping如下：\n" + indexMapping
    mappingDict = json.loads(indexMapping)
    mappings = json.dumps(mappingDict[index]["mappings"])
    newMapping = "\"mappings\" : " + mappings
    return newMapping
def createIndexStatement(oldIndexName):
    settingStr = getSettings(oldIndexName, oldClusterHost, oldClusterUserName, oldClusterPassword)
    mappingStr = getMapping(oldIndexName, oldClusterHost, oldClusterUserName, oldClusterPassword)
    createstatement = "{\n" + str(settingStr) + ",\n" + str(mappingStr) + "\n}"
    return createstatement
def createIndex(oldIndexName, newIndexName=""):
    if (newIndexName == "") :
        newIndexName = oldIndexName
    createstatement = createIndexStatement(oldIndexName)
    print "新索引 " + newIndexName + " 的setting和mapping如下：\n" + createstatement
    endpoint = "/" + newIndexName
    createResult = httpPut(newClusterHost, endpoint, createstatement, newClusterUser, newClusterPassword)
    print "新索引 " + newIndexName + " 创建结果：" + createResult
## main
indexList = getIndices(oldClusterHost, oldClusterUserName, oldClusterPassword)
systemIndex = []
for index in indexList:
    if (index.startswith(".")):
        systemIndex.append(index)
    else :
        createIndex(index, index)
if (len(systemIndex) > 0) :
    for index in systemIndex:
        print index + "  或许是系统索引，不会重新创建，如有需要，请单独处理～"
说明 使用logstash做数据迁移，请参考 Logstash迁移Elasticsearch数据方法解读。

============


如果面试的时候碰到这样一个面试题：ElasticSearch（以下简称ES） 在数据量很大的情况下（数十亿级别）如何提高查询效率？

很多时候数据量大了，特别是有几亿条数据的时候，可能你会懵逼的发现，跑个搜索怎么一下 5~10s，坑爹了。

第一次搜索的时候，是 5~10s，后面反而就快了，可能就几百毫秒。

然后你就很懵，每个用户第一次访问都会比较慢，比较卡么？所以你要是没玩儿过 ES，或者就是自己玩玩儿 Demo，被问到这个问题容易懵逼，显示出你对 ES 确实玩的不怎么样？

说实话，ES 性能优化是没有银弹的。啥意思呢？就是不要期待着随手调一个参数，就可以万能的应对所有的性能慢的场景。

也许有的场景是你换个参数，或者调整一下语法，就可以搞定，但是绝对不是所有场景都可以这样。

性能优化的杀手锏：Filesystem Cache

你往 ES 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 Filesystem Cache 里面去。



整个过程，如下图所示：





ES 的搜索引擎严重依赖于底层的 Filesystem Cache，你如果给 Filesystem Cache 更多的内存，尽量让内存可以容纳所有的 IDX Segment File 索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。



性能差距究竟可以有多大？我们之前很多的测试和压测，如果走磁盘一般肯定上秒，搜索性能绝对是秒级别的，1 秒、5 秒、10 秒。



但如果是走 Filesystem Cache，是走纯内存的，那么一般来说性能比走磁盘要高一个数量级，基本上就是毫秒级的，从几毫秒到几百毫秒不等。



来看一个真实的案例：某个公司 ES 节点有 3 台机器，每台机器看起来内存很多 64G，总内存就是 64 * 3 = 192G。



每台机器给 ES JVM Heap 是 32G，那么剩下来留给 Filesystem Cache 的就是每台机器才 32G，总共集群里给 Filesystem Cache 的就是 32 * 3 = 96G 内存。



而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，ES 数据量是 1T，那么每台机器的数据量是 300G。



这样性能好吗？



Filesystem Cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。



归根结底，你要让 ES 性能好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。



根据我们自己的生产环境实践经验，最佳的情况下，是仅仅在 ES 中就存少量的数据。



也就是说，你要用来搜索的那些索引，如果内存留给 Filesystem Cache 的是 100G，那么你就将索引数据控制在 100G 以内。这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在1秒以内。



比如说你现在有一行数据：id，name，age .... 30 个字段。但是你现在搜索，只需要根据 id，name，age 三个字段来搜索。



如果你傻乎乎往 ES 里写入一行数据所有的字段，就会导致 90% 的数据是不用来搜索的。



但是呢，这些数据硬是占据了 ES 机器上的 Filesystem Cache 的空间，单条数据的数据量越大，就会导致 Filesystem Cahce 能缓存的数据就越少。



其实，仅仅写入 ES 中要用来检索的少数几个字段就可以了，比如说就写入 es id，name，age 三个字段。



然后你可以把其他的字段数据存在 MySQL/HBase 里，我们一般是建议用 ES + HBase 这么一个架构。



HBase是列式数据库，其特点是适用于海量数据的在线存储，就是对 HBase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了。



从 ES 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id，然后根据 doc id 到 HBase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。



而写入 ES 的数据最好小于等于，或者是略微大于 ES 的 Filesystem Cache 的内存容量。



然后你从 ES 检索可能就花费 20ms，然后再根据 ES 返回的 id 去 HBase 里查询，查 20 条数据，可能也就耗费个 30ms。



如果你像原来那么玩儿，1T 数据都放 ES，可能会每次查询都是 5~10s，而现在性能就会很高，每次查询就是 50ms。







数据预热



假如你就按照上述的方案去做了，ES 集群中每个机器写入的数据量还是超过了 Filesystem Cache 一倍。



比如说你写入一台机器 60G 数据，结果 Filesystem Cache 就 30G，还是有 30G 数据留在了磁盘上。



这种情况下，其实可以做数据预热。举个例子，拿微博来说，你可以把一些大 V，平时看的人很多的数据，提前在后台搞个系统。



然后每隔一会儿，自己的后台系统去搜索一下热数据，刷到 Filesystem Cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。



或者是电商，你可以将平时查看最多的一些商品，比如说 iPhone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 Filesystem Cache 里去。



总之，就是对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统。



然后对热数据每隔一段时间，就提前访问一下，让数据进入 Filesystem Cache 里面去。这样下次别人访问的时候，性能一定会好很多。





冷热分离



ES 可以做类似于 MySQL 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。



最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 Filesystem OS Cache 里，别让冷数据给冲刷掉。



还是来一个例子，假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 Shard。3 台机器放热数据 Index，另外 3 台机器放冷数据 Index。



这样的话，你大量的时间是在访问热数据 Index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 Filesystem Cache 里面了，就可以确保热数据的访问性能是很高的。



但是对于冷数据而言，是在别的 Index 里的，跟热数据 Index 不在相同的机器上，大家互相之间都没什么联系了。



如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。







ES中的关联查询



对于 MySQL，我们经常有一些复杂的关联查询，在 ES 里该怎么玩儿？



ES 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 ES 中。搜索的时候，就不需要利用 ES 的搜索语法来完成 Join 之类的关联搜索了。





Document 模型设计



Document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。



ES 能支持的操作就那么多，不要考虑用 ES 做一些它不好操作的事情。如果真的有那种操作，尽量在 Document 模型设计的时候，写入的时候就完成。



另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。







分页性能优化



ES 的分页是较坑的，为啥呢？举个例子吧，假如你每页是 10 条数据，你现在要查询第 100 页，实际上是会把每个 Shard 上存储的前 1000 条数据都查到一个协调节点上。



如果你有 5 个 Shard，那么就有 5000 条数据，接着协调节点对这 5000 条数据进行一些合并、处理，再获取到最终第 100 页的 10 条数据。



由于是分布式的，你要查第 100 页的 10 条数据，不可能说从 5 个 Shard，每个 Shard 就查 2 条数据，最后到协调节点合并成 10 条数据吧？



你必须得从每个 Shard 都查 1000 条数据过来，然后根据你的需求进行排序、筛选等等操作，最后再次分页，拿到里面第 100 页的数据。



你翻页的时候，翻的越深，每个 Shard 返回的数据就越多，而且协调节点处理的时间越长，非常坑爹。所以用 ES 做分页的时候，你会发现越翻到后面，就越是慢。



我们之前也是遇到过这个问题，用 ES 作分页，前几页就几十毫秒，翻到 10 页或者几十页的时候，基本上就要 5~10 秒才能查出来一页数据了。



有什么解决方案吗？两个思路：



一、不允许深度分页（默认深度分页性能很差）。跟产品经理说，你系统不允许翻那么深的页，默认翻的越深，性能就越差。



二、类似于 App 里的推荐商品不断下拉出来一页一页的；类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 Scroll API，关于如何使用，大家可以自行上网搜索学习一下。



Scroll是如何做的呢？它会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页、下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。



但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。



所以现在很多产品，都是不允许你随意翻页的，你只能往下拉，一页一页的翻。



使用时需要注意，初始化必须指定 Scroll 参数，告诉 ES 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。



除了用 Scroll API，你也可以用 search_after 来做。search_after 的思想是使用前一页的结果来帮助检索下一页的数据。



显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 Sort 字段。
